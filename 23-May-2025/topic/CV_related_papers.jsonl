{'arxiv_id': 'arXiv:2505.16969', 'title': '3D Equivariant Visuomotor Policy Learning via Spherical Projection', 'authors': 'Boce Hu, Dian Wang, David Klee, Heng Tian, Xupeng Zhu, Haojie Huang, Robert Platt, Robin Walters', 'link': 'https://arxiv.org/abs/2505.16969', 'abstract': 'Equivariant models have recently been shown to improve the data efficiency of diffusion policy by a significant margin. However, prior work that explored this direction focused primarily on point cloud inputs generated by multiple cameras fixed in the workspace. This type of point cloud input is not compatible with the now-common setting where the primary input modality is an eye-in-hand RGB camera like a GoPro. This paper closes this gap by incorporating into the diffusion policy model a process that projects features from the 2D RGB camera image onto a sphere. This enables us to reason about symmetries in SO(3) without explicitly reconstructing a point cloud. We perform extensive experiments in both simulation and the real world that demonstrate that our method consistently outperforms strong baselines in terms of both performance and sample efficiency. Our work is the first SO(3)-equivariant policy learning framework for robotic manipulation that works using only monocular RGB inputs.', 'abstract_zh': 'Equivariant模型已被证明显著提高了扩散策略的数据效率。然而，此前在此方向上的工作主要集中在由工作空间中固定多个相机生成的点云输入。这种点云输入与当前主要输入模态为手持RGB相机（如GoPro）的情况不兼容。本文通过将特征从2D RGB相机图像投影到球面上的方法，填补了这一缺口，使我们能够在不显式重建点云的情况下推理SO(3)中的对称性。我们在模拟和现实世界中的大量实验表明，我们的方法在性能和样本效率方面均优于强baseline。我们的工作是第一个仅使用单目RGB输入实现SO(3)-等变政策学习的机器人操作框架。', 'title_zh': '基于球面投影的3D等变视觉运动策略学习'}
{'arxiv_id': 'arXiv:2505.16726', 'title': 'D-LIO: 6DoF Direct LiDAR-Inertial Odometry based on Simultaneous Truncated Distance Field Mapping', 'authors': 'Lucia Coto-Elena, J.E. Maese, L. Merino, F. Caballero', 'link': 'https://arxiv.org/abs/2505.16726', 'abstract': 'This paper presents a new approach for 6DoF Direct LiDAR-Inertial Odometry (D-LIO) based on the simultaneous mapping of truncated distance fields on CPU. Such continuous representation (in the vicinity of the points) enables working with raw 3D LiDAR data online, avoiding the need of LiDAR feature selection and tracking, simplifying the odometry pipeline and easily generalizing to many scenarios. The method is based on the proposed Fast Truncated Distance Field (Fast-TDF) method as a convenient tool to represent the environment. Such representation enables i) solving the LiDAR point-cloud registration as a nonlinear optimization process without the need of selecting/tracking LiDAR features in the input data, ii) simultaneously producing an accurate truncated distance field map of the environment, and iii) updating such map at constant time independently of its size. The approach is tested using open datasets, aerial and ground. It is also benchmarked against other state-of-the-art odometry approaches, demonstrating the same or better level of accuracy with the added value of an online-generated TDF representation of the environment, that can be used for other robotics tasks as planning or collision avoidance. The source code is publicly available at this https URL', 'abstract_zh': '本文提出了一种基于CPU实时映射截断距离场的6DoF直接LiDAR-惯性里程计（D-LIO）新方法。这种连续表示（在点的附近）使得能够在线处理原始3D LiDAR数据，避免了LiDAR特征选择和跟踪的需要，简化了里程计管道，并易于适应多种场景。该方法基于提出的快速截断距离场（Fast-TDF）方法作为环境表示的便捷工具。这种表示使得：i) 可以将LiDAR点云配准作为无需选择/跟踪输入数据中LiDAR特征的非线性优化过程来解决；ii) 同时生成环境的准确截断距离场地图；iii) 独立于地图大小以恒定时间更新此类地图。该方法使用开放数据集、空域和地面数据进行了测试，并与现有的其他先进里程计方法进行了基准测试，证明了与之相当或更高的精度水平，并增加了在线生成环境截断距离场表示的附加价值，该表示可用于其他机器人任务如规划或碰撞避免。源代码可在以下网址获取。', 'title_zh': 'D-LIO：基于同时截断距离场映射的6自由度直接激光雷达-惯性里程计'}
{'arxiv_id': 'arXiv:2505.16165', 'title': 'RE-TRIP : Reflectivity Instance Augmented Triangle Descriptor for 3D Place Recognition', 'authors': 'Yechan Park, Gyuhyeon Pak, Euntai Kim', 'link': 'https://arxiv.org/abs/2505.16165', 'abstract': 'While most people associate LiDAR primarily with its ability to measure distances and provide geometric information about the environment (via point clouds), LiDAR also captures additional data, including reflectivity or intensity values. Unfortunately, when LiDAR is applied to Place Recognition (PR) in mobile robotics, most previous works on LiDAR-based PR rely only on geometric measurements, neglecting the additional reflectivity information that LiDAR provides. In this paper, we propose a novel descriptor for 3D PR, named RE-TRIP (REflectivity-instance augmented TRIangle descriPtor). This new descriptor leverages both geometric measurements and reflectivity to enhance robustness in challenging scenarios such as geometric degeneracy, high geometric similarity, and the presence of dynamic objects. To implement RE-TRIP in real-world applications, we further propose (1) a keypoint extraction method, (2) a key instance segmentation method, (3) a RE-TRIP matching method, and (4) a reflectivity-combined loop verification method. Finally, we conduct a series of experiments to demonstrate the effectiveness of RE-TRIP. Applied to public datasets (i.e., HELIPR, FusionPortable) containing diverse scenarios such as long corridors, bridges, large-scale urban areas, and highly dynamic environments -- our experimental results show that the proposed method outperforms existing state-of-the-art methods in terms of Scan Context, Intensity Scan Context, and STD.', 'abstract_zh': '一种结合反射率的三维位置识别人工智能增强三角描述子（RE-TRIP）', 'title_zh': 'RE-TRIP : 反射性实例增强三角描述子的三维场所识别'}
{'arxiv_id': 'arXiv:2505.15863', 'title': 'Generative AI for Autonomous Driving: A Review', 'authors': 'Katharina Winter, Abhishek Vivekanandan, Rupert Polley, Yinzhe Shen, Christian Schlauch, Mohamed-Khalil Bouzidi, Bojan Derajic, Natalie Grabowsky, Annajoyce Mariani, Dennis Rochau, Giovanni Lucente, Harsh Yadav, Firas Mualla, Adam Molin, Sebastian Bernhard, Christian Wirth, Ömer Şahin Taş, Nadja Klein, Fabian B. Flohr, Hanno Gottschalk', 'link': 'https://arxiv.org/abs/2505.15863', 'abstract': 'Generative AI (GenAI) is rapidly advancing the field of Autonomous Driving (AD), extending beyond traditional applications in text, image, and video generation. We explore how generative models can enhance automotive tasks, such as static map creation, dynamic scenario generation, trajectory forecasting, and vehicle motion planning. By examining multiple generative approaches ranging from Variational Autoencoder (VAEs) over Generative Adversarial Networks (GANs) and Invertible Neural Networks (INNs) to Generative Transformers (GTs) and Diffusion Models (DMs), we highlight and compare their capabilities and limitations for AD-specific applications. Additionally, we discuss hybrid methods integrating conventional techniques with generative approaches, and emphasize their improved adaptability and robustness. We also identify relevant datasets and outline open research questions to guide future developments in GenAI. Finally, we discuss three core challenges: safety, interpretability, and realtime capabilities, and present recommendations for image generation, dynamic scenario generation, and planning.', 'abstract_zh': '生成式AI（GenAI）迅速推动了自动驾驶（AD）领域的发展，超越了传统文本、图像和视频生成的应用。我们探索生成模型如何增强汽车任务，如静态地图创建、动态场景生成、轨迹预测和车辆运动规划。通过考察从变分自编码器（VAEs）、生成对抗网络（GANs）、可逆神经网络（INNs）到生成转换器（GTs）和扩散模型（DMs）等多种生成方法，我们强调并比较了它们在AD特定应用中的能力和局限性。此外，我们讨论了将传统技术与生成方法相结合的混合方法，并强调了其改进的适应性和鲁棒性。我们还确定了相关数据集，并概述了开放的研究问题，以指导GenAI未来的开发。最后，我们讨论了三个核心挑战：安全性、可解释性和实时能力，并为图像生成、动态场景生成和规划提出了建议。', 'title_zh': '自主驾驶中的生成AI：一个综述'}
{'arxiv_id': 'arXiv:2505.16199', 'title': 'Velocity Completion Task and Method for Event-based Player Positional Data in Soccer', 'authors': 'Rikuhei Umemoto, Keisuke Fujii', 'link': 'https://arxiv.org/abs/2505.16199', 'abstract': "In many real-world complex systems, the behavior can be observed as a collection of discrete events generated by multiple interacting agents. Analyzing the dynamics of these multi-agent systems, especially team sports, often relies on understanding the movement and interactions of individual agents. However, while providing valuable snapshots, event-based positional data typically lacks the continuous temporal information needed to directly calculate crucial properties such as velocity. This absence severely limits the depth of dynamic analysis, preventing a comprehensive understanding of individual agent behaviors and emergent team strategies. To address this challenge, we propose a new method to simultaneously complete the velocity of all agents using only the event-based positional data from team sports. Based on this completed velocity information, we investigate the applicability of existing team sports analysis and evaluation methods. Experiments using soccer event data demonstrate that neural network-based approaches outperformed rule-based methods regarding velocity completion error, considering the underlying temporal dependencies and graph structure of player-to-player or player-to-ball interaction. Moreover, the space evaluation results obtained using the completed velocity are closer to those derived from complete tracking data, highlighting our method's potential for enhanced team sports system analysis.", 'abstract_zh': '在许多现实世界的复杂系统中，行为可以观察到是由多个相互作用的代理生成的离散事件组成的集合。分析这些多代理系统的动力学，尤其是在研究团队运动时，通常依赖于理解单个代理的运动和相互作用。然而，虽然事件定位数据提供了有价值的快照，但它们通常缺乏直接计算关键属性（如速度）所需的连续时间信息。这种缺失严重限制了动力学分析的深度，阻碍了对单个代理行为和涌现团队策略的全面理解。为了解决这一挑战，我们提出了一个新的方法，仅使用团队运动的事件定位数据来同时完成所有代理的速度。基于这种完成的速度信息，我们探讨了现有团队运动分析和评估方法的应用性。使用足球事件数据的实验证明，基于神经网络的方法在速度填充误差方面优于基于规则的方法，考虑到球员之间或球员与球之间相互作用的潜在时间依赖性和图结构。此外，使用完成的速度进行的空间评估结果与基于完全追踪数据得出的结果更为接近，突显了我们方法在增强团队运动系统分析方面的潜力。', 'title_zh': '基于事件的足球运动员位置数据的速率完成任务与方法'}
{'arxiv_id': 'arXiv:2505.16915', 'title': 'DetailMaster: Can Your Text-to-Image Model Handle Long Prompts?', 'authors': 'Qirui Jiao, Daoyuan Chen, Yilun Huang, Xika Lin, Ying Shen, Yaliang Li', 'link': 'https://arxiv.org/abs/2505.16915', 'abstract': "While recent text-to-image (T2I) models show impressive capabilities in synthesizing images from brief descriptions, their performance significantly degrades when confronted with long, detail-intensive prompts required in professional applications. We present DetailMaster, the first comprehensive benchmark specifically designed to evaluate T2I models' systematical abilities to handle extended textual inputs that contain complex compositional requirements. Our benchmark introduces four critical evaluation dimensions: Character Attributes, Structured Character Locations, Multi-Dimensional Scene Attributes, and Explicit Spatial/Interactive Relationships. The benchmark comprises long and detail-rich prompts averaging 284.89 tokens, with high quality validated by expert annotators. Evaluation on 7 general-purpose and 5 long-prompt-optimized T2I models reveals critical performance limitations: state-of-the-art models achieve merely ~50% accuracy in key dimensions like attribute binding and spatial reasoning, while all models showing progressive performance degradation as prompt length increases. Our analysis highlights systemic failures in structural comprehension and detail overload handling, motivating future research into architectures with enhanced compositional reasoning. We open-source the dataset, data curation code, and evaluation tools to advance detail-rich T2I generation and enable broad applications that would otherwise be infeasible due to the lack of a dedicated benchmark.", 'abstract_zh': 'DetailMaster：专为评估文本到图像模型处理长细节文本能力的综合基准', 'title_zh': 'DetailMaster: 你的文本到图像模型能处理长提示吗？'}
{'arxiv_id': 'arXiv:2505.16875', 'title': 'T2I-ConBench: Text-to-Image Benchmark for Continual Post-training', 'authors': 'Zhehao Huang, Yuhang Liu, Yixin Lou, Zhengbao He, Mingzhen He, Wenxing Zhou, Tao Li, Kehan Li, Zeyi Huang, Xiaolin Huang', 'link': 'https://arxiv.org/abs/2505.16875', 'abstract': 'Continual post-training adapts a single text-to-image diffusion model to learn new tasks without incurring the cost of separate models, but naive post-training causes forgetting of pretrained knowledge and undermines zero-shot compositionality. We observe that the absence of a standardized evaluation protocol hampers related research for continual post-training. To address this, we introduce T2I-ConBench, a unified benchmark for continual post-training of text-to-image models. T2I-ConBench focuses on two practical scenarios, item customization and domain enhancement, and analyzes four dimensions: (1) retention of generality, (2) target-task performance, (3) catastrophic forgetting, and (4) cross-task generalization. It combines automated metrics, human-preference modeling, and vision-language QA for comprehensive assessment. We benchmark ten representative methods across three realistic task sequences and find that no approach excels on all fronts. Even joint "oracle" training does not succeed for every task, and cross-task generalization remains unsolved. We release all datasets, code, and evaluation tools to accelerate research in continual post-training for text-to-image models.', 'abstract_zh': '持续后训练适应单个文本到图像扩散模型以学习新任务，同时避免单独模型的高昂成本，但 naive 后训练会导致先验知识的遗忘并削弱零-shot 组合性。我们观察到缺乏标准化评估协议阻碍了持续后训练相关研究。为解决这一问题，我们引入了 T2I-ConBench，这是一个统一的持续后训练基准，专门针对项目个性化和领域增强两种实用场景，并从四个维度进行分析：（1）保持通用性，（2）目标任务性能，（3）灾难性遗忘，以及（4）跨任务泛化。该基准结合了自动化指标、人类偏好建模和视觉-语言问答，进行全面评估。我们在三个现实任务序列中对十种代表性方法进行了基准测试，发现没有一种方法在所有方面都表现出色。即使联合“先验”训练也无法在所有任务中成功，跨任务泛化问题尚未解决。我们将所有数据集、代码和评估工具公开，以促进文本到图像模型持续后训练的研究。', 'title_zh': 'T2I-ConBench: 文本到图像持续后训练基准'}
{'arxiv_id': 'arXiv:2505.16836', 'title': 'Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning', 'authors': 'Fanrui Zhang, Dian Li, Qiang Zhang, Chenjun, sinbadliu, Junxiong Lin, Jiahong Yan, Jiawei Liu, Zheng-Jun Zha', 'link': 'https://arxiv.org/abs/2505.16836', 'abstract': 'The rapid spread of multimodal misinformation on social media has raised growing concerns, while research on video misinformation detection remains limited due to the lack of large-scale, diverse datasets. Existing methods often overfit to rigid templates and lack deep reasoning over deceptive content. To address these challenges, we introduce FakeVV, a large-scale benchmark comprising over 100,000 video-text pairs with fine-grained, interpretable annotations. In addition, we further propose Fact-R1, a novel framework that integrates deep reasoning with collaborative rule-based reinforcement learning. Fact-R1 is trained through a three-stage process: (1) misinformation long-Chain-of-Thought (CoT) instruction tuning, (2) preference alignment via Direct Preference Optimization (DPO), and (3) Group Relative Policy Optimization (GRPO) using a novel verifiable reward function. This enables Fact-R1 to exhibit emergent reasoning behaviors comparable to those observed in advanced text-based reinforcement learning systems, but in the more complex multimodal misinformation setting. Our work establishes a new paradigm for misinformation detection, bridging large-scale video understanding, reasoning-guided alignment, and interpretable verification.', 'abstract_zh': '大规模多模态错误信息在社交媒体上的迅速传播引发了广泛关注，但由于缺乏大规模、多样化的数据集，有关视频错误信息检测的研究仍然有限。现有方法往往过度依赖固定模板，并缺乏对欺骗性内容的深层推理。为应对这些挑战，我们引入了FakeVV，这是一个包含超过100,000个详尽标注的视频-文本对的大规模基准数据集。此外，我们还提出了Fact-R1，这是一种融合深度推理与协作规则强化学习的新框架。Fact-R1通过三个阶段的训练过程进行训练：（1）错误信息长链推理指令调优，（2）通过直接偏好优化（DPO）进行偏好对齐，（3）使用新的可验证奖励函数进行组相对策略优化（GRPO）。这使得Fact-R1能够在更复杂的多模态错误信息环境中展现出类似于高级文本强化学习系统中的新兴推理行为。我们的研究确立了一种新的错误信息检测范式，结合了大规模视频理解、推理引导的对齐和可解释的验证。', 'title_zh': '面向可解释的视频错误信息检测的深度推理方法'}
{'arxiv_id': 'arXiv:2505.16792', 'title': "REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training", 'authors': 'Ziqiao Wang, Wangbo Zhao, Yuhao Zhou, Zekai Li, Zhiyuan Liang, Mingjia Shi, Xuanlei Zhao, Pengfei Zhou, Kaipeng Zhang, Zhangyang Wang, Kai Wang, Yang You', 'link': 'https://arxiv.org/abs/2505.16792', 'abstract': "Diffusion Transformers (DiTs) deliver state-of-the-art image quality, yet their training remains notoriously slow. A recent remedy -- representation alignment (REPA) that matches DiT hidden features to those of a non-generative teacher (e.g. DINO) -- dramatically accelerates the early epochs but plateaus or even degrades performance later. We trace this failure to a capacity mismatch: once the generative student begins modelling the joint data distribution, the teacher's lower-dimensional embeddings and attention patterns become a straitjacket rather than a guide. We then introduce HASTE (Holistic Alignment with Stage-wise Termination for Efficient training), a two-phase schedule that keeps the help and drops the hindrance. Phase I applies a holistic alignment loss that simultaneously distills attention maps (relational priors) and feature projections (semantic anchors) from the teacher into mid-level layers of the DiT, yielding rapid convergence. Phase II then performs one-shot termination that deactivates the alignment loss, once a simple trigger such as a fixed iteration is hit, freeing the DiT to focus on denoising and exploit its generative capacity. HASTE speeds up training of diverse DiTs without architecture changes. On ImageNet 256X256, it reaches the vanilla SiT-XL/2 baseline FID in 50 epochs and matches REPA's best FID in 500 epochs, amounting to a 28X reduction in optimization steps. HASTE also improves text-to-image DiTs on MS-COCO, demonstrating to be a simple yet principled recipe for efficient diffusion training across various tasks. Our code is available at this https URL .", 'abstract_zh': '扩散变换器(DiTs)提供了最先进的图像质量，但其训练仍然极度缓慢。一种近期的解决方案——表示对齐(REPA)，能够在早期阶段显著加速训练，但在之后的阶段则会停滞或甚至降低性能。我们将其失败归因于容量不匹配：一旦生成性学生开始建模联合数据分布，教师较低维度的嵌入和注意力模式反而成为了束缚而非指导。然后，我们提出了HASTE（全方位对齐与阶段终止以提高训练效率），这是一种两阶段的时间表，它保持帮助并放弃阻碍。第一阶段应用了全方位对齐损失，同时从教师中蒸馏出注意力图（关系先验）和特征投影（语义锚点）到DiT的中间层，实现快速收敛。第二阶段则进行了单次终止，一旦触发一个简单的条件（如固定迭代次数），就禁用对齐损失，释放DiT专注于降噪并利用其生成能力。HASTE在不改变架构的情况下加快了各种DiTs的训练。在ImageNet 256X256上，它在50个周期内达到了vanilla SiT-XL/2基线的FID，并在500个周期内匹配了REPA最佳的FID，相当于减少了28倍的优化步骤。HASTE还改进了MS-COCO上的文本到图像DiTs，展示了它是针对各种任务实现高效扩散训练的一个简单而原理性的配方。我们的代码可在以下链接获取。', 'title_zh': 'REPA在不再有效时依旧奏效：早停的整体对齐增强扩散训练'}
{'arxiv_id': 'arXiv:2505.16740', 'title': 'Robust Vision-Based Runway Detection through Conformal Prediction and Conformal mAP', 'authors': 'Alya Zouzou, Léo andéol, Mélanie Ducoffe, Ryma Boumazouza', 'link': 'https://arxiv.org/abs/2505.16740', 'abstract': 'We explore the use of conformal prediction to provide statistical uncertainty guarantees for runway detection in vision-based landing systems (VLS). Using fine-tuned YOLOv5 and YOLOv6 models on aerial imagery, we apply conformal prediction to quantify localization reliability under user-defined risk levels. We also introduce Conformal mean Average Precision (C-mAP), a novel metric aligning object detection performance with conformal guarantees. Our results show that conformal prediction can improve the reliability of runway detection by quantifying uncertainty in a statistically sound way, increasing safety on-board and paving the way for certification of ML system in the aerospace domain.', 'abstract_zh': '我们探索使用共形预测为基于视觉的着陆系统（VLS）中的跑道检测提供统计不确定性保证。我们通过在航空航天图像上微调YOLOv5和YOLOv6模型，并应用共形预测来在用户定义的风险水平下量化定位可靠性。我们还引入了共形均值平均精确度（C-mAP），这是一种将目标检测性能与共形保证相契合的新指标。我们的结果表明，共形预测可以通过以统计学上合理的方式量化不确定性来提高跑道检测的可靠性，从而增强机载安全并为航空航天领域中的ML系统认证铺平道路。', 'title_zh': '基于先验校准的鲁棒视景跑道检测与Conformal mAP'}
{'arxiv_id': 'arXiv:2505.16561', 'title': 'Auto-nnU-Net: Towards Automated Medical Image Segmentation', 'authors': 'Jannis Becktepe, Leona Hennig, Steffen Oeltze-Jafra, Marius Lindauer', 'link': 'https://arxiv.org/abs/2505.16561', 'abstract': 'Medical Image Segmentation (MIS) includes diverse tasks, from bone to organ segmentation, each with its own challenges in finding the best segmentation model. The state-of-the-art AutoML-related MIS-framework nnU-Net automates many aspects of model configuration but remains constrained by fixed hyperparameters and heuristic design choices. As a full-AutoML framework for MIS, we propose Auto-nnU-Net, a novel nnU-Net variant enabling hyperparameter optimization (HPO), neural architecture search (NAS), and hierarchical NAS (HNAS). Additionally, we propose Regularized PriorBand to balance model accuracy with the computational resources required for training, addressing the resource constraints often faced in real-world medical settings that limit the feasibility of extensive training procedures. We evaluate our approach across diverse MIS datasets from the well-established Medical Segmentation Decathlon, analyzing the impact of AutoML techniques on segmentation performance, computational efficiency, and model design choices. The results demonstrate that our AutoML approach substantially improves the segmentation performance of nnU-Net on 6 out of 10 datasets and is on par on the other datasets while maintaining practical resource requirements. Our code is available at this https URL.', 'abstract_zh': '医学图像分割（MIS）包括从骨骼到器官分割的多样化任务，每项任务在寻找最佳分割模型时都有其独特的挑战。作为MIS的全AutoML框架，我们提出Auto-nnU-Net，这是一种新型的nnU-Net变体，支持超参数优化（HPO）、神经架构搜索（NAS）和分层NAS（HNAS）。此外，我们提出Regularized PriorBand，以平衡模型准确性与训练所需的计算资源，解决实际医疗环境中常面临的资源限制问题，这限制了全面训练程序的可行性。我们在Medical Segmentation Decathlon这个广泛认可的数据集上评估了我们的方法，分析了AutoML技术对分割性能、计算效率和模型设计选择的影响。结果表明，我们的AutoML方法在6个数据集上显著提高了nnU-Net的分割性能，在其他数据集上的表现与之相当，同时保持了实际的资源需求。', 'title_zh': '自动-nnU-Net：迈向自动化的医学图像分割'}
{'arxiv_id': 'arXiv:2505.16540', 'title': 'TextureSAM: Towards a Texture Aware Foundation Model for Segmentation', 'authors': 'Inbal Cohen, Boaz Meivar, Peihan Tu, Shai Avidan, Gal Oren', 'link': 'https://arxiv.org/abs/2505.16540', 'abstract': "Segment Anything Models (SAM) have achieved remarkable success in object segmentation tasks across diverse datasets. However, these models are predominantly trained on large-scale semantic segmentation datasets, which introduce a bias toward object shape rather than texture cues in the image. This limitation is critical in domains such as medical imaging, material classification, and remote sensing, where texture changes define object boundaries. In this study, we investigate SAM's bias toward semantics over textures and introduce a new texture-aware foundation model, TextureSAM, which performs superior segmentation in texture-dominant scenarios. To achieve this, we employ a novel fine-tuning approach that incorporates texture augmentation techniques, incrementally modifying training images to emphasize texture features. By leveraging a novel texture-alternation of the ADE20K dataset, we guide TextureSAM to prioritize texture-defined regions, thereby mitigating the inherent shape bias present in the original SAM model. Our extensive experiments demonstrate that TextureSAM significantly outperforms SAM-2 on both natural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation datasets. The code and texture-augmented dataset will be publicly available.", 'abstract_zh': 'Segment Anything模型(SAM)在多种数据集中实现了物体分割任务的显著成功。然而，这些模型主要在大规模语义分割数据集上进行训练，这会导致模型偏向于对象形状而非图像中的纹理线索。这一局限性在医学成像、材料分类和遥感等领域尤为关键，因为在这些领域，纹理变化定义了对象边界。在本研究中，我们探讨了SAM对语义信息的偏好，并引入了一种新的纹理感知基础模型——TextureSAM，该模型在纹理主导场景下的分割表现更优。为此，我们采用了一种创新的微调方法，结合了纹理增强技术，逐步修改训练图像以突出纹理特征。通过利用ADE20K数据集的新型纹理变换，我们引导TextureSAM优先关注由纹理定义的区域，从而减轻了原SAM模型固有的形状偏见。广泛的经验表明，TextureSAM在自然纹理（+0.2 mIoU）和合成纹理（+0.18 mIoU）基础分割数据集上的表现均显著优于SAM-2。代码和纹理增强数据集将公开提供。', 'title_zh': 'TextureSAM: 面向语义分割的纹理感知基础模型'}
{'arxiv_id': 'arXiv:2505.16452', 'title': 'CMRINet: Joint Groupwise Registration and Segmentation for Cardiac Function Quantification from Cine-MRI', 'authors': 'Mohamed S. Elmahdy, Marius Staring, Patrick J. H. de Koning, Samer Alabed, Mahan Salehi, Faisal Alandejani, Michael Sharkey, Ziad Aldabbagh, Andrew J. Swift, Rob J. van der Geest', 'link': 'https://arxiv.org/abs/2505.16452', 'abstract': 'Accurate and efficient quantification of cardiac function is essential for the estimation of prognosis of cardiovascular diseases (CVDs). One of the most commonly used metrics for evaluating cardiac pumping performance is left ventricular ejection fraction (LVEF). However, LVEF can be affected by factors such as inter-observer variability and varying pre-load and after-load conditions, which can reduce its reproducibility. Additionally, cardiac dysfunction may not always manifest as alterations in LVEF, such as in heart failure and cardiotoxicity diseases. An alternative measure that can provide a relatively load-independent quantitative assessment of myocardial contractility is myocardial strain and strain rate. By using LVEF in combination with myocardial strain, it is possible to obtain a thorough description of cardiac function. Automated estimation of LVEF and other volumetric measures from cine-MRI sequences can be achieved through segmentation models, while strain calculation requires the estimation of tissue displacement between sequential frames, which can be accomplished using registration models. These tasks are often performed separately, potentially limiting the assessment of cardiac function. To address this issue, in this study we propose an end-to-end deep learning (DL) model that jointly estimates groupwise (GW) registration and segmentation for cardiac cine-MRI images. The proposed anatomically-guided Deep GW network was trained and validated on a large dataset of 4-chamber view cine-MRI image series of 374 subjects. A quantitative comparison with conventional GW registration using elastix and two DL-based methods showed that the proposed model improved performance and substantially reduced computation time.', 'abstract_zh': '准确而高效的评估心脏功能对于心血管疾病（CVDs）预后估计至关重要。左室射血分数（LVEF）是评估心脏泵血性能的一个常用指标。然而，LVEF 可受观测者间差异和不同前负荷及后负荷条件的影响，这会降低其可重复性。此外，心脏功能障碍可能不会总是表现为LVEF 的改变，例如在心力衰竭和心肌毒性疾病中。一种相对独立于负荷的评估心肌收缩能力的量化指标是心肌应变和应变率。通过结合LVEF 和心肌应变，可以更全面地描述心脏功能。从 cine-MRI 序列中自动化估计 LVEF 及其他容积测量值可通过分割模型实现，而应变计算需要估计连续帧之间的组织位移，这可以通过注册模型实现。这些任务通常分开进行，这可能限制了心脏功能的评估。为解决这一问题，在本研究中我们提出了一种端到端的深度学习（DL）模型，用于联合估计心脏 cine-MRI 图像的群组级注册和分割。所提出的解剖导向的深度群组级网络在4 腔视图 cine-MRI 图像系列的374 个受试者的大型数据集上进行了训练和验证。与使用 elastix 的传统群组级注册以及两种基于DL 的方法相比，提出的模型在定量比较中表现更好，并大幅减少了计算时间。', 'title_zh': 'CMRINet：心脏功能定量的 cine-MRI 联合群组配准与分割'}
{'arxiv_id': 'arXiv:2505.16419', 'title': 'Investigating Fine- and Coarse-grained Structural Correspondences Between Deep Neural Networks and Human Object Image Similarity Judgments Using Unsupervised Alignment', 'authors': 'Soh Takahashi, Masaru Sasaki, Ken Takeda, Masafumi Oizumi', 'link': 'https://arxiv.org/abs/2505.16419', 'abstract': 'The learning mechanisms by which humans acquire internal representations of objects are not fully understood. Deep neural networks (DNNs) have emerged as a useful tool for investigating this question, as they have internal representations similar to those of humans as a byproduct of optimizing their objective functions. While previous studies have shown that models trained with various learning paradigms - such as supervised, self-supervised, and CLIP - acquire human-like representations, it remains unclear whether their similarity to human representations is primarily at a coarse category level or extends to finer details. Here, we employ an unsupervised alignment method based on Gromov-Wasserstein Optimal Transport to compare human and model object representations at both fine-grained and coarse-grained levels. The unique feature of this method compared to conventional representational similarity analysis is that it estimates optimal fine-grained mappings between the representation of each object in human and model representations. We used this unsupervised alignment method to assess the extent to which the representation of each object in humans is correctly mapped to the corresponding representation of the same object in models. Using human similarity judgments of 1,854 objects from the THINGS dataset, we find that models trained with CLIP consistently achieve strong fine- and coarse-grained matching with human object representations. In contrast, self-supervised models showed limited matching at both fine- and coarse-grained levels, but still formed object clusters that reflected human coarse category structure. Our results offer new insights into the role of linguistic information in acquiring precise object representations and the potential of self-supervised learning to capture coarse categorical structures.', 'abstract_zh': '人类获取物体内部表示的学习机制尚不完全理解。深层神经网络（DNNs）已成为研究这一问题的一个有用工具，因为它们在优化目标函数的过程中产生了类似于人类的内部表示。虽然以前的研究表明，使用不同学习范式（如监督学习、自监督学习和CLIP）训练的模型获得了类似人类的表现，但它们与人类表示的相似性主要是粗略类别层面的，还是延伸到了更精细的细节层面仍然不清楚。在这里，我们利用基于Gromov-Wasserstein最优传输的无监督对齐方法，在粗略和精细层面比较人类和模型的物体表示。与传统的表征相似性分析相比，这种方法的独特之处在于它估计了人类和模型中每个物体表示之间的最优精细映射。我们使用这种无监督对齐方法评估了人类中每个物体表示是否正确映射到模型中相同物体的相应表示。使用来自THINGS数据集的1,854个物体的人类相似性判断，我们发现使用CLIP训练的模型在粗略和精细层面与人类物体表示的匹配较强。相比之下，自监督模型在粗细两个层面的匹配有限，但仍然形成了反映人类粗略类别结构的对象集群。我们的结果提供了关于语言信息在获得精确物体表示中的作用以及自监督学习捕捉粗略类别结构潜力的新见解。', 'title_zh': '基于无监督对齐探究深度神经网络与人类对象图像相似性判断之间细粒度和粗粒度结构对应关系'}
{'arxiv_id': 'arXiv:2505.16376', 'title': 'DeCafNet: Delegate and Conquer for Efficient Temporal Grounding in Long Videos', 'authors': 'Zijia Lu, A S M Iftekhar, Gaurav Mittal, Tianjian Meng, Xiawei Wang, Cheng Zhao, Rohith Kukkala, Ehsan Elhamifar, Mei Chen', 'link': 'https://arxiv.org/abs/2505.16376', 'abstract': "Long Video Temporal Grounding (LVTG) aims at identifying specific moments within lengthy videos based on user-provided text queries for effective content retrieval. The approach taken by existing methods of dividing video into clips and processing each clip via a full-scale expert encoder is challenging to scale due to prohibitive computational costs of processing a large number of clips in long videos. To address this issue, we introduce DeCafNet, an approach employing ``delegate-and-conquer'' strategy to achieve computation efficiency without sacrificing grounding performance. DeCafNet introduces a sidekick encoder that performs dense feature extraction over all video clips in a resource-efficient manner, while generating a saliency map to identify the most relevant clips for full processing by the expert encoder. To effectively leverage features from sidekick and expert encoders that exist at different temporal resolutions, we introduce DeCaf-Grounder, which unifies and refines them via query-aware temporal aggregation and multi-scale temporal refinement for accurate grounding. Experiments on two LTVG benchmark datasets demonstrate that DeCafNet reduces computation by up to 47\\% while still outperforming existing methods, establishing a new state-of-the-art for LTVG in terms of both efficiency and performance. Our code is available at this https URL.", 'abstract_zh': '长视频时间定位（LVTG）旨在基于用户提供的文本查询在 lengthy 视频中识别特定时刻，以实现有效的内容检索。现有的方法通过将视频分割成片段并在全规模专家编码器处理每个片段，虽然有效，但由于处理长视频中大量片段的计算成本高昂，难以扩展。为了解决这一问题，我们引入了 DeCafNet，该方法采用“委托-征服”策略以实现计算效率，同时不牺牲定位性能。DeCafNet 引入了辅助编码器，该编码器以资源高效的方式对所有视频片段进行密集特征提取，并生成显著性图以识别由专家编码器进行完整处理的最相关片段。为了有效地利用来自辅助编码器和专家编码器在不同时间分辨率上存在的特征，我们引入了 DeCaf-Grounder，通过查询感知的时间聚合和多尺度时间细化统一并精炼它们，从而实现准确的时间定位。在两个 LVTG 标准数据集上的实验表明，DeCafNet 可将计算量减少多达 47% 且性能仍优于现有方法，从而在效率和性能方面均建立了新的 LVTG 状态-of-艺术水平。我们的代码可在以下网址获得。', 'title_zh': 'DeCafNet：委托与征服，实现长视频高效时空语义接地'}
{'arxiv_id': 'arXiv:2505.16335', 'title': 'FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design', 'authors': 'Renjie Wei, Songqiang Xu, Qingyu Guo, Meng Li', 'link': 'https://arxiv.org/abs/2505.16335', 'abstract': 'Visual autoregressive (VAR) modeling has marked a paradigm shift in image generation from next-token prediction to next-scale prediction. VAR predicts a set of tokens at each step from coarse to fine scale, leading to better image quality and faster inference speed compared to existing diffusion models. However, the large parameter size and computation cost hinder its deployment on edge devices. To reduce the memory and computation cost, we propose FPQVAR, an efficient post-training floating-point (FP) quantization framework for VAR featuring algorithm and hardware co-design. At the algorithm level, we first identify the challenges of quantizing VAR. To address them, we propose Dual Format Quantization for the highly imbalanced input activation. We further propose Group-wise Hadamard Transformation and GHT-Aware Learnable Transformation to address the time-varying outlier channels. At the hardware level, we design the first low-bit FP quantizer and multiplier with lookup tables on FPGA and propose the first FPGA-based VAR accelerator featuring low-bit FP computation and an elaborate two-level pipeline. Extensive experiments show that compared to the state-of-the-art quantization method, our proposed FPQVAR significantly improves Fréchet Inception Distance (FID) from 10.83 to 3.58, Inception Score (IS) from 175.9 to 241.5 under 4-bit quantization. FPQVAR also significantly improves the performance of 6-bit quantized VAR, bringing it on par with the FP16 model. Our accelerator on AMD-Xilinx VCK190 FPGA achieves a throughput of 1.1 image/s, which is 3.1x higher than the integer-based accelerator. It also demonstrates 3.6x and 2.8x higher energy efficiency compared to the integer-based accelerator and GPU baseline, respectively.', 'abstract_zh': '视觉自回归（VAR）建模标志着图像生成从下一个token预测到下一个尺度预测的范式转变。VAR通过从粗尺度到细尺度预测一组token，相比现有的扩散模型，产生了更好的图像质量并加快了推断速度。然而，其庞大的参数量和计算成本阻碍了其在边缘设备上的部署。为了降低内存和计算成本，我们提出了一种高效的事后训练浮点（FP）量化框架FPQVAR，结合了算法和硬件协同设计。在算法层面，我们首先识别了量化VAR的挑战，并提出了双重格式量化以应对高度不平衡的输入激活。我们进一步提出了组内哈达玛变换及基于组内哈达玛变换的可学习变换，以应对时间变化的异常通道。在硬件层面，我们设计了首个用于FPGA的低比特浮点量化器和乘法器，并提出了首个基于FPGA的VAR加速器，该加速器实现了低比特浮点计算和细致两级流水线。广泛实验显示，相比最先进的量化方法，我们提出的FPQVAR在4比特量化下显著提高了弗雷chet inception距离（FID）至3.58（从10.83），提高了风格转移分数（IS）至241.5（从175.9）。FPQVAR还显著提高了6比特量化VAR的性能，使其与FP16模型持平。我们的加速器在AMD-Xilinx VCK190 FPGA上实现了每秒1.1张图像的吞吐量，比整数基加速器高3.1倍。与整数基加速器相比，其能效提升了3.6倍，与GPU基线相比，能效提高了2.8倍。', 'title_zh': 'FPQVAR：基于FPGA硬件协同设计的浮点量化视觉自回归模型'}
{'arxiv_id': 'arXiv:2505.16314', 'title': 'NTIRE 2025 challenge on Text to Image Generation Model Quality Assessment', 'authors': 'Shuhao Han, Haotian Fan, Fangyuan Kong, Wenjie Liao, Chunle Guo, Chongyi Li, Radu Timofte, Liang Li, Tao Li, Junhui Cui, Yunqiu Wang, Yang Tai, Jingwei Sun, Jianhui Sun, Xinli Yue, Tianyi Wang, Huan Hou, Junda Lu, Xinyang Huang, Zitang Zhou, Zijian Zhang, Xuhui Zheng, Xuecheng Wu, Chong Peng, Xuezhi Cao, Trong-Hieu Nguyen-Mau, Minh-Hoang Le, Minh-Khoa Le-Phan, Duy-Nam Ly, Hai-Dang Nguyen, Minh-Triet Tran, Yukang Lin, Yan Hong, Chuanbiao Song, Siyuan Li, Jun Lan, Zhichao Zhang, Xinyue Li, Wei Sun, Zicheng Zhang, Yunhao Li, Xiaohong Liu, Guangtao Zhai, Zitong Xu, Huiyu Duan, Jiarui Wang, Guangji Ma, Liu Yang, Lu Liu, Qiang Hu, Xiongkuo Min, Zichuan Wang, Zhenchen Tang, Bo Peng, Jing Dong, Fengbin Guan, Zihao Yu, Yiting Lu, Wei Luo, Xin Li, Minhao Lin, Haofeng Chen, Xuanxuan He, Kele Xu, Qisheng Xu, Zijian Gao, Tianjiao Wan, Bo-Cheng Qiu, Chih-Chung Hsu, Chia-ming Lee, Yu-Fan Lin, Bo Yu, Zehao Wang, Da Mu, Mingxiu Chen, Junkang Fang, Huamei Sun, Wending Zhao, Zhiyu Wang, Wang Liu, Weikang Yu, Puhong Duan, Bin Sun, Xudong Kang, Shutao Li, Shuai He, Lingzhi Fu, Heng Cong, Rongyu Zhang, Jiarong He, Zhishan Qiao, Yongqing Huang, Zewen Chen, Zhe Pang, Juan Wang, Jian Guo, Zhizhuo Shao, Ziyu Feng, Bing Li, Weiming Hu', 'link': 'https://arxiv.org/abs/2505.16314', 'abstract': 'This paper reports on the NTIRE 2025 challenge on Text to Image (T2I) generation model quality assessment, which will be held in conjunction with the New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2025. The aim of this challenge is to address the fine-grained quality assessment of text-to-image generation models. This challenge evaluates text-to-image models from two aspects: image-text alignment and image structural distortion detection, and is divided into the alignment track and the structural track. The alignment track uses the EvalMuse-40K, which contains around 40K AI-Generated Images (AIGIs) generated by 20 popular generative models. The alignment track has a total of 371 registered participants. A total of 1,883 submissions are received in the development phase, and 507 submissions are received in the test phase. Finally, 12 participating teams submitted their models and fact sheets. The structure track uses the EvalMuse-Structure, which contains 10,000 AI-Generated Images (AIGIs) with corresponding structural distortion mask. A total of 211 participants have registered in the structure track. A total of 1155 submissions are received in the development phase, and 487 submissions are received in the test phase. Finally, 8 participating teams submitted their models and fact sheets. Almost all methods have achieved better results than baseline methods, and the winning methods in both tracks have demonstrated superior prediction performance on T2I model quality assessment.', 'abstract_zh': 'NTIRE 2025挑战赛：文本到图像生成模型质量评估', 'title_zh': 'NTIRE 2025挑战赛：文本到图像生成模型质量评估'}
{'arxiv_id': 'arXiv:2505.16195', 'title': 'SpecMaskFoley: Steering Pretrained Spectral Masked Generative Transformer Toward Synchronized Video-to-audio Synthesis via ControlNet', 'authors': 'Zhi Zhong, Akira Takahashi, Shuyang Cui, Keisuke Toyama, Shusuke Takahashi, Yuki Mitsufuji', 'link': 'https://arxiv.org/abs/2505.16195', 'abstract': 'Foley synthesis aims to synthesize high-quality audio that is both semantically and temporally aligned with video frames. Given its broad application in creative industries, the task has gained increasing attention in the research community. To avoid the non-trivial task of training audio generative models from scratch, adapting pretrained audio generative models for video-synchronized foley synthesis presents an attractive direction. ControlNet, a method for adding fine-grained controls to pretrained generative models, has been applied to foley synthesis, but its use has been limited to handcrafted human-readable temporal conditions. In contrast, from-scratch models achieved success by leveraging high-dimensional deep features extracted using pretrained video encoders. We have observed a performance gap between ControlNet-based and from-scratch foley models. To narrow this gap, we propose SpecMaskFoley, a method that steers the pretrained SpecMaskGIT model toward video-synchronized foley synthesis via ControlNet. To unlock the potential of a single ControlNet branch, we resolve the discrepancy between the temporal video features and the time-frequency nature of the pretrained SpecMaskGIT via a frequency-aware temporal feature aligner, eliminating the need for complicated conditioning mechanisms widely used in prior arts. Evaluations on a common foley synthesis benchmark demonstrate that SpecMaskFoley could even outperform strong from-scratch baselines, substantially advancing the development of ControlNet-based foley synthesis models. Demo page: this https URL', 'abstract_zh': 'SpecMaskFoley：一种通过ControlNet引导的预训练SpecMaskGIT模型用于视频同步FOLEY合成的方法', 'title_zh': 'SpecMaskFoley: 通过ControlNet引导预训练频谱掩蔽生成变换器实现同步视频到音频合成'}
{'arxiv_id': 'arXiv:2505.16181', 'title': 'Understanding Generative AI Capabilities in Everyday Image Editing Tasks', 'authors': 'Mohammad Reza Taesiri, Brandon Collins, Logan Bolton, Viet Dac Lai, Franck Dernoncourt, Trung Bui, Anh Totti Nguyen', 'link': 'https://arxiv.org/abs/2505.16181', 'abstract': 'Generative AI (GenAI) holds significant promise for automating everyday image editing tasks, especially following the recent release of GPT-4o on March 25, 2025. However, what subjects do people most often want edited? What kinds of editing actions do they want to perform (e.g., removing or stylizing the subject)? Do people prefer precise edits with predictable outcomes or highly creative ones? By understanding the characteristics of real-world requests and the corresponding edits made by freelance photo-editing wizards, can we draw lessons for improving AI-based editors and determine which types of requests can currently be handled successfully by AI editors? In this paper, we present a unique study addressing these questions by analyzing 83k requests from the past 12 years (2013-2025) on the Reddit community, which collected 305k PSR-wizard edits. According to human ratings, approximately only 33% of requests can be fulfilled by the best AI editors (including GPT-4o, Gemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on low-creativity requests that require precise editing than on more open-ended tasks. They often struggle to preserve the identity of people and animals, and frequently make non-requested touch-ups. On the other side of the table, VLM judges (e.g., o1) perform differently from human judges and may prefer AI edits more than human edits. Code and qualitative examples are available at: this https URL', 'abstract_zh': '生成式AI（GenAI）在自动化日常图像编辑任务方面展现出显著的潜力，尤其是在2025年3月25日GPT-4o发布之后。人们最常希望编辑哪些主题？他们希望执行哪种编辑动作（例如，移除或风格化主题）？人们偏好精确的编辑还是高度创意的编辑？通过对过去12年（2013-2025） Reddit社区中83,000个请求及其305,000个PSR-巫师编辑的分析，我们了解真实世界请求的特点和相应的编辑结果，能否从中汲取改进基于AI的编辑器的教训，并确定哪些类型的请求当前可以由AI编辑器成功处理？在本文中，我们呈现了一项独特研究，通过分析Reddit社区过去12年中的83,000个请求及其305,000个PSR-巫师编辑来回答这些问题。根据人类评级，仅约33%的请求可以由最佳AI编辑器（包括GPT-4o、Gemini-2.0-Flash、SeedEdit）满足。有趣的是，AI编辑器在需要精确编辑的低创意请求上表现不如更开放的任务。它们往往难以保留人物和动物的身份，并经常进行未请求的润色。相比之下，VLM评判者（如o1）的表现不同于人类评判者，可能会更偏好AI编辑而非人类编辑。相关代码和定性示例可在以下链接获取：this https URL', 'title_zh': '理解生成式人工智能在日常图像编辑任务中的能力'}
{'arxiv_id': 'arXiv:2505.16175', 'title': 'QuickVideo: Real-Time Long Video Understanding with System Algorithm Co-Design', 'authors': 'Benjamin Schneider, Dongfu Jiang, Chao Du, Tianyu Pang, Wenhu Chen', 'link': 'https://arxiv.org/abs/2505.16175', 'abstract': 'Long-video understanding has emerged as a crucial capability in real-world applications such as video surveillance, meeting summarization, educational lecture analysis, and sports broadcasting. However, it remains computationally prohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential video decoding, the process of converting the raw bit stream to RGB frames can take up to a minute for hour-long video inputs, and 2) costly prefilling of up to several million tokens for LLM inference, resulting in high latency and memory use. To address these challenges, we propose QuickVideo, a system-algorithm co-design that substantially accelerates long-video understanding to support real-time downstream applications. It comprises three key innovations: QuickDecoder, a parallelized CPU-based video decoder that achieves 2-3 times speedup by splitting videos into keyframe-aligned intervals processed concurrently; QuickPrefill, a memory-efficient prefilling method using KV-cache pruning to support more frames with less GPU memory; and an overlapping scheme that overlaps CPU video decoding with GPU inference. Together, these components infernece time reduce by a minute on long video inputs, enabling scalable, high-quality video understanding even on limited hardware. Experiments show that QuickVideo generalizes across durations and sampling rates, making long video processing feasible in practice.', 'abstract_zh': '长视频理解在视频监控、会议总结、教育讲座分析和体育广播等实际应用中日益成为关键能力。然而，由于两个瓶颈，这一能力对于VideoLLMs来说仍然是计算上不可行的：1) 顺序视频解码，将原始位流转换为RGB帧的过程可能需要几分钟来处理一个小时的视频输入；2) 预填充LLM推理所需的高达数百万个标记，导致高延迟和大量内存使用。为了解决这些挑战，我们提出了一种系统-算法协同设计的方案QuickVideo，以显著加速长视频理解，支持实时下游应用。该方案包含了三个关键创新：QuickDecoder，一种基于并行CPU的视频解码器，通过按关键帧对齐的区间并行处理视频，实现2-3倍的速度提升；QuickPrefill，一种高效的预填充方法，通过使用KV缓存剪枝来支持更多帧，减少GPU内存使用；以及一种重叠方案，该方案将CPU视频解码与GPU推理重叠。这些组件共同减少了长视频推理时间一分钟，即使在限制硬件条件下也能实现可扩展且高质量的视频理解。实验表明，QuickVideo能够在不同持续时间和采样率下泛化，使实际中的长视频处理成为可能。', 'title_zh': 'QuickVideo: 实时长视频理解的系统算法协同设计'}
{'arxiv_id': 'arXiv:2505.16027', 'title': 'Benchmarking Chest X-ray Diagnosis Models Across Multinational Datasets', 'authors': 'Qinmei Xu, Yiheng Li, Xianghao Zhan, Ahmet Gorkem Er, Brittany Dashevsky, Chuanjun Xu, Mohammed Alawad, Mengya Yang, Liu Ya, Changsheng Zhou, Xiao Li, Haruka Itakura, Olivier Gevaert', 'link': 'https://arxiv.org/abs/2505.16027', 'abstract': 'Foundation models leveraging vision-language pretraining have shown promise in chest X-ray (CXR) interpretation, yet their real-world performance across diverse populations and diagnostic tasks remains insufficiently evaluated. This study benchmarks the diagnostic performance and generalizability of foundation models versus traditional convolutional neural networks (CNNs) on multinational CXR datasets. We evaluated eight CXR diagnostic models - five vision-language foundation models and three CNN-based architectures - across 37 standardized classification tasks using six public datasets from the USA, Spain, India, and Vietnam, and three private datasets from hospitals in China. Performance was assessed using AUROC, AUPRC, and other metrics across both shared and dataset-specific tasks. Foundation models outperformed CNNs in both accuracy and task coverage. MAVL, a model incorporating knowledge-enhanced prompts and structured supervision, achieved the highest performance on public (mean AUROC: 0.82; AUPRC: 0.32) and private (mean AUROC: 0.95; AUPRC: 0.89) datasets, ranking first in 14 of 37 public and 3 of 4 private tasks. All models showed reduced performance on pediatric cases, with average AUROC dropping from 0.88 +/- 0.18 in adults to 0.57 +/- 0.29 in children (p = 0.0202). These findings highlight the value of structured supervision and prompt design in radiologic AI and suggest future directions including geographic expansion and ensemble modeling for clinical deployment. Code for all evaluated models is available at this https URL', 'abstract_zh': '基于视觉-语言预训练的foundation模型在胸部X光片(CXR)解释中展现出了潜力，但其在多元人群和诊断任务中的实际性能仍需进一步评估。本研究对比了foundation模型与传统卷积神经网络(CNNs)在跨国CXR数据集上的诊断性能和泛化能力。我们评估了八种CXR诊断模型——五种视觉-语言foundation模型和三种CNN基线架构——在来自美国、西班牙、印度和越南的六个公开数据集以及来自中国医院的三个私有数据集上的37项标准化分类任务。性能评估使用了AUROC、AUPRC和其他指标，涵盖共用和私有数据集任务。foundation模型在准确性和任务覆盖面上均优于CNNs。MAVL模型，一种结合了知识增强提示和结构化监督的模型，在公开和私有数据集上达到了最高性能（公开数据集平均AUROC: 0.82；AUPRC: 0.32；私有数据集平均AUROC: 0.95；AUPRC: 0.89），分别在37项公开任务中的14项和4项私有任务中的3项中排名第一。所有模型在儿科病例中表现较差，成人AUROC平均值从0.88±0.18下降到儿童的0.57±0.29（p=0.0202）。这些发现突显了结构化监督和提示设计在放射学AI中的价值，并建议未来研究的方向包括地理扩展和集成建模以供临床部署。所有评估模型的代码可在以下链接获得。', 'title_zh': '多国数据集上胸片诊断模型基准测试'}
{'arxiv_id': 'arXiv:2505.15997', 'title': 'Domain Adaptive Skin Lesion Classification via Conformal Ensemble of Vision Transformers', 'authors': 'Mehran Zoravar, Shadi Alijani, Homayoun Najjaran', 'link': 'https://arxiv.org/abs/2505.15997', 'abstract': "Exploring the trustworthiness of deep learning models is crucial, especially in critical domains such as medical imaging decision support systems. Conformal prediction has emerged as a rigorous means of providing deep learning models with reliable uncertainty estimates and safety guarantees. However, conformal prediction results face challenges due to the backbone model's struggles in domain-shifted scenarios, such as variations in different sources. To aim this challenge, this paper proposes a novel framework termed Conformal Ensemble of Vision Transformers (CE-ViTs) designed to enhance image classification performance by prioritizing domain adaptation and model robustness, while accounting for uncertainty. The proposed method leverages an ensemble of vision transformer models in the backbone, trained on diverse datasets including HAM10000, Dermofit, and Skin Cancer ISIC datasets. This ensemble learning approach, calibrated through the combined mentioned datasets, aims to enhance domain adaptation through conformal learning. Experimental results underscore that the framework achieves a high coverage rate of 90.38\\%, representing an improvement of 9.95\\% compared to the HAM10000 model. This indicates a strong likelihood that the prediction set includes the true label compared to singular models. Ensemble learning in CE-ViTs significantly improves conformal prediction performance, increasing the average prediction set size for challenging misclassified samples from 1.86 to 3.075.", 'abstract_zh': '探索深度学习模型的可信度至关重要，尤其是在医疗影像决策支持系统等关键领域。基于变换器的 conformal 集成框架 (CE-ViTs) 旨在通过优先考虑领域适应和模型鲁棒性来提升图像分类性能，同时考虑不确定性。', 'title_zh': '基于视觉变换器同化ensemble的皮肤病变分类域适应方法'}
{'arxiv_id': 'arXiv:2505.15825', 'title': 'Multilinear subspace learning for person re-identification based fusion of high order tensor features', 'authors': 'Ammar Chouchane, Mohcene Bessaoudi, Hamza Kheddar, Abdelmalik Ouamane, Tiago Vieira, Mahmoud Hassaballah', 'link': 'https://arxiv.org/abs/2505.15825', 'abstract': "Video surveillance image analysis and processing is a challenging field in computer vision, with one of its most difficult tasks being Person Re-Identification (PRe-ID). PRe-ID aims to identify and track target individuals who have already been detected in a network of cameras, using a robust description of their pedestrian images. The success of recent research in person PRe-ID is largely due to effective feature extraction and representation, as well as the powerful learning of these features to reliably discriminate between pedestrian images. To this end, two powerful features, Convolutional Neural Networks (CNN) and Local Maximal Occurrence (LOMO), are modeled on multidimensional data using the proposed method, High-Dimensional Feature Fusion (HDFF). Specifically, a new tensor fusion scheme is introduced to leverage and combine these two types of features in a single tensor, even though their dimensions are not identical. To enhance the system's accuracy, we employ Tensor Cross-View Quadratic Analysis (TXQDA) for multilinear subspace learning, followed by cosine similarity for matching. TXQDA efficiently facilitates learning while reducing the high dimensionality inherent in high-order tensor data. The effectiveness of our approach is verified through experiments on three widely-used PRe-ID datasets: VIPeR, GRID, and PRID450S. Extensive experiments demonstrate that our approach outperforms recent state-of-the-art methods.", 'abstract_zh': '视频监控图像分析与处理是计算机视觉中的一个挑战性领域，其中最困难的任务之一是行人重识别（PRe-ID）。PRe-ID旨在通过稳健描述行人的图像来识别和跟踪已在摄像头网络中被检测到的目标个体。近年来行人PRe-ID研究取得成功的主要原因是有效的特征提取和表示，以及这些特征的强大学习能力，以可靠地区分行人的图像。为此，本文提出了高维特征融合（HDFF）方法，利用该方法在多维数据上建模两种强大力量特征，卷积神经网络（CNN）和局部最大出现（LOMO）。具体而言，引入了一种新的张量融合方案，以在同一张量中利用和结合这两种特征，即使它们的维度不一致。为了提高系统的准确性，我们使用张量跨视图二次分析（TXQDA）进行多线性子空间学习，随后使用余弦相似度进行匹配。TXQDA有效地促进了学习并降低了高阶张量数据固有的高维度。通过在三个广泛使用的PRe-ID数据集VIPeR、GRID和PRID450S上的实验验证了我们方法的有效性。广泛实验表明，我们方法优于最近的先进方法。', 'title_zh': '基于高阶张量特征融合的人重识别多线性子空间学习'}
