{'arxiv_id': 'arXiv:2505.16969', 'title': '3D Equivariant Visuomotor Policy Learning via Spherical Projection', 'authors': 'Boce Hu, Dian Wang, David Klee, Heng Tian, Xupeng Zhu, Haojie Huang, Robert Platt, Robin Walters', 'link': 'https://arxiv.org/abs/2505.16969', 'abstract': 'Equivariant models have recently been shown to improve the data efficiency of diffusion policy by a significant margin. However, prior work that explored this direction focused primarily on point cloud inputs generated by multiple cameras fixed in the workspace. This type of point cloud input is not compatible with the now-common setting where the primary input modality is an eye-in-hand RGB camera like a GoPro. This paper closes this gap by incorporating into the diffusion policy model a process that projects features from the 2D RGB camera image onto a sphere. This enables us to reason about symmetries in SO(3) without explicitly reconstructing a point cloud. We perform extensive experiments in both simulation and the real world that demonstrate that our method consistently outperforms strong baselines in terms of both performance and sample efficiency. Our work is the first SO(3)-equivariant policy learning framework for robotic manipulation that works using only monocular RGB inputs.', 'abstract_zh': 'Equivariant模型已被证明显著提高了扩散策略的数据效率。然而，此前在此方向上的工作主要集中在由工作空间中固定多个相机生成的点云输入。这种点云输入与当前主要输入模态为手持RGB相机（如GoPro）的情况不兼容。本文通过将特征从2D RGB相机图像投影到球面上的方法，填补了这一缺口，使我们能够在不显式重建点云的情况下推理SO(3)中的对称性。我们在模拟和现实世界中的大量实验表明，我们的方法在性能和样本效率方面均优于强baseline。我们的工作是第一个仅使用单目RGB输入实现SO(3)-等变政策学习的机器人操作框架。', 'title_zh': '基于球面投影的3D等变视觉运动策略学习'}
{'arxiv_id': 'arXiv:2505.16912', 'title': 'UAV See, UGV Do: Aerial Imagery and Virtual Teach Enabling Zero-Shot Ground Vehicle Repeat', 'authors': 'Desiree Fisker, Alexander Krawciw, Sven Lilge, Melissa Greeff, Timothy D. Barfoot', 'link': 'https://arxiv.org/abs/2505.16912', 'abstract': 'This paper presents Virtual Teach and Repeat (VirT&R): an extension of the Teach and Repeat (T&R) framework that enables GPS-denied, zero-shot autonomous ground vehicle navigation in untraversed environments. VirT&R leverages aerial imagery captured for a target environment to train a Neural Radiance Field (NeRF) model so that dense point clouds and photo-textured meshes can be extracted. The NeRF mesh is used to create a high-fidelity simulation of the environment for piloting an unmanned ground vehicle (UGV) to virtually define a desired path. The mission can then be executed in the actual target environment by using NeRF-derived point cloud submaps associated along the path and an existing LiDAR Teach and Repeat (LT&R) framework. We benchmark the repeatability of VirT&R on over 12 km of autonomous driving data using physical markings that allow a sim-to-real lateral path-tracking error to be obtained and compared with LT&R. VirT&R achieved measured root mean squared errors (RMSE) of 19.5 cm and 18.4 cm in two different environments, which are slightly less than one tire width (24 cm) on the robot used for testing, and respective maximum errors were 39.4 cm and 47.6 cm. This was done using only the NeRF-derived teach map, demonstrating that VirT&R has similar closed-loop path-tracking performance to LT&R but does not require a human to manually teach the path to the UGV in the actual environment.', 'abstract_zh': 'Virtual Teach and Repeat (VirT&R): 无GPS环境下的零样本自主地面车辆导航扩展方法', 'title_zh': 'UAV 观察，UGV 复制：基于航空影像和虚拟示教的零样本地面车辆重复任务'}
{'arxiv_id': 'arXiv:2505.16892', 'title': 'FlashBack: Consistency Model-Accelerated Shared Autonomy', 'authors': 'Luzhe Sun, Jingtian Ji, Xiangshan Tan, Matthew R. Walter', 'link': 'https://arxiv.org/abs/2505.16892', 'abstract': "Shared autonomy is an enabling technology that provides users with control authority over robots that would otherwise be difficult if not impossible to directly control. Yet, standard methods make assumptions that limit their adoption in practice-for example, prior knowledge of the user's goals or the objective (i.e., reward) function that they wish to optimize, knowledge of the user's policy, or query-level access to the user during training. Diffusion-based approaches to shared autonomy do not make such assumptions and instead only require access to demonstrations of desired behaviors, while allowing the user to maintain control authority. However, these advantages have come at the expense of high computational complexity, which has made real-time shared autonomy all but impossible. To overcome this limitation, we propose Consistency Shared Autonomy (CSA), a shared autonomy framework that employs a consistency model-based formulation of diffusion. Key to CSA is that it employs the distilled probability flow of ordinary differential equations (PF ODE) to generate high-fidelity samples in a single step. This results in inference speeds significantly than what is possible with previous diffusion-based approaches to shared autonomy, enabling real-time assistance in complex domains with only a single function evaluation. Further, by intervening on flawed actions at intermediate states of the PF ODE, CSA enables varying levels of assistance. We evaluate CSA on a variety of challenging simulated and real-world robot control problems, demonstrating significant improvements over state-of-the-art methods both in terms of task performance and computational efficiency.", 'abstract_zh': '共享自主性是一种使能技术，为用户提供对机器人控制的权威，这些机器人否则直接控制起来可能会非常困难甚至不可能。然而，标准方法做出的一些假设限制了它们在实践中的应用——例如，对用户的目標或他们希望优化的目标函数（即奖励函数）的先验知识，对用户政策的了解，或在训练过程中对用户的查询级访问。基于扩散的方法并不会做出这些假设，而是只需要访问所期望行为的示范，同时允许用户保持控制权威。然而，这些优势以高计算复杂性为代价，使其实时共享自主性几乎不可能实现。为了克服这一限制，我们提出了基于一致性模型的扩散一致性共享自主性（CSA）框架。CSA的关键在于使用普通微分方程的精练概率流（PF ODE）一次性生成高保真样本。这使得推断速度显著快于先前基于扩散的方法，能够在仅需一次函数评估的情况下实现复杂领域中的实时辅助。此外，通过在PF ODE的中间状态上干预错误行为，CSA能够提供不同程度的辅助。我们对各种具有挑战性的模拟和真实世界机器人控制问题进行了评估，展示了与最先进的方法相比，在任务性能和计算效率方面的显著改进。', 'title_zh': 'FlashBack: 一致性模型加速的共享自治'}
{'arxiv_id': 'arXiv:2505.16726', 'title': 'D-LIO: 6DoF Direct LiDAR-Inertial Odometry based on Simultaneous Truncated Distance Field Mapping', 'authors': 'Lucia Coto-Elena, J.E. Maese, L. Merino, F. Caballero', 'link': 'https://arxiv.org/abs/2505.16726', 'abstract': 'This paper presents a new approach for 6DoF Direct LiDAR-Inertial Odometry (D-LIO) based on the simultaneous mapping of truncated distance fields on CPU. Such continuous representation (in the vicinity of the points) enables working with raw 3D LiDAR data online, avoiding the need of LiDAR feature selection and tracking, simplifying the odometry pipeline and easily generalizing to many scenarios. The method is based on the proposed Fast Truncated Distance Field (Fast-TDF) method as a convenient tool to represent the environment. Such representation enables i) solving the LiDAR point-cloud registration as a nonlinear optimization process without the need of selecting/tracking LiDAR features in the input data, ii) simultaneously producing an accurate truncated distance field map of the environment, and iii) updating such map at constant time independently of its size. The approach is tested using open datasets, aerial and ground. It is also benchmarked against other state-of-the-art odometry approaches, demonstrating the same or better level of accuracy with the added value of an online-generated TDF representation of the environment, that can be used for other robotics tasks as planning or collision avoidance. The source code is publicly available at this https URL', 'abstract_zh': '本文提出了一种基于CPU实时映射截断距离场的6DoF直接LiDAR-惯性里程计（D-LIO）新方法。这种连续表示（在点的附近）使得能够在线处理原始3D LiDAR数据，避免了LiDAR特征选择和跟踪的需要，简化了里程计管道，并易于适应多种场景。该方法基于提出的快速截断距离场（Fast-TDF）方法作为环境表示的便捷工具。这种表示使得：i) 可以将LiDAR点云配准作为无需选择/跟踪输入数据中LiDAR特征的非线性优化过程来解决；ii) 同时生成环境的准确截断距离场地图；iii) 独立于地图大小以恒定时间更新此类地图。该方法使用开放数据集、空域和地面数据进行了测试，并与现有的其他先进里程计方法进行了基准测试，证明了与之相当或更高的精度水平，并增加了在线生成环境截断距离场表示的附加价值，该表示可用于其他机器人任务如规划或碰撞避免。源代码可在以下网址获取。', 'title_zh': 'D-LIO：基于同时截断距离场映射的6自由度直接激光雷达-惯性里程计'}
{'arxiv_id': 'arXiv:2505.16682', 'title': 'MEbots: Integrating a RISC-V Virtual Platform with a Robotic Simulator for Energy-aware Design', 'authors': 'Giovanni Pollo, Mohamed Amine Hamdi, Matteo Risso, Lorenzo Ruotolo, Pietro Furbatto, Matteo Isoldi, Yukai Chen, Alessio Burrello, Enrico Macii, Massimo Poncino, Daniele Jahier Pagliari, Sara Vinco', 'link': 'https://arxiv.org/abs/2505.16682', 'abstract': "Virtual Platforms (VPs) enable early software validation of autonomous systems' electronics, reducing costs and time-to-market. While many VPs support both functional and non-functional simulation (e.g., timing, power), they lack the capability of simulating the environment in which the system operates. In contrast, robotics simulators lack accurate timing and power features. This twofold shortcoming limits the effectiveness of the design flow, as the designer can not fully evaluate the features of the solution under development. This paper presents a novel, fully open-source framework bridging this gap by integrating a robotics simulator (Webots) with a VP for RISC-V-based systems (MESSY). The framework enables a holistic, mission-level, energy-aware co-simulation of electronics in their surrounding environment, streamlining the exploration of design configurations and advanced power management policies.", 'abstract_zh': '虚拟平台与机器人模拟器集成的开源框架：面向RISC-V系统的环境感知联合仿真', 'title_zh': 'MEbots：集成RISC-V虚拟平台的机器人模拟器能效感知设计'}
{'arxiv_id': 'arXiv:2505.16662', 'title': 'Joint Magnetometer-IMU Calibration via Maximum A Posteriori Estimation', 'authors': 'Chuan Huang, Gustaf Hendeby, Isaac Skog', 'link': 'https://arxiv.org/abs/2505.16662', 'abstract': 'This paper presents a new approach for jointly calibrating magnetometers and inertial measurement units, focusing on improving calibration accuracy and computational efficiency. The proposed method formulates the calibration problem as a maximum a posteriori estimation problem, treating both the calibration parameters and orientation trajectory of the sensors as unknowns. This formulation enables efficient optimization with closed-form derivatives. The method is compared against two state-of-the-art approaches in terms of computational complexity and estimation accuracy. Simulation results demonstrate that the proposed method achieves lower root mean square error in calibration parameters while maintaining competitive computational efficiency. Further validation through real-world experiments confirms the practical benefits of our approach: it effectively reduces position drift in a magnetic field-aided inertial navigation system by more than a factor of two on most datasets. Moreover, the proposed method calibrated 30 magnetometers in less than 2 minutes. The contributions include a new calibration method, an analysis of existing methods, and a comprehensive empirical evaluation. Datasets and algorithms are made publicly available to promote reproducible research.', 'abstract_zh': '本文提出了一种新的方法，用于同时校准磁力计和惯性测量单元，重点关注提高校准精度和计算效率。所提出的方法将校准问题形式化为最大后验估计问题，将传感器的校准参数和姿态轨迹均视为未知数。这种形式化使得可以通过闭式导数进行高效优化。该方法在计算复杂性和估计准确性方面与两种最先进的方法进行了比较。仿真结果表明，所提出的方法在保持竞争力的计算效率的同时，校准参数的均方根误差较低。通过实际实验进一步验证了该方法的实际优势：它在大多数数据集上使磁场辅助惯性导航系统的位置漂移降低了两倍以上。此外，所提出的方法在不到2分钟内完成了30个磁力计的校准。本研究的贡献包括一种新的校准方法、一种对现有方法的分析以及全面的经验性评估。所有数据集和算法均已公开发布，以促进可重复研究。', 'title_zh': '基于最大后验估计的磁强计-IMU 联合标定'}
{'arxiv_id': 'arXiv:2505.16609', 'title': 'Monitoring Electrostatic Adhesion Forces via Acoustic Pressure', 'authors': 'Huacen Wang, Jiarui Zou, Zeju Zheng, Hongqiang Wang', 'link': 'https://arxiv.org/abs/2505.16609', 'abstract': 'Electrostatic adhesion is widely used in mobile robotics, haptics, and robotic end effectors for its adaptability to diverse substrates and low energy consumption. Force sensing is important for feedback control, interaction, and monitoring in the EA system. However, EA force monitoring often relies on bulky and expensive sensors, increasing the complexity and weight of the entire system. This paper presents an acoustic-pressure-based method to monitor EA forces without contacting the adhesion pad. When the EA pad is driven by a bipolar square-wave voltage to adhere a conductive object, periodic acoustic pulses arise from the EA system. We employed a microphone to capture these acoustic pressure signals and investigate the influence of peak pressure values. Results show that the peak value of acoustic pressure increased with the mass and contact area of the adhered object, as well as with the amplitude and frequency of the driving voltage. We applied this technique to mass estimation of various objects and simultaneous monitoring of two EA systems. Then, we integrated this technique into an EA end effector that enables monitoring the change of adhered object mass during transport. The proposed technique offers a low-cost, non-contact, and multi-object monitoring solution for EA end effectors in handling tasks.', 'abstract_zh': '基于声压的无接触EA力监测方法及其在.End Effector处理任务中的多对象监测应用', 'title_zh': '通过声压监测静电吸附力'}
{'arxiv_id': 'arXiv:2505.16596', 'title': 'Safe Uncertainty-Aware Learning of Robotic Suturing', 'authors': 'Wilbert Peter Empleo, Yitaek Kim, Hansoul Kim, Thiusius Rajeeth Savarimuthu, Iñigo Iturrate', 'link': 'https://arxiv.org/abs/2505.16596', 'abstract': "Robot-Assisted Minimally Invasive Surgery is currently fully manually controlled by a trained surgeon. Automating this has great potential for alleviating issues, e.g., physical strain, highly repetitive tasks, and shortages of trained surgeons. For these reasons, recent works have utilized Artificial Intelligence methods, which show promising adaptability. Despite these advances, there is skepticism of these methods because they lack explainability and robust safety guarantees. This paper presents a framework for a safe, uncertainty-aware learning method. We train an Ensemble Model of Diffusion Policies using expert demonstrations of needle insertion. Using an Ensemble model, we can quantify the policy's epistemic uncertainty, which is used to determine Out-Of-Distribution scenarios. This allows the system to release control back to the surgeon in the event of an unsafe scenario. Additionally, we implement a model-free Control Barrier Function to place formal safety guarantees on the predicted action. We experimentally evaluate our proposed framework using a state-of-the-art robotic suturing simulator. We evaluate multiple scenarios, such as dropping the needle, moving the camera, and moving the phantom. The learned policy is robust to these perturbations, showing corrective behaviors and generalization, and it is possible to detect Out-Of-Distribution scenarios. We further demonstrate that the Control Barrier Function successfully limits the action to remain within our specified safety set in the case of unsafe predictions.", 'abstract_zh': '机器人辅助微创手术目前完全由训练有素的外科医生手动控制。通过自动化这一过程有望解决诸如身体疲劳、高度重复任务以及训练有素的外科医生短缺等问题。由于这些原因，近期的研究利用人工智能方法，显示出良好的适应性。尽管取得了这些进展，但人们对这些方法持怀疑态度，因为它们缺乏可解释性和 robust 安全保证。本文提出了一种安全、不确定性和意识的学习框架。我们使用专家展示的针插入示范训练了一个扩散策略集成模型。使用集成模型，可以量化策略的证伪不确定性，用于确定分布外场景。这使得系统在出现不安全场景时能够将控制权交还给外科医生。此外，我们实施了一个无模型的控制障碍函数，以正式的安全保证约束预测动作。我们使用最先进的机器人缝合模拟器实验性地评估了我们提出的框架。我们评估了多种场景，例如针脱落、移动摄像机和移动假体。学习到的策略对这些干扰具有鲁棒性，表现出纠正行为和泛化能力，并能够检测分布外场景。进一步证明，控制障碍函数成功地限制了动作在不安全预测的情况下保持在指定的安全集内。', 'title_zh': 'Safe 不确定性意识学习的机器人缝合'}
{'arxiv_id': 'arXiv:2505.16547', 'title': 'Find the Fruit: Designing a Zero-Shot Sim2Real Deep RL Planner for Occlusion Aware Plant Manipulation', 'authors': 'Nitesh Subedi, Hsin-Jung Yang, Devesh K. Jha, Soumik Sarkar', 'link': 'https://arxiv.org/abs/2505.16547', 'abstract': 'This paper presents an end-to-end deep reinforcement learning (RL) framework for occlusion-aware robotic manipulation in cluttered plant environments. Our approach enables a robot to interact with a deformable plant to reveal hidden objects of interest, such as fruits, using multimodal observations. We decouple the kinematic planning problem from robot control to simplify zero-shot sim2real transfer for the trained policy. Our results demonstrate that the trained policy, deployed using our framework, achieves up to 86.7% success in real-world trials across diverse initial conditions. Our findings pave the way toward autonomous, perception-driven agricultural robots that intelligently interact with complex foliage plants to "find the fruit" in challenging occluded scenarios, without the need for explicitly designed geometric and dynamic models of every plant scenario.', 'abstract_zh': '本文提出了一种端到端的深度强化学习框架，用于处理遮挡的植物环境中的物体感知机器人操作。我们的方法使机器人能够通过多模态观察与可变形植物互动，以揭示感兴趣的目标物体，如水果。我们将运动规划问题与机器人控制分离，简化了训练策略的零样本仿真实现。实验结果表明，使用我们框架部署的训练策略在多种初始条件下的实际试验中成功率达到86.7%。我们的研究为自主、感知驱动的农业机器人铺平了道路，这些机器人能够智能地与复杂的叶状植物交互，以便在挑战性遮挡场景中“找到水果”，而无需为每种植物场景显式设计几何和动力学模型。', 'title_zh': '寻找果实：设计一种用于遮挡感知植物操作的零-shot 离线到在线深度 reinforcement 学习规划器'}
{'arxiv_id': 'arXiv:2505.16517', 'title': 'ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models', 'authors': 'Zirui Song, Guangxian Ouyang, Mingzhe Li, Yuheng Ji, Chenxi Wang, Zixiang Xu, Zeyu Zhang, Xiaoqing Zhang, Qian Jiang, Zhenhao Chen, Zhongzhi Li, Rui Yan, Xiuying Chen', 'link': 'https://arxiv.org/abs/2505.16517', 'abstract': 'Large Vision-Language Models (LVLMs) have recently advanced robotic manipulation by leveraging vision for scene perception and language for instruction following. However, existing methods rely heavily on costly human-annotated training datasets, which limits their generalization and causes them to struggle in out-of-domain (OOD) scenarios, reducing real-world adaptability. To address these challenges, we propose ManipLVM-R1, a novel reinforcement learning framework that replaces traditional supervision with Reinforcement Learning using Verifiable Rewards (RLVR). By directly optimizing for task-aligned outcomes, our method enhances generalization and physical reasoning while removing the dependence on costly annotations. Specifically, we design two rule-based reward functions targeting key robotic manipulation subtasks: an Affordance Perception Reward to enhance localization of interaction regions, and a Trajectory Match Reward to ensure the physical plausibility of action paths. These rewards provide immediate feedback and impose spatial-logical constraints, encouraging the model to go beyond shallow pattern matching and instead learn deeper, more systematic reasoning about physical interactions.', 'abstract_zh': 'Large Vision-Language Models (LVLMs)通过融合视觉场景感知和语言指令跟随， recently advanced robotic manipulation. However, existing methods heavily rely on costly human-annotated training datasets, limiting generalization and causing struggles in out-of-domain scenarios, reducing real-world adaptability. To address these challenges, we propose ManipLVM-R1, a novel reinforcement learning framework that replaces traditional supervision with Reinforcement Learning using Verifiable Rewards (RLVR). By directly optimizing for task-aligned outcomes, our method enhances generalization and physical reasoning while removing the dependence on costly annotations. Specifically, we design two rule-based reward functions targeting key robotic manipulation subtasks: an Affordance Perception Reward to enhance localization of interaction regions, and a Trajectory Match Reward to ensure the physical plausibility of action paths. These rewards provide immediate feedback and impose spatial-logical constraints, encouraging the model to go beyond shallow pattern matching and instead learn deeper, more systematic reasoning about physical interactions.', 'title_zh': 'ManipLVM-R1: 基于强化学习的大型视觉语言模型在具身操作推理中的应用'}
{'arxiv_id': 'arXiv:2505.16498', 'title': 'Human-like Semantic Navigation for Autonomous Driving using Knowledge Representation and Large Language Models', 'authors': 'Augusto Luis Ballardini, Miguel Ángel Sotelo', 'link': 'https://arxiv.org/abs/2505.16498', 'abstract': 'Achieving full automation in self-driving vehicles remains a challenge, especially in dynamic urban environments where navigation requires real-time adaptability. Existing systems struggle to handle navigation plans when faced with unpredictable changes in road layouts, spontaneous detours, or missing map data, due to their heavy reliance on predefined cartographic information. In this work, we explore the use of Large Language Models to generate Answer Set Programming rules by translating informal navigation instructions into structured, logic-based reasoning. ASP provides non-monotonic reasoning, allowing autonomous vehicles to adapt to evolving scenarios without relying on predefined maps. We present an experimental evaluation in which LLMs generate ASP constraints that encode real-world urban driving logic into a formal knowledge representation. By automating the translation of informal navigation instructions into logical rules, our method improves adaptability and explainability in autonomous navigation. Results show that LLM-driven ASP rule generation supports semantic-based decision-making, offering an explainable framework for dynamic navigation planning that aligns closely with how humans communicate navigational intent.', 'abstract_zh': '在动态城市环境中实现自动驾驶车辆的完全自动化仍然具有挑战性，尤其需要实时适应能力。现有系统在面对道路布局的不可预测变化、自发绕行或缺失的地图数据时难以处理导航计划，因为它们严重依赖预定义的地图信息。在本工作中，我们探索使用大型语言模型生成逻辑规划规则，通过将非正式的导航指令翻译为结构化、基于逻辑的推理。ASP提供了非单调推理，使自动驾驶车辆能够在不依赖预定义地图的情况下适应不断变化的场景。我们提供了一个实验评估，在该评估中，LLM生成ASP约束，将实际城市的驾驶逻辑编码为形式化的知识表示。通过自动化将非正式的导航指令转换为逻辑规则，我们的方法提高了自主导航的适应性和可解释性。结果显示，由LLM驱动的ASP规则生成支持基于语义的决策，提供了一个与人类导航意图交流方式紧密对齐的可解释动态导航规划框架。', 'title_zh': '使用知识表示和大规模语言模型实现类人类语义导航的自主驾驶技术'}
{'arxiv_id': 'arXiv:2505.16478', 'title': 'Unified Multi-Rate Model Predictive Control for a Jet-Powered Humanoid Robot', 'authors': "Davide Gorbani, Giuseppe L'Erario, Hosameldin Awadalla Omer Mohamed, Daniele Pucci", 'link': 'https://arxiv.org/abs/2505.16478', 'abstract': "We propose a novel Model Predictive Control (MPC) framework for a jet-powered flying humanoid robot. The controller is based on a linearised centroidal momentum model to represent the flight dynamics, augmented with a second-order nonlinear model to explicitly account for the slow and nonlinear dynamics of jet propulsion. A key contribution is the introduction of a multi-rate MPC formulation that handles the different actuation rates of the robot's joints and jet engines while embedding the jet dynamics directly into the predictive model. We validated the framework using the jet-powered humanoid robot iRonCub, performing simulations in Mujoco; the simulation results demonstrate the robot's ability to recover from external disturbances and perform stable, non-abrupt flight manoeuvres, validating the effectiveness of the proposed approach.", 'abstract_zh': '我们提出了一种用于喷气动力 humanoïd 机器人的一种新颖的模型预测控制 (MPC) 框架。该控制器基于线性化的质心动量模型来表示飞行动力学，并通过二次非线性模型明确考虑喷气推进的慢速和非线性动力学。一个关键贡献是引入了一种多速率 MPC 表达式，该表达式处理了机器人关节和喷气发动机的不同作动速率，并将喷气动力学直接嵌入到预测模型中。我们使用喷气动力 humanoïd 机器人 iRonCub 进行了验证，在 Mujoco 中进行了仿真；仿真结果证明了该机器人能够从外部干扰中恢复，并执行稳定的非突变飞行机动，验证了所提出方法的有效性。', 'title_zh': '统一的多速率模型预测控制方法应用于喷气动力人形机器人'}
{'arxiv_id': 'arXiv:2505.16453', 'title': 'SpineWave: Harnessing Fish Rigid-Flexible Spinal Kinematics for Enhancing Biomimetic Robotic Locomotion', 'authors': 'Qu He, Weikun Li, Guangmin Dai, Hao Chen, Qimeng Liu, Xiaoqing Tian, Jie You, Weicheng Cui, Michael S. Triantafyllou, Dixia Fan', 'link': 'https://arxiv.org/abs/2505.16453', 'abstract': 'Fish have endured millions of years of evolution, and their distinct rigid-flexible body structures offer inspiration for overcoming challenges in underwater robotics, such as limited mobility, high energy consumption, and adaptability. This paper introduces SpineWave, a biomimetic robotic fish featuring a fish-spine-like rigid-flexible transition structure. The structure integrates expandable fishbone-like ribs and adjustable magnets, mimicking the stretch and recoil of fish muscles to balance rigidity and flexibility. In addition, we employed an evolutionary algorithm to optimize the hydrodynamics of the robot, achieving significant improvements in swimming performance. Real-world tests demonstrated robustness and potential for environmental monitoring, underwater exploration, and industrial inspection. These tests established SpineWave as a transformative platform for aquatic robotics.', 'abstract_zh': '鱼类经过数百万年的进化，其独特的刚柔并济的身体结构为克服水下机器人领域中的有限移动性、高能耗和适应能力等挑战提供了灵感。本文介绍了一种名为SpineWave的仿生水下机器人鱼，该机器人鱼采用了类似鱼类脊椎的刚柔过渡结构，结合可扩展的鱼骨状肋骨和可调节的磁铁，模拟鱼类肌肉的拉伸和回弹，实现刚性和柔性的平衡。此外，我们还采用进化算法优化了机器人的流体力学性能，显著提高了其游泳性能。实地测试表明，SpineWave具有较强的环境监测、水下探索和工业检测的潜力，奠定了其在水下机器人领域的革新平台地位。', 'title_zh': '脊柱波：利用鱼类刚柔脊柱运动学增强生物仿生机器人运动性能'}
{'arxiv_id': 'arXiv:2505.16394', 'title': 'Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)', 'authors': 'Zhenjie Yang, Xiaosong Jia, Qifeng Li, Xue Yang, Maoqing Yao, Junchi Yan', 'link': 'https://arxiv.org/abs/2505.16394', 'abstract': 'Reinforcement Learning (RL) can mitigate the causal confusion and distribution shift inherent to imitation learning (IL). However, applying RL to end-to-end autonomous driving (E2E-AD) remains an open problem for its training difficulty, and IL is still the mainstream paradigm in both academia and industry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated promising results in neural planning; however, these methods typically require privileged information as input rather than raw sensor data. We fill this gap by designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently train an auxiliary privileged world model paired with a neural planner that uses privileged information as input. Subsequently, we introduce a raw sensor world model trained via our proposed Guidance Mechanism, which ensures consistency between the raw sensor world model and the privileged world model during rollouts. Finally, the raw sensor world model combines the prior knowledge embedded in the heads of the privileged world model to effectively guide the training of the raw sensor policy. Raw2Drive is so far the only RL based end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it achieves state-of-the-art performance.', 'abstract_zh': '基于原始传感器数据的Model-based Reinforcement Learning方法在端到端自动驾驶中的应用：Raw2Drive', 'title_zh': 'Raw2Drive：与世界模型对齐的强化学习在端到端自动驾驶中的应用（在CARLA v2中）'}
{'arxiv_id': 'arXiv:2505.16377', 'title': 'VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving', 'authors': 'Yansong Qu, Zilin Huang, Zihao Sheng, Jiancong Chen, Sikai Chen, Samuel Labi', 'link': 'https://arxiv.org/abs/2505.16377', 'abstract': 'Reinforcement learning (RL)-based autonomous driving policy learning faces critical limitations such as low sample efficiency and poor generalization; its reliance on online interactions and trial-and-error learning is especially unacceptable in safety-critical scenarios. Existing methods including safe RL often fail to capture the true semantic meaning of "safety" in complex driving contexts, leading to either overly conservative driving behavior or constraint violations. To address these challenges, we propose VL-SAFE, a world model-based safe RL framework with Vision-Language model (VLM)-as-safety-guidance paradigm, designed for offline safe policy learning. Specifically, we construct offline datasets containing data collected by expert agents and labeled with safety scores derived from VLMs. A world model is trained to generate imagined rollouts together with safety estimations, allowing the agent to perform safe planning without interacting with the real environment. Based on these imagined trajectories and safety evaluations, actor-critic learning is conducted under VLM-based safety guidance to optimize the driving policy more safely and efficiently. Extensive evaluations demonstrate that VL-SAFE achieves superior sample efficiency, generalization, safety, and overall performance compared to existing baselines. To the best of our knowledge, this is the first work that introduces a VLM-guided world model-based approach for safe autonomous driving. The demo video and code can be accessed at: this https URL', 'abstract_zh': '基于视觉语言模型的强化学习安全自动驾驶策略学习框架：VL-SAFE', 'title_zh': 'VL-SAFE: 基于世界模型的视觉-语言引导安全感知强化学习自主驾驶'}
{'arxiv_id': 'arXiv:2505.16289', 'title': 'TacCompress: A Benchmark for Multi-Point Tactile Data Compression in Dexterous Manipulation', 'authors': 'Yang Li, Yan Zhao, Zhengxue Cheng, Hengdi Zhang', 'link': 'https://arxiv.org/abs/2505.16289', 'abstract': "Though robotic dexterous manipulation has progressed substantially recently, challenges like in-hand occlusion still necessitate fine-grained tactile perception, leading to the integration of more tactile sensors into robotic hands. Consequently, the increased data volume imposes substantial bandwidth pressure on signal transmission from the hand's controller. However, the acquisition and compression of multi-point tactile signals based on the dexterous hands' physical structures have not been thoroughly explored. In this paper, our contributions are twofold. First, we introduce a Multi-Point Tactile Dataset for Dexterous Hand Grasping (Dex-MPTD). This dataset captures tactile signals from multiple contact sensors across various objects and grasping poses, offering a comprehensive benchmark for advancing dexterous robotic manipulation research. Second, we investigate both lossless and lossy compression on Dex-MPTD by converting tactile data into images and applying six lossless and five lossy image codecs for efficient compression. Experimental results demonstrate that tactile data can be losslessly compressed to as low as 0.0364 bits per sub-sample (bpss), achieving approximately 200$\\times$ compression ratio compared to the raw tactile data. Efficient lossy compressors like HM and VTM can achieve about 1000x data reductions while preserving acceptable data fidelity. The exploration of lossy compression also reveals that screen-content-targeted coding tools outperform general-purpose codecs in compressing tactile data.", 'abstract_zh': '多点触觉数据集用于灵巧手抓取（Dex-MPTD）及其压缩研究', 'title_zh': 'TacCompress: 多点触觉数据在灵巧操作中的压缩基准'}
{'arxiv_id': 'arXiv:2505.16249', 'title': 'Manipulating Elasto-Plastic Objects With 3D Occupancy and Learning-Based Predictive Control', 'authors': 'Zhen Zhang, Xiangyu Chu, Yunxi Tang, Lulu Zhao, Jing Huang, Zhongliang Jiang, K. W. Samuel Au', 'link': 'https://arxiv.org/abs/2505.16249', 'abstract': 'Manipulating elasto-plastic objects remains a significant challenge due to severe self-occlusion, difficulties of representation, and complicated dynamics. This work proposes a novel framework for elasto-plastic object manipulation with a quasi-static assumption for motions, leveraging 3D occupancy to represent such objects, a learned dynamics model trained with 3D occupancy, and a learning-based predictive control algorithm to address these challenges effectively. We build a novel data collection platform to collect full spatial information and propose a pipeline for generating a 3D occupancy dataset. To infer the 3D occupancy during manipulation, an occupancy prediction network is trained with multiple RGB images supervised by the generated dataset. We design a deep neural network empowered by a 3D convolution neural network (CNN) and a graph neural network (GNN) to predict the complex deformation with the inferred 3D occupancy results. A learning-based predictive control algorithm is introduced to plan the robot actions, incorporating a novel shape-based action initialization module specifically designed to improve the planner efficiency. The proposed framework in this paper can successfully shape the elasto-plastic objects into a given goal shape and has been verified in various experiments both in simulation and the real world.', 'abstract_zh': '基于拟静态假设的弹塑性物体操纵新框架', 'title_zh': '基于3D occupancy和学习驱动的预测控制的弹性塑性物体操控'}
{'arxiv_id': 'arXiv:2505.16214', 'title': 'Behavioral Safety Assessment towards Large-scale Deployment of Autonomous Vehicles', 'authors': 'Henry X. Liu, Xintao Yan, Haowei Sun, Tinghan Wang, Zhijie Qiao, Haojie Zhu, Shengyin Shen, Shuo Feng, Greg Stevens, Greg McGuire', 'link': 'https://arxiv.org/abs/2505.16214', 'abstract': "Autonomous vehicles (AVs) have significantly advanced in real-world deployment in recent years, yet safety continues to be a critical barrier to widespread adoption. Traditional functional safety approaches, which primarily verify the reliability, robustness, and adequacy of AV hardware and software systems from a vehicle-centric perspective, do not sufficiently address the AV's broader interactions and behavioral impact on the surrounding traffic environment. To overcome this limitation, we propose a paradigm shift toward behavioral safety, a comprehensive approach focused on evaluating AV responses and interactions within the traffic environment. To systematically assess behavioral safety, we introduce a third-party AV safety assessment framework comprising two complementary evaluation components: the Driver Licensing Test and the Driving Intelligence Test. The Driver Licensing Test evaluates the AV's reactive behaviors under controlled scenarios, ensuring basic behavioral competency. In contrast, the Driving Intelligence Test assesses the AV's interactive behaviors within naturalistic traffic conditions, quantifying the frequency of safety-critical events to deliver statistically meaningful safety metrics before large-scale deployment. We validated our proposed framework using this http URL, an open-source Level 4 AV, tested both in simulated environments and on the physical test track at the University of Michigan's Mcity Testing Facility. The results indicate that this http URL passed 6 out of 14 scenarios and exhibited a crash rate of 3.01e-3 crashes per mile, approximately 1,000 times higher than the average human driver crash rate. During the tests, we also uncovered several unknown unsafe scenarios for this http URL. These findings underscore the necessity of behavioral safety evaluations for improving AV safety performance prior to widespread public deployment.", 'abstract_zh': '自主驾驶车辆的行为安全性评估框架', 'title_zh': '面向自动驾驶大规模部署的行为安全评估'}
{'arxiv_id': 'arXiv:2505.16196', 'title': 'SEM: Enhancing Spatial Understanding for Robust Robot Manipulation', 'authors': 'Xuewu Lin, Tianwei Lin, Lichao Huang, Hongyu Xie, Yiwei Jin, Keyu Li, Zhizhong Su', 'link': 'https://arxiv.org/abs/2505.16196', 'abstract': 'A key challenge in robot manipulation lies in developing policy models with strong spatial understanding, the ability to reason about 3D geometry, object relations, and robot embodiment. Existing methods often fall short: 3D point cloud models lack semantic abstraction, while 2D image encoders struggle with spatial reasoning. To address this, we propose SEM (Spatial Enhanced Manipulation model), a novel diffusion-based policy framework that explicitly enhances spatial understanding from two complementary perspectives. A spatial enhancer augments visual representations with 3D geometric context, while a robot state encoder captures embodiment-aware structure through graphbased modeling of joint dependencies. By integrating these modules, SEM significantly improves spatial understanding, leading to robust and generalizable manipulation across diverse tasks that outperform existing baselines.', 'abstract_zh': '机器人操作中的一个关键挑战在于开发具有强烈空间理解能力的策略模型，能够对3D几何、物体关系和机器人自身进行推理。现有方法往往存在不足：3D点云模型缺乏语义抽象，而2D图像编码器在空间推理方面存在问题。为解决这一问题，我们提出SEM（空间增强操作模型）这一新颖的基于扩散的策略框架，从两个互补的角度显式增强空间理解。空间增强器通过添加3D几何上下文来增强视觉表示，而机器人状态编码器则通过基于图的关节依赖建模来捕获感知机器人自身的结构。通过整合这些模块，SEM显著提高了空间理解能力，实现了在多种任务中的鲁棒性和广泛适用性，超越了现有基准方法。', 'title_zh': 'SEM: 提升空间理解以实现稳健机器人操作'}
{'arxiv_id': 'arXiv:2505.16187', 'title': 'EasyInsert: A Data-Efficient and Generalizable Insertion Policy', 'authors': 'Guanghe Li, Junming Zhao, Shengjie Wang, Yang Gao', 'link': 'https://arxiv.org/abs/2505.16187', 'abstract': 'Insertion task is highly challenging that requires robots to operate with exceptional precision in cluttered environments. Existing methods often have poor generalization capabilities. They typically function in restricted and structured environments, and frequently fail when the plug and socket are far apart, when the scene is densely cluttered, or when handling novel objects. They also rely on strong assumptions such as access to CAD models or a digital twin in simulation. To address this, we propose EasyInsert, a framework which leverages the human intuition that relative pose (delta pose) between plug and socket is sufficient for successful insertion, and employs efficient and automated real-world data collection with minimal human labor to train a generalizable model for relative pose prediction. During execution, EasyInsert follows a coarse-to-fine execution procedure based on predicted delta pose, and successfully performs various insertion tasks. EasyInsert demonstrates strong zero-shot generalization capability for unseen objects in cluttered environments, handling cases with significant initial pose deviations while maintaining high sample efficiency and requiring little human effort. In real-world experiments, with just 5 hours of training data, EasyInsert achieves over 90% success in zero-shot insertion for 13 out of 15 unseen novel objects, including challenging objects like Type-C cables, HDMI cables, and Ethernet cables. Furthermore, with only one human demonstration and 4 minutes of automatically collected data for fine-tuning, it reaches over 90% success rate for all 15 objects.', 'abstract_zh': '插入任务在杂乱环境中要求机器人以极高的精度操作，极具挑战性。现有方法通常泛化能力较弱，通常在受限和结构化的环境中运行，并且在插头和插座距离较远、场景高度杂乱或处理新型物体时经常失败。它们还依赖于较强的假设，如获取CAD模型或在模拟中使用数字孪生。为解决这一问题，我们提出了一种名为EasyInsert的框架，该框架利用了人类直觉，即插头和插座之间的相对姿态（delta姿态）足以实现成功的插入，并通过最少的人工劳动有效地收集高效的现实世界数据来训练一个泛化能力强的相对姿态预测模型。在执行过程中，EasyInsert采用了从粗到细的执行流程，基于预测的delta姿态成功完成了多种插入任务。EasyInsert在杂乱环境中对未见过的物体展示了强大的零样本泛化能力，能够处理显著的初始姿态偏离情况，同时保持高样本效率并要求较少的人工努力。在真实世界实验中，仅用5小时的训练数据，EasyInsert对13个未见过的新型物体的零样本插入成功率超过90%，包括Type-C电缆、HDMI电缆和以太网电缆等具有挑战性的物体。此外，通过一次人工演示和4分钟的自动收集数据进行微调，它对所有15个物体的插入成功率超过90%。', 'title_zh': 'EasyInsert: 一种数据高效且泛化能力强的插入策略'}
{'arxiv_id': 'arXiv:2505.16167', 'title': 'Tactile-based Reinforcement Learning for Adaptive Grasping under Observation Uncertainties', 'authors': 'Xiao Hu, Yang Ye', 'link': 'https://arxiv.org/abs/2505.16167', 'abstract': 'Robotic manipulation in industrial scenarios such as construction commonly faces uncertain observations in which the state of the manipulating object may not be accurately captured due to occlusions and partial observables. For example, object status estimation during pipe assembly, rebar installation, and electrical installation can be impacted by observation errors. Traditional vision-based grasping methods often struggle to ensure robust stability and adaptability. To address this challenge, this paper proposes a tactile simulator that enables a tactile-based adaptive grasping method to enhance grasping robustness. This approach leverages tactile feedback combined with the Proximal Policy Optimization (PPO) reinforcement learning algorithm to dynamically adjust the grasping posture, allowing adaptation to varying grasping conditions under inaccurate object state estimations. Simulation results demonstrate that the proposed method effectively adapts grasping postures, thereby improving the success rate and stability of grasping tasks.', 'abstract_zh': '工业场景下（如建筑施工）的机器人操作常面临不确定的观测问题，由于遮挡和部分可观测性，操作对象的状态可能无法准确捕捉。例如，在管道组装、钢筋安装和电气安装过程中，观测误差会影响物体状态估计。传统的基于视觉的抓取方法往往难以确保抓取的 robust 稳定性和适应性。为应对这一挑战，本文提出一种触觉模拟器，以增强抓取的 robust 性。该方法利用触觉反馈结合 Proximal Policy Optimization (PPO) 强化学习算法动态调整抓取姿态，适应不同不准确物体状态估计下的抓取条件。仿真结果表明，所提方法能够有效调整抓取姿态，从而提高抓取任务的成功率和稳定性。', 'title_zh': '基于触觉的强化学习在观测不确定性下的自适应抓取'}
{'arxiv_id': 'arXiv:2505.16087', 'title': 'Event-based Reconfiguration Control for Time-varying Formation of Robot Swarms in Narrow Spaces', 'authors': 'Duy-Nam Bui, Manh Duong Phung, Hung Pham Duy', 'link': 'https://arxiv.org/abs/2505.16087', 'abstract': "This study proposes an event-based reconfiguration control to navigate a robot swarm through challenging environments with narrow passages such as valleys, tunnels, and corridors. The robot swarm is modeled as an undirected graph, where each node represents a robot capable of collecting real-time data on the environment and the states of other robots in the formation. This data serves as the input for the controller to provide dynamic adjustments between the desired and straight-line configurations. The controller incorporates a set of behaviors, designed using artificial potential fields, to meet the requirements of goal-oriented motion, formation maintenance, tailgating, and collision avoidance. The stability of the formation control is guaranteed via the Lyapunov theorem. Simulation and comparison results show that the proposed controller not only successfully navigates the robot swarm through narrow spaces but also outperforms other established methods in key metrics including the success rate, heading order, speed, travel time, and energy efficiency. Software-in-the-loop tests have also been conducted to validate the controller's applicability in practical scenarios. The source code of the controller is available at this https URL.", 'abstract_zh': '基于事件的重构控制方法在狭窄通道环境下的机器人集群导航', 'title_zh': '基于事件的重构控制方法研究：狭窄空间中时间变化的机器人 swarm 形态控制'}
{'arxiv_id': 'arXiv:2505.16084', 'title': 'Motion Priors Reimagined: Adapting Flat-Terrain Skills for Complex Quadruped Mobility', 'authors': 'Zewei Zhang, Chenhao Li, Takahiro Miki, Marco Hutter', 'link': 'https://arxiv.org/abs/2505.16084', 'abstract': "Reinforcement learning (RL)-based legged locomotion controllers often require meticulous reward tuning to track velocities or goal positions while preserving smooth motion on various terrains. Motion imitation methods via RL using demonstration data reduce reward engineering but fail to generalize to novel environments. We address this by proposing a hierarchical RL framework in which a low-level policy is first pre-trained to imitate animal motions on flat ground, thereby establishing motion priors. A subsequent high-level, goal-conditioned policy then builds on these priors, learning residual corrections that enable perceptive locomotion, local obstacle avoidance, and goal-directed navigation across diverse and rugged terrains. Simulation experiments illustrate the effectiveness of learned residuals in adapting to progressively challenging uneven terrains while still preserving the locomotion characteristics provided by the motion priors. Furthermore, our results demonstrate improvements in motion regularization over baseline models trained without motion priors under similar reward setups. Real-world experiments with an ANYmal-D quadruped robot confirm our policy's capability to generalize animal-like locomotion skills to complex terrains, demonstrating smooth and efficient locomotion and local navigation performance amidst challenging terrains with obstacles.", 'abstract_zh': '基于 reinforcement learning 的腿足运动控制器通常需要精细调节奖励以在各种地形上跟踪速度或目标位置同时保持平滑运动。通过使用演示数据的 reinforcement learning 方法进行运动模仿可以减少奖励工程，但难以泛化到新型环境。为此，我们提出了一种分层 reinforcement learning 框架，在该框架中，低层策略首先在平坦地面预训练以模仿动物运动，从而建立运动先验；随后，高层、基于目标的策略在此基础上学习残差修正，以实现感知驱动的运动、局部障碍物避免以及在多样且崎岖地形上的目标导向导航。仿真实验表明，所学残差在适应逐渐更具挑战性的不平地形时依然能够保留由运动先验提供的运动特征。此外，我们的结果表明，在提供运动先验的基线模型下，我们的策略在类似奖励设置下的运动正则化方面有所改进。实际实验中，使用 ANYmal-D 四足机器人验证了策略能够将类似的动物运动技能泛化到复杂地形，并在具有障碍物的挑战性地形中实现了平滑且高效的运动和局部导航性能。', 'title_zh': '重设想运动先验：适应复杂四足移动的平面地形技能'}
{'arxiv_id': 'arXiv:2505.16062', 'title': 'WaveTouch: Active Tactile Sensing Using Vibro-Feedback for Classification of Variable Stiffness and Infill Density Objects', 'authors': 'Danissa Sandykbayeva, Valeriya Kostyukova, Aditya Shekhar Nittala, Zhanat Kappassov, Bakhtiyar Orazbayev', 'link': 'https://arxiv.org/abs/2505.16062', 'abstract': 'The perception and recognition of the surroundings is one of the essential tasks for a robot. With preliminary knowledge about a target object, it can perform various manipulation tasks such as rolling motion, palpation, and force control. Minimizing possible damage to the sensing system and testing objects during manipulation are significant concerns that persist in existing research solutions. To address this need, we designed a new type of tactile sensor based on the active vibro-feedback for object stiffness classification. With this approach, the classification can be performed during the gripping process, enabling the robot to quickly estimate the appropriate level of gripping force required to avoid damaging or dropping the object. This contrasts with passive vibration sensing, which requires to be triggered by object movement and is often inefficient for establishing a secure grip. The main idea is to observe the received changes in artificially injected vibrations that propagate through objects with different physical properties and molecular structures. The experiments with soft subjects demonstrated higher absorption of the received vibrations, while the opposite is true for the rigid subjects that not only demonstrated low absorption but also enhancement of the vibration signal.', 'abstract_zh': '基于主动振动反馈的触觉传感器及其在物体刚度分类中的应用', 'title_zh': 'WaveTouch: 基于振动反馈的活性触觉传感用于变刚度和填充密度物体分类'}
{'arxiv_id': 'arXiv:2505.16055', 'title': 'Proactive Hierarchical Control Barrier Function-Based Safety Prioritization in Close Human-Robot Interaction Scenarios', 'authors': 'Patanjali Maithania, Aliasghar Araba, Farshad Khorramia, Prashanth Krishnamurthya', 'link': 'https://arxiv.org/abs/2505.16055', 'abstract': 'In collaborative human-robot environments, the unpredictable and dynamic nature of human motion can lead to situations where collisions become unavoidable. In such cases, it is essential for the robotic system to proactively mitigate potential harm through intelligent control strategies. This paper presents a hierarchical control framework based on Control Barrier Functions (CBFs) designed to ensure safe and adaptive operation of autonomous robotic manipulators during close-proximity human-robot interaction. The proposed method introduces a relaxation variable that enables real-time prioritization of safety constraints, allowing the robot to dynamically manage collision risks based on the criticality of different parts of the human body. A secondary constraint mechanism is incorporated to resolve infeasibility by increasing the priority of imminent threats. The framework is experimentally validated on a Franka Research 3 robot equipped with a ZED2i AI camera for real-time human pose and body detection. Experimental results confirm that the CBF-based controller, integrated with depth sensing, facilitates responsive and safe human-robot collaboration, while providing detailed risk analysis and maintaining robust performance in highly dynamic settings.', 'abstract_zh': '基于控制屏障函数的层次控制框架：在动态人体运动环境中实现自主机器人操作的安全与适应性', 'title_zh': '主动分层控制约束函数基于的安全优先级在近距离人机交互场景中'}
{'arxiv_id': 'arXiv:2505.16042', 'title': 'Reference Free Platform Adaptive Locomotion for Quadrupedal Robots using a Dynamics Conditioned Policy', 'authors': 'David Rytz, Suyoung Choi, Wanming Yu, Wolfgang Merkt, Jemin Hwangbo, Ioannis Havoutis', 'link': 'https://arxiv.org/abs/2505.16042', 'abstract': 'This article presents Platform Adaptive Locomotion (PAL), a unified control method for quadrupedal robots with different morphologies and dynamics. We leverage deep reinforcement learning to train a single locomotion policy on procedurally generated robots. The policy maps proprioceptive robot state information and base velocity commands into desired joint actuation targets, which are conditioned using a latent embedding of the temporally local system dynamics. We explore two conditioning strategies - one using a GRU-based dynamics encoder and another using a morphology-based property estimator - and show that morphology-aware conditioning outperforms temporal dynamics encoding regarding velocity task tracking for our hardware test on ANYmal C. Our results demonstrate that both approaches achieve robust zero-shot transfer across multiple unseen simulated quadrupeds. Furthermore, we demonstrate the need for careful robot reference modelling during training, enabling us to reduce the velocity tracking error by up to 30% compared to the baseline method. Despite PAL not surpassing the best-performing reference-free controller in all cases, our analysis uncovers critical design choices and informs improvements to the state of the art.', 'abstract_zh': '平台自适应步行（PAL）：一种用于不同形态和动力学四足机器人的一体化控制方法', 'title_zh': '参考自由的平台自适应四足机器人运动控制基于动力学条件策略'}
{'arxiv_id': 'arXiv:2505.15954', 'title': 'Integrating Robotic Navigation with Blockchain: A Novel PoS-Based Approach for Heterogeneous Robotic Teams', 'authors': 'Nasim Paykari, Ali Alfatemi, Damian M. Lyons, Mohamed Rahouti', 'link': 'https://arxiv.org/abs/2505.15954', 'abstract': 'This work explores a novel integration of blockchain methodologies with Wide Area Visual Navigation (WAVN) to address challenges in visual navigation for a heterogeneous team of mobile robots deployed for unstructured applications in agriculture, forestry, etc. Focusing on overcoming challenges such as GPS independence, environmental changes, and computational limitations, the study introduces the Proof of Stake (PoS) mechanism, commonly used in blockchain systems, into the WAVN framework \\cite{Lyons_2022}. This integration aims to enhance the cooperative navigation capabilities of robotic teams by prioritizing robot contributions based on their navigation reliability. The methodology involves a stake weight function, consensus score with PoS, and a navigability function, addressing the computational complexities of robotic cooperation and data validation. This innovative approach promises to optimize robotic teamwork by leveraging blockchain principles, offering insights into the scalability, efficiency, and overall system performance. The project anticipates significant advancements in autonomous navigation and the broader application of blockchain technology beyond its traditional financial context.', 'abstract_zh': '本研究探索了区块链方法与广域视觉导航（WAVN）的新型集成，以解决异构移动机器人团队在农业、林业等非结构化应用中视觉导航面临的挑战。该研究专注于克服如GPS独立性、环境变化和计算限制等挑战，通过将区块链系统中常用的权益证明（PoS）机制引入WAVN框架（参见[@Lyons_2022]），旨在通过优先考虑机器人基于其导航 reliability 的贡献来增强机器人团队的协同导航能力。该方法论包括质押权重函数、结合PoS的共识分数以及导航性函数，以应对机器人合作和数据验证的计算复杂性。这一创新方法有望通过利用区块链原则优化机器人团队协作，为自主导航和区块链技术在更广泛领域的应用提供深刻的见解。该项目预期在自主导航和区块链技术的非金融应用方面取得重大进展。', 'title_zh': '基于PoS的新颖方法：区块链与异构机器人团队的导航集成'}
{'arxiv_id': 'arXiv:2505.15939', 'title': 'Human Workload Prediction: Lag Horizon Selection', 'authors': 'Mark-Robin Giolando, Julie A. Adams', 'link': 'https://arxiv.org/abs/2505.15939', 'abstract': "Human-robot teams must be aware of human workload when operating in uncertain, dynamic environments. Prior work employed physiological response metrics from wearable sensors to estimate the current human workload; however, these estimates only enable robots to respond to under- or overload conditions reactively. Current human workload prediction approaches are limited to short prediction horizons and fail to investigate variable lag horizons' impact on predictions. This letter investigates the impact of lag horizons on both univariate and multivariate time series forecasting models for human workload prediction. A key finding is that univariate predictions required longer lag horizons of 240 seconds (s), whereas multivariate workload predictions sufficed with shorter lag horizons with diminishing returns around 120s.", 'abstract_zh': '人类和机器人团队在不确定、动态环境中操作时需要意识到人类的工作负荷。本信研究了滞后时间窗对单变量和多变量时间序列工作负荷预测模型的影响。关键发现是，单变量预测需要较长的滞后时间窗（240秒），而多变量工作负荷预测在约120秒时具有递减的回报。', 'title_zh': '人类工作负载预测：滞后时间窗选择'}
{'arxiv_id': 'arXiv:2505.15925', 'title': 'VERDI: VLM-Embedded Reasoning for Autonomous Driving', 'authors': 'Bowen Feng, Zhiting Mei, Baiang Li, Julian Ost, Roger Girgis, Anirudha Majumdar, Felix Heide', 'link': 'https://arxiv.org/abs/2505.15925', 'abstract': 'While autonomous driving (AD) stacks struggle with decision making under partial observability and real-world complexity, human drivers are capable of commonsense reasoning to make near-optimal decisions with limited information. Recent work has attempted to leverage finetuned Vision-Language Models (VLMs) for trajectory planning at inference time to emulate human behavior. Despite their success in benchmark evaluations, these methods are often impractical to deploy (a 70B parameter VLM inference at merely 8 tokens per second requires more than 160G of memory), and their monolithic network structure prohibits safety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for autonomous Driving (VERDI), a training-time framework that distills the reasoning process and commonsense knowledge of VLMs into the AD stack. VERDI augments modular differentiable end-to-end (e2e) AD models by aligning intermediate module outputs at the perception, prediction, and planning stages with text features explaining the driving reasoning process produced by VLMs. By encouraging alignment in latent space, \\textsc{VERDI} enables the modular AD stack to internalize structured reasoning, without incurring the inference-time costs of large VLMs. We demonstrate the effectiveness of our method on the NuScenes dataset and find that VERDI outperforms existing e2e methods that do not embed reasoning by 10% in $\\ell_{2}$ distance, while maintaining high inference speed.', 'abstract_zh': 'VERDI：嵌入视觉语言模型的自主驾驶推理', 'title_zh': 'VERDI: 嵌入VLM的自主驾驶推理'}
{'arxiv_id': 'arXiv:2505.17016', 'title': 'Interactive Post-Training for Vision-Language-Action Models', 'authors': 'Shuhan Tan, Kairan Dou, Yue Zhao, Philipp Krähenbühl', 'link': 'https://arxiv.org/abs/2505.17016', 'abstract': 'We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based interactive post-training paradigm that fine-tunes pretrained Vision-Language-Action (VLA) models using only sparse binary success rewards. Existing VLA training pipelines rely heavily on offline expert demonstration data and supervised imitation, limiting their ability to adapt to new tasks and environments under low-data regimes. RIPT-VLA addresses this by enabling interactive post-training with a stable policy optimization algorithm based on dynamic rollout sampling and leave-one-out advantage estimation.\nRIPT-VLA has the following characteristics. First, it applies to various VLA models, resulting in an improvement on the lightweight QueST model by 21.2%, and the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it is computationally efficient and data-efficient: with only one demonstration, RIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success rate within 15 iterations. Furthermore, we demonstrate that the policy learned by RIPT-VLA generalizes across different tasks and scenarios and is robust to the initial state context. These results highlight RIPT-VLA as a practical and effective paradigm for post-training VLA models through minimal supervision.', 'abstract_zh': 'RIPT-VLA：基于稀疏二元成功率 reward 的简单可扩展的交互式后训练强化学习范式', 'title_zh': '训练后交互式微调用于视觉-语言-动作模型'}
{'arxiv_id': 'arXiv:2505.17006', 'title': 'CoMo: Learning Continuous Latent Motion from Internet Videos for Scalable Robot Learning', 'authors': 'Jiange Yang, Yansong Shi, Haoyi Zhu, Mingyu Liu, Kaijing Ma, Yating Wang, Gangshan Wu, Tong He, Limin Wang', 'link': 'https://arxiv.org/abs/2505.17006', 'abstract': 'Learning latent motion from Internet videos is crucial for building generalist robots. However, existing discrete latent action methods suffer from information loss and struggle with complex and fine-grained dynamics. We propose CoMo, which aims to learn more informative continuous motion representations from diverse, internet-scale videos. CoMo employs a early temporal feature difference mechanism to prevent model collapse and suppress static appearance noise, effectively discouraging shortcut learning problem. Furthermore, guided by the information bottleneck principle, we constrain the latent motion embedding dimensionality to achieve a better balance between retaining sufficient action-relevant information and minimizing the inclusion of action-irrelevant appearance noise. Additionally, we also introduce two new metrics for more robustly and affordably evaluating motion and guiding motion learning methods development: (i) the linear probing MSE of action prediction, and (ii) the cosine similarity between past-to-current and future-to-current motion embeddings. Critically, CoMo exhibits strong zero-shot generalization, enabling it to generate continuous pseudo actions for previously unseen video domains. This capability facilitates unified policy joint learning using pseudo actions derived from various action-less video datasets (such as cross-embodiment videos and, notably, human demonstration videos), potentially augmented with limited labeled robot data. Extensive experiments show that policies co-trained with CoMo pseudo actions achieve superior performance with both diffusion and autoregressive architectures in simulated and real-world settings.', 'abstract_zh': '从互联网视频中学习潜在运动对于构建通用机器人至关重要。然而，现有的离散潜在动作方法存在信息损失问题，并且难以处理复杂的细粒度动态。我们提出CoMo，旨在从多样化的互联网规模视频中学习更具信息量的连续运动表示。CoMo采用早期时间特征差异机制防止模型崩溃并抑制静态外观噪声，有效地避免了捷径学习问题。此外，遵循信息瓶颈原则，我们约束潜在运动嵌入的维度，以实现保留足够相关动作信息与最小化不相关外观噪声包含之间的更好平衡。此外，我们还引入了两个新的评估指标，以更稳健和经济的方式评估运动，并指导运动学习方法的发展：（i）动作预测的线性探针均方误差，以及(ii) 过去到当前和未来到当前运动嵌入的余弦相似度。关键的是，CoMo表现出强大的零样本泛化能力，能够为之前未见过的视频域生成连续伪动作。这一能力使得使用来自各种无动作视频数据集（如跨体裁视频和值得注意的人类示范视频）派生的伪动作进行统一策略联合学习成为可能，这些数据集可能辅以有限的标注机器人数据。大量实验表明，与CoMo伪动作协同训练的策略在模拟和真实世界环境中使用扩散和自回归架构时表现出优越性能。', 'title_zh': 'CoMo: 从互联网视频学习连续潜运动以实现可扩展的机器人学习'}
{'arxiv_id': 'arXiv:2505.16985', 'title': 'Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation', 'authors': 'Moru Liu, Hao Dong, Jessica Kelly, Olga Fink, Mario Trapp', 'link': 'https://arxiv.org/abs/2505.16985', 'abstract': 'Out-of-distribution (OOD) detection and segmentation are crucial for deploying machine learning models in safety-critical applications such as autonomous driving and robot-assisted surgery. While prior research has primarily focused on unimodal image data, real-world applications are inherently multimodal, requiring the integration of multiple modalities for improved OOD detection. A key challenge is the lack of supervision signals from unknown data, leading to overconfident predictions on OOD samples. To address this challenge, we propose Feature Mixing, an extremely simple and fast method for multimodal outlier synthesis with theoretical support, which can be further optimized to help the model better distinguish between in-distribution (ID) and OOD data. Feature Mixing is modality-agnostic and applicable to various modality combinations. Additionally, we introduce CARLA-OOD, a novel multimodal dataset for OOD segmentation, featuring synthetic OOD objects across diverse scenes and weather conditions. Extensive experiments on SemanticKITTI, nuScenes, CARLA-OOD datasets, and the MultiOOD benchmark demonstrate that Feature Mixing achieves state-of-the-art performance with a $10 \\times$ to $370 \\times$ speedup. Our source code and dataset will be available at this https URL.', 'abstract_zh': '超出分布(OOD)检测与分割对于自主驾驶和机器人辅助手术等安全关键应用的机器学习模型部署至关重要。虽然之前的研究主要集中在单模态图像数据上，但现实世界的应用是多模态的，需要整合多种模态以提高OOD检测性能。一个主要挑战是对未知数据缺乏监督信号，导致对OOD样本的过度自信预测。为解决这一挑战，我们提出了一种特征混合(Feature Mixing)方法，这是一种理论上支持的、极简且高效的多模态离群值合成方法，并可通过进一步优化帮助模型更好地区分分布内(ID)和OOD数据。特征混合方法不依赖于模态，适用于各种模态组合。此外，我们引入了CARLA-OOD，这是一种用于OOD分割的新型多模态数据集，包含不同场景和天气条件下合成的OOD对象。在SemanticKITTI、nuScenes、CARLA-OOD数据集和MultiOOD基准上的广泛实验表明，特征混合方法实现了最先进的性能，并且具有10至370倍的速度提升。我们的源代码和数据集将在此网页获得。', 'title_zh': '极简多模态离群点合成用于异常检测和分割'}
{'arxiv_id': 'arXiv:2505.16928', 'title': 'Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning', 'authors': 'Bosung Kim, Prithviraj Ammanabrolu', 'link': 'https://arxiv.org/abs/2505.16928', 'abstract': "We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks that advances long-context understanding in embodied AI. $\\infty$-THOR provides: (1) a generation framework for synthesizing scalable, reproducible, and unlimited long-horizon trajectories; (2) a novel embodied QA task, Needle(s) in the Embodied Haystack, where multiple scattered clues across extended trajectories test agents' long-context reasoning ability; and (3) a long-horizon dataset and benchmark suite featuring complex tasks that span hundreds of environment steps, each paired with ground-truth action sequences. To enable this capability, we explore architectural adaptations, including interleaved Goal-State-Action modeling, context extension techniques, and Context Parallelism, to equip LLM-based agents for extreme long-context reasoning and interaction. Experimental results and analyses highlight the challenges posed by our benchmark and provide insights into training strategies and model behaviors under long-horizon conditions. Our work provides a foundation for the next generation of embodied AI systems capable of robust, long-term reasoning and planning.", 'abstract_zh': '我们介绍了$\\infty$-THOR：一种新的长期视野身躯化任务框架，提升身躯化AI中的长上下文理解能力。$\\infty$-THOR提供了：（1）一种生成框架，用于合成可扩展、可重复且无上限的长期视野轨迹；（2）一种新颖的身躯化问答任务——身躯化的干草堆中的针，多个分散的线索贯穿于长期轨迹中，测试智能体的长上下文推理能力；（3）一个包含复杂任务的长期视野数据集和基准测试套件，这些任务跨越数百个环境步骤，并配以真实动作序列。为了实现这一能力，我们探索了架构上的适应性改进，包括交错的目标-状态-动作建模、上下文扩展技术和上下文并行性，以使基于大规模语言模型的智能体具备极端长上下文推理和交互的能力。实验结果和分析突显了我们基准所面临的挑战，并提供了长视野条件下训练策略和模型行为的见解。我们的工作为下一代能够在长期、稳健推理和规划中表现出色的身躯化AI系统奠定了基础。', 'title_zh': '超越实体干草堆中的针：长上下文推理的环境、建筑与训练考虑因素'}
{'arxiv_id': 'arXiv:2505.16902', 'title': 'RealEngine: Simulating Autonomous Driving in Realistic Context', 'authors': 'Junzhe Jiang, Nan Song, Jingyu Li, Xiatian Zhu, Li Zhang', 'link': 'https://arxiv.org/abs/2505.16902', 'abstract': 'Driving simulation plays a crucial role in developing reliable driving agents by providing controlled, evaluative environments. To enable meaningful assessments, a high-quality driving simulator must satisfy several key requirements: multi-modal sensing capabilities (e.g., camera and LiDAR) with realistic scene rendering to minimize observational discrepancies; closed-loop evaluation to support free-form trajectory behaviors; highly diverse traffic scenarios for thorough evaluation; multi-agent cooperation to capture interaction dynamics; and high computational efficiency to ensure affordability and scalability. However, existing simulators and benchmarks fail to comprehensively meet these fundamental criteria. To bridge this gap, this paper introduces RealEngine, a novel driving simulation framework that holistically integrates 3D scene reconstruction and novel view synthesis techniques to achieve realistic and flexible closed-loop simulation in the driving context. By leveraging real-world multi-modal sensor data, RealEngine reconstructs background scenes and foreground traffic participants separately, allowing for highly diverse and realistic traffic scenarios through flexible scene composition. This synergistic fusion of scene reconstruction and view synthesis enables photorealistic rendering across multiple sensor modalities, ensuring both perceptual fidelity and geometric accuracy. Building upon this environment, RealEngine supports three essential driving simulation categories: non-reactive simulation, safety testing, and multi-agent interaction, collectively forming a reliable and comprehensive benchmark for evaluating the real-world performance of driving agents.', 'abstract_zh': '实时引擎：面向驾驶代理评估的综合三维场景重建与新型视图合成框架', 'title_zh': 'RealEngine: 在现实情境中模拟自动驾驶'}
{'arxiv_id': 'arXiv:2505.16856', 'title': 'Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only', 'authors': 'Wei Xiao, Jiacheng Liu, Zifeng Zhuang, Runze Suo, Shangke Lyu, Donglin Wang', 'link': 'https://arxiv.org/abs/2505.16856', 'abstract': 'Improving the performance of pre-trained policies through online reinforcement learning (RL) is a critical yet challenging topic. Existing online RL fine-tuning methods require continued training with offline pretrained Q-functions for stability and performance. However, these offline pretrained Q-functions commonly underestimate state-action pairs beyond the offline dataset due to the conservatism in most offline RL methods, which hinders further exploration when transitioning from the offline to the online setting. Additionally, this requirement limits their applicability in scenarios where only pre-trained policies are available but pre-trained Q-functions are absent, such as in imitation learning (IL) pre-training. To address these challenges, we propose a method for efficient online RL fine-tuning using solely the offline pre-trained policy, eliminating reliance on pre-trained Q-functions. We introduce PORL (Policy-Only Reinforcement Learning Fine-Tuning), which rapidly initializes the Q-function from scratch during the online phase to avoid detrimental pessimism. Our method not only achieves competitive performance with advanced offline-to-online RL algorithms and online RL approaches that leverage data or policies prior, but also pioneers a new path for directly fine-tuning behavior cloning (BC) policies.', 'abstract_zh': '通过在线强化学习提高预训练策略性能：一种仅基于预训练策略的有效在线微调方法', 'title_zh': '仅使用离线预训练策略的高效在线RL微调'}
{'arxiv_id': 'arXiv:2505.16815', 'title': 'Perceptual Quality Assessment for Embodied AI', 'authors': 'Chunyi Li, Jiaohao Xiao, Jianbo Zhang, Farong Wen, Zicheng Zhang, Yuan Tian, Xiangyang Zhu, Xiaohong Liu, Zhengxue Cheng, Weisi Lin, Guangtao Zhai', 'link': 'https://arxiv.org/abs/2505.16815', 'abstract': 'Embodied AI has developed rapidly in recent years, but it is still mainly deployed in laboratories, with various distortions in the Real-world limiting its application. Traditionally, Image Quality Assessment (IQA) methods are applied to predict human preferences for distorted images; however, there is no IQA method to assess the usability of an image in embodied tasks, namely, the perceptual quality for robots. To provide accurate and reliable quality indicators for future embodied scenarios, we first propose the topic: IQA for Embodied AI. Specifically, we (1) based on the Mertonian system and meta-cognitive theory, constructed a perception-cognition-decision-execution pipeline and defined a comprehensive subjective score collection process; (2) established the Embodied-IQA database, containing over 36k reference/distorted image pairs, with more than 5m fine-grained annotations provided by Vision Language Models/Vision Language Action-models/Real-world robots; (3) trained and validated the performance of mainstream IQA methods on Embodied-IQA, demonstrating the need to develop more accurate quality indicators for Embodied AI. We sincerely hope that through evaluation, we can promote the application of Embodied AI under complex distortions in the Real-world. Project page: this https URL', 'abstract_zh': '自主研发的智能体AI近年来取得了 rapid进展，但主要仍局限于实验室环境，在实际应用场景中的应用受到各种扭曲的限制。传统图像质量评估（IQA）方法用于预测人类对扭曲图像的偏好；然而，缺乏评估图像在自主任务中可用性的方法，即对机器人的知觉质量评估。为提供未来自主场景中的准确可靠质量指标，我们首先提出课题：面向自主的图像质量评估（IQA for Embodied AI）。具体而言，我们(1)基于梅隆系统和元认知理论，构建了感知-认知-决策-执行管道，并定义了综合的主观评分收集过程；(2)建立了包含超过36000个参考/扭曲图像对的自主-IQA数据库，提供了超过500万细粒度的注解，这些注解来自视觉语言模型/视觉语言动作模型/真实世界机器人；(3)对主流的IQA方法在自主-IQA上的性能进行了训练和验证，展示了为自主智能体开发更准确质量指标的必要性。我们诚挚希望通过评估，促进自主研发的智能体在复杂扭曲的实际应用中的应用。项目页面：this https URL。', 'title_zh': '感知质量评估 for 体态人工智能'}
{'arxiv_id': 'arXiv:2505.16278', 'title': 'DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving', 'authors': 'Zhenjie Yang, Yilin Chai, Xiaosong Jia, Qifeng Li, Yuqian Shao, Xuekai Zhu, Haisheng Su, Junchi Yan', 'link': 'https://arxiv.org/abs/2505.16278', 'abstract': 'End-to-end autonomous driving (E2E-AD) demands effective processing of multi-view sensory data and robust handling of diverse and complex driving scenarios, particularly rare maneuvers such as aggressive turns. Recent success of Mixture-of-Experts (MoE) architecture in Large Language Models (LLMs) demonstrates that specialization of parameters enables strong scalability. In this work, we propose DriveMoE, a novel MoE-based E2E-AD framework, with a Scene-Specialized Vision MoE and a Skill-Specialized Action MoE. DriveMoE is built upon our $\\pi_0$ Vision-Language-Action (VLA) baseline (originally from the embodied AI field), called Drive-$\\pi_0$. Specifically, we add Vision MoE to Drive-$\\pi_0$ by training a router to select relevant cameras according to the driving context dynamically. This design mirrors human driving cognition, where drivers selectively attend to crucial visual cues rather than exhaustively processing all visual information. In addition, we add Action MoE by training another router to activate specialized expert modules for different driving behaviors. Through explicit behavioral specialization, DriveMoE is able to handle diverse scenarios without suffering from modes averaging like existing models. In Bench2Drive closed-loop evaluation experiments, DriveMoE achieves state-of-the-art (SOTA) performance, demonstrating the effectiveness of combining vision and action MoE in autonomous driving tasks. We will release our code and models of DriveMoE and Drive-$\\pi_0$.', 'abstract_zh': '基于MoE架构的端到端自主驾驶（DriveMoE）：场景专业化视觉MoE和技能专业化动作MoE融合', 'title_zh': 'DriveMoE：端到端自动驾驶中视觉-语言-动作模型的混合专家机制'}
{'arxiv_id': 'arXiv:2505.16165', 'title': 'RE-TRIP : Reflectivity Instance Augmented Triangle Descriptor for 3D Place Recognition', 'authors': 'Yechan Park, Gyuhyeon Pak, Euntai Kim', 'link': 'https://arxiv.org/abs/2505.16165', 'abstract': 'While most people associate LiDAR primarily with its ability to measure distances and provide geometric information about the environment (via point clouds), LiDAR also captures additional data, including reflectivity or intensity values. Unfortunately, when LiDAR is applied to Place Recognition (PR) in mobile robotics, most previous works on LiDAR-based PR rely only on geometric measurements, neglecting the additional reflectivity information that LiDAR provides. In this paper, we propose a novel descriptor for 3D PR, named RE-TRIP (REflectivity-instance augmented TRIangle descriPtor). This new descriptor leverages both geometric measurements and reflectivity to enhance robustness in challenging scenarios such as geometric degeneracy, high geometric similarity, and the presence of dynamic objects. To implement RE-TRIP in real-world applications, we further propose (1) a keypoint extraction method, (2) a key instance segmentation method, (3) a RE-TRIP matching method, and (4) a reflectivity-combined loop verification method. Finally, we conduct a series of experiments to demonstrate the effectiveness of RE-TRIP. Applied to public datasets (i.e., HELIPR, FusionPortable) containing diverse scenarios such as long corridors, bridges, large-scale urban areas, and highly dynamic environments -- our experimental results show that the proposed method outperforms existing state-of-the-art methods in terms of Scan Context, Intensity Scan Context, and STD.', 'abstract_zh': '一种结合反射率的三维位置识别人工智能增强三角描述子（RE-TRIP）', 'title_zh': 'RE-TRIP : 反射性实例增强三角描述子的三维场所识别'}
{'arxiv_id': 'arXiv:2505.15863', 'title': 'Generative AI for Autonomous Driving: A Review', 'authors': 'Katharina Winter, Abhishek Vivekanandan, Rupert Polley, Yinzhe Shen, Christian Schlauch, Mohamed-Khalil Bouzidi, Bojan Derajic, Natalie Grabowsky, Annajoyce Mariani, Dennis Rochau, Giovanni Lucente, Harsh Yadav, Firas Mualla, Adam Molin, Sebastian Bernhard, Christian Wirth, Ömer Şahin Taş, Nadja Klein, Fabian B. Flohr, Hanno Gottschalk', 'link': 'https://arxiv.org/abs/2505.15863', 'abstract': 'Generative AI (GenAI) is rapidly advancing the field of Autonomous Driving (AD), extending beyond traditional applications in text, image, and video generation. We explore how generative models can enhance automotive tasks, such as static map creation, dynamic scenario generation, trajectory forecasting, and vehicle motion planning. By examining multiple generative approaches ranging from Variational Autoencoder (VAEs) over Generative Adversarial Networks (GANs) and Invertible Neural Networks (INNs) to Generative Transformers (GTs) and Diffusion Models (DMs), we highlight and compare their capabilities and limitations for AD-specific applications. Additionally, we discuss hybrid methods integrating conventional techniques with generative approaches, and emphasize their improved adaptability and robustness. We also identify relevant datasets and outline open research questions to guide future developments in GenAI. Finally, we discuss three core challenges: safety, interpretability, and realtime capabilities, and present recommendations for image generation, dynamic scenario generation, and planning.', 'abstract_zh': '生成式AI（GenAI）迅速推动了自动驾驶（AD）领域的发展，超越了传统文本、图像和视频生成的应用。我们探索生成模型如何增强汽车任务，如静态地图创建、动态场景生成、轨迹预测和车辆运动规划。通过考察从变分自编码器（VAEs）、生成对抗网络（GANs）、可逆神经网络（INNs）到生成转换器（GTs）和扩散模型（DMs）等多种生成方法，我们强调并比较了它们在AD特定应用中的能力和局限性。此外，我们讨论了将传统技术与生成方法相结合的混合方法，并强调了其改进的适应性和鲁棒性。我们还确定了相关数据集，并概述了开放的研究问题，以指导GenAI未来的开发。最后，我们讨论了三个核心挑战：安全性、可解释性和实时能力，并为图像生成、动态场景生成和规划提出了建议。', 'title_zh': '自主驾驶中的生成AI：一个综述'}
