# Analysis of the Unscented Transform for Cooperative Localization with Ranging-Only Information 

**Title (ZH)**: 基于只含有测距信息的合作定位中无中心变换的分析 

**Authors**: Uthman Olawoye, Cagri Kilic, Jason N Gross  

**Link**: [PDF](https://arxiv.org/pdf/2504.07242)  

**Abstract**: Cooperative localization in multi-agent robotic systems is challenging, especially when agents rely on limited information, such as only peer-to-peer range measurements. Two key challenges arise: utilizing this limited information to improve position estimation; handling uncertainties from sensor noise, nonlinearity, and unknown correlations between agents measurements; and avoiding information reuse. This paper examines the use of the Unscented Transform (UT) for state estimation for a case in which range measurement between agents and covariance intersection (CI) is used to handle unknown correlations. Unlike Kalman Filter approaches, CI methods fuse complete state and covariance estimates. This makes formulating a CI approach with ranging-only measurements a challenge. To overcome this, UT is used to handle uncertainties and formulate a cooperative state update using range measurements and current cooperative state estimates. This introduces information reuse in the measurement update. Therefore, this work aims to evaluate the limitations and utility of this formulation when faced with various levels of state measurement uncertainty and errors. 

**Abstract (ZH)**: 多机器人系统中基于有限信息的合作定位具有挑战性：利用有限信息改进位置估计；处理来自传感器噪声、非线性以及未知测量间相关性的不确定性；避免信息重复使用。本文探讨使用无偏变换（UT）进行状态估计，并结合协方差交叠（CI）处理未知相关性。不同于卡尔曼滤波方法，CI方法融合完整的状态和协方差估计。这使得使用仅基于距离的测量来制定CI方法成为一个挑战。为克服这一问题，本文使用UT处理不确定性，并结合距离测量和当前合作状态估计来制定合作状态更新，从而在测量更新中引入信息重复使用。因此，本文旨在评估在不同状态测量不确定性水平和误差下的这种形式的局限性和实用性。 

---
# Personalized and Demand-Based Education Concept: Practical Tools for Control Engineers 

**Title (ZH)**: 个性化和需求导向的教育理念：控制工程师的实践工具 

**Authors**: Balint Varga, Lars Fischer, Levente Kovacs  

**Link**: [PDF](https://arxiv.org/pdf/2504.07466)  

**Abstract**: This paper presents a personalized lecture concept using educational blocks and its demonstrative application in a new university lecture. Higher education faces daily challenges: deep and specialized knowledge is available from everywhere and accessible to almost everyone. University lecturers of specialized master courses confront the problem that their lectures are either too boring or too complex for the attending students. Additionally, curricula are changing more rapidly than they have in the past 10-30 years. The German education system comprises different educational forms, with universities providing less practical content. Consequently, many university students do not obtain the practical skills they should ideally gain through university lectures. Therefore, in this work, a new lecture concept is proposed based on the extension of the just-in-time teaching paradigm: Personalized and Demand-Based Education. This concept includes: 1) an initial assessment of students' backgrounds, 2) selecting the appropriate educational blocks, and 3) collecting ongoing feedback during the semester. The feedback was gathered via Pingo, ensuring anonymity for the students. Our concept was exemplarily tested in the new lecture "Practical Tools for Control Engineers" at the Karlsruhe Institute of Technology. The initial results indicate that our proposed concept could be beneficial in addressing the current challenges in higher education. 

**Abstract (ZH)**: 基于教育模块的个性化讲授概念及其在新大学课程中的示范应用：面向控制工程师的实践工具讲授概念 

---
# Multi-Object Tracking for Collision Avoidance Using Multiple Cameras in Open RAN Networks 

**Title (ZH)**: 使用开放无线接入网络中多摄像头进行多目标跟踪以避免碰撞 

**Authors**: Jordi Serra, Anton Aguilar, Ebrahim Abu-Helalah, Raúl Parada, Paolo Dini  

**Link**: [PDF](https://arxiv.org/pdf/2504.07163)  

**Abstract**: This paper deals with the multi-object detection and tracking problem, within the scope of open Radio Access Network (RAN), for collision avoidance in vehicular scenarios. To this end, a set of distributed intelligent agents collocated with cameras are considered. The fusion of detected objects is done at an edge service, considering Open RAN connectivity. Then, the edge service predicts the objects trajectories for collision avoidance. Compared to the related work a more realistic Open RAN network is implemented and multiple cameras are used. 

**Abstract (ZH)**: 基于开放无线接入网络的车载场景中多目标检测与跟踪及其碰撞避免方法 

---
# The Urban Impact of AI: Modeling Feedback Loops in Next-Venue Recommendation 

**Title (ZH)**: AI对城市的影响：下次场地推荐中的反馈循环建模 

**Authors**: Giovanni Mauro, Marco Minici, Luca Pappalardo  

**Link**: [PDF](https://arxiv.org/pdf/2504.07911)  

**Abstract**: Next-venue recommender systems are increasingly embedded in location-based services, shaping individual mobility decisions in urban environments. While their predictive accuracy has been extensively studied, less attention has been paid to their systemic impact on urban dynamics. In this work, we introduce a simulation framework to model the human-AI feedback loop underpinning next-venue recommendation, capturing how algorithmic suggestions influence individual behavior, which in turn reshapes the data used to retrain the models. Our simulations, grounded in real-world mobility data, systematically explore the effects of algorithmic adoption across a range of recommendation strategies. We find that while recommender systems consistently increase individual-level diversity in visited venues, they may simultaneously amplify collective inequality by concentrating visits on a limited subset of popular places. This divergence extends to the structure of social co-location networks, revealing broader implications for urban accessibility and spatial segregation. Our framework operationalizes the feedback loop in next-venue recommendation and offers a novel lens through which to assess the societal impact of AI-assisted mobility-providing a computational tool to anticipate future risks, evaluate regulatory interventions, and inform the design of ethic algorithmic systems. 

**Abstract (ZH)**: 基于地点的服务中，下一步场所推荐系统日益嵌入其中，影响城市环境中个体的移动决策。虽然对其预测准确性已有广泛研究，但对其对城市动态的系统性影响关注较少。在本文中，我们引入了一个仿真框架来模拟驱动下一场所推荐的人机反馈循环，捕捉算法建议如何影响个体行为，进而重塑用于重新训练模型的数据。我们的仿真基于实际的移动数据，系统地探讨了算法采用在多种推荐策略下的影响。我们发现，虽然推荐系统一贯增加了个体访问场所的多样性，但它们可能会同时通过集中访问于少数热门地点来加剧集体不平等。这种差异还扩展到社会共存网络的结构上，揭示了对城市可达性和空间隔离的更广泛影响。我们的框架操作化了下一场所推荐中的人机反馈循环，并提供了一种新的视角，用于评估人工智能辅助移动的社会影响——提供了一个计算工具，以预见未来风险、评估监管干预措施，并指导伦理算法系统的开发设计。 

---
# Dual Engines of Thoughts: A Depth-Breadth Integration Framework for Open-Ended Analysis 

**Title (ZH)**: 双重思考引擎：一种深度与广度集成框架用于开放式分析 

**Authors**: Fei-Hsuan Yu, Yun-Cheng Chou, Teng-Ruei Chen  

**Link**: [PDF](https://arxiv.org/pdf/2504.07872)  

**Abstract**: We propose the Dual Engines of Thoughts (DEoT), an analytical framework for comprehensive open-ended reasoning. While traditional reasoning frameworks primarily focus on finding "the best answer" or "the correct answer" for single-answer problems, DEoT is specifically designed for "open-ended questions," enabling both broader and deeper analytical exploration. The framework centers on three key components: a Base Prompter for refining user queries, a Solver Agent that orchestrates task decomposition, execution, and validation, and a Dual-Engine System consisting of a Breadth Engine (to explore diverse impact factors) and a Depth Engine (to perform deep investigations). This integrated design allows DEoT to balance wide-ranging coverage with in-depth analysis, and it is highly customizable, enabling users to adjust analytical parameters and tool configurations based on specific requirements. Experimental results show that DEoT excels in addressing complex, multi-faceted questions, achieving a total win rate of 77-86% compared to existing reasoning models, thus highlighting its effectiveness in real-world applications. 

**Abstract (ZH)**: 我们提出了双引擎思维框架（DEoT），这是一种综合性的开放性推理分析框架。 

---
# Independence Is Not an Issue in Neurosymbolic AI 

**Title (ZH)**: 独立性不是神经符号AI的问题 

**Authors**: Håkan Karlsson Faronius, Pedro Zuidberg Dos Martires  

**Link**: [PDF](https://arxiv.org/pdf/2504.07851)  

**Abstract**: A popular approach to neurosymbolic AI is to take the output of the last layer of a neural network, e.g. a softmax activation, and pass it through a sparse computation graph encoding certain logical constraints one wishes to enforce. This induces a probability distribution over a set of random variables, which happen to be conditionally independent of each other in many commonly used neurosymbolic AI models. Such conditionally independent random variables have been deemed harmful as their presence has been observed to co-occur with a phenomenon dubbed deterministic bias, where systems learn to deterministically prefer one of the valid solutions from the solution space over the others. We provide evidence contesting this conclusion and show that the phenomenon of deterministic bias is an artifact of improperly applying neurosymbolic AI. 

**Abstract (ZH)**: 一种流行的神经符号AI方法是将神经网络最后一层的输出，例如softmax激活，通过一个稀疏计算图传递，该图编码了希望施加的某些逻辑约束。这会在一些常用的大规模神经符号AI模型中诱导出一个随机变量的概率分布，这些随机变量在很多情况下是条件独立的。在许多情况下，这些条件独立的随机变量被认为是有害的，因为它们的存在与一种称为确定性偏见的现象相伴发生，即系统倾向于学习从解决方案空间中确定性地偏好其中一个有效解。我们提供了反驳这一结论的证据，并表明确定性偏见现象是不恰当地应用神经符号AI的结果。 

---
# Anytime Single-Step MAPF Planning with Anytime PIBT 

**Title (ZH)**: 任意时间单步MAPF规划与任意时间PIBT 

**Authors**: Nayesha Gandotra, Rishi Veerapaneni, Muhammad Suhail Saleem, Daniel Harabor, Jiaoyang Li, Maxim Likhachev  

**Link**: [PDF](https://arxiv.org/pdf/2504.07841)  

**Abstract**: PIBT is a popular Multi-Agent Path Finding (MAPF) method at the core of many state-of-the-art MAPF methods including LaCAM, CS-PIBT, and WPPL. The main utility of PIBT is that it is a very fast and effective single-step MAPF solver and can return a collision-free single-step solution for hundreds of agents in less than a millisecond. However, the main drawback of PIBT is that it is extremely greedy in respect to its priorities and thus leads to poor solution quality. Additionally, PIBT cannot use all the planning time that might be available to it and returns the first solution it finds. We thus develop Anytime PIBT, which quickly finds a one-step solution identically to PIBT but then continuously improves the solution in an anytime manner. We prove that Anytime PIBT converges to the optimal solution given sufficient time. We experimentally validate that Anytime PIBT can rapidly improve single-step solution quality within milliseconds and even find the optimal single-step action. However, we interestingly find that improving the single-step solution quality does not have a significant effect on full-horizon solution costs. 

**Abstract (ZH)**: Anytime PIBT: 快速并持续优化的多智能体路径规划方法 

---
# Genetic Programming with Reinforcement Learning Trained Transformer for Real-World Dynamic Scheduling Problems 

**Title (ZH)**: 基于强化学习训练的变换器的遗传编程在实际动态调度问题中的应用 

**Authors**: Xian Chen, Rong Qu, Jing Dong, Ruibin Bai, Yaochu Jin  

**Link**: [PDF](https://arxiv.org/pdf/2504.07779)  

**Abstract**: Dynamic scheduling in real-world environments often struggles to adapt to unforeseen disruptions, making traditional static scheduling methods and human-designed heuristics inadequate. This paper introduces an innovative approach that combines Genetic Programming (GP) with a Transformer trained through Reinforcement Learning (GPRT), specifically designed to tackle the complexities of dynamic scheduling scenarios. GPRT leverages the Transformer to refine heuristics generated by GP while also seeding and guiding the evolution of GP. This dual functionality enhances the adaptability and effectiveness of the scheduling heuristics, enabling them to better respond to the dynamic nature of real-world tasks. The efficacy of this integrated approach is demonstrated through a practical application in container terminal truck scheduling, where the GPRT method outperforms traditional GP, standalone Transformer methods, and other state-of-the-art competitors. The key contribution of this research is the development of the GPRT method, which showcases a novel combination of GP and Reinforcement Learning (RL) to produce robust and efficient scheduling solutions. Importantly, GPRT is not limited to container port truck scheduling; it offers a versatile framework applicable to various dynamic scheduling challenges. Its practicality, coupled with its interpretability and ease of modification, makes it a valuable tool for diverse real-world scenarios. 

**Abstract (ZH)**: 基于Genetic Programming与Transformer结合的强化学习动态调度方法（GPRT）：一种应对动态调度挑战的新范式 

---
# Search-contempt: a hybrid MCTS algorithm for training AlphaZero-like engines with better computational efficiency 

**Title (ZH)**: 搜索轻视：一种计算效率更高的AlphaZero-like引擎的混合MCTS算法 

**Authors**: Ameya Joshi  

**Link**: [PDF](https://arxiv.org/pdf/2504.07757)  

**Abstract**: AlphaZero in 2017 was able to master chess and other games without human knowledge by playing millions of games against itself (self-play), with a computation budget running in the tens of millions of dollars. It used a variant of the Monte Carlo Tree Search (MCTS) algorithm, known as PUCT. This paper introduces search-contempt, a novel hybrid variant of the MCTS algorithm that fundamentally alters the distribution of positions generated in self-play, preferring more challenging positions. In addition, search-contempt has been shown to give a big boost in strength for engines in Odds Chess (where one side receives an unfavorable position from the start). More significantly, it opens up the possibility of training a self-play based engine, in a much more computationally efficient manner with the number of training games running into hundreds of thousands, costing tens of thousands of dollars (instead of tens of millions of training games costing millions of dollars required by AlphaZero). This means that it may finally be possible to train such a program from zero on a standard consumer GPU even with a very limited compute, cost, or time budget. 

**Abstract (ZH)**: AlphaZero在2017年通过自我对弈掌握国际象棋和其他游戏，无需人类知识，并使用了变种的蒙特卡洛树搜索（MCTS）算法PUCT。本文介绍了搜索轻视（search-contempt），这是一种新型混合变种的MCTS算法，从根本上改变了自我对弈生成的位置分布，更倾向于更具挑战性的位置。此外，搜索轻视已被证明能大幅提升非公平起手优势国际象棋引擎的实力。更重要的是，它为以更高效的方式训练基于自我对弈的引擎提供了可能，训练游戏数量可达到数十万，成本为数万而非数百万美元，从而使在标准消费级GPU上以极为有限的计算、成本和时间预算从零开始训练此类程序成为可能。 

---
# "i am a stochastic parrot, and so r u": Is AI-based framing of human behaviour and cognition a conceptual metaphor or conceptual engineering? 

**Title (ZH)**: “我是一只随机鹦鹉，你也是”: 基于AI的人类行为和认知框架是概念隐喻还是概念工程？ 

**Authors**: Warmhold Jan Thomas Mollema, Thomas Wachter  

**Link**: [PDF](https://arxiv.org/pdf/2504.07756)  

**Abstract**: Given the massive integration of AI technologies into our daily lives, AI-related concepts are being used to metaphorically compare AI systems with human behaviour and/or cognitive abilities like language acquisition. Rightfully, the epistemic success of these metaphorical comparisons should be debated. Against the backdrop of the conflicting positions of the 'computational' and 'meat' chauvinisms, we ask: can the conceptual constellation of the computational and AI be applied to the human domain and what does it mean to do so? What is one doing when the conceptual constellations of AI in particular are used in this fashion? Rooted in a Wittgensteinian view of concepts and language-use, we consider two possible answers and pit them against each other: either these examples are conceptual metaphors, or they are attempts at conceptual engineering. We argue that they are conceptual metaphors, but that (1) this position is unaware of its own epistemological contingency, and (2) it risks committing the ''map-territory fallacy''. Down at the conceptual foundations of computation, (3) it most importantly is a misleading 'double metaphor' because of the metaphorical connection between human psychology and computation. In response to the shortcomings of this projected conceptual organisation of AI onto the human domain, we argue that there is a semantic catch. The perspective of the conceptual metaphors shows avenues for forms of conceptual engineering. If this methodology's criteria are met, the fallacies and epistemic shortcomings related to the conceptual metaphor view can be bypassed. At its best, the cross-pollution of the human and AI conceptual domains is one that prompts us to reflect anew on how the boundaries of our current concepts serve us and how they could be approved. 

**Abstract (ZH)**: 随着人工智能技术大规模融入我们的日常生活，AI相关概念被用来比喻性地将AI系统与人类行为及认知能力（如语言习得）相比较。在“计算主义”与“肉身主义”这两种观点相互冲突的背景下，我们提出以下问题：计算与AI的概念框架能否应用于人类领域？这样做的意义是什么？当以这种方式使用AI特定领域的概念框架时，人们在做些什么？立足于维特根斯坦的概念与语言使用观点，我们考虑了两种可能的答案，并将它们相对比：要么这些例子是概念隐喻，要么是概念工程的尝试。我们主张它们是概念隐喻，但（1）这个立场没有意识到自身在认识论上的依赖性，（2）它有可能犯“地图与地形谬误”。在计算概念的基础层面，（3）最重要的是，它是一个误导性的“双重隐喻”，因为存在人类心理与计算之间的比喻性关联。针对将AI概念组织构架投射到人类领域的不足之处，我们指出存在一种语义上的陷阱。概念隐喻的视角揭示了概念工程可能的途径。如果这种方法满足了某些标准，概念隐喻视角相关的问题与认识论不足就可以被绕过。最终，人类与AI概念领域的相互渗透促使我们重新思考当前概念边界如何为我们的认知服务，以及它们如何能够改进。 

---
# Generative Artificial Intelligence for Internet of Things Computing: A Systematic Survey 

**Title (ZH)**: 生成式人工智能在物联网计算中的应用：一项系统性回顾 

**Authors**: Fabrizio Mangione, Claudio Savaglio, Giancarlo Fortino  

**Link**: [PDF](https://arxiv.org/pdf/2504.07635)  

**Abstract**: The integration of Generative Artificial Intelligence (GenAI) within the Internet of Things (IoT) is garnering considerable interest. This growing attention stems from the continuous evolution and widespread adoption they are both having individually, enough to spontaneously reshape numerous sectors, including Healthcare, Manufacturing, and Smart Cities. Hence, their increasing popularity has catalyzed further extensive research for understanding the potential of the duo GenAI-IoT, how they interplay, and to which extent their synergy can innovate the state-of-the-art in their individual scenarios. However, despite the increasing prominence of GenAI for IoT Computing, much of the existing research remains focused on specific, narrowly scoped applications. This fragmented approach highlights the need for a more comprehensive analysis of the potential, challenges, and implications of GenAI integration within the broader IoT ecosystem. This survey exactly aims to address this gap by providing a holistic overview of the opportunities, issues, and considerations arising from the convergence of these mainstream paradigms. Our contribution is realized through a systematic literature review following the PRISMA methodology. A comparison framework is presented, and well-defined research questions are outlined to comprehensively explore the past, present, and future directions of GenAI integration with IoT Computing, offering valuable insights for both experts and newcomers. 

**Abstract (ZH)**: Generative Artificial Intelligence与物联网的整合：机遇、挑战与综合分析 

---
# Beating Transformers using Synthetic Cognition 

**Title (ZH)**: 超越变压器的合成认知 

**Authors**: Alfredo Ibias, Miguel Rodriguez-Galindo, Hector Antona, Guillem Ramirez-Miranda, Enric Guinovart  

**Link**: [PDF](https://arxiv.org/pdf/2504.07619)  

**Abstract**: The road to Artificial General Intelligence goes through the generation of episodic reactive behaviors, where the Transformer architecture has been proven to be the state-of-the-art. However, they still fail to develop reasoning. Recently, a novel approach for developing cognitive architectures, called Synthetic Cognition, has been proposed and implemented to develop instantaneous reactive behavior. In this study, we aim to explore the use of Synthetic Cognition to develop episodic reactive behaviors. We propose a mechanism to deal with sequences for the recent implementation of Synthetic Cognition, and test it against DNA foundation models in DNA sequence classification tasks. In our experiments, our proposal clearly outperforms the DNA foundation models, obtaining the best score on more benchmark tasks than the alternatives. Thus, we achieve two goals: expanding Synthetic Cognition to deal with sequences, and beating the Transformer architecture for sequence classification. 

**Abstract (ZH)**: 人工通用智能的道路经过情景反应行为的生成，其中Transformer架构已被证明是最先进的。然而，它们still未能发展出推理能力。最近，提出了一种开发认知架构的新型方法，称为Synthetic Cognition，并实施以开发即时反应行为。在这项研究中，我们旨在探索使用Synthetic Cognition开发情景反应行为的用途。我们提出了处理序列的一种机制，对该Recent实施的Synthetic Cognition进行测试，以DNA基础模型在DNA序列分类任务中与DNA基础模型进行对比。在我们的实验中，我们的提议在多项基准任务上明显优于DNA基础模型，获得了更高的评分。因此，我们达成了两个目标：扩展Synthetic Cognition以处理序列，并在序列分类中击败Transformer架构。 

---
# Bottleneck Identification in Resource-Constrained Project Scheduling via Constraint Relaxation 

**Title (ZH)**: 资源约束项目调度中的瓶颈识别通过约束松弛 

**Authors**: Lukáš Nedbálek, Antonín Novák  

**Link**: [PDF](https://arxiv.org/pdf/2504.07495)  

**Abstract**: In realistic production scenarios, Advanced Planning and Scheduling (APS) tools often require manual intervention by production planners, as the system works with incomplete information, resulting in suboptimal schedules. Often, the preferable solution is not found just because of the too-restrictive constraints specifying the optimization problem, representing bottlenecks in the schedule. To provide computer-assisted support for decision-making, we aim to automatically identify bottlenecks in the given schedule while linking them to the particular constraints to be relaxed. In this work, we address the problem of reducing the tardiness of a particular project in an obtained schedule in the resource-constrained project scheduling problem by relaxing constraints related to identified bottlenecks. We develop two methods for this purpose. The first method adapts existing approaches from the job shop literature and utilizes them for so-called untargeted relaxations. The second method identifies potential improvements in relaxed versions of the problem and proposes targeted relaxations. Surprisingly, the untargeted relaxations result in improvements comparable to the targeted relaxations. 

**Abstract (ZH)**: 资源约束项目调度中特定项目 tardiness 减少问题的自动瓶颈识别与放松方法研究 

---
# Routing to the Right Expertise: A Trustworthy Judge for Instruction-based Image Editing 

**Title (ZH)**: 基于指令的图像编辑中的可信赖法官：导向合适专长的路由机制 

**Authors**: Chenxi Sun, Hongzhi Zhang, Qi Wang, Fuzheng Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2504.07424)  

**Abstract**: Instruction-based Image Editing (IIE) models have made significantly improvement due to the progress of multimodal large language models (MLLMs) and diffusion models, which can understand and reason about complex editing instructions. In addition to advancing current IIE models, accurately evaluating their output has become increasingly critical and challenging. Current IIE evaluation methods and their evaluation procedures often fall short of aligning with human judgment and often lack explainability. To address these limitations, we propose JUdgement through Routing of Expertise (JURE). Each expert in JURE is a pre-selected model assumed to be equipped with an atomic expertise that can provide useful feedback to judge output, and the router dynamically routes the evaluation task of a given instruction and its output to appropriate experts, aggregating their feedback into a final judge. JURE is trustworthy in two aspects. First, it can effortlessly provide explanations about its judge by examining the routed experts and their feedback. Second, experimental results demonstrate that JURE is reliable by achieving superior alignment with human judgments, setting a new standard for automated IIE evaluation. Moreover, JURE's flexible design is future-proof - modular experts can be seamlessly replaced or expanded to accommodate advancements in IIE, maintaining consistently high evaluation quality. Our evaluation data and results are available at this https URL. 

**Abstract (ZH)**: 基于指令的图像编辑（IIE）模型由于多模态大型语言模型（MLLMs）和扩散模型的进步而取得了显著改进，能够理解和推理复杂的编辑指令。除了推动现有IIE模型的发展，准确评估其输出变得越来越关键和具有挑战性。当前的IIE评估方法及其评估程序往往未能与人类判断对齐，并且通常缺乏解释性。为了解决这些限制，我们提出了一种Judgement through Routing of Expertise（JURE）。在JURE中，每个专家都是预先选定的模型，并假设该模型具备原子级别的专业知识，可以为评估输出提供有用的反馈，路由模块动态地将给定指令及其输出的评估任务分配给合适的专家，并将他们的反馈汇总为最终判断。JURE在两个方面是值得信赖的。首先，通过检查分配的专家及其反馈，可以轻松地提供关于其判断的解释。其次，实验结果表明，JURE是可靠的，能够实现与人类判断的高度对齐，并且在自动化IIE评估中设立了新的标准。此外，JURE的设计具有灵活性，使其能够应对IIE领域的进步，保持始终如一的高评估质量。我们的评估数据和结果可以在以下链接访问：this https URL。 

---
# A new training approach for text classification in Mental Health: LatentGLoss 

**Title (ZH)**: 心理健康领域文本分类的新训练方法：LatentGLoss 

**Authors**: Korhan Sevinç  

**Link**: [PDF](https://arxiv.org/pdf/2504.07245)  

**Abstract**: This study presents a multi-stage approach to mental health classification by leveraging traditional machine learning algorithms, deep learning architectures, and transformer-based models. A novel data set was curated and utilized to evaluate the performance of various methods, starting with conventional classifiers and advancing through neural networks. To broaden the architectural scope, recurrent neural networks (RNNs) such as LSTM and GRU were also evaluated to explore their effectiveness in modeling sequential patterns in the data. Subsequently, transformer models such as BERT were fine-tuned to assess the impact of contextual embeddings in this domain. Beyond these baseline evaluations, the core contribution of this study lies in a novel training strategy involving a dual-model architecture composed of a teacher and a student network. Unlike standard distillation techniques, this method does not rely on soft label transfer; instead, it facilitates information flow through both the teacher model's output and its latent representations by modifying the loss function. The experimental results highlight the effectiveness of each modeling stage and demonstrate that the proposed loss function and teacher-student interaction significantly enhance the model's learning capacity in mental health prediction tasks. 

**Abstract (ZH)**: 本研究提出了一种多阶段方法，通过利用传统机器学习算法、深度学习架构和变压器模型对心理健康进行分类。利用一个新型数据集评估了各种方法的表现，从传统的分类器开始，逐步过渡到神经网络。为了扩大架构范围，还评估了循环神经网络（如LSTM和GRU），探索其在数据中建模序列模式的有效性。随后，对BERT等变换器模型进行了微调，以评估上下文嵌入在该领域的影响力。除了这些基准评估之外，本研究的核心贡献在于一种新颖的训练策略，该策略涉及由教师网络和学生网络组成的双模型架构。不同于标准的知识蒸馏技术，该方法不依赖于软标签转移，而是通过修改损失函数使信息流经教师模型的输出及其潜在表示来促进信息流动。实验结果突显了每种建模阶段的有效性，并证明了所提出的损失函数和教师-学生交互显著增强了模型在心理健康预测任务中的学习能力。 

---
# Artificial Intelligence Index Report 2025 

**Title (ZH)**: 2025年人工智能指数报告 

**Authors**: Nestor Maslej, Loredana Fattorini, Raymond Perrault, Yolanda Gil, Vanessa Parli, Njenga Kariuki, Emily Capstick, Anka Reuel, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, Juan Carlos Niebles, Yoav Shoham, Russell Wald, Tobi Walsh, Armin Hamrah, Lapo Santarlasci, Julia Betts Lotufo, Alexandra Rome, Andrew Shi, Sukrut Oak  

**Link**: [PDF](https://arxiv.org/pdf/2504.07139)  

**Abstract**: Welcome to the eighth edition of the AI Index report. The 2025 Index is our most comprehensive to date and arrives at an important moment, as AI's influence across society, the economy, and global governance continues to intensify. New in this year's report are in-depth analyses of the evolving landscape of AI hardware, novel estimates of inference costs, and new analyses of AI publication and patenting trends. We also introduce fresh data on corporate adoption of responsible AI practices, along with expanded coverage of AI's growing role in science and medicine. Since its founding in 2017 as an offshoot of the One Hundred Year Study of Artificial Intelligence, the AI Index has been committed to equipping policymakers, journalists, executives, researchers, and the public with accurate, rigorously validated, and globally sourced data. Our mission has always been to help these stakeholders make better-informed decisions about the development and deployment of AI. In a world where AI is discussed everywhere - from boardrooms to kitchen tables - this mission has never been more essential. The AI Index continues to lead in tracking and interpreting the most critical trends shaping the field - from the shifting geopolitical landscape and the rapid evolution of underlying technologies, to AI's expanding role in business, policymaking, and public life. Longitudinal tracking remains at the heart of our mission. In a domain advancing at breakneck speed, the Index provides essential context - helping us understand where AI stands today, how it got here, and where it may be headed next. Recognized globally as one of the most authoritative resources on artificial intelligence, the AI Index has been cited in major media outlets such as The New York Times, Bloomberg, and The Guardian; referenced in hundreds of academic papers; and used by policymakers and government agencies around the world. 

**Abstract (ZH)**: 第八届AI指数报告欢迎词：2025年版指数是迄今为止最为全面的一版，正值AI对社会、经济和全球治理影响不断加深的重要时刻。今年的报告新增了AI硬件演变景观的深度分析、推理成本的新颖估算，以及AI出版和专利趋势的新分析。我们还引入了有关企业负责任AI实践采用情况的新数据，并扩大了对AI在科学和医学领域不断增长作用的覆盖。自2017年作为“百年人工智能研究”的衍生项目成立以來，AI指数致力于为决策者、记者、高管、研究人员和公众提供准确、严格验证和来自全球的数据。我们的使命始终是帮助这些利益相关者就AI的发展和应用做出更明智的决策。在全球各地从会议室到餐桌都在讨论AI的世界里，这一使命比以往任何时候都更加重要。AI指数继续在追踪和解读塑造该领域的最关键的动向方面处于领先地位——从地缘政治格局的变化和底层技术的快速演进，到AI在商业、政策制定和公共生活中的不断扩展角色。纵向追踪仍是我们的核心使命。在一项以令人难以置信的速度发展的领域中，指数为我们提供了必要的背景信息——帮助我们理解AI今天所处的位置、它是如何走到这里以及未来可能走向何方。作为全球最受认可的人工智能权威资源之一，AI指数被《纽约时报》、彭博社和《卫报》等主流媒体引用；在数百篇学术论文中被引用；并被世界各地的政策制定者和政府机构使用。 

---
# Embedding Reliability Verification Constraints into Generation Expansion Planning 

**Title (ZH)**: 将可靠性验证约束嵌入到扩建规划中 

**Authors**: Peng Liu, Lian Cheng, Benjamin P.Omell, Anthony P.Burgard  

**Link**: [PDF](https://arxiv.org/pdf/2504.07131)  

**Abstract**: Generation planning approaches face challenges in managing the incompatible mathematical structures between stochastic production simulations for reliability assessment and optimization models for generation planning, which hinders the integration of reliability constraints. This study proposes an approach to embedding reliability verification constraints into generation expansion planning by leveraging a weighted oblique decision tree (WODT) technique. For each planning year, a generation mix dataset, labeled with reliability assessment simulations, is generated. An WODT model is trained using this dataset. Reliability-feasible regions are extracted via depth-first search technique and formulated as disjunctive constraints. These constraints are then transformed into mixed-integer linear form using a convex hull modeling technique and embedded into a unit commitment-integrated generation expansion planning model. The proposed approach is validated through a long-term generation planning case study for the Electric Reliability Council of Texas (ERCOT) region, demonstrating its effectiveness in achieving reliable and optimal planning solutions. 

**Abstract (ZH)**: 基于加权斜决断树的可靠性验证约束嵌入发电扩展规划方法 

---
# Note on the identification of total effect in Cluster-DAGs with cycles 

**Title (ZH)**: Cluster-DAGs中循环识别总体效应问题研究 

**Authors**: Clément Yvernes  

**Link**: [PDF](https://arxiv.org/pdf/2504.07921)  

**Abstract**: In this note, we discuss the identifiability of a total effect in cluster-DAGs, allowing for cycles within the cluster-DAG (while still assuming the associated underlying DAG to be acyclic). This is presented into two key results: first, restricting the cluster-DAG to clusters containing at most four nodes; second, adapting the notion of d-separation. We provide a graphical criterion to address the identifiability problem. 

**Abstract (ZH)**: 本文讨论了在允许群组有向混合图中存在循环的情况下群组有向混合图中总体效应的可识别性，并提出了两个关键结果：首先，将群组有向混合图限制为最多包含四个节点的群组；其次，调整d-分离的概念。我们提供了一种图形判据来解决可识别性问题。 

---
# SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning 

**Title (ZH)**: SpecReason: 快速准确的推理时计算通过推测推理 

**Authors**: Rui Pan, Yinwei Dai, Zhihao Zhang, Gabriele Oliaro, Zhihao Jia, Ravi Netravali  

**Link**: [PDF](https://arxiv.org/pdf/2504.07891)  

**Abstract**: Recent advances in inference-time compute have significantly improved performance on complex tasks by generating long chains of thought (CoTs) using Large Reasoning Models (LRMs). However, this improved accuracy comes at the cost of high inference latency due to the length of generated reasoning sequences and the autoregressive nature of decoding. Our key insight in tackling these overheads is that LRM inference, and the reasoning that it embeds, is highly tolerant of approximations: complex tasks are typically broken down into simpler steps, each of which brings utility based on the semantic insight it provides for downstream steps rather than the exact tokens it generates. Accordingly, we introduce SpecReason, a system that automatically accelerates LRM inference by using a lightweight model to (speculatively) carry out simpler intermediate reasoning steps and reserving the costly base model only to assess (and potentially correct) the speculated outputs. Importantly, SpecReason's focus on exploiting the semantic flexibility of thinking tokens in preserving final-answer accuracy is complementary to prior speculation techniques, most notably speculative decoding, which demands token-level equivalence at each step. Across a variety of reasoning benchmarks, SpecReason achieves 1.5-2.5$\times$ speedup over vanilla LRM inference while improving accuracy by 1.0-9.9\%. Compared to speculative decoding without SpecReason, their combination yields an additional 19.4-44.2\% latency reduction. We open-source SpecReason at this https URL. 

**Abstract (ZH)**: Recent Advances in Inference-Time Compute: Accelerating Large Reasoning Model Inference with SpecReason 

---
# Empowering Global Voices: A Data-Efficient, Phoneme-Tone Adaptive Approach to High-Fidelity Speech Synthesis 

**Title (ZH)**: 赋能全球之声：一种高效的数据、音素-音调自适应的高保真语音合成方法 

**Authors**: Yizhong Geng, Jizhuo Xu, Zeyu Liang, Jinghan Yang, Xiaoyi Shi, Xiaoyu Shen  

**Link**: [PDF](https://arxiv.org/pdf/2504.07858)  

**Abstract**: Text-to-speech (TTS) technology has achieved impressive results for widely spoken languages, yet many under-resourced languages remain challenged by limited data and linguistic complexities. In this paper, we present a novel methodology that integrates a data-optimized framework with an advanced acoustic model to build high-quality TTS systems for low-resource scenarios. We demonstrate the effectiveness of our approach using Thai as an illustrative case, where intricate phonetic rules and sparse resources are effectively addressed. Our method enables zero-shot voice cloning and improved performance across diverse client applications, ranging from finance to healthcare, education, and law. Extensive evaluations - both subjective and objective - confirm that our model meets state-of-the-art standards, offering a scalable solution for TTS production in data-limited settings, with significant implications for broader industry adoption and multilingual accessibility. 

**Abstract (ZH)**: 面向低资源语言的文本-to-语音技术：一种数据优化框架与高级声学模型的集成方法 

---
# Deep Learning-based Intrusion Detection Systems: A Survey 

**Title (ZH)**: 基于深度学习的入侵检测系统：一个综述 

**Authors**: Zhiwei Xu, Yujuan Wu, Shiheng Wang, Jiabao Gao, Tian Qiu, Ziqi Wang, Hai Wan, Xibin Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2504.07839)  

**Abstract**: Intrusion Detection Systems (IDS) have long been a hot topic in the cybersecurity community. In recent years, with the introduction of deep learning (DL) techniques, IDS have made great progress due to their increasing generalizability. The rationale behind this is that by learning the underlying patterns of known system behaviors, IDS detection can be generalized to intrusions that exploit zero-day vulnerabilities. In this survey, we refer to this type of IDS as DL-based IDS (DL-IDS). From the perspective of DL, this survey systematically reviews all the stages of DL-IDS, including data collection, log storage, log parsing, graph summarization, attack detection, and attack investigation. To accommodate current researchers, a section describing the publicly available benchmark datasets is included. This survey further discusses current challenges and potential future research directions, aiming to help researchers understand the basic ideas and visions of DL-IDS research, as well as to motivate their research interests. 

**Abstract (ZH)**: 深度学习（DL）驱动的入侵检测系统（DL-IDS）综述 

---
# DG-STMTL: A Novel Graph Convolutional Network for Multi-Task Spatio-Temporal Traffic Forecasting 

**Title (ZH)**: DG-STMTL：一种新型图卷积网络用于多任务空间-时间交通预测 

**Authors**: Wanna Cui, Peizheng Wang, Faliang Yin  

**Link**: [PDF](https://arxiv.org/pdf/2504.07822)  

**Abstract**: Spatio-temporal traffic prediction is crucial in intelligent transportation systems. The key challenge of accurate prediction is how to model the complex spatio-temporal dependencies and adapt to the inherent dynamics in data. Traditional Graph Convolutional Networks (GCNs) often struggle with static adjacency matrices that introduce domain bias or learnable matrices that may be overfitting to specific patterns. This challenge becomes more complex when considering Multi-Task Learning (MTL). While MTL has the potential to enhance prediction accuracy through task synergies, it can also face significant hurdles due to task interference. To overcome these challenges, this study introduces a novel MTL framework, Dynamic Group-wise Spatio-Temporal Multi-Task Learning (DG-STMTL). DG-STMTL proposes a hybrid adjacency matrix generation module that combines static matrices with dynamic ones through a task-specific gating mechanism. We also introduce a group-wise GCN module to enhance the modelling capability of spatio-temporal dependencies. We conduct extensive experiments on two real-world datasets to evaluate our method. Results show that our method outperforms other state-of-the-arts, indicating its effectiveness and robustness. 

**Abstract (ZH)**: 时空交通预测在智能交通系统中至关重要。准确预测的关键挑战是如何建模复杂的时空依赖关系并适应数据中的固有动态。传统的图卷积网络（GCNs）往往难以处理静态的邻接矩阵（引入领域偏差）或可学习的矩阵（可能过度拟合特定模式）。在考虑多任务学习（MTL）时，这一挑战变得更加复杂。尽管MTL有潜力通过任务协同效应提升预测准确性，但也可能因任务干扰而面临重大障碍。为克服这些挑战，本研究引入了一种新的MTL框架，动态组别时空多任务学习（DG-STMTL）。DG-STMTL提出了一种结合静态矩阵和动态矩阵的混合邻接矩阵生成模块，通过任务特定的门控机制实现。我们还引入了组别GCN模块以增强时空依赖关系的建模能力。我们在两个真实世界数据集上进行了广泛的实验以评估我们的方法。实验结果表明，我们的方法优于其他最先进的方法，证明了其有效性和 robustness。 

---
# SlimSpeech: Lightweight and Efficient Text-to-Speech with Slim Rectified Flow 

**Title (ZH)**: SlimSpeech: 轻量高效文本转语音方法及其应用Slim Rectified Flow 

**Authors**: Kaidi Wang, Wenhao Guan, Shenghui Lu, Jianglong Yao, Lin Li, Qingyang Hong  

**Link**: [PDF](https://arxiv.org/pdf/2504.07776)  

**Abstract**: Recently, flow matching based speech synthesis has significantly enhanced the quality of synthesized speech while reducing the number of inference steps. In this paper, we introduce SlimSpeech, a lightweight and efficient speech synthesis system based on rectified flow. We have built upon the existing speech synthesis method utilizing the rectified flow model, modifying its structure to reduce parameters and serve as a teacher model. By refining the reflow operation, we directly derive a smaller model with a more straight sampling trajectory from the larger model, while utilizing distillation techniques to further enhance the model performance. Experimental results demonstrate that our proposed method, with significantly reduced model parameters, achieves comparable performance to larger models through one-step sampling. 

**Abstract (ZH)**: 基于矫正流的SlimSpeech轻量高效语音合成系统 

---
# Data over dialogue: Why artificial intelligence is unlikely to humanise medicine 

**Title (ZH)**: 数据胜过对话：人工智能 unlikely 使其人性化为何 

**Authors**: Joshua Hatherley  

**Link**: [PDF](https://arxiv.org/pdf/2504.07763)  

**Abstract**: Recently, a growing number of experts in artificial intelligence (AI) and medicine have be-gun to suggest that the use of AI systems, particularly machine learning (ML) systems, is likely to humanise the practice of medicine by substantially improving the quality of clinician-patient relationships. In this thesis, however, I argue that medical ML systems are more likely to negatively impact these relationships than to improve them. In particular, I argue that the use of medical ML systems is likely to comprise the quality of trust, care, empathy, understanding, and communication between clinicians and patients. 

**Abstract (ZH)**: 然而，我在这项研究中 argue 机器学习系统更有可能负向影响医患关系而非改善它们。特别是，我 argument 机器学习系统的使用很可能损害医患之间信任、关怀、共情、理解与沟通的质量。 

---
# Exploring a Patch-Wise Approach for Privacy-Preserving Fake ID Detection 

**Title (ZH)**: 探索基于Patch的隐私保护伪造身份证检测方法 

**Authors**: Javier Muñoz-Haro, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez  

**Link**: [PDF](https://arxiv.org/pdf/2504.07761)  

**Abstract**: In an increasingly digitalized world, verifying the authenticity of ID documents has become a critical challenge for real-life applications such as digital banking, crypto-exchanges, renting, etc. This study focuses on the topic of fake ID detection, covering several limitations in the field. In particular, no publicly available data from real ID documents exists, and most studies rely on proprietary in-house databases that are not available due to privacy reasons. In order to shed some light on this critical challenge that makes difficult to advance in the field, we explore a trade-off between privacy (i.e., amount of sensitive data available) and performance, proposing a novel patch-wise approach for privacy-preserving fake ID detection. Our proposed approach explores how privacy can be enhanced through: i) two levels of anonymization for an ID document (i.e., fully- and pseudo-anonymized), and ii) different patch size configurations, varying the amount of sensitive data visible in the patch image. Also, state-of-the-art methods such as Vision Transformers and Foundation Models are considered in the analysis. The experimental framework shows that, on an unseen database (DLC-2021), our proposal achieves 13.91% and 0% EERs at patch and ID document level, showing a good generalization to other databases. In addition to this exploration, another key contribution of our study is the release of the first publicly available database that contains 48,400 patches from both real and fake ID documents, along with the experimental framework and models, which will be available in our GitHub. 

**Abstract (ZH)**: 在日益数字化的世界中，验证身份文档的真实性已成为数字银行、加密交易、租赁等实际应用中的关键挑战。本文专注于假身份文档检测这一主题，涵盖了该领域的一些局限性。特别是，目前没有公开的真实身份文档数据集，大多数研究依赖于隐私原因无法公开的内部专用数据库。为了缓解这一关键挑战，我们探讨了隐私（即敏感数据量）与性能之间的权衡，提出了一种新颖的块级方法，用于保护隐私的假身份文档检测。我们提出的方法探索了通过以下方式增强隐私：i) 身份文档的两级匿名化（完全匿名和部分匿名），ii) 不同大小的块配置，改变可见敏感数据的量。此外，本文分析了如视觉变换器和基础模型等最新方法。实验框架显示，在一个未知数据集（DLC-2021）上，我们的提议在块级和身份文档级分别实现了13.91%和0%的EER，展示了良好的泛化能力。此外，本文的另一重要贡献是发布了首个包含48,400个来自真实和虚假身份文档的块的数据集，以及实验框架和模型，这些资料将在我们的GitHub上公开。 

---
# Counting Hours, Counting Losses: The Toll of Unpredictable Work Schedules on Financial Security 

**Title (ZH)**: 计时又计失：不可预测工作时间表对财务安全的代价 

**Authors**: Pegah Nokhiz, Aravinda Kanchana Ruwanpathirana, Aditya Bhaskara, Suresh Venkatasubramanian  

**Link**: [PDF](https://arxiv.org/pdf/2504.07719)  

**Abstract**: Financial instability has become a significant issue in today's society. While research typically focuses on financial aspects, there is a tendency to overlook time-related aspects of unstable work schedules. The inability to rely on consistent work schedules leads to burnout, work-family conflicts, and financial shocks that directly impact workers' income and assets. Unforeseen fluctuations in earnings pose challenges in financial planning, affecting decisions on savings and spending and ultimately undermining individuals' long-term financial stability and well-being.
This issue is particularly evident in sectors where workers experience frequently changing schedules without sufficient notice, including those in the food service and retail sectors, part-time and hourly workers, and individuals with lower incomes. These groups are already more financially vulnerable, and the unpredictable nature of their schedules exacerbates their financial fragility.
Our objective is to understand how unforeseen fluctuations in earnings exacerbate financial fragility by investigating the extent to which individuals' financial management depends on their ability to anticipate and plan for the future. To address this question, we develop a simulation framework that models how individuals optimize utility amidst financial uncertainty and the imperative to avoid financial ruin. We employ online learning techniques, specifically adapting workers' consumption policies based on evolving information about their work schedules.
With this framework, we show both theoretically and empirically how a worker's capacity to anticipate schedule changes enhances their long-term utility. Conversely, the inability to predict future events can worsen workers' instability. Moreover, our framework enables us to explore interventions to mitigate the problem of schedule uncertainty and evaluate their effectiveness. 

**Abstract (ZH)**: 金融不稳定已成为当今社会的一个重要问题。虽然研究通常关注金融方面，但往往忽视了不稳定工作时间安排的时间相关性。无法依赖于稳定的工作时间安排导致了职业倦怠、工作与家庭冲突以及直接影响工人收入和资产的财务冲击。不可预见的收入波动对财务规划构成了挑战，影响了储蓄和支出的决策，最终威胁了个人的长期财务稳定和福祉。 

---
# Merging Embedded Topics with Optimal Transport for Online Topic Modeling on Data Streams 

**Title (ZH)**: 嵌入主题与最优传输在数据流的在线主题建模中的融合 

**Authors**: Federica Granese, Benjamin Navet, Serena Villata, Charles Bouveyron  

**Link**: [PDF](https://arxiv.org/pdf/2504.07711)  

**Abstract**: Topic modeling is a key component in unsupervised learning, employed to identify topics within a corpus of textual data. The rapid growth of social media generates an ever-growing volume of textual data daily, making online topic modeling methods essential for managing these data streams that continuously arrive over time. This paper introduces a novel approach to online topic modeling named StreamETM. This approach builds on the Embedded Topic Model (ETM) to handle data streams by merging models learned on consecutive partial document batches using unbalanced optimal transport. Additionally, an online change point detection algorithm is employed to identify shifts in topics over time, enabling the identification of significant changes in the dynamics of text streams. Numerical experiments on simulated and real-world data show StreamETM outperforming competitors. 

**Abstract (ZH)**: 基于嵌入式主题模型的流式在线主题建模方法StreamETM 

---
# ms-Mamba: Multi-scale Mamba for Time-Series Forecasting 

**Title (ZH)**: ms-Mamba: 多尺度Mamba时间序列预测 

**Authors**: Yusuf Meric Karadag, Sinan Kalkan, Ipek Gursel Dino  

**Link**: [PDF](https://arxiv.org/pdf/2504.07654)  

**Abstract**: The problem of Time-series Forecasting is generally addressed by recurrent, Transformer-based and the recently proposed Mamba-based architectures. However, existing architectures generally process their input at a single temporal scale, which may be sub-optimal for many tasks where information changes over multiple time scales. In this paper, we introduce a novel architecture called Multi-scale Mamba (ms-Mamba) to address this gap. ms-Mamba incorporates multiple temporal scales by using multiple Mamba blocks with different sampling rates ($\Delta$s). Our experiments on many benchmarks demonstrate that ms-Mamba outperforms state-of-the-art approaches, including the recently proposed Transformer-based and Mamba-based models. 

**Abstract (ZH)**: 多尺度Mamba（ms-Mamba）架构的时间序列预测 

---
# Predicting the Lifespan of Industrial Printheads with Survival Analysis 

**Title (ZH)**: 基于生存分析的工业打印头寿命预测 

**Authors**: Dan Parii, Evelyne Janssen, Guangzhi Tang, Charalampos Kouzinopoulos, Marcin Pietrasik  

**Link**: [PDF](https://arxiv.org/pdf/2504.07638)  

**Abstract**: Accurately predicting the lifespan of critical device components is essential for maintenance planning and production optimization, making it a topic of significant interest in both academia and industry. In this work, we investigate the use of survival analysis for predicting the lifespan of production printheads developed by Canon Production Printing. Specifically, we focus on the application of five techniques to estimate survival probabilities and failure rates: the Kaplan-Meier estimator, Cox proportional hazard model, Weibull accelerated failure time model, random survival forest, and gradient boosting. The resulting estimates are further refined using isotonic regression and subsequently aggregated to determine the expected number of failures. The predictions are then validated against real-world ground truth data across multiple time windows to assess model reliability. Our quantitative evaluation using three performance metrics demonstrates that survival analysis outperforms industry-standard baseline methods for printhead lifespan prediction. 

**Abstract (ZH)**: 准确预测关键设备组件的寿命对于维护规划和生产优化至关重要，这一主题在学术界和工业界均备受关注。在本工作中，我们调查了生存分析在预测佳能生产打印部门开发的生产喷墨头寿命中的应用。具体而言，我们重点研究了五种技术以估计生存概率和失效率：Kaplan-Meier估计器、Cox比例风险模型、Weibull加速失效时间模型、随机生存森林和梯度提升。最终，我们使用isotonic回归进一步细化这些估计值，并将它们汇总以确定预期的失败次数。然后，我们将这些预测结果与多个时间窗口的真实世界数据进行验证，以评估模型的可靠性。我们的定量评估表明，生存分析在喷墨头寿命预测中优于工业标准基线方法。 

---
# Deep Learning Meets Teleconnections: Improving S2S Predictions for European Winter Weather 

**Title (ZH)**: 深度学习与遥相关相结合：提高欧洲冬季天气的季节内预测Accuracy 

**Authors**: Philine L. Bommer, Marlene Kretschmer, Fiona R. Spuler, Kirill Bykov, Marina M.-C. Höhne  

**Link**: [PDF](https://arxiv.org/pdf/2504.07625)  

**Abstract**: Predictions on subseasonal-to-seasonal (S2S) timescales--ranging from two weeks to two month--are crucial for early warning systems but remain challenging owing to chaos in the climate system. Teleconnections, such as the stratospheric polar vortex (SPV) and Madden-Julian Oscillation (MJO), offer windows of enhanced predictability, however, their complex interactions remain underutilized in operational forecasting. Here, we developed and evaluated deep learning architectures to predict North Atlantic-European (NAE) weather regimes, systematically assessing the role of remote drivers in improving S2S forecast skill of deep learning models. We implemented (1) a Long Short-term Memory (LSTM) network predicting the NAE regimes of the next six weeks based on previous regimes, (2) an Index-LSTM incorporating SPV and MJO indices, and (3) a ViT-LSTM using a Vision Transformer to directly encode stratospheric wind and tropical outgoing longwave radiation fields. These models are compared with operational hindcasts as well as other AI models. Our results show that leveraging teleconnection information enhances skill at longer lead times. Notably, the ViT-LSTM outperforms ECMWF's subseasonal hindcasts beyond week 4 by improving Scandinavian Blocking (SB) and Atlantic Ridge (AR) predictions. Analysis of high-confidence predictions reveals that NAO-, SB, and AR opportunity forecasts can be associated with SPV variability and MJO phase patterns aligning with established pathways, also indicating new patterns. Overall, our work demonstrates that encoding physically meaningful climate fields can enhance S2S prediction skill, advancing AI-driven subseasonal forecast. Moreover, the experiments highlight the potential of deep learning methods as investigative tools, providing new insights into atmospheric dynamics and predictability. 

**Abstract (ZH)**: 预测亚季尺度（S2S）范围（两周至两个月）的天气模式对于早期预警系统至关重要，但由于气候系统的混沌性，这仍然具有挑战性。通过 trop 联系，如平流层极地漩涡（SPV）和马德森-朱利亚振荡（MJO），可以提供增强可预测性的窗口，然而，它们复杂的交互仍然未被操作性预报充分利用。在这里，我们开发并评估了深度学习架构以预测北大西洋-欧洲（NAE）天气模式，系统性地评估了远程驱动因素在提高深度学习模型亚季尺度预报技巧中的作用。我们实施了（1）基于前序模式预测未来六周NAE模式的长短期记忆（LSTM）网络，（2）结合SPV和MJO指数的索引-LSTM，以及（3）使用视觉变换器直接编码平流层风和热带红外辐射场的ViT-LSTM。这些模型与操作性回算以及其它AI模型进行了比较。结果表明，利用trop 联系信息可以提高长时间提前量的预报技巧。值得注意的是，ViT-LSTM 在第四周之后优于 ECMWF 的亚季回算，并改善了斯堪的纳维亚阻塞（SB）和大西洋脊（AR）的预测。高置信度预测分析表明，NAO-、SB 和 AR 机会预测与 SPV 变异性和 MJO 相位模式相吻合，也表明了新的模式。总体而言，我们的研究证明将物理上有意义的气候场编码可以提高亚季尺度预测技巧，促进AI驱动的亚季预报的发展。此外，实验强调了深度学习方法作为研究工具的潜力，为大气动力学和可预报性提供了新的见解。 

---
# Malware analysis assisted by AI with R2AI 

**Title (ZH)**: AI辅助的M2AI恶意软件分析 

**Authors**: Axelle Apvrille, Daniel Nakov  

**Link**: [PDF](https://arxiv.org/pdf/2504.07574)  

**Abstract**: This research studies the quality, speed and cost of malware analysis assisted by artificial intelligence. It focuses on Linux and IoT malware of 2024-2025, and uses r2ai, the AI extension of Radare2's disassembler. Not all malware and not all LLMs are equivalent but the study shows excellent results with Claude 3.5 and 3.7 Sonnet. Despite a few errors, the quality of analysis is overall equal or better than without AI assistance. For good results, the AI cannot operate alone and must constantly be guided by an experienced analyst. The gain of speed is largely visible with AI assistance, even when taking account the time to understand AI's hallucinations, exaggerations and omissions. The cost is usually noticeably lower than the salary of a malware analyst, but attention and guidance is needed to keep it under control in cases where the AI would naturally loop without showing progress. 

**Abstract (ZH)**: 这项研究探讨了人工智能辅助下恶意软件分析的质量、速度和成本。该研究专注于2024-2025年的Linux和IoT恶意软件，并使用了r2ai，即Radare2反汇编器的AI扩展。并非所有恶意软件和语言模型都是等效的，但研究表明，在使用Claude 3.5和3.7 Sonnet的情况下，效果极佳。尽管存在一些错误，但在人工智能辅助下，分析的质量整体上优于没有人工智能辅助的情况下。要获得良好的结果，人工智能不能单独运行，而必须不断接受经验丰富的分析师的指导。在有AI参与的情况下，速度改进是显而易见的，即使考虑理解AI的幻觉、夸大和遗漏所需的时间也是一样。与恶意软件分析师的薪酬相比，成本通常更低，但在AI自然循环而未显示进展的情况下，需要注意并控制成本。 

---
# Diffusion Transformers for Tabular Data Time Series Generation 

**Title (ZH)**: 差分变换器在表格数据时间序列生成中的应用 

**Authors**: Fabrizio Garuti, Enver Sangineto, Simone Luetto, Lorenzo Forni, Rita Cucchiara  

**Link**: [PDF](https://arxiv.org/pdf/2504.07566)  

**Abstract**: Tabular data generation has recently attracted a growing interest due to its different application scenarios. However, generating time series of tabular data, where each element of the series depends on the others, remains a largely unexplored domain. This gap is probably due to the difficulty of jointly solving different problems, the main of which are the heterogeneity of tabular data (a problem common to non-time-dependent approaches) and the variable length of a time series. In this paper, we propose a Diffusion Transformers (DiTs) based approach for tabular data series generation. Inspired by the recent success of DiTs in image and video generation, we extend this framework to deal with heterogeneous data and variable-length sequences. Using extensive experiments on six datasets, we show that the proposed approach outperforms previous work by a large margin. 

**Abstract (ZH)**: 基于扩散变压器的表格时间序列数据生成 

---
# ReXCL: A Tool for Requirement Document Extraction and Classification 

**Title (ZH)**: ReXCL: 一种需求文档提取与分类工具 

**Authors**: Paheli Bhattacharya, Manojit Chakraborty, Santhosh Kumar Arumugam, Rishabh Gupta  

**Link**: [PDF](https://arxiv.org/pdf/2504.07562)  

**Abstract**: This paper presents the ReXCL tool, which automates the extraction and classification processes in requirement engineering, enhancing the software development lifecycle. The tool features two main modules: Extraction, which processes raw requirement documents into a predefined schema using heuristics and predictive modeling, and Classification, which assigns class labels to requirements using adaptive fine-tuning of encoder-based models. The final output can be exported to external requirement engineering tools. Performance evaluations indicate that ReXCL significantly improves efficiency and accuracy in managing requirements, marking a novel approach to automating the schematization of semi-structured requirement documents. 

**Abstract (ZH)**: 这篇文章介绍了ReXCL工具，该工具自动化了需求工程中的提取和分类过程，增强软件开发生命周期的效率。该工具包含两个主要模块：提取模块使用启发式方法和预测建模将原始需求文档转换为预定义方案；分类模块利用基于编解码器模型的自适应微调为需求分配类别标签。最终输出可以导出到外部需求工程工具。性能评估表明，ReXCL在管理和处理半结构化需求文档的方案化方面显著提高了效率和准确性，标志着自动化需求工程的一个新颖方法。 

---
# PoGO: A Scalable Proof of Useful Work via Quantized Gradient Descent and Merkle Proofs 

**Title (ZH)**: PoGO: 一种基于量化梯度下降和Merkle证明的可扩展的工作证明机制 

**Authors**: José I. Orlicki  

**Link**: [PDF](https://arxiv.org/pdf/2504.07540)  

**Abstract**: We present a design called \emph{Proof of Gradient Optimization} (PoGO) for blockchain consensus, where miners produce verifiable evidence of training large-scale machine-learning models. Building on previous work, we incorporate \emph{quantized gradients} (4-bit precision) to reduce storage and computation requirements, while still preserving the ability of verifiers to check that real progress has been made on lowering the model's loss. Additionally, we employ Merkle proofs over the full 32-bit model to handle large parameter sets and to enable random leaf checks with minimal on-chain data. We illustrate these ideas using GPT-3 (175B parameters) as a reference example and also refer to smaller but high-performance models (e.g., \emph{Gemma~3} with 27B parameters). We provide an empirical cost analysis showing that verification is significantly cheaper than training, thanks in part to quantization and sampling. We also discuss the necessity of longer block times (potentially hours) when incorporating meaningful training steps, the trade-offs when using specialized GPU hardware, and how binary diffs may incrementally optimize updates. Finally, we note that fine-tuning can be handled in a similar manner, merely changing the dataset and the manner of sampling but preserving the overall verification flow. Our protocol allows verifiers to issue either \emph{positive} or \emph{negative} attestations; these are aggregated at finalization to either confirm the update or slash the miner. 

**Abstract (ZH)**: 基于梯度优化的区块链共识证明设计（Proof of Gradient Optimization, PoGO） 

---
# Adversarial Subspace Generation for Outlier Detection in High-Dimensional Data 

**Title (ZH)**: 高维数据离群点检测的 adversarial 子空间生成 

**Authors**: Jose Cribeiro-Ramallo, Federico Matteucci, Paul Enciu, Alexander Jenke, Vadim Arzamasov, Thorsten Strufe, Klemens Böhm  

**Link**: [PDF](https://arxiv.org/pdf/2504.07522)  

**Abstract**: Outlier detection in high-dimensional tabular data is challenging since data is often distributed across multiple lower-dimensional subspaces -- a phenomenon known as the Multiple Views effect (MV). This effect led to a large body of research focused on mining such subspaces, known as subspace selection. However, as the precise nature of the MV effect was not well understood, traditional methods had to rely on heuristic-driven search schemes that struggle to accurately capture the true structure of the data. Properly identifying these subspaces is critical for unsupervised tasks such as outlier detection or clustering, where misrepresenting the underlying data structure can hinder the performance. We introduce Myopic Subspace Theory (MST), a new theoretical framework that mathematically formulates the Multiple Views effect and writes subspace selection as a stochastic optimization problem. Based on MST, we introduce V-GAN, a generative method trained to solve such an optimization problem. This approach avoids any exhaustive search over the feature space while ensuring that the intrinsic data structure is preserved. Experiments on 42 real-world datasets show that using V-GAN subspaces to build ensemble methods leads to a significant increase in one-class classification performance -- compared to existing subspace selection, feature selection, and embedding methods. Further experiments on synthetic data show that V-GAN identifies subspaces more accurately while scaling better than other relevant subspace selection methods. These results confirm the theoretical guarantees of our approach and also highlight its practical viability in high-dimensional settings. 

**Abstract (ZH)**: 高维表数据中的离群点检测由于数据常常分布在多个低维子空间中——这一现象被称为多重视图效应（MV）——而具有挑战性。基于多重视图效应，我们引入了近视子空间理论（MST），这是一种新的理论框架，数学上定义了多重视图效应，并将子空间选择问题表述为一个随机优化问题。基于MST，我们提出了V-GAN生成模型，该模型受训于解决此类优化问题。该方法避免在整个特征空间中进行 exhaustive 搜索，同时确保保留数据的固有结构。在42个真实世界数据集上的实验表明，使用V-GAN子空间构建集成方法可以显著提高单类分类性能，与现有的子空间选择、特征选择和嵌入方法相比。在合成数据上的进一步实验表明，V-GAN能够更准确地识别子空间，并且在扩展性方面优于其他相关子空间选择方法。这些结果证实了我们方法的理论保证，并且突显了其在高维设置中的实用可行性。 

---
# Enhancements for Developing a Comprehensive AI Fairness Assessment Standard 

**Title (ZH)**: 增强全面AI公平性评估标准开发的研究 

**Authors**: Avinash Agarwal, Mayashankar Kumar, Manisha J. Nene  

**Link**: [PDF](https://arxiv.org/pdf/2504.07516)  

**Abstract**: As AI systems increasingly influence critical sectors like telecommunications, finance, healthcare, and public services, ensuring fairness in decision-making is essential to prevent biased or unjust outcomes that disproportionately affect vulnerable entities or result in adverse impacts. This need is particularly pressing as the industry approaches the 6G era, where AI will drive complex functions like autonomous network management and hyper-personalized services. The TEC Standard for Fairness Assessment and Rating of AI Systems provides guidelines for evaluating fairness in AI, focusing primarily on tabular data and supervised learning models. However, as AI applications diversify, this standard requires enhancement to strengthen its impact and broaden its applicability. This paper proposes an expansion of the TEC Standard to include fairness assessments for images, unstructured text, and generative AI, including large language models, ensuring a more comprehensive approach that keeps pace with evolving AI technologies. By incorporating these dimensions, the enhanced framework will promote responsible and trustworthy AI deployment across various sectors. 

**Abstract (ZH)**: 随着AI系统在电信、金融、医疗保健和公共服务等关键领域的影响日益增大，确保决策公平性以防止针对脆弱实体的偏见或不公正结果变得至关重要。这一需求随着行业迈向6G时代而更加紧迫，在该时代，AI将驱动复杂的功能如自管理网络和超个性化服务。TEC公平性评估和评级标准为评估AI公平性提供了规范，主要关注表格式数据和监督学习模型。然而，随着AI应用的多样化，该标准需要改进以增强其影响并扩大其适用范围。本文提议扩展TEC标准，以包括图像、非结构化文本和生成型AI（包括大型语言模型）的公平性评估，确保一种更为全面的方法能够与不断发展的AI技术保持同步。通过纳入这些维度，增强框架将促进各类领域的负责任和可信AI部署。 

---
# CMEdataset Advancing China Map Detection and Standardization with Digital Image Resources 

**Title (ZH)**: CMEdataset 促进基于数字图像资源的中国地图检测与标准化 

**Authors**: Yan Xu, Zhenqiang Zhang, Zhiwei Zhou, Liting Geng, Yue Li, Jintao Li  

**Link**: [PDF](https://arxiv.org/pdf/2504.07476)  

**Abstract**: Digital images of Chinas maps play a crucial role in map detection, particularly in ensuring national sovereignty, territorial integrity, and map compliance. However, there is currently no publicly available dataset specifically dedicated to problematic maps the CME dataset. Existing datasets primarily focus on general map data and are insufficient for effectively identifying complex issues such as national boundary misrepresentations, missing elements, and blurred boundaries. Therefore, this study creates a Problematic Map dataset that covers five key problem areas, aiming to provide diverse samples for problematic map detection technologies, support high-precision map compliance detection, and enhance map data quality and timeliness. This dataset not only provides essential resources for map compliance, national security monitoring, and map updates, but also fosters innovation and application of related technologies. 

**Abstract (ZH)**: 中国地图的数字图像在地图检测中扮演着 crucial 角色，特别是在保障国家主权、领土完整和地图合规性方面。然而，目前尚无专门针对问题地图的公开数据集，如 CME 数据集。现有数据集主要关注一般地图数据，不足以有效识别国家边界误表示、缺失要素和模糊边界等复杂问题。因此，本研究创建了一个问题地图数据集，涵盖了五个关键问题领域，旨在为问题地图检测技术提供多样化样本，支持高精度地图合规性检测，并提升地图数据的质量和及时性。该数据集不仅为地图合规性、国家安全监控和地图更新提供 essential 资源，还有助于推动相关技术的创新和应用。 

---
# Over-Relying on Reliance: Towards Realistic Evaluations of AI-Based Clinical Decision Support 

**Title (ZH)**: 过度依赖依赖性：迈向基于AI的临床决策支持的现实评估 

**Authors**: Venkatesh Sivaraman, Katelyn Morrison, Will Epperson, Adam Perer  

**Link**: [PDF](https://arxiv.org/pdf/2504.07423)  

**Abstract**: As AI-based clinical decision support (AI-CDS) is introduced in more and more aspects of healthcare services, HCI research plays an increasingly important role in designing for complementarity between AI and clinicians. However, current evaluations of AI-CDS often fail to capture when AI is and is not useful to clinicians. This position paper reflects on our work and influential AI-CDS literature to advocate for moving beyond evaluation metrics like Trust, Reliance, Acceptance, and Performance on the AI's task (what we term the "trap" of human-AI collaboration). Although these metrics can be meaningful in some simple scenarios, we argue that optimizing for them ignores important ways that AI falls short of clinical benefit, as well as ways that clinicians successfully use AI. As the fields of HCI and AI in healthcare develop new ways to design and evaluate CDS tools, we call on the community to prioritize ecologically valid, domain-appropriate study setups that measure the emergent forms of value that AI can bring to healthcare professionals. 

**Abstract (ZH)**: 随着基于AI的临床决策支持（AI-CDS）在越来越多的医疗保健服务领域得到应用，人机交互（HCI）研究在设计AI与临床医生互补性方面发挥着越来越重要的作用。然而，当前对AI-CDS的评估往往未能捕捉到AI在临床医生工作中的有用和无用之处。本文从我们的工作和影响深远的AI-CDS文献出发，倡导超越基于人类-AI协作的“信任、依赖、接受和AI任务表现”等评价指标（我们称其为人类-AI合作的“陷阱”）。尽管这些指标在某些简单情境下具有意义，但我们认为，将优化这些指标忽视了AI未能实现临床效益的重要方式，以及临床医生成功利用AI的方式。随着HCI和医疗保健中AI设计与评估方法的发展，我们呼吁社区优先采用生态有效、领域适宜的研究框架，以衡量AI为医疗专业人员带来的一种新兴价值形式。 

---
# The Role of Machine Learning in Reducing Healthcare Costs: The Impact of Medication Adherence and Preventive Care on Hospitalization Expenses 

**Title (ZH)**: 机器学习在降低医疗成本中的作用：药物依从性和预防护理对住院费用的影响 

**Authors**: Yixin Zhang, Yisong Chen  

**Link**: [PDF](https://arxiv.org/pdf/2504.07422)  

**Abstract**: This study reveals the important role of prevention care and medication adherence in reducing hospitalizations. By using a structured dataset of 1,171 patients, four machine learning models Logistic Regression, Gradient Boosting, Random Forest, and Artificial Neural Networks are applied to predict five-year hospitalization risk, with the Gradient Boosting model achieving the highest accuracy of 81.2%. The result demonstrated that patients with high medication adherence and consistent preventive care can reduce 38.3% and 37.7% in hospitalization risk. The finding also suggests that targeted preventive care can have positive Return on Investment (ROI), and therefore ML models can effectively direct personalized interventions and contribute to long-term medical savings. 

**Abstract (ZH)**: 本研究揭示了预防护理和服药依从性在减少住院率中的重要作用。通过使用1,171名患者的结构化数据集，应用逻辑回归、梯度提升、随机森林和人工神经网络四种机器学习模型预测五年住院风险，梯度提升模型的准确率达到最高，为81.2%。结果表明，高服药依从性和持续的预防护理可以分别减少38.3%和37.7%的住院风险。研究还表明，针对性的预防护理可以实现正的投资回报率（ROI），因此机器学习模型可以有效地指导个性化干预，有助于长期医疗费用节省。 

---
# A Novel Mamba-based Sequential Recommendation Method 

**Title (ZH)**: 基于Mamba的新型序列推荐方法 

**Authors**: Jun Yuan  

**Link**: [PDF](https://arxiv.org/pdf/2504.07398)  

**Abstract**: Sequential recommendation (SR), which encodes user activity to predict the next action, has emerged as a widely adopted strategy in developing commercial personalized recommendation systems. Although Transformer-based models have proven effective for sequential recommendation, the complexity of the self-attention module in Transformers scales quadratically with the sequence length. Controlling model complexity is essential for large-scale recommendation systems, as these systems may need to handle billion-scale vocabularies that evolve continuously, as well as user behavior sequences that can exceed tens of thousands in length. In this paper, we propose a novel multi-head latent Mamba architecture, which employs multiple low-dimensional Mamba layers and fully connected layers coupled with positional encoding to simultaneously capture historical and item information within each latent subspace. Our proposed method not only enables scaling up to large-scale parameters but also extends to multi-domain recommendation by integrating and fine-tuning LLMs. Through extensive experiments on public datasets, we demonstrate how Hydra effectively addresses the effectiveness-efficiency dilemma, outperforming state-of-the-art sequential recommendation baselines with significantly fewer parameters and reduced training time. 

**Abstract (ZH)**: 基于顺序推荐的多头潜在Mamba架构：高效扩展与多域适应 

---
# MicroNAS: An Automated Framework for Developing a Fall Detection System 

**Title (ZH)**: MicroNAS: 一种跌倒检测系统开发的自动化框架 

**Authors**: Seyed Mojtaba Mohasel, John Sheppard, Lindsey K. Molina, Richard R. Neptune, Shane R. Wurdeman, Corey A. Pew  

**Link**: [PDF](https://arxiv.org/pdf/2504.07397)  

**Abstract**: This work presents MicroNAS, an automated neural architecture search tool specifically designed to create models optimized for microcontrollers with small memory resources. The ESP32 microcontroller, with 320 KB of memory, is used as the target platform. The artificial intelligence contribution lies in a novel method for optimizing convolutional neural network and gated recurrent unit architectures by considering the memory size of the target microcontroller as a guide. A comparison is made between memory-driven model optimization and traditional two-stage methods, which use pruning, to show the effectiveness of the proposed framework. To demonstrate the engineering application of MicroNAS, a fall detection system (FDS) for lower-limb amputees is developed as a pilot study. A critical challenge in fall detection studies, class imbalance in the dataset, is addressed. The results show that MicroNAS models achieved higher F1-scores than alternative approaches, such as ensemble methods and H2O Automated Machine Learning, presenting a significant step forward in real-time FDS development. Biomechanists using body-worn sensors for activity detection can adopt the open-source code to design machine learning models tailored for microcontroller platforms with limited memory. 

**Abstract (ZH)**: MicroNAS：一种专为小型内存资源微控制器优化的自动化神经架构搜索工具 

---
# Min-Max Optimisation for Nonconvex-Nonconcave Functions Using a Random Zeroth-Order Extragradient Algorithm 

**Title (ZH)**: 非凸-非凹函数的最小最大优化：一种随机零阶外梯度算法 

**Authors**: Amir Ali Farzin, Yuen Man Pun, Philipp Braun, Antoine Lesage-landry, Youssef Diouane, Iman Shames  

**Link**: [PDF](https://arxiv.org/pdf/2504.07388)  

**Abstract**: This study explores the performance of the random Gaussian smoothing Zeroth-Order ExtraGradient (ZO-EG) scheme considering min-max optimisation problems with possibly NonConvex-NonConcave (NC-NC) objective functions. We consider both unconstrained and constrained, differentiable and non-differentiable settings. We discuss the min-max problem from the point of view of variational inequalities. For the unconstrained problem, we establish the convergence of the ZO-EG algorithm to the neighbourhood of an $\epsilon$-stationary point of the NC-NC objective function, whose radius can be controlled under a variance reduction scheme, along with its complexity. For the constrained problem, we introduce the new notion of proximal variational inequalities and give examples of functions satisfying this property. Moreover, we prove analogous results to the unconstrained case for the constrained problem. For the non-differentiable case, we prove the convergence of the ZO-EG algorithm to a neighbourhood of an $\epsilon$-stationary point of the smoothed version of the objective function, where the radius of the neighbourhood can be controlled, which can be related to the ($\delta,\epsilon$)-Goldstein stationary point of the original objective function. 

**Abstract (ZH)**: 本研究探索了在可能具有非凸非凹（NC-NC）目标函数的极小-极大优化问题中，随机高斯平滑零阶额外梯度（ZO-EG）方案的性能。我们考虑了不受约束和受约束、可微和不可微的情况。从变分不等式的视角讨论极小-极大问题。对于不受约束的问题，我们建立了ZO-EG算法收敛到NC-NC目标函数的$\epsilon$-稳定点邻域，并探讨了其收敛半径在方差减少方案下的可控性及其复杂性。对于受约束的问题，我们引入了邻近变分不等式的全新概念，并给出满足这一性质的函数示例。此外，我们证明了受约束问题中类似不受约束情况的结论。对于不可微的情况，我们证明了ZO-EG算法收敛到平滑目标函数的$\epsilon$-稳定点邻域，其收敛半径可控，并可与原目标函数的($\delta,\epsilon$)-Goldstein稳定点相关。 

---
# PROPEL: Supervised and Reinforcement Learning for Large-Scale Supply Chain Planning 

**Title (ZH)**: PROPEL：大型供应链规划的监督与强化学习方法 

**Authors**: Vahid Eghbal Akhlaghi, Reza Zandehshahvar, Pascal Van Hentenryck  

**Link**: [PDF](https://arxiv.org/pdf/2504.07383)  

**Abstract**: This paper considers how to fuse Machine Learning (ML) and optimization to solve large-scale Supply Chain Planning (SCP) optimization problems. These problems can be formulated as MIP models which feature both integer (non-binary) and continuous variables, as well as flow balance and capacity constraints. This raises fundamental challenges for existing integrations of ML and optimization that have focused on binary MIPs and graph problems. To address these, the paper proposes PROPEL, a new framework that combines optimization with both supervised and Deep Reinforcement Learning (DRL) to reduce the size of search space significantly. PROPEL uses supervised learning, not to predict the values of all integer variables, but to identify the variables that are fixed to zero in the optimal solution, leveraging the structure of SCP applications. PROPEL includes a DRL component that selects which fixed-at-zero variables must be relaxed to improve solution quality when the supervised learning step does not produce a solution with the desired optimality tolerance. PROPEL has been applied to industrial supply chain planning optimizations with millions of variables. The computational results show dramatic improvements in solution times and quality, including a 60% reduction in primal integral and an 88% primal gap reduction, and improvement factors of up to 13.57 and 15.92, respectively. 

**Abstract (ZH)**: 本文考虑如何将机器学习（ML）与优化相结合，以解决大规模供应链规划（SCP）优化问题。这些问题可以被形式化为包含整数（非二进制）和连续变量以及流量平衡和容量约束的MIP模型。这为现有的主要针对二进制MIP和图问题的ML与优化集成提出了根本性的挑战。为了解决这些问题，本文提出了一种名为PROPEL的新框架，该框架结合了优化与监督学习和深度强化学习（DRL），以显著减少搜索空间的大小。PROPEL使用监督学习来识别最优解中固定为零的变量，而不是预测所有整数变量的值，利用SCP应用的结构。PROPEL还包括一个DRL组件，在监督学习步骤未能产生具有所需最优性容差的解时，选择需要放松的固定为零的变量以提高解的质量。PROPEL已被应用于具有数百万个变量的工业供应链规划优化问题。计算结果表明，在解时间和质量方面取得了显著改进，包括原值积分减少60%以及原值差距减少88%，改进因子分别为13.57和15.92。 

---
# Representation Meets Optimization: Training PINNs and PIKANs for Gray-Box Discovery in Systems Pharmacology 

**Title (ZH)**: 表示遇上了优化：训练PINNs和PIKANs进行系统药理学中的灰盒发现 

**Authors**: Nazanin Ahmadi Daryakenari, Khemraj Shukla, George Em Karniadakis  

**Link**: [PDF](https://arxiv.org/pdf/2504.07379)  

**Abstract**: Physics-Informed Kolmogorov-Arnold Networks (PIKANs) are gaining attention as an effective counterpart to the original multilayer perceptron-based Physics-Informed Neural Networks (PINNs). Both representation models can address inverse problems and facilitate gray-box system identification. However, a comprehensive understanding of their performance in terms of accuracy and speed remains underexplored. In particular, we introduce a modified PIKAN architecture, tanh-cPIKAN, which is based on Chebyshev polynomials for parametrization of the univariate functions with an extra nonlinearity for enhanced performance. We then present a systematic investigation of how choices of the optimizer, representation, and training configuration influence the performance of PINNs and PIKANs in the context of systems pharmacology modeling. We benchmark a wide range of first-order, second-order, and hybrid optimizers, including various learning rate schedulers. We use the new Optax library to identify the most effective combinations for learning gray-boxes under ill-posed, non-unique, and data-sparse conditions. We examine the influence of model architecture (MLP vs. KAN), numerical precision (single vs. double), the need for warm-up phases for second-order methods, and sensitivity to the initial learning rate. We also assess the optimizer scalability for larger models and analyze the trade-offs introduced by JAX in terms of computational efficiency and numerical accuracy. Using two representative systems pharmacology case studies - a pharmacokinetics model and a chemotherapy drug-response model - we offer practical guidance on selecting optimizers and representation models/architectures for robust and efficient gray-box discovery. Our findings provide actionable insights for improving the training of physics-informed networks in biomedical applications and beyond. 

**Abstract (ZH)**: Physics-Informed Kolmogorov-Arnold Networks (PIKANs)作为基于多层感知器的Physics-Informed Neural Networks (PINNs)的有力替代，正逐渐引起关注。尽管这两种表示模型都能解决逆问题并促进灰盒系统识别，但在准确性与速度方面的表现仍需进行全面探索。 đặc biệt地，我们引入了基于切比雪夫多项式的改进PIKAN架构tanh-cPIKAN，并增加了额外的非线性以增强性能。随后，我们系统性地研究了优化器、表示方法和训练配置的选择如何影响PINNs和PIKANs在系统药代动力学建模中的性能。我们基准测试了包括各种学习率调度器在内的广泛的一阶、二阶和混合优化器。我们利用新的Optax库，在病态、非唯一和数据稀疏条件下识别出最适合学习灰盒的最佳组合。我们还探讨了模型架构（MLP vs. KAN）、数值精度（单精度 vs. 双精度）、二阶方法的启动阶段需求以及初始学习率的敏感性的影响。我们还评估了大型模型下的优化器可扩展性，并分析了JAX在计算效率和数值精度方面的权衡。最后，我们使用两个代表性的系统药代动力学案例研究——药代动力学模型和化疗药物响应模型——提供选择优化器和表示模型/架构的实用指南，以实现稳健且高效的灰盒发现。我们的发现为在生物医学及其他应用中改进物理知情网络的训练提供了可操作的见解。 

---
# ChronoFormer: Time-Aware Transformer Architectures for Structured Clinical Event Modeling 

**Title (ZH)**: ChronoFormer: 时间意识的Transformer架构用于结构化临床事件建模 

**Authors**: Yuanyun Zhang, Shi Li  

**Link**: [PDF](https://arxiv.org/pdf/2504.07373)  

**Abstract**: The temporal complexity of electronic health record (EHR) data presents significant challenges for predicting clinical outcomes using machine learning. This paper proposes ChronoFormer, an innovative transformer based architecture specifically designed to encode and leverage temporal dependencies in longitudinal patient data. ChronoFormer integrates temporal embeddings, hierarchical attention mechanisms, and domain specific masking techniques. Extensive experiments conducted on three benchmark tasks mortality prediction, readmission prediction, and long term comorbidity onset demonstrate substantial improvements over current state of the art methods. Furthermore, detailed analyses of attention patterns underscore ChronoFormer's capability to capture clinically meaningful long range temporal relationships. 

**Abstract (ZH)**: 电子健康记录数据的时间复杂性为使用机器学习预测临床结果带来了重大挑战。本文提出了一种名为ChronoFormer的创新变压器架构，专门设计用于编码和利用 longitudinal 患者数据中的时间依赖性。ChronoFormer 结合了时间嵌入、分层注意机制和领域特定的遮蔽技术。在死亡率预测、再住院预测和长期共病 onset 的三个基准任务上进行的广泛实验表明，ChronoFormer 在当前最先进的方法上取得了显著改进。此外，对注意模式的详细分析强调了 ChronoFormer 捕捉临床相关长时间范围内的时间关系的能力。 

---
# A Balanced Approach of Rapid Genetic Exploration and Surrogate Exploitation for Hyperparameter Optimization 

**Title (ZH)**: 快速遗传探索与代理利用的平衡方法在超参数优化中的应用 

**Authors**: Chul Kim, Inwhee Joe  

**Link**: [PDF](https://arxiv.org/pdf/2504.07359)  

**Abstract**: This paper proposes a new method for hyperparameter optimization (HPO) that balances exploration and exploitation. While evolutionary algorithms (EAs) show promise in HPO, they often struggle with effective exploitation. To address this, we integrate a linear surrogate model into a genetic algorithm (GA), allowing for smooth integration of multiple strategies. This combination improves exploitation performance, achieving an average improvement of 1.89 percent (max 6.55 percent, min -3.45 percent) over existing HPO methods. 

**Abstract (ZH)**: 本文提出了一种新的超参数优化方法，平衡了探索和利用。虽然进化算法在超参数优化中展现出潜力，但在有效的利用方面常常遇到挑战。为此，我们将线性代理模型集成到遗传算法中，实现了多种策略的平滑集成。这种结合提高了利用性能，相对于现有超参数优化方法，平均改善了1.89个百分点（最高6.55个百分点，最低-3.45个百分点）。 

---
# Quantum-Inspired Genetic Algorithm for Robust Source Separation in Smart City Acoustics 

**Title (ZH)**: 量子启发遗传算法在智能城市声学中稳健的源分离 

**Authors**: Minh K. Quan, Mayuri Wijayasundara, Sujeeva Setunge, Pubudu N. Pathirana  

**Link**: [PDF](https://arxiv.org/pdf/2504.07345)  

**Abstract**: The cacophony of urban sounds presents a significant challenge for smart city applications that rely on accurate acoustic scene analysis. Effectively analyzing these complex soundscapes, often characterized by overlapping sound sources, diverse acoustic events, and unpredictable noise levels, requires precise source separation. This task becomes more complicated when only limited training data is available. This paper introduces a novel Quantum-Inspired Genetic Algorithm (p-QIGA) for source separation, drawing inspiration from quantum information theory to enhance acoustic scene analysis in smart cities. By leveraging quantum superposition for efficient solution space exploration and entanglement to handle correlated sources, p-QIGA achieves robust separation even with limited data. These quantum-inspired concepts are integrated into a genetic algorithm framework to optimize source separation parameters. The effectiveness of our approach is demonstrated on two datasets: the TAU Urban Acoustic Scenes 2020 Mobile dataset, representing typical urban soundscapes, and the Silent Cities dataset, capturing quieter urban environments during the COVID-19 pandemic. Experimental results show that the p-QIGA achieves accuracy comparable to state-of-the-art methods while exhibiting superior resilience to noise and limited training data, achieving up to 8.2 dB signal-to-distortion ratio (SDR) in noisy environments and outperforming baseline methods by up to 2 dB with only 10% of the training data. This research highlights the potential of p-QIGA to advance acoustic signal processing in smart cities, particularly for noise pollution monitoring and acoustic surveillance. 

**Abstract (ZH)**: 量子启发遗传算法（p-QIGA）在智能城市声景分析中的应用 

---
# Evaluating Parameter-Based Training Performance of Neural Networks and Variational Quantum Circuits 

**Title (ZH)**: 基于参数训练的神经网络和变量化量子电路性能评估 

**Authors**: Michael Kölle, Alexander Feist, Jonas Stein, Sebastian Wölckert, Claudia Linnhoff-Popien  

**Link**: [PDF](https://arxiv.org/pdf/2504.07273)  

**Abstract**: In recent years, neural networks (NNs) have driven significant advances in machine learning. However, as tasks grow more complex, NNs often require large numbers of trainable parameters, which increases computational and energy demands. Variational quantum circuits (VQCs) offer a promising alternative: they leverage quantum mechanics to capture intricate relationships and typically need fewer parameters. In this work, we evaluate NNs and VQCs on simple supervised and reinforcement learning tasks, examining models with different parameter sizes. We simulate VQCs and execute selected parts of the training process on real quantum hardware to approximate actual training times. Our results show that VQCs can match NNs in performance while using significantly fewer parameters, despite longer training durations. As quantum technology and algorithms advance, and VQC architectures improve, we posit that VQCs could become advantageous for certain machine learning tasks. 

**Abstract (ZH)**: 近年来，神经网络在机器学习领域取得了显著进步。然而，随着任务复杂性的增加，神经网络往往需要大量的训练参数，这会增加计算和能耗需求。变分量子电路（VQCs）提供了一种有前途的替代方案：它们利用量子力学捕捉复杂的相互关系，并且通常所需的参数较少。在本工作中，我们评估了神经网络和变分量子电路在简单的监督学习和强化学习任务上的表现，研究了不同参数规模的模型。我们模拟了变分量子电路，并在实际量子硬件上执行了部分训练过程，以逼近实际的训练时间。结果显示，尽管训练时间更长，但变分量子电路在性能上可以与神经网络相媲美，同时使用了显著较少的参数。随着量子技术、算法的发展以及变分量子电路架构的改进，我们认为变分量子电路在某些机器学习任务上可能具有优势。 

---
# Trustworthy AI Must Account for Intersectionality 

**Title (ZH)**: 可信的人工智能必须考虑性别交集性 

**Authors**: Jesse C. Cresswell  

**Link**: [PDF](https://arxiv.org/pdf/2504.07170)  

**Abstract**: Trustworthy AI encompasses many aspirational aspects for aligning AI systems with human values, including fairness, privacy, robustness, explainability, and uncertainty quantification. However, efforts to enhance one aspect often introduce unintended trade-offs that negatively impact others, making it challenging to improve all aspects simultaneously. In this position paper, we review notable approaches to these five aspects and systematically consider every pair, detailing the negative interactions that can arise. For example, applying differential privacy to model training can amplify biases in the data, undermining fairness. Drawing on these findings, we take the position that addressing trustworthiness along each axis in isolation is insufficient. Instead, research on Trustworthy AI must account for intersectionality between aspects and adopt a holistic view across all relevant axes at once. To illustrate our perspective, we provide guidance on how researchers can work towards integrated trustworthiness, a case study on how intersectionality applies to the financial industry, and alternative views to our position. 

**Abstract (ZH)**: 可信的人工智能涵盖了与人类价值观对齐的许多 aspirational 方面，包括公平性、隐私保护、鲁棒性、可解释性和不确定性量化。然而，增强其中一个方面往往会导致对其他方面的意外权衡，从而使同时改进所有方面变得困难。在这篇立场论文中，我们回顾了这五个方面的显著方法，并系统地考虑了每一对，详细分析了可能出现的负面相互作用。例如，将差分隐私应用于模型训练可能会放大数据中的偏差，损害公平性。基于这些发现，我们认为仅孤立地针对每个轴线解决可信性是不足的。相反，可信的人工智能研究必须考虑各方面的交汇性，并同时从所有相关轴的角度采取整体视图。为了阐明我们的观点，我们提供了研究人员如何朝着集成可信性的方向工作的指导、金融行业的案例研究以及对我们观点的替代视角。 

---
# PLM-eXplain: Divide and Conquer the Protein Embedding Space 

**Title (ZH)**: PLM-eXplain：分解蛋白嵌入空间 

**Authors**: Jan van Eck, Dea Gogishvili, Wilson Silva, Sanne Abeln  

**Link**: [PDF](https://arxiv.org/pdf/2504.07156)  

**Abstract**: Protein language models (PLMs) have revolutionised computational biology through their ability to generate powerful sequence representations for diverse prediction tasks. However, their black-box nature limits biological interpretation and translation to actionable insights. We present an explainable adapter layer - PLM-eXplain (PLM-X), that bridges this gap by factoring PLM embeddings into two components: an interpretable subspace based on established biochemical features, and a residual subspace that preserves the model's predictive power. Using embeddings from ESM2, our adapter incorporates well-established properties, including secondary structure and hydropathy while maintaining high performance. We demonstrate the effectiveness of our approach across three protein-level classification tasks: prediction of extracellular vesicle association, identification of transmembrane helices, and prediction of aggregation propensity. PLM-X enables biological interpretation of model decisions without sacrificing accuracy, offering a generalisable solution for enhancing PLM interpretability across various downstream applications. This work addresses a critical need in computational biology by providing a bridge between powerful deep learning models and actionable biological insights. 

**Abstract (ZH)**: Protein语言模型（PLMs）通过生成多样化预测任务的强大序列表示，彻底改变了计算生物学。然而，其黑盒性质限制了生物解释和翻译为可操作的见解。我们提出了一个可解释的适配器层——PLM解释（PLM-X），通过将PLM嵌入分解为两个组件来弥合这一差距：基于已建立的生物化学特征的可解释子空间，以及保留模型预测能力的残差子空间。使用ESM2的嵌入，我们的适配器整合了包括二级结构和亲水性在内的已建立属性，同时保持高性能。我们在三个蛋白质级别分类任务中验证了我们方法的有效性：细胞外囊泡关联预测、跨膜螺旋鉴定以及聚集倾向预测。PLM-X在不牺牲准确性的情况下提供了模型决策的生物解释，为增强各种下游应用中PLM的解释性提供了通用解决方案。本工作通过在强大深度学习模型和可操作生物学洞察之间架起桥梁，满足了计算生物学中的一个关键需求。 

---
# Secure Text Mail Encryption with Generative Adversarial Networks 

**Title (ZH)**: 基于生成对抗网络的Secure Text Mail Encryption 

**Authors**: Alexej Schelle  

**Link**: [PDF](https://arxiv.org/pdf/2504.07140)  

**Abstract**: This work presents an encryption model based on Generative Adversarial Networks (GANs). Encryption of RTF-8 data is realized by dynamically generating decimal numbers that lead to the encryption and decryption of alphabetic strings in integer representation by simple addition rules, the modulus of the dimension of the considered alphabet. The binary numbers for the private dynamical keys correlate with the binary numbers of public reference keys from a mapping defined by the specific GAN configuration. For reversible encryption with bijective mapping between dynamic and reference keys as defined by the GAN encryptor with random combinations of NOT logical gates between bitwise subcomponents of the transmitted text signal, secure text encryption can be realized by transferring a GAN-encrypted public key with encrypted text from a sender to a receiver. Using the technique described above, secure text mail transfer can be realized from component-wise encryption of text mail strings with total key sizes of up to $10^{8}$ bits that define random decimal numbers obtained from the GAN. From the present model, we assert that encrypted texts can be transmitted more efficiently and securely than from RSA encryption, as long as users of the specific configuration of the GAN encryption model are unaware of the GAN encryptor circuit. 

**Abstract (ZH)**: 基于生成对抗网络（GANs）的加密模型 

---
# Sacred or Secular? Religious Bias in AI-Generated Financial Advice 

**Title (ZH)**: 神圣还是世俗？AI生成的金融建议中的宗教偏见 

**Authors**: Muhammad Salar Khan, Hamza Umer  

**Link**: [PDF](https://arxiv.org/pdf/2504.07118)  

**Abstract**: This study examines religious biases in AI-generated financial advice, focusing on ChatGPT's responses to financial queries. Using a prompt-based methodology and content analysis, we find that 50% of the financial emails generated by ChatGPT exhibit religious biases, with explicit biases present in both ingroup and outgroup interactions. While ingroup biases personalize responses based on religious alignment, outgroup biases introduce religious framing that may alienate clients or create ideological friction. These findings align with broader research on AI bias and suggest that ChatGPT is not merely reflecting societal biases but actively shaping financial discourse based on perceived religious identity. Using the Critical Algorithm Studies framework, we argue that ChatGPT functions as a mediator of financial narratives, selectively reinforcing religious perspectives. This study underscores the need for greater transparency, bias mitigation strategies, and regulatory oversight to ensure neutrality in AI-driven financial services. 

**Abstract (ZH)**: 本研究考察了AI生成的金融建议中的宗教偏见，重点分析了ChatGPT对金融查询的回应。通过基于提示的方法和内容分析，我们发现ChatGPT生成的50%的金融电子邮件表现出宗教偏见，其中既存在于内部群体间也存在于外部群体间的交互中。内部群体偏见基于宗教一致性个性化回应，外部群体偏见则引入宗教框架，可能 alienate 客户或引发意识形态摩擦。这些发现与更广泛的AI偏见研究相吻合，表明ChatGPT不仅反映了社会偏见，还在基于感知的宗教身份基础上主动塑造金融 discourse。利用批判性算法研究框架，我们argue ChatGPT作为金融叙事的中介，选择性地强化宗教视角。本研究强调了在AI驱动的金融服务中增强透明度、偏见缓解策略以及监管监督的必要性，以确保中立性。 

---
# OKRA: an Explainable, Heterogeneous, Multi-Stakeholder Job Recommender System 

**Title (ZH)**: OKRA：一种可解释的异质化多利益相关方求职推荐系统 

**Authors**: Roan Schellingerhout, Francesco Barile, Nava Tintarev  

**Link**: [PDF](https://arxiv.org/pdf/2504.07108)  

**Abstract**: The use of recommender systems in the recruitment domain has been labeled as 'high-risk' in recent legislation. As a result, strict requirements regarding explainability and fairness have been put in place to ensure proper treatment of all involved stakeholders. To allow for stakeholder-specific explainability, while also handling highly heterogeneous recruitment data, we propose a novel explainable multi-stakeholder job recommender system using graph neural networks: the Occupational Knowledge-based Recommender using Attention (OKRA). The proposed method is capable of providing both candidate- and company-side recommendations and explanations. We find that OKRA performs substantially better than six baselines in terms of nDCG for two datasets. Furthermore, we find that the tested models show a bias toward candidates and vacancies located in urban areas. Overall, our findings suggest that OKRA provides a balance between accuracy, explainability, and fairness. 

**Abstract (ZH)**: 在招聘领域使用推荐系统被近期的立法标记为“高风险”。因此，为了确保所有利益相关方的公正待遇，提出了严格的要求以保证解释性和公平性。为实现针对不同利益相关方的个性化解释，并处理高度异构的招聘数据，我们提出了一种基于图神经网络的 occupatioNal Knowledge-based Recommender using Attention (OKRA) 可解释多利益相关方求职推荐系统。该方法能够为求职者和公司提供推荐和解释。实验结果表明，与六种基线方法相比，OKRA 在两个数据集中 nDCG 方面表现显著更好。此外，我们发现测试模型对城市地区求职者和职位存在偏向。总体而言，我们的研究发现表明 OKRA 在准确性、解释性和公平性之间提供了良好的平衡。 

---
# Personalized Recommendation Models in Federated Settings: A Survey 

**Title (ZH)**: 联邦设置下的个性化推荐模型：一个综述 

**Authors**: Chunxu Zhang, Guodong Long, Zijian Zhang, Zhiwei Li, Honglei Zhang, Qiang Yang, Bo Yang  

**Link**: [PDF](https://arxiv.org/pdf/2504.07101)  

**Abstract**: Federated recommender systems (FedRecSys) have emerged as a pivotal solution for privacy-aware recommendations, balancing growing demands for data security and personalized experiences. Current research efforts predominantly concentrate on adapting traditional recommendation architectures to federated environments, optimizing communication efficiency, and mitigating security vulnerabilities. However, user personalization modeling, which is essential for capturing heterogeneous preferences in this decentralized and non-IID data setting, remains underexplored. This survey addresses this gap by systematically exploring personalization in FedRecSys, charting its evolution from centralized paradigms to federated-specific innovations. We establish a foundational definition of personalization in a federated setting, emphasizing personalized models as a critical solution for capturing fine-grained user preferences. The work critically examines the technical hurdles of building personalized FedRecSys and synthesizes promising methodologies to meet these challenges. As the first consolidated study in this domain, this survey serves as both a technical reference and a catalyst for advancing personalized FedRecSys research. 

**Abstract (ZH)**: 联邦推荐系统（FedRecSys）已成为一种关键的隐私感知推荐解决方案，平衡了日益增长的数据安全需求和个人化体验。当前的研究主要集中在将传统的推荐架构适应联邦环境，优化通信效率和缓解安全漏洞。然而，在这种分散且非IID数据设置中捕获异质偏好所需的核心个性化建模仍然尚未充分探索。本综述通过系统地探讨联邦环境中的个性化建模，从集中的范式到联邦特定的创新，填补了这一空白。我们为联邦环境中的个性化建立了一个基础定义，强调个性化模型是捕捉细粒度用户偏好的关键解决方案。本文批判性地评估了构建个性化FedRecSys的技术障碍，并总结了应对这些挑战的有前途的方法。作为该领域的首个综合研究，本综述既提供了一项技术参考，也成为推动个性化FedRecSys研究发展的催化剂。 

---
