# GenEAva: Generating Cartoon Avatars with Fine-Grained Facial Expressions from Realistic Diffusion-based Faces 

**Title (ZH)**: GenEAva: 从现实主义扩散基人脸生成细粒度面部表情的卡通 avatar 

**Authors**: Hao Yu, Rupayan Mallick, Margrit Betke, Sarah Adel Bargal  

**Link**: [PDF](https://arxiv.org/pdf/2504.07945)  

**Abstract**: Cartoon avatars have been widely used in various applications, including social media, online tutoring, and gaming. However, existing cartoon avatar datasets and generation methods struggle to present highly expressive avatars with fine-grained facial expressions and are often inspired from real-world identities, raising privacy concerns. To address these challenges, we propose a novel framework, GenEAva, for generating high-quality cartoon avatars with fine-grained facial expressions. Our approach fine-tunes a state-of-the-art text-to-image diffusion model to synthesize highly detailed and expressive facial expressions. We then incorporate a stylization model that transforms these realistic faces into cartoon avatars while preserving both identity and expression. Leveraging this framework, we introduce the first expressive cartoon avatar dataset, GenEAva 1.0, specifically designed to capture 135 fine-grained facial expressions, featuring 13,230 expressive cartoon avatars with a balanced distribution across genders, racial groups, and age ranges. We demonstrate that our fine-tuned model generates more expressive faces than the state-of-the-art text-to-image diffusion model SDXL. We also verify that the cartoon avatars generated by our framework do not include memorized identities from fine-tuning data. The proposed framework and dataset provide a diverse and expressive benchmark for future research in cartoon avatar generation. 

**Abstract (ZH)**: 基于细粒度面部表情生成的高质量卡通头像框架及数据集 

---
# AerialVG: A Challenging Benchmark for Aerial Visual Grounding by Exploring Positional Relations 

**Title (ZH)**: AerialVG: 通过探索位置关系的无人机视觉定位挑战基准 

**Authors**: Junli Liu, Qizhi Chen, Zhigang Wang, Yiwen Tang, Yiting Zhang, Chi Yan, Dong Wang, Xuelong Li, Bin Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2504.07836)  

**Abstract**: Visual grounding (VG) aims to localize target objects in an image based on natural language descriptions. In this paper, we propose AerialVG, a new task focusing on visual grounding from aerial views. Compared to traditional VG, AerialVG poses new challenges, \emph{e.g.}, appearance-based grounding is insufficient to distinguish among multiple visually similar objects, and positional relations should be emphasized. Besides, existing VG models struggle when applied to aerial imagery, where high-resolution images cause significant difficulties. To address these challenges, we introduce the first AerialVG dataset, consisting of 5K real-world aerial images, 50K manually annotated descriptions, and 103K objects. Particularly, each annotation in AerialVG dataset contains multiple target objects annotated with relative spatial relations, requiring models to perform comprehensive spatial reasoning. Furthermore, we propose an innovative model especially for the AerialVG task, where a Hierarchical Cross-Attention is devised to focus on target regions, and a Relation-Aware Grounding module is designed to infer positional relations. Experimental results validate the effectiveness of our dataset and method, highlighting the importance of spatial reasoning in aerial visual grounding. The code and dataset will be released. 

**Abstract (ZH)**: 基于无人机视角的视觉定位（AerialVG）：一个新的研究任务 

---
# Benchmarking Multi-Organ Segmentation Tools for Multi-Parametric T1-weighted Abdominal MRI 

**Title (ZH)**: 多参数T1加权腹部MRI多器官分割工具基准测试 

**Authors**: Nicole Tran, Anisa Prasad, Yan Zhuang, Tejas Sudharshan Mathai, Boah Kim, Sydney Lewis, Pritam Mukherjee, Jianfei Liu, Ronald M. Summers  

**Link**: [PDF](https://arxiv.org/pdf/2504.07729)  

**Abstract**: The segmentation of multiple organs in multi-parametric MRI studies is critical for many applications in radiology, such as correlating imaging biomarkers with disease status (e.g., cirrhosis, diabetes). Recently, three publicly available tools, such as MRSegmentator (MRSeg), TotalSegmentator MRI (TS), and TotalVibeSegmentator (VIBE), have been proposed for multi-organ segmentation in MRI. However, the performance of these tools on specific MRI sequence types has not yet been quantified. In this work, a subset of 40 volumes from the public Duke Liver Dataset was curated. The curated dataset contained 10 volumes each from the pre-contrast fat saturated T1, arterial T1w, venous T1w, and delayed T1w phases, respectively. Ten abdominal structures were manually annotated in these volumes. Next, the performance of the three public tools was benchmarked on this curated dataset. The results indicated that MRSeg obtained a Dice score of 80.7 $\pm$ 18.6 and Hausdorff Distance (HD) error of 8.9 $\pm$ 10.4 mm. It fared the best ($p < .05$) across the different sequence types in contrast to TS and VIBE. 

**Abstract (ZH)**: 多参数MRI研究中多个器官的分割对于放射学中的许多应用至关重要，例如将影像生物标志物与疾病状态（如肝硬化、糖尿病）相关联。近期提出了三种公开可用的工具，如MRSegmentator (MRSeg)、TotalSegmentator MRI (TS) 和TotalVibeSegmentator (VIBE)，以进行MRI中的多器官分割。然而，这些工具在特定MRI序列类型上的性能尚未量化。在这项工作中，选用了公共Duke Liver Dataset中的40个体积作为子集。该数据集包含分别来自预对比脂肪饱和T1、动脉T1w、静脉T1w和延迟T1w相位的10个体积，并在这些体积中手动标注了10个腹腔结构。随后，这三种公开工具在该制定的数据集上的性能进行了基准测试。结果显示，MRSeg的Dice分数为80.7 $\pm$ 18.6，Hausdorff距离误差为8.9 $\pm$ 10.4 mm，并且在不同序列类型中表现最佳（$p < .05$），优于TS和VIBE。 

---
# RASMD: RGB And SWIR Multispectral Driving Dataset for Robust Perception in Adverse Conditions 

**Title (ZH)**: RASMD: RGB和SWIR多光谱驾驶数据集在恶劣条件下的稳健感知 

**Authors**: Youngwan Jin, Michal Kovac, Yagiz Nalcakan, Hyeongjin Ju, Hanbin Song, Sanghyeop Yeo, Shiho Kim  

**Link**: [PDF](https://arxiv.org/pdf/2504.07603)  

**Abstract**: Current autonomous driving algorithms heavily rely on the visible spectrum, which is prone to performance degradation in adverse conditions like fog, rain, snow, glare, and high contrast. Although other spectral bands like near-infrared (NIR) and long-wave infrared (LWIR) can enhance vision perception in such situations, they have limitations and lack large-scale datasets and benchmarks. Short-wave infrared (SWIR) imaging offers several advantages over NIR and LWIR. However, no publicly available large-scale datasets currently incorporate SWIR data for autonomous driving. To address this gap, we introduce the RGB and SWIR Multispectral Driving (RASMD) dataset, which comprises 100,000 synchronized and spatially aligned RGB-SWIR image pairs collected across diverse locations, lighting, and weather conditions. In addition, we provide a subset for RGB-SWIR translation and object detection annotations for a subset of challenging traffic scenarios to demonstrate the utility of SWIR imaging through experiments on both object detection and RGB-to-SWIR image translation. Our experiments show that combining RGB and SWIR data in an ensemble framework significantly improves detection accuracy compared to RGB-only approaches, particularly in conditions where visible-spectrum sensors struggle. We anticipate that the RASMD dataset will advance research in multispectral imaging for autonomous driving and robust perception systems. 

**Abstract (ZH)**: 当前的自动驾驶算法高度依赖可见光谱，在雾、雨、雪、反光和高对比度等不良条件下性能易受损。尽管近红外（NIR）和长波红外（LWIR）等其他光谱带能在这些情况下增强视觉感知，但它们受限于缺乏大规模数据集和基准。短波红外（SWIR）成像相较于NIR和LWIR具有多项优势。然而，目前尚无公开的大规模数据集包含SWIR数据用于自动驾驶。为填补这一空白，我们介绍了RGB和SWIR多光谱驾驶（RASMD）数据集，该数据集包含100,000对同步且空间对齐的RGB-SWIR图像对，覆盖多种地理位置、光照和天气条件。此外，我们提供了一部分RGB-SWIR图像转换和物体检测注释，用于展示SWIR成像在物体检测和RGB-to-SWIR图像转换实验中的实用性。实验结果表明，在集成框架中结合RGB和SWIR数据显著提高了检测准确性，特别是在可见光谱传感器性能不佳的条件下。我们预计RASMD数据集将推动自动驾驶中多光谱成像的研究和稳健感知系统的进展。 

---
# Benchmarking Image Embeddings for E-Commerce: Evaluating Off-the Shelf Foundation Models, Fine-Tuning Strategies and Practical Trade-offs 

**Title (ZH)**: 电子商务中图像嵌入的基准测试：现成基础模型、微调策略及实际权衡的研究 

**Authors**: Urszula Czerwinska, Cenk Bircanoglu, Jeremy Chamoux  

**Link**: [PDF](https://arxiv.org/pdf/2504.07567)  

**Abstract**: We benchmark foundation models image embeddings for classification and retrieval in e-Commerce, evaluating their suitability for real-world applications. Our study spans embeddings from pre-trained convolutional and transformer models trained via supervised, self-supervised, and text-image contrastive learning. We assess full fine-tuning and transfer learning (top-tuning) on six diverse e-Commerce datasets: fashion, consumer goods, cars, food, and retail. Results show full fine-tuning consistently performs well, while text-image and self-supervised embeddings can match its performance with less training. While supervised embeddings remain stable across architectures, SSL and contrastive embeddings vary significantly, often benefiting from top-tuning. Top-tuning emerges as an efficient alternative to full fine-tuning, reducing computational costs. We also explore cross-tuning, noting its impact depends on dataset characteristics. Our findings offer practical guidelines for embedding selection and fine-tuning strategies, balancing efficiency and performance. 

**Abstract (ZH)**: 我们对标基础模型的图像嵌入在电子商务中的分类和检索性能，评估其适用于现实世界应用的 suitability。研究涵盖了通过监督学习、半监督学习和文本-图像对比学习预训练的卷积和变换模型的嵌入。我们在六个不同的电子商务数据集中评估了全程微调和迁移学习（顶微调）的效果：时尚、消费品、汽车、食品和零售。结果显示，全程微调的效果始终较好，而文本-图像和自我监督嵌入在较少的训练情况下可以达到类似效果。虽然监督嵌入在不同架构中表现出一致性，但自我监督和对比嵌入变化显著，通常可以从顶微调中获益。顶微调作为一种高效替代全程微调的解决方案，减少了计算成本。我们还探讨了跨微调，其影响取决于数据集的特性。我们的研究结果提供了嵌入选择和微调策略的实际指导，平衡了效率和性能。 

---
# Synthetic CT Generation from Time-of-Flight Non-Attenutaion-Corrected PET for Whole-Body PET Attenuation Correction 

**Title (ZH)**: 基于飞行时间未校正衰减的PET生成全身PET衰减校正的合成CT 

**Authors**: Weijie Chen, James Wang, Alan McMillan  

**Link**: [PDF](https://arxiv.org/pdf/2504.07450)  

**Abstract**: Positron Emission Tomography (PET) imaging requires accurate attenuation correction (AC) to account for photon loss due to tissue density variations. In PET/MR systems, computed tomography (CT), which offers a straightforward estimation of AC is not available. This study presents a deep learning approach to generate synthetic CT (sCT) images directly from Time-of-Flight (TOF) non-attenuation corrected (NAC) PET images, enhancing AC for PET/MR. We first evaluated models pre-trained on large-scale natural image datasets for a CT-to-CT reconstruction task, finding that the pre-trained model outperformed those trained solely on medical datasets. The pre-trained model was then fine-tuned using an institutional dataset of 35 TOF NAC PET and CT volume pairs, achieving the lowest mean absolute error (MAE) of 74.49 HU and highest peak signal-to-noise ratio (PSNR) of 28.66 dB within the body contour region. Visual assessments demonstrated improved reconstruction of both bone and soft tissue structures from TOF NAC PET images. This work highlights the effectiveness of using pre-trained deep learning models for medical image translation tasks. Future work will assess the impact of sCT on PET attenuation correction and explore additional neural network architectures and datasets to further enhance performance and practical applications in PET imaging. 

**Abstract (ZH)**: 基于深度学习的TOF非 attenuated PET图像生成合成CT以改进PET/MR衰减校正 

---
# FAIR-SIGHT: Fairness Assurance in Image Recognition via Simultaneous Conformal Thresholding and Dynamic Output Repair 

**Title (ZH)**: FAIR-SIGHT：通过同时进行同尺度阈值调整和动态输出修复实现图像识别中的公平性保障 

**Authors**: Arya Fayyazi, Mehdi Kamal, Massoud Pedram  

**Link**: [PDF](https://arxiv.org/pdf/2504.07395)  

**Abstract**: We introduce FAIR-SIGHT, an innovative post-hoc framework designed to ensure fairness in computer vision systems by combining conformal prediction with a dynamic output repair mechanism. Our approach calculates a fairness-aware non-conformity score that simultaneously assesses prediction errors and fairness violations. Using conformal prediction, we establish an adaptive threshold that provides rigorous finite-sample, distribution-free guarantees. When the non-conformity score for a new image exceeds the calibrated threshold, FAIR-SIGHT implements targeted corrective adjustments, such as logit shifts for classification and confidence recalibration for detection, to reduce both group and individual fairness disparities, all without the need for retraining or having access to internal model parameters. Comprehensive theoretical analysis validates our method's error control and convergence properties. At the same time, extensive empirical evaluations on benchmark datasets show that FAIR-SIGHT significantly reduces fairness disparities while preserving high predictive performance. 

**Abstract (ZH)**: FAIR-SIGHT：一种结合一致预测和动态输出修复机制的后验公平性保障框架 

---
# Objaverse++: Curated 3D Object Dataset with Quality Annotations 

**Title (ZH)**: Objaverse++: 经质量标注的3D对象数据集 

**Authors**: Chendi Lin, Heshan Liu, Qunshu Lin, Zachary Bright, Shitao Tang, Yihui He, Minghao Liu, Ling Zhu, Cindy Le  

**Link**: [PDF](https://arxiv.org/pdf/2504.07334)  

**Abstract**: This paper presents Objaverse++, a curated subset of Objaverse enhanced with detailed attribute annotations by human experts. Recent advances in 3D content generation have been driven by large-scale datasets such as Objaverse, which contains over 800,000 3D objects collected from the Internet. Although Objaverse represents the largest available 3D asset collection, its utility is limited by the predominance of low-quality models. To address this limitation, we manually annotate 10,000 3D objects with detailed attributes, including aesthetic quality scores, texture color classifications, multi-object composition flags, transparency characteristics, etc. Then, we trained a neural network capable of annotating the tags for the rest of the Objaverse dataset. Through experiments and a user study on generation results, we demonstrate that models pre-trained on our quality-focused subset achieve better performance than those trained on the larger dataset of Objaverse in image-to-3D generation tasks. In addition, by comparing multiple subsets of training data filtered by our tags, our results show that the higher the data quality, the faster the training loss converges. These findings suggest that careful curation and rich annotation can compensate for the raw dataset size, potentially offering a more efficient path to develop 3D generative models. We release our enhanced dataset of approximately 500,000 curated 3D models to facilitate further research on various downstream tasks in 3D computer vision. In the near future, we aim to extend our annotations to cover the entire Objaverse dataset. 

**Abstract (ZH)**: Objaverse++：由人力专家详细标注的精选子集 

---
# Identifying regions of interest in whole slide images of renal cell carcinoma 

**Title (ZH)**: 在肾细胞癌全视野图像中识别感兴趣区域 

**Authors**: Mohammed Lamine Benomar, Nesma Settouti, Eric Debreuve, Xavier Descombes, Damien Ambrosetti  

**Link**: [PDF](https://arxiv.org/pdf/2504.07313)  

**Abstract**: The histopathological images contain a huge amount of information, which can make diagnosis an extremely timeconsuming and tedious task. In this study, we developed a completely automated system to detect regions of interest (ROIs) in whole slide images (WSI) of renal cell carcinoma (RCC), to reduce time analysis and assist pathologists in making more accurate decisions. The proposed approach is based on an efficient texture descriptor named dominant rotated local binary pattern (DRLBP) and color transformation to reveal and exploit the immense texture variability at the microscopic high magnifications level. Thereby, the DRLBPs retain the structural information and utilize the magnitude values in a local neighborhood for more discriminative power. For the classification of the relevant ROIs, feature extraction of WSIs patches was performed on the color channels separately to form the histograms. Next, we used the most frequently occurring patterns as a feature selection step to discard non-informative features. The performances of different classifiers on a set of 1800 kidney cancer patches originating from 12 whole slide images were compared and evaluated. Furthermore, the small size of the image dataset allows to investigate deep learning approach based on transfer learning for image patches classification by using deep features and fine-tuning methods. High recognition accuracy was obtained and the classifiers are efficient, the best precision result was 99.17% achieved with SVM. Moreover, transfer learning models perform well with comparable performance, and the highest precision using ResNet-50 reached 98.50%. The proposed approach results revealed a very efficient image classification and demonstrated efficacy in identifying ROIs. This study presents an automatic system to detect regions of interest relevant to the diagnosis of kidney cancer in whole slide histopathology images. 

**Abstract (ZH)**: 一种基于DRLBP和颜色转换的自动肾细胞 carcinoma感兴趣区域检测方法：应用于全-slide病理图像的诊断辅助系统 

---
# RP-SAM2: Refining Point Prompts for Stable Surgical Instrument Segmentation 

**Title (ZH)**: RP-SAM2: 优化点提示以实现稳定的手术器械分割 

**Authors**: Nuren Zhaksylyk, Ibrahim Almakky, Jay Paranjape, S. Swaroop Vedula, Shameema Sikder, Vishal M. Patel, Mohammad Yaqub  

**Link**: [PDF](https://arxiv.org/pdf/2504.07117)  

**Abstract**: Accurate surgical instrument segmentation is essential in cataract surgery for tasks such as skill assessment and workflow optimization. However, limited annotated data makes it difficult to develop fully automatic models. Prompt-based methods like SAM2 offer flexibility yet remain highly sensitive to the point prompt placement, often leading to inconsistent segmentations. We address this issue by introducing RP-SAM2, which incorporates a novel shift block and a compound loss function to stabilize point prompts. Our approach reduces annotator reliance on precise point positioning while maintaining robust segmentation capabilities. Experiments on the Cataract1k dataset demonstrate that RP-SAM2 improves segmentation accuracy, with a 2% mDSC gain, a 21.36% reduction in mHD95, and decreased variance across random single-point prompt results compared to SAM2. Additionally, on the CaDIS dataset, pseudo masks generated by RP-SAM2 for fine-tuning SAM2's mask decoder outperformed those generated by SAM2. These results highlight RP-SAM2 as a practical, stable and reliable solution for semi-automatic instrument segmentation in data-constrained medical settings. The code is available at this https URL. 

**Abstract (ZH)**: 准确的手术器械分割对于白内障手术中的技能评估和工作流程优化至关重要。然而，标注数据的限制使得开发完全自动的模型变得困难。SAM2等基于提示的方法虽具有灵活性，但对点提示位置的高度敏感性常导致分割结果不一致。我们通过引入RP-SAM2解决了这一问题，该方法结合了一个新型移位块和复合损失函数以稳定点提示。我们的方法减少了对精确点定位的标注员依赖，同时保持了稳健的分割能力。实验结果表明，RP-SAM2在Cataract1k数据集上的分割准确率得到提高，mDSC提升2%，mHD95减少21.36%，并且随机单点提示结果的方差降低。此外，在CaDIS数据集上，RP-SAM2生成的伪掩码在Fine-tuning SAM2的掩码解码器时表现出色，优于SAM2生成的掩码。这些结果突显了RP-SAM2在数据受限医疗环境下的实用、稳定和可靠半自动器械分割解决方案。代码可在以下链接获取：this https URL。 

---
