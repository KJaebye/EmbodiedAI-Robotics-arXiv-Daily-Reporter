{'arxiv_id': 'arXiv:2505.11350', 'title': 'Search-TTA: A Multimodal Test-Time Adaptation Framework for Visual Search in the Wild', 'authors': 'Derek Ming Siang Tan, Shailesh, Boyang Liu, Alok Raj, Qi Xuan Ang, Weiheng Dai, Tanishq Duhan, Jimmy Chiun, Yuhong Cao, Florian Shkurti, Guillaume Sartoretti', 'link': 'https://arxiv.org/abs/2505.11350', 'abstract': "To perform autonomous visual search for environmental monitoring, a robot may leverage satellite imagery as a prior map. This can help inform coarse, high-level search and exploration strategies, even when such images lack sufficient resolution to allow fine-grained, explicit visual recognition of targets. However, there are some challenges to overcome with using satellite images to direct visual search. For one, targets that are unseen in satellite images are underrepresented (compared to ground images) in most existing datasets, and thus vision models trained on these datasets fail to reason effectively based on indirect visual cues. Furthermore, approaches which leverage large Vision Language Models (VLMs) for generalization may yield inaccurate outputs due to hallucination, leading to inefficient search. To address these challenges, we introduce Search-TTA, a multimodal test-time adaptation framework that can accept text and/or image input. First, we pretrain a remote sensing image encoder to align with CLIP's visual encoder to output probability distributions of target presence used for visual search. Second, our framework dynamically refines CLIP's predictions during search using a test-time adaptation mechanism. Through a feedback loop inspired by Spatial Poisson Point Processes, gradient updates (weighted by uncertainty) are used to correct (potentially inaccurate) predictions and improve search performance. To validate Search-TTA's performance, we curate a visual search dataset based on internet-scale ecological data. We find that Search-TTA improves planner performance by up to 9.7%, particularly in cases with poor initial CLIP predictions. It also achieves comparable performance to state-of-the-art VLMs. Finally, we deploy Search-TTA on a real UAV via hardware-in-the-loop testing, by simulating its operation within a large-scale simulation that provides onboard sensing.", 'abstract_zh': '用于环境监测的自主视觉搜索中，机器人可以利用卫星影像作为先验地图。这可以帮助制定粗略的高阶搜索和探索策略，即使这些影像缺乏足够的分辨率以进行精细的明确视觉目标识别。然而，使用卫星影像引导视觉搜索时仍存在一些挑战。首先，在大多数现有数据集中，未在卫星影像中观察到的目标相比于地面影像来说被严重低估，因此基于这些数据集训练的视觉模型难以根据间接视觉线索进行有效的推理。此外，依赖大型视觉语言模型（VLMs）进行泛化的方法可能会因为幻觉而导致不准确的输出，从而导致搜索效率低下。为解决这些挑战，我们引入了Search-TTA，这是一种多模态测试时间自适应框架，能够接受文本和/或图像输入。首先，我们预训练一个遥感图像编码器使其与CLIP的视觉编码器对齐，以输出用于视觉搜索的目标存在概率分布。其次，我们的框架在搜索过程中使用测试时间自适应机制动态细化CLIP的预测。通过一个深受空间泊松点过程启发的反馈循环，加权不确定性的梯度更新被用于纠正（可能不准确的）预测并提高搜索性能。为了验证Search-TTA的表现，我们基于互联网规模的生态学数据构建了一个视觉搜索数据集。我们发现，Search-TTA可以将规划者性能提高高达9.7%，特别是在初始CLIP预测较差的情况下。此外，它在性能上可以与最先进的视觉语言模型相媲美。最后，我们通过在大规模仿真中模拟其操作并在提供机载感测的硬件环境中部署Search-TTA，实现了其在实际无人机上的应用。', 'title_zh': 'Search-TTA：视觉检索中的多模态测试时自适应框架'}
{'arxiv_id': 'arXiv:2505.11066', 'title': 'A Multi-modal Fusion Network for Terrain Perception Based on Illumination Aware', 'authors': 'Rui Wang, Shichun Yang, Yuyi Chen, Zhuoyang Li, Zexiang Tong, Jianyi Xu, Jiayi Lu, Xinjie Feng, Yaoguang Cao', 'link': 'https://arxiv.org/abs/2505.11066', 'abstract': 'Road terrains play a crucial role in ensuring the driving safety of autonomous vehicles (AVs). However, existing sensors of AVs, including cameras and Lidars, are susceptible to variations in lighting and weather conditions, making it challenging to achieve real-time perception of road conditions. In this paper, we propose an illumination-aware multi-modal fusion network (IMF), which leverages both exteroceptive and proprioceptive perception and optimizes the fusion process based on illumination features. We introduce an illumination-perception sub-network to accurately estimate illumination features. Moreover, we design a multi-modal fusion network which is able to dynamically adjust weights of different modalities according to illumination features. We enhance the optimization process by pre-training of the illumination-perception sub-network and incorporating illumination loss as one of the training constraints. Extensive experiments demonstrate that the IMF shows a superior performance compared to state-of-the-art methods. The comparison results with single modality perception methods highlight the comprehensive advantages of multi-modal fusion in accurately perceiving road terrains under varying lighting conditions. Our dataset is available at: this https URL.', 'abstract_zh': '道路地形在确保自动驾驶车辆（AVs）行驶安全中起着重要作用。然而，现有AV传感器，包括相机和激光雷达，对光照和天气条件的变化较为敏感，使得实时感知道路条件具有挑战性。本文提出了一种照明感知多模态融合网络（IMF），利用外部和内部感知，并基于照明特征优化融合过程。我们引入了一个照明感知子网络来准确估计照明特征。此外，我们设计了一个多模态融合网络，能够根据照明特征动态调整不同模态的权重。我们通过照明感知子网络的预训练和将照明损失纳入训练约束来增强优化过程。广泛的实验表明，IMF展示了优于现有先进方法的性能。与单一模态感知方法的对比结果突显了在变化光照条件下多模态融合在准确感知道路地形方面的综合优势。我们的数据集可在以下链接获取：this https URL。', 'title_zh': '基于照明感知的多模态融合网络用于地形感知'}
{'arxiv_id': 'arXiv:2505.10887', 'title': 'InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer Interaction', 'authors': 'Bin Lei, Weitai Kang, Zijian Zhang, Winson Chen, Xi Xie, Shan Zuo, Mimi Xie, Ali Payani, Mingyi Hong, Yan Yan, Caiwen Ding', 'link': 'https://arxiv.org/abs/2505.10887', 'abstract': 'This paper introduces \\textsc{InfantAgent-Next}, a generalist agent capable of interacting with computers in a multimodal manner, encompassing text, images, audio, and video. Unlike existing approaches that either build intricate workflows around a single large model or only provide workflow modularity, our agent integrates tool-based and pure vision agents within a highly modular architecture, enabling different models to collaboratively solve decoupled tasks in a step-by-step manner. Our generality is demonstrated by our ability to evaluate not only pure vision-based real-world benchmarks (i.e., OSWorld), but also more general or tool-intensive benchmarks (e.g., GAIA and SWE-Bench). Specifically, we achieve $\\mathbf{7.27\\%}$ accuracy on OSWorld, higher than Claude-Computer-Use. Codes and evaluation scripts are open-sourced at this https URL.', 'abstract_zh': '这篇论文介绍了\\textsc{InfantAgent-Next}，这是一种能够以多模态方式与计算机交互的通用代理，涵盖了文本、图像、音频和视频。不同于现有方法要么围绕单一大型模型构建复杂的 workflow，要么仅提供 workflow 模块化，我们的代理在高度模块化的架构中集成了基于工具和纯视觉的代理，使不同的模型能够以分步方式协作解决解耦的任务。我们通过在纯视觉基础的真实世界基准（如 OSWorld）以及更通用或工具密集型基准（如 GAIA 和 SWE-Bench）上的评估来展示其通用性。具体而言，我们在 OSWorld 上达到了 $\\mathbf{7.27\\%}$ 的准确率，高于 Claude-Computer-Use。代码和评估脚本已在此 <https://> 开源。', 'title_zh': 'InfantAgent-Next: 一种多模态通用代理agents用于自动化计算机交互'}
{'arxiv_id': 'arXiv:2505.11436', 'title': 'GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art', 'authors': 'Chenkai Zhang, Yiming Lei, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang', 'link': 'https://arxiv.org/abs/2505.11436', 'abstract': "Video Comment Art enhances user engagement by providing creative content that conveys humor, satire, or emotional resonance, requiring a nuanced and comprehensive grasp of cultural and contextual subtleties. Although Multimodal Large Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated strong reasoning abilities in STEM tasks (e.g. mathematics and coding), they still struggle to generate creative expressions such as resonant jokes and insightful satire. Moreover, existing benchmarks are constrained by their limited modalities and insufficient categories, hindering the exploration of comprehensive creativity in video-based Comment Art creation. To address these limitations, we introduce GODBench, a novel benchmark that integrates video and text modalities to systematically evaluate MLLMs' abilities to compose Comment Art. Furthermore, inspired by the propagation patterns of waves in physics, we propose Ripple of Thought (RoT), a multi-step reasoning framework designed to enhance the creativity of MLLMs. Extensive experiments reveal that existing MLLMs and CoT methods still face significant challenges in understanding and generating creative video comments. In contrast, RoT provides an effective approach to improve creative composing, highlighting its potential to drive meaningful advancements in MLLM-based creativity. GODBench is publicly available at this https URL.", 'abstract_zh': '视频评论艺术通过提供传递幽默、讽刺或情感共鸣的创意内容来增强用户参与度，需要对文化与情境的细微差异有精深的理解。尽管多模态大语言模型（MLLMs）和链式思考（CoT）在STEM任务（如数学和编码）中展现了强大的推理能力，但在生成富有创意的表达（如共鸣的笑话和深刻的讽刺）方面仍然力有未逮。此外，当前的基准受限于其有限的模态和不足的分类，阻碍了在基于视频的评论艺术创作中进行全面创意探索。为克服这些限制，我们引入了GODBench，这是一种新颖的基准，结合了视频和文本模态，系统性地评估MLLMs生成评论艺术的能力。此外，受到物理波传播模式的启发，我们提出了波纹思考（RoT）多步推理框架，旨在增强MLLMs的创造力。大量实验证明，现有MLLMs和CoT方法在理解和生成创意视频评论方面仍然面临重大挑战。相比之下，RoT提供了一种有效的方法来提高创造性写作水平，突显了它在基于MLLM的创意发展中巨大潜力。GODBench已在以下网址公开发布：this https URL。', 'title_zh': 'GODBench：视频评论艺术中多模态大型语言模型的基准测试'}
{'arxiv_id': 'arXiv:2505.11404', 'title': 'Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner', 'authors': 'Wenchuan Zhang, Penghao Zhang, Jingru Guo, Tao Cheng, Jie Chen, Shuwan Zhang, Zhang Zhang, Yuhao Yi, Hong Bu', 'link': 'https://arxiv.org/abs/2505.11404', 'abstract': 'Recent advances in vision language models (VLMs) have enabled broad progress in the general medical field. However, pathology still remains a more challenging subdomain, with current pathology specific VLMs exhibiting limitations in both diagnostic accuracy and reasoning plausibility. Such shortcomings are largely attributable to the nature of current pathology datasets, which are primarily composed of image description pairs that lack the depth and structured diagnostic paradigms employed by real world pathologists. In this study, we leverage pathology textbooks and real world pathology experts to construct high-quality, reasoning-oriented datasets. Building on this, we introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs for knowledge infusion; (2) supervised fine-tuning on 500k high-quality Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement learning using Group Relative Policy Optimization and Decoupled Clip and Dynamic sAmpling Policy Optimization strategies for multimodal reasoning quality refinement. To further assess the alignment quality of our dataset, we propose PathoCLIP, trained on the same figure-caption corpus used for continued pretraining. Comprehensive experimental results demonstrate that both PathoCLIP and Patho-R1 achieve robust performance across a wide range of pathology-related tasks, including zero-shot classification, cross-modal retrieval, Visual Question Answering, and Multiple Choice Question. Our project is available at the Patho-R1 repository: this https URL.', 'abstract_zh': 'Recent Advances in Vision Language Models for Pathology: Leveraging High-Quality Datasets and Reasoning-Oriented Training for Enhanced Diagnostic Accuracy and Reasoning Plausibility', 'title_zh': 'Patho-R1：一种基于多模态强化学习的病理专家推理器'}
{'arxiv_id': 'arXiv:2505.11326', 'title': 'Temporally-Grounded Language Generation: A Benchmark for Real-Time Vision-Language Models', 'authors': 'Keunwoo Peter Yu, Joyce Chai', 'link': 'https://arxiv.org/abs/2505.11326', 'abstract': 'Vision-language models (VLMs) have shown remarkable progress in offline tasks such as image captioning and video question answering. However, real-time interactive environments impose new demands on VLMs, requiring them to generate utterances that are not only semantically accurate but also precisely timed. We identify two core capabilities necessary for such settings -- $\\textit{perceptual updating}$ and $\\textit{contingency awareness}$ -- and propose a new benchmark task, $\\textbf{Temporally-Grounded Language Generation (TGLG)}$, to evaluate them. TGLG requires models to generate utterances in response to streaming video such that both content and timing align with dynamic visual input. To support this benchmark, we curate evaluation datasets from sports broadcasting and egocentric human interaction domains, and introduce a new metric, $\\textbf{TRACE}$, to evaluate TGLG by jointly measuring semantic similarity and temporal alignment. Finally, we present $\\textbf{Vision-Language Model with Time-Synchronized Interleaving (VLM-TSI)}$, a model that interleaves visual and linguistic tokens in a time-synchronized manner, enabling real-time language generation without relying on turn-based assumptions. Experimental results show that VLM-TSI significantly outperforms a strong baseline, yet overall performance remains modest -- highlighting the difficulty of TGLG and motivating further research in real-time VLMs. Code and data available $\\href{this https URL}{here}$.', 'abstract_zh': '视觉语言模型在实时互动环境中的时间接地语句生成', 'title_zh': '时间驱动的语言生成：实时视觉-语言模型基准'}
{'arxiv_id': 'arXiv:2505.11217', 'title': 'Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI models in Sound Localization', 'authors': 'Yanhao Jia, Ji Xie, S Jivaganesh, Hao Li, Xu Wu, Mengmi Zhang', 'link': 'https://arxiv.org/abs/2505.11217', 'abstract': 'Imagine hearing a dog bark and turning toward the sound only to see a parked car, while the real, silent dog sits elsewhere. Such sensory conflicts test perception, yet humans reliably resolve them by prioritizing sound over misleading visuals. Despite advances in multimodal AI integrating vision and audio, little is known about how these systems handle cross-modal conflicts or whether they favor one modality. In this study, we systematically examine modality bias and conflict resolution in AI sound localization. We assess leading multimodal models and benchmark them against human performance in psychophysics experiments across six audiovisual conditions, including congruent, conflicting, and absent cues. Humans consistently outperform AI, demonstrating superior resilience to conflicting or missing visuals by relying on auditory information. In contrast, AI models often default to visual input, degrading performance to near chance levels. To address this, we finetune a state-of-the-art model using a stereo audio-image dataset generated via 3D simulations. Even with limited training data, the refined model surpasses existing benchmarks. Notably, it also mirrors human-like horizontal localization bias favoring left-right precision-likely due to the stereo audio structure reflecting human ear placement. These findings underscore how sensory input quality and system architecture shape multimodal representation accuracy.', 'abstract_zh': '多模态AI中的模态偏向与冲突解决研究', 'title_zh': 'Seeing Sound, Hearing Sight: 探索AI模型在声音定位中的模态偏差与冲突'}
{'arxiv_id': 'arXiv:2505.11131', 'title': 'One Image is Worth a Thousand Words: A Usability Preservable Text-Image Collaborative Erasing Framework', 'authors': 'Feiran Li, Qianqian Xu, Shilong Bao, Zhiyong Yang, Xiaochun Cao, Qingming Huang', 'link': 'https://arxiv.org/abs/2505.11131', 'abstract': 'Concept erasing has recently emerged as an effective paradigm to prevent text-to-image diffusion models from generating visually undesirable or even harmful content. However, current removal methods heavily rely on manually crafted text prompts, making it challenging to achieve a high erasure (efficacy) while minimizing the impact on other benign concepts (usability). In this paper, we attribute the limitations to the inherent gap between the text and image modalities, which makes it hard to transfer the intricately entangled concept knowledge from text prompts to the image generation process. To address this, we propose a novel solution by directly integrating visual supervision into the erasure process, introducing the first text-image Collaborative Concept Erasing (Co-Erasing) framework. Specifically, Co-Erasing describes the concept jointly by text prompts and the corresponding undesirable images induced by the prompts, and then reduces the generating probability of the target concept through negative guidance. This approach effectively bypasses the knowledge gap between text and image, significantly enhancing erasure efficacy. Additionally, we design a text-guided image concept refinement strategy that directs the model to focus on visual features most relevant to the specified text concept, minimizing disruption to other benign concepts. Finally, comprehensive experiments suggest that Co-Erasing outperforms state-of-the-art erasure approaches significantly with a better trade-off between efficacy and usability. Codes are available at this https URL.', 'abstract_zh': '视觉监督引导的文本-图像协作概念消除框架', 'title_zh': '一张图片胜过千言万语：一种可保留可用性的文本-图像协作擦除框架'}
{'arxiv_id': 'arXiv:2505.11109', 'title': 'MAVOS-DD: Multilingual Audio-Video Open-Set Deepfake Detection Benchmark', 'authors': 'Florinel-Alin Croitoru, Vlad Hondru, Marius Popescu, Radu Tudor Ionescu, Fahad Shahbaz Khan, Mubarak Shah', 'link': 'https://arxiv.org/abs/2505.11109', 'abstract': 'We present the first large-scale open-set benchmark for multilingual audio-video deepfake detection. Our dataset comprises over 250 hours of real and fake videos across eight languages, with 60% of data being generated. For each language, the fake videos are generated with seven distinct deepfake generation models, selected based on the quality of the generated content. We organize the training, validation and test splits such that only a subset of the chosen generative models and languages are available during training, thus creating several challenging open-set evaluation setups. We perform experiments with various pre-trained and fine-tuned deepfake detectors proposed in recent literature. Our results show that state-of-the-art detectors are not currently able to maintain their performance levels when tested in our open-set scenarios. We publicly release our data and code at: this https URL.', 'abstract_zh': '我们首次提出了一个大规模开放集多语言音频-视频深度假信息检测基准。', 'title_zh': 'MAVOS-DD: 多语言音频-视频开放集深度假信息检测基准'}
{'arxiv_id': 'arXiv:2505.10604', 'title': 'MIRAGE: A Multi-modal Benchmark for Spatial Perception, Reasoning, and Intelligence', 'authors': 'Chonghan Liu, Haoran Wang, Felix Henry, Pu Miao, Yajie Zhang, Yu Zhao, Peiran Wu', 'link': 'https://arxiv.org/abs/2505.10604', 'abstract': "Spatial perception and reasoning are core components of human cognition, encompassing object recognition, spatial relational understanding, and dynamic reasoning. Despite progress in computer vision, existing benchmarks reveal significant gaps in models' abilities to accurately recognize object attributes and reason about spatial relationships, both essential for dynamic reasoning. To address these limitations, we propose MIRAGE, a multi-modal benchmark designed to evaluate models' capabilities in Counting (object attribute recognition), Relation (spatial relational reasoning), and Counting with Relation. Through diverse and complex scenarios requiring fine-grained recognition and reasoning, MIRAGE highlights critical limitations in state-of-the-art models, underscoring the need for improved representations and reasoning frameworks. By targeting these foundational abilities, MIRAGE provides a pathway toward spatiotemporal reasoning in future research.", 'abstract_zh': '空间感知与推理是人类认知的核心组成部分，包括物体识别、空间关系理解以及动态推理。尽管计算机视觉取得了进展，现有基准数据集仍揭示了模型在准确识别物体属性和推理空间关系方面的显著不足，这两者对于动态推理至关重要。为解决这些局限性，我们提出MIRAGE，一个多模态基准数据集，旨在评估模型在计数（物体属性识别）、关系（空间关系推理）以及计数与关系方面的能力。通过涉及精细识别和推理的复杂场景，MIRAGE 突显了先进模型的关键局限性，强调了改进表示和推理框架的必要性。通过针对这些基础能力，MIRAGE 为未来研究向时空推理方向发展提供了途径。', 'title_zh': 'MIRAGE：多模态空间感知、推理与智能基准'}
