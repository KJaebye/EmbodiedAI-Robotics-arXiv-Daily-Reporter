{'arxiv_id': 'arXiv:2505.11495', 'title': 'Bracing for Impact: Robust Humanoid Push Recovery and Locomotion with Reduced Order Models', 'authors': 'Lizhi Yang, Blake Werner, Adrian B.Ghansah, Aaron D. Ames', 'link': 'https://arxiv.org/abs/2505.11495', 'abstract': "Push recovery during locomotion will facilitate the deployment of humanoid robots in human-centered environments. In this paper, we present a unified framework for walking control and push recovery for humanoid robots, leveraging the arms for push recovery while dynamically walking. The key innovation is to use the environment, such as walls, to facilitate push recovery by combining Single Rigid Body model predictive control (SRB-MPC) with Hybrid Linear Inverted Pendulum (HLIP) dynamics to enable robust locomotion, push detection, and recovery by utilizing the robot's arms to brace against such walls and dynamically adjusting the desired contact forces and stepping patterns. Extensive simulation results on a humanoid robot demonstrate improved perturbation rejection and tracking performance compared to HLIP alone, with the robot able to recover from pushes up to 100N for 0.2s while walking at commanded speeds up to 0.5m/s. Robustness is further validated in scenarios with angled walls and multi-directional pushes.", 'abstract_zh': '在人本环境中，步行过程中的推力恢复将促进类人机器人部署。本文提出了一种结合使用双臂进行推力恢复的类人机器人步行控制和推力恢复统一框架，利用单一刚体模型预测控制（SRB-MPC）与混合线性倒 pendulum（HLIP）动力学结合技术，在动态行走过程中利用环境（如墙壁）辅助推力恢复，通过机器人手臂支撑墙壁并动态调整期望接触力和步态模式，实现稳健的行走、推力检测和恢复。仿真结果表明，与仅使用HLIP相比，该方法具有更好的扰动 rejection 和跟踪性能，在行走速度高达0.5m/s的情况下，机器人能够从最大100N的推力中恢复过来。鲁棒性还在斜墙和多方向推力的场景中得到验证。', 'title_zh': '预判冲击：基于降阶模型的鲁棒人形推倒恢复与移动控制'}
{'arxiv_id': 'arXiv:2505.11494', 'title': 'SHIELD: Safety on Humanoids via CBFs In Expectation on Learned Dynamics', 'authors': 'Lizhi Yang, Blake Werner, Ryan K. Cosner, David Fridovich-Keil, Preston Culbertson, Aaron D. Ames', 'link': 'https://arxiv.org/abs/2505.11494', 'abstract': "Robot learning has produced remarkably effective ``black-box'' controllers for complex tasks such as dynamic locomotion on humanoids. Yet ensuring dynamic safety, i.e., constraint satisfaction, remains challenging for such policies. Reinforcement learning (RL) embeds constraints heuristically through reward engineering, and adding or modifying constraints requires retraining. Model-based approaches, like control barrier functions (CBFs), enable runtime constraint specification with formal guarantees but require accurate dynamics models. This paper presents SHIELD, a layered safety framework that bridges this gap by: (1) training a generative, stochastic dynamics residual model using real-world data from hardware rollouts of the nominal controller, capturing system behavior and uncertainties; and (2) adding a safety layer on top of the nominal (learned locomotion) controller that leverages this model via a stochastic discrete-time CBF formulation enforcing safety constraints in probability. The result is a minimally-invasive safety layer that can be added to the existing autonomy stack to give probabilistic guarantees of safety that balance risk and performance. In hardware experiments on an Unitree G1 humanoid, SHIELD enables safe navigation (obstacle avoidance) through varied indoor and outdoor environments using a nominal (unknown) RL controller and onboard perception.", 'abstract_zh': 'SHIELD：一种分层安全框架以实现动态安全的机器人学习控制器', 'title_zh': 'SHIELD: 基于学习的动力学期望值下的CBFs安全性保障用于类人机器人'}
{'arxiv_id': 'arXiv:2505.11476', 'title': 'UMArm: Untethered, Modular, Wearable, Soft Pneumatic Arm', 'authors': 'Runze Zuo, Dong Heon Han, Richard Li, Saima Jamal, Daniel Bruder', 'link': 'https://arxiv.org/abs/2505.11476', 'abstract': "Robotic arms are essential to modern industries, however, their adaptability to unstructured environments remains limited. Soft robotic arms, particularly those actuated pneumatically, offer greater adaptability in unstructured environments and enhanced safety for human-robot interaction. However, current pneumatic soft arms are constrained by limited degrees of freedom, precision, payload capacity, and reliance on bulky external pressure regulators. In this work, a novel pneumatically driven rigid-soft hybrid arm, ``UMArm'', is presented. The shortcomings of pneumatically actuated soft arms are addressed by densely integrating high-force-to-weight-ratio, self-regulated McKibben actuators onto a lightweight rigid spine structure. The modified McKibben actuators incorporate valves and controllers directly inside, eliminating the need for individual pressure lines and external regulators, significantly reducing system weight and complexity. Full untethered operation, high payload capacity, precision, and directionally tunable compliance are achieved by the UMArm. Portability is demonstrated through a wearable assistive arm experiment, and versatility is showcased by reconfiguring the system into an inchworm robot. The results of this work show that the high-degree-of-freedom, external-regulator-free pneumatically driven arm systems like the UMArm possess great potential for real-world unstructured environments.", 'abstract_zh': '软体机器人手臂在现代工业中至关重要，然而它们在非结构化环境中的适应性仍然有限。气动驱动的软体机器人手臂，特别是在非结构化环境中提供了更大的适应性，并增强了与机器人的人机交互安全性。然而，当前的气动软体手臂受到自由度有限、精度不足、承载能力有限以及需要体积较大的外部压力调节器的限制。本文提出了一种新型气动驱动的刚柔混合手臂“UMArm”。通过紧密集成高功率重量比、自我调节的麦基宾执行器到轻质刚性脊柱结构上，解决了气动驱动软体手臂的不足之处。修改后的麦基宾执行器在内部集成了阀门和控制器，消除了单独的压力管路和外部调节器的需要，大幅减轻了系统的重量和复杂性。UMArm 实现了全解耦操作、高承载能力、高精度和方向可调的顺应性。通过穿戴辅助手臂实验展示了其便携性，并通过重新配置系统作为inchworm机器人展示了其多功能性。研究结果表明，如UMArm这样的外部调节器自由的高自由度气动驱动手臂系统在实际非结构化环境中具有巨大潜力。', 'title_zh': 'UM Arm: 无缆线、模块化、可穿戴、软气动臂'}
{'arxiv_id': 'arXiv:2505.11474', 'title': 'REACT: Runtime-Enabled Active Collision-avoidance Technique for Autonomous Driving', 'authors': 'Heye Huang, Hao Cheng, Zhiyuan Zhou, Zijin Wang, Qichao Liu, Xiaopeng Li', 'link': 'https://arxiv.org/abs/2505.11474', 'abstract': "Achieving rapid and effective active collision avoidance in dynamic interactive traffic remains a core challenge for autonomous driving. This paper proposes REACT (Runtime-Enabled Active Collision-avoidance Technique), a closed-loop framework that integrates risk assessment with active avoidance control. By leveraging energy transfer principles and human-vehicle-road interaction modeling, REACT dynamically quantifies runtime risk and constructs a continuous spatial risk field. The system incorporates physically grounded safety constraints such as directional risk and traffic rules to identify high-risk zones and generate feasible, interpretable avoidance behaviors. A hierarchical warning trigger strategy and lightweight system design enhance runtime efficiency while ensuring real-time responsiveness. Evaluations across four representative high-risk scenarios including car-following braking, cut-in, rear-approaching, and intersection conflict demonstrate REACT's capability to accurately identify critical risks and execute proactive avoidance. Its risk estimation aligns closely with human driver cognition (i.e., warning lead time < 0.4 s), achieving 100% safe avoidance with zero false alarms or missed detections. Furthermore, it exhibits superior real-time performance (< 50 ms latency), strong foresight, and generalization. The lightweight architecture achieves state-of-the-art accuracy, highlighting its potential for real-time deployment in safety-critical autonomous systems.", 'abstract_zh': '实现动态交互交通中快速有效的主动碰撞避让仍然是自动驾驶的核心挑战。本文提出了一种实时启用的主动碰撞 avoidance 技术（REACT），这是一种闭环框架，将风险评估与主动避让控制结合在一起。通过利用能量传输原理和人-车-路交互建模，REACT 动态量化运行时风险并构建连续的空间风险场。系统融入了基于物理的安全约束，如方向性风险和交通规则，以识别高风险区域并生成可行且可解释的避让行为。层次化的预警触发策略和轻量级系统设计增强了运行时效率，同时确保实时响应能力。在四种代表性高风险场景（包括跟车制动、切入、后车接近和交叉口冲突）上进行的评估显示，REACT 能够准确识别关键风险并执行主动避让。其风险估计与人类驾驶员的认知一致（即预警提前时间 < 0.4 秒），实现了 100% 的安全避让且无误报或漏检。此外，它还展示了卓越的实时性能（< 50 毫秒延迟）、较强的预见性和泛化能力。轻量级的架构实现了最先进的准确性，突显了其在安全关键的自动驾驶系统中进行实时部署的潜力。', 'title_zh': 'REACT：运行时启用的主动避碰技术在自动驾驶中的应用'}
{'arxiv_id': 'arXiv:2505.11467', 'title': 'Exploiting Radiance Fields for Grasp Generation on Novel Synthetic Views', 'authors': 'Abhishek Kashyap, Henrik Andreasson, Todor Stoyanov', 'link': 'https://arxiv.org/abs/2505.11467', 'abstract': 'Vision based robot manipulation uses cameras to capture one or more images of a scene containing the objects to be manipulated. Taking multiple images can help if any object is occluded from one viewpoint but more visible from another viewpoint. However, the camera has to be moved to a sequence of suitable positions for capturing multiple images, which requires time and may not always be possible, due to reachability constraints. So while additional images can produce more accurate grasp poses due to the extra information available, the time-cost goes up with the number of additional views sampled. Scene representations like Gaussian Splatting are capable of rendering accurate photorealistic virtual images from user-specified novel viewpoints. In this work, we show initial results which indicate that novel view synthesis can provide additional context in generating grasp poses. Our experiments on the Graspnet-1billion dataset show that novel views contributed force-closure grasps in addition to the force-closure grasps obtained from sparsely sampled real views while also improving grasp coverage. In the future we hope this work can be extended to improve grasp extraction from radiance fields constructed with a single input image, using for example diffusion models or generalizable radiance fields.', 'abstract_zh': '基于视觉的机器人操作通过相机捕捉包含待操作物体的一个或多个场景图像。多张图像的获取有助于遮挡物体在某视角不可见而在另一视角可见的情况，但相机需要移动到一系列适合的位置来捕捉多张图像，这需要时间，且由于可达性约束，不一定总能实现。因此，虽然额外的图像可以提供更多的信息从而更准确地生成抓取姿态，但额外视角的数量也增加了时间成本。场景表示法如Gaussian Splatting能够从用户指定的新视角渲染出准确的逼真虚拟图像。在本文中，我们展示了初步结果，表明新的视角合成能够为生成抓取姿态提供额外的上下文信息。我们的实验在Graspnet-1billion数据集上表明，新视角不仅提供了从稀疏采样的真实视角获得的力闭合抓取，还提高了抓取覆盖率。未来，我们希望这项工作能够扩展应用，通过例如扩散模型或通用辐射场从单张输入图像构建的辐射场中提取抓取。', 'title_zh': '利用辐射场生成新型合成视图上的 grasping 操作'}
{'arxiv_id': 'arXiv:2505.11420', 'title': 'Self-supervised perception for tactile skin covered dexterous hands', 'authors': 'Akash Sharma, Carolina Higuera, Chaithanya Krishna Bodduluri, Zixi Liu, Taosha Fan, Tess Hellebrekers, Mike Lambeta, Byron Boots, Michael Kaess, Tingfan Wu, Francois Robert Hogan, Mustafa Mukadam', 'link': 'https://arxiv.org/abs/2505.11420', 'abstract': 'We present Sparsh-skin, a pre-trained encoder for magnetic skin sensors distributed across the fingertips, phalanges, and palm of a dexterous robot hand. Magnetic tactile skins offer a flexible form factor for hand-wide coverage with fast response times, in contrast to vision-based tactile sensors that are restricted to the fingertips and limited by bandwidth. Full hand tactile perception is crucial for robot dexterity. However, a lack of general-purpose models, challenges with interpreting magnetic flux and calibration have limited the adoption of these sensors. Sparsh-skin, given a history of kinematic and tactile sensing across a hand, outputs a latent tactile embedding that can be used in any downstream task. The encoder is self-supervised via self-distillation on a variety of unlabeled hand-object interactions using an Allegro hand sensorized with Xela uSkin. In experiments across several benchmark tasks, from state estimation to policy learning, we find that pretrained Sparsh-skin representations are both sample efficient in learning downstream tasks and improve task performance by over 41% compared to prior work and over 56% compared to end-to-end learning.', 'abstract_zh': 'Sparsh-skin: 一种用于灵巧机器人手指尖、掌骨和手掌部位分布的磁触觉传感器的预训练编码器', 'title_zh': '自我监督的触觉感知技术应用于灵巧手套'}
{'arxiv_id': 'arXiv:2505.11366', 'title': 'Learning Multimodal AI Algorithms for Amplifying Limited User Input into High-dimensional Control Space', 'authors': 'Ali Rabiee, Sima Ghafoori, MH Farhadi, Robert Beyer, Xiangyu Bai, David J Lin, Sarah Ostadabbas, Reza Abiri', 'link': 'https://arxiv.org/abs/2505.11366', 'abstract': 'Current invasive assistive technologies are designed to infer high-dimensional motor control signals from severely paralyzed patients. However, they face significant challenges, including public acceptance, limited longevity, and barriers to commercialization. Meanwhile, noninvasive alternatives often rely on artifact-prone signals, require lengthy user training, and struggle to deliver robust high-dimensional control for dexterous tasks. To address these issues, this study introduces a novel human-centered multimodal AI approach as intelligent compensatory mechanisms for lost motor functions that could potentially enable patients with severe paralysis to control high-dimensional assistive devices, such as dexterous robotic arms, using limited and noninvasive inputs. In contrast to the current state-of-the-art (SoTA) noninvasive approaches, our context-aware, multimodal shared-autonomy framework integrates deep reinforcement learning algorithms to blend limited low-dimensional user input with real-time environmental perception, enabling adaptive, dynamic, and intelligent interpretation of human intent for complex dexterous manipulation tasks, such as pick-and-place. The results from our ARAS (Adaptive Reinforcement learning for Amplification of limited inputs in Shared autonomy) trained with synthetic users over 50,000 computer simulation episodes demonstrated the first successful implementation of the proposed closed-loop human-in-the-loop paradigm, outperforming the SoTA shared autonomy algorithms. Following a zero-shot sim-to-real transfer, ARAS was evaluated on 23 human subjects, demonstrating high accuracy in dynamic intent detection and smooth, stable 3D trajectory control for dexterous pick-and-place tasks. ARAS user study achieved a high task success rate of 92.88%, with short completion times comparable to those of SoTA invasive assistive technologies.', 'abstract_zh': '当前侵入性辅助技术旨在从严重瘫痪患者中推断高维度运动控制信号，但面临公众接受度低、使用寿命有限以及商业化困难等挑战。同时，非侵入性替代方案常常依赖易受干扰的信号、需要长时间用户训练，并且难以提供稳健的高维度控制以执行灵巧任务。为解决这些问题，本研究提出了一种以用户为中心的多模态AI方法，作为失去运动功能的智能补偿机制，有望使严重瘫痪的患者能够使用有限的非侵入性输入控制高维度辅助设备，如灵巧的机械臂。与当前最先进的（SoTA）非侵入性方法不同，我们的上下文感知多模态自助协同时，结合了深度强化学习算法，将有限的低维度用户输入与实时环境感知相结合，从而实现对复杂灵巧操作任务（如拾取和放置）的人类意图的适应性、动态和智能解释。与合成用户训练了50,000个计算机仿真回路后训练的ARAS（适应性强化学习放大有限输入的自助协同时）算法表明，首次成功实现了所提议的闭环人机在网络中的范式，并优于现有的自助协作算法。经过零样本从仿真到现实的迁移，ARAS 在23名人类受试者上进行了评估，显示出在动态意图检测和灵巧拾取放置任务中的平滑、稳定的3D轨迹控制中的高准确性。ARAS用户研究实现了92.88%的任务成功率，完成时间与现有的侵入性辅助技术相当。', 'title_zh': '学习多模态AI算法以放大有限用户输入的高维控制空间'}
{'arxiv_id': 'arXiv:2505.11350', 'title': 'Search-TTA: A Multimodal Test-Time Adaptation Framework for Visual Search in the Wild', 'authors': 'Derek Ming Siang Tan, Shailesh, Boyang Liu, Alok Raj, Qi Xuan Ang, Weiheng Dai, Tanishq Duhan, Jimmy Chiun, Yuhong Cao, Florian Shkurti, Guillaume Sartoretti', 'link': 'https://arxiv.org/abs/2505.11350', 'abstract': "To perform autonomous visual search for environmental monitoring, a robot may leverage satellite imagery as a prior map. This can help inform coarse, high-level search and exploration strategies, even when such images lack sufficient resolution to allow fine-grained, explicit visual recognition of targets. However, there are some challenges to overcome with using satellite images to direct visual search. For one, targets that are unseen in satellite images are underrepresented (compared to ground images) in most existing datasets, and thus vision models trained on these datasets fail to reason effectively based on indirect visual cues. Furthermore, approaches which leverage large Vision Language Models (VLMs) for generalization may yield inaccurate outputs due to hallucination, leading to inefficient search. To address these challenges, we introduce Search-TTA, a multimodal test-time adaptation framework that can accept text and/or image input. First, we pretrain a remote sensing image encoder to align with CLIP's visual encoder to output probability distributions of target presence used for visual search. Second, our framework dynamically refines CLIP's predictions during search using a test-time adaptation mechanism. Through a feedback loop inspired by Spatial Poisson Point Processes, gradient updates (weighted by uncertainty) are used to correct (potentially inaccurate) predictions and improve search performance. To validate Search-TTA's performance, we curate a visual search dataset based on internet-scale ecological data. We find that Search-TTA improves planner performance by up to 9.7%, particularly in cases with poor initial CLIP predictions. It also achieves comparable performance to state-of-the-art VLMs. Finally, we deploy Search-TTA on a real UAV via hardware-in-the-loop testing, by simulating its operation within a large-scale simulation that provides onboard sensing.", 'abstract_zh': '用于环境监测的自主视觉搜索中，机器人可以利用卫星影像作为先验地图。这可以帮助制定粗略的高阶搜索和探索策略，即使这些影像缺乏足够的分辨率以进行精细的明确视觉目标识别。然而，使用卫星影像引导视觉搜索时仍存在一些挑战。首先，在大多数现有数据集中，未在卫星影像中观察到的目标相比于地面影像来说被严重低估，因此基于这些数据集训练的视觉模型难以根据间接视觉线索进行有效的推理。此外，依赖大型视觉语言模型（VLMs）进行泛化的方法可能会因为幻觉而导致不准确的输出，从而导致搜索效率低下。为解决这些挑战，我们引入了Search-TTA，这是一种多模态测试时间自适应框架，能够接受文本和/或图像输入。首先，我们预训练一个遥感图像编码器使其与CLIP的视觉编码器对齐，以输出用于视觉搜索的目标存在概率分布。其次，我们的框架在搜索过程中使用测试时间自适应机制动态细化CLIP的预测。通过一个深受空间泊松点过程启发的反馈循环，加权不确定性的梯度更新被用于纠正（可能不准确的）预测并提高搜索性能。为了验证Search-TTA的表现，我们基于互联网规模的生态学数据构建了一个视觉搜索数据集。我们发现，Search-TTA可以将规划者性能提高高达9.7%，特别是在初始CLIP预测较差的情况下。此外，它在性能上可以与最先进的视觉语言模型相媲美。最后，我们通过在大规模仿真中模拟其操作并在提供机载感测的硬件环境中部署Search-TTA，实现了其在实际无人机上的应用。', 'title_zh': 'Search-TTA：视觉检索中的多模态测试时自适应框架'}
{'arxiv_id': 'arXiv:2505.11214', 'title': 'Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions', 'authors': 'Wei Zhao, Gongsheng Li, Zhefei Gong, Pengxiang Ding, Han Zhao, Donglin Wang', 'link': 'https://arxiv.org/abs/2505.11214', 'abstract': 'Vision-Language-Action (VLA) models have recently become highly prominent in the field of robotics. Leveraging vision-language foundation models trained on large-scale internet data, the VLA model can generate robotic actions directly from visual observations and human instructions through a single end-to-end neural network. Despite their effectiveness, current VLA models usually accept only one form of human prompting, language instructions, which may constrain their applicability in open-ended human-robot interactions. For example, a user might expect the robot to retrieve an object shown in an image, follow an instruction written on the whiteboard, or imitate a behavior demonstrated in a video, rather than relying solely on language-based descriptions. To address this gap, we introduce OE-VLA, which explores the potential of VLA models for open-ended multimodal instructions. Extensive results demonstrate that our OE-VLA not only achieves comparable performance to traditional VLA models with linguistic input but also delivers impressive results across four additional categories of open-ended tasks. The proposed methodology could significantly expand the applications of VLA models across various everyday scenarios and facilitate human-robot interaction.', 'abstract_zh': 'Vision-Language-Action (VLA) 模型在机器人领域Recently Became Highly Prominent并提出了OE-VLA以探索VLA模型在开放域多模态指令中的潜力', 'title_zh': '探索开放型多模态指令下视觉-语言-动作模型的潜力'}
{'arxiv_id': 'arXiv:2505.11175', 'title': 'Real-Time Verification of Embodied Reasoning for Generative Skill Acquisition', 'authors': 'Bo Yue, Shuqi Guo, Kaiyu Hu, Chujiao Wang, Benyou Wang, Kui Jia, Guiliang Liu', 'link': 'https://arxiv.org/abs/2505.11175', 'abstract': 'Generative skill acquisition enables embodied agents to actively learn a scalable and evolving repertoire of control skills, crucial for the advancement of large decision models. While prior approaches often rely on supervision signals from generalist agents (e.g., LLMs), their effectiveness in complex 3D environments remains unclear; exhaustive evaluation incurs substantial computational costs, significantly hindering the efficiency of skill learning. Inspired by recent successes in verification models for mathematical reasoning, we propose VERGSA (Verifying Embodied Reasoning in Generative Skill Acquisition), a framework that systematically integrates real-time verification principles into embodied skill learning. VERGSA establishes 1) a seamless extension from verification of mathematical reasoning into embodied learning by dynamically incorporating contextually relevant tasks into prompts and defining success metrics for both subtasks and overall tasks, and 2) an automated, scalable reward labeling scheme that synthesizes dense reward signals by iteratively finalizing the contribution of scene configuration and subtask learning to overall skill acquisition. To the best of our knowledge, this approach constitutes the first comprehensive training dataset for verification-driven generative skill acquisition, eliminating arduous manual reward engineering. Experiments validate the efficacy of our approach: 1) the exemplar task pool improves the average task success rates by 21%, 2) our verification model boosts success rates by 24% for novel tasks and 36% for encountered tasks, and 3) outperforms LLM-as-a-Judge baselines in verification quality.', 'abstract_zh': '生成技能获取使 embodied 代理能够主动学习可扩展且不断演变的控制技能库，对于大型决策模型的发展至关重要。受近期数学推理验证模型成功经验的启发，我们提出 VERGSA（验证生成技能获取中的 embodied 推理验证），该框架系统地将实时验证原则整合到 embodied 技能学习中。VERGSA 通过动态引入与上下文相关的任务并为子任务和总体任务定义成功度量，实现了 1) 从数学推理验证平滑扩展到 embodied 学习，以及 2) 自动化、可扩展的奖励标签方案，通过迭代确定场景配置和子任务学习对整体技能获取的贡献来合成密集奖励信号。据我们所知，本方法构成了首个基于验证的生成技能获取的全面训练数据集，消除了繁琐的手动奖励工程。实验验证了本方法的有效性：1) 样例任务池将平均任务成功率提高 21%，2) 我们的验证模型将新任务的成功率提高 24%，遇到任务的成功率提高 36%，3) 在验证质量上优于 LLM 作为裁判的基线方法。', 'title_zh': '实时验证具身推理在生成技能习得中的正确性'}
{'arxiv_id': 'arXiv:2505.11164', 'title': 'Parkour in the Wild: Learning a General and Extensible Agile Locomotion Policy Using Multi-expert Distillation and RL Fine-tuning', 'authors': 'Nikita Rudin, Junzhe He, Joshua Aurand, Marco Hutter', 'link': 'https://arxiv.org/abs/2505.11164', 'abstract': "Legged robots are well-suited for navigating terrains inaccessible to wheeled robots, making them ideal for applications in search and rescue or space exploration. However, current control methods often struggle to generalize across diverse, unstructured environments. This paper introduces a novel framework for agile locomotion of legged robots by combining multi-expert distillation with reinforcement learning (RL) fine-tuning to achieve robust generalization. Initially, terrain-specific expert policies are trained to develop specialized locomotion skills. These policies are then distilled into a unified foundation policy via the DAgger algorithm. The distilled policy is subsequently fine-tuned using RL on a broader terrain set, including real-world 3D scans. The framework allows further adaptation to new terrains through repeated fine-tuning. The proposed policy leverages depth images as exteroceptive inputs, enabling robust navigation across diverse, unstructured terrains. Experimental results demonstrate significant performance improvements over existing methods in synthesizing multi-terrain skills into a single controller. Deployment on the ANYmal D robot validates the policy's ability to navigate complex environments with agility and robustness, setting a new benchmark for legged robot locomotion.", 'abstract_zh': '基于多专家提炼与强化学习微调的腿足机器人灵活运动框架', 'title_zh': '在野外的Parkour：基于多专家蒸馏和RL微调的学习通用可扩展敏捷运动政策'}
{'arxiv_id': 'arXiv:2505.11146', 'title': 'X2C: A Dataset Featuring Nuanced Facial Expressions for Realistic Humanoid Imitation', 'authors': 'Peizhen Li, Longbing Cao, Xiao-Ming Wu, Runze Yang, Xiaohan Yu', 'link': 'https://arxiv.org/abs/2505.11146', 'abstract': 'The ability to imitate realistic facial expressions is essential for humanoid robots engaged in affective human-robot communication. However, the lack of datasets containing diverse humanoid facial expressions with proper annotations hinders progress in realistic humanoid facial expression imitation. To address these challenges, we introduce X2C (Anything to Control), a dataset featuring nuanced facial expressions for realistic humanoid imitation. With X2C, we contribute: 1) a high-quality, high-diversity, large-scale dataset comprising 100,000 (image, control value) pairs. Each image depicts a humanoid robot displaying a diverse range of facial expressions, annotated with 30 control values representing the ground-truth expression configuration; 2) X2CNet, a novel human-to-humanoid facial expression imitation framework that learns the correspondence between nuanced humanoid expressions and their underlying control values from X2C. It enables facial expression imitation in the wild for different human performers, providing a baseline for the imitation task, showcasing the potential value of our dataset; 3) real-world demonstrations on a physical humanoid robot, highlighting its capability to advance realistic humanoid facial expression imitation. Code and Data: this https URL', 'abstract_zh': '仿人机器人进行真实的情感人类-机器人交流的能力依赖于能够模仿现实的面部表情。然而，缺乏包含多样仿人面部表情并附有适当标注的数据集阻碍了此类仿真的进步。为应对这些挑战，我们引入了X2C（Anything to Control），一个用于现实仿人面部表情模仿的细致面部表情数据集。通过X2C，我们贡献了：1）一个高质量、高多样性和大规模的数据集，包含100,000个（图像，控制值）对。每张图像展示了仿人机器人展示的一系列面部表情，并带有30个控制值以表示真实的情感配置；2）X2CNet，一种新颖的人类到仿人面部表情模仿框架，该框架从X2C中学习细致的仿人面部表情与其底层控制值之间的对应关系，使得不同的表演者可以在野外实现面部表情模仿，并为模仿任务提供基准，展示了我们数据集的潜在价值；3）在物理仿人机器人上的实际演示，突显了其在推进现实仿人面部表情模仿方面的能力。代码和数据：[这里提供链接]。', 'title_zh': 'X2C: 一个展示细腻面部表情的数据集以实现逼真的人形机器人模仿'}
{'arxiv_id': 'arXiv:2505.11142', 'title': 'Open-Source Multi-Viewpoint Surgical Telerobotics', 'authors': 'Guido Caccianiga, Yarden Sharon, Bernard Javot, Senya Polikovsky, Gökce Ergün, Ivan Capobianco, André L. Mihaljevic, Anton Deguet, Katherine J. Kuchenbecker', 'link': 'https://arxiv.org/abs/2505.11142', 'abstract': "As robots for minimally invasive surgery (MIS) gradually become more accessible and modular, we believe there is a great opportunity to rethink and expand the visualization and control paradigms that have characterized surgical teleoperation since its inception. We conjecture that introducing one or more additional adjustable viewpoints in the abdominal cavity would not only unlock novel visualization and collaboration strategies for surgeons but also substantially boost the robustness of machine perception toward shared autonomy. Immediate advantages include controlling a second viewpoint and teleoperating surgical tools from a different perspective, which would allow collaborating surgeons to adjust their views independently and still maneuver their robotic instruments intuitively. Furthermore, we believe that capturing synchronized multi-view 3D measurements of the patient's anatomy would unlock advanced scene representations. Accurate real-time intraoperative 3D perception will allow algorithmic assistants to directly control one or more robotic instruments and/or robotic cameras. Toward these goals, we are building a synchronized multi-viewpoint, multi-sensor robotic surgery system by integrating high-performance vision components and upgrading the da Vinci Research Kit control logic. This short paper reports a functional summary of our setup and elaborates on its potential impacts in research and future clinical practice. By fully open-sourcing our system, we will enable the research community to reproduce our setup, improve it, and develop powerful algorithms, effectively boosting clinical translation of cutting-edge research.", 'abstract_zh': '随着用于微创手术（MIS）的机器人逐渐变得更加易获取和模块化，我们相信有机会重新思考和扩展自远程手术操作成立以来一直主导的可视化和控制范式。我们推测，在腹腔内引入一个或多个可调节的视角不仅能够解锁新的可视化和协作策略，还能够显著提高机器感知向共享自主性的鲁棒性。即时优势包括控制第二个视角和从不同角度远程操作手术工具，这将使合作的外科医生能够独立调整他们的视角并仍然能够直观地操作其机器人器械。此外，我们认为捕获患者解剖结构的同步多视角三维测量将解锁高级场景表示。准确的实时术中三维感知将允许算法助手直接控制一个或多个机器人器械和/或机器人摄像机。为了实现这些目标，我们正在构建一个同步多视角、多传感器的机器人手术系统，通过整合高性能视觉组件并升级da Vinci Research Kit的控制逻辑。本文简要报告了我们系统的功能概述，并详细说明了其在研究和未来临床实践中的潜在影响。通过完全开源我们的系统，我们将使研究社区能够复制我们的设置、改进它并开发强大的算法，从而有效地促进前沿研究临床转化。', 'title_zh': '开源多视角手术 tele manipulation'}
{'arxiv_id': 'arXiv:2505.11123', 'title': 'Conditioning Matters: Training Diffusion Policies is Faster Than You Think', 'authors': 'Zibin Dong, Yicheng Liu, Yinchuan Li, Hang Zhao, Jianye Hao', 'link': 'https://arxiv.org/abs/2505.11123', 'abstract': 'Diffusion policies have emerged as a mainstream paradigm for building vision-language-action (VLA) models. Although they demonstrate strong robot control capabilities, their training efficiency remains suboptimal. In this work, we identify a fundamental challenge in conditional diffusion policy training: when generative conditions are hard to distinguish, the training objective degenerates into modeling the marginal action distribution, a phenomenon we term loss collapse. To overcome this, we propose Cocos, a simple yet general solution that modifies the source distribution in the conditional flow matching to be condition-dependent. By anchoring the source distribution around semantics extracted from condition inputs, Cocos encourages stronger condition integration and prevents the loss collapse. We provide theoretical justification and extensive empirical results across simulation and real-world benchmarks. Our method achieves faster convergence and higher success rates than existing approaches, matching the performance of large-scale pre-trained VLAs using significantly fewer gradient steps and parameters. Cocos is lightweight, easy to implement, and compatible with diverse policy architectures, offering a general-purpose improvement to diffusion policy training.', 'abstract_zh': 'Cocos: A Simple and General Solution to Prevent Loss Collapse in Conditional Diffusion Policy Training', 'title_zh': 'conditioning 事项：训练扩散策略比你想象的要快。'}
{'arxiv_id': 'arXiv:2505.11116', 'title': 'Planar Velocity Estimation for Fast-Moving Mobile Robots Using Event-Based Optical Flow', 'authors': 'Liam Boyle, Jonas Kühne, Nicolas Baumann, Niklas Bastuck, Michele Magno', 'link': 'https://arxiv.org/abs/2505.11116', 'abstract': 'Accurate velocity estimation is critical in mobile robotics, particularly for driver assistance systems and autonomous driving. Wheel odometry fused with Inertial Measurement Unit (IMU) data is a widely used method for velocity estimation; however, it typically requires strong assumptions, such as non-slip steering, or complex vehicle dynamics models that do not hold under varying environmental conditions like slippery surfaces. We introduce an approach to velocity estimation that is decoupled from wheel-to-surface traction assumptions by leveraging planar kinematics in combination with optical flow from event cameras pointed perpendicularly at the ground. The asynchronous micro-second latency and high dynamic range of event cameras make them highly robust to motion blur, a common challenge in vision-based perception techniques for autonomous driving. The proposed method is evaluated through in-field experiments on a 1:10 scale autonomous racing platform and compared to precise motion capture data, demonstrating not only performance on par with the state-of-the-art Event-VIO method but also a 38.3 % improvement in lateral error. Qualitative experiments at highway speeds of up to 32 m/s further confirm the effectiveness of our approach, indicating significant potential for real-world deployment.', 'abstract_zh': '准确的速度估计在移动机器人中至关重要，特别是在驾驶员辅助系统和自动驾驶领域。通过结合事件相机的地平面光学流与惯性测量单元（IMU）数据的车轮里程计融合方法，可以在无需轮子与地面摩擦假设的情况下进行速度估计。这种方法在1:10比例的自动驾驶赛车平台上与精确运动捕捉数据进行对比实验，不仅与最先进的事件-VIO方法性能相当，还表现出38.3%的侧向误差改善。在最高达32 m/s的高速公路速度下进行的定性实验进一步证实了该方法的有效性，显示出其在真实世界部署中的巨大潜力。', 'title_zh': '基于事件驱动光学流的快速移动移动机器人平面速度估计'}
{'arxiv_id': 'arXiv:2505.11108', 'title': 'PARSEC: Preference Adaptation for Robotic Object Rearrangement from Scene Context', 'authors': 'Kartik Ramachandruni, Sonia Chernova', 'link': 'https://arxiv.org/abs/2505.11108', 'abstract': "Object rearrangement is a key task for household robots requiring personalization without explicit instructions, meaningful object placement in environments occupied with objects, and generalization to unseen objects and new environments. To facilitate research addressing these challenges, we introduce PARSEC, an object rearrangement benchmark for learning user organizational preferences from observed scene context to place objects in a partially arranged environment. PARSEC is built upon a novel dataset of 110K rearrangement examples crowdsourced from 72 users, featuring 93 object categories and 15 environments. We also propose ContextSortLM, an LLM-based rearrangement model that places objects in partially arranged environments by adapting to user preferences from prior and current scene context while accounting for multiple valid placements. We evaluate ContextSortLM and existing personalized rearrangement approaches on the PARSEC benchmark and complement these findings with a crowdsourced evaluation of 108 online raters ranking model predictions based on alignment with user preferences. Our results indicate that personalized rearrangement models leveraging multiple scene context sources perform better than models relying on a single context source. Moreover, ContextSortLM outperforms other models in placing objects to replicate the target user's arrangement and ranks among the top two in all three environment categories, as rated by online evaluators. Importantly, our evaluation highlights challenges associated with modeling environment semantics across different environment categories and provides recommendations for future work.", 'abstract_zh': '基于场景上下文学习用户组织偏好的家用机器人物体重排基准：PARSEC及其应用', 'title_zh': 'PARSEC: 基于场景上下文的机器人物体重排偏好适应'}
{'arxiv_id': 'arXiv:2505.11032', 'title': 'DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy', 'authors': 'Yuran Wang, Ruihai Wu, Yue Chen, Jiarui Wang, Jiaqi Liang, Ziyu Zhu, Haoran Geng, Jitendra Malik, Pieter Abbeel, Hao Dong', 'link': 'https://arxiv.org/abs/2505.11032', 'abstract': 'Garment manipulation is a critical challenge due to the diversity in garment categories, geometries, and deformations. Despite this, humans can effortlessly handle garments, thanks to the dexterity of our hands. However, existing research in the field has struggled to replicate this level of dexterity, primarily hindered by the lack of realistic simulations of dexterous garment manipulation. Therefore, we propose DexGarmentLab, the first environment specifically designed for dexterous (especially bimanual) garment manipulation, which features large-scale high-quality 3D assets for 15 task scenarios, and refines simulation techniques tailored for garment modeling to reduce the sim-to-real gap. Previous data collection typically relies on teleoperation or training expert reinforcement learning (RL) policies, which are labor-intensive and inefficient. In this paper, we leverage garment structural correspondence to automatically generate a dataset with diverse trajectories using only a single expert demonstration, significantly reducing manual intervention. However, even extensive demonstrations cannot cover the infinite states of garments, which necessitates the exploration of new algorithms. To improve generalization across diverse garment shapes and deformations, we propose a Hierarchical gArment-manipuLation pOlicy (HALO). It first identifies transferable affordance points to accurately locate the manipulation area, then generates generalizable trajectories to complete the task. Through extensive experiments and detailed analysis of our method and baseline, we demonstrate that HALO consistently outperforms existing methods, successfully generalizing to previously unseen instances even with significant variations in shape and deformation where others fail. Our project page is available at: this https URL.', 'abstract_zh': '衣物操作是一个关键挑战，由于衣物类别、几何形状和变形的多样性。尽管如此，人类可以通过手部的灵巧性轻松处理衣物。然而，现有领域的研究难以复制这种灵巧性，主要是因为缺乏真实的灵巧衣物操作模拟。因此，我们提出了DexGarmentLab，这一专门设计用于灵巧（尤其是双臂操作）衣物操作的环境，包含15种任务场景的大规模高质量3D资产，并针对衣物建模精化仿真技术以减少仿真与现实之间的差距。之前的数据收集通常依赖于遥操作或训练专家强化学习（RL）策略，这既耗时又低效。在本文中，我们利用衣物结构对应关系，仅通过一个专家演示 自动生成具有多样轨迹的数据集，显著减少了手动干预。然而，即便是大量的演示也不能覆盖衣物的无限状态，这需要探索新的算法。为了提高在多样化衣物形状和变形下的泛化能力，我们提出了分层gArment操作策略（HALO）。它首先识别可转移的 affordance 点以准确确定操作区域，然后生成可泛化的轨迹以完成任务。通过广泛的实验和对我们方法及其基线的详细分析，我们证明HALO在多种形状和变形条件下始终优于现有方法，能够成功泛化到未见过的实例，即使在其他方法失败的情况下也是如此。我们项目页面可访问：[this https URL]', 'title_zh': 'DexGarmentLab: 可通用策略的灵巧服装 manipulation 环境'}
{'arxiv_id': 'arXiv:2505.10973', 'title': 'GROQLoco: Generalist and RObot-agnostic Quadruped Locomotion Control using Offline Datasets', 'authors': 'Narayanan PP, Sarvesh Prasanth Venkatesan, Srinivas Kantha Reddy, Shishir Kolathaya', 'link': 'https://arxiv.org/abs/2505.10973', 'abstract': 'Recent advancements in large-scale offline training have demonstrated the potential of generalist policy learning for complex robotic tasks. However, applying these principles to legged locomotion remains a challenge due to continuous dynamics and the need for real-time adaptation across diverse terrains and robot morphologies. In this work, we propose GROQLoco, a scalable, attention-based framework that learns a single generalist locomotion policy across multiple quadruped robots and terrains, relying solely on offline datasets. Our approach leverages expert demonstrations from two distinct locomotion behaviors - stair traversal (non-periodic gaits) and flat terrain traversal (periodic gaits) - collected across multiple quadruped robots, to train a generalist model that enables behavior fusion for both behaviors. Crucially, our framework operates directly on proprioceptive data from all robots without incorporating any robot-specific encodings. The policy is directly deployable on an Intel i7 nuc, producing low-latency control outputs without any test-time optimization. Our extensive experiments demonstrate strong zero-shot transfer across highly diverse quadruped robots and terrains, including hardware deployment on the Unitree Go1, a commercially available 12kg robot. Notably, we evaluate challenging cross-robot training setups where different locomotion skills are unevenly distributed across robots, yet observe successful transfer of both flat walking and stair traversal behaviors to all robots at test time. We also show preliminary walking on Stoch 5, a 70kg quadruped, on flat and outdoor terrains without requiring any fine tuning. These results highlight the potential for robust generalist locomotion across diverse robots and terrains.', 'abstract_zh': 'Recent advancements in large-scale offline training have demonstrated the potential of generalist policy learning for complex robotic tasks.然而，将这些原则应用于腿足运动仍然面临挑战，原因在于连续的动力学特性和在不同地形和机器人形态下进行实时适应的需要。在本文中，我们提出了一种可扩展的、基于注意力的框架GROQLoco，该框架利用多腿足机器人和地形上的离线数据集训练出适用于多种四足机器人和地形的通用运动策略。我们的方法利用了来自两类不同运动行为（楼梯穿越和平坦地形穿越）的专家演示数据进行训练，从而训练出能够融合两种行为的通用模型。关键的是，我们的框架直接在所有机器人的本体感觉数据上运行，而无需任何特定于机器人的编码。该策略可以直接部署在Intel i7 nuc上，生产低延迟的控制输出，并且不需要在测试时进行任何优化。我们广泛的经验表明，这种通用策略能够在高度多样化的四足机器人和地形上实现零样本迁移，包括在商业可用的12kg机器人Unitree Go1上进行硬件部署。此外，我们在Stoch 5（一个70kg的四足机器人）平坦和户外地形上展示了初步的行走能力，而无需任何微调。这些结果突显了在不同机器人和地形上实现鲁棒通用运动的潜力。', 'title_zh': 'GROQLoco: 通用且机器人无关的四足 locomotion 控制基于离线数据集'}
{'arxiv_id': 'arXiv:2505.10923', 'title': 'GrowSplat: Constructing Temporal Digital Twins of Plants with Gaussian Splats', 'authors': 'Simeon Adebola, Shuangyu Xie, Chung Min Kim, Justin Kerr, Bart M. van Marrewijk, Mieke van Vlaardingen, Tim van Daalen, Robert van Loo, Jose Luis Susa Rincon, Eugen Solowjow, Rick van de Zedde, Ken Goldberg', 'link': 'https://arxiv.org/abs/2505.10923', 'abstract': 'Accurate temporal reconstructions of plant growth are essential for plant phenotyping and breeding, yet remain challenging due to complex geometries, occlusions, and non-rigid deformations of plants. We present a novel framework for building temporal digital twins of plants by combining 3D Gaussian Splatting with a robust sample alignment pipeline. Our method begins by reconstructing Gaussian Splats from multi-view camera data, then leverages a two-stage registration approach: coarse alignment through feature-based matching and Fast Global Registration, followed by fine alignment with Iterative Closest Point. This pipeline yields a consistent 4D model of plant development in discrete time steps. We evaluate the approach on data from the Netherlands Plant Eco-phenotyping Center, demonstrating detailed temporal reconstructions of Sequoia and Quinoa species. Videos and Images can be seen at this https URL', 'abstract_zh': '植物生长精确的时间重建对于植物表型分析和育种至关重要，但由于植物几何结构的复杂性、遮挡和非刚性变形，这一任务依然具有挑战性。我们提出了一种结合3D高斯点云技术和稳健的样本对齐管道的新框架，以构建植物的时间数字孪生体。该方法首先从多视角相机数据中重建高斯点云，然后利用两阶段注册方法：基于特征的粗略对齐和快速全局注册，随后通过迭代最近点进行精细对齐。此管道生成了一致的离散时间步长的4D植物发育模型。我们在荷兰植物生态表型中心的数据上评估了该方法，展示了红杉和藜麦物种的详细时间重建结果。更多视频和图片请参见此链接：此 https URL。', 'title_zh': 'GrowSplat: 基于高斯点构建植物的时空数字双胞胎'}
{'arxiv_id': 'arXiv:2505.10918', 'title': 'Unleashing Humanoid Reaching Potential via Real-world-Ready Skill Space', 'authors': 'Zhikai Zhang, Chao Chen, Han Xue, Jilong Wang, Sikai Liang, Yun Liu, Zongzhang Zhang, He Wang, Li Yi', 'link': 'https://arxiv.org/abs/2505.10918', 'abstract': 'Humans possess a large reachable space in the 3D world, enabling interaction with objects at varying heights and distances. However, realizing such large-space reaching on humanoids is a complex whole-body control problem and requires the robot to master diverse skills simultaneously-including base positioning and reorientation, height and body posture adjustments, and end-effector pose control. Learning from scratch often leads to optimization difficulty and poor sim2real transferability. To address this challenge, we propose Real-world-Ready Skill Space (R2S2). Our approach begins with a carefully designed skill library consisting of real-world-ready primitive skills. We ensure optimal performance and robust sim2real transfer through individual skill tuning and sim2real evaluation. These skills are then ensembled into a unified latent space, serving as a structured prior that helps task execution in an efficient and sim2real transferable manner. A high-level planner, trained to sample skills from this space, enables the robot to accomplish real-world goal-reaching tasks. We demonstrate zero-shot sim2real transfer and validate R2S2 in multiple challenging goal-reaching scenarios.', 'abstract_zh': '面向现实世界的技能空间（R2S2）', 'title_zh': '面向现实世界的类人达 manipulability 通过技能空间释放潜力'}
{'arxiv_id': 'arXiv:2505.10911', 'title': 'ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations', 'authors': 'Jiahui Zhang, Yusen Luo, Abrar Anwar, Sumedh Anand Sontakke, Joseph J Lim, Jesse Thomason, Erdem Biyik, Jesse Zhang', 'link': 'https://arxiv.org/abs/2505.10911', 'abstract': "We introduce ReWiND, a framework for learning robot manipulation tasks solely from language instructions without per-task demonstrations. Standard reinforcement learning (RL) and imitation learning methods require expert supervision through human-designed reward functions or demonstrations for every new task. In contrast, ReWiND starts from a small demonstration dataset to learn: (1) a data-efficient, language-conditioned reward function that labels the dataset with rewards, and (2) a language-conditioned policy pre-trained with offline RL using these rewards. Given an unseen task variation, ReWiND fine-tunes the pre-trained policy using the learned reward function, requiring minimal online interaction. We show that ReWiND's reward model generalizes effectively to unseen tasks, outperforming baselines by up to 2.4x in reward generalization and policy alignment metrics. Finally, we demonstrate that ReWiND enables sample-efficient adaptation to new tasks, beating baselines by 2x in simulation and improving real-world pretrained bimanual policies by 5x, taking a step towards scalable, real-world robot learning. See website at this https URL.", 'abstract_zh': '我们介绍ReWiND框架，该框架能仅从语言指令学习机器人操作任务，而无需每项任务的具体示范。标准强化学习（RL）和模仿学习方法需要通过人工设计的奖励函数或示范来获取专家监督，以完成每个新任务。相比之下，ReWiND 从一个小的示范数据集开始学习：(1) 一个数据效率高、受语言条件指导的奖励函数，该函数可以为数据集打上奖励标签，(2) 一个受语言条件指导的策略，该策略在使用这些奖励进行离线强化学习预训练后进行微调。面对未见过的任务变体，ReWiND 使用学习到的奖励函数微调预训练策略，所需的在线交互最少。我们证明，ReWiND 的奖励模型能有效泛化到未见过的任务，其在奖励泛化和策略对齐度量上比基线高出2.4倍。最后，我们展示了ReWiND 能实现样本高效的新任务适应，其在模拟中优于基线2倍，在实际预训练双臂策略的表现上提升了5倍，朝着可扩展的、面向实际应用的机器人学习迈出了一步。详见网站：https://www.example.com', 'title_zh': 'ReWiND：语言引导的奖励教机器人策略无需新演示'}
{'arxiv_id': 'arXiv:2505.10884', 'title': 'Estimating Deformable-Rigid Contact Interactions for a Deformable Tool via Learning and Model-Based Optimization', 'authors': 'Mark Van der Merwe, Miquel Oller, Dmitry Berenson, Nima Fazeli', 'link': 'https://arxiv.org/abs/2505.10884', 'abstract': "Dexterous manipulation requires careful reasoning over extrinsic contacts. The prevalence of deforming tools in human environments, the use of deformable sensors, and the increasing number of soft robots yields a need for approaches that enable dexterous manipulation through contact reasoning where not all contacts are well characterized by classical rigid body contact models. Here, we consider the case of a deforming tool dexterously manipulating a rigid object. We propose a hybrid learning and first-principles approach to the modeling of simultaneous motion and force transfer of tools and objects. The learned module is responsible for jointly estimating the rigid object's motion and the deformable tool's imparted contact forces. We then propose a Contact Quadratic Program to recover forces between the environment and object subject to quasi-static equilibrium and Coulomb friction. The results is a system capable of modeling both intrinsic and extrinsic motions, contacts, and forces during dexterous deformable manipulation. We train our method in simulation and show that our method outperforms baselines under varying block geometries and physical properties, during pushing and pivoting manipulations, and demonstrate transfer to real world interactions. Video results can be found at this https URL.", 'abstract_zh': '灵巧操作需要细致的接触推理。变形工具在人类环境中的普遍性、使用变形传感器以及软机器人数量的增加，产生了通过接触推理实现灵巧操作的需求，其中并非所有接触都能用经典的刚体接触模型充分描述。在此，我们考虑一种灵巧操作刚体对象的变形工具的情况。我们提出了一种结合学习和第一原理的方法来建模工具和物体的同时运动和力传递。学习模块负责联合估计刚体对象的运动和变形工具施加的接触力。然后，我们提出了一种接触二次规划方法，以恢复环境与对象之间的力，条件是满足准静态平衡和库仑摩擦。该系统能够建模灵巧变形操作中的内在和外在运动、接触和力。我们在仿真中训练该方法，并在不同几何结构和物理属性的块件以及推拉操作下展示了其性能优于基线方法，并展示了其在现实世界交互中的应用潜力。相关视频结果可在此处查看：this https URL。', 'title_zh': '基于学习和模型优化的可变形工具可变形-刚性接触交互估计'}
{'arxiv_id': 'arXiv:2505.10872', 'title': 'REI-Bench: Can Embodied Agents Understand Vague Human Instructions in Task Planning?', 'authors': 'Chenxi Jiang, Chuhao Zhou, Jianfei Yang', 'link': 'https://arxiv.org/abs/2505.10872', 'abstract': 'Robot task planning decomposes human instructions into executable action sequences that enable robots to complete a series of complex tasks. Although recent large language model (LLM)-based task planners achieve amazing performance, they assume that human instructions are clear and straightforward. However, real-world users are not experts, and their instructions to robots often contain significant vagueness. Linguists suggest that such vagueness frequently arises from referring expressions (REs), whose meanings depend heavily on dialogue context and environment. This vagueness is even more prevalent among the elderly and children, who robots should serve more. This paper studies how such vagueness in REs within human instructions affects LLM-based robot task planning and how to overcome this issue. To this end, we propose the first robot task planning benchmark with vague REs (REI-Bench), where we discover that the vagueness of REs can severely degrade robot planning performance, leading to success rate drops of up to 77.9%. We also observe that most failure cases stem from missing objects in planners. To mitigate the REs issue, we propose a simple yet effective approach: task-oriented context cognition, which generates clear instructions for robots, achieving state-of-the-art performance compared to aware prompt and chains of thought. This work contributes to the research community of human-robot interaction (HRI) by making robot task planning more practical, particularly for non-expert users, e.g., the elderly and children.', 'abstract_zh': '基于语言模型的机器人任务规划如何应对人类指令中的歧义性参考表达，并提出解决方案', 'title_zh': 'REI-Bench: 体现代理能否理解任务规划中的模糊人类指令？'}
{'arxiv_id': 'arXiv:2505.10847', 'title': 'Robust 2D lidar-based SLAM in arboreal environments without IMU/GNSS', 'authors': 'Paola Nazate-Burgos, Miguel Torres-Torriti, Sergio Aguilera-Marinovic, Tito Arévalo, Shoudong Huang, Fernando Auat Cheein', 'link': 'https://arxiv.org/abs/2505.10847', 'abstract': "Simultaneous localization and mapping (SLAM) approaches for mobile robots remains challenging in forest or arboreal fruit farming environments, where tree canopies obstruct Global Navigation Satellite Systems (GNSS) signals. Unlike indoor settings, these agricultural environments possess additional challenges due to outdoor variables such as foliage motion and illumination variability. This paper proposes a solution based on 2D lidar measurements, which requires less processing and storage, and is more cost-effective, than approaches that employ 3D lidars. Utilizing the modified Hausdorff distance (MHD) metric, the method can solve the scan matching robustly and with high accuracy without needing sophisticated feature extraction. The method's robustness was validated using public datasets and considering various metrics, facilitating meaningful comparisons for future research. Comparative evaluations against state-of-the-art algorithms, particularly A-LOAM, show that the proposed approach achieves lower positional and angular errors while maintaining higher accuracy and resilience in GNSS-denied settings. This work contributes to the advancement of precision agriculture by enabling reliable and autonomous navigation in challenging outdoor environments.", 'abstract_zh': '基于2D激光雷达测距的森林或树冠果园移动机器人 SLAM 方法研究', 'title_zh': '基于激光雷达的鲁棒二维树冠环境SLAM Without IMU/GNSS'}
{'arxiv_id': 'arXiv:2505.10770', 'title': 'Geofenced Unmanned Aerial Robotic Defender for Deer Detection and Deterrence (GUARD)', 'authors': 'Ebasa Temesgen, Mario Jerez, Greta Brown, Graham Wilson, Sree Ganesh Lalitaditya Divakarla, Sarah Boelter, Oscar Nelson, Robert McPherson, Maria Gini', 'link': 'https://arxiv.org/abs/2505.10770', 'abstract': 'Wildlife-induced crop damage, particularly from deer, threatens agricultural productivity. Traditional deterrence methods often fall short in scalability, responsiveness, and adaptability to diverse farmland environments. This paper presents an integrated unmanned aerial vehicle (UAV) system designed for autonomous wildlife deterrence, developed as part of the Farm Robotics Challenge. Our system combines a YOLO-based real-time computer vision module for deer detection, an energy-efficient coverage path planning algorithm for efficient field monitoring, and an autonomous charging station for continuous operation of the UAV. In collaboration with a local Minnesota farmer, the system is tailored to address practical constraints such as terrain, infrastructure limitations, and animal behavior. The solution is evaluated through a combination of simulation and field testing, demonstrating robust detection accuracy, efficient coverage, and extended operational time. The results highlight the feasibility and effectiveness of drone-based wildlife deterrence in precision agriculture, offering a scalable framework for future deployment and extension.', 'abstract_zh': '野生生物引起的作物损害，尤其是鹿造成的损害，威胁着农业 productivity。传统的驱赶方法在可扩展性、响应能力和适应多样农田环境方面往往不尽如人意。本文介绍了一种集成的无人机系统，该系统旨在自主驱赶野生动物，作为农场机器人挑战赛的一部分开发。该系统结合了基于YOLO的实时计算机视觉模块进行鹿的检测、高效覆盖率路径规划算法进行高效的田地监测以及自主充电站以实现无人机的连续操作。与明尼苏达州一名当地农民合作，该系统针对地形、基础设施限制和动物行为等实际约束进行了定制。通过模拟和实地测试对该解决方案进行了评估，结果显示了稳健的检测精度、高效的覆盖范围以及延长的操作时间。研究结果突显了基于无人机的野生动物驱赶在精准农业中的可行性和有效性，为其未来的部署和扩展提供了一个可扩展的框架。', 'title_zh': '地理围栏无人机防护系统： Deer检测与驱避（GUARD）'}
{'arxiv_id': 'arXiv:2505.10760', 'title': 'Counterfactual Behavior Cloning: Offline Imitation Learning from Imperfect Human Demonstrations', 'authors': 'Shahabedin Sagheb, Dylan P. Losey', 'link': 'https://arxiv.org/abs/2505.10760', 'abstract': "Learning from humans is challenging because people are imperfect teachers. When everyday humans show the robot a new task they want it to perform, humans inevitably make errors (e.g., inputting noisy actions) and provide suboptimal examples (e.g., overshooting the goal). Existing methods learn by mimicking the exact behaviors the human teacher provides -- but this approach is fundamentally limited because the demonstrations themselves are imperfect. In this work we advance offline imitation learning by enabling robots to extrapolate what the human teacher meant, instead of only considering what the human actually showed. We achieve this by hypothesizing that all of the human's demonstrations are trying to convey a single, consistent policy, while the noise and sub-optimality within their behaviors obfuscates the data and introduces unintentional complexity. To recover the underlying policy and learn what the human teacher meant, we introduce Counter-BC, a generalized version of behavior cloning. Counter-BC expands the given dataset to include actions close to behaviors the human demonstrated (i.e., counterfactual actions that the human teacher could have intended, but did not actually show). During training Counter-BC autonomously modifies the human's demonstrations within this expanded region to reach a simple and consistent policy that explains the underlying trends in the human's dataset. Theoretically, we prove that Counter-BC can extract the desired policy from imperfect data, multiple users, and teachers of varying skill levels. Empirically, we compare Counter-BC to state-of-the-art alternatives in simulated and real-world settings with noisy demonstrations, standardized datasets, and real human teachers. See videos of our work here: this https URL", 'abstract_zh': '从人类那里学习具有挑战性，因为人类是不完美的教师。当日常中的人类向机器人展示他们想要机器人执行的新任务时，人类不可避免地会犯错误（例如，输入噪声动作）并提供次优示例（例如，超出目标）。现有方法通过模仿人类教师提供的 exact 行为来学习——但这种方法本质上是有限的，因为示范本身是不完美的。在本工作中，我们通过使机器人能够推断人类教师的意图，而不是仅仅考虑人类实际展示的内容，推进了离线模仿学习。我们假设所有的人类示范都在尝试传达一个单一且一致的策略，而行为中的噪声和次优性混淆了数据并引入了无意中的复杂性。为了恢复底层策略并学习人类教师的意图，我们引入了 Counter-BC，一种行为克隆的通用版本。Counter-BC 扩展了给定的数据集，包括靠近人类示范的动作（即，人类教师可能意图展示但未实际展示的反事实动作）。在训练过程中，Counter-BC 自动修改扩展现有区域中的人类示范，以达到一个简单且一致的策略，该策略解释了人类数据集中的基本趋势。理论上，我们证明 Counter-BC 可以从不完善的数据、多位用户和不同技能水平的教师中提取所需策略。实验上，我们在嘈杂示范、标准化数据集和真实人类教师的模拟和真实世界场景中，将 Counter-BC 与最先进的替代方法进行了比较。更多信息及视频请访问：this https URL', 'title_zh': '反事实行为克隆：从 imperfect 人类示范中进行离线模仿学习'}
{'arxiv_id': 'arXiv:2505.10755', 'title': 'Infinigen-Sim: Procedural Generation of Articulated Simulation Assets', 'authors': 'Abhishek Joshi, Beining Han, Jack Nugent, Yiming Zuo, Jonathan Liu, Hongyu Wen, Stamatis Alexandropoulos, Tao Sun, Alexander Raistrick, Gaowen Liu, Yi Shao, Jia Deng', 'link': 'https://arxiv.org/abs/2505.10755', 'abstract': 'We introduce Infinigen-Sim, a toolkit which enables users to create diverse and realistic articulated object procedural generators. These tools are composed of high-level utilities for use creating articulated assets in Blender, as well as an export pipeline to integrate the resulting assets into common robotics simulators. We demonstrate our system by creating procedural generators for 5 common articulated object categories. Experiments show that assets sampled from these generators are useful for movable object segmentation, training generalizable reinforcement learning policies, and sim-to-real transfer of imitation learning policies.', 'abstract_zh': 'Infinigen-Sim：一种用于创建多样化和现实主义 articulated 物体过程生成器的工具包', 'title_zh': 'Infinigen-Sim：articulated simulation资产的程序生成'}
{'arxiv_id': 'arXiv:2505.10696', 'title': 'TartanGround: A Large-Scale Dataset for Ground Robot Perception and Navigation', 'authors': 'Manthan Patel, Fan Yang, Yuheng Qiu, Cesar Cadena, Sebastian Scherer, Marco Hutter, Wenshan Wang', 'link': 'https://arxiv.org/abs/2505.10696', 'abstract': 'We present TartanGround, a large-scale, multi-modal dataset to advance the perception and autonomy of ground robots operating in diverse environments. This dataset, collected in various photorealistic simulation environments includes multiple RGB stereo cameras for 360-degree coverage, along with depth, optical flow, stereo disparity, LiDAR point clouds, ground truth poses, semantic segmented images, and occupancy maps with semantic labels. Data is collected using an integrated automatic pipeline, which generates trajectories mimicking the motion patterns of various ground robot platforms, including wheeled and legged robots. We collect 910 trajectories across 70 environments, resulting in 1.5 million samples. Evaluations on occupancy prediction and SLAM tasks reveal that state-of-the-art methods trained on existing datasets struggle to generalize across diverse scenes. TartanGround can serve as a testbed for training and evaluation of a broad range of learning-based tasks, including occupancy prediction, SLAM, neural scene representation, perception-based navigation, and more, enabling advancements in robotic perception and autonomy towards achieving robust models generalizable to more diverse scenarios. The dataset and codebase for data collection will be made publicly available upon acceptance. Webpage: this https URL', 'abstract_zh': '我们介绍TartanGround：一个大规模多模态数据集，旨在推进地面机器人在多样化环境中感知与自主性的进步。', 'title_zh': 'TartanGround：大规模机器人地面感知与导航数据集'}
{'arxiv_id': 'arXiv:2505.10695', 'title': 'Predicting Human Behavior in Autonomous Systems: A Collaborative Machine Teaching Approach for Reducing Transfer of Control Events', 'authors': 'Julian Wolter, Amr Gomaa', 'link': 'https://arxiv.org/abs/2505.10695', 'abstract': "As autonomous systems become integral to various industries, effective strategies for fault handling are essential to ensure reliability and efficiency. Transfer of Control (ToC), a traditional approach for interrupting automated processes during faults, is often triggered unnecessarily in non-critical situations. To address this, we propose a data-driven method that uses human interaction data to train AI models capable of preemptively identifying and addressing issues or assisting users in resolution. Using an interactive tool simulating an industrial vacuum cleaner, we collected data and developed an LSTM-based model to predict user behavior. Our findings reveal that even data from non-experts can effectively train models to reduce unnecessary ToC events, enhancing the system's robustness. This approach highlights the potential of AI to learn directly from human problem-solving behaviors, complementing sensor data to improve industrial automation and human-AI collaboration.", 'abstract_zh': '随着自主系统在各个行业中的广泛应用，有效的故障处理策略对于确保可靠性和效率至关重要。传统的人工控制转移（ToC）方法往往在非关键情况下被不必要地触发。为了解决这个问题，我们提出了一种基于数据的方法，该方法利用人类交互数据来训练AI模型，能够预判性地识别和解决潜在问题或协助用户解决问题。通过模拟工业吸尘器的交互工具收集数据，并开发了基于LSTM的模型以预测用户行为。研究发现，即使是非专家的数据也能有效训练模型以减少不必要的ToC事件，从而增强系统的鲁棒性。该方法突显了AI直接从人类问题解决行为中学习的潜力，弥补传感器数据以改善工业自动化和人机协作。', 'title_zh': '使用合作机器教学方法预测自主系统中的人类行为：减少控制转移事件的研究'}
{'arxiv_id': 'arXiv:2505.10694', 'title': 'Modular Robot Control with Motor Primitives', 'authors': 'Moses C. Nah, Johannes Lachner, Neville Hogan', 'link': 'https://arxiv.org/abs/2505.10694', 'abstract': 'Despite a slow neuromuscular system, humans easily outperform modern robot technology, especially in physical contact tasks. How is this possible? Biological evidence indicates that motor control of biological systems is achieved by a modular organization of motor primitives, which are fundamental building blocks of motor behavior. Inspired by neuro-motor control research, the idea of using simpler building blocks has been successfully used in robotics. Nevertheless, a comprehensive formulation of modularity for robot control remains to be established. In this paper, we introduce a modular framework for robot control using motor primitives. We present two essential requirements to achieve modular robot control: independence of modules and closure of stability. We describe key control modules and demonstrate that a wide range of complex robotic behaviors can be generated from this small set of modules and their combinations. The presented modular control framework demonstrates several beneficial properties for robot control, including task-space control without solving Inverse Kinematics, addressing the problems of kinematic singularity and kinematic redundancy, and preserving passivity for contact and physical interactions. Further advantages include exploiting kinematic singularity to maintain high external load with low torque compensation, as well as controlling the robot beyond its end-effector, extending even to external objects. Both simulation and actual robot experiments are presented to validate the effectiveness of our modular framework. We conclude that modularity may be an effective constructive framework for achieving robotic behaviors comparable to human-level performance.', 'abstract_zh': '虽然神经肌肉系统反应较慢，但人类在物理接触任务中仍然能够超越现代机器人技术。这是如何可能的？生物学证据表明，生物系统的运动控制是由运动原型模块化组织实现的，这些原型是运动行为的基本构建块。受神经运动控制研究的启发，使用更简单的模块构建单元的方法已经在机器人技术中取得了成功。然而，机器人控制模块化的全面形式仍然需要建立。在本文中，我们提出了一个基于运动原型的机器人控制模块化框架。我们介绍了实现模块化机器人控制的两个关键要求：模块独立性和稳定性封闭性。我们描述了关键控制模块，并证明通过这些模块及其组合，可以生成广泛的复杂机器人行为。所提出的模块化控制框架展示了机器人控制的若干有益特性，包括无需求解逆运动学的空间控制、解决运动奇异性和运动冗余性问题、保持接触和物理交互中的功率守恒，以及其他优点，如利用运动奇异点在低扭矩补偿下维持高外部负载，以及控制机器人超出其末端执行器，甚至扩展到外部物体。我们通过仿真和实际机器人实验验证了模块化框架的有效性。我们得出结论，模块化可能是一种实现接近人类水平性能的机器人行为的有效建构框架。', 'title_zh': '模模块化机器人控制与运动primitive'}
{'arxiv_id': 'arXiv:2505.11439', 'title': 'SurgPose: Generalisable Surgical Instrument Pose Estimation using Zero-Shot Learning and Stereo Vision', 'authors': 'Utsav Rai, Haozheng Xu, Stamatia Giannarou', 'link': 'https://arxiv.org/abs/2505.11439', 'abstract': 'Accurate pose estimation of surgical tools in Robot-assisted Minimally Invasive Surgery (RMIS) is essential for surgical navigation and robot control. While traditional marker-based methods offer accuracy, they face challenges with occlusions, reflections, and tool-specific designs. Similarly, supervised learning methods require extensive training on annotated datasets, limiting their adaptability to new tools. Despite their success in other domains, zero-shot pose estimation models remain unexplored in RMIS for pose estimation of surgical instruments, creating a gap in generalising to unseen surgical tools. This paper presents a novel 6 Degrees of Freedom (DoF) pose estimation pipeline for surgical instruments, leveraging state-of-the-art zero-shot RGB-D models like the FoundationPose and SAM-6D. We advanced these models by incorporating vision-based depth estimation using the RAFT-Stereo method, for robust depth estimation in reflective and textureless environments. Additionally, we enhanced SAM-6D by replacing its instance segmentation module, Segment Anything Model (SAM), with a fine-tuned Mask R-CNN, significantly boosting segmentation accuracy in occluded and complex conditions. Extensive validation reveals that our enhanced SAM-6D surpasses FoundationPose in zero-shot pose estimation of unseen surgical instruments, setting a new benchmark for zero-shot RGB-D pose estimation in RMIS. This work enhances the generalisability of pose estimation for unseen objects and pioneers the application of RGB-D zero-shot methods in RMIS.', 'abstract_zh': '基于RGB-D的零样本手术器械6自由度姿态估计方法在机器人辅助微创手术中的应用', 'title_zh': 'SurgPose: 使用零样本学习和立体视觉的可泛化手术器械姿态估计'}
{'arxiv_id': 'arXiv:2505.11383', 'title': 'Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation', 'authors': 'Zihan Wang, Seungjun Lee, Gim Hee Lee', 'link': 'https://arxiv.org/abs/2505.11383', 'abstract': 'Vision-and-Language Navigation (VLN) is a core task where embodied agents leverage their spatial mobility to navigate in 3D environments toward designated destinations based on natural language instructions. Recently, video-language large models (Video-VLMs) with strong generalization capabilities and rich commonsense knowledge have shown remarkable performance when applied to VLN tasks. However, these models still encounter the following challenges when applied to real-world 3D navigation: 1) Insufficient understanding of 3D geometry and spatial semantics; 2) Limited capacity for large-scale exploration and long-term environmental memory; 3) Poor adaptability to dynamic and changing this http URL address these limitations, we propose Dynam3D, a dynamic layered 3D representation model that leverages language-aligned, generalizable, and hierarchical 3D representations as visual input to train 3D-VLM in navigation action prediction. Given posed RGB-D images, our Dynam3D projects 2D CLIP features into 3D space and constructs multi-level 3D patch-instance-zone representations for 3D geometric and semantic understanding with a dynamic and layer-wise update strategy. Our Dynam3D is capable of online encoding and localization of 3D instances, and dynamically updates them in changing environments to provide large-scale exploration and long-term memory capabilities for navigation. By leveraging large-scale 3D-language pretraining and task-specific adaptation, our Dynam3D sets new state-of-the-art performance on VLN benchmarks including R2R-CE, REVERIE-CE and NavRAG-CE under monocular settings. Furthermore, experiments for pre-exploration, lifelong memory, and real-world robot validate the effectiveness of practical deployment.', 'abstract_zh': '动态层次三维表示模型：面向真实世界3D导航的视语音导航（VLN）', 'title_zh': 'Dynam3D: 动态分层3D令牌助力视觉-语言导航大模型'}
{'arxiv_id': 'arXiv:2505.11376', 'title': 'Decoupling Collision Avoidance in and for Optimal Control using Least-Squares Support Vector Machines', 'authors': 'Dries Dirckx, Wilm Decré, Jan Swevers', 'link': 'https://arxiv.org/abs/2505.11376', 'abstract': "This paper details an approach to linearise differentiable but non-convex collision avoidance constraints tailored to convex shapes. It revisits introducing differential collision avoidance constraints for convex objects into an optimal control problem (OCP) using the separating hyperplane theorem. By framing this theorem as a classification problem, the hyperplanes are eliminated as optimisation variables from the OCP. This effectively transforms non-convex constraints into linear constraints. A bi-level algorithm computes the hyperplanes between the iterations of an optimisation solver and subsequently embeds them as parameters into the OCP. Experiments demonstrate the approach's favourable scalability towards cluttered environments and its applicability to various motion planning approaches. It decreases trajectory computation times between 50\\% and 90\\% compared to a state-of-the-art approach that directly includes the hyperplanes as variables in the optimal control problem.", 'abstract_zh': '本文详细介绍了针对凸形物体的一种差分碰撞避免约束线性化的办法。通过重新审视使用分离超平面定理将差分碰撞避免约束引入最优控制问题（OCP）的方法，将分离超平面定理框架化为分类问题，从而在OCP中消除超平面作为优化变量，将非凸约束有效转换为线性约束。双层算法在优化求解器的迭代过程中计算超平面，并将其嵌入为参数到OCP中。实验表明，该方法在复杂环境下的可扩展性优越，并适用于多种运动规划方法，与直接将超平面作为变量纳入最优控制问题的最新方法相比，轨迹计算时间减少50%至90%。', 'title_zh': '基于最小二乘支持向量机的碰撞避免与最优控制解耦'}
{'arxiv_id': 'arXiv:2505.11247', 'title': 'LD-Scene: LLM-Guided Diffusion for Controllable Generation of Adversarial Safety-Critical Driving Scenarios', 'authors': 'Mingxing Peng, Yuting Xie, Xusen Guo, Ruoyu Yao, Hai Yang, Jun Ma', 'link': 'https://arxiv.org/abs/2505.11247', 'abstract': 'Ensuring the safety and robustness of autonomous driving systems necessitates a comprehensive evaluation in safety-critical scenarios. However, these safety-critical scenarios are rare and difficult to collect from real-world driving data, posing significant challenges to effectively assessing the performance of autonomous vehicles. Typical existing methods often suffer from limited controllability and lack user-friendliness, as extensive expert knowledge is essentially required. To address these challenges, we propose LD-Scene, a novel framework that integrates Large Language Models (LLMs) with Latent Diffusion Models (LDMs) for user-controllable adversarial scenario generation through natural language. Our approach comprises an LDM that captures realistic driving trajectory distributions and an LLM-based guidance module that translates user queries into adversarial loss functions, facilitating the generation of scenarios aligned with user queries. The guidance module integrates an LLM-based Chain-of-Thought (CoT) code generator and an LLM-based code debugger, enhancing the controllability and robustness in generating guidance functions. Extensive experiments conducted on the nuScenes dataset demonstrate that LD-Scene achieves state-of-the-art performance in generating realistic, diverse, and effective adversarial scenarios. Furthermore, our framework provides fine-grained control over adversarial behaviors, thereby facilitating more effective testing tailored to specific driving scenarios.', 'abstract_zh': '确保自主驾驶系统的安全性和稳健性需要在关键安全性场景中进行全面评估。然而，这些关键安全性场景在真实世界驾驶数据中稀少且难以收集，给有效评估自主车辆性能带来了重大挑战。现有典型方法往往控制能力有限且缺乏用户友好性，因为需要广泛的专业知识。为应对这些挑战，我们提出LD-Scene，这一将大规模语言模型（LLMs）与潜在扩散模型（LDMs）相结合的新型框架，以自然语言实现用户可控的对抗性场景生成。该方法包括一个捕捉现实驾驶轨迹分布的LDM以及基于LLM的指导模块，将用户查询转换为对抗损失函数，促进生成与用户查询对齐的场景。指导模块集成了基于LLM的Chain-of-Thought（CoT）代码生成器和基于LLM的代码调试器，增强了生成指导函数的控制能力和稳健性。在nuScenes数据集上的广泛实验显示，LD-Scene在生成现实、多样且有效的对抗性场景方面达到了最先进的性能。此外，我们的框架提供对对抗行为的细粒度控制，从而促进了更有效的针对特定驾驶场景的测试。', 'title_zh': 'LD-场景：受LLM指导的扩散模型用于生成可控制的对抗性安全关键驾驶场景'}
{'arxiv_id': 'arXiv:2505.11191', 'title': 'Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration', 'authors': 'Kasra Borazjani, Payam Abdisarabshali, Fardis Nadimi, Naji Khosravan, Minghui Liwang, Xianbin Wang, Yiguang Hong, Seyyedali Hosseinalipour', 'link': 'https://arxiv.org/abs/2505.11191', 'abstract': 'As embodied AI systems become increasingly multi-modal, personalized, and interactive, they must learn effectively from diverse sensory inputs, adapt continually to user preferences, and operate safely under resource and privacy constraints. These challenges expose a pressing need for machine learning models capable of swift, context-aware adaptation while balancing model generalization and personalization. Here, two methods emerge as suitable candidates, each offering parts of these capabilities: Foundation Models (FMs) provide a pathway toward generalization across tasks and modalities, whereas Federated Learning (FL) offers the infrastructure for distributed, privacy-preserving model updates and user-level model personalization. However, when used in isolation, each of these approaches falls short of meeting the complex and diverse capability requirements of real-world embodied environments. In this vision paper, we introduce Federated Foundation Models (FFMs) for embodied AI, a new paradigm that unifies the strengths of multi-modal multi-task (M3T) FMs with the privacy-preserving distributed nature of FL, enabling intelligent systems at the wireless edge. We collect critical deployment dimensions of FFMs in embodied AI ecosystems under a unified framework, which we name "EMBODY": Embodiment heterogeneity, Modality richness and imbalance, Bandwidth and compute constraints, On-device continual learning, Distributed control and autonomy, and Yielding safety, privacy, and personalization. For each, we identify concrete challenges and envision actionable research directions. We also present an evaluation framework for deploying FFMs in embodied AI systems, along with the associated trade-offs.', 'abstract_zh': '面向体态智能的联合基础模型：统一隐私保护分布式学习与多模态多任务适应性', 'title_zh': '多模态多任务（M3T）联邦基础模型在具身人工智能中的边缘集成：潜力与挑战'}
{'arxiv_id': 'arXiv:2505.11136', 'title': 'Reinforcement Learning for AMR Charging Decisions: The Impact of Reward and Action Space Design', 'authors': 'Janik Bischoff, Alexandru Rinciog, Anne Meyer', 'link': 'https://arxiv.org/abs/2505.11136', 'abstract': 'We propose a novel reinforcement learning (RL) design to optimize the charging strategy for autonomous mobile robots in large-scale block stacking warehouses. RL design involves a wide array of choices that can mostly only be evaluated through lengthy experimentation. Our study focuses on how different reward and action space configurations, ranging from flexible setups to more guided, domain-informed design configurations, affect the agent performance. Using heuristic charging strategies as a baseline, we demonstrate the superiority of flexible, RL-based approaches in terms of service times. Furthermore, our findings highlight a trade-off: While more open-ended designs are able to discover well-performing strategies on their own, they may require longer convergence times and are less stable, whereas guided configurations lead to a more stable learning process but display a more limited generalization potential. Our contributions are threefold. First, we extend SLAPStack, an open-source, RL-compatible simulation-framework to accommodate charging strategies. Second, we introduce a novel RL design for tackling the charging strategy problem. Finally, we introduce several novel adaptive baseline heuristics and reproducibly evaluate the design using a Proximal Policy Optimization agent and varying different design configurations, with a focus on reward.', 'abstract_zh': '我们提出了一种新颖的强化学习（RL）设计，以优化大型区块堆叠仓库中自主移动机器人充电策略。我们的研究重点在于不同的奖励和动作空间配置，从灵活设置到更具指导性和领域导向的设计配置，如何影响智能体的表现。基于启发式充电策略的基准，我们展示了基于RL的灵活方法在服务时间方面的优越性。我们的研究还揭示了一种权衡：虽然更加开放的设计能够自行发现高性能策略，但可能需要更长的收敛时间和较低的稳定性，而指导性配置则导致更稳定的训练过程，但展示出更有限的泛化潜力。我们的贡献包括三个方面：首先，我们将SLAPStack扩展为一种兼容RL的开源仿真框架，以容纳充电策略；其次，我们提出了一种新的RL设计以解决充电策略问题；最后，我们引入了几种新的自适应基线启发式方法，并通过改变不同的设计配置和奖励策略，使用Proximal Policy Optimization智能体进行了可重现的评估。', 'title_zh': '基于强化学习的AMR充电决策：奖励和动作空间设计的影响'}
{'arxiv_id': 'arXiv:2505.10947', 'title': 'Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov Functions', 'authors': 'Kehan Long, Jorge Cortés, Nikolay Atanasov', 'link': 'https://arxiv.org/abs/2505.10947', 'abstract': 'We study the problem of certifying the stability of closed-loop systems under control policies derived from optimal control or reinforcement learning (RL). Classical Lyapunov methods require a strict step-wise decrease in the Lyapunov function but such a certificate is difficult to construct for a learned control policy. The value function associated with an RL policy is a natural Lyapunov function candidate but it is not clear how it should be modified. To gain intuition, we first study the linear quadratic regulator (LQR) problem and make two key observations. First, a Lyapunov function can be obtained from the value function of an LQR policy by augmenting it with a residual term related to the system dynamics and stage cost. Second, the classical Lyapunov decrease requirement can be relaxed to a generalized Lyapunov condition requiring only decrease on average over multiple time steps. Using this intuition, we consider the nonlinear setting and formulate an approach to learn generalized Lyapunov functions by augmenting RL value functions with neural network residual terms. Our approach successfully certifies the stability of RL policies trained on Gymnasium and DeepMind Control benchmarks. We also extend our method to jointly train neural controllers and stability certificates using a multi-step Lyapunov loss, resulting in larger certified inner approximations of the region of attraction compared to the classical Lyapunov approach. Overall, our formulation enables stability certification for a broad class of systems with learned policies by making certificates easier to construct, thereby bridging classical control theory and modern learning-based methods.', 'abstract_zh': '研究基于最优控制或强化学习策略的闭环系统稳定性的认证问题', 'title_zh': '使用广义李雅普诺夫函数认证强化学习策略的稳定性'}
{'arxiv_id': 'arXiv:2505.10816', 'title': 'mmMirror: Device Free mmWave Indoor NLoS Localization Using Van-Atta-Array IRS', 'authors': 'Yihe Yan, Zhenguo Shi, Yanxiang Wang, Cheng Jiang, Chun Tung Chou, Wen Hu', 'link': 'https://arxiv.org/abs/2505.10816', 'abstract': 'Industry 4.0 is transforming manufacturing and logistics by integrating robots into shared human environments, such as factories, warehouses, and healthcare facilities. However, the risk of human-robot collisions, especially in Non-Line-of-Sight (NLoS) scenarios like around corners, remains a critical challenge. Existing solutions, such as vision-based and LiDAR systems, often fail under occlusion, lighting constraints, or privacy concerns, while RF-based systems are limited by range and accuracy.\nTo address these limitations, we propose mmMirror, a novel system leveraging a Van Atta Array-based millimeter-wave (mmWave) reconfigurable intelligent reflecting surface (IRS) for precise, device-free NLoS localization. mmMirror integrates seamlessly with existing frequency-modulated continuous-wave (FMCW) radars and offers: (i) robust NLoS localization with centimeter-level accuracy at ranges up to 3 m, (ii) seamless uplink and downlink communication between radar and IRS, (iii) support for multi-radar and multi-target scenarios via dynamic beam steering, and (iv) reduced scanning latency through adaptive time slot allocation. Implemented using commodity 24 GHz radars and a PCB-based IRS prototype, mmMirror demonstrates its potential in enabling safe human-robot interactions in dynamic and complex environments.', 'abstract_zh': '基于Van Atta阵列毫米波可重构智能反射表面的mmMirror：非视距场景下的精确无设备定位系统', 'title_zh': 'mmMirror: 基于Van-Atta-阵列IRS的无设备毫米波室内非视距定位'}
{'arxiv_id': 'arXiv:2505.10705', 'title': 'Embodied AI in Machine Learning -- is it Really Embodied?', 'authors': 'Matej Hoffmann, Shubhan Parag Patni', 'link': 'https://arxiv.org/abs/2505.10705', 'abstract': 'Embodied Artificial Intelligence (Embodied AI) is gaining momentum in the machine learning communities with the goal of leveraging current progress in AI (deep learning, transformers, large language and visual-language models) to empower robots. In this chapter we put this work in the context of "Good Old-Fashioned Artificial Intelligence" (GOFAI) (Haugeland, 1989) and the behavior-based or embodied alternatives (R. A. Brooks 1991; Pfeifer and Scheier 2001). We claim that the AI-powered robots are only weakly embodied and inherit some of the problems of GOFAI. Moreover, we review and critically discuss the possibility of cross-embodiment learning (Padalkar et al. 2024). We identify fundamental roadblocks and propose directions on how to make progress.', 'abstract_zh': '具身人工智能：从传统人工智能到具身学习的研究', 'title_zh': '机器学习中的具身AI——它真地是具身的吗？'}
{'arxiv_id': 'arXiv:2505.10690', 'title': 'Decision Making in Urban Traffic: A Game Theoretic Approach for Autonomous Vehicles Adhering to Traffic Rules', 'authors': 'Keqi Shu, Minghao Ning, Ahmad Alghooneh, Shen Li, Mohammad Pirani, Amir Khajepour', 'link': 'https://arxiv.org/abs/2505.10690', 'abstract': 'One of the primary challenges in urban autonomous vehicle decision-making and planning lies in effectively managing intricate interactions with diverse traffic participants characterized by unpredictable movement patterns. Additionally, interpreting and adhering to traffic regulations within rapidly evolving traffic scenarios pose significant hurdles. This paper proposed a rule-based autonomous vehicle decision-making and planning framework which extracts right-of-way from traffic rules to generate behavioural parameters, integrating them to effectively adhere to and navigate through traffic regulations. The framework considers the strong interaction between traffic participants mathematically by formulating the decision-making and planning problem into a differential game. By finding the Nash equilibrium of the problem, the autonomous vehicle is able to find optimal decisions. The proposed framework was tested under simulation as well as full-size vehicle platform, the results show that the ego vehicle is able to safely interact with surrounding traffic participants while adhering to traffic rules.', 'abstract_zh': '城市自动驾驶车辆决策与规划中的主要挑战在于有效地管理多样且不可预测的交通参与者之间的复杂交互。此外，在快速变化的交通场景中解释和遵守交通规则也带来了重大障碍。本文提出了一种基于规则的自动驾驶车辆决策与规划框架，从交通规则中提取优先通行权，生成行为参数，并将这些参数整合以有效遵守和导航通过交通规则。该框架通过将决策与规划问题形式化为微分博弈来考虑交通参与者之间强烈的交互作用。通过找到问题的纳什均衡，自动驾驶车辆能够找到最优决策。所提出框架在仿真和全尺寸车辆平台上进行了测试，结果显示自动驾驶车辆能够在遵守交通规则的同时安全地与周围交通参与者互动。', 'title_zh': '城市交通中的决策制定：一种自主车辆遵守交通规则的游戏理论方法'}
{'arxiv_id': 'arXiv:2505.10646', 'title': 'Accelerating Visual-Policy Learning through Parallel Differentiable Simulation', 'authors': 'Haoxiang You, Yilang Liu, Ian Abraham', 'link': 'https://arxiv.org/abs/2505.10646', 'abstract': 'In this work, we propose a computationally efficient algorithm for visual policy learning that leverages differentiable simulation and first-order analytical policy gradients. Our approach decouple the rendering process from the computation graph, enabling seamless integration with existing differentiable simulation ecosystems without the need for specialized differentiable rendering software. This decoupling not only reduces computational and memory overhead but also effectively attenuates the policy gradient norm, leading to more stable and smoother optimization. We evaluate our method on standard visual control benchmarks using modern GPU-accelerated simulation. Experiments show that our approach significantly reduces wall-clock training time and consistently outperforms all baseline methods in terms of final returns. Notably, on complex tasks such as humanoid locomotion, our method achieves a $4\\times$ improvement in final return, and successfully learns a humanoid running policy within 4 hours on a single GPU.', 'abstract_zh': '本研究提出了一种利用可微模拟和一阶分析性策略梯度的计算高效视觉策略学习算法。该方法将渲染过程与计算图解耦，使其能够无缝集成到现有的可微模拟生态系统中，无需专门的可微渲染软件。这种解耦不仅减少了计算和内存开销，还有效地减弱了策略梯度范数，从而使优化更加稳定和平滑。我们在使用现代GPU加速模拟的标准视觉控制基准上评估了该方法。实验表明，该方法显著减少了 wall-clock 训练时间，并在最终回报上始终优于所有基线方法。特别是在复杂任务如人形机器人行走中，该方法在单个GPU上4小时内成功学习了一个人形机器人跑步策略，并实现了最终回报4倍的提升。', 'title_zh': '通过并行可微模拟加速视觉策略学习'}
