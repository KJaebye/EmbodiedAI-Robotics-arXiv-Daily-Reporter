{'arxiv_id': 'arXiv:2504.10225', 'title': 'A Quasi-Steady-State Black Box Simulation Approach for the Generation of g-g-g-v Diagrams', 'authors': 'Frederik Werner, Simon Sagmeister, Mattia Piccinini, Johannes Betz', 'link': 'https://arxiv.org/abs/2504.10225', 'abstract': 'The classical g-g diagram, representing the achievable acceleration space for a vehicle, is commonly used as a constraint in trajectory planning and control due to its computational simplicity. To address non-planar road geometries, this concept can be extended to incorporate g-g constraints as a function of vehicle speed and vertical acceleration, commonly referred to as g-g-g-v diagrams. However, the estimation of g-g-g-v diagrams is an open problem. Existing simulation-based approaches struggle to isolate non-transient, open-loop stable states across all combinations of speed and acceleration, while optimization-based methods often require simplified vehicle equations and have potential convergence issues. In this paper, we present a novel, open-source, quasi-steady-state black box simulation approach that applies a virtual inertial force in the longitudinal direction. The method emulates the load conditions associated with a specified longitudinal acceleration while maintaining constant vehicle speed, enabling open-loop steering ramps in a purely QSS manner. Appropriate regulation of the ramp steer rate inherently mitigates transient vehicle dynamics when determining the maximum feasible lateral acceleration. Moreover, treating the vehicle model as a black box eliminates model mismatch issues, allowing the use of high-fidelity or proprietary vehicle dynamics models typically unsuited for optimization approaches. An open-source version of the proposed method is available at: this https URL', 'abstract_zh': '经典g-g图，表示车辆可实现的加速度空间，由于其计算简单常被用作轨迹规划和控制中的约束条件。为了应对非平面路形几何结构，该概念可以扩展为结合速度和垂向加速度的g-g约束，通常称为g-g-g-v图。然而，g-g-g-v图的估算仍是一个开放问题。现有基于仿真的方法难以在所有速度和加速度组合下隔离出非瞬态、开环稳定状态，而基于优化的方法通常需要简化车辆方程，且存在收敛性问题。在本文中，我们提出了一种新颖的开源准稳态黑盒仿真方法，在纵向方向应用虚拟惯性力。该方法模拟了指定纵向加速度关联的载荷条件，同时保持恒定车速，以纯准稳态方式实现开环转向坡道。合理调节坡道转向速率本身可以缓解确定最大可行侧向加速度时的瞬态车辆动力学。此外，将车辆模型视为黑盒消除了模型不匹配问题，允许使用通常不适用于优化方法的高保真或专有车辆动力学模型。所提方法的开源版本可在以下链接获取：this https URL', 'title_zh': '擬稳态黑箱仿真方法生成g-g-g-v图示'}
{'arxiv_id': 'arXiv:2504.09882', 'title': 'SIO-Mapper: A Framework for Lane-Level HD Map Construction Using Satellite Images and OpenStreetMap with No On-Site Visits', 'authors': 'Younghun Cho, Jee-Hwan Ryu', 'link': 'https://arxiv.org/abs/2504.09882', 'abstract': 'High-definition (HD) maps, particularly those containing lane-level information regarded as ground truth, are crucial for vehicle localization research. Traditionally, constructing HD maps requires highly accurate sensor measurements collection from the target area, followed by manual annotation to assign semantic information. Consequently, HD maps are limited in terms of geographic coverage. To tackle this problem, in this paper, we propose SIO-Mapper, a novel lane-level HD map construction framework that constructs city-scale maps without physical site visits by utilizing satellite images and OpenStreetmap data. One of the key contributions of SIO-Mapper is its ability to extract lane information more accurately by introducing SIO-Net, a novel deep learning network that integrates features from satellite image and OpenStreetmap using both Transformer-based and convolution-based encoders. Furthermore, to overcome challenges in merging lanes over large areas, we introduce a novel lane integration methodology that combines cluster-based and graph-based approaches. This algorithm ensures the seamless aggregation of lane segments with high accuracy and coverage, even in complex road environments. We validated SIO-Mapper on the Naver Labs Open Dataset and NuScenes dataset, demonstrating better performance in various environments including Korea, the United States, and Singapore compared to the state-of-the-art lane-level HD mapconstruction methods.', 'abstract_zh': '高-definition (HD) 地图，特别是包含车道级信息且作为ground truth的HD地图，对于车辆定位研究至关重要。传统的HD地图构建方法需要在目标区域收集高精度传感器测量数据，并进行人工标注以分配语义信息。因此，HD地图在地理覆盖范围上受到限制。为解决这一问题，本文提出了一种名为SIO-Mapper的新颖车道级HD地图构建框架，该框架通过利用卫星图像和OpenStreetMap数据构建城市规模的地图，而无需进行实地考察。SIO-Mapper的一个关键贡献是通过引入结合卫星图像和OpenStreetMap特征的新型深度学习网络SIO-Net来更准确地提取车道信息，该网络集成了基于Transformer和卷积编码器。此外，为了克服大规模区域车道合并的挑战，我们引入了一种新的车道合并方法，结合了基于聚类和基于图的方法。该算法确保即使在复杂道路环境中也能以高度准确和广泛的精度无缝聚合车道片段。我们在Naver Labs Open Dataset和NuScenes数据集上验证了SIO-Mapper，结果表明在韩国、美国和新加坡等不同环境中，其性能优于最先进的车道级HD地图构建方法。', 'title_zh': 'SIO-Mapper：基于卫星图像和OpenStreetMap的无需现场勘查的车道级高清地图构建框架'}
{'arxiv_id': 'arXiv:2504.09495', 'title': 'Debiasing 6-DOF IMU via Hierarchical Learning of Continuous Bias Dynamics', 'authors': 'Ben Liu, Tzu-Yuan Lin, Wei Zhang, Maani Ghaffari', 'link': 'https://arxiv.org/abs/2504.09495', 'abstract': 'This paper develops a deep learning approach to the online debiasing of IMU gyroscopes and accelerometers. Most existing methods rely on implicitly learning a bias term to compensate for raw IMU data. Explicit bias learning has recently shown its potential as a more interpretable and motion-independent alternative. However, it remains underexplored and faces challenges, particularly the need for ground truth bias data, which is rarely available. To address this, we propose a neural ordinary differential equation (NODE) framework that explicitly models continuous bias dynamics, requiring only pose ground truth, often available in datasets. This is achieved by extending the canonical NODE framework to the matrix Lie group for IMU kinematics with a hierarchical training strategy. The validation on two public datasets and one real-world experiment demonstrates significant accuracy improvements in IMU measurements, reducing errors in both pure IMU integration and visual-inertial odometry.', 'abstract_zh': '基于神经常微分方程的IMU陀螺仪和加速度计在线去偏见的深度学习方法', 'title_zh': '基于层次学习连续偏置动态的6-DOF IMU去偏差化'}
{'arxiv_id': 'arXiv:2504.09461', 'title': 'ADDT -- A Digital Twin Framework for Proactive Safety Validation in Autonomous Driving Systems', 'authors': 'Bo Yu, Chaoran Yuan, Zishen Wan, Jie Tang, Fadi Kurdahi, Shaoshan Liu', 'link': 'https://arxiv.org/abs/2504.09461', 'abstract': 'Autonomous driving systems continue to face safety-critical failures, often triggered by rare and unpredictable corner cases that evade conventional testing. We present the Autonomous Driving Digital Twin (ADDT) framework, a high-fidelity simulation platform designed to proactively identify hidden faults, evaluate real-time performance, and validate safety before deployment. ADDT combines realistic digital models of driving environments, vehicle dynamics, sensor behavior, and fault conditions to enable scalable, scenario-rich stress-testing under diverse and adverse conditions. It supports adaptive exploration of edge cases using reinforcement-driven techniques, uncovering failure modes that physical road testing often misses. By shifting from reactive debugging to proactive simulation-driven validation, ADDT enables a more rigorous and transparent approach to autonomous vehicle safety engineering. To accelerate adoption and facilitate industry-wide safety improvements, the entire ADDT framework has been released as open-source software, providing developers with an accessible and extensible tool for comprehensive safety testing at scale.', 'abstract_zh': '自主驾驶系统继续面临安全关键性故障，这些故障通常由传统测试难以覆盖的罕见且不可预测的边缘情况触发。我们提出了自主驾驶数字双胞胎（ADDT）框架，这是一个高保真度的仿真平台，旨在主动识别隐藏故障、评估实时性能并在部署前验证安全性。ADDT 结合了驾驶环境、车辆动力学、传感器行为和故障条件的现实数字模型，以实现多样化且恶劣条件下的可扩展、场景丰富的压力测试。它支持使用强化学习驱动技术进行边缘案例的自适应探索，揭示物理道路测试往往难以发现的故障模式。通过从被动调试转向主动的仿真驱动验证，ADDT 使得自主车辆安全工程更加严谨和透明。为了加速采用并推动全行业的安全改进，整个ADDT框架已被开源发布，提供了开发人员进行全面安全测试的可访问且可扩展的工具。', 'title_zh': 'ADDT -- 一种用于自主驾驶系统前瞻性安全验证的数字孪生框架'}
{'arxiv_id': 'arXiv:2504.09836', 'title': 'Score Matching Diffusion Based Feedback Control and Planning of Nonlinear Systems', 'authors': 'Karthik Elamvazhuthi, Darshan Gadginmath, Fabio Pasqualetti', 'link': 'https://arxiv.org/abs/2504.09836', 'abstract': "We propose a novel control-theoretic framework that leverages principles from generative modeling -- specifically, Denoising Diffusion Probabilistic Models (DDPMs) -- to stabilize control-affine systems with nonholonomic constraints. Unlike traditional stochastic approaches, which rely on noise-driven dynamics in both forward and reverse processes, our method crucially eliminates the need for noise in the reverse phase, making it particularly relevant for control applications. We introduce two formulations: one where noise perturbs all state dimensions during the forward phase while the control system enforces time reversal deterministically, and another where noise is restricted to the control channels, embedding system constraints directly into the forward process.\nFor controllable nonlinear drift-free systems, we prove that deterministic feedback laws can exactly reverse the forward process, ensuring that the system's probability density evolves correctly without requiring artificial diffusion in the reverse phase. Furthermore, for linear time-invariant systems, we establish a time-reversal result under the second formulation. By eliminating noise in the backward process, our approach provides a more practical alternative to machine learning-based denoising methods, which are unsuitable for control applications due to the presence of stochasticity. We validate our results through numerical simulations on benchmark systems, including a unicycle model in a domain with obstacles, a driftless five-dimensional system, and a four-dimensional linear system, demonstrating the potential for applying diffusion-inspired techniques in linear, nonlinear, and settings with state space constraints.", 'abstract_zh': '我们提出了一种新颖的控制理论框架，利用生成模型原理——特别是去噪扩散概率模型（DDPMs）——来稳定具有非完整约束的控制协调系统。与依赖正向和逆向过程中的噪声驱动动力学的传统随机方法不同，我们的方法在逆向阶段完全消除了噪声的需要，使其特别适用于控制应用。我们引入了两种形式：一种是噪声在正向阶段扰动所有状态维度，而控制系统在时间反演过程中确定性地起作用；另一种是噪声仅限于控制通道，在正向过程中直接嵌入系统约束。对于可控的无漂移非线性系统，我们证明了确定性的反馈法则可以在正向过程中精确地实现时间反演，从而确保系统的概率密度正确演化，而不必在逆向阶段引入人工扩散。此外，对于线性时不变系统，在第二种形式下，我们建立了时间反演结果。通过在逆向过程中消除噪声，我们的方法提供了一种比基于机器学习的去噪方法更实用的替代方案，这类方法由于存在随机性而不适用于控制应用。我们通过在基准系统上的数值模拟验证了这些结果，包括具有障碍物的单轮车模型、无漂移的五维系统和四维线性系统，展示了以扩散启发技术应用于线性、非线性和状态空间约束设置中的潜力。', 'title_zh': '基于评分匹配扩散的非线性系统反馈控制与规划'}
{'arxiv_id': 'arXiv:2504.09517', 'title': 'RoboComm: A DID-based scalable and privacy-preserving Robot-to-Robot interaction over state channels', 'authors': 'Roshan Singh, Sushant Pandey', 'link': 'https://arxiv.org/abs/2504.09517', 'abstract': "In a multi robot system establishing trust amongst untrusted robots from different organisations while preserving a robot's privacy is a challenge. Recently decentralized technologies such as smart contract and blockchain are being explored for applications in robotics. However, the limited transaction processing and high maintenance cost hinder the widespread adoption of such approaches. Moreover, blockchain transactions be they on public or private permissioned blockchain are publically readable which further fails to preserve the confidentiality of the robot's data and privacy of the robot.\nIn this work, we propose RoboComm a Decentralized Identity based approach for privacy-preserving interaction between robots. With DID a component of Self-Sovereign Identity; robots can authenticate each other independently without relying on any third-party service. Verifiable Credentials enable private data associated with a robot to be stored within the robot's hardware, unlike existing blockchain based approaches where the data has to be on the blockchain. We improve throughput by allowing message exchange over state channels. Being a blockchain backed solution RoboComm provides a trustworthy system without relying on a single party. Moreover, we implement our proposed approach to demonstrate the feasibility of our solution.", 'abstract_zh': '基于分布式身份的身份保护型机器人通信方法', 'title_zh': 'RoboComm：基于DID的可扩展且隐私保护的机器人间状态通道交互'}
{'arxiv_id': 'arXiv:2504.10445', 'title': 'RealWebAssist: A Benchmark for Long-Horizon Web Assistance with Real-World Users', 'authors': 'Suyu Ye, Haojun Shi, Darren Shih, Hyokun Yun, Tanya Roosta, Tianmin Shu', 'link': 'https://arxiv.org/abs/2504.10445', 'abstract': "To achieve successful assistance with long-horizon web-based tasks, AI agents must be able to sequentially follow real-world user instructions over a long period. Unlike existing web-based agent benchmarks, sequential instruction following in the real world poses significant challenges beyond performing a single, clearly defined task. For instance, real-world human instructions can be ambiguous, require different levels of AI assistance, and may evolve over time, reflecting changes in the user's mental state. To address this gap, we introduce RealWebAssist, a novel benchmark designed to evaluate sequential instruction-following in realistic scenarios involving long-horizon interactions with the web, visual GUI grounding, and understanding ambiguous real-world user instructions. RealWebAssist includes a dataset of sequential instructions collected from real-world human users. Each user instructs a web-based assistant to perform a series of tasks on multiple websites. A successful agent must reason about the true intent behind each instruction, keep track of the mental state of the user, understand user-specific routines, and ground the intended tasks to actions on the correct GUI elements. Our experimental results show that state-of-the-art models struggle to understand and ground user instructions, posing critical challenges in following real-world user instructions for long-horizon web assistance.", 'abstract_zh': '实现长时间网络任务成功辅助，AI代理必须能够在长时间内依次遵循现实世界用户指令。为了填补这一空白，我们引入了RealWebAssist，一个用于评估在网络环境下进行长时间交互、可视化GUI定位以及理解模糊的现实世界用户指令中顺序指令跟随的新基准。RealWebAssist包含从真实世界人类用户收集的顺序指令数据集。每个用户指导基于网络的助手在多个网站上执行一系列任务。成功的代理必须理解每条指令的真实意图，跟踪用户的心理状态，理解用户特定的例行公事，并将目标任务与正确的GUI元素上的操作对接。我们的实验结果表明，最先进的模型难以理解并对接用户指令，在长时间网络辅助中遵循真实世界用户指令面临着关键挑战。', 'title_zh': 'RealWebAssist：一种基于真实用户的大规模网页辅助基准'}
{'arxiv_id': 'arXiv:2504.10412', 'title': 'AI-Driven Code Refactoring: Using Graph Neural Networks to Enhance Software Maintainability', 'authors': 'Gopichand Bandarupalli', 'link': 'https://arxiv.org/abs/2504.10412', 'abstract': 'This study explores Graph Neural Networks (GNNs) as a transformative tool for code refactoring, using abstract syntax trees (ASTs) to boost software maintainability. It analyzes a dataset of 2 million snippets from CodeSearchNet and a custom 75000-file GitHub Python corpus, comparing GNNs against rule-based SonarQube and decision trees. Metrics include cyclomatic complexity (target below 10), coupling (target below 5), and refactoring precision. GNNs achieve 92% accuracy, reducing complexity by 35% and coupling by 33%, outperforming SonarQube (78%, 16%) and decision trees (85%, 25%). Preprocessing fixed 60% of syntax errors. Bar graphs, tables, and AST visuals clarify results. This offers a scalable AI-driven path to cleaner codebases, which is crucial for software engineering.', 'abstract_zh': '本研究探讨图神经网络（GNNs）作为代码重构的变革性工具的应用，使用抽象语法树（ASTs）提升软件可维护性。它分析了CodeSearchNet的20万段代码片段和自定义的75000文件GitHub Python语料库，并将GNNs与基于规则的SonarQube和决策树进行比较。评估指标包括圈复杂度（目标值低于10）、内聚性（目标值低于5）和重构精度。GNNs实现了92%的准确率，降低了35%的复杂度和33%的内聚性，优于SonarQube（78%，16%）和决策树（85%，25%）。预处理固定了60%的语法错误。柱状图、表格和AST可视化图展示了结果。这为更清洁的代码库提供了一个可扩展的AI驱动路径，这对于软件工程至关重要。', 'title_zh': '基于AI的代码重构：使用图神经网络提升软件可维护性'}
{'arxiv_id': 'arXiv:2504.09635', 'title': 'A Two-Stage Interpretable Matching Framework for Causal Inference', 'authors': 'Sahil Shikalgar, Md. Noor-E-Alam', 'link': 'https://arxiv.org/abs/2504.09635', 'abstract': 'Matching in causal inference from observational data aims to construct treatment and control groups with similar distributions of covariates, thereby reducing confounding and ensuring an unbiased estimation of treatment effects. This matched sample closely mimics a randomized controlled trial (RCT), thus improving the quality of causal estimates. We introduce a novel Two-stage Interpretable Matching (TIM) framework for transparent and interpretable covariate matching. In the first stage, we perform exact matching across all available covariates. For treatment and control units without an exact match in the first stage, we proceed to the second stage. Here, we iteratively refine the matching process by removing the least significant confounder in each iteration and attempting exact matching on the remaining covariates. We learn a distance metric for the dropped covariates to quantify closeness to the treatment unit(s) within the corresponding strata. We used these high- quality matches to estimate the conditional average treatment effects (CATEs). To validate TIM, we conducted experiments on synthetic datasets with varying association structures and correlations. We assessed its performance by measuring bias in CATE estimation and evaluating multivariate overlap between treatment and control groups before and after matching. Additionally, we apply TIM to a real-world healthcare dataset from the Centers for Disease Control and Prevention (CDC) to estimate the causal effect of high cholesterol on diabetes. Our results demonstrate that TIM improves CATE estimates, increases multivariate overlap, and scales effectively to high-dimensional data, making it a robust tool for causal inference in observational data.', 'abstract_zh': '基于观察数据因果推断中的匹配旨在构建具有相似协变量分布的处理组和控制组，从而减少混杂因素，确保治疗效果的无偏估计。这种匹配样本类似于随机对照试验（RCT），从而提高了因果估计的质量。我们提出了一种新颖的两阶段可解释匹配（TIM）框架，以实现透明和可解释的协变量匹配。在第一阶段，我们在所有可用的协变量上进行精确匹配。对于在第一阶段中没有精确匹配的处理和控制单元，我们进入第二阶段。在第二阶段，我们通过在每次迭代中移除最不显著的混杂因素并尝试在剩余协变量上进行精确匹配来逐步改进匹配过程。我们为删除的协变量学习一个距离度量，以量化匹配单元在相应分层中的接近程度。我们使用这些高质量匹配来估计条件平均处理效果（CATE）。为了验证TIM的有效性，我们在具有不同关联结构和相关性的合成数据集上进行了实验，并通过测量CATE估计偏倚和评估匹配前后处理组和控制组的多变量重叠来评估其性能。此外，我们将TIM应用于美国疾病控制与预防中心（CDC）的实时医疗数据集，以估计高胆固醇对糖尿病的因果效应。我们的结果表明，TIM可以改进CATE估计，增加多变量重叠，并有效处理高维数据，使其成为观察数据中因果推断的一个稳健工具。', 'title_zh': '两阶段可解释匹配框架用于因果推理'}
{'arxiv_id': 'arXiv:2504.09582', 'title': 'Reduction of Supervision for Biomedical Knowledge Discovery', 'authors': 'Christos Theodoropoulos, Andrei Catalin Coman, James Henderson, Marie-Francine Moens', 'link': 'https://arxiv.org/abs/2504.09582', 'abstract': "Knowledge discovery is hindered by the increasing volume of publications and the scarcity of extensive annotated data. To tackle the challenge of information overload, it is essential to employ automated methods for knowledge extraction and processing. Finding the right balance between the level of supervision and the effectiveness of models poses a significant challenge. While supervised techniques generally result in better performance, they have the major drawback of demanding labeled data. This requirement is labor-intensive and time-consuming and hinders scalability when exploring new domains. In this context, our study addresses the challenge of identifying semantic relationships between biomedical entities (e.g., diseases, proteins) in unstructured text while minimizing dependency on supervision. We introduce a suite of unsupervised algorithms based on dependency trees and attention mechanisms and employ a range of pointwise binary classification methods. Transitioning from weakly supervised to fully unsupervised settings, we assess the methods' ability to learn from data with noisy labels. The evaluation on biomedical benchmark datasets explores the effectiveness of the methods. Our approach tackles a central issue in knowledge discovery: balancing performance with minimal supervision. By gradually decreasing supervision, we assess the robustness of pointwise binary classification techniques in handling noisy labels, revealing their capability to shift from weakly supervised to entirely unsupervised scenarios. Comprehensive benchmarking offers insights into the effectiveness of these techniques, suggesting an encouraging direction toward adaptable knowledge discovery systems, representing progress in creating data-efficient methodologies for extracting useful insights when annotated data is limited.", 'abstract_zh': '知识发现受出版物数量增加和标注数据稀缺性的阻碍。为应对信息过载的挑战，有必要采用自动化方法进行知识提取和处理。在监督程度和模型有效性之间找到合适的平衡是一项重大挑战。虽然监督技术通常能获得更好的性能，但它们的主要缺点是需要标注数据，这既耗时又费力，当探索新领域时会阻碍可扩展性。在此背景下，我们的研究旨在通过最小化对监督的依赖来识别生物医学实体（如疾病、蛋白质）之间的语义关系。我们引入了一套基于依存树和注意力机制的无监督算法，并采用了一系列点-wise二分类方法。从弱监督过渡到完全无监督设置，我们评估了这些方法从噪声标签数据中学习的能力。在生物医学基准数据集上的评估探索了这些方法的有效性。我们的方法解决了知识发现中的一个核心问题：在最小监督条件下平衡性能。通过逐渐减少监督，我们评估了点-wise二分类技术在处理噪声标签数据方面的鲁棒性，揭示了它们能够从弱监督场景过渡到完全无监督场景的能力。全面的基准测试为这些技术的有效性提供了见解，表明在标注数据有限的情况下开发高效数据提取系统的前景是令人鼓舞的。', 'title_zh': '生物医学知识发现中的监督减少'}
{'arxiv_id': 'arXiv:2504.09574', 'title': 'Improved FOX Optimization Algorithm', 'authors': 'Mahmood A. Jumaah, Yossra H. Ali, Tarik A. Rashid', 'link': 'https://arxiv.org/abs/2504.09574', 'abstract': 'Optimization algorithms are essential for solving many real-world problems. However, challenges such as premature convergence to local optima and the difficulty of effectively balancing exploration and exploitation often hinder their performance. To address these issues, this paper proposes an improved FOX optimization algorithm, Improved FOX (IFOX). The IFOX algorithm introduces a new adaptive mechanism for balancing exploration and exploitation based on fitness values. It also reduces the number of hyperparameters and simplifies the core equations of the original FOX. To evaluate its effectiveness, IFOX has been tested on classical uni-modal and multi-modal benchmark functions, as well as on benchmark sets from the Congress on Evolutionary Computation (CEC), in addition to two engineering design problems: Pressure Vessel Design and Economic Load Dispatch. The results show that IFOX outperforms existing optimization algorithms, achieving superior results on 51 benchmark functions. These findings underscore the strong potential of IFOX as a competitive and robust optimization algorithm for a wide range of applications.', 'abstract_zh': '改进的FOX优化算法：IFOX及其在多种优化问题中的应用', 'title_zh': '改进的FOX优化算法'}
{'arxiv_id': 'arXiv:2504.09302', 'title': 'Application of Contrastive Learning on ECG Data: Evaluating Performance in Japanese and Classification with Around 100 Labels', 'authors': 'Junichiro Takahashi, JingChuan Guan, Masataka Sato, Kaito Baba, Kazuto Haruguchi, Daichi Nagashima, Satoshi Kodera, Norihiko Takeda', 'link': 'https://arxiv.org/abs/2504.09302', 'abstract': 'The electrocardiogram (ECG) is a fundamental tool in cardiovascular diagnostics due to its powerful and non-invasive nature. One of the most critical usages is to determine whether more detailed examinations are necessary, with users ranging across various levels of expertise. Given this diversity in expertise, it is essential to assist users to avoid critical errors. Recent studies in machine learning have addressed this challenge by extracting valuable information from ECG data. Utilizing language models, these studies have implemented multimodal models aimed at classifying ECGs according to labeled terms. However, the number of classes was reduced, and it remains uncertain whether the technique is effective for languages other than English. To move towards practical application, we utilized ECG data from regular patients visiting hospitals in Japan, maintaining a large number of Japanese labels obtained from actual ECG readings. Using a contrastive learning framework, we found that even with 98 labels for classification, our Japanese-based language model achieves accuracy comparable to previous research. This study extends the applicability of multimodal machine learning frameworks to broader clinical studies and non-English languages.', 'abstract_zh': '心电图（ECG）是心血管诊断中的一个基本工具，由于其强大且非侵入性的性质。其最关键的应用之一是确定是否需要进行更详细的检查，使用者涵盖了不同程度的专业水平。鉴于这种专业水平的多样性，协助用户避免关键错误是必不可少的。最近的机器学习研究通过从ECG数据中提取有价值的信息来应对这一挑战。利用语言模型，这些研究实现了多模态模型，用于根据标注术语对ECG进行分类。然而，分类的类别数量被减少，并且尚不确定该技术是否对除英语以外的其他语言有效。为了向实际应用迈进，我们利用了日本医院普通患者的心电图数据，保持了大量的实际心电图读数获得的日本标签。通过对比学习框架，我们发现即使有98个分类标签，我们基于日语的语言模型也能达到与之前研究相当的精度。这项研究扩展了多模态机器学习框架在更广泛的临床研究和非英语语言中的适用性。', 'title_zh': '基于对比学习的心电图数据应用：在日本人群中的性能评估与大约100个类别的分类'}
{'arxiv_id': 'arXiv:2504.09197', 'title': 'Graph Learning-Driven Multi-Vessel Association: Fusing Multimodal Data for Maritime Intelligence', 'authors': 'Yuxu Lu, Kaisen Yang, Dong Yang, Haifeng Ding, Jinxian Weng, Ryan Wen Liu', 'link': 'https://arxiv.org/abs/2504.09197', 'abstract': 'Ensuring maritime safety and optimizing traffic management in increasingly crowded and complex waterways require effective waterway monitoring. However, current methods struggle with challenges arising from multimodal data, such as dimensional disparities, mismatched target counts, vessel scale variations, occlusions, and asynchronous data streams from systems like the automatic identification system (AIS) and closed-circuit television (CCTV). Traditional multi-target association methods often struggle with these complexities, particularly in densely trafficked waterways. To overcome these issues, we propose a graph learning-driven multi-vessel association (GMvA) method tailored for maritime multimodal data fusion. By integrating AIS and CCTV data, GMvA leverages time series learning and graph neural networks to capture the spatiotemporal features of vessel trajectories effectively. To enhance feature representation, the proposed method incorporates temporal graph attention and spatiotemporal attention, effectively capturing both local and global vessel interactions. Furthermore, a multi-layer perceptron-based uncertainty fusion module computes robust similarity scores, and the Hungarian algorithm is adopted to ensure globally consistent and accurate target matching. Extensive experiments on real-world maritime datasets confirm that GMvA delivers superior accuracy and robustness in multi-target association, outperforming existing methods even in challenging scenarios with high vessel density and incomplete or unevenly distributed AIS and CCTV data.', 'abstract_zh': '确保 maritime 安全和优化日益拥挤复杂的水道交通管理需要有效的水道监控。为了应对多模态数据带来的挑战，如维度差异、目标计数不匹配、船只规模变化、遮挡以及来自自动识别系统（AIS）和闭路电视（CCTV）等系统的异步数据流，传统多目标关联方法在稠密交通水道中常常难以应对这些复杂性。为此，我们提出了一种基于图学习的多船只关联（GMvA）方法，专门用于海洋多模态数据融合。通过整合AIS和CCTV数据，GMvA利用时序学习和图神经网络来有效捕捉船只轨迹的空间时间特征。为进一步增强特征表示，该方法引入了时间图注意力和时空注意力，有效捕捉局部和全局船只交互。此外，基于多层感知器的不确定性融合模块计算稳健的相似度分数，并采用哈特尔顿算法以确保全局一致且准确的目标匹配。实验证明，GMvA在多目标关联中表现出更优的准确性和鲁棒性，即使在高船只密度和AIS及CCTV数据分布不均的具有挑战性场景中也能超越现有方法。', 'title_zh': '基于图学习的多血管关联：融合多模态数据的 maritime 智能'}
{'arxiv_id': 'arXiv:2504.09058', 'title': 'Towards Stepwise Domain Knowledge-Driven Reasoning Optimization and Reflection Improvement', 'authors': 'Chengyuan Liu, Shihang Wang, Lizhi Qing, Kaisong Song, Junjie Cao, Jun Lin, Ji Zhang, Ang Li, Kun Kuang, Fei Wu', 'link': 'https://arxiv.org/abs/2504.09058', 'abstract': 'Recently, stepwise supervision on Chain of Thoughts (CoTs) presents an enhancement on the logical reasoning tasks such as coding and math, with the help of Monte Carlo Tree Search (MCTS). However, its contribution to tasks requiring domain-specific expertise and knowledge remains unexplored. Motivated by the interest, we identify several potential challenges of vanilla MCTS within this context, and propose the framework of Stepwise Domain Knowledge-Driven Reasoning Optimization, employing the MCTS algorithm to develop step-level supervision for problems that require essential comprehension, reasoning, and specialized knowledge. Additionally, we also introduce the Preference Optimization towards Reflection Paths, which iteratively learns self-reflection on the reasoning thoughts from better perspectives. We have conducted extensive experiments to evaluate the advantage of the methodologies. Empirical results demonstrate the effectiveness on various legal-domain problems. We also report a diverse set of valuable findings, hoping to encourage the enthusiasm to the research of domain-specific LLMs and MCTS.', 'abstract_zh': '基于蒙特卡洛树搜索的逐步领域知识驱动推理优化及其偏好优化研究', 'title_zh': '逐步领域知识驱动的推理优化与反思提升'}
{'arxiv_id': 'arXiv:2504.09046', 'title': 'An Enhanced Iterative Deepening Search Algorithm for the Unrestricted Container Rehandling Problem', 'authors': 'Ruoqi Wang, Jiawei Li', 'link': 'https://arxiv.org/abs/2504.09046', 'abstract': 'In container terminal yards, the Container Rehandling Problem (CRP) involves rearranging containers between stacks under specific operational rules, and it is a pivotal optimization challenge in intelligent container scheduling systems. Existing CRP studies primarily focus on minimizing reallocation costs using two-dimensional bay structures, considering factors such as container size, weight, arrival sequences, and retrieval priorities. This paper introduces an enhanced deepening search algorithm integrated with improved lower bounds to boost search efficiency. To further reduce the search space, we design mutually consistent pruning rules to avoid excessive computational overhead. The proposed algorithm is validated on three widely used benchmark datasets for the Unrestricted Container Rehandling Problem (UCRP). Experimental results demonstrate that our approach outperforms state-of-the-art exact algorithms in solving the more general UCRP variant, particularly exhibiting superior efficiency when handling containers within the same priority group under strict time constraints.', 'abstract_zh': '集装箱 terminal堆场中的集装箱重新处理问题（CRP）涉及在特定操作规则下在堆栈之间重新安排集装箱，这是智能集装箱调度系统中的关键优化挑战。现有CRP研究主要集中在使用二维bay结构最小化重新分配成本，考虑因素包括集装箱尺寸、重量、到达顺序和取回优先级。本文提出了一种增强的逐步搜索算法，并结合改进的下界方法以提升搜索效率。为了进一步减小搜索空间，设计了一套互斥的剪枝规则以避免过多的计算开销。所提出的方法在Unrestricted Container Rehandling Problem (UCRP)的三个广泛使用的基准数据集上进行了验证。实验结果表明，在解决更通用的UCRP变种时，我们的方法超越了现有最先进的精确算法，特别是在严格时间约束下处理相同优先级组内的集装箱时表现出更优的效率。', 'title_zh': '改进的迭代加深搜索算法用于解决无约束集装箱回取问题'}
{'arxiv_id': 'arXiv:2504.08909', 'title': 'Hybrid AI-Physical Modeling for Penetration Bias Correction in X-band InSAR DEMs: A Greenland Case Study', 'authors': 'Islam Mansour, Georg Fischer, Ronny Haensch, Irena Hajnsek', 'link': 'https://arxiv.org/abs/2504.08909', 'abstract': 'Digital elevation models derived from Interferometric Synthetic Aperture Radar (InSAR) data over glacial and snow-covered regions often exhibit systematic elevation errors, commonly termed "penetration bias." We leverage existing physics-based models and propose an integrated correction framework that combines parametric physical modeling with machine learning. We evaluate the approach across three distinct training scenarios - each defined by a different set of acquisition parameters - to assess overall performance and the model\'s ability to generalize. Our experiments on Greenland\'s ice sheet using TanDEM-X data show that the proposed hybrid model corrections significantly reduce the mean and standard deviation of DEM errors compared to a purely physical modeling baseline. The hybrid framework also achieves significantly improved generalization than a pure ML approach when trained on data with limited diversity in acquisition parameters.', 'abstract_zh': '基于干涉雷达合成孔径雷达（InSAR）数据的冰川和雪盖区域数字 elevation 模型衍生出的系统性高程误差（称为“穿透偏差”）的物理模型与机器学习集成校正框架', 'title_zh': '基于X波段InSAR DEMs的穿透偏差校正的混合AI-物理建模：以格陵兰为例的研究'}
{'arxiv_id': 'arXiv:2504.08737', 'title': 'Latency-Aware 2-Opt Monotonic Local Search for Distributed Constraint Optimization', 'authors': 'Ben Rachmut, Roie Zivan, William Yeoh', 'link': 'https://arxiv.org/abs/2504.08737', 'abstract': 'Researchers recently extended Distributed Constraint Optimization Problems (DCOPs) to Communication-Aware DCOPs so that they are applicable in scenarios in which messages can be arbitrarily delayed. Distributed asynchronous local search and inference algorithms designed for CA-DCOPs are less vulnerable to message latency than their counterparts for regular DCOPs. However, unlike local search algorithms for (regular) DCOPs that converge to k-opt solutions (with k > 1), that is, they converge to solutions that cannot be improved by a group of k agents), local search CA-DCOP algorithms are limited to 1-opt solutions only. In this paper, we introduce Latency-Aware Monotonic Distributed Local Search-2 (LAMDLS-2), where agents form pairs and coordinate bilateral assignment replacements. LAMDLS-2 is monotonic, converges to a 2-opt solution, and is also robust to message latency, making it suitable for CA-DCOPs. Our results indicate that LAMDLS-2 converges faster than MGM-2, a benchmark algorithm, to a similar 2-opt solution, in various message latency scenarios.', 'abstract_zh': '研究人员最近将分布式约束优化问题（DCOPs）扩展为通信感知分布式约束优化问题（CA-DCOPs），以便在消息可能任意延迟的情况下适用。设计用于CA-DCOPs的分布式异步局部搜索和推理算法比用于常规DCOPs的相应算法对消息延迟不太敏感。然而，与可以收敛到k-最优解（k>1，即无法通过一组k个代理进行改进的解）的常规DCOPs的局部搜索算法不同，CA-DCOP算法仅能收敛到1-最优解。在这篇论文中，我们介绍了通信延迟感知单调分布式局部搜索-2（LAMDLS-2），其中代理形成对并协调双边指派替换。LAMDLS-2是单调的，可以收敛到2-最优解，并且还对消息延迟具有鲁棒性，使其适合用于CA-DCOPs。我们的结果表明，与基准算法MGM-2相比，LAMDLS-2可以在各种消息延迟场景下更快地收敛到相似的2-最优解。', 'title_zh': '面向延迟的2-Opt单调局部搜索在分布式约束优化中的应用'}
{'arxiv_id': 'arXiv:2504.10368', 'title': 'S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models', 'authors': 'Wenyuan Zhang, Shuaiyi Nie, Xinghua Zhang, Zefeng Zhang, Tingwen Liu', 'link': 'https://arxiv.org/abs/2504.10368', 'abstract': "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning Models' (LRMs) performance on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit chains of thought, their reliance on deep analytical thinking may limit their system 1 thinking capabilities. Moreover, a lack of benchmark currently exists to evaluate LRMs' performance in tasks that require such capabilities. To fill this gap, S1-Bench presents a set of simple, diverse, and naturally clear questions across multiple domains and languages, specifically designed to assess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs reveals significant lower efficiency tendencies, with outputs averaging 15.5 times longer than those of traditional small LLMs. Additionally, LRMs often identify correct answers early but continue unnecessary deliberation, with some models even producing numerous errors. These findings highlight the rigid reasoning patterns of current LRMs and underscore the substantial development needed to achieve balanced dual-system thinking capabilities that can adapt appropriately to task complexity.", 'abstract_zh': 'S1-Bench: 一种用于评估大型推理模型在偏向直觉系统1思考的简单任务上性能的新基准', 'title_zh': 'S1-Bench: 一种评估大型推理模型系统1思维能力的简单基准'}
{'arxiv_id': 'arXiv:2504.10340', 'title': 'Forecasting from Clinical Textual Time Series: Adaptations of the Encoder and Decoder Language Model Families', 'authors': 'Shahriar Noroozizadeh, Sayantan Kumar, Jeremy C. Weiss', 'link': 'https://arxiv.org/abs/2504.10340', 'abstract': 'Clinical case reports encode rich, temporal patient trajectories that are often underexploited by traditional machine learning methods relying on structured data. In this work, we introduce the forecasting problem from textual time series, where timestamped clinical findings--extracted via an LLM-assisted annotation pipeline--serve as the primary input for prediction. We systematically evaluate a diverse suite of models, including fine-tuned decoder-based large language models and encoder-based transformers, on tasks of event occurrence prediction, temporal ordering, and survival analysis. Our experiments reveal that encoder-based models consistently achieve higher F1 scores and superior temporal concordance for short- and long-horizon event forecasting, while fine-tuned masking approaches enhance ranking performance. In contrast, instruction-tuned decoder models demonstrate a relative advantage in survival analysis, especially in early prognosis settings. Our sensitivity analyses further demonstrate the importance of time ordering, which requires clinical time series construction, as compared to text ordering, the format of the text inputs that LLMs are classically trained on. This highlights the additional benefit that can be ascertained from time-ordered corpora, with implications for temporal tasks in the era of widespread LLM use.', 'abstract_zh': '基于文本时间序列的临床案例报告预测问题探究：编码丰富的患者轨迹并系统评估不同模型', 'title_zh': '从临床文本时间序列预测：编码器和解码器语言模型家族的适应性研究'}
{'arxiv_id': 'arXiv:2504.10309', 'title': 'AutoStyle-TTS: Retrieval-Augmented Generation based Automatic Style Matching Text-to-Speech Synthesis', 'authors': 'Dan Luo, Chengyuan Ma, Weiqin Li, Jun Wang, Wei Chen, Zhiyong Wu', 'link': 'https://arxiv.org/abs/2504.10309', 'abstract': 'With the advancement of speech synthesis technology, users have higher expectations for the naturalness and expressiveness of synthesized speech. But previous research ignores the importance of prompt selection. This study proposes a text-to-speech (TTS) framework based on Retrieval-Augmented Generation (RAG) technology, which can dynamically adjust the speech style according to the text content to achieve more natural and vivid communication effects. We have constructed a speech style knowledge database containing high-quality speech samples in various contexts and developed a style matching scheme. This scheme uses embeddings, extracted by Llama, PER-LLM-Embedder,and Moka, to match with samples in the knowledge database, selecting the most appropriate speech style for synthesis. Furthermore, our empirical research validates the effectiveness of the proposed method. Our demo can be viewed at: this https URL', 'abstract_zh': '随着语音合成技术的发展，用户对合成语音的自然度和表现力有了更高的期望。但之前的研究所忽视了提示选择的重要性。本研究提出了一种基于检索增强生成（RAG）技术的文本到语音（TTS）框架，可以根据文本内容动态调整语音风格，以实现更加自然和生动的交流效果。我们构建了一个包含多种情境下的高质量语音样本的语音风格知识数据库，并开发了一种风格匹配方案。该方案使用Llama、PER-LLM-Embedder和Moka提取的嵌入向量，与知识数据库中的样本进行匹配，选取最适合的语音风格进行合成。此外，我们的实证研究验证了所提出方法的有效性。我们的演示可以查看：this https URL。', 'title_zh': 'AutoStyle-TTS：检索增强生成驱动的自动风格匹配文本转语音合成'}
{'arxiv_id': 'arXiv:2504.10188', 'title': 'Efficient Generative Model Training via Embedded Representation Warmup', 'authors': 'Deyuan Liu, Peng Sun, Xufeng Li, Tao Lin', 'link': 'https://arxiv.org/abs/2504.10188', 'abstract': "Diffusion models excel at generating high-dimensional data but fall short in training efficiency and representation quality compared to self-supervised methods. We identify a key bottleneck: the underutilization of high-quality, semantically rich representations during training notably slows down convergence. Our systematic analysis reveals a critical representation processing region -- primarily in the early layers -- where semantic and structural pattern learning takes place before generation can occur. To address this, we propose Embedded Representation Warmup (ERW), a plug-and-play framework where in the first stage we get the ERW module serves as a warmup that initializes the early layers of the diffusion model with high-quality, pretrained representations. This warmup minimizes the burden of learning representations from scratch, thereby accelerating convergence and boosting performance. Our theoretical analysis demonstrates that ERW's efficacy depends on its precise integration into specific neural network layers -- termed the representation processing region -- where the model primarily processes and transforms feature representations for later generation. We further establish that ERW not only accelerates training convergence but also enhances representation quality: empirically, our method achieves a 40$\\times$ acceleration in training speed compared to REPA, the current state-of-the-art methods. Code is available at this https URL.", 'abstract_zh': '差分模型在生成高维数据方面表现出色，但在训练效率和表示质量方面逊色于自我监督方法。我们识别出一个关键瓶颈：训练过程中对高质量语义丰富的表示利用不足显著减慢了收敛速度。我们的系统性分析揭示了一个关键的表示处理区域——主要在早期层中，语义和结构模式学习在此前发生，之后才能进行生成。为了解决这一问题，我们提出了一种即插即用框架嵌入表示预热（ERW），在第一阶段，ERW模块作为预热过程，用预训练的高质量表示初始化差分模型的早期层。这种预热过程减少了从头学习表示的负担，从而加速了收敛并提高了性能。我们的理论分析表明，ERW的有效性取决于其精确集成到特定的神经网络层——称为表示处理区域——其中模型主要处理和转换特征表示以供后续生成。此外，我们进一步证明，ERW不仅加速了训练收敛，还提高了表示质量：实证结果显示，与当前最先进的方法REPA相比，我们的方法实现了训练速度40倍的加速。代码可在以下网址获取。', 'title_zh': '嵌入表示预热实现高效的生成模型训练'}
{'arxiv_id': 'arXiv:2504.10158', 'title': 'COUNTS: Benchmarking Object Detectors and Multimodal Large Language Models under Distribution Shifts', 'authors': 'Jiansheng Li, Xingxuan Zhang, Hao Zou, Yige Guo, Renzhe Xu, Yilong Liu, Chuzhao Zhu, Yue He, Peng Cui', 'link': 'https://arxiv.org/abs/2504.10158', 'abstract': 'Current object detectors often suffer significant perfor-mance degradation in real-world applications when encountering distributional shifts. Consequently, the out-of-distribution (OOD) generalization capability of object detectors has garnered increasing attention from researchers. Despite this growing interest, there remains a lack of a large-scale, comprehensive dataset and evaluation benchmark with fine-grained annotations tailored to assess the OOD generalization on more intricate tasks like object detection and grounding. To address this gap, we introduce COUNTS, a large-scale OOD dataset with object-level annotations. COUNTS encompasses 14 natural distributional shifts, over 222K samples, and more than 1,196K labeled bounding boxes. Leveraging COUNTS, we introduce two novel benchmarks: O(OD)2 and OODG. O(OD)2 is designed to comprehensively evaluate the OOD generalization capabilities of object detectors by utilizing controlled distribution shifts between training and testing data. OODG, on the other hand, aims to assess the OOD generalization of grounding abilities in multimodal large language models (MLLMs). Our findings reveal that, while large models and extensive pre-training data substantially en hance performance in in-distribution (IID) scenarios, significant limitations and opportunities for improvement persist in OOD contexts for both object detectors and MLLMs. In visual grounding tasks, even the advanced GPT-4o and Gemini-1.5 only achieve 56.7% and 28.0% accuracy, respectively. We hope COUNTS facilitates advancements in the development and assessment of robust object detectors and MLLMs capable of maintaining high performance under distributional shifts.', 'abstract_zh': '当前的目标检测器在遇到分布漂移时往往会出现显著的性能下降，因此目标检测器的离分布外（OOD）泛化能力引起了越来越多研究者的关注。尽管如此，仍缺乏一个大规模、综合且包含细粒度注释的离分布外评估基准，以评估物体检测等复杂任务中的离分布外泛化能力。为填补这一空白，我们介绍了COUNTS，这是一个包含物体级别注释的大规模离分布外数据集。COUNTS 包含了14种自然分布漂移，逾222,000个样本和超过1,196,000个标注边界框。利用COUNTS，我们引入了两个新的基准测试：O(OD)2和OODG。O(OD)2旨在通过在训练数据和测试数据之间使用受控的分布漂移来全面评估物体检测器的离分布外泛化能力。OODG则旨在评估多模态大语言模型（MLLMs）的离分布外语义分割能力。我们的研究发现，虽然大型模型和大量的预训练数据在分布式内（IID）场景中显著提高了性能，但在分布式外（OOD）场景中，物体检测器和MLLMs仍存在显著的局限性和改进机会。在视觉语义分割任务中，即使是先进的GPT-4o和Gemini-1.5也只能分别达到56.7%和28.0%的准确率。我们希望COUNTS能促进稳健物体检测器和MLLMs的发展和评估，这些模型能在分布漂移的情况下保持高性能。', 'title_zh': 'COUNTS: 在分布偏移下检测器和多模态大语言模型的基准测试'}
{'arxiv_id': 'arXiv:2504.10149', 'title': 'BoTTA: Benchmarking on-device Test Time Adaptation', 'authors': 'Michal Danilowski, Soumyajit Chatterjee, Abhirup Ghosh', 'link': 'https://arxiv.org/abs/2504.10149', 'abstract': 'The performance of deep learning models depends heavily on test samples at runtime, and shifts from the training data distribution can significantly reduce accuracy. Test-time adaptation (TTA) addresses this by adapting models during inference without requiring labeled test data or access to the original training set. While research has explored TTA from various perspectives like algorithmic complexity, data and class distribution shifts, model architectures, and offline versus continuous learning, constraints specific to mobile and edge devices remain underexplored. We propose BoTTA, a benchmark designed to evaluate TTA methods under practical constraints on mobile and edge devices. Our evaluation targets four key challenges caused by limited resources and usage conditions: (i) limited test samples, (ii) limited exposure to categories, (iii) diverse distribution shifts, and (iv) overlapping shifts within a sample. We assess state-of-the-art TTA methods under these scenarios using benchmark datasets and report system-level metrics on a real testbed. Furthermore, unlike prior work, we align with on-device requirements by advocating periodic adaptation instead of continuous inference-time adaptation. Experiments reveal key insights: many recent TTA algorithms struggle with small datasets, fail to generalize to unseen categories, and depend on the diversity and complexity of distribution shifts. BoTTA also reports device-specific resource use. For example, while SHOT improves accuracy by $2.25\\times$ with $512$ adaptation samples, it uses $1.08\\times$ peak memory on Raspberry Pi versus the base model. BoTTA offers actionable guidance for TTA in real-world, resource-constrained deployments.', 'abstract_zh': '基于移动和边缘设备约束的Test-time适应性基准（BoTTA）', 'title_zh': 'BoTTA: 在设备上测试时间适应性基准测试'}
{'arxiv_id': 'arXiv:2504.10146', 'title': 'GeoUni: A Unified Model for Generating Geometry Diagrams, Problems and Problem Solutions', 'authors': 'Jo-Ku Cheng, Zeren Zhang, Ran Chen, Jingyang Deng, Ziran Qin, Jinwen Ma', 'link': 'https://arxiv.org/abs/2504.10146', 'abstract': 'We propose GeoUni, the first unified geometry expert model capable of generating problem solutions and diagrams within a single framework in a way that enables the creation of unique and individualized geometry problems. Traditionally, solving geometry problems and generating diagrams have been treated as separate tasks in machine learning, with no models successfully integrating both to support problem creation. However, we believe that mastery in geometry requires frictionless integration of all of these skills, from solving problems to visualizing geometric relationships, and finally, crafting tailored problems. Our extensive experiments demonstrate that GeoUni, with only 1.5B parameters, achieves performance comparable to larger models such as DeepSeek-R1 with 671B parameters in geometric reasoning tasks. GeoUni also excels in generating precise geometric diagrams, surpassing both text-to-image models and unified models, including the GPT-4o image generation. Most importantly, GeoUni is the only model capable of successfully generating textual problems with matching diagrams based on specific knowledge points, thus offering a wider range of capabilities that extend beyond current models.', 'abstract_zh': 'GeoUni：首个能够在单一框架内生成问题解决方案和图表的统一几何专家模型', 'title_zh': 'GeoUni：生成几何图、问题及解答的统一模型'}
{'arxiv_id': 'arXiv:2504.10109', 'title': 'Lightweight Trustworthy Distributed Clustering', 'authors': 'Hongyang Li, Caesar Wu, Mohammed Chadli, Said Mammar, Pascal Bouvry', 'link': 'https://arxiv.org/abs/2504.10109', 'abstract': 'Ensuring data trustworthiness within individual edge nodes while facilitating collaborative data processing poses a critical challenge in edge computing systems (ECS), particularly in resource-constrained scenarios such as autonomous systems sensor networks, industrial IoT, and smart cities. This paper presents a lightweight, fully distributed k-means clustering algorithm specifically adapted for edge environments, leveraging a distributed averaging approach with additive secret sharing, a secure multiparty computation technique, during the cluster center update phase to ensure the accuracy and trustworthiness of data across nodes.', 'abstract_zh': '确保在边缘节点上保障数据可信性的同时促进协作数据处理是边缘计算系统（ECS）中的一个关键挑战，特别是在自主系统传感器网络、工业物联网和智慧城市等资源受限的场景中。本文提出了一种专门为边缘环境设计的轻量级全分布式K-means聚类算法，在聚类中心更新阶段利用分布式平均方法与加性秘密共享技术，一种安全多方计算技术，以确保跨节点的数据准确性和可信性。', 'title_zh': '轻量级可信分布式聚类'}
{'arxiv_id': 'arXiv:2504.10028', 'title': 'Sequence models for by-trial decoding of cognitive strategies from neural data', 'authors': 'Rick den Otter, Gabriel Weindel, Sjoerd Stuit, Leendert van Maanen', 'link': 'https://arxiv.org/abs/2504.10028', 'abstract': 'Understanding the sequence of cognitive operations that underlie decision-making is a fundamental challenge in cognitive neuroscience. Traditional approaches often rely on group-level statistics, which obscure trial-by-trial variations in cognitive strategies. In this study, we introduce a novel machine learning method that combines Hidden Multivariate Pattern analysis with a Structured State Space Sequence model to decode cognitive strategies from electroencephalography data at the trial level. We apply this method to a decision-making task, where participants were instructed to prioritize either speed or accuracy in their responses. Our results reveal an additional cognitive operation, labeled Confirmation, which seems to occur predominantly in the accuracy condition but also frequently in the speed condition. The modeled probability that this operation occurs is associated with higher probability of responding correctly as well as changes of mind, as indexed by electromyography data. By successfully modeling cognitive operations at the trial level, we provide empirical evidence for dynamic variability in decision strategies, challenging the assumption of homogeneous cognitive processes within experimental conditions. Our approach shows the potential of sequence modeling in cognitive neuroscience to capture trial-level variability that is obscured by aggregate analyses. The introduced method offers a new way to detect and understand cognitive strategies in a data-driven manner, with implications for both theoretical research and practical applications in many fields.', 'abstract_zh': '理解认知操作序列以阐明决策过程是认知神经科学中的基本挑战。传统方法通常依赖于组级统计，这会掩盖每次试次中认知策略的变化。在本研究中，我们引入了一种新的机器学习方法，结合隐藏多元模式分析与结构状态空间序列模型，从脑电图数据中在试次级别解码认知策略。我们在一个决策任务中应用了这种方法，要求参与者在响应时优先考虑速度或准确性。我们的结果揭示了一种额外的认知操作，命名为确认，其主要发生在准确性条件中，但在速度条件中也很频繁。该操作发生的模型概率与正确反应的概率以及通过肌电图数据指数的心理改变有关。通过在试次级别成功建模认知操作，我们提供了内部分布不均的认知过程的实证证据，挑战了实验条件下认知过程均质性的假设。我们的方法展示了序列建模在认知神经科学中捕捉由聚合分析掩盖的试次级变化的潜力。所介绍的方法为以数据驱动的方式检测和理解认知策略提供了新途径，对理论研究和许多领域的实际应用具有重要意义。', 'title_zh': '基于试次解码的认知策略神经数据序列模型'}
{'arxiv_id': 'arXiv:2504.10014', 'title': 'Air Quality Prediction with A Meteorology-Guided Modality-Decoupled Spatio-Temporal Network', 'authors': 'Hang Yin, Yan-Ming Zhang, Jian Xu, Jian-Long Chang, Yin Li, Cheng-Lin Liu', 'link': 'https://arxiv.org/abs/2504.10014', 'abstract': "Air quality prediction plays a crucial role in public health and environmental protection. Accurate air quality prediction is a complex multivariate spatiotemporal problem, that involves interactions across temporal patterns, pollutant correlations, spatial station dependencies, and particularly meteorological influences that govern pollutant dispersion and chemical transformations. Existing works underestimate the critical role of atmospheric conditions in air quality prediction and neglect comprehensive meteorological data utilization, thereby impairing the modeling of dynamic interdependencies between air quality and meteorological data. To overcome this, we propose MDSTNet, an encoder-decoder framework that explicitly models air quality observations and atmospheric conditions as distinct modalities, integrating multi-pressure-level meteorological data and weather forecasts to capture atmosphere-pollution dependencies for prediction. Meantime, we construct ChinaAirNet, the first nationwide dataset combining air quality records with multi-pressure-level meteorological observations. Experimental results on ChinaAirNet demonstrate MDSTNet's superiority, substantially reducing 48-hour prediction errors by 17.54\\% compared to the state-of-the-art model. The source code and dataset will be available on github.", 'abstract_zh': '空气质量预测在公共卫生和环境保护中起着至关重要的作用。准确的空气质量预测是一个复杂的多变量时空问题，涉及时间模式、污染物质之间的相互作用、站点之间的空间依赖性，以及特别重要的气象条件对污染物扩散和化学转化的调控。现有研究低估了大气条件在空气质量预测中的关键作用，忽视了全面利用气象数据，从而影响了空气质量与气象数据之间动态依赖性的建模。为了解决这一问题，我们提出MDSTNet，这是一种将空气质量观察与大气条件明确建模的编码-解码框架，整合多气压级气象数据和天气预报，以捕捉大气-污染依赖性进行预测。同时，我们构建了中国首个将空气质量记录与多气压级气象观测结合的ChinaAirNet数据集。在ChinaAirNet上的实验结果表明，MDSTNet的优越性，相比最先进的模型显著降低了48小时预测误差17.54%。源代码和数据集将在GitHub上提供。', 'title_zh': '气象引导的模态解耦空时网络空气质量预测'}
{'arxiv_id': 'arXiv:2504.10005', 'title': 'Session-based Recommender Systems: User Interest as a Stochastic Process in the Latent Space', 'authors': 'Klaudia Balcer, Piotr Lipinski', 'link': 'https://arxiv.org/abs/2504.10005', 'abstract': 'This paper jointly addresses the problem of data uncertainty, popularity bias, and exposure bias in session-based recommender systems. We study the symptoms of this bias both in item embeddings and in recommendations. We propose treating user interest as a stochastic process in the latent space and providing a model-agnostic implementation of this mathematical concept. The proposed stochastic component consists of elements: debiasing item embeddings with regularization for embedding uniformity, modeling dense user interest from session prefixes, and introducing fake targets in the data to simulate extended exposure. We conducted computational experiments on two popular benchmark datasets, Diginetica and YooChoose 1/64, as well as several modifications of the YooChoose dataset with different ratios of popular items. The results show that the proposed approach allows us to mitigate the challenges mentioned.', 'abstract_zh': '本文联合解决了会话推荐系统中的数据不确定性、流行度偏差和曝光偏差问题。我们研究了这种偏差在项目嵌入和推荐中的症状。我们建议将用户兴趣视为潜空间中的随机过程，并提供了一个模型无关的该数学概念的实现方法。提出的随机成分包括：通过嵌入一致性正则化去偏差项目嵌入、从会话前缀建模密集用户兴趣以及在数据中引入假目标以模拟延长曝光。我们在两个流行的基准数据集Diginetica和YooChoose 1/64，以及具有不同受欢迎项目比例的YooChoose数据集的多个变体上进行了计算实验。结果显示，所提出的方法能够缓解上述挑战。', 'title_zh': '基于会话的推荐系统：用户兴趣在潜在空间中的随机过程'}
{'arxiv_id': 'arXiv:2504.09963', 'title': 'Towards Unbiased Federated Graph Learning: Label and Topology Perspectives', 'authors': 'Zhengyu Wu, Boyang Pang, Xunkai Li, Yinlin Zhu, Daohan Su, Bowen Fan, Rong-Hua Li, Guoren Wang, Chenghu Zhou', 'link': 'https://arxiv.org/abs/2504.09963', 'abstract': 'Federated Graph Learning (FGL) enables privacy-preserving, distributed training of graph neural networks without sharing raw data. Among its approaches, subgraph-FL has become the dominant paradigm, with most work focused on improving overall node classification accuracy. However, these methods often overlook fairness due to the complexity of node features, labels, and graph structures. In particular, they perform poorly on nodes with disadvantaged properties, such as being in the minority class within subgraphs or having heterophilous connections (neighbors with dissimilar labels or misleading features). This reveals a critical issue: high accuracy can mask degraded performance on structurally or semantically marginalized nodes. To address this, we advocate for two fairness goals: (1) improving representation of minority class nodes for class-wise fairness and (2) mitigating topological bias from heterophilous connections for topology-aware fairness. We propose FairFGL, a novel framework that enhances fairness through fine-grained graph mining and collaborative learning. On the client side, the History-Preserving Module prevents overfitting to dominant local classes, while the Majority Alignment Module refines representations of heterophilous majority-class nodes. The Gradient Modification Module transfers minority-class knowledge from structurally favorable clients to improve fairness. On the server side, FairFGL uploads only the most influenced subset of parameters to reduce communication costs and better reflect local distributions. A cluster-based aggregation strategy reconciles conflicting updates and curbs global majority dominance . Extensive evaluations on eight benchmarks show FairFGL significantly improves minority-group performance , achieving up to a 22.62 percent Macro-F1 gain while enhancing convergence over state-of-the-art baselines.', 'abstract_zh': '联邦图学习（FGL） enables 保护隐私的分布式图神经网络训练而无需共享原始数据。其方法中，子图-FL已成为主导范式，大多数工作专注于提高整体节点分类准确性。然而，这些方法往往由于节点特征、标签和图结构的复杂性而忽视了公平性。特别是，它们在具有不利属性的节点上表现不佳，例如子图中处于少数类别的节点或具有异质连接（邻居具有相似标签或误导性特征的节点）。这揭示了一个关键问题：高准确性可能掩盖了在结构上或语义上边缘化的节点上的性能下降。为了解决这个问题，我们提倡两个公平目标：（1）通过类别公平性改进少数类节点的表示，（2）通过拓扑感知公平性减轻来自异质连接的拓扑偏差。我们提出FairFGL，一种通过细粒度图挖掘和协作学习增强公平性的新颖框架。在客户端，历史保留模块防止对主导局部类别的过度拟合，而多数对齐模块细化少数类节点的表示。梯度修改模块将少数类知识从结构上有利的客户端转移以提高公平性。在服务器端，FairFGL只上传受最大影响的参数子集以降低通信成本并更好地反映局部分布。基于集群的聚合策略解决冲突更新并遏制全局多数群体优势。在八个基准上的广泛评估表明，FairFGL显着改善了少数群体的表现，在增强与最新 baseline 相比的收敛性的同时，宏F1度量提高了22.62个百分点。', 'title_zh': '面向无偏 federated 图学习：从标签和拓扑视角探讨'}
{'arxiv_id': 'arXiv:2504.09906', 'title': 'Plasticity-Aware Mixture of Experts for Learning Under QoE Shifts in Adaptive Video Streaming', 'authors': 'Zhiqiang He, Zhi Liu', 'link': 'https://arxiv.org/abs/2504.09906', 'abstract': 'Adaptive video streaming systems are designed to optimize Quality of Experience (QoE) and, in turn, enhance user satisfaction. However, differences in user profiles and video content lead to different weights for QoE factors, resulting in user-specific QoE functions and, thus, varying optimization objectives. This variability poses significant challenges for neural networks, as they often struggle to generalize under evolving targets - a phenomenon known as plasticity loss that prevents conventional models from adapting effectively to changing optimization objectives. To address this limitation, we propose the Plasticity-Aware Mixture of Experts (PA-MoE), a novel learning framework that dynamically modulates network plasticity by balancing memory retention with selective forgetting. In particular, PA-MoE leverages noise injection to promote the selective forgetting of outdated knowledge, thereby endowing neural networks with enhanced adaptive capabilities. In addition, we present a rigorous theoretical analysis of PA-MoE by deriving a regret bound that quantifies its learning performance. Experimental evaluations demonstrate that PA-MoE achieves a 45.5% improvement in QoE over competitive baselines in dynamic streaming environments. Further analysis reveals that the model effectively mitigates plasticity loss by optimizing neuron utilization. Finally, a parameter sensitivity study is performed by injecting varying levels of noise, and the results align closely with our theoretical predictions.', 'abstract_zh': '自适应视频流传输系统旨在优化用户体验（QoE），进而提升用户满意度。然而，用户特征和视频内容的不同导致QoE因素的权重不同，从而产生用户特异性的QoE函数和不同的优化目标。这种变异性给神经网络带来了显著挑战，因为它们往往难以在目标变化时进行泛化——这种现象被称为塑性损失，阻碍了传统模型的有效适应。为解决这一局限性，我们提出了自适应意识专家混合模型（PA-MoE），这是一种新颖的学习框架，通过平衡记忆保留与选择性遗忘来动态调节网络的塑性。PA-MoE 利用噪声注入促进对过时知识的选择性遗忘，从而增强神经网络的自适应能力。此外，我们通过推导出量化PA-MoE学习性能的遗憾界来对其进行了严格的理论分析。实验评估表明，在动态流传输环境中，PA-MoE 在用户体验（QoE）方面比竞争对手的基线方法提高了45.5%。进一步的分析显示，该模型通过优化神经元利用效率有效地缓解了塑性损失。最后，通过注入不同水平的噪声进行了参数敏感性研究，实验结果与我们的理论预测高度一致。', 'title_zh': '考虑感知质量变化的自适应视频 streaming 中的弹性混合专家学习'}
{'arxiv_id': 'arXiv:2504.09877', 'title': 'Constructing Micro Knowledge Graphs from Technical Support Documents', 'authors': 'Atul Kumar, Nisha Gupta, Saswati Dana', 'link': 'https://arxiv.org/abs/2504.09877', 'abstract': 'Short technical support pages such as IBM Technotes are quite common in technical support domain. These pages can be very useful as the knowledge sources for technical support applications such as chatbots, search engines and question-answering (QA) systems. Information extracted from documents to drive technical support applications is often stored in the form of Knowledge Graph (KG). Building KGs from a large corpus of documents poses a challenge of granularity because a large number of entities and actions are present in each page. The KG becomes virtually unusable if all entities and actions from these pages are stored in the KG. Therefore, only key entities and actions from each page are extracted and stored in the KG. This approach however leads to loss of knowledge represented by entities and actions left out of the KG as they are no longer available to graph search and reasoning functions. We propose a set of techniques to create micro knowledge graph (micrograph) for each of such web pages. The micrograph stores all the entities and actions in a page and also takes advantage of the structure of the page to represent exactly in which part of that page these entities and actions appeared, and also how they relate to each other. These micrographs can be used as additional knowledge sources by technical support applications. We define schemas for representing semi-structured and plain text knowledge present in the technical support web pages. Solutions in technical support domain include procedures made of steps. We also propose a technique to extract procedures from these webpages and the schemas to represent them in the micrographs. We also discuss how technical support applications can take advantage of the micrographs.', 'abstract_zh': 'IBM技术笔记等简短的技术支持页面在技术支撑领域非常常见。这些页面可以作为聊天机器人、搜索引擎和问答系统等技术支持应用的知识来源非常有用。从大量文档中构建知识图谱（KG）面临着粒度问题，因为每个页面中包含大量的实体和动作。如果将这些页面中的所有实体和动作都存储在知识图谱中，知识图谱将变得实际上无法使用。因此，仅从每个页面中提取关键的实体和动作并存储在知识图谱中。然而，这种方法会导致失去未包含在知识图谱中的实体和动作所代表的知识，这些知识对图搜索和推理功能不再可用。我们提出了一套技术来为每个这类网页创建微知识图谱（micrograph）。微知识图谱存储页面中的所有实体和动作，并利用页面结构来精确表示这些实体和动作出现在页面的哪个部分，以及它们之间的关系。这些微知识图谱可以作为技术支持应用的额外知识来源。我们定义了表示技术支持网页中半结构化和纯文本知识的模式。技术支持领域的解决方案包括步骤组成的流程，我们还提出了一种从这些网页中提取流程并用微知识图谱表示它们的技术。我们还讨论了技术支持应用如何利用微知识图谱。', 'title_zh': '从技术支撑文档构建微知识图谱'}
{'arxiv_id': 'arXiv:2504.09876', 'title': 'HDC: Hierarchical Distillation for Multi-level Noisy Consistency in Semi-Supervised Fetal Ultrasound Segmentation', 'authors': 'Tran Quoc Khanh Le, Nguyen Lan Vi Vu, Ha-Hieu Pham, Xuan-Loc Huynh, Tien-Huy Nguyen, Minh Huu Nhat Le, Quan Nguyen, Hien D. Nguyen', 'link': 'https://arxiv.org/abs/2504.09876', 'abstract': 'Transvaginal ultrasound is a critical imaging modality for evaluating cervical anatomy and detecting physiological changes. However, accurate segmentation of cervical structures remains challenging due to low contrast, shadow artifacts, and fuzzy boundaries. While convolutional neural networks (CNNs) have shown promising results in medical image segmentation, their performance is often limited by the need for large-scale annotated datasets - an impractical requirement in clinical ultrasound imaging. Semi-supervised learning (SSL) offers a compelling solution by leveraging unlabeled data, but existing teacher-student frameworks often suffer from confirmation bias and high computational costs. We propose HDC, a novel semi-supervised segmentation framework that integrates Hierarchical Distillation and Consistency learning within a multi-level noise mean-teacher framework. Unlike conventional approaches that rely solely on pseudo-labeling, we introduce a hierarchical distillation mechanism that guides feature-level learning via two novel objectives: (1) Correlation Guidance Loss to align feature representations between the teacher and main student branch, and (2) Mutual Information Loss to stabilize representations between the main and noisy student branches. Our framework reduces model complexity while improving generalization. Extensive experiments on two fetal ultrasound datasets, FUGC and PSFH, demonstrate that our method achieves competitive performance with significantly lower computational overhead than existing multi-teacher models.', 'abstract_zh': '阴道超声是评估宫颈解剖结构和检测生理变化的关键成像模态。然而，由于对比度低、阴影伪影和边界模糊，宫颈结构的准确分割仍然具有挑战性。尽管卷积神经网络（CNN）在医学图像分割方面取得了有前途的结果，但其性能常受限于对大规模标注数据集的需求——这在临床超声成像中是不切实际的要求。半监督学习（SSL）通过利用未标注数据提供了一个有吸引力的解决方案，但现有教师-学生框架往往受到确认偏见和高计算成本的影响。我们提出了一种名为HDC的新颖半监督分割框架，该框架在多层次噪声教师框架内整合了层次蒸馏和一致性学习。与依赖伪标签的传统方法不同，我们引入了一种层次蒸馏机制，通过两个新颖目标来引导特征级学习：（1）相关性指导损失，用于对齐教师和主学生分支的特征表示；（2）互信息损失，用于稳定主学生分支和嘈杂学生分支之间的表示。我们的框架在减少模型复杂性的同时提高了泛化能力。在两个胎超数据集FUGC和PSFH上的广泛实验表明，我们的方法在显著降低计算开销的前提下，实现了与现有多教师模型相当的性能。', 'title_zh': 'HDC：层次蒸馏在半监督胎儿超声分割中的多层级噪声一致性精炼'}
{'arxiv_id': 'arXiv:2504.09873', 'title': 'Truncated Matrix Completion - An Empirical Study', 'authors': 'Rishhabh Naik, Nisarg Trivedi, Davoud Ataee Tarzanagh, Laura Balzano', 'link': 'https://arxiv.org/abs/2504.09873', 'abstract': 'Low-rank Matrix Completion (LRMC) describes the problem where we wish to recover missing entries of partially observed low-rank matrix. Most existing matrix completion work deals with sampling procedures that are independent of the underlying data values. While this assumption allows the derivation of nice theoretical guarantees, it seldom holds in real-world applications. In this paper, we consider various settings where the sampling mask is dependent on the underlying data values, motivated by applications in sensing, sequential decision-making, and recommender systems. Through a series of experiments, we study and compare the performance of various LRMC algorithms that were originally successful for data-independent sampling patterns.', 'abstract_zh': '低秩矩阵完成（LRMC）描述了我们希望恢复部分观测低秩矩阵中缺失条目的问题。大多数现有的矩阵完成工作处理的是与底层数据值无关的采样过程。虽然这一假设允许得出良好的理论保证，但在实际应用中往往不成立。在本文中，我们考虑各种采样掩码依赖于底层数据值的情景，这些情景受到传感、顺序决策和推荐系统应用的启发。通过一系列实验，我们研究并比较了原本对数据无关采样模式成功的各种低秩矩阵完成算法的性能。', 'title_zh': '截断矩阵完成：一项经验研究'}
{'arxiv_id': 'arXiv:2504.09865', 'title': 'Labeling Messages as AI-Generated Does Not Reduce Their Persuasive Effects', 'authors': 'Isabel O. Gallegos, Chen Shani, Weiyan Shi, Federico Bianchi, Izzy Gainsburg, Dan Jurafsky, Robb Willer', 'link': 'https://arxiv.org/abs/2504.09865', 'abstract': "As generative artificial intelligence (AI) enables the creation and dissemination of information at massive scale and speed, it is increasingly important to understand how people perceive AI-generated content. One prominent policy proposal requires explicitly labeling AI-generated content to increase transparency and encourage critical thinking about the information, but prior research has not yet tested the effects of such labels. To address this gap, we conducted a survey experiment (N=1601) on a diverse sample of Americans, presenting participants with an AI-generated message about several public policies (e.g., allowing colleges to pay student-athletes), randomly assigning whether participants were told the message was generated by (a) an expert AI model, (b) a human policy expert, or (c) no label. We found that messages were generally persuasive, influencing participants' views of the policies by 9.74 percentage points on average. However, while 94.6% of participants assigned to the AI and human label conditions believed the authorship labels, labels had no significant effects on participants' attitude change toward the policies, judgments of message accuracy, nor intentions to share the message with others. These patterns were robust across a variety of participant characteristics, including prior knowledge of the policy, prior experience with AI, political party, education level, or age. Taken together, these results imply that, while authorship labels would likely enhance transparency, they are unlikely to substantially affect the persuasiveness of the labeled content, highlighting the need for alternative strategies to address challenges posed by AI-generated information.", 'abstract_zh': '随着生成式人工智能（AI）能够大规模快速地创造和传播信息，理解人们如何感知AI生成内容变得越来越重要。一个突出的政策建议是明确标注AI生成的内容以增强透明度并促进对信息的批判性思考，但此前的研究尚未对此类标签的效果进行测试。为填补这一空白，我们对美国多元样本组（N=1601）进行了问卷实验，向参与者展示关于若干公共政策（例如，允许大学支付运动员薪酬）的AI生成信息，并随机分配参与者是否被告知该信息是由（a）专家AI模型、（b）人类政策专家或（c）无标签生成。我们发现，这些信息通常具有说服力，平均影响参与者对政策的看法9.74个百分点。然而，在AI和人类标签条件下被指派的94.6%的参与者相信了作者身份标签，但标签对参与者对政策的态度变化、对信息准确性的判断以及分享信息意愿等方面均无显著影响。这些模式在多种参与者特征（包括政策知识、AI经验、政治党派、教育水平或年龄）中均表现 robust。综上所述，这些结果表明，虽然作者身份标签可能增强透明度，但它们不太可能显著影响标记内容的说服力，这突出了需要寻找替代策略以应对AI生成信息所带来的挑战。', 'title_zh': '将消息标记为AI生成并不会降低其说服效果。'}
{'arxiv_id': 'arXiv:2504.09860', 'title': 'SUMART: SUMmARizing Translation from Wordy to Concise Expression', 'authors': 'Naoto Nishida, Jun Rekimoto', 'link': 'https://arxiv.org/abs/2504.09860', 'abstract': 'We propose SUMART, a method for summarizing and compressing the volume of verbose subtitle translations. SUMART is designed for understanding translated captions (e.g., interlingual conversations via subtitle translation or when watching movies in foreign language audio and translated captions). SUMART is intended for users who want a big-picture and fast understanding of the conversation, audio, video content, and speech in a foreign language. During the training data collection, when a speaker makes a verbose statement, SUMART employs a large language model on-site to compress the volume of subtitles. This compressed data is then stored in a database for fine-tuning purposes. Later, SUMART uses data pairs from those non-compressed ASR results and compressed translated results for fine-tuning the translation model to generate more concise translations for practical uses. In practical applications, SUMART utilizes this trained model to produce concise translation results. Furthermore, as a practical application, we developed an application that allows conversations using subtitle translation in augmented reality spaces. As a pilot study, we conducted qualitative surveys using a SUMART prototype and a survey on the summarization model for SUMART. We envision the most effective use case of this system is where users need to consume a lot of information quickly (e.g., Speech, lectures, podcasts, Q&A in conferences).', 'abstract_zh': 'SUMART：一种用于总结和压缩冗长字幕翻译的 方法', 'title_zh': 'SUMART: 从冗长表达总结到简洁表达的翻译'}
{'arxiv_id': 'arXiv:2504.09851', 'title': 'Carbon-Efficient 3D DNN Acceleration: Optimizing Performance and Sustainability', 'authors': 'Aikaterini Maria Panteleaki, Konstantinos Balaskas, Georgios Zervakis, Hussam Amrouch, Iraklis Anagnostopoulos', 'link': 'https://arxiv.org/abs/2504.09851', 'abstract': 'As Deep Neural Networks (DNNs) continue to drive advancements in artificial intelligence, the design of hardware accelerators faces growing concerns over embodied carbon footprint due to complex fabrication processes. 3D integration improves performance but introduces sustainability challenges, making carbon-aware optimization essential. In this work, we propose a carbon-efficient design methodology for 3D DNN accelerators, leveraging approximate computing and genetic algorithm-based design space exploration to optimize Carbon Delay Product (CDP). By integrating area-efficient approximate multipliers into Multiply-Accumulate (MAC) units, our approach effectively reduces silicon area and fabrication overhead while maintaining high computational accuracy. Experimental evaluations across three technology nodes (45nm, 14nm, and 7nm) show that our method reduces embodied carbon by up to 30% with negligible accuracy drop.', 'abstract_zh': '随着深度神经网络（DNNs）继续推动人工智能的进步，由于复杂制造工艺导致的人体碳足迹问题使得硬件加速器的设计面临日益增长的担忧。3D集成提高了性能但也引入了可持续性挑战，因此碳意识优化变得至关重要。在本工作中，我们提出了一种用于3D DNN加速器的碳高效设计方法，通过利用近似计算和基于遗传算法的设计空间探索来优化碳延迟积（CDP）。通过将面积高效的近似乘法器整合到乘加（MAC）单元中，我们的方法有效地减少了硅面积和制造开销，同时保持了高计算精度。在三种技术节点（45nm、14nm和7nm）下的实验评估表明，我们的方法在几乎无精度损失的情况下将人体碳足迹最多减少了30%。', 'title_zh': '碳效率的3D DNN加速：优化性能与可持续性'}
{'arxiv_id': 'arXiv:2504.09846', 'title': 'GlyTwin: Digital Twin for Glucose Control in Type 1 Diabetes Through Optimal Behavioral Modifications Using Patient-Centric Counterfactuals', 'authors': 'Asiful Arefeen, Saman Khamesian, Maria Adela Grando, Bithika Thompson, Hassan Ghasemzadeh', 'link': 'https://arxiv.org/abs/2504.09846', 'abstract': 'Frequent and long-term exposure to hyperglycemia (i.e., high blood glucose) increases the risk of chronic complications such as neuropathy, nephropathy, and cardiovascular disease. Current technologies like continuous subcutaneous insulin infusion (CSII) and continuous glucose monitoring (CGM) primarily model specific aspects of glycemic control-like hypoglycemia prediction or insulin delivery. Similarly, most digital twin approaches in diabetes management simulate only physiological processes. These systems lack the ability to offer alternative treatment scenarios that support proactive behavioral interventions. To address this, we propose GlyTwin, a novel digital twin framework that uses counterfactual explanations to simulate optimal treatments for glucose regulation. Our approach helps patients and caregivers modify behaviors like carbohydrate intake and insulin dosing to avoid abnormal glucose events. GlyTwin generates behavioral treatment suggestions that proactively prevent hyperglycemia by recommending small adjustments to daily choices, reducing both frequency and duration of these events. Additionally, it incorporates stakeholder preferences into the intervention design, making recommendations patient-centric and tailored. We evaluate GlyTwin on AZT1D, a newly constructed dataset with longitudinal data from 21 type 1 diabetes (T1D) patients on automated insulin delivery systems over 26 days. Results show GlyTwin outperforms state-of-the-art counterfactual methods, generating 76.6% valid and 86% effective interventions. These findings demonstrate the promise of counterfactual-driven digital twins in delivering personalized healthcare.', 'abstract_zh': '频繁且长期的高血糖暴露增加慢性并发症（如神经病变、肾病和心血管疾病）的风险。现有的技术如持续皮下胰岛素输注（CSII）和连续血糖监测（CGM）主要模拟血糖控制的特定方面，如低血糖预测或胰岛素输送。类似地，大多数糖尿病管理中的数字孪生方法仅模拟生理过程。这些系统缺乏提供替代治疗方案的能力，以支持前瞻性的行为干预。为了弥补这一不足，我们提出了GlyTwin，一种新颖的数字孪生框架，使用反事实解释来模拟葡萄糖调节的最佳治疗方案。我们的方法帮助患者和护理人员调整碳水化合物摄入和胰岛素剂量，以避免异常血糖事件。GlyTwin生成行为治疗建议，通过建议日常选择的小调整，前瞻性地预防高血糖，减少这些事件的频率和持续时间。此外，它将利益相关者的需求纳入干预设计中，使建议具有患者中心性和个性化。我们在AZT1D数据集上评估了GlyTwin，该数据集包含21名使用自动化胰岛素输送系统的1型糖尿病（T1D）患者26天的纵向数据。结果显示，GlyTwin优于最先进的反事实方法，产生了76.6%有效且86%有效的干预措施。这些发现表明，反事实驱动的数字孪生在提供个性化医疗保健方面的潜力。', 'title_zh': 'GlyTwin: 基于患者中心反事实分析的1型糖尿病血糖控制数字孪生通过最优行为修改'}
{'arxiv_id': 'arXiv:2504.09844', 'title': 'OVERLORD: Ultimate Scaling of DataLoader for Multi-Source Large Foundation Model Training', 'authors': 'Juntao Zhao, Qi Lu, Wei Jia, Borui Wan, Lei Zuo, Junda Feng, Jianyu Jiang, Yangrui Chen, Shuaishuai Cao, Jialing He, Kaihua Jiang, Yuanzhe Hu, Yanghua Peng, Haibin Lin, Xin Liu, Chuan Wu', 'link': 'https://arxiv.org/abs/2504.09844', 'abstract': 'Modern frameworks for training large foundation models (LFMs) employ data loaders in a data parallel paradigm. While this design offers implementation simplicity, it introduces two fundamental challenges. First, due to the quadratic computational complexity of the attention operator, the non-uniform sample distribution over data-parallel ranks leads to a significant workload imbalance among loaders, which degrades the training efficiency. This paradigm also impedes the implementation of data mixing algorithms (e.g., curriculum learning) over different datasets. Second, to acquire a broad range of capability, LFMs training ingests data from diverse sources, each with distinct file access states. Colocating massive datasets within loader instances can easily exceed local pod memory capacity. Additionally, heavy sources with higher transformation latency require larger worker pools, further exacerbating memory consumption.\nWe present OVERLORD, an industrial-grade distributed data loading architecture with three innovations: (1) A centralized and declarative data plane, which facilitates elastic data orchestration strategy, such as long-short context, multimodal, and curriculum learning; (2) Disaggregated multisource preprocessing through role-specific actors, i.e., Source Loaders and Data Constructors, leveraging autoscaling for Source Loaders towards heterogeneous and evolving source preprocessing cost; (3) Shadow Loaders with differential checkpointing for uninterrupted fault recovery. Deployed on production clusters scaling to multi-thousand GPU, OVERLORD achieves: (1) 4.5x end-to-end training throughput improvement, (2) a minimum 3.6x reduction in CPU memory usage, with further improvements to be added in later experiments.', 'abstract_zh': '工业级分布式数据加载架构 OVERLORD：面向大规模基础模型训练的创新设计', 'title_zh': 'OVERLORD: 多源大型基础模型训练的 DataLoader 最终扩展方案'}
{'arxiv_id': 'arXiv:2504.09839', 'title': 'SafeSpeech: Robust and Universal Voice Protection Against Malicious Speech Synthesis', 'authors': 'Zhisheng Zhang, Derui Wang, Qianyi Yang, Pengyang Huang, Junhan Pu, Yuxin Cao, Kai Ye, Jie Hao, Yixian Yang', 'link': 'https://arxiv.org/abs/2504.09839', 'abstract': "Speech synthesis technology has brought great convenience, while the widespread usage of realistic deepfake audio has triggered hazards. Malicious adversaries may unauthorizedly collect victims' speeches and clone a similar voice for illegal exploitation (\\textit{e.g.}, telecom fraud). However, the existing defense methods cannot effectively prevent deepfake exploitation and are vulnerable to robust training techniques. Therefore, a more effective and robust data protection method is urgently needed. In response, we propose a defensive framework, \\textit{\\textbf{SafeSpeech}}, which protects the users' audio before uploading by embedding imperceptible perturbations on original speeches to prevent high-quality synthetic speech. In SafeSpeech, we devise a robust and universal proactive protection technique, \\textbf{S}peech \\textbf{PE}rturbative \\textbf{C}oncealment (\\textbf{SPEC}), that leverages a surrogate model to generate universally applicable perturbation for generative synthetic models. Moreover, we optimize the human perception of embedded perturbation in terms of time and frequency domains. To evaluate our method comprehensively, we conduct extensive experiments across advanced models and datasets, both subjectively and objectively. Our experimental results demonstrate that SafeSpeech achieves state-of-the-art (SOTA) voice protection effectiveness and transferability and is highly robust against advanced adaptive adversaries. Moreover, SafeSpeech has real-time capability in real-world tests. The source code is available at \\href{this https URL}{this https URL}.", 'abstract_zh': '语音合成技术带来了极大的便利，而广泛使用的逼真深伪音频也引发了安全隐患。恶意攻击者可能未经授权收集受害者讲话，并克隆相似的声音进行非法利用（例如电信诈骗）。然而，现有的防御方法无法有效防止深伪利用，并且容易受到鲁棒训练技术的攻击。因此，一种更有效且鲁棒的数据保护方法迫在眉睫。为此，我们提出了一种防御框架——SafeSpeech，该框架通过在上传前在原始讲话中嵌入不可感知的扰动来保护用户的音频，以防止高质量合成语音。在SafeSpeech中，我们设计了一种鲁棒且通用的主动保护技术——Speech Perturbative Concealment (SPEC)，该技术利用代理模型为生成型合成模型生成通用适用的扰动。此外，我们从时间和频率域优化嵌入扰动的人类感知。为了全面评估我们的方法，我们在高级模型和数据集上进行了广泛的实验，从主观和客观两个方面进行评估。实验结果表明，SafeSpeech实现了最先进的语音保护效果和可移植性，并且对先进的自适应对手具有高度鲁棒性。此外，SafeSpeech在实际测试中具有实时能力。源代码可在https://this-link-url.com/获得。', 'title_zh': 'SafeSpeech:稳健且通用的恶意语音合成防护'}
{'arxiv_id': 'arXiv:2504.09831', 'title': 'Offline Dynamic Inventory and Pricing Strategy: Addressing Censored and Dependent Demand', 'authors': 'Korel Gundem, Zhengling Qi', 'link': 'https://arxiv.org/abs/2504.09831', 'abstract': 'In this paper, we study the offline sequential feature-based pricing and inventory control problem where the current demand depends on the past demand levels and any demand exceeding the available inventory is lost. Our goal is to leverage the offline dataset, consisting of past prices, ordering quantities, inventory levels, covariates, and censored sales levels, to estimate the optimal pricing and inventory control policy that maximizes long-term profit. While the underlying dynamic without censoring can be modeled by Markov decision process (MDP), the primary obstacle arises from the observed process where demand censoring is present, resulting in missing profit information, the failure of the Markov property, and a non-stationary optimal policy. To overcome these challenges, we first approximate the optimal policy by solving a high-order MDP characterized by the number of consecutive censoring instances, which ultimately boils down to solving a specialized Bellman equation tailored for this problem. Inspired by offline reinforcement learning and survival analysis, we propose two novel data-driven algorithms to solving these Bellman equations and, thus, estimate the optimal policy. Furthermore, we establish finite sample regret bounds to validate the effectiveness of these algorithms. Finally, we conduct numerical experiments to demonstrate the efficacy of our algorithms in estimating the optimal policy. To the best of our knowledge, this is the first data-driven approach to learning optimal pricing and inventory control policies in a sequential decision-making environment characterized by censored and dependent demand. The implementations of the proposed algorithms are available at this https URL', 'abstract_zh': '基于历史数据的序贯特征定价与库存控制问题研究：考虑 censored 和依赖需求的最优策略学习', 'title_zh': '离线动态库存与定价策略：应对受限且相关的需求'}
{'arxiv_id': 'arXiv:2504.09812', 'title': 'Efficient Multi-Task Modeling through Automated Fusion of Trained Models', 'authors': 'Jingxuan Zhou, Weidong Bao, Ji Wang, Zhengyi Zhong, Dayu Zhang', 'link': 'https://arxiv.org/abs/2504.09812', 'abstract': 'Although multi-task learning is widely applied in intelligent services, traditional multi-task modeling methods often require customized designs based on specific task combinations, resulting in a cumbersome modeling process. Inspired by the rapid development and excellent performance of single-task models, this paper proposes an efficient multi-task modeling method that can automatically fuse trained single-task models with different structures and tasks to form a multi-task model. As a general framework, this method allows modelers to simply prepare trained models for the required tasks, simplifying the modeling process while fully utilizing the knowledge contained in the trained models. This eliminates the need for excessive focus on task relationships and model structure design. To achieve this goal, we consider the structural differences among various trained models and employ model decomposition techniques to hierarchically decompose them into multiple operable model components. Furthermore, we have designed an Adaptive Knowledge Fusion (AKF) module based on Transformer, which adaptively integrates intra-task and inter-task knowledge based on model components. Through the proposed method, we achieve efficient and automated construction of multi-task models, and its effectiveness is verified through extensive experiments on three datasets.', 'abstract_zh': '尽管多任务学习在智能服务中广泛应用，传统多任务建模方法往往需要根据特定的任务组合进行定制化设计，导致建模过程繁琐。受单任务模型快速发展和优异性能的启发，本文提出了一种高效的多任务建模方法，可以自动融合结构和任务不同的训练好的单任务模型，形成多任务模型。作为一种通用框架，该方法允许建模者仅需准备所需的训练模型，简化建模过程，同时充分利用训练模型中包含的知识。这种方法消除了过度关注任务关系和模型结构设计的需要。为了实现这一目标，我们考虑了各种训练模型之间的结构差异，并采用模型分解技术，逐级分解为多个可操作的模型组件。此外，我们基于Transformer设计了一个自适应知识融合（AKF）模块，根据模型组件自适应地整合任务内和任务间知识。通过所提出的方法，我们实现了多任务模型的有效和自动构建，并通过三个数据集上的 extensive 实验验证了其有效性。', 'title_zh': '通过训练模型的自动融合实现高效的多任务建模'}
{'arxiv_id': 'arXiv:2504.09809', 'title': 'See or Recall: A Sanity Check for the Role of Vision in Solving Visualization Question Answer Tasks with Multimodal LLMs', 'authors': 'Zhimin Li, Haichao Miao, Xinyuan Yan, Valerio Pascucci, Matthew Berger, Shusen Liu', 'link': 'https://arxiv.org/abs/2504.09809', 'abstract': 'Recent developments in multimodal large language models (MLLM) have equipped language models to reason about vision and language jointly. This permits MLLMs to both perceive and answer questions about data visualization across a variety of designs and tasks. Applying MLLMs to a broad range of visualization tasks requires us to properly evaluate their capabilities, and the most common way to conduct evaluation is through measuring a model\'s visualization reasoning capability, analogous to how we would evaluate human understanding of visualizations (e.g., visualization literacy). However, we found that in the context of visualization question answering (VisQA), how an MLLM perceives and reasons about visualizations can be fundamentally different from how humans approach the same problem. During the evaluation, even without visualization, the model could correctly answer a substantial portion of the visualization test questions, regardless of whether any selection options were provided. We hypothesize that the vast amount of knowledge encoded in the language model permits factual recall that supersedes the need to seek information from the visual signal. It raises concerns that the current VisQA evaluation may not fully capture the models\' visualization reasoning capabilities. To address this, we propose a comprehensive sanity check framework that integrates a rule-based decision tree and a sanity check table to disentangle the effects of "seeing" (visual processing) and "recall" (reliance on prior knowledge). This validates VisQA datasets for evaluation, highlighting where models are truly "seeing", positively or negatively affected by the factual recall, or relying on inductive biases for question answering. Our study underscores the need for careful consideration in designing future visualization understanding studies when utilizing MLLMs.', 'abstract_zh': '近期多模态大型语言模型的发展使语言模型能够联合推理视觉和语言。这使得多模态大型语言模型能够感知并回答各种设计和任务的数据可视化问题。将多模态大型语言模型应用于广泛的可视化任务需要我们合理评估其能力，最常见的方式是通过测量模型的可视化推理能力来评估，类似于评估人类对可视化图表的理解能力（如可视化素养）。然而，我们发现，在可视化问题回答（VisQA）的背景下，多模态大型语言模型对视觉信息的感知和推理方式与人类处理相同问题的方式可能存在根本不同。在评估过程中，即使不提供任何视觉信息，模型也能正确回答大量可视化测试问题，无论是否有选择选项。我们推测，语言模型中编码的大量知识使得其能够进行事实回忆，从而超越了从视觉信号中获取信息的需要。这引发了当前VisQA评估可能未能全面捕获模型的可视化推理能力的担忧。为了应对这一问题，我们提出了一种综合的常识检查框架，该框架结合了基于规则的决策树和常识检查表，以分离“看到”（视觉处理）和“回忆”（依赖先验知识）的影响。这一框架验证了用于评估的VisQA数据集，强调了模型在真正“看到”、受事实回忆正向或负向影响，或依赖归纳偏置进行问题回答的方面。我们的研究强调了在利用多模态大型语言模型进行未来可视化理解研究时需要仔细考虑的重要性。', 'title_zh': '看看或回忆：对多模态LLMs在解答可视化问答任务中视觉作用的一种合理性检查'}
{'arxiv_id': 'arXiv:2504.09800', 'title': 'Multi-task Federated Learning with Encoder-Decoder Structure: Enabling Collaborative Learning Across Different Tasks', 'authors': 'Jingxuan Zhou, Weidong Bao, Ji Wang, Dayu Zhang, Xiongtao Zhang, Yaohong Zhang', 'link': 'https://arxiv.org/abs/2504.09800', 'abstract': 'Federated learning has been extensively studied and applied due to its ability to ensure data security in distributed environments while building better models. However, clients participating in federated learning still face limitations, as clients with different structures or tasks cannot participate in learning together. In view of this, constructing a federated learning framework that allows collaboration between clients with different model structures and performing different tasks, enabling them to share valuable knowledge to enhance model efficiency, holds significant practical implications for the widespread application of federated learning. To achieve this goal, we propose a multi-task federated learning with encoder-decoder structure (M-Fed). Specifically, given the widespread adoption of the encoder-decoder architecture in current models, we leverage this structure to share intra-task knowledge through traditional federated learning methods and extract general knowledge from the encoder to achieve cross-task knowledge sharing. The training process is similar to traditional federated learning, and we incorporate local decoder and global decoder information into the loss function. The local decoder iteratively updates and gradually approaches the global decoder until sufficient cross-task knowledge sharing is achieved. Our method is lightweight and modular, demonstrating innovation compared to previous research. It enables clients performing different tasks to share general knowledge while maintaining the efficiency of traditional federated learning systems. We conducted experiments on two widely used benchmark datasets to verify the feasibility of M-Fed and compared it with traditional methods. The experimental results demonstrate the effectiveness of M-Fed in multi-task federated learning.', 'abstract_zh': '联邦学习因其在分布式环境中保障数据安全的同时构建更好模型的能力而得到了广泛研究和应用。然而，参与联邦学习的客户端仍然面临局限性，即结构或任务不同的客户端无法共同参与学习。鉴于此，构建一个允许结构和任务不同的客户端进行合作的联邦学习框架，使它们能够分享有价值的知识以提高模型效率，对于联邦学习的广泛应用具有重要的实际意义。为实现这一目标，我们提出了一种基于编码器-解码器结构的多任务联邦学习方法（M-Fed）。具体而言，鉴于当前模型广泛采用编码器-解码器架构，我们利用这一结构通过传统联邦学习方法共享同一任务的知识，并借助编码器提取通用知识以实现跨任务知识共享。训练过程与传统联邦学习类似，我们将本地解码器和全局解码器信息纳入损失函数中。本地解码器迭代更新并逐渐接近全局解码器，直到实现足够的跨任务知识共享。我们的方法轻量且模块化，相较于先前研究具有创新性。它使执行不同任务的客户端能够共享通用知识，同时保持传统联邦学习系统的效率。我们在两个广泛使用的基准数据集上进行了实验以验证M-Fed的有效性，并将其与传统方法进行了比较。实验结果证明了M-Fed在多任务联邦学习中的有效性。', 'title_zh': '基于编码器-解码器结构的多任务联邦学习：实现不同任务间的协作学习'}
{'arxiv_id': 'arXiv:2504.09795', 'title': 'VDocRAG: Retrieval-Augmented Generation over Visually-Rich Documents', 'authors': 'Ryota Tanaka, Taichi Iki, Taku Hasegawa, Kyosuke Nishida, Kuniko Saito, Jun Suzuki', 'link': 'https://arxiv.org/abs/2504.09795', 'abstract': 'We aim to develop a retrieval-augmented generation (RAG) framework that answers questions over a corpus of visually-rich documents presented in mixed modalities (e.g., charts, tables) and diverse formats (e.g., PDF, PPTX). In this paper, we introduce a new RAG framework, VDocRAG, which can directly understand varied documents and modalities in a unified image format to prevent missing information that occurs by parsing documents to obtain text. To improve the performance, we propose novel self-supervised pre-training tasks that adapt large vision-language models for retrieval by compressing visual information into dense token representations while aligning them with textual content in documents. Furthermore, we introduce OpenDocVQA, the first unified collection of open-domain document visual question answering datasets, encompassing diverse document types and formats. OpenDocVQA provides a comprehensive resource for training and evaluating retrieval and question answering models on visually-rich documents in an open-domain setting. Experiments show that VDocRAG substantially outperforms conventional text-based RAG and has strong generalization capability, highlighting the potential of an effective RAG paradigm for real-world documents.', 'abstract_zh': '我们旨在开发一种检索增强生成（RAG）框架，用于回答以混合模态（如图表、表格）和多种格式（如PDF、PPTX）呈现的丰富视觉文档中的问题。在本文中，我们介绍了一种新的RAG框架VDocRAG，该框架可以直接理解和统一处理各类文档和模态的信息，避免了通过解析文档获取文本时可能出现的信息遗漏。为了提升性能，我们提出了新的自监督预训练任务，将大型视觉语言模型适应于检索任务，通过将视觉信息压缩为密集的token表示并与文档中的文本内容对齐来改进检索效果。此外，我们引入了OpenDocVQA，这是第一个统一的开放领域文档视觉问答数据集，涵盖了多种文档类型和格式。OpenDocVQA 为在开放领域环境下的视觉丰富文档进行检索和问答模型的训练和评估提供了全面的资源。实验结果显示VDocRAG显著优于传统的基于文本的RAG，并具有较强的泛化能力，突显了有效RAG范式在现实世界文档中的潜力。', 'title_zh': 'VDocRAG：丰富视觉元素的检索增强生成'}
{'arxiv_id': 'arXiv:2504.09779', 'title': '"All Roads Lead to ChatGPT": How Generative AI is Eroding Social Interactions and Student Learning Communities', 'authors': 'Irene Hou, Owen Man, Kate Hamilton, Srishty Muthusekaran, Jeffin Johnykutty, Leili Zadeh, Stephen MacNeil', 'link': 'https://arxiv.org/abs/2504.09779', 'abstract': "The widespread adoption of generative AI is already impacting learning and help-seeking. While the benefits of generative AI are well-understood, recent studies have also raised concerns about increased potential for cheating and negative impacts on students' metacognition and critical thinking. However, the potential impacts on social interactions, peer learning, and classroom dynamics are not yet well understood. To investigate these aspects, we conducted 17 semi-structured interviews with undergraduate computing students across seven R1 universities in North America. Our findings suggest that help-seeking requests are now often mediated by generative AI. For example, students often redirected questions from their peers to generative AI instead of providing assistance themselves, undermining peer interaction. Students also reported feeling increasingly isolated and demotivated as the social support systems they rely on begin to break down. These findings are concerning given the important role that social interactions play in students' learning and sense of belonging.", 'abstract_zh': '生成式人工智能的广泛应用已影响学习和求助行为。尽管生成式人工智能带来的益处已被充分了解，但最近的研究也提出了关于其增加作弊可能性以及对学生成本元认知和批判性思维负面影响的担忧。然而，其对社会互动、同伴学习和课堂动态的影响尚未得到充分理解。为了探究这些方面，我们对北美洲七所R1大学的本科生进行了17次半结构化访谈。研究发现，求助请求现在经常通过生成式人工智能进行中介。例如，学生常将问题从同伴转向生成式人工智能，而不是相互提供帮助，从而削弱了同伴互动。此外，学生还报告称，随着他们依赖的社会支持系统开始瓦解，他们感到越来越孤立和缺乏动力。这些发现令人担忧，因为社会互动在学生的学习和归属感中扮演着重要角色。', 'title_zh': '“条条大路通ChatGPT”：生成式人工智能如何侵蚀社会互动和学生学习社区'}
{'arxiv_id': 'arXiv:2504.09734', 'title': 'Dynamik: Syntactically-Driven Dynamic Font Sizing for Emphasis of Key Information', 'authors': 'Naoto Nishida, Yoshio Ishiguro, Jun Rekiomto, Naomi Yamashita', 'link': 'https://arxiv.org/abs/2504.09734', 'abstract': "In today's globalized world, there are increasing opportunities for individuals to communicate using a common non-native language (lingua franca). Non-native speakers often have opportunities to listen to foreign languages, but may not comprehend them as fully as native speakers do. To aid real-time comprehension, live transcription of subtitles is frequently used in everyday life (e.g., during Zoom conversations, watching YouTube videos, or on social networking sites). However, simultaneously reading subtitles while listening can increase cognitive load. In this study, we propose Dynamik, a system that reduces cognitive load during reading by decreasing the size of less important words and enlarging important ones, thereby enhancing sentence contrast. Our results indicate that Dynamik can reduce certain aspects of cognitive load, specifically, participants' perceived performance and effort among individuals with low proficiency in English, as well as enhance the users' sense of comprehension, especially among people with low English ability. We further discuss our methods' applicability to other languages and potential improvements and further research directions.", 'abstract_zh': '全球化背景下，个体使用共同的非母语语言（通用语）进行沟通的机会日益增多。非母语使用者常常有机会聆听外语，但可能不像母语使用者那样完全理解。为了实时理解，日常生活中经常使用实时字幕转录（例如在Zoom对话、观看YouTube视频或在社交媒体上）。然而，同时阅读字幕和聆听可能会增加认知负荷。本研究提出了一种名为Dynamik的系统，通过减少不重要词汇的大小并放大重要词汇，从而降低阅读时的认知负荷，增强句子对比度。研究表明，Dynamik可以降低低英语 proficiency 用户的认知负荷感和努力感，并增强用户的理解感，特别是在低英语能力用户中更为明显。我们进一步讨论了该方法在其他语言中的适用性和潜在改进以及进一步的研究方向。', 'title_zh': '动态字体大小调整：基于句法的关键信息强调'}
{'arxiv_id': 'arXiv:2504.09716', 'title': 'Dominated Actions in Imperfect-Information Games', 'authors': 'Sam Ganzfried', 'link': 'https://arxiv.org/abs/2504.09716', 'abstract': 'Dominance is a fundamental concept in game theory. In strategic-form games dominated strategies can be identified in polynomial time. As a consequence, iterative removal of dominated strategies can be performed efficiently as a preprocessing step for reducing the size of a game before computing a Nash equilibrium. For imperfect-information games in extensive form, we could convert the game to strategic form and then iteratively remove dominated strategies in the same way; however, this conversion may cause an exponential blowup in game size. In this paper we define and study the concept of dominated actions in imperfect-information games. Our main result is a polynomial-time algorithm for determining whether an action is dominated (strictly or weakly) by any mixed strategy in n-player games, which can be extended to an algorithm for iteratively removing dominated actions. This allows us to efficiently reduce the size of the game tree as a preprocessing step for Nash equilibrium computation. We explore the role of dominated actions empirically in the "All In or Fold" No-Limit Texas Hold\'em poker variant.', 'abstract_zh': 'dominance在博弈论中是一个基本概念。在战略型博弈中，占优策略可以在多项式时间内被识别。因此，可以通过迭代移除占优策略作为预处理步骤，减少博弈规模后再计算纳什均衡，这可以高效地进行。对于不完善信息的扩展形式博弈，可以将博弈转换为战略型形式并同样迭代移除占优策略；然而，这种转换可能导致博弈规模的指数级增长。本文定义并研究了不完善信息博弈中的占优行动概念。我们的主要成果是提出了一个多项式时间算法来确定一个行动是否被任何混合策略严格地或弱地占优（适用于n-player博弈），该算法可以扩展为迭代移除占优行动的算法。这使我们能够在计算纳什均衡之前作为预处理步骤，有效减少博弈树的规模。我们通过实验探索了占优行动在“全押或弃牌”无限德州扑克变种中的作用。', 'title_zh': 'imperfect-information 游戏中的支配行动'}
{'arxiv_id': 'arXiv:2504.09714', 'title': 'Evaluating the Quality of Benchmark Datasets for Low-Resource Languages: A Case Study on Turkish', 'authors': 'Ayşe Aysu Cengiz, Ahmet Kaan Sever, Elif Ecem Ümütlü, Naime Şeyma Erdem, Burak Aytan, Büşra Tufan, Abdullah Topraksoy, Esra Darıcı, Cagri Toraman', 'link': 'https://arxiv.org/abs/2504.09714', 'abstract': 'The reliance on translated or adapted datasets from English or multilingual resources introduces challenges regarding linguistic and cultural suitability. This study addresses the need for robust and culturally appropriate benchmarks by evaluating the quality of 17 commonly used Turkish benchmark datasets. Using a comprehensive framework that assesses six criteria, both human and LLM-judge annotators provide detailed evaluations to identify dataset strengths and shortcomings.\nOur results reveal that 70% of the benchmark datasets fail to meet our heuristic quality standards. The correctness of the usage of technical terms is the strongest criterion, but 85% of the criteria are not satisfied in the examined datasets. Although LLM judges demonstrate potential, they are less effective than human annotators, particularly in understanding cultural common sense knowledge and interpreting fluent, unambiguous text. GPT-4o has stronger labeling capabilities for grammatical and technical tasks, while Llama3.3-70B excels at correctness and cultural knowledge evaluation. Our findings emphasize the urgent need for more rigorous quality control in creating and adapting datasets for low-resource languages.', 'abstract_zh': '依赖于从英语或多种语言资源翻译或改编的语料库引入了语言和文化适宜性方面的问题。本研究通过评估17个常用土耳其语基准数据集的质量，来解决 robust 和文化适应性基准的需求。使用综合框架评估六项指标，人和LLM评判员提供详细的评估来识别数据集的优点和不足。\n\n我们的结果显示，70%的基准数据集未能达到我们的启发式质量标准。技术术语使用正确性是最重要的指标，但在检查的数据集中，85%的指标未被满足。尽管LLM评判员显示出潜力，但在理解和解释文化常识知识以及解释流畅、无歧义的文本方面，它们的效果不如人类标注者。GPT-4o在语法和技术任务的标签能力方面更强，而Llama3.3-70B在正确性和文化知识评估方面表现出色。我们的研究结果强调了在为低资源语言创建和改编数据集时进行更严格质量控制的迫切需求。', 'title_zh': '低资源语言基准数据集质量评估：以土耳其语为例'}
{'arxiv_id': 'arXiv:2504.09704', 'title': 'Transformer-Based Representation Learning for Robust Gene Expression Modeling and Cancer Prognosis', 'authors': 'Shuai Jiang, Saeed Hassanpour', 'link': 'https://arxiv.org/abs/2504.09704', 'abstract': 'Transformer-based models have achieved remarkable success in natural language and vision tasks, but their application to gene expression analysis remains limited due to data sparsity, high dimensionality, and missing values. We present GexBERT, a transformer-based autoencoder framework for robust representation learning of gene expression data. GexBERT learns context-aware gene embeddings by pretraining on large-scale transcriptomic profiles with a masking and restoration objective that captures co-expression relationships among thousands of genes. We evaluate GexBERT across three critical tasks in cancer research: pan-cancer classification, cancer-specific survival prediction, and missing value imputation. GexBERT achieves state-of-the-art classification accuracy from limited gene subsets, improves survival prediction by restoring expression of prognostic anchor genes, and outperforms conventional imputation methods under high missingness. Furthermore, its attention-based interpretability reveals biologically meaningful gene patterns across cancer types. These findings demonstrate the utility of GexBERT as a scalable and effective tool for gene expression modeling, with translational potential in settings where gene coverage is limited or incomplete.', 'abstract_zh': '基于Transformer的框架GexBERT在基因表达数据分析中的稳健表示学习', 'title_zh': '基于Transformer的表示学习方法用于健壮的基因表达建模与癌症预后'}
{'arxiv_id': 'arXiv:2504.09680', 'title': 'SPOT: Spatio-Temporal Pattern Mining and Optimization for Load Consolidation in Freight Transportation Networks', 'authors': 'Sikai Cheng, Amira Hijazi, Jeren Konak, Alan Erera, Pascal Van Hentenryck', 'link': 'https://arxiv.org/abs/2504.09680', 'abstract': 'Freight consolidation has significant potential to reduce transportation costs and mitigate congestion and pollution. An effective load consolidation plan relies on carefully chosen consolidation points to ensure alignment with existing transportation management processes, such as driver scheduling, personnel planning, and terminal operations. This complexity represents a significant challenge when searching for optimal consolidation strategies. Traditional optimization-based methods provide exact solutions, but their computational complexity makes them impractical for large-scale instances and they fail to leverage historical data. Machine learning-based approaches address these issues but often ignore operational constraints, leading to infeasible consolidation plans.\nThis work proposes SPOT, an end-to-end approach that integrates the benefits of machine learning (ML) and optimization for load consolidation. The ML component plays a key role in the planning phase by identifying the consolidation points through spatio-temporal clustering and constrained frequent itemset mining, while the optimization selects the most cost-effective feasible consolidation routes for a given operational day. Extensive experiments conducted on industrial load data demonstrate that SPOT significantly reduces travel distance and transportation costs (by about 50% on large terminals) compared to the existing industry-standard load planning strategy and a neighborhood-based heuristic. Moreover, the ML component provides valuable tactical-level insights by identifying frequently recurring consolidation opportunities that guide proactive planning. In addition, SPOT is computationally efficient and can be easily scaled to accommodate large transportation networks.', 'abstract_zh': '货物 consolidation 在降低运输成本和缓解拥堵与污染方面具有显著潜力。有效的负载 consolidation 计划依赖于精心选择的 consolidation 点，以确保与现有的运输管理流程，如驾驶员调度、人员规划和码头运营等相一致。这一复杂性构成了寻找最优 consolidation 策略的显著挑战。传统基于优化的方法能提供精确的解决方案，但其计算复杂性使其不适合大规模实例，并且无法利用历史数据。基于机器学习的方法解决了这些难题，但通常忽视了操作约束，导致不可行的 consolidation 计划。\n\n本文提出了一种端到端的框架 SPOT，该框架将机器学习（ML）和优化的优势结合起来用于负载 consolidation。机器学习组件在规划阶段发挥关键作用，通过时空聚类和受限频繁项挖掘来识别 consolidation 点，而优化则为给定的操作日选择最具成本效益的可行 consolidation 路线。在工业负载数据上的 extensive 实验表明，与现有行业标准的负载规划策略和基于邻域的启发式方法相比，SPOT 可显著减少行驶距离和运输成本（大型码头情况下约减少 50%）。此外，机器学习组件提供了有价值的战术级见解，通过识别频繁出现的 consolidation 机会来指导主动规划。此外，SPOT 具有计算效率，并且可以轻松扩展以适应大规模的运输网络。', 'title_zh': 'SPOT：货运运输网络中基于时空模式挖掘与优化的负载整合方法'}
{'arxiv_id': 'arXiv:2504.09627', 'title': 'Slow Thinking for Sequential Recommendation', 'authors': 'Junjie Zhang, Beichen Zhang, Wenqi Sun, Hongyu Lu, Wayne Xin Zhao, Yu Chen, Ji-Rong Wen', 'link': 'https://arxiv.org/abs/2504.09627', 'abstract': 'To develop effective sequential recommender systems, numerous methods have been proposed to model historical user behaviors. Despite the effectiveness, these methods share the same fast thinking paradigm. That is, for making recommendations, these methods typically encodes user historical interactions to obtain user representations and directly match these representations with candidate item representations. However, due to the limited capacity of traditional lightweight recommendation models, this one-step inference paradigm often leads to suboptimal performance. To tackle this issue, we present a novel slow thinking recommendation model, named STREAM-Rec. Our approach is capable of analyzing historical user behavior, generating a multi-step, deliberative reasoning process, and ultimately delivering personalized recommendations. In particular, we focus on two key challenges: (1) identifying the suitable reasoning patterns in recommender systems, and (2) exploring how to effectively stimulate the reasoning capabilities of traditional recommenders. To this end, we introduce a three-stage training framework. In the first stage, the model is pretrained on large-scale user behavior data to learn behavior patterns and capture long-range dependencies. In the second stage, we design an iterative inference algorithm to annotate suitable reasoning traces by progressively refining the model predictions. This annotated data is then used to fine-tune the model. Finally, in the third stage, we apply reinforcement learning to further enhance the model generalization ability. Extensive experiments validate the effectiveness of our proposed method.', 'abstract_zh': '开发有效的序列推荐系统的方法已提出了许多种，用于建模用户的历史行为。尽管这些方法有效，但它们大多遵循快速思维范式。具体而言，这些方法通常通过编码用户的 histórico 行为互动来获取用户表示，并直接将这些表示与候选项目表示进行匹配，以进行推荐。然而，由于传统轻量级推荐模型容量有限，这种一步推理范式往往导致性能不佳。为应对这一问题，我们提出了一种新的慢思考推荐模型，名为 STREAM-Rec。我们的方法能够分析用户历史行为，生成多步、详尽的推理过程，并最终提供个性化推荐。特别是在两个关键挑战上：(1) 在推荐系统中识别合适的推理模式，(2) 探索如何有效地激发传统推荐器的推理能力。为此，我们引入了一个三阶段训练框架。在第一阶段，模型在大规模用户行为数据上进行预训练，以学习行为模式并捕捉长期依赖关系。在第二阶段，我们设计了一种迭代推理算法，通过逐步改进模型预测来标注合适的推理轨迹。然后使用这些标注数据进行模型微调。最后，在第三阶段，我们应用强化学习进一步增强模型的泛化能力。广泛实验验证了我们提出方法的有效性。', 'title_zh': '慢思考用于序列推荐'}
{'arxiv_id': 'arXiv:2504.09620', 'title': 'Metropolis-Hastings Captioning Game: Knowledge Fusion of Vision Language Models via Decentralized Bayesian Inference', 'authors': 'Yuta Matsui, Ryosuke Yamaki, Ryo Ueda, Seitaro Shinagawa, Tadahiro Taniguchi', 'link': 'https://arxiv.org/abs/2504.09620', 'abstract': "We propose the Metropolis-Hastings Captioning Game (MHCG), a method to fuse knowledge of multiple vision-language models (VLMs) by learning from each other. Although existing methods that combine multiple models suffer from inference costs and architectural constraints, MHCG avoids these problems by performing decentralized Bayesian inference through a process resembling a language game. The knowledge fusion process establishes communication between two VLM agents alternately captioning images and learning from each other. We conduct two image-captioning experiments with two VLMs, each pre-trained on a different dataset. The first experiment demonstrates that MHCG achieves consistent improvement in reference-free evaluation metrics. The second experiment investigates how MHCG contributes to sharing VLMs' category-level vocabulary by observing the occurrence of the vocabulary in the generated captions.", 'abstract_zh': '我们提出了一种名为Metropolis-Hastings Captioning Game (MHCG)的方法，这是一种通过相互学习融合多种视觉-语言模型（VLMs）知识的方法。通过一个类似语言游戏的过程进行去中心化的贝叶斯推断，MHCG避免了现有方法面临的推理成本和架构限制问题。知识融合过程通过交替对图像进行说明并相互学习在两个VLM代理之间建立通信。我们使用两个分别在不同数据集上预训练的VLMs进行了两项图像说明实验。第一个实验展示了MHCG在参考无关评估指标中的持续改进。第二个实验探讨了MHCG如何通过观察生成说明中词汇的出现来促进VLMs类别级别词汇的共享。', 'title_zh': '基于梅特罗波利斯-哈特灵采样的话语生成游戏：分散式贝叶斯推断驱动的视觉语言模型知识融合'}
{'arxiv_id': 'arXiv:2504.09546', 'title': 'A simulation-heuristics dual-process model for intuitive physics', 'authors': 'Shiqian Li, Yuxi Ma, Jiajun Yan, Bo Dai, Yujia Peng, Chi Zhang, Yixin Zhu', 'link': 'https://arxiv.org/abs/2504.09546', 'abstract': 'The role of mental simulation in human physical reasoning is widely acknowledged, but whether it is employed across scenarios with varying simulation costs and where its boundary lies remains unclear. Using a pouring-marble task, our human study revealed two distinct error patterns when predicting pouring angles, differentiated by simulation time. While mental simulation accurately captured human judgments in simpler scenarios, a linear heuristic model better matched human predictions when simulation time exceeded a certain boundary. Motivated by these observations, we propose a dual-process framework, Simulation-Heuristics Model (SHM), where intuitive physics employs simulation for short-time simulation but switches to heuristics when simulation becomes costly. By integrating computational methods previously viewed as separate into a unified model, SHM quantitatively captures their switching mechanism. The SHM aligns more precisely with human behavior and demonstrates consistent predictive performance across diverse scenarios, advancing our understanding of the adaptive nature of intuitive physical reasoning.', 'abstract_zh': '心智模拟在人类物理推理中的作用得到了广泛认可，但在不同模拟成本的场景下是否被普遍使用以及其边界尚不清楚。通过抛珠实验，我们的研究揭示了在预测倾倒角度时存在两种不同的错误模式，这些模式受模拟时间的影响。心智模拟在简单场景中能准确捕捉人类判断，但在模拟时间超出一定边界时，线性启发式模型能更好地匹配人类预测。基于这些观察，我们提出了一种双重过程框架——心智模拟-启发式模型（SHM），该模型认为直观物理在短时间内使用模拟，而在模拟成本增加时转而使用启发式策略。通过将先前被认为是独立的计算方法整合到一个统一模型中，SHM 能量化地捕捉它们的转换机制。SHM 更精确地符合人类行为，并在不同场景中表现出一致的预测性能，有助于我们理解直观物理推理的适应性本质。', 'title_zh': '直观物理的仿真-启发式双过程模型'}
{'arxiv_id': 'arXiv:2504.09499', 'title': 'Decoding the mechanisms of the Hattrick football manager game using Bayesian network structure learning for optimal decision-making', 'authors': 'Anthony C. Constantinou, Nicholas Higgins, Neville K. Kitson', 'link': 'https://arxiv.org/abs/2504.09499', 'abstract': "Hattrick is a free web-based probabilistic football manager game with over 200,000 users competing for titles at national and international levels. Launched in Sweden in 1997 as part of an MSc project, the game's slow-paced design has fostered a loyal community, with many users remaining active for decades. Hattrick's game-engine mechanics are partially hidden, and users have attempted to decode them with incremental success over the years. Rule-based, statistical and machine learning models have been developed to aid this effort and are widely used by the community. However, these models or tools have not been formally described or evaluated in the scientific literature. This study is the first to explore Hattrick using structure learning techniques and Bayesian networks, integrating both data and domain knowledge to develop models capable of explaining and simulating the game engine. We present a comprehensive analysis assessing the effectiveness of structure learning algorithms in relation to knowledge-based structures, and show that while structure learning may achieve a higher overall network fit, it does not result in more accurate predictions for selected variables of interest, when compared to knowledge-based networks that produce a lower overall network fit. Additionally, we introduce and publicly share a fully specified Bayesian network model that matches the performance of top models used by the Hattrick community. We further demonstrate how analysis extends beyond prediction by providing a visual representation of conditional dependencies, and using the best performing Bayesian network model for in-game decision-making. To support future research, we make all data, graphical structures, and models publicly available online.", 'abstract_zh': 'Hattrick作为一种基于网页的概率足球管理游戏，拥有超过200,000名用户，在国家级和国际级别上角逐冠军。该游戏于1997年在瑞典作为一项硕士项目的一部分推出，其缓慢的游戏节奏培养了一个忠诚的社区，许多用户已经活跃数十年。Hattrick的游戏机制部分隐藏，用户在过去几年中试图解码这些机制并取得了一定的成功。基于规则、统计和机器学习模型已被开发用于辅助这一努力，并在社区中广泛使用。然而，这些模型或工具并未在科学文献中正式描述或评估。本研究首次使用结构学习技术和贝叶斯网络探讨Hattrick，结合数据和领域知识开发能够解释和模拟游戏机制的模型。我们呈现了一个全面的分析，评估了结构学习算法在知识导向结构下的有效性，并展示了虽然结构学习可能实现更高的整体网络拟合度，但在与产生较低整体拟合度的知识导向网络进行比较时，并未对选定的变量产生更准确的预测。此外，我们引入并公开分享了一个完全指定的贝叶斯网络模型，其性能与Hattrick社区中使用的顶级模型相当。我们进一步展示分析如何超越预测，通过提供条件依赖的可视化表示，并使用性能最优的贝叶斯网络模型进行游戏中决策。为了支持未来的研究，我们在线公开了所有数据、图形结构和模型。', 'title_zh': '使用贝叶斯网络结构学习解析Hatrick足球经理游戏的机制以实现最优决策'}
{'arxiv_id': 'arXiv:2504.09493', 'title': 'Federated Prototype Graph Learning', 'authors': 'Zhengyu Wu, Xunkai Li, Yinlin Zhu, Rong-Hua Li, Guoren Wang, Chenghu Zhou', 'link': 'https://arxiv.org/abs/2504.09493', 'abstract': 'In recent years, Federated Graph Learning (FGL) has gained significant attention for its distributed training capabilities in graph-based machine intelligence applications, mitigating data silos while offering a new perspective for privacy-preserve large-scale graph learning. However, multi-level FGL heterogeneity presents various client-server collaboration challenges: (1) Model-level: The variation in clients for expected performance and scalability necessitates the deployment of heterogeneous models. Unfortunately, most FGL methods rigidly demand identical client models due to the direct model weight aggregation on the server. (2) Data-level: The intricate nature of graphs, marked by the entanglement of node profiles and topology, poses an optimization dilemma. This implies that models obtained by federated training struggle to achieve superior performance. (3) Communication-level: Some FGL methods attempt to increase message sharing among clients or between clients and the server to improve training, which inevitably leads to high communication costs. In this paper, we propose FedPG as a general prototype-guided optimization method for the above multi-level FGL heterogeneity. Specifically, on the client side, we integrate multi-level topology-aware prototypes to capture local graph semantics. Subsequently, on the server side, leveraging the uploaded prototypes, we employ topology-guided contrastive learning and personalized technology to tailor global prototypes for each client, broadcasting them to improve local training. Experiments demonstrate that FedPG outperforms SOTA baselines by an average of 3.57\\% in accuracy while reducing communication costs by 168x.', 'abstract_zh': '联邦图学习中的多层异质性及其通用原型导向优化方法：FedPG', 'title_zh': '联邦原型图学习'}
{'arxiv_id': 'arXiv:2504.09463', 'title': 'Comorbidity-Informed Transfer Learning for Neuro-developmental Disorder Diagnosis', 'authors': 'Xin Wen, Shijie Guo, Wenbo Ning, Rui Cao, Jie Xiang, Xiaobo Liu, Jintai Chen', 'link': 'https://arxiv.org/abs/2504.09463', 'abstract': 'Neuro-developmental disorders are manifested as dysfunctions in cognition, communication, behaviour and adaptability, and deep learning-based computer-aided diagnosis (CAD) can alleviate the increasingly strained healthcare resources on neuroimaging. However, neuroimaging such as fMRI contains complex spatio-temporal features, which makes the corresponding representations susceptible to a variety of distractions, thus leading to less effective in CAD. For the first time, we present a Comorbidity-Informed Transfer Learning(CITL) framework for diagnosing neuro-developmental disorders using fMRI. In CITL, a new reinforced representation generation network is proposed, which first combines transfer learning with pseudo-labelling to remove interfering patterns from the temporal domain of fMRI and generates new representations using encoder-decoder architecture. The new representations are then trained in an architecturally simple classification network to obtain CAD model. In particular, the framework fully considers the comorbidity mechanisms of neuro-developmental disorders and effectively integrates them with semi-supervised learning and transfer learning, providing new perspectives on interdisciplinary. Experimental results demonstrate that CITL achieves competitive accuracies of 76.32% and 73.15% for detecting autism spectrum disorder and attention deficit hyperactivity disorder, respectively, which outperforms existing related transfer learning work for 7.2% and 0.5% respectively.', 'abstract_zh': '基于共病的转移学习在功能性磁共振成像中诊断神经发育障碍', 'title_zh': '共病导向的迁移学习在神经发育障碍诊断中的应用'}
{'arxiv_id': 'arXiv:2504.09459', 'title': 'Measuring Leakage in Concept-Based Methods: An Information Theoretic Approach', 'authors': 'Mikael Makonnen, Moritz Vandenhirtz, Sonia Laguna, Julia E Vogt', 'link': 'https://arxiv.org/abs/2504.09459', 'abstract': 'Concept Bottleneck Models (CBMs) aim to enhance interpretability by structuring predictions around human-understandable concepts. However, unintended information leakage, where predictive signals bypass the concept bottleneck, compromises their transparency. This paper introduces an information-theoretic measure to quantify leakage in CBMs, capturing the extent to which concept embeddings encode additional, unintended information beyond the specified concepts. We validate the measure through controlled synthetic experiments, demonstrating its effectiveness in detecting leakage trends across various configurations. Our findings highlight that feature and concept dimensionality significantly influence leakage, and that classifier choice impacts measurement stability, with XGBoost emerging as the most reliable estimator. Additionally, preliminary investigations indicate that the measure exhibits the anticipated behavior when applied to soft joint CBMs, suggesting its reliability in leakage quantification beyond fully synthetic settings. While this study rigorously evaluates the measure in controlled synthetic experiments, future work can extend its application to real-world datasets.', 'abstract_zh': 'CBMs的信息论度量：量化概念瓶颈模型中的信息泄露', 'title_zh': '基于概念的方法中的泄露量测：一种信息论方法'}
{'arxiv_id': 'arXiv:2504.09428', 'title': 'FROG: Effective Friend Recommendation in Online Games via Modality-aware User Preferences', 'authors': 'Qiwei Wang, Dandan Lin, Wenqing Lin, Ziming Wu', 'link': 'https://arxiv.org/abs/2504.09428', 'abstract': 'Due to the convenience of mobile devices, the online games have become an important part for user entertainments in reality, creating a demand for friend recommendation in online games. However, none of existing approaches can effectively incorporate the multi-modal user features (\\emph{e.g.}, images and texts) with the structural information in the friendship graph, due to the following limitations: (1) some of them ignore the high-order structural proximity between users, (2) some fail to learn the pairwise relevance between users at modality-specific level, and (3) some cannot capture both the local and global user preferences on different modalities. By addressing these issues, in this paper, we propose an end-to-end model \\textsc{FROG} that better models the user preferences on potential friends. Comprehensive experiments on both offline evaluation and online deployment at \\kw{Tencent} have demonstrated the superiority of \\textsc{FROG} over existing approaches.', 'abstract_zh': '基于多模态用户特征和友谊图结构信息的端到端好友推荐模型FROG', 'title_zh': 'FROG：基于模态aware用户偏好在线游戏中有效的朋友推荐'}
{'arxiv_id': 'arXiv:2504.09398', 'title': 'Composable NLP Workflows for BERT-based Ranking and QA System', 'authors': 'Gaurav Kumar, Murali Mohana Krishna Dandu', 'link': 'https://arxiv.org/abs/2504.09398', 'abstract': 'There has been a lot of progress towards building NLP models that scale to multiple tasks. However, real-world systems contain multiple components and it is tedious to handle cross-task interaction with varying levels of text granularity. In this work, we built an end-to-end Ranking and Question-Answering (QA) system using Forte, a toolkit that makes composable NLP pipelines. We utilized state-of-the-art deep learning models such as BERT, RoBERTa in our pipeline, evaluated the performance on MS-MARCO and Covid-19 datasets using metrics such as BLUE, MRR, F1 and compared the results of ranking and QA systems with their corresponding benchmark results. The modular nature of our pipeline and low latency of reranker makes it easy to build complex NLP applications easily.', 'abstract_zh': '面向多个任务的自然语言处理模型研究：基于Forte的端到端排名与问答系统', 'title_zh': '基于BERT的排名与问答系统可组合自然语言处理工作流'}
{'arxiv_id': 'arXiv:2504.09396', 'title': 'Adaptive Insurance Reserving with CVaR-Constrained Reinforcement Learning under Macroeconomic Regimes', 'authors': 'Stella C. Dong, James R. Finlay', 'link': 'https://arxiv.org/abs/2504.09396', 'abstract': "This paper proposes a reinforcement learning (RL) framework for insurance reserving that integrates tail-risk sensitivity, macroeconomic regime modeling, and regulatory compliance. The reserving problem is formulated as a finite-horizon Markov Decision Process (MDP), in which reserve adjustments are optimized using Proximal Policy Optimization (PPO) subject to Conditional Value-at-Risk (CVaR) constraints. To enhance policy robustness across varying economic conditions, the agent is trained using a regime-aware curriculum that progressively increases volatility exposure.\nThe reward structure penalizes reserve shortfall, capital inefficiency, and solvency floor violations, with design elements informed by Solvency II and Own Risk and Solvency Assessment (ORSA) frameworks. Empirical evaluations on two industry datasets--Workers' Compensation, and Other Liability--demonstrate that the RL-CVaR agent achieves superior performance relative to classical reserving methods across multiple criteria, including tail-risk control (CVaR$_{0.95}$), capital efficiency, and regulatory violation rate. The framework also accommodates fixed-shock stress testing and regime-stratified analysis, providing a principled and extensible approach to reserving under uncertainty.", 'abstract_zh': '本文提出了一种整合尾部风险敏感性、宏观经济状态建模和监管合规性的 reinforcement learning (RL) 保险赔付准备金框架。将准备金问题形式化为有限 horizon 马尔可夫决策过程 (MDP)，并通过条件价值-at-风险 (CVaR) 约束使用近端策略优化 (PPO) 优化准备金调整。为了增强政策在不同经济条件下的鲁棒性，使用状态感知的训练课程来逐步增加波动性暴露进行训练。奖励结构惩罚准备金短缺、资本效率低下和偿付能力底线违规，设计元素受到偿付能力 II 和Own Risk and Solvency Assessment (ORSA) 架构的启发。在两个行业数据集——工伤赔偿和其它责任——上的实证评估表明，RL-CVaR 剂量相对于经典准备金方法在多个标准上实现了更高的性能，包括尾部风险控制 (CVaR$_{0.95}$)、资本效率和监管违规率。该框架还支持固定冲击的压力测试和状态分层分析，提供了一种原理明晰且可扩展的不确定性下准备金建模方法。', 'title_zh': '宏观经济环境下基于CVaR约束强化学习的适应性保险准备金计提'}
{'arxiv_id': 'arXiv:2504.09352', 'title': 'Explorer: Robust Collection of Interactable GUI Elements', 'authors': 'Iason Chaimalas, Arnas Vyšniauskas, Gabriel Brostow', 'link': 'https://arxiv.org/abs/2504.09352', 'abstract': "Automation of existing Graphical User Interfaces (GUIs) is important but hard to achieve. Upstream of making the GUI user-accessible or somehow scriptable, even the data-collection to understand the original interface poses significant challenges. For example, large quantities of general UI data seem helpful for training general machine learning (ML) models, but accessibility for each person can hinge on the ML's precision on a specific app. We therefore take the perspective that a given user needs confidence, that the relevant UI elements are being detected correctly throughout one app or digital environment. We mostly assume that the target application is known in advance, so that data collection and ML-training can be personalized for the test-time target domain. The proposed Explorer system focuses on detecting on-screen buttons and text-entry fields, i.e. interactables, where the training process has access to a live version of the application. The live application can run on almost any popular platform except iOS phones, and the collection is especially streamlined for Android phones or for desktop Chrome browsers. Explorer also enables the recording of interactive user sessions, and subsequent mapping of how such sessions overlap and sometimes loop back to similar states. We show how having such a map enables a kind of path planning through the GUI, letting a user issue audio commands to get to their destination. Critically, we are releasing our code for Explorer openly at this https URL.", 'abstract_zh': '现有的图形用户界面（GUI）的自动化至关重要但难以实现。在使GUI对用户可用或以某种方式使其脚本化之前，甚至收集数据以理解原始界面本身都面临着重大挑战。例如，大量通用UI数据似乎有助于训练通用机器学习（ML）模型，但每个人的数据访问能力取决于ML在特定应用上的精度。因此，我们从一个用户需要对相关UI元素在整个应用或数字环境中被正确检测的程度具有信心的角度出发。我们假设目标应用程序事先已知，以便数据收集和ML训练可以针对测试时的目标领域进行个性化处理。所提出的Explorer系统专注于检测屏幕上的按钮和文本输入字段，即可交互元素，并且训练过程可以访问应用的实时版本。该应用可以在除iOS手机之外的几乎所有主流平台上运行，数据收集特别简化了Android手机或桌面Chrome浏览器的采集过程。Explorer还能够记录交互式用户会话，并在随后映射这些会话如何相互重叠，有时又循环回到类似的状态。我们展示了此类地图如何使用户能够通过GUI进行路径规划，并发出语音命令到达目的地的重要之处。关键的是，我们正在在此公开发布Explorer的代码。', 'title_zh': 'Explorer: 坚韧的交互式GUI元素收集器'}
{'arxiv_id': 'arXiv:2504.09346', 'title': '"It\'s not a representation of me": Examining Accent Bias and Digital Exclusion in Synthetic AI Voice Services', 'authors': 'Shira Michel, Sufi Kaur, Sarah Elizabeth Gillespie, Jeffrey Gleason, Christo Wilson, Avijit Ghosh', 'link': 'https://arxiv.org/abs/2504.09346', 'abstract': "Recent advances in artificial intelligence (AI) speech generation and voice cloning technologies have produced naturalistic speech and accurate voice replication, yet their influence on sociotechnical systems across diverse accents and linguistic traits is not fully understood. This study evaluates two synthetic AI voice services (Speechify and ElevenLabs) through a mixed methods approach using surveys and interviews to assess technical performance and uncover how users' lived experiences influence their perceptions of accent variations in these speech technologies. Our findings reveal technical performance disparities across five regional, English-language accents and demonstrate how current speech generation technologies may inadvertently reinforce linguistic privilege and accent-based discrimination, potentially creating new forms of digital exclusion. Overall, our study highlights the need for inclusive design and regulation by providing actionable insights for developers, policymakers, and organizations to ensure equitable and socially responsible AI speech technologies.", 'abstract_zh': 'Recent Advances in Artificial Intelligence Speech Generation and Voice Cloning Technologies and Their Influence on Sociotechnical Systems Across Diverse Accents and Linguistic Traits: A Mixed Methods Study Evaluating Technical Performance and Perceptions of Accent Variations in Synthetic AI Voice Services', 'title_zh': '“这不是我的表现”：探究口音偏见与合成AI声音服务中的数字排斥现象'}
{'arxiv_id': 'arXiv:2504.09283', 'title': 'Semantic Commit: Helping Users Update Intent Specifications for AI Memory at Scale', 'authors': 'Priyan Vaithilingam, Munyeong Kim, Frida-Cecilia Acosta-Parenteau, Daniel Lee, Amine Mhedhbi, Elena L. Glassman, Ian Arawjo', 'link': 'https://arxiv.org/abs/2504.09283', 'abstract': 'How do we update AI memory of user intent as intent changes? We consider how an AI interface may assist the integration of new information into a repository of natural language data. Inspired by software engineering concepts like impact analysis, we develop methods and a UI for managing semantic changes with non-local effects, which we call "semantic conflict resolution." The user commits new intent to a project -- makes a "semantic commit" -- and the AI helps the user detect and resolve semantic conflicts within a store of existing information representing their intent (an "intent specification"). We develop an interface, SemanticCommit, to better understand how users resolve conflicts when updating intent specifications such as Cursor Rules and game design documents. A knowledge graph-based RAG pipeline drives conflict detection, while LLMs assist in suggesting resolutions. We evaluate our technique on an initial benchmark. Then, we report a 12 user within-subjects study of SemanticCommit for two task domains -- game design documents, and AI agent memory in the style of ChatGPT memories -- where users integrated new information into an existing list. Half of our participants adopted a workflow of impact analysis, where they would first flag conflicts without AI revisions then resolve conflicts locally, despite having access to a global revision feature. We argue that AI agent interfaces, such as software IDEs like Cursor and Windsurf, should provide affordances for impact analysis and help users validate AI retrieval independently from generation. Our work speaks to how AI agent designers should think about updating memory as a process that involves human feedback and decision-making.', 'abstract_zh': '如何更新用户意图的AI记忆？我们考虑AI界面如何协助将新信息整合到自然语言数据仓库中。受软件工程概念如影响分析的启发，我们开发了管理具有非局部效应的语义变化的方法和界面，称之为“语义冲突解决”。用户将新的意图提交给项目——进行“语义提交”——并由AI帮助用户在现有信息库中检测和解决与他们意图相关的语义冲突（“意图规范”）。我们开发了界面SemanticCommit，以更好地理解用户在更新意图规范（如光标规则和游戏设计文档）时如何解决冲突。基于知识图谱的RAG管道驱动冲突检测，而LLM帮助提出解决方案。我们对初始基准进行了技术评估，然后报告了12名用户的内组研究，研究对象是两个任务领域——游戏设计文档和类似ChatGPT记忆的AI代理记忆，其中用户将新信息整合到现有列表中。我们的参与者中有一半采用了影响分析的工作流程，即首先标记冲突而不进行AI修订，然后在有全局修订功能的情况下局部解决冲突。我们认为，AI代理界面，如同Cursor和Windsurf这样的软件IDE，应提供影响分析的功能，帮助用户独立于生成验证AI检索结果。我们的工作讨论了AI代理设计师在更新记忆时应如何考虑包含人类反馈和决策过程的过程。', 'title_zh': '语义提交：帮助用户大规模更新AI记忆的意图规范'}
{'arxiv_id': 'arXiv:2504.09225', 'title': 'AMNet: An Acoustic Model Network for Enhanced Mandarin Speech Synthesis', 'authors': 'Yubing Cao, Yinfeng Yu, Yongming Li, Liejun Wang', 'link': 'https://arxiv.org/abs/2504.09225', 'abstract': "This paper presents AMNet, an Acoustic Model Network designed to improve the performance of Mandarin speech synthesis by incorporating phrase structure annotation and local convolution modules. AMNet builds upon the FastSpeech 2 architecture while addressing the challenge of local context modeling, which is crucial for capturing intricate speech features such as pauses, stress, and intonation. By embedding a phrase structure parser into the model and introducing a local convolution module, AMNet enhances the model's sensitivity to local information. Additionally, AMNet decouples tonal characteristics from phonemes, providing explicit guidance for tone modeling, which improves tone accuracy and pronunciation. Experimental results demonstrate that AMNet outperforms baseline models in subjective and objective evaluations. The proposed model achieves superior Mean Opinion Scores (MOS), lower Mel Cepstral Distortion (MCD), and improved fundamental frequency fitting $F0 (R^2)$, confirming its ability to generate high-quality, natural, and expressive Mandarin speech.", 'abstract_zh': 'AMNet：一种通过引入短时卷积模块和短语结构注解以提高 Mandarin 语音合成性能的声学模型网络', 'title_zh': 'AMNet：一种增强型 Mandarin 语音合成声学模型网络'}
{'arxiv_id': 'arXiv:2504.09210', 'title': 'FairACE: Achieving Degree Fairness in Graph Neural Networks via Contrastive and Adversarial Group-Balanced Training', 'authors': 'Jiaxin Liu, Xiaoqian Jiang, Cangqi Zhou, Jing Zhang', 'link': 'https://arxiv.org/abs/2504.09210', 'abstract': 'Fairness has been a significant challenge in graph neural networks (GNNs) since degree biases often result in un-equal prediction performance among nodes with varying degrees. Existing GNN models focus on prediction accuracy, frequently overlooking fairness across different degree groups. To addressthis issue, we propose a novel GNN framework, namely Fairness- Aware Asymmetric Contrastive Ensemble (FairACE), which inte-grates asymmetric contrastive learning with adversarial training to improve degree fairness. FairACE captures one-hop local neighborhood information and two-hop monophily similarity to create fairer node representations and employs a degree fairness regulator to balance performance between high-degree and low-degree nodes. During model training, a novel group-balanced fairness loss is proposed to minimize classification disparities across degree groups. In addition, we also propose a novel fairness metric, the Accuracy Distribution Gap (ADG), which can quantitatively assess and ensure equitable performance across different degree-based node groups. Experimental results on both synthetic and real-world datasets demonstrate that FairACE significantly improves degree fairness metrics while maintaining competitive accuracy in comparison to the state-of-the-art GNN models.', 'abstract_zh': '公平意识异构对比增强图神经网络框架 (Fairness-Aware Asymmetric Contrastive Ensemble, FairACE)', 'title_zh': 'FairACE：通过对比和对抗组平衡训练在图神经网络中实现度公平性'}
{'arxiv_id': 'arXiv:2504.09185', 'title': "Repetitive Contrastive Learning Enhances Mamba's Selectivity in Time Series Prediction", 'authors': 'Wenbo Yan, Hanzhong Cao, Ying Tan', 'link': 'https://arxiv.org/abs/2504.09185', 'abstract': "Long sequence prediction is a key challenge in time series forecasting. While Mamba-based models have shown strong performance due to their sequence selection capabilities, they still struggle with insufficient focus on critical time steps and incomplete noise suppression, caused by limited selective abilities. To address this, we introduce Repetitive Contrastive Learning (RCL), a token-level contrastive pretraining framework aimed at enhancing Mamba's selective capabilities. RCL pretrains a single Mamba block to strengthen its selective abilities and then transfers these pretrained parameters to initialize Mamba blocks in various backbone models, improving their temporal prediction performance. RCL uses sequence augmentation with Gaussian noise and applies inter-sequence and intra-sequence contrastive learning to help the Mamba module prioritize information-rich time steps while ignoring noisy ones. Extensive experiments show that RCL consistently boosts the performance of backbone models, surpassing existing methods and achieving state-of-the-art results. Additionally, we propose two metrics to quantify Mamba's selective capabilities, providing theoretical, qualitative, and quantitative evidence for the improvements brought by RCL.", 'abstract_zh': '长期序列预测是时间序列 forecasting 中的关键挑战。虽然基于 Mamba 的模型由于其序列选择能力表现出强大的性能，但它们仍难以集中关注关键的时间步长并完全抑制噪声，这主要是由于选择能力的局限性所致。为解决这一问题，我们引入了重复对比学习（RCL），这是一种针对增强 Mamba 的选择能力而设计的 token 级对比预训练框架。RCL 预训练单个 Mamba 模块以增强其选择能力，然后将预训练参数转移以初始化各种骨干模型中的 Mamba 模块，从而提高其时间预测性能。RCL 使用高斯噪声进行序列增强，并应用跨序列和序列内对比学习，以帮助 Mamba 模块优先处理信息丰富的时间步长并忽略噪声时间步长。广泛实验表明，RCL 一致性地提升了骨干模型的性能，超越了现有方法并达到了最先进的成果。此外，我们提出了两种度量标准来量化 Mamba 的选择能力，提供了 RCL 所带来的改进的理论、定性和定量证据。', 'title_zh': '重复对比学习增强Mamba在时间序列预测中的选择性'}
{'arxiv_id': 'arXiv:2504.09184', 'title': 'Parameterized Synthetic Text Generation with SimpleStories', 'authors': 'Lennart Finke, Thomas Dooms, Mat Allen, Juan Diego Rodriguez, Noa Nabeshima, Dan Braun', 'link': 'https://arxiv.org/abs/2504.09184', 'abstract': 'We present SimpleStories, a large synthetic story dataset in simple language, consisting of 2 million stories each in English and Japanese. Our method employs parametrization of prompts with features at multiple levels of abstraction, allowing for systematic control over story characteristics to ensure broad syntactic and semantic diversity. Building on and addressing limitations in the TinyStories dataset, our approach demonstrates that simplicity and variety can be achieved simultaneously in synthetic text generation at scale.', 'abstract_zh': '我们提出SimpleStories，一个使用简单语言构成的大型合成故事数据集，包含200万个英文和日文故事。我们的方法通过多层抽象特征参数化提示，允许对故事特征进行系统控制，以确保广泛的句法和语义多样性。基于并解决了TinyStories数据集的局限性，我们的方法表明，在大规模合成文本生成中，简洁性和多样性可以同时实现。', 'title_zh': '参数化合成文本生成：SimpleStories方法'}
{'arxiv_id': 'arXiv:2504.09179', 'title': 'A Confounding Factors-Inhibition Adversarial Learning Framework for Multi-site fMRI Mental Disorder Identification', 'authors': 'Xin Wen, Shijie Guo, Wenbo Ning, Rui Cao, Yan Niu, Bin Wan, Peng Wei, Xiaobo Liu, Jie Xiang', 'link': 'https://arxiv.org/abs/2504.09179', 'abstract': 'In open data sets of functional magnetic resonance imaging (fMRI), the heterogeneity of the data is typically attributed to a combination of factors, including differences in scanning procedures, the presence of confounding effects, and population diversities between multiple sites. These factors contribute to the diminished effectiveness of representation learning, which in turn affects the overall efficacy of subsequent classification procedures. To address these limitations, we propose a novel multi-site adversarial learning network (MSalNET) for fMRI-based mental disorder detection. Firstly, a representation learning module is introduced with a node information assembly (NIA) mechanism to better extract features from functional connectivity (FC). This mechanism aggregates edge information from both horizontal and vertical directions, effectively assembling node information. Secondly, to generalize the feature across sites, we proposed a site-level feature extraction module that can learn from individual FC data, which circumvents additional prior information. Lastly, an adversarial learning network is proposed as a means of balancing the trade-off between individual classification and site regression tasks, with the introduction of a novel loss function. The proposed method was evaluated on two multi-site fMRI datasets, i.e., Autism Brain Imaging Data Exchange (ABIDE) and ADHD-200. The results indicate that the proposed method achieves a better performance than other related algorithms with the accuracy of 75.56 and 68.92 in ABIDE and ADHD-200 datasets, respectively. Furthermore, the result of the site regression indicates that the proposed method reduces site variability from a data-driven perspective. The most discriminative brain regions revealed by NIA are consistent with statistical findings, uncovering the "black box" of deep learning to a certain extent.', 'abstract_zh': '一种用于功能性磁共振成像基于精神疾病检测的新型多站点对抗学习网络（MSalNET）', 'title_zh': '多中心fMRI精神障碍识别的共变量抑制对抗学习框架'}
{'arxiv_id': 'arXiv:2504.09164', 'title': 'Can postgraduate translation students identify machine-generated text?', 'authors': 'Michael Farrell', 'link': 'https://arxiv.org/abs/2504.09164', 'abstract': 'Given the growing use of generative artificial intelligence as a tool for creating multilingual content and bypassing both machine and traditional translation methods, this study explores the ability of linguistically trained individuals to discern machine-generated output from human-written text (HT). After brief training sessions on the textual anomalies typically found in synthetic text (ST), twenty-three postgraduate translation students analysed excerpts of Italian prose and assigned likelihood scores to indicate whether they believed they were human-written or AI-generated (ChatGPT-4o). The results show that, on average, the students struggled to distinguish between HT and ST, with only two participants achieving notable accuracy. Closer analysis revealed that the students often identified the same textual anomalies in both HT and ST, although features such as low burstiness and self-contradiction were more frequently associated with ST. These findings suggest the need for improvements in the preparatory training. Moreover, the study raises questions about the necessity of editing synthetic text to make it sound more human-like and recommends further research to determine whether AI-generated text is already sufficiently natural-sounding not to require further refinement.', 'abstract_zh': '生成式人工智能作为创建多语言内容并绕过机器和传统翻译方法的工具日益普及：本研究探讨了语言训练人员识别机器生成输出与人类撰写的文本的能力（HT）。经过简短的培训 sessions 有关合成文本（ST）中通常存在的文本异常，二十多名翻译硕士学生分析了意大利散文片段，并分配概率分数以表明他们认为这些片段是人类撰写的还是 AI 生成的（ChatGPT-4o）。结果显示，平均而言，学生难以区分 HT 和 ST，仅有两名参与者表现出较高的准确性。进一步分析表明，学生在 HT 和 ST 中经常识别相同的文本异常，尽管诸如低burstiness 和自相矛盾等特征更常与 ST 相关。这些发现表明需要改进预备培训。此外，研究还提出了一个问题，即是否有必要编辑合成文本使其听起来更接近人类撰写的内容，并建议进一步研究以确定 AI 生成的文本是否已足够自然无需进一步润色。', 'title_zh': '研究生翻译学生能否识别机器生成的文本？'}
{'arxiv_id': 'arXiv:2504.09101', 'title': 'Synthetic Aircraft Trajectory Generation Using Time-Based VQ-VAE', 'authors': 'Abdulmajid Murad, Massimiliano Ruocco', 'link': 'https://arxiv.org/abs/2504.09101', 'abstract': 'In modern air traffic management, generating synthetic flight trajectories has emerged as a promising solution for addressing data scarcity, protecting sensitive information, and supporting large-scale analyses. In this paper, we propose a novel method for trajectory synthesis by adapting the Time-Based Vector Quantized Variational Autoencoder (TimeVQVAE). Our approach leverages time-frequency domain processing, vector quantization, and transformer-based priors to capture both global and local dynamics in flight data. By discretizing the latent space and integrating transformer priors, the model learns long-range spatiotemporal dependencies and preserves coherence across entire flight paths. We evaluate the adapted TimeVQVAE using an extensive suite of quality, statistical, and distributional metrics, as well as a flyability assessment conducted in an open-source air traffic simulator. Results indicate that TimeVQVAE outperforms a temporal convolutional VAE baseline, generating synthetic trajectories that mirror real flight data in terms of spatial accuracy, temporal consistency, and statistical properties. Furthermore, the simulator-based assessment shows that most generated trajectories maintain operational feasibility, although occasional outliers underscore the potential need for additional domain-specific constraints. Overall, our findings underscore the importance of multi-scale representation learning for capturing complex flight behaviors and demonstrate the promise of TimeVQVAE in producing representative synthetic trajectories for downstream tasks such as model training, airspace design, and air traffic forecasting.', 'abstract_zh': '现代空中交通管理中基于时间矢量量化变分自编码器的轨迹合成方法', 'title_zh': '基于时间的VQ-VAE合成飞机轨迹生成'}
{'arxiv_id': 'arXiv:2504.09064', 'title': 'PQS (Prune, Quantize, and Sort): Low-Bitwidth Accumulation of Dot Products in Neural Network Computations', 'authors': 'Vikas Natesh, H.T. Kung', 'link': 'https://arxiv.org/abs/2504.09064', 'abstract': 'We present PQS, which uses three techniques together - Prune, Quantize, and Sort - to achieve low-bitwidth accumulation of dot products in neural network computations. In conventional quantized (e.g., 8-bit) dot products, partial results are accumulated into wide (e.g., 32-bit) accumulators to avoid overflows when accumulating intermediate partial sums. However, such wide accumulators increase memory bandwidth usage and reduce energy efficiency. We show that iterative N:M pruning in floating point followed by quantization to 8 (or fewer) bits, and accumulation of partial products in a sorted order ("small to large") allows for accurate, compressed models with short dot product lengths that do not require wide accumulators. We design, analyze, and implement the PQS algorithm to eliminate accumulation overflows at inference time for several neural networks. Our method offers a 2.5x reduction in accumulator bitwidth while achieving model accuracy on par with floating-point baselines for multiple image classification tasks.', 'abstract_zh': 'PQS: 结合剪枝、量化和排序的技术实现神经网络计算中低比特宽的点积累积', 'title_zh': 'PQS（修剪、量化和排序）：神经网络计算中点积的低位宽累积'}
{'arxiv_id': 'arXiv:2504.09063', 'title': 'A Practical Approach to using Supervised Machine Learning Models to Classify Aviation Safety Occurrences', 'authors': 'Bryan Y. Siow', 'link': 'https://arxiv.org/abs/2504.09063', 'abstract': 'This paper describes a practical approach of using supervised machine learning (ML) models to assist safety investigators to classify aviation occurrences into either incident or serious incident categories. Our implementation currently deployed as a ML web application is trained on a labelled dataset derived from publicly available aviation investigation reports. A selection of five supervised learning models (Support Vector Machine, Logistic Regression, Random Forest Classifier, XGBoost and K-Nearest Neighbors) were evaluated. This paper showed the best performing ML algorithm was the Random Forest Classifier with accuracy = 0.77, F1 Score = 0.78 and MCC = 0.51 (average of 100 sample runs). The study had also explored the effect of applying Synthetic Minority Over-sampling Technique (SMOTE) to the imbalanced dataset, and the overall observation ranged from no significant effect to substantial degradation in performance for some of the models after the SMOTE adjustment.', 'abstract_zh': '本文描述了一种实用的方法，使用监督机器学习（ML）模型辅助航空事故调查人员将航空事件分类为事件或严重事件类别。当前部署的基于标记数据集（源自公开的航空调查报告）训练的ML网络应用评估了五种监督学习模型（支持向量机、逻辑回归、随机森林分类器、XGBoost和K-近邻）。结果显示，随机森林分类器表现最佳，准确率=0.77，F1分值=0.78，MCC=0.51（100次样本运行的平均值）。研究还探讨了在不平衡数据集中应用合成少数类过采样技术（SMOTE）的效果，整体观察结果显示，对于一些模型，SMOTE调整后性能显著下降。', 'title_zh': '一种实用的方法，使用监督机器学习模型对 aviation 安全事件进行分类'}
{'arxiv_id': 'arXiv:2504.09060', 'title': 'Multimodal 3D Genome Pre-training', 'authors': 'Minghao Yang, Pengteng Li, Yan Liang, Qianyi Cai, Zhihang Zheng, Shichen Zhang, Pengfei Zhang, Zhi-An Huang, Hui Xiong', 'link': 'https://arxiv.org/abs/2504.09060', 'abstract': 'Deep learning techniques have driven significant progress in various analytical tasks within 3D genomics in computational biology. However, a holistic understanding of 3D genomics knowledge remains underexplored. Here, we propose MIX-HIC, the first multimodal foundation model of 3D genome that integrates both 3D genome structure and epigenomic tracks, which obtains unified and comprehensive semantics. For accurate heterogeneous semantic fusion, we design the cross-modal interaction and mapping blocks for robust unified representation, yielding the accurate aggregation of 3D genome knowledge. Besides, we introduce the first large-scale dataset comprising over 1 million pairwise samples of Hi-C contact maps and epigenomic tracks for high-quality pre-training, enabling the exploration of functional implications in 3D genomics. Extensive experiments show that MIX-HIC can significantly surpass existing state-of-the-art methods in diverse downstream tasks. This work provides a valuable resource for advancing 3D genomics research.', 'abstract_zh': '深度学习技术在计算生物学中的三维基因组各种分析任务中取得了显著进展。然而，对三维基因组知识的整体理解仍然未被充分探索。为此，我们提出了MIX-HIC，这是首个结合三维基因组结构和表观基因组轨迹的多模态基础模型，能够获得统一和全面的语义。为了实现准确的异质语义融合，我们设计了跨模态交互和映射模块，以获得稳健的统一表示，并准确聚集三维基因组知识。此外，我们引入了首个包含超过一百万个Hi-C接触图对和表观基因组轨迹的大规模数据集，用于高质量的预训练，从而能够探索三维基因组的功能含义。广泛实验表明，MIX-HIC在多种下游任务中显著优于现有最先进的方法。这项工作为促进三维基因组研究提供了 valuable 资源。', 'title_zh': '多模态3D基因组预训练'}
{'arxiv_id': 'arXiv:2504.09039', 'title': 'Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via Dynamic Mask and Concept-Aware Optimization', 'authors': 'Gen Li, Yang Xiao, Jie Ji, Kaiyuan Deng, Bo Hui, Linke Guo, Xiaolong Ma', 'link': 'https://arxiv.org/abs/2504.09039', 'abstract': 'Text-to-image (T2I) diffusion models have achieved remarkable success in generating high-quality images from textual prompts. However, their ability to store vast amounts of knowledge raises concerns in scenarios where selective forgetting is necessary, such as removing copyrighted content, reducing biases, or eliminating harmful concepts. While existing unlearning methods can remove certain concepts, they struggle with multi-concept forgetting due to instability, residual knowledge persistence, and generation quality degradation. To address these challenges, we propose \\textbf{Dynamic Mask coupled with Concept-Aware Loss}, a novel unlearning framework designed for multi-concept forgetting in diffusion models. Our \\textbf{Dynamic Mask} mechanism adaptively updates gradient masks based on current optimization states, allowing selective weight modifications that prevent interference with unrelated knowledge. Additionally, our \\textbf{Concept-Aware Loss} explicitly guides the unlearning process by enforcing semantic consistency through superclass alignment, while a regularization loss based on knowledge distillation ensures that previously unlearned concepts remain forgotten during sequential unlearning. We conduct extensive experiments to evaluate our approach. Results demonstrate that our method outperforms existing unlearning techniques in forgetting effectiveness, output fidelity, and semantic coherence, particularly in multi-concept scenarios. Our work provides a principled and flexible framework for stable and high-fidelity unlearning in generative models. The code will be released publicly.', 'abstract_zh': '文本到图像（T2I）扩散模型在从文本提示生成高质量图像方面取得了显著成功。然而，在需要选择性遗忘的场景中，如删除版权内容、减少偏见或消除有害概念时，它们庞大的知识存储能力引发了关注。虽然现有的遗忘方法可以移除某些概念，但在处理多概念遗忘时却因不稳定、残留知识持久存在以及生成质量下降而遇到了困难。为了解决这些问题，我们提出了一种新的多概念遗忘框架——动态掩码结合概念感知损失（Dynamic Mask coupled with Concept-Aware Loss），该框架旨在扩散模型中实现多概念遗忘。我们的动态掩码机制根据当前的优化状态自适应更新梯度掩码，允许选择性地修改权重以防止与无关知识的干扰。此外，我们提出的概念感知损失明确地指导遗忘过程，通过超类对齐确保语义一致性，而基于知识蒸馏的正则化损失则确保在顺序遗忘过程中之前未学习的概念能够被遗忘。我们进行了广泛的实验以评估我们的方法。结果表明，我们的方法在遗忘效果、输出保真度和语义一致性方面优于现有遗忘技术，尤其是在多概念场景中。我们的工作提供了一个原则性和灵活性兼具的框架，用于生成模型中的稳定和高保真遗忘。代码将公开发布。', 'title_zh': '塑形记忆：通过动态掩码和概念意识优化在扩散模型中实现多概念遗忘'}
{'arxiv_id': 'arXiv:2504.09014', 'title': 'MSCCL++: Rethinking GPU Communication Abstractions for Cutting-edge AI Applications', 'authors': 'Aashaka Shah, Abhinav Jangda, Binyang Li, Caio Rocha, Changho Hwang, Jithin Jose, Madan Musuvathi, Olli Saarikivi, Peng Cheng, Qinghua Zhou, Roshan Dathathri, Saeed Maleki, Ziyue Yang', 'link': 'https://arxiv.org/abs/2504.09014', 'abstract': 'Modern cutting-edge AI applications are being developed over fast-evolving, heterogeneous, nascent hardware devices. This requires frequent reworking of the AI software stack to adopt bottom-up changes from new hardware, which takes time for general-purpose software libraries. Consequently, real applications often develop custom software stacks optimized for their specific workloads and hardware. Custom stacks help quick development and optimization, but incur a lot of redundant efforts across applications in writing non-portable code. This paper discusses an alternative communication library interface for AI applications that offers both portability and performance by reducing redundant efforts while maintaining flexibility for customization. We present MSCCL++, a novel abstraction of GPU communication based on separation of concerns: (1) a primitive interface provides a minimal hardware abstraction as a common ground for software and hardware developers to write custom communication, and (2) higher-level portable interfaces and specialized implementations enable optimization for different hardware environments. This approach makes the primitive interface reusable across applications while enabling highly flexible optimization. Compared to state-of-the-art baselines (NCCL, RCCL, and MSCCL), MSCCL++ achieves speedups of up to 3.8$\\times$ for collective communication and up to 15\\% for real-world AI inference workloads. MSCCL++ is in production of multiple AI services provided by Microsoft Azure, and is also adopted by RCCL, the GPU collective communication library maintained by AMD. MSCCL++ is open-source and available at this https URL.', 'abstract_zh': '现代AI应用正在快速发展变化的、异构的新兴硬件设备上进行开发。这要求频繁调整AI软件栈以适应来自新硬件的底层变化，通用软件库需要时间进行适应。因此，实际应用中常常会开发针对其特定工作负载和硬件的定制软件栈。定制栈有助于快速开发和优化，但会在编写非便携代码时产生大量重复工作。本文讨论了一种AI应用的替代通信库接口，该接口通过减少重复工作同时保持定制灵活性来实现便携性和性能。我们提出了MSCCL++，这是一种基于关注点分离的GPU通信的新抽象：（1）原始接口提供了一个最小的硬件抽象作为软件和硬件开发者的共同基础，用于编写自定义通信；（2）高级别便携接口和专门实现能够针对不同的硬件环境进行优化。这种方法使得原始接口可以在应用之间重用，同时允许高度灵活的优化。与最先进的基线（NCCL、RCCL和MSCCL）相比，MSCCL++在集体通信中的性能提高了3.8倍，在实际AI推理工作负载中的性能提高了15%。MSCCL++被微软Azure提供的多个AI服务生产部署，并被AMD维护的GPU集体通信库RCCL采用。MSCCL++是开源软件，可在以下链接获取。', 'title_zh': 'MSCCL++: 重新思考面向前沿AI应用的GPU通信抽象'}
{'arxiv_id': 'arXiv:2504.08970', 'title': 'On Large-scale Evaluation of Embedding Models for Knowledge Graph Completion', 'authors': 'Nasim Shirvani-Mahdavi, Farahnaz Akrami, Chengkai Li', 'link': 'https://arxiv.org/abs/2504.08970', 'abstract': "Knowledge graph embedding (KGE) models are extensively studied for knowledge graph completion, yet their evaluation remains constrained by unrealistic benchmarks. Commonly used datasets are either faulty or too small to reflect real-world data. Few studies examine the role of mediator nodes, which are essential for modeling n-ary relationships, or investigate model performance variation across domains. Standard evaluation metrics rely on the closed-world assumption, which penalizes models for correctly predicting missing triples, contradicting the fundamental goals of link prediction. These metrics often compress accuracy assessment into a single value, obscuring models' specific strengths and weaknesses. The prevailing evaluation protocol operates under the unrealistic assumption that an entity's properties, for which values are to be predicted, are known in advance. While alternative protocols such as property prediction, entity-pair ranking and triple classification address some of these limitations, they remain underutilized. This paper conducts a comprehensive evaluation of four representative KGE models on large-scale datasets FB-CVT-REV and FB+CVT-REV. Our analysis reveals critical insights, including substantial performance variations between small and large datasets, both in relative rankings and absolute metrics, systematic overestimation of model capabilities when n-ary relations are binarized, and fundamental limitations in current evaluation protocols and metrics.", 'abstract_zh': '知识图嵌入（KGE）模型在知识图完成中的广泛研究仍未摆脱不现实的评估基准的限制。', 'title_zh': '大规模评估嵌入模型在知识图谱补全中的效果'}
{'arxiv_id': 'arXiv:2504.08947', 'title': 'Forecasting Cryptocurrency Prices using Contextual ES-adRNN with Exogenous Variables', 'authors': 'Slawek Smyl, Grzegorz Dudek, Paweł Pełka', 'link': 'https://arxiv.org/abs/2504.08947', 'abstract': 'In this paper, we introduce a new approach to multivariate forecasting cryptocurrency prices using a hybrid contextual model combining exponential smoothing (ES) and recurrent neural network (RNN). The model consists of two tracks: the context track and the main track. The context track provides additional information to the main track, extracted from representative series. This information as well as information extracted from exogenous variables is dynamically adjusted to the individual series forecasted by the main track. The RNN stacked architecture with hierarchical dilations, incorporating recently developed attentive dilated recurrent cells, allows the model to capture short and long-term dependencies across time series and dynamically weight input information. The model generates both point daily forecasts and predictive intervals for one-day, one-week and four-week horizons. We apply our model to forecast prices of 15 cryptocurrencies based on 17 input variables and compare its performance with that of comparative models, including both statistical and ML ones.', 'abstract_zh': '本研究提出了一种结合指数平滑和递归神经网络的混合上下文模型，用于多变量预测加密货币价格。该模型包含两个轨道：上下文轨道和主轨道。上下文轨道为主轨道提供额外信息，这些信息来自代表性序列，并且该信息以及来自外生变量的信息会动态调整以适应主轨道预测的个体序列。采用嵌套扩张的递归神经网络堆叠架构，结合了近期发展的注意扩张递归单元，使模型能够捕捉时间序列中的短期和长期依赖关系，并动态加权输入信息。该模型生成了一天、一周和四周时间范围内的点预测和预测区间。我们将该模型应用于基于17个输入变量预测15种加密货币的价格，并将其性能与统计模型和机器学习模型的性能进行了比较。', 'title_zh': '基于外生变量的上下文ES-adRNN加密货币价格预测'}
{'arxiv_id': 'arXiv:2504.08940', 'title': 'Combining Forecasts using Meta-Learning: A Comparative Study for Complex Seasonality', 'authors': 'Grzegorz Dudek', 'link': 'https://arxiv.org/abs/2504.08940', 'abstract': 'In this paper, we investigate meta-learning for combining forecasts generated by models of different types. While typical approaches for combining forecasts involve simple averaging, machine learning techniques enable more sophisticated methods of combining through meta-learning, leading to improved forecasting accuracy. We use linear regression, $k$-nearest neighbors, multilayer perceptron, random forest, and long short-term memory as meta-learners. We define global and local meta-learning variants for time series with complex seasonality and compare meta-learners on multiple forecasting problems, demonstrating their superior performance compared to simple averaging.', 'abstract_zh': '本文探讨了使用元学习方法结合不同类型模型生成的预测值。虽然传统的预测组合方法通常涉及简单的平均值，但机器学习技术通过元学习使得预测组合更加复杂，从而提高预测准确性。我们使用线性回归、$k$-最近邻、多层感知器、随机森林和长短期记忆网络作为元学习器。我们为具有复杂季节性的时间序列定义了全局和局部元学习变体，并在多个预测问题上比较了元学习器的性能，证明了它们优于简单的平均值方法。', 'title_zh': '基于元学习的组合预测：复杂季节性的比较研究'}
{'arxiv_id': 'arXiv:2504.08923', 'title': 'A convergence law for continuous logic and continuous structures with finite domains', 'authors': 'Vera Koponen', 'link': 'https://arxiv.org/abs/2504.08923', 'abstract': "We consider continuous relational structures with finite domain $[n] := \\{1, \\ldots, n\\}$ and a many valued logic, $CLA$, with values in the unit interval and which uses continuous connectives and continuous aggregation functions. $CLA$ subsumes first-order logic on ``conventional'' finite structures. To each relation symbol $R$ and identity constraint $ic$ on a tuple the length of which matches the arity of $R$ we associate a continuous probability density function $\\mu_R^{ic} : [0, 1] \\to [0, \\infty)$.\nWe also consider a probability distribution on the set $\\mathbf{W}_n$ of continuous structures with domain $[n]$ which is such that for every relation symbol $R$, identity constraint $ic$, and tuple $\\bar{a}$ satisfying $ic$, the distribution of the value of $R(\\bar{a})$ is given by $\\mu_R^{ic}$, independently of the values for other relation symbols or other tuples.\nIn this setting we prove that every formula in $CLA$ is asymptotically equivalent to a formula without any aggregation function. This is used to prove a convergence law for $CLA$ which reads as follows for formulas without free variables: If $\\varphi \\in CLA$ has no free variable and $I \\subseteq [0, 1]$ is an interval, then there is $\\alpha \\in [0, 1]$ such that, as $n$ tends to infinity, the probability that the value of $\\varphi$ is in $I$ tends to $\\alpha$.", 'abstract_zh': '我们考虑具有有限域$[n] := \\{1, \\ldots, n\\}$的连续关系结构及取值于单位 Interval 的多值逻辑 $CLA$，该逻辑使用连续联结词和连续聚集函数。$CLA$ 包含在“常规”有限结构上的一阶逻辑。对于每个关系符 $R$ 和相应的身份约束 $ic$，以及满足 $ic$ 的元组 $\\bar{a}$，我们关联一个连续概率密度函数 $\\mu_R^{ic} : [0, 1] \\to [0, \\infty)$。\n\n我们还考虑了在连续结构集合 $\\mathbf{W}_n$ 上的概率分布，该集合的域为 $[n]$，使得对于每一个关系符 $R$、身份约束 $ic$ 和满足 $ic$ 的元组 $\\bar{a}$，$R(\\bar{a})$ 的值分布由 $\\mu_R^{ic}$ 给出，并且与其他关系符或元组的值无关。\n\n在此背景下，我们证明了 $CLA$ 中的每个公式在无穷大时几乎等价于不含聚集函数的公式。这被用来证明 $CLA$ 的收敛定律如下：如果 $\\varphi \\in CLA$ 没有自由变量，且 $I \\subseteq [0, 1]$ 是一个区间，则存在 $\\alpha \\in [0, 1]$，当 $n$ 趋向无穷时，$\\varphi$ 的值在 $I$ 中的概率趋于 $\\alpha$。', 'title_zh': '连续逻辑中连续结构有限域的收敛定律'}
{'arxiv_id': 'arXiv:2504.08919', 'title': 'Are We Merely Justifying Results ex Post Facto? Quantifying Explanatory Inversion in Post-Hoc Model Explanations', 'authors': 'Zhen Tan, Song Wang, Yifan Li, Yu Kong, Jundong Li, Tianlong Chen, Huan Liu', 'link': 'https://arxiv.org/abs/2504.08919', 'abstract': 'Post-hoc explanation methods provide interpretation by attributing predictions to input features. Natural explanations are expected to interpret how the inputs lead to the predictions. Thus, a fundamental question arises: Do these explanations unintentionally reverse the natural relationship between inputs and outputs? Specifically, are the explanations rationalizing predictions from the output rather than reflecting the true decision process? To investigate such explanatory inversion, we propose Inversion Quantification (IQ), a framework that quantifies the degree to which explanations rely on outputs and deviate from faithful input-output relationships. Using the framework, we demonstrate on synthetic datasets that widely used methods such as LIME and SHAP are prone to such inversion, particularly in the presence of spurious correlations, across tabular, image, and text domains. Finally, we propose Reproduce-by-Poking (RBP), a simple and model-agnostic enhancement to post-hoc explanation methods that integrates forward perturbation checks. We further show that under the IQ framework, RBP theoretically guarantees the mitigation of explanatory inversion. Empirically, for example, on the synthesized data, RBP can reduce the inversion by 1.8% on average across iconic post-hoc explanation approaches and domains.', 'abstract_zh': '后验解释方法通过将预测归因于输入特征来提供解释。自然解释期望解释输入如何导致预测。因此，一个基本问题出现了：这些解释是否无意中逆转了输入与输出之间的自然关系？具体来说，它们是否在解释预测时理性化了输出，而不是反映真正的决策过程？为了调查这种解释逆转现象，我们提出了逆转量化（Inversion Quantification, IQ）框架，该框架量化了解释依赖于输出的程度以及偏离忠实的输入-输出关系的程度。使用该框架，我们演示在合成数据集上，广泛使用的LIME和SHAP方法在存在虚假相关性时，尤其是对于表格、图像和文本领域中的广泛后验解释方法，容易发生逆转。最后，我们提出了一种简单且模型无关的增强方法Reproduce-by-Poking（RBP），该方法整合了前向扰动检查。进一步证明，在IQ框架下，RBP理论上保证了解释逆转的缓解。实验中，例如，在合成数据上，RBP可以将图标型后验解释方法和领域中的逆转平均减少1.8%。', 'title_zh': '我们在 merely 后面的内容翻译有误，正确的翻译应为：\n\nAre We Merely Justifying Results A Posteriori? Quantifying Explanatory Inversion in Post-Hoc Model Explanations'}
{'arxiv_id': 'arXiv:2504.08896', 'title': 'Position: Beyond Euclidean -- Foundation Models Should Embrace Non-Euclidean Geometries', 'authors': 'Neil He, Jiahong Liu, Buze Zhang, Ngoc Bui, Ali Maatouk, Menglin Yang, Irwin King, Melanie Weber, Rex Ying', 'link': 'https://arxiv.org/abs/2504.08896', 'abstract': 'In the era of foundation models and Large Language Models (LLMs), Euclidean space has been the de facto geometric setting for machine learning architectures. However, recent literature has demonstrated that this choice comes with fundamental limitations. At a large scale, real-world data often exhibit inherently non-Euclidean structures, such as multi-way relationships, hierarchies, symmetries, and non-isotropic scaling, in a variety of domains, such as languages, vision, and the natural sciences. It is challenging to effectively capture these structures within the constraints of Euclidean spaces. This position paper argues that moving beyond Euclidean geometry is not merely an optional enhancement but a necessity to maintain the scaling law for the next-generation of foundation models. By adopting these geometries, foundation models could more efficiently leverage the aforementioned structures. Task-aware adaptability that dynamically reconfigures embeddings to match the geometry of downstream applications could further enhance efficiency and expressivity. Our position is supported by a series of theoretical and empirical investigations of prevalent foundation this http URL, we outline a roadmap for integrating non-Euclidean geometries into foundation models, including strategies for building geometric foundation models via fine-tuning, training from scratch, and hybrid approaches.', 'abstract_zh': '在基础模型和大规模语言模型的时代超越欧几里得几何：非欧几里得几何在下一代基础模型中的必要性', 'title_zh': '位置：超越欧几里得——基础模型应采纳非欧几里得几何'}
{'arxiv_id': 'arXiv:2504.08872', 'title': 'Personalizing Federated Learning for Hierarchical Edge Networks with Non-IID Data', 'authors': 'Seunghyun Lee, Omid Tavallaie, Shuaijun Chen, Kanchana Thilakarathna, Suranga Seneviratne, Adel Nadjaran Toosi, Albert Y. Zomaya', 'link': 'https://arxiv.org/abs/2504.08872', 'abstract': 'Accommodating edge networks between IoT devices and the cloud server in Hierarchical Federated Learning (HFL) enhances communication efficiency without compromising data privacy. However, devices connected to the same edge often share geographic or contextual similarities, leading to varying edge-level data heterogeneity with different subsets of labels per edge, on top of device-level heterogeneity. This hierarchical non-Independent and Identically Distributed (non-IID) nature, which implies that each edge has its own optimization goal, has been overlooked in HFL research. Therefore, existing edge-accommodated HFL demonstrates inconsistent performance across edges in various hierarchical non-IID scenarios. To ensure robust performance with diverse edge-level non-IID data, we propose a Personalized Hierarchical Edge-enabled Federated Learning (PHE-FL), which personalizes each edge model to perform well on the unique class distributions specific to each edge. We evaluated PHE-FL across 4 scenarios with varying levels of edge-level non-IIDness, with extreme IoT device level non-IIDness. To accurately assess the effectiveness of our personalization approach, we deployed test sets on each edge server instead of the cloud server, and used both balanced and imbalanced test sets. Extensive experiments show that PHE-FL achieves up to 83 percent higher accuracy compared to existing federated learning approaches that incorporate edge networks, given the same number of training rounds. Moreover, PHE-FL exhibits improved stability, as evidenced by reduced accuracy fluctuations relative to the state-of-the-art FedAvg with two-level (edge and cloud) aggregation.', 'abstract_zh': '多层次边缘增强的个性化联邦学习（PHE-FL）：提高边缘非IID数据下的通信效率和数据隐私保护', 'title_zh': '基于非IID数据的层次边缘网络个性化联邦学习'}
{'arxiv_id': 'arXiv:2504.08866', 'title': 'On Transfer-based Universal Attacks in Pure Black-box Setting', 'authors': 'Mohammad A.A.K. Jalwana, Naveed Akhtar, Ajmal Mian, Nazanin Rahnavard, Mubarak Shah', 'link': 'https://arxiv.org/abs/2504.08866', 'abstract': 'Despite their impressive performance, deep visual models are susceptible to transferable black-box adversarial attacks. Principally, these attacks craft perturbations in a target model-agnostic manner. However, surprisingly, we find that existing methods in this domain inadvertently take help from various priors that violate the black-box assumption such as the availability of the dataset used to train the target model, and the knowledge of the number of classes in the target model. Consequently, the literature fails to articulate the true potency of transferable black-box attacks. We provide an empirical study of these biases and propose a framework that aids in a prior-free transparent study of this paradigm. Using our framework, we analyze the role of prior knowledge of the target model data and number of classes in attack performance. We also provide several interesting insights based on our analysis, and demonstrate that priors cause overestimation in transferability scores. Finally, we extend our framework to query-based attacks. This extension inspires a novel image-blending technique to prepare data for effective surrogate model training.', 'abstract_zh': '尽管深度视觉模型表现 impressive，但它们易受转移性黑盒对抗攻击的影响。现有方法在这一领域意外地依赖了违反黑盒假设的各种先验，如目标模型训练数据集的可用性和目标模型类别的数量。因此，现有文献未能充分阐述转移性黑盒攻击的真实效能。我们提供了一种实验研究这些偏见的方法，并提出了一种框架，以帮助在无需先验知识的情况下进行透明的研究。利用该框架，我们分析了目标模型数据和类别数量的先验知识如何影响攻击性能，并根据分析提供了若干有趣的见解，证明先验会导致转移性得分的高估。最后，我们将该框架扩展到查询式攻击。这一扩展激发了一种新颖的图像融合技术，用于有效训练替代模型。', 'title_zh': '基于转移的通用攻击在纯黑盒设置中'}
{'arxiv_id': 'arXiv:2504.08861', 'title': 'Diachronic and synchronic variation in the performance of adaptive machine learning systems: The ethical challenges', 'authors': 'Joshua Hatherley, Robert Sparrow', 'link': 'https://arxiv.org/abs/2504.08861', 'abstract': 'Objectives: Machine learning (ML) has the potential to facilitate "continual learning" in medicine, in which an ML system continues to evolve in response to exposure to new data over time, even after being deployed in a clinical setting. In this paper, we provide a tutorial on the range of ethical issues raised by the use of such "adaptive" ML systems in medicine that have, thus far, been neglected in the literature.\nTarget audience: The target audiences for this tutorial are the developers of machine learning AI systems, healthcare regulators, the broader medical informatics community, and practicing clinicians.\nScope: Discussions of adaptive ML systems to date have overlooked the distinction between two sorts of variance that such systems may exhibit -- diachronic evolution (change over time) and synchronic variation (difference between cotemporaneous instantiations of the algorithm at different sites) -- and under-estimated the significance of the latter. We highlight the challenges that diachronic evolution and synchronic variation present for the quality of patient care, informed consent, and equity, and discuss the complex ethical trade-offs involved in the design of such systems.', 'abstract_zh': '目标：机器学习（ML）有潜力促进医学中的“持续学习”，即在临床应用后，ML系统能持续进化以响应新数据的暴露。本文提供了一篇关于此类“自适应”ML系统在医学中使用所引发的一系列伦理问题的教程，目前这些伦理问题在文献中尚未得到充分关注。\n\n目标受众：本文的目标受众包括机器学习AI系统的开发者、医疗监管机构、更广泛的医学信息学社区以及临床实践者。\n\n范围：迄今为止关于自适应ML系统的讨论未能区分这类系统可能出现的两类变异——历时演变（随时间变化）和共时变异（不同地点同一时间点算法实例之间的差异）——并且低估了后者的意义。本文强调历时演变和共时变异对患者护理质量、知情同意和公平性带来的挑战，并讨论了设计这类系统时复杂的伦理权衡。', 'title_zh': '适应性机器学习系统历时与共时表现的变化：伦理挑战'}
{'arxiv_id': 'arXiv:2504.08860', 'title': 'A Nonlinear Hash-based Optimization Method for SpMV on GPUs', 'authors': 'Chen Yan, Boyu Diao, Hangda Liu, Zhulin An, Yongjun Xu', 'link': 'https://arxiv.org/abs/2504.08860', 'abstract': "Sparse matrix-vector multiplication (SpMV) is a fundamental operation with a wide range of applications in scientific computing and artificial intelligence. However, the large scale and sparsity of sparse matrix often make it a performance bottleneck. In this paper, we highlight the effectiveness of hash-based techniques in optimizing sparse matrix reordering, introducing the Hash-based Partition (HBP) format, a lightweight SpMV approach. HBP retains the performance benefits of the 2D-partitioning method while leveraging the hash transformation's ability to group similar elements, thereby accelerating the pre-processing phase of sparse matrix reordering. Additionally, we achieve parallel load balancing across matrix blocks through a competitive method. Our experiments, conducted on both Nvidia Jetson AGX Orin and Nvidia RTX 4090, show that in the pre-processing step, our method offers an average speedup of 3.53 times compared to the sorting approach and 3.67 times compared to the dynamic programming method employed in Regu2D. Furthermore, in SpMV, our method achieves a maximum speedup of 3.32 times on Orin and 3.01 times on RTX4090 against the CSR format in sparse matrices from the University of Florida Sparse Matrix Collection.", 'abstract_zh': '基于哈希的稀疏矩阵重新排序技术在稀疏矩阵向量乘法中的应用优化', 'title_zh': '基于非线性哈希的SpMV在GPU上的优化方法'}
{'arxiv_id': 'arXiv:2504.08859', 'title': 'PolyConf: Unlocking Polymer Conformation Generation through Hierarchical Generative Models', 'authors': 'Fanmeng Wang, Wentao Guo, Qi Ou, Hongshuai Wang, Haitao Lin, Hongteng Xu, Zhifeng Gao', 'link': 'https://arxiv.org/abs/2504.08859', 'abstract': "Polymer conformation generation is a critical task that enables atomic-level studies of diverse polymer materials. While significant advances have been made in designing various conformation generation methods for small molecules and proteins, these methods struggle to generate polymer conformations due to polymers' unique structural characteristics. The scarcity of polymer conformation datasets further limits progress, making this promising area largely unexplored. In this work, we propose PolyConf, a pioneering tailored polymer conformation generation method that leverages hierarchical generative models to unlock new possibilities for this task. Specifically, we decompose the polymer conformation into a series of local conformations (i.e., the conformations of its repeating units), generating these local conformations through an autoregressive model. We then generate corresponding orientation transformations via a diffusion model to assemble these local conformations into the complete polymer conformation. Moreover, we develop the first benchmark with a high-quality polymer conformation dataset derived from molecular dynamics simulations to boost related research in this area. The comprehensive evaluation demonstrates that PolyConf consistently generates high-quality polymer conformations, facilitating advancements in polymer modeling and simulation.", 'abstract_zh': '聚合物构象生成是实现多样化聚合物材料原子级研究的关键任务。虽然在设计适用于小分子和蛋白质的各种构象生成方法方面取得了显著进展，但这些方法在生成聚合物构象时遇到困难，因为聚合物具有独特的结构性质。聚合物构象数据集的稀缺性进一步限制了进展，使得这一有前景的领域尚未得到充分探索。在本文中，我们提出了PolyConf，这是一种创新的定制化聚合物构象生成方法，利用层级生成模型为该任务解锁新可能。具体来说，我们将聚合物构象分解为一系列局部构象（即其重复单元的构象），并通过自回归模型生成这些局部构象。然后，我们通过扩散模型生成相应的方向变换来将这些局部构象组装成完整的聚合物构象。此外，我们还开发了首个以分子动力学模拟数据为基础的高质量聚合物构象基准数据集，以促进该领域的相关研究。全面的评估表明，PolyConf 一贯可以生成高质量的聚合物构象，从而促进聚合物建模和模拟的进步。', 'title_zh': 'PolyConf：通过层次生成模型解锁聚合物构象生成'}
{'arxiv_id': 'arXiv:2504.08855', 'title': 'Exponential Shift: Humans Adapt to AI Economies', 'authors': 'Kevin J McNamara, Rhea Pritham Marpu', 'link': 'https://arxiv.org/abs/2504.08855', 'abstract': 'This paper explores how artificial intelligence (AI) and robotics are transforming the global labor market. Human workers, limited to a 33% duty cycle due to rest and holidays, cost $14 to $55 per hour. In contrast, digital labor operates nearly 24/7 at just $0.10 to $0.50 per hour. We examine sectors like healthcare, education, manufacturing, and retail, finding that 40-70% of tasks could be automated. Yet, human skills like emotional intelligence and adaptability remain essential. Humans process 5,000-20,000 tokens (units of information) per hour, while AI far exceeds this, though its energy use-3.5 to 7 times higher than humans-could offset 20-40% of cost savings. Using real-world examples, such as AI in journalism and law, we illustrate these dynamics and propose six strategies-like a 4-day workweek and retraining-to ensure a fair transition to an AI-driven economy.', 'abstract_zh': '人工智能与机器人技术如何重塑全球劳动力市场：确保向以AI驱动的经济过渡的策略', 'title_zh': '指数变换：人类适应AI经济'}
{'arxiv_id': 'arXiv:2504.08853', 'title': 'Artificial Intelligence (AI) and the Relationship between Agency, Autonomy, and Moral Patiency', 'authors': 'Paul Formosa, Inês Hipólito, Thomas Montefiore', 'link': 'https://arxiv.org/abs/2504.08853', 'abstract': 'The proliferation of Artificial Intelligence (AI) systems exhibiting complex and seemingly agentive behaviours necessitates a critical philosophical examination of their agency, autonomy, and moral status. In this paper we undertake a systematic analysis of the differences between basic, autonomous, and moral agency in artificial systems. We argue that while current AI systems are highly sophisticated, they lack genuine agency and autonomy because: they operate within rigid boundaries of pre-programmed objectives rather than exhibiting true goal-directed behaviour within their environment; they cannot authentically shape their engagement with the world; and they lack the critical self-reflection and autonomy competencies required for full autonomy. Nonetheless, we do not rule out the possibility of future systems that could achieve a limited form of artificial moral agency without consciousness through hybrid approaches to ethical decision-making. This leads us to suggest, by appealing to the necessity of consciousness for moral patiency, that such non-conscious AMAs might represent a case that challenges traditional assumptions about the necessary connection between moral agency and moral patiency.', 'abstract_zh': '人工智能系统表现出复杂且似乎具有自主行为的普及 necessitates 对其自主性、自治性和道德地位进行批判性的哲学审视。在本文中，我们对人工系统中基本、自主和道德自主性的差异进行了系统的分析。我们提出，尽管当前的人工智能系统极为先进，但它们缺乏真正的自主性和自主性，因为：它们在预先编程的目标范围内运作，而不是在环境中体现出真正的目标导向行为；它们不能真正塑造与世界的互动；并且它们缺乏实现全面自主所必需的批判性自我反思和自主性能力。然而，我们并不排除未来能够通过伦理决策的混合方法实现有限形式的非意识道德自主性的系统可能性。这促使我们通过主张意识对于道德受动性的必要性，提出这样的无意识AMA可能构成一个挑战传统关于道德自主性和道德受动性之间必要联系假设的案例。', 'title_zh': '人工智能（AI）与代理、自主与道德责任之间的关系'}
{'arxiv_id': 'arXiv:2504.08849', 'title': 'Exploring Cognitive Attributes in Financial Decision-Making', 'authors': 'Mallika Mainali, Rosina O. Weber', 'link': 'https://arxiv.org/abs/2504.08849', 'abstract': 'Cognitive attributes are fundamental to metacognition, shaping how individuals process information, evaluate choices, and make decisions. To develop metacognitive artificial intelligence (AI) models that reflect human reasoning, it is essential to account for the attributes that influence reasoning patterns and decision-maker behavior, often leading to different or even conflicting choices. This makes it crucial to incorporate cognitive attributes in designing AI models that align with human decision-making processes, especially in high-stakes domains such as finance, where decisions have significant real-world consequences. However, existing AI alignment research has primarily focused on value alignment, often overlooking the role of individual cognitive attributes that distinguish decision-makers. To address this issue, this paper (1) analyzes the literature on cognitive attributes, (2) establishes five criteria for defining them, and (3) categorizes 19 domain-specific cognitive attributes relevant to financial decision-making. These three components provide a strong basis for developing AI systems that accurately reflect and align with human decision-making processes in financial contexts.', 'abstract_zh': '认知属性是元认知的基础，影响个体处理信息、评估选择和做出决策的方式。为了开发反映人类推理的元认知人工智能（AI）模型，必须考虑那些影响推理模式和决策者行为的认知属性，这些属性可能导致不同的甚至冲突的决策。因此，在设计与人类决策过程相一致的AI模型时，尤其是在金融等高风险领域，需要特别注意这些认知属性。然而，现有的AI对齐研究主要集中在价值对齐上，往往忽视了区分决策者的个体认知属性的作用。为了应对这一问题，本文（1）分析了认知属性的相关文献，（2）制定了定义这些属性的五项标准，并（3）分类整理了与金融决策相关的19个领域特定的认知属性。这三项内容为开发能够在金融环境中准确反映和对齐人类决策过程的AI系统提供了坚实的基础。', 'title_zh': '探索财务管理中的认知特征'}
{'arxiv_id': 'arXiv:2504.08844', 'title': 'Artificial Intelligence Augmented Medical Imaging Reconstruction in Radiation Therapy', 'authors': 'Di Xu', 'link': 'https://arxiv.org/abs/2504.08844', 'abstract': 'Efficiently acquired and precisely reconstructed imaging are crucial to the success of modern radiation therapy (RT). Computed tomography (CT) and magnetic resonance imaging (MRI) are two common modalities for providing RT treatment planning and delivery guidance/monitoring. In recent decades, artificial intelligence (AI) has emerged as a powerful and widely adopted technique across various fields, valued for its efficiency and convenience enabled by implicit function definition and data-driven feature representation learning. Here, we present a series of AI-driven medical imaging reconstruction frameworks for enhanced radiotherapy, designed to improve CT image reconstruction quality and speed, refine dual-energy CT (DECT) multi-material decomposition (MMD), and significantly accelerate 4D MRI acquisition.', 'abstract_zh': '高效获取和精确重建影像对现代放射治疗的成功至关重要。计算机断层扫描（CT）和磁共振成像（MRI）是提供放疗治疗计划和执行指导/监控的两种常见成像模态。近年来，人工智能（AI）作为一种强大且广泛应用的技术，在各种领域中备受重视，因其通过隐函数定义和数据驱动特征表示学习而具备的高效性和便捷性。我们提出了一系列基于人工智能的医疗影像重建框架，旨在提高CT图像重建质量与速度、细化双能量CT（DECT）多材料分解（MMD）并显著加速4D MRI采集。', 'title_zh': '人工智能增强的放射治疗中医学影像重建'}
{'arxiv_id': 'arXiv:2504.08840', 'title': 'Adaptive Shrinkage Estimation For Personalized Deep Kernel Regression In Modeling Brain Trajectories', 'authors': 'Vasiliki Tassopoulou, Haochang Shou, Christos Davatzikos', 'link': 'https://arxiv.org/abs/2504.08840', 'abstract': "Longitudinal biomedical studies monitor individuals over time to capture dynamics in brain development, disease progression, and treatment effects. However, estimating trajectories of brain biomarkers is challenging due to biological variability, inconsistencies in measurement protocols (e.g., differences in MRI scanners), scarcity, and irregularity in longitudinal measurements. Herein, we introduce a novel personalized deep kernel regression framework for forecasting brain biomarkers, with application to regional volumetric measurements. Our approach integrates two key components: a population model that captures brain trajectories from a large and diverse cohort, and a subject-specific model that captures individual trajectories. To optimally combine these, we propose Adaptive Shrinkage Estimation, which effectively balances population and subject-specific models. We assess our model's performance through predictive accuracy metrics, uncertainty quantification, and validation against external clinical studies. Benchmarking against state-of-the-art statistical and machine learning models -- including linear mixed effects models, generalized additive models, and deep learning methods -- demonstrates the superior predictive performance of our approach. Additionally, we apply our method to predict trajectories of composite neuroimaging biomarkers, which highlights the versatility of our approach in modeling the progression of longitudinal neuroimaging biomarkers. Furthermore, validation on three external neuroimaging studies confirms the robustness of our method across different clinical contexts. We make the code available at this https URL.", 'abstract_zh': '纵向生物医学研究通过随时间监测个体来捕捉大脑发育、疾病进展和治疗效果的动力学。然而，由于生物学变异性、测量协议不一致（例如，MRI扫描器的差异）、纵向测量数据稀缺且不规则，估计大脑生物标志物轨迹具有挑战性。在此，我们提出了一种新颖的个性化深核回归框架，用于预测脑部生物标志物，应用于区域体积测量。我们的方法结合了两个关键组件：一个群体模型，用于捕捉大规模多样队列中的脑轨迹，以及一个个体模型，用于捕捉个体轨迹。为了最优地结合这两个模型，我们提出了自适应收缩估计，有效地平衡了群体和个体模型。我们通过预测准确度指标、不确定性量化以及与外部临床研究的验证来评估模型性能。与最新的统计和机器学习模型（包括线性混合效应模型、广义加性模型和深度学习方法）进行基准测试，证明了我们方法的优越预测性能。此外，我们将该方法应用于预测复合神经成像生物标志物的轨迹，突显了我们在建模纵向神经成像生物标志物进展方面的灵活性。进一步的外部神经成像研究验证了我们方法在不同临床背景下的稳健性。我们已在该网页地址提供了代码。', 'title_zh': '自适应收缩估计在建模脑轨迹中的个性化深度核回归'}
{'arxiv_id': 'arXiv:2504.08832', 'title': 'Generative AI in Collaborative Academic Report Writing: Advantages, Disadvantages, and Ethical Considerations', 'authors': 'Mahshid Sadeghpour, Arathi Arakala, Asha Rao', 'link': 'https://arxiv.org/abs/2504.08832', 'abstract': 'The availability and abundance of GenAI tools to administer tasks traditionally managed by people have raised concerns, particularly within the education and academic sectors, as some students may highly rely on these tools to complete the assignments designed to enable learning. This article focuses on informing students about the significance of investing their time during their studies on developing essential life-long learning skills using their own critical thinking, rather than depending on AI models that are susceptible to misinformation, hallucination, and bias. As we transition to an AI-centric era, it is important to educate students on how these models work, their pitfalls, and the ethical concerns associated with feeding data to such tools.', 'abstract_zh': 'GenAI工具在执行传统由人类管理的任务方面的可用性和丰富性引发了关切，尤其是在教育和学术领域，因为一些学生可能过度依赖这些工具来完成旨在促进学习的作业。本文旨在告知学生，在学习期间投资于通过自主批判性思维发展终身学习技能的重要性，而不是依赖于容易受 misinformation、hallucination 和偏见影响的AI模型。随着我们过渡到AI为中心的时代，教育学生了解这些模型的工作原理、局限性以及向此类工具输入数据所涉及的伦理问题至关重要。', 'title_zh': '生成式人工智能在协作学术报告写作中的优势、劣势与伦理考量'}
{'arxiv_id': 'arXiv:2504.08829', 'title': 'Datum-wise Transformer for Synthetic Tabular Data Detection in the Wild', 'authors': 'G. Charbel N. Kindji, Elisa Fromont, Lina Maria Rojas-Barahona, Tanguy Urvoy', 'link': 'https://arxiv.org/abs/2504.08829', 'abstract': "The growing power of generative models raises major concerns about the authenticity of published content. To address this problem, several synthetic content detection methods have been proposed for uniformly structured media such as image or text. However, little work has been done on the detection of synthetic tabular data, despite its importance in industry and government. This form of data is complex to handle due to the diversity of its structures: the number and types of the columns may vary wildly from one table to another. We tackle the tough problem of detecting synthetic tabular data ''in the wild'', i.e. when the model is deployed on table structures it has never seen before. We introduce a novel datum-wise transformer architecture and show that it outperforms existing models. Furthermore, we investigate the application of domain adaptation techniques to enhance the effectiveness of our model, thereby providing a more robust data-forgery detection solution.", 'abstract_zh': '生成模型能力的不断增长引起了关于发表内容真实性的重要关注。为了解决这一问题，已经为图像或文本等均匀结构的合成内容检测提出了多种方法。然而，在行业和政府中至关重要的合成表格数据的检测工作却较少。由于表格结构的多样性，这种数据形式处理起来非常复杂：一个表格中的列数和类型可能会与另一个表格相差很大。我们解决了在模型未见过的全新表格结构上检测合成表格数据这一棘手问题。我们提出了一种新颖的datum-wise变压器架构，并证明其优于现有模型。此外，我们探讨了域适应技术的应用，以增强我们模型的有效性，从而提供更稳健的数据伪造检测解决方案。', 'title_zh': '面向数据样本的Transformer合成表格数据检测方法'}
{'arxiv_id': 'arXiv:2504.08827', 'title': 'PatchTrAD: A Patch-Based Transformer focusing on Patch-Wise Reconstruction Error for Time Series Anomaly Detection', 'authors': 'Samy-Melwan Vilhes, Gilles Gasso, Mokhtar Z Alaya', 'link': 'https://arxiv.org/abs/2504.08827', 'abstract': 'Time series anomaly detection (TSAD) focuses on identifying whether observations in streaming data deviate significantly from normal patterns. With the prevalence of connected devices, anomaly detection on time series has become paramount, as it enables real-time monitoring and early detection of irregular behaviors across various application domains. In this work, we introduce PatchTrAD, a Patch-based Transformer model for time series anomaly detection. Our approach leverages a Transformer encoder along with the use of patches under a reconstructionbased framework for anomaly detection. Empirical evaluations on multiple benchmark datasets show that PatchTrAD is on par, in terms of detection performance, with state-of-the-art deep learning models for anomaly detection while being time efficient during inference.', 'abstract_zh': '基于 patch 的 Transformer 时间序列异常检测（PatchTrAD）', 'title_zh': 'PatchTrAD：一种基于_patch_的变换器，侧重于序列表征误差的时间序列异常检测'}
{'arxiv_id': 'arXiv:2504.08823', 'title': 'FM-LoRA: Factorized Low-Rank Meta-Prompting for Continual Learning', 'authors': 'Xiaobing Yu, Jin Yang, Xiao Wu, Peijie Qiu, Xiaofeng Liu', 'link': 'https://arxiv.org/abs/2504.08823', 'abstract': 'How to adapt a pre-trained model continuously for sequential tasks with different prediction class labels and domains and finally learn a generalizable model across diverse tasks is a long-lasting challenge. Continual learning (CL) has emerged as a promising approach to leverage pre-trained models (e.g., Transformers) for sequential tasks. While many existing CL methods incrementally store additional learned structures, such as Low-Rank Adaptation (LoRA) adapters or prompts and sometimes even preserve features from previous samples to maintain performance. This leads to unsustainable parameter growth and escalating storage costs as the number of tasks increases. Moreover, current approaches often lack task similarity awareness, which further hinders the models ability to effectively adapt to new tasks without interfering with previously acquired knowledge. To address these challenges, we propose FM-LoRA, a novel and efficient low-rank adaptation method that integrates both a dynamic rank selector (DRS) and dynamic meta-prompting (DMP). This framework allocates model capacity more effectively across tasks by leveraging a shared low-rank subspace critical for preserving knowledge, thereby avoiding continual parameter expansion. Extensive experiments on various CL benchmarks, including ImageNet-R, CIFAR100, and CUB200 for class-incremental learning (CIL), and DomainNet for domain-incremental learning (DIL), with Transformers backbone demonstrate that FM-LoRA effectively mitigates catastrophic forgetting while delivering robust performance across a diverse range of tasks and domains.', 'abstract_zh': '如何持续适应不同预测类别标签和领域的一系列任务，并最终学习适用于多种任务的一般化模型，一直是长期挑战。连续学习（CL）作为一种利用预训练模型（如变换器）进行系列任务的方法而崭露头角。尽管现有许多CL方法通过存储额外的学习结构（如LoRA适配器或提示）来逐步增加模型参数，甚至有时还会保留先前样本的特征以维持性能，但这导致了参数量的不可持续增长和存储成本的急剧上升，尤其是在任务数量增加时。此外，当前的方法往往缺乏对任务相似性的感知，这进一步阻碍了模型有效地适应新任务而不干扰之前获得的知识。为了解决这些挑战，我们提出了FM-LoRA，一种新颖且高效的低秩适应方法，结合了动态秩选择器（DRS）和动态元提示（DMP）。该框架通过利用对保留知识至关重要的共享低秩子空间，更有效地分配模型容量，从而避免了持续的参数扩展。在包括具有类增量学习（CIL）的ImageNet-R、CIFAR100和CUB200，以及具有领域增量学习（DIL）的DomainNet在内的各种CL基准测试上，以变换器作为骨干网络的广泛实验表明，FM-LoRA有效地缓解了灾难性遗忘现象，同时在多种任务和领域上提供了稳健的性能。', 'title_zh': 'FM-LoRA: 因子分解低秩元提示在连续学习中的应用'}
{'arxiv_id': 'arXiv:2504.08817', 'title': 'Exploring utilization of generative AI for research and education in data-driven materials science', 'authors': 'Takahiro Misawa, Ai Koizumi, Ryo Tamura, Kazuyoshi Yoshimi', 'link': 'https://arxiv.org/abs/2504.08817', 'abstract': 'Generative AI has recently had a profound impact on various fields, including daily life, research, and education. To explore its efficient utilization in data-driven materials science, we organized a hackathon -- AIMHack2024 -- in July 2024. In this hackathon, researchers from fields such as materials science, information science, bioinformatics, and condensed matter physics worked together to explore how generative AI can facilitate research and education. Based on the results of the hackathon, this paper presents topics related to (1) conducting AI-assisted software trials, (2) building AI tutors for software, and (3) developing GUI applications for software. While generative AI continues to evolve rapidly, this paper provides an early record of its application in data-driven materials science and highlights strategies for integrating AI into research and education.', 'abstract_zh': '生成式AI近年来在日常生活、研究和教育等领域产生了深远影响。为了探索其在数据驱动材料科学中的高效利用，我们在2024年7月组织了一场黑客松——AIMHack2024。在此次黑客松中，来自材料科学、信息科学、生物信息学和凝聚态物理学的研究人员共同努力，探讨了生成式AI如何促进研究和教育。基于黑客松的成果，本文呈现了以下几个方面的内容：(1) AI辅助软件试验，(2) 构建软件AI导师，(3) 开发软件GUI应用。虽然生成式AI仍在迅速发展，但本文提供了其在数据驱动材料科学中早期应用的记录，并强调了将AI整合到研究和教育中的策略。', 'title_zh': '探索生成式AI在数据驱动材料科学中的应用与教育研究'}
{'arxiv_id': 'arXiv:2504.08810', 'title': 'PriM: Principle-Inspired Material Discovery through Multi-Agent Collaboration', 'authors': 'Zheyuan Lai, Yingming Pu', 'link': 'https://arxiv.org/abs/2504.08810', 'abstract': 'Complex chemical space and limited knowledge scope with biases holds immense challenge for human scientists, yet in automated materials discovery. Existing intelligent methods relies more on numerical computation, leading to inefficient exploration and results with hard-interpretability. To bridge this gap, we introduce a principles-guided material discovery system powered by language inferential multi-agent system (MAS), namely PriM. Our framework integrates automated hypothesis generation with experimental validation in a roundtable system of MAS, enabling systematic exploration while maintaining scientific rigor. Based on our framework, the case study of nano helix demonstrates higher materials exploration rate and property value while providing transparent reasoning pathways. This approach develops an automated-and-transparent paradigm for material discovery, with broad implications for rational design of functional materials. Code is publicly available at our \\href{this https URL}{GitHub}.', 'abstract_zh': '复杂化学空间和有限的知识范围带有偏见，给人类材料科学家带来了巨大的挑战，但在自动材料发现中则有所不同。现有智能方法更多依赖数值计算，导致探索效率低下且结果难以解释。为解决这一问题，我们介绍了一种基于原理导向的材料发现系统PriM，该系统通过语言推断多智能体系统（MAS）来实现。我们的框架将自动假设生成与实验验证整合在一个MAS圆桌系统中，既保证了系统性探索又保持了科学严谨性。基于此框架，纳米螺旋结构的案例研究展示了更高的材料探索率和性能值，并提供了透明的推理路径。该方法开发了一种自动且透明的材料发现范式，对于功能材料的理性设计具有广泛的影响。代码已公开发布在我们的GitHub。', 'title_zh': 'PriM: 原理启发的多代理协作材料发现'}
{'arxiv_id': 'arXiv:2504.08793', 'title': 'A Constraint Programming Model For Serial Batch Scheduling With Minimum Batch Size', 'authors': 'Jorge A. Huertas, Pascal Van Hentenryck', 'link': 'https://arxiv.org/abs/2504.08793', 'abstract': 'In serial batch (s-batch) scheduling, jobs are grouped in batches and processed sequentially within their batch. This paper considers multiple parallel machines, nonidentical job weights and release times, and sequence-dependent setup times between batches of different families. Although s-batch has been widely studied in the literature, very few papers have taken into account a minimum batch size, typical in practical settings such as semiconductor manufacturing and the metal industry. The problem with this minimum batch size requirement has been mostly tackled with dynamic programming and meta-heuristics, and no article has ever used constraint programming (CP) to do so. This paper fills this gap by proposing, for the first time, a CP model for s-batching with minimum batch size. The computational experiments on standard cases compare the CP model with two existing mixed-integer programming (MIP) models from the literature. The results demonstrate the versatility of the proposed CP model to handle multiple variations of s-batching; and its ability to produce, in large instances, better solutions than the MIP models faster.', 'abstract_zh': '基于最小批次大小的串行批次调度的约束编程模型', 'title_zh': '一种最小批次大小约束编程模型的序列批次调度方法'}
{'arxiv_id': 'arXiv:2504.08782', 'title': 'Embedding Hidden Adversarial Capabilities in Pre-Trained Diffusion Models', 'authors': 'Lucas Beerens, Desmond J. Higham', 'link': 'https://arxiv.org/abs/2504.08782', 'abstract': 'We introduce a new attack paradigm that embeds hidden adversarial capabilities directly into diffusion models via fine-tuning, without altering their observable behavior or requiring modifications during inference. Unlike prior approaches that target specific images or adjust the generation process to produce adversarial outputs, our method integrates adversarial functionality into the model itself. The resulting tampered model generates high-quality images indistinguishable from those of the original, yet these images cause misclassification in downstream classifiers at a high rate. The misclassification can be targeted to specific output classes. Users can employ this compromised model unaware of its embedded adversarial nature, as it functions identically to a standard diffusion model. We demonstrate the effectiveness and stealthiness of our approach, uncovering a covert attack vector that raises new security concerns. These findings expose a risk arising from the use of externally-supplied models and highlight the urgent need for robust model verification and defense mechanisms against hidden threats in generative models. The code is available at this https URL .', 'abstract_zh': '我们提出了一种新的攻击范式，通过微调将隐藏的 adversarial 功能直接嵌入到扩散模型中，而不改变其可观察行为或在推理过程中需要进行修改。与之前针对特定图像或调整生成过程以产生 adversarial 输出的方法不同，我们的方法将 adversarial 功能集成到模型本身中。修改后的模型生成的高质量图像与原始图像无法区分，但这些图像在下游分类器中会导致高错误分类率。错误分类可以针对特定输出类。用户可以使用此受妥协的模型而不察觉其嵌入的 adversarial 性质，因为它与标准扩散模型功能相同。我们展示了该方法的有效性和隐蔽性，揭示了一种隐蔽的攻击向量，引发了新的安全问题。这些发现揭示了使用外部提供的模型所带来的风险，并强调了对抗生成模型中隐藏威胁的鲁棒模型验证和防御机制的迫切需求。代码见此链接： this https URL 。', 'title_zh': '在预训练扩散模型中嵌入隐藏对抗能力'}
{'arxiv_id': 'arXiv:2504.08769', 'title': 'High-order expansion of Neural Ordinary Differential Equations flows', 'authors': 'Dario Izzo, Sebastien Origer, Giacomo Acciarini, Francesco Biscani', 'link': 'https://arxiv.org/abs/2504.08769', 'abstract': 'Artificial neural networks, widely recognised for their role in machine learning, are now transforming the study of ordinary differential equations (ODEs), bridging data-driven modelling with classical dynamical systems and enabling the development of infinitely deep neural models. However, the practical applicability of these models remains constrained by the opacity of their learned dynamics, which operate as black-box systems with limited explainability, thereby hindering trust in their deployment. Existing approaches for the analysis of these dynamical systems are predominantly restricted to first-order gradient information due to computational constraints, thereby limiting the depth of achievable insight. Here, we introduce Event Transition Tensors, a framework based on high-order differentials that provides a rigorous mathematical description of neural ODE dynamics on event manifolds. We demonstrate its versatility across diverse applications: characterising uncertainties in a data-driven prey-predator control model, analysing neural optimal feedback dynamics, and mapping landing trajectories in a three-body neural Hamiltonian system. In all cases, our method enhances the interpretability and rigour of neural ODEs by expressing their behaviour through explicit mathematical structures. Our findings contribute to a deeper theoretical foundation for event-triggered neural differential equations and provide a mathematical construct for explaining complex system dynamics.', 'abstract_zh': '人工神经网络在机器学习中的广泛应用现在正 transforming 普通微分方程（ODEs）的研究，将基于数据的建模与经典动力系统相结合，促进了无限深神经模型的发展。然而，这些模型的实际应用仍然受到其学习动力学不透明性的限制，这些动力学作为黑盒系统运行，解释性有限，从而阻碍了对其部署的信任。现有基于动力系统分析的方法主要受限于一阶梯度信息，由于计算约束而无法提供更深层次的见解。为此，我们引入了基于高阶微分的事件过渡张量框架，该框架为神经ODE动力学在事件流形上的数学描述提供了严格的数学描述。我们展示了其在多种应用中的灵活性：在数据驱动的捕食者-猎物控制模型中表征不确定性、分析神经最优反馈动力学以及在三体神经哈密顿系统中映射着陆轨迹。在所有这些情况下，我们的方法通过显式的数学结构增强了神经ODE的可解释性和严谨性。我们的研究结果为事件触发神经微分方程提供了更深厚的理论基础，并提供了一个解释复杂系统动力学的数学构造。', 'title_zh': '高阶展开的神经普通微分方程流'}
{'arxiv_id': 'arXiv:2504.08757', 'title': 'A Framework for Lightweight Responsible Prompting Recommendation', 'authors': 'Tiago Machado, Sara E. Berger, Cassia Sanctos, Vagner Figueiredo de Santana, Lemara Williams, Zhaoqing Wu', 'link': 'https://arxiv.org/abs/2504.08757', 'abstract': 'Computer Science and Design practitioners have been researching and proposing alternatives for a dearth of recommendations, standards, or best practices in user interfaces for decades. Now, with the advent of generative Artificial Intelligence (GenAI), we have yet again an emerging, powerful technology that lacks sufficient guidance in terms of possible interactions, inputs, and outcomes. In this context, this work proposes a lightweight framework for responsible prompting recommendation to be added before the prompt is sent to GenAI. The framework is comprised of (1) a human-curated dataset for recommendations, (2) a red team dataset for assessing recommendations, (3) a sentence transformer for semantics mapping, (4) a similarity metric to map input prompt to recommendations, (5) a set of similarity thresholds, (6) quantized sentence embeddings, (7) a recommendation engine, and (8) an evaluation step to use the red team dataset. With the proposed framework and open-source system, the contributions presented can be applied in multiple contexts where end-users can benefit from guidance for interacting with GenAI in a more responsible way, recommending positive values to be added and harmful sentences to be removed.', 'abstract_zh': '计算机科学与设计实践者在几十年的时间里一直致力于研究并提出用户界面方面的替代方案，由于缺乏足够的建议、标准或最佳实践。随着生成式人工智能（GenAI）的兴起，当前又面临缺乏足够指导的问题，特别是在可能的交互、输入和输出方面。在此背景下，本文提出了一种轻量级的负责任提示推荐框架，在将提示发送给GenAI之前添加该框架。该框架包括：（1）由人类策划的数据集用于推荐；（2）由红队策划的数据集用于评估推荐；（3）一个句子变换器用于语义映射；（4）一个相似度度量用于将输入提示映射到推荐上；（5）一组相似度阈值；（6）量化句子嵌入；（7）一个推荐引擎；以及（8）一个评估步骤，用于使用红队数据集。通过提出的框架和开放源码系统，本文的贡献可以在多种场景中应用，使最终用户能够以更负责任的方式与GenAI互动，并推荐增加积极价值和移除有害语句。', 'title_zh': '轻量级负责任提示推荐框架'}
{'arxiv_id': 'arXiv:2504.08756', 'title': 'MHTS: Multi-Hop Tree Structure Framework for Generating Difficulty-Controllable QA Datasets for RAG Evaluation', 'authors': 'Jeongsoo Lee, Daeyong Kwon, Kyohoon Jin, Junnyeong Jeong, Minwoo Sim, Minwoo Kim', 'link': 'https://arxiv.org/abs/2504.08756', 'abstract': 'Existing RAG benchmarks often overlook query difficulty, leading to inflated performance on simpler questions and unreliable evaluations. A robust benchmark dataset must satisfy three key criteria: quality, diversity, and difficulty, which capturing the complexity of reasoning based on hops and the distribution of supporting evidence. In this paper, we propose MHTS (Multi-Hop Tree Structure), a novel dataset synthesis framework that systematically controls multi-hop reasoning complexity by leveraging a multi-hop tree structure to generate logically connected, multi-chunk queries. Our fine-grained difficulty estimation formula exhibits a strong correlation with the overall performance metrics of a RAG system, validating its effectiveness in assessing both retrieval and answer generation capabilities. By ensuring high-quality, diverse, and difficulty-controlled queries, our approach enhances RAG evaluation and benchmarking capabilities.', 'abstract_zh': '现有的RAG基准常常忽视查询难度，导致简单的查询性能被夸大并对评估结果可靠性产生影响。一个稳健的基准数据集必须满足三个关键标准：质量、多样性和难度，这些标准能够捕捉基于跳跃的推理复杂性以及支持证据的分布情况。在本文中，我们提出MHTS（多跳树结构），这是一种新颖的数据集合成框架，通过利用多跳树结构系统地控制多跳推理的复杂性来生成逻辑连贯的多块查询。我们精细的难度估计公式与RAG系统整体性能指标之间存在很强的关联性，验证了其在评估检索和答案生成能力方面的有效性。通过确保高质量、多样性和难度控制的查询，我们的方法提升了RAG的评估和基准测试能力。', 'title_zh': 'MHTS: 多跳树结构框架用于生成可控制难度的QA数据集以评估RAG系统'}
{'arxiv_id': 'arXiv:2504.08755', 'title': 'Delving into: the quantification of Ai-generated content on the internet (synthetic data)', 'authors': 'Dirk HR Spennemann', 'link': 'https://arxiv.org/abs/2504.08755', 'abstract': 'While it is increasingly evident that the internet is becoming saturated with content created by generated Ai large language models, accurately measuring the scale of this phenomenon has proven challenging. By analyzing the frequency of specific keywords commonly used by ChatGPT, this paper demonstrates that such linguistic markers can effectively be used to esti-mate the presence of generative AI content online. The findings suggest that at least 30% of text on active web pages originates from AI-generated sources, with the actual proportion likely ap-proaching 40%. Given the implications of autophagous loops, this is a sobering realization.', 'abstract_zh': '尽管互联网上由生成式AI大型语言模型创作的内容越来越 saturted，准确测量这一现象的规模仍然颇具挑战性。通过分析ChatGPT常用的具体关键词频率，本文证明这些语言标志可以有效用于估计在线生成式AI内容的数量。研究发现，至少30%的活跃网页文本源自AI生成的来源，实际比例可能接近40%。鉴于自噬循环的影响，这一发现令人警醒。', 'title_zh': '探究：互联网上AI生成内容的数量化（合成数据）'}
{'arxiv_id': 'arXiv:2504.08746', 'title': 'Enhancing Recommender Systems Using Textual Embeddings from Pre-trained Language Models', 'authors': 'Ngoc Luyen Le, Marie-Hélène Abel', 'link': 'https://arxiv.org/abs/2504.08746', 'abstract': 'Recent advancements in language models and pre-trained language models like BERT and RoBERTa have revolutionized natural language processing, enabling a deeper understanding of human-like language. In this paper, we explore enhancing recommender systems using textual embeddings from pre-trained language models to address the limitations of traditional recommender systems that rely solely on explicit features from users, items, and user-item interactions. By transforming structured data into natural language representations, we generate high-dimensional embeddings that capture deeper semantic relationships between users, items, and contexts. Our experiments demonstrate that this approach significantly improves recommendation accuracy and relevance, resulting in more personalized and context-aware recommendations. The findings underscore the potential of PLMs to enhance the effectiveness of recommender systems.', 'abstract_zh': 'recent advancements in 语言模型和预训练语言模型（如BERT和RoBERTa）极大地革新了自然语言处理，使得对类人的语言有了更深层次的理解。在本文中，我们探讨了利用预训练语言模型生成的文本嵌入来增强推荐系统，以解决传统推荐系统仅依赖用户、物品及其互动的显式特征所面临的限制。通过将结构化数据转化为自然语言表示，我们生成了高维嵌入，捕捉了用户、物品和上下文之间更深层的语义关系。我们的实验表明，这种方法显著提高了推荐的准确性和相关性，从而产生了更加个性化和上下文相关的推荐。这些发现强调了预训练语言模型（PLMs）在提升推荐系统效果方面的潜力。', 'title_zh': '使用预训练语言模型的文本嵌入增强推荐系统'}
{'arxiv_id': 'arXiv:2504.08738', 'title': 'AI-Driven Sentiment Analytics: Unlocking Business Value in the E-Commerce Landscape_v1', 'authors': 'Qianye Wu, Chengxuan Xia, Sixuan Tian', 'link': 'https://arxiv.org/abs/2504.08738', 'abstract': 'The rapid growth of e-commerce has led to an overwhelming volume of customer feedback, from product reviews to service interactions. Extracting meaningful insights from this data is crucial for businesses aiming to improve customer satisfaction and optimize decision-making. This paper presents an AI-driven sentiment analysis system designed specifically for e-commerce applications, balancing accuracy with interpretability. Our approach integrates traditional machine learning techniques with modern deep learning models, allowing for a more nuanced understanding of customer sentiment while ensuring transparency in decision-making. Experimental results show that our system outperforms standard sentiment analysis methods, achieving an accuracy of 89.7% on diverse, large-scale datasets. Beyond technical performance, real-world implementation across multiple e-commerce platforms demonstrates tangible improvements in customer engagement and operational efficiency. This study highlights both the potential and the challenges of applying AI to sentiment analysis in a commercial setting, offering insights into practical deployment strategies and areas for future refinement.', 'abstract_zh': '电子商务的迅猛增长产生了大量客户反馈，从产品评价到服务互动。从这些数据中提取有意义的洞察对于提高客户满意度和优化决策至关重要。本文提出了一种基于AI的情感分析系统，专为电子商务应用设计，兼顾准确性和可解释性。我们的方法结合了传统的机器学习技术和现代深度学习模型，实现了更细致的情感理解，同时保证了决策的透明度。实验结果表明，我们的系统在多样化的大型数据集上优于标准的情感分析方法，准确率达到89.7%。除了技术性能，跨多个电子商务平台的实际应用还证明了客户参与度和运营效率的实际提升。本文突出了将AI应用于商业环境中的情感分析的潜力与挑战，提供了实用部署策略和未来改进领域的一些见解。', 'title_zh': '基于AI驱动的情感分析：在电子商务领域解锁商业价值'}
{'arxiv_id': 'arXiv:2503.17374', 'title': 'Intanify AI Platform: Embedded AI for Automated IP Audit and Due Diligence', 'authors': 'Viktor Dorfler, Dylan Dryden, Viet Lee', 'link': 'https://arxiv.org/abs/2503.17374', 'abstract': 'In this paper we introduce a Platform created in order to support SMEs\' endeavor to extract value from their intangible assets effectively. To implement the Platform, we developed five knowledge bases using a knowledge-based ex-pert system shell that contain knowledge from intangible as-set consultants, patent attorneys and due diligence lawyers. In order to operationalize the knowledge bases, we developed a "Rosetta Stone", an interpreter unit for the knowledge bases outside the shell and embedded in the plat-form. Building on the initial knowledge bases we have created a system of red flags, risk scoring, and valuation with the involvement of the same experts; these additional systems work upon the initial knowledge bases and therefore they can be regarded as meta-knowledge-representations that take the form of second-order knowledge graphs. All this clever technology is dressed up in an easy-to-handle graphical user interface that we will showcase at the conference. The initial platform was finished mid-2024; therefore, it qualifies as an "emerging application of AI" and "deployable AI", while development continues. The two firms that provided experts for developing the knowledge bases obtained a white-label version of the product (i.e. it runs under their own brand "powered by Intanify"), and there are two completed cases.', 'abstract_zh': '本研究介绍了用于支持中小企业有效挖掘无形资产价值的平台。为了实施该平台，我们使用基于知识的专家系统外壳开发了五个知识库，这些知识库包含了无形资产咨询顾问、专利律师和尽职调查律师的知识。为了使这些知识库能够操作化，我们开发了一个“罗塞塔石碑”——一种知识库的解释单元，将其嵌入平台中。基于初始知识库，我们创建了一套红灯信号、风险评分和估值系统，并且这些额外系统基于初始知识库工作，因此可以被视为元知识表示，采取了二阶知识图的形式。所有这些聪明的技术都被封装在一个易于使用的图形用户界面中，我们将在会议上展示这一界面。初始平台于2024年中完成，因此它符合“新兴AI应用”和“可部署AI”的标准，而开发仍在继续。为开发知识库提供专家的两家公司获得了白标版本的产品（即，它在它们自己的品牌下运行，标示为“由Intanify提供技术支持”），并且目前已经完成了两个案例。', 'title_zh': 'Intanify AI平台：嵌入式AI自动IP审计与尽职调查'}
