# Generation of Real-time Robotic Emotional Expressions Learning from Human Demonstration in Mixed Reality 

**Title (ZH)**: 实时机器人情感表达学习在混合现实中的人类示范教学 

**Authors**: Chao Wang, Michael Gienger, Fan Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2508.08999)  

**Abstract**: Expressive behaviors in robots are critical for effectively conveying their emotional states during interactions with humans. In this work, we present a framework that autonomously generates realistic and diverse robotic emotional expressions based on expert human demonstrations captured in Mixed Reality (MR). Our system enables experts to teleoperate a virtual robot from a first-person perspective, capturing their facial expressions, head movements, and upper-body gestures, and mapping these behaviors onto corresponding robotic components including eyes, ears, neck, and arms. Leveraging a flow-matching-based generative process, our model learns to produce coherent and varied behaviors in real-time in response to moving objects, conditioned explicitly on given emotional states. A preliminary test validated the effectiveness of our approach for generating autonomous expressions. 

**Abstract (ZH)**: 机器人在与人类互动过程中有效传达情绪状态的表达行为至关重要。在本工作中，我们提出了一种基于混合现实（MR）中捕获的专家人类示范的框架，以自主生成真实且多样的机器人情感表达。该系统允许专家从第一人称视角远程操作虚拟机器人，捕获其面部表情、头部运动和上半身手势，并将这些行为映射到相应的机器人组件，包括眼睛、耳朵、颈部和手臂。利用基于流匹配的生成过程，我们的模型能够在实时中学习根据给定的情感状态生成一致且多样的行为。初步测试验证了该方法生成自主表达的有效性。 

---
# Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion 

**Title (ZH)**: 无监督技能发现作为探索学习敏捷移动的方式 

**Authors**: Seungeun Rho, Kartik Garg, Morgan Byrd, Sehoon Ha  

**Link**: [PDF](https://arxiv.org/pdf/2508.08982)  

**Abstract**: Exploration is crucial for enabling legged robots to learn agile locomotion behaviors that can overcome diverse obstacles. However, such exploration is inherently challenging, and we often rely on extensive reward engineering, expert demonstrations, or curriculum learning - all of which limit generalizability. In this work, we propose Skill Discovery as Exploration (SDAX), a novel learning framework that significantly reduces human engineering effort. SDAX leverages unsupervised skill discovery to autonomously acquire a diverse repertoire of skills for overcoming obstacles. To dynamically regulate the level of exploration during training, SDAX employs a bi-level optimization process that autonomously adjusts the degree of exploration. We demonstrate that SDAX enables quadrupedal robots to acquire highly agile behaviors including crawling, climbing, leaping, and executing complex maneuvers such as jumping off vertical walls. Finally, we deploy the learned policy on real hardware, validating its successful transfer to the real world. 

**Abstract (ZH)**: 探索对于使腿式机器人学习过障敏捷运动行为至关重要。然而，这样的探索本身具有挑战性，我们常常依赖广泛的奖励工程、专家演示或课程学习——所有这些都限制了泛化能力。在本工作中，我们提出了技能发现作为探索（SDAX），这是一个新颖的学习框架，显著减少了人类工程努力。SDAX 利用无监督技能发现自动生成克服障碍所需的各种技能。为了在训练期间动态调节探索水平，SDAX 采用了一种双层优化过程，自主调整探索程度。我们展示了 SDAX 使四足机器人获得包括爬行、攀爬、跳跃以及从垂直墙面跳下的复杂动作在内的高度敏捷行为。最后，我们在实际硬件上部署所学策略，验证其成功转移至现实世界。 

---
# Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors 

**Title (ZH)**: 面向 affordance 意识的类人先验驱动的灵巧抓取方法研究 

**Authors**: Haoyu Zhao, Linghao Zhuang, Xingyue Zhao, Cheng Zeng, Haoran Xu, Yuming Jiang, Jun Cen, Kexiang Wang, Jiayan Guo, Siteng Huang, Xin Li, Deli Zhao, Hua Zou  

**Link**: [PDF](https://arxiv.org/pdf/2508.08896)  

**Abstract**: A dexterous hand capable of generalizable grasping objects is fundamental for the development of general-purpose embodied AI. However, previous methods focus narrowly on low-level grasp stability metrics, neglecting affordance-aware positioning and human-like poses which are crucial for downstream manipulation. To address these limitations, we propose AffordDex, a novel framework with two-stage training that learns a universal grasping policy with an inherent understanding of both motion priors and object affordances. In the first stage, a trajectory imitator is pre-trained on a large corpus of human hand motions to instill a strong prior for natural movement. In the second stage, a residual module is trained to adapt these general human-like motions to specific object instances. This refinement is critically guided by two components: our Negative Affordance-aware Segmentation (NAA) module, which identifies functionally inappropriate contact regions, and a privileged teacher-student distillation process that ensures the final vision-based policy is highly successful. Extensive experiments demonstrate that AffordDex not only achieves universal dexterous grasping but also remains remarkably human-like in posture and functionally appropriate in contact location. As a result, AffordDex significantly outperforms state-of-the-art baselines across seen objects, unseen instances, and even entirely novel categories. 

**Abstract (ZH)**: 一种具备普适抓取能力的灵巧夹持器对于通用AGI的发展是基本的。之前的方法专注于低层次的抓取稳定性指标，忽略了知觉类适应性性和类人的姿态，为解决这些问题D我们我们提出了一种AffordD-DD框架D具备两阶段训练D能够学习到具有内在运动先先先和物体知知觉能力的抓取策略D第一阶段D轨迹模仿器预训练于大量基于人类手部运动的数据集以建立先先自然运动先D前D第二阶段D残差差生成训练DD适应于普D类类类人的运动到D特定DAD设定D这里的精细化由由步D由决定性的是两个模块D负D感知DD适应性分割D模块D识别功能不合适的区域DDD特权教师师生知识蒸馏过程D确保最终最终最终政策D高度成功D广泛的实验表明DDD不仅实现了普DD灵D的的抓取DDD姿态仍非常DD类人的D功能合适D因此DDAffordD-DD显著优于现有DD先进基准基线线素D对于未知D类类以前未见过类D和类全D类D分类 

---
# Boosting Action-Information via a Variational Bottleneck on Unlabelled Robot Videos 

**Title (ZH)**: 通过变分瓶颈在未标记机器人视频上增强动作-信息传输 

**Authors**: Haoyu Zhang, Long Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2508.08743)  

**Abstract**: Learning from demonstrations (LfD) typically relies on large amounts of action-labeled expert trajectories, which fundamentally constrains the scale of available training data. A promising alternative is to learn directly from unlabeled video demonstrations. However, we find that existing methods tend to encode latent actions that share little mutual information with the true robot actions, leading to suboptimal control performance. To address this limitation, we introduce a novel framework that explicitly maximizes the mutual information between latent actions and true actions, even in the absence of action labels. Our method leverage the variational information-bottleneck to extract action-relevant representations while discarding task-irrelevant information. We provide a theoretical analysis showing that our objective indeed maximizes the mutual information between latent and true actions. Finally, we validate our approach through extensive experiments: first in simulated robotic environments and then on real-world robotic platforms, the experimental results demonstrate that our method significantly enhances mutual information and consistently improves policy performance. 

**Abstract (ZH)**: 从演示学习（LfD）通常依赖大量带动作标签的专家轨迹，这从根本上限制了可用训练数据的规模。一个有前景的替代方案是从未标记的视频演示中直接学习。然而，我们发现现有方法往往会编码与真实机器人动作共享少量互信息的潜在动作，导致次优控制性能。为解决这一局限，我们提出了一种新的框架，该框架显式地最大化潜在动作与真实动作之间的互信息，即使在缺乏动作标签的情况下也是如此。我们的方法利用变分信息瓶颈来提取与动作相关的信息同时摒弃与任务无关的信息。我们提供理论分析表明，我们的目标确实最大化了潜在动作与真实动作之间的互信息。最后，通过广泛的实验验证了我们的方法：首先在模拟机器人环境中，然后在真实世界机器人平台上，实验结果表明我们的方法显著提升了互信息并一致地提高了策略性能。 

---
# OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing 

**Title (ZH)**: 全方位感官模型：与语义对齐的视觉-触觉-语言-行动模型 

**Authors**: Zhengxue Cheng, Yiqian Zhang, Wenkang Zhang, Haoyu Li, Keyu Wang, Li Song, Hengdi Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2508.08706)  

**Abstract**: Recent vision-language-action (VLA) models build upon vision-language foundations, and have achieved promising results and exhibit the possibility of task generalization in robot manipulation. However, due to the heterogeneity of tactile sensors and the difficulty of acquiring tactile data, current VLA models significantly overlook the importance of tactile perception and fail in contact-rich tasks. To address this issue, this paper proposes OmniVTLA, a novel architecture involving tactile sensing. Specifically, our contributions are threefold. First, our OmniVTLA features a dual-path tactile encoder framework. This framework enhances tactile perception across diverse vision-based and force-based tactile sensors by using a pretrained vision transformer (ViT) and a semantically-aligned tactile ViT (SA-ViT). Second, we introduce ObjTac, a comprehensive force-based tactile dataset capturing textual, visual, and tactile information for 56 objects across 10 categories. With 135K tri-modal samples, ObjTac supplements existing visuo-tactile datasets. Third, leveraging this dataset, we train a semantically-aligned tactile encoder to learn a unified tactile representation, serving as a better initialization for OmniVTLA. Real-world experiments demonstrate substantial improvements over state-of-the-art VLA baselines, achieving 96.9% success rates with grippers, (21.9% higher over baseline) and 100% success rates with dexterous hands (6.2% higher over baseline) in pick-and-place tasks. Besides, OmniVTLA significantly reduces task completion time and generates smoother trajectories through tactile sensing compared to existing VLA. 

**Abstract (ZH)**: Recent Vision-LANGUAGE-Action (VLA) Models Incorporate Tactile Sensing for Task Generalization in Robot Manipulation: A Novel Architecture Named OmniVTLA 

---
# Communication Efficient Robotic Mixed Reality with Gaussian Splatting Cross-Layer Optimization 

**Title (ZH)**: 通信高效的机器人混合现实：高斯点绘制跨层优化 

**Authors**: Chenxuan Liu, He Li, Zongze Li, Shuai Wang, Wei Xu, Kejiang Ye, Derrick Wing Kwan Ng, Chengzhong Xu  

**Link**: [PDF](https://arxiv.org/pdf/2508.08624)  

**Abstract**: Realizing low-cost communication in robotic mixed reality (RoboMR) systems presents a challenge, due to the necessity of uploading high-resolution images through wireless channels. This paper proposes Gaussian splatting (GS) RoboMR (GSMR), which enables the simulator to opportunistically render a photo-realistic view from the robot's pose by calling ``memory'' from a GS model, thus reducing the need for excessive image uploads. However, the GS model may involve discrepancies compared to the actual environments. To this end, a GS cross-layer optimization (GSCLO) framework is further proposed, which jointly optimizes content switching (i.e., deciding whether to upload image or not) and power allocation (i.e., adjusting to content profiles) across different frames by minimizing a newly derived GSMR loss function. The GSCLO problem is addressed by an accelerated penalty optimization (APO) algorithm that reduces computational complexity by over $10$x compared to traditional branch-and-bound and search algorithms. Moreover, variants of GSCLO are presented to achieve robust, low-power, and multi-robot GSMR. Extensive experiments demonstrate that the proposed GSMR paradigm and GSCLO method achieve significant improvements over existing benchmarks on both wheeled and legged robots in terms of diverse metrics in various scenarios. For the first time, it is found that RoboMR can be achieved with ultra-low communication costs, and mixture of data is useful for enhancing GS performance in dynamic scenarios. 

**Abstract (ZH)**: 低成本实现机器人混合现实（RoboMR）系统中的通信挑战：基于高斯点绘（GS）的RoboMR（GSMR）及跨层优化框架（GSCLO） 

---
# DeepFleet: Multi-Agent Foundation Models for Mobile Robots 

**Title (ZH)**: DeepFleet：移动机器人多智能体基础模型 

**Authors**: Ameya Agaskar, Sriram Siva, William Pickering, Kyle O'Brien, Charles Kekeh, Ang Li, Brianna Gallo Sarker, Alicia Chua, Mayur Nemade, Charun Thattai, Jiaming Di, Isaac Iyengar, Ramya Dharoor, Dino Kirouani, Jimmy Erskine, Tamir Hegazy, Scott Niekum, Usman A. Khan, Federico Pecora, Joseph W. Durham  

**Link**: [PDF](https://arxiv.org/pdf/2508.08574)  

**Abstract**: We introduce DeepFleet, a suite of foundation models designed to support coordination and planning for large-scale mobile robot fleets. These models are trained on fleet movement data, including robot positions, goals, and interactions, from hundreds of thousands of robots in Amazon warehouses worldwide. DeepFleet consists of four architectures that each embody a distinct inductive bias and collectively explore key points in the design space for multi-agent foundation models: the robot-centric (RC) model is an autoregressive decision transformer operating on neighborhoods of individual robots; the robot-floor (RF) model uses a transformer with cross-attention between robots and the warehouse floor; the image-floor (IF) model applies convolutional encoding to a multi-channel image representation of the full fleet; and the graph-floor (GF) model combines temporal attention with graph neural networks for spatial relationships. In this paper, we describe these models and present our evaluation of the impact of these design choices on prediction task performance. We find that the robot-centric and graph-floor models, which both use asynchronous robot state updates and incorporate the localized structure of robot interactions, show the most promise. We also present experiments that show that these two models can make effective use of larger warehouses operation datasets as the models are scaled up. 

**Abstract (ZH)**: DeepFleet：用于大规模移动机器人车队协调与规划的一套基础模型 

---
# AZRA: Extending the Affective Capabilities of Zoomorphic Robots using Augmented Reality 

**Title (ZH)**: AZRA：利用增强现实扩展拟人机器人的情感能力 

**Authors**: Shaun Macdonald, Salma ElSayed, Mark McGill  

**Link**: [PDF](https://arxiv.org/pdf/2508.08507)  

**Abstract**: Zoomorphic robots could serve as accessible and practical alternatives for users unable or unwilling to keep pets. However, their affective interactions are often simplistic and short-lived, limiting their potential for domestic adoption. In order to facilitate more dynamic and nuanced affective interactions and relationships between users and zoomorphic robots we present AZRA, a novel augmented reality (AR) framework that extends the affective capabilities of these robots without physical modifications. To demonstrate AZRA, we augment a zoomorphic robot, Petit Qoobo, with novel emotional displays (face, light, sound, thought bubbles) and interaction modalities (voice, touch, proximity, gaze). Additionally, AZRA features a computational model of emotion to calculate the robot's emotional responses, daily moods, evolving personality and needs. We highlight how AZRA can be used for rapid participatory prototyping and enhancing existing robots, then discuss implications on future zoomorphic robot development. 

**Abstract (ZH)**: 拟人化机器人可以为无法或不愿意养宠物的用户提供易用且实用的替代方案。然而，它们的情感交互往往过于简单且短暂，限制了其在家庭环境中的潜在应用。为了促进用户与拟人化机器人之间更富有动态性和层次性的情感交互和关系，我们提出了一种名为AZRA的新型增强现实（AR）框架，该框架可以通过软件扩展机器人的情感能力，而无需进行物理改造。为了展示AZRA，我们对拟人化机器人Petit Qoobo进行了增强，增加了新型的情感表现（面部、光线、声音、思维泡）和交互方式（语音、触觉、接近度、注视）。此外，AZRA还包含一个情感计算模型，用于计算机器人的情感反应、日常情绪、个性演变以及需求。我们强调AZRA在快速参与式原型制作和增强现有机器人方面的应用，然后讨论其对未来拟人化机器人发展的潜在影响。 

---
# A Minimal Model for Emergent Collective Behaviors in Autonomous Robotic Multi-Agent Systems 

**Title (ZH)**: 一种自主机器人多agent系统中 Emergent Collective Behaviors 的最小模型 

**Authors**: Hossein B. Jond  

**Link**: [PDF](https://arxiv.org/pdf/2508.08473)  

**Abstract**: Collective behaviors such as swarming and flocking emerge from simple, decentralized interactions in biological systems. Existing models, such as Vicsek and Cucker-Smale, lack collision avoidance, whereas the Olfati-Saber model imposes rigid formations, limiting their applicability in swarm robotics. To address these limitations, this paper proposes a minimal yet expressive model that governs agent dynamics using relative positions, velocities, and local density, modulated by two tunable parameters: the spatial offset and kinetic offset. The model achieves spatially flexible, collision-free behaviors that reflect naturalistic group dynamics. Furthermore, we extend the framework to cognitive autonomous systems, enabling energy-aware phase transitions between swarming and flocking through adaptive control parameter tuning. This cognitively inspired approach offers a robust foundation for real-world applications in multi-robot systems, particularly autonomous aerial swarms. 

**Abstract (ZH)**: 集体行为如集群和鸟群飞行源自生物系统中简单的分散交互。现有的模型如Vicsek和Cucker-Smale缺乏碰撞避免功能，而Olfati-Saber模型则限制了形成了刚性队形，限制了其在 swarm 机器人中的应用。为解决这些局限性，本文提出一个简洁但表达能力较强的模型，该模型利用相对位置、速度和局部密度来调控代理动态，并由两个可调参数：空间偏移和动能偏移来调制。该模型实现了空间灵活且无碰撞的行为，反映了自然群体动力学特征。此外，我们将该框架扩展到认知自主系统中，通过自适应控制参数调制实现集群和鸟群飞行之间的能效感知相变。这种受认知启发的方法为多机器人系统中的实际应用，特别是自主空中集群，提供了坚实的理论基础。 

---
# Whole-Body Coordination for Dynamic Object Grasping with Legged Manipulators 

**Title (ZH)**: 基于腿式 manipulator 的动态物体抓取的全身协调控制 

**Authors**: Qiwei Liang, Boyang Cai, Rongyi He, Hui Li, Tao Teng, Haihan Duan, Changxin Huang, Runhao Zeng  

**Link**: [PDF](https://arxiv.org/pdf/2508.08328)  

**Abstract**: Quadrupedal robots with manipulators offer strong mobility and adaptability for grasping in unstructured, dynamic environments through coordinated whole-body control. However, existing research has predominantly focused on static-object grasping, neglecting the challenges posed by dynamic targets and thus limiting applicability in dynamic scenarios such as logistics sorting and human-robot collaboration. To address this, we introduce DQ-Bench, a new benchmark that systematically evaluates dynamic grasping across varying object motions, velocities, heights, object types, and terrain complexities, along with comprehensive evaluation metrics. Building upon this benchmark, we propose DQ-Net, a compact teacher-student framework designed to infer grasp configurations from limited perceptual cues. During training, the teacher network leverages privileged information to holistically model both the static geometric properties and dynamic motion characteristics of the target, and integrates a grasp fusion module to deliver robust guidance for motion planning. Concurrently, we design a lightweight student network that performs dual-viewpoint temporal modeling using only the target mask, depth map, and proprioceptive state, enabling closed-loop action outputs without reliance on privileged data. Extensive experiments on DQ-Bench demonstrate that DQ-Net achieves robust dynamic objects grasping across multiple task settings, substantially outperforming baseline methods in both success rate and responsiveness. 

**Abstract (ZH)**: 四足 manipulator 机器人通过协调全身控制在未结构化和动态环境中提供了强大的移动性和适应性，以实现抓取。然而，现有研究主要集中在静态物体抓取上，忽视了动态目标所带来的挑战，从而限制了其在物流分拣和人机协作等动态场景中的应用。为解决这一问题，我们引入了 DQ-Bench，这是一个新的基准系统地评估不同物体运动、速度、高度、物体类型和地形复杂度下的动态抓取，并提供了全面的评估指标。基于这一基准，我们提出了一种紧凑的教师-学生框架 DQ-Net，用于从有限的感知线索中推断抓取配置。在训练过程中，教师网络利用专属信息整体建模目标的静态几何特性和动态运动特征，并集成了一个抓取融合模块，以提供稳健的运动规划指导。同时，我们设计了一个轻量级的学生网络，仅使用目标掩码、深度图和本体内省状态进行双视角时序建模，从而实现闭环动作输出，无需依赖专属数据。在 DQ-Bench 上进行的大量实验表明，DQ-Net 在多种任务设置下实现了稳健的动态物体抓取，其成功率和响应性显著优于Baseline方法。 

---
# Forecast-Driven MPC for Decentralized Multi-Robot Collision Avoidance 

**Title (ZH)**: 基于预测的分散多机器人碰撞 avoidance 控制 

**Authors**: Hadush Hailu, Bruk Gebregziabher, Prudhvi Raj  

**Link**: [PDF](https://arxiv.org/pdf/2508.08264)  

**Abstract**: The Iterative Forecast Planner (IFP) is a geometric planning approach that offers lightweight computations, scal- able, and reactive solutions for multi-robot path planning in decentralized, communication-free settings. However, it struggles in symmetric configurations, where mirrored interactions often lead to collisions and deadlocks. We introduce eIFP-MPC, an optimized and extended version of IFP that improves robustness and path consistency in dense, dynamic environments. The method refines threat prioritization using a time-to-collision heuristic, stabilizes path generation through cost-based via- point selection, and ensures dynamic feasibility by incorporating model predictive control (MPC) into the planning process. These enhancements are tightly integrated into the IFP to preserve its efficiency while improving its adaptability and stability. Ex- tensive simulations across symmetric and high-density scenarios show that eIFP-MPC significantly reduces oscillations, ensures collision-free motion, and improves trajectory efficiency. The results demonstrate that geometric planners can be strengthened through optimization, enabling robust performance at scale in complex multi-agent environments. 

**Abstract (ZH)**: 基于迭代预测规划者的增强多机器人路径规划方法(eIFP-MPC) 

---
# Koopman Operator Based Linear Model Predictive Control for Quadruped Trotting 

**Title (ZH)**: 基于科氏算子的线性模型预测控制四足足动捷步 

**Authors**: Chun-Ming Yang, Pranav A. Bhounsule  

**Link**: [PDF](https://arxiv.org/pdf/2508.08259)  

**Abstract**: Online optimal control of quadruped robots would enable them to adapt to varying inputs and changing conditions in real time. A common way of achieving this is linear model predictive control (LMPC), where a quadratic programming (QP) problem is formulated over a finite horizon with a quadratic cost and linear constraints obtained by linearizing the equations of motion and solved on the fly. However, the model linearization may lead to model inaccuracies. In this paper, we use the Koopman operator to create a linear model of the quadrupedal system in high dimensional space which preserves the nonlinearity of the equations of motion. Then using LMPC, we demonstrate high fidelity tracking and disturbance rejection on a quadrupedal robot. This is the first work that uses the Koopman operator theory for LMPC of quadrupedal locomotion. 

**Abstract (ZH)**: 基于科恩曼算子的四足机器人在线最优控制：高保真跟踪与扰动 rejection 

---
# Humanoid Robot Acrobatics Utilizing Complete Articulated Rigid Body Dynamics 

**Title (ZH)**: 利用完整刚体 articulated 动力学的人形机器人杂技技巧 

**Authors**: Gerald Brantner  

**Link**: [PDF](https://arxiv.org/pdf/2508.08258)  

**Abstract**: Endowing humanoid robots with the ability to perform highly dynamic motions akin to human-level acrobatics has been a long-standing challenge. Successfully performing these maneuvers requires close consideration of the underlying physics in both trajectory optimization for planning and control during execution. This is particularly challenging due to humanoids' high degree-of-freedom count and associated exponentially scaling complexities, which makes planning on the explicit equations of motion intractable. Typical workarounds include linearization methods and model approximations. However, neither are sufficient because they produce degraded performance on the true robotic system. This paper presents a control architecture comprising trajectory optimization and whole-body control, intermediated by a matching model abstraction, that enables the execution of acrobatic maneuvers, including constraint and posture behaviors, conditioned on the unabbreviated equations of motion of the articulated rigid body model. A review of underlying modeling and control methods is given, followed by implementation details including model abstraction, trajectory optimization and whole-body controller. The system's effectiveness is analyzed in simulation. 

**Abstract (ZH)**: 赋予类人机器人执行类似人类杂技的高动态动作的能力是一个长期存在的挑战。成功执行这些动作需要在轨迹优化计划和执行控制中密切考虑其背后的物理学。由于类人机器人具有很高的自由度和相应的指数级复杂性，使运动规划适用于显式的运动方程无法实现。常见的解决方法包括线性化方法和模型近似。然而，这两种方法都不足以在真正的机器人系统中产生满意的性能。本文提出了一种控制架构，包括轨迹优化和全身控制，并通过匹配模型抽象中介，能够在 articulated 刚体模型完整运动方程约束下执行杂技动作，包括约束和姿态行为。文中提供了基本建模和控制方法的回顾，以及模型抽象、轨迹优化和全身控制器的实现细节。系统的效果在仿真中进行了分析。 

---
# Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding 

**Title (ZH)**: 空间轨迹：通过空间-时间理解增强VLA模型 

**Authors**: Maxim A. Patratskiy, Alexey K. Kovalev, Aleksandr I. Panov  

**Link**: [PDF](https://arxiv.org/pdf/2508.09032)  

**Abstract**: Vision-Language-Action models have demonstrated remarkable capabilities in predicting agent movements within virtual environments and real-world scenarios based on visual observations and textual instructions. Although recent research has focused on enhancing spatial and temporal understanding independently, this paper presents a novel approach that integrates both aspects through visual prompting. We introduce a method that projects visual traces of key points from observations onto depth maps, enabling models to capture both spatial and temporal information simultaneously. The experiments in SimplerEnv show that the mean number of tasks successfully solved increased for 4% compared to SpatialVLA and 19% compared to TraceVLA. Furthermore, we show that this enhancement can be achieved with minimal training data, making it particularly valuable for real-world applications where data collection is challenging. The project page is available at this https URL. 

**Abstract (ZH)**: 视觉-语言-行动模型在基于视觉观察和文本指令预测智能体在虚拟环境和现实世界中动作方面展现了显著的能力。尽管近期的研究侧重于独立增强空间和时间理解，本文提出了一种通过视觉提示集成两者的新型方法。我们介绍了一种将观察中的关键点视觉轨迹投影到深度图上的方法，使模型能够同时捕捉空间和时间信息。在SimplerEnv的实验中，与SpatialVLA相比，成功解决的任务平均数量增加了4%；与TraceVLA相比，增加了19%。此外，我们展示了这种增强可以在最少的数据训练下实现，使其特别适用于数据收集具有挑战性的现实世界应用。项目页面可通过以下链接访问：this https URL。 

---
# DiffPhysCam: Differentiable Physics-Based Camera Simulation for Inverse Rendering and Embodied AI 

**Title (ZH)**: DiffPhysCam: 基于可微物理的相机仿真及其在逆向渲染和具身AI中的应用 

**Authors**: Bo-Hsun Chen, Nevindu M. Batagoda, Dan Negrut  

**Link**: [PDF](https://arxiv.org/pdf/2508.08831)  

**Abstract**: We introduce DiffPhysCam, a differentiable camera simulator designed to support robotics and embodied AI applications by enabling gradient-based optimization in visual perception pipelines. Generating synthetic images that closely mimic those from real cameras is essential for training visual models and enabling end-to-end visuomotor learning. Moreover, differentiable rendering allows inverse reconstruction of real-world scenes as digital twins, facilitating simulation-based robotics training. However, existing virtual cameras offer limited control over intrinsic settings, poorly capture optical artifacts, and lack tunable calibration parameters -- hindering sim-to-real transfer. DiffPhysCam addresses these limitations through a multi-stage pipeline that provides fine-grained control over camera settings, models key optical effects such as defocus blur, and supports calibration with real-world data. It enables both forward rendering for image synthesis and inverse rendering for 3D scene reconstruction, including mesh and material texture optimization. We show that DiffPhysCam enhances robotic perception performance in synthetic image tasks. As an illustrative example, we create a digital twin of a real-world scene using inverse rendering, simulate it in a multi-physics environment, and demonstrate navigation of an autonomous ground vehicle using images generated by DiffPhysCam. 

**Abstract (ZH)**: DiffPhysCam：一种用于支持机器人和体现AI应用的可微摄像头模拟器 

---
# OpenCUA: Open Foundations for Computer-Use Agents 

**Title (ZH)**: 开放CUA：计算机使用代理的基础框架 

**Authors**: Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Dikang Du, Hao Hu, Huarong Chen, Zaida Zhou, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Flood Sung, Y.Charles, Zhilin Yang, Tao Yu  

**Link**: [PDF](https://arxiv.org/pdf/2508.09123)  

**Abstract**: Vision-language models have demonstrated impressive capabilities as computer-use agents (CUAs) capable of automating diverse computer tasks. As their commercial potential grows, critical details of the most capable CUA systems remain closed. As these agents will increasingly mediate digital interactions and execute consequential decisions on our behalf, the research community needs access to open CUA frameworks to study their capabilities, limitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive open-source framework for scaling CUA data and foundation models. Our framework consists of: (1) an annotation infrastructure that seamlessly captures human computer-use demonstrations; (2) AgentNet, the first large-scale computer-use task dataset spanning 3 operating systems and 200+ applications and websites; (3) a scalable pipeline that transforms demonstrations into state-action pairs with reflective long Chain-of-Thought reasoning that sustain robust performance gains as data scales. Our end-to-end agent models demonstrate strong performance across CUA benchmarks. In particular, OpenCUA-32B achieves an average success rate of 34.8% on OSWorld-Verified, establishing a new state-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA (GPT-4o). Further analysis confirms that our approach generalizes well across domains and benefits significantly from increased test-time computation. We release our annotation tool, datasets, code, and models to build open foundations for further CUA research. 

**Abstract (ZH)**: Vision-language 模型展示了作为计算机使用代理（CUA）的强大能力，能够自动化各种计算机任务。随着其商业潜力的增长，最强大的 CUA 系统的关键细节仍然保密。由于这些代理将越来越多地调解数字交互并代表我们执行重要的决策，研究界需要访问开源的 CUA 框架以研究其能力和风险。为了解决这一差距，我们提出了 OpenCUA——一个全面的开源框架，用于扩展 CUA 数据和基础模型。我们的框架包括：（1）一种注释基础设施，无缝捕获人类计算机使用演示；（2）AgentNet，首款跨 3 个操作系统和 200 多个应用程序和网站的大规模计算机使用任务数据集；（3）一个可扩展的工作流，将演示转换为状态-动作对，并结合反思性的长链推理以支持数据扩增后的稳健性能提升。我们的端到端代理模型在 CUA 基准测试中表现出色。特别是，OpenCUA-32B 在 OSWorld-Verified 上取得了 34.8% 的平均成功率，成为开源模型中的新最佳表现，并超过了 OpenAI CUA（GPT-4o）。进一步的分析证实，我们的方法在不同领域中具有良好的泛化能力，并且从增加的测试时计算中受益显著。我们开源了我们的注释工具、数据集、代码和模型，以构建进一步 CUA 研究的基础。 

---
# Reducing Cognitive Load in Multi-Agent Reinforcement Learning for Mathematical Problem Solving: Decoupling Reasoning and Code Generation 

**Title (ZH)**: 多Agent强化学习中数学问题求解中的认知负荷降低：推理与代码生成解耦 

**Authors**: Dayu Wang, Jiaye Yang, Weikang Li, Jiahui Liang, Yang Li  

**Link**: [PDF](https://arxiv.org/pdf/2508.08882)  

**Abstract**: Current tool-integrated mathematical reasoning systems often adopt a single-agent paradigm, where one large language model handles problem reasoning, code generation, and code execution in an integrated workflow. While this design eases coordination, we hypothesize that it imposes cognitive load interference, as the agent must interleave long-horizon reasoning with precise program synthesis. We validate this hypothesis through a controlled comparison between a reasoning-only agent and a reasoning-plus-code agent, finding that the latter produces significantly fewer correct reasoning paths despite having tool-calling capabilities. To address this, we propose a dual-agent hybrid framework: a Reasoning Agent performs stepwise problem decomposition, and a Code Agent handles code generation and execution. Training combines imitation learning and reinforcement learning: the Code Agent receives strong rewards for matching intermediate ground-truth programs and weaker rewards for valid execution, while the Reasoning Agent is optimized chiefly via final-answer accuracy using advantage estimation to credit intermediate steps. This decoupled role design reduces cognitive interference and promotes stable reasoning-coding coordination. 

**Abstract (ZH)**: 当前集成工具的数学推理系统通常采用单代理范式，其中一个大规模语言模型在集成的工作流中处理问题推理、代码生成和代码执行。虽然这种设计简化了协调，但我们假设它会增加认知负荷干扰，因为代理必须交错进行长期推理与精确程序合成。我们通过对比仅推理代理和推理加编码代理来进行受控比较，发现后者尽管具有工具调用能力，但生成正确推理路径却显著较少。为解决这一问题，我们提出了一种双代理混合框架：推理代理进行逐步问题分解，编码代理处理代码生成和执行。训练结合了模仿学习和强化学习：编码代理因匹配中间真实程序而获得较强奖励，因有效执行而获得较弱奖励，而推理代理主要通过最终答案准确性的优化进行优化，并通过优势估计来奖励中间步骤。这种分离的角色设计减少了认知干扰，促进了稳定推理编码协调。 

---
# Efficient Agent: Optimizing Planning Capability for Multimodal Retrieval Augmented Generation 

**Title (ZH)**: 高效代理：多模态检索增强生成的规划能力优化 

**Authors**: Yuechen Wang, Yuming Qiao, Dan Meng, Jun Yang, Haonan Lu, Zhenyu Yang, Xudong Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2508.08816)  

**Abstract**: Multimodal Retrieval-Augmented Generation (mRAG) has emerged as a promising solution to address the temporal limitations of Multimodal Large Language Models (MLLMs) in real-world scenarios like news analysis and trending topics. However, existing approaches often suffer from rigid retrieval strategies and under-utilization of visual information. To bridge this gap, we propose E-Agent, an agent framework featuring two key innovations: a mRAG planner trained to dynamically orchestrate multimodal tools based on contextual reasoning, and a task executor employing tool-aware execution sequencing to implement optimized mRAG workflows. E-Agent adopts a one-time mRAG planning strategy that enables efficient information retrieval while minimizing redundant tool invocations. To rigorously assess the planning capabilities of mRAG systems, we introduce the Real-World mRAG Planning (RemPlan) benchmark. This novel benchmark contains both retrieval-dependent and retrieval-independent question types, systematically annotated with essential retrieval tools required for each instance. The benchmark's explicit mRAG planning annotations and diverse question design enhance its practical relevance by simulating real-world scenarios requiring dynamic mRAG decisions. Experiments across RemPlan and three established benchmarks demonstrate E-Agent's superiority: 13% accuracy gain over state-of-the-art mRAG methods while reducing redundant searches by 37%. 

**Abstract (ZH)**: 多模态检索增强生成（mRAG）已 emerges as a promising solution to address the temporal limitations of多模态大型语言模型（MLLMs）在如新闻分析和趋势话题等现实场景中的时间限制。然而，现有方法往往受制于僵化的检索策略和视觉信息的不足利用。为了解决这一问题，我们提出E-Agent，这是一种代理框架，具备两项关键创新：一个多模态工具规划器（mRAG planner），它根据上下文推理动态 orchestrate 多模态工具；以及一个工具感知执行序列的任务执行者（task executor），用于实现优化的 mRAG 工作流。E-Agent 采用一次性 mRAG 规划策略，从而实现高效的信息检索同时尽量减少冗余工具的调用。为了严格评估 mRAG 系统的规划能力，我们引入了 Real-World mRAG 规划 (RemPlan) 基准。这一新颖的基准包含了检索依赖性和检索独立性的问题类型，并为每个实例系统地标注了必要的检索工具。基准中的显式 mRAG 规划注解和多样化的问题设计使其更具实际意义，模拟了要求动态 mRAG 决策的现实场景。跨 RemPlan 和三个现有基准的实验证明了 E-Agent 的优越性：相比最先进的 mRAG 方法，准确率提高了 13%，同时减少了 37% 的冗余搜索。 

---
# Simulating Generative Social Agents via Theory-Informed Workflow Design 

**Title (ZH)**: 基于理论指导的工作流设计模拟生成性社会代理 

**Authors**: Yuwei Yan, Jinghua Piao, Xiaochong Lan, Chenyang Shao, Pan Hui, Yong Li  

**Link**: [PDF](https://arxiv.org/pdf/2508.08726)  

**Abstract**: Recent advances in large language models have demonstrated strong reasoning and role-playing capabilities, opening new opportunities for agent-based social simulations. However, most existing agents' implementations are scenario-tailored, without a unified framework to guide the design. This lack of a general social agent limits their ability to generalize across different social contexts and to produce consistent, realistic behaviors. To address this challenge, we propose a theory-informed framework that provides a systematic design process for LLM-based social agents. Our framework is grounded in principles from Social Cognition Theory and introduces three key modules: motivation, action planning, and learning. These modules jointly enable agents to reason about their goals, plan coherent actions, and adapt their behavior over time, leading to more flexible and contextually appropriate responses. Comprehensive experiments demonstrate that our theory-driven agents reproduce realistic human behavior patterns under complex conditions, achieving up to 75% lower deviation from real-world behavioral data across multiple fidelity metrics compared to classical generative baselines. Ablation studies further show that removing motivation, planning, or learning modules increases errors by 1.5 to 3.2 times, confirming their distinct and essential contributions to generating realistic and coherent social behaviors. 

**Abstract (ZH)**: 近期大规模语言模型的发展展示了强大的推理和角色扮演能力，为基于代理的社会模拟打开了新局面。然而，现有代理的实现大多针对特定场景，缺乏统一的设计框架。这限制了代理在不同社会情境下的泛化能力和产生一致、真实行为的能力。为应对这一挑战，我们提出了一种理论驱动框架，为基于大规模语言模型的社会代理提供系统化的设计流程。该框架基于社会认知理论的原则，引入了三大关键模块：动机、行动规划和学习。这些模块共同使代理能够推理其目标、规划连贯的行动，并随时间调整其行为，从而产生更具灵活性和上下文适宜性的响应。全面的实验表明，我们的理论驱动代理在复杂条件下能够重现真实的人类行为模式，相比经典生成基准，在多个保真度指标下平均降低75%的行为偏差。进一步的消融研究表明，去除动机、规划或学习模块会导致错误增加1.5到3.2倍，证实了它们对生成真实且连贯的社会行为的独特而必不可少的贡献。 

---
# Shape Completion and Real-Time Visualization in Robotic Ultrasound Spine Acquisitions 

**Title (ZH)**: 基于机器人超声脊柱检查中的形狀补全与实时可视化 

**Authors**: Miruna-Alexandra Gafencu, Reem Shaban, Yordanka Velikova, Mohammad Farid Azampour, Nassir Navab  

**Link**: [PDF](https://arxiv.org/pdf/2508.08923)  

**Abstract**: Ultrasound (US) imaging is increasingly used in spinal procedures due to its real-time, radiation-free capabilities; however, its effectiveness is hindered by shadowing artifacts that obscure deeper tissue structures. Traditional approaches, such as CT-to-US registration, incorporate anatomical information from preoperative CT scans to guide interventions, but they are limited by complex registration requirements, differences in spine curvature, and the need for recent CT imaging. Recent shape completion methods can offer an alternative by reconstructing spinal structures in US data, while being pretrained on large set of publicly available CT scans. However, these approaches are typically offline and have limited reproducibility. In this work, we introduce a novel integrated system that combines robotic ultrasound with real-time shape completion to enhance spinal visualization. Our robotic platform autonomously acquires US sweeps of the lumbar spine, extracts vertebral surfaces from ultrasound, and reconstructs the complete anatomy using a deep learning-based shape completion network. This framework provides interactive, real-time visualization with the capability to autonomously repeat scans and can enable navigation to target locations. This can contribute to better consistency, reproducibility, and understanding of the underlying anatomy. We validate our approach through quantitative experiments assessing shape completion accuracy and evaluations of multiple spine acquisition protocols on a phantom setup. Additionally, we present qualitative results of the visualization on a volunteer scan. 

**Abstract (ZH)**: 基于超声成像的脊柱可视化综合系统：结合机器人超声与实时形状完成技术 

---
# Opening Musical Creativity? Embedded Ideologies in Generative-AI Music Systems 

**Title (ZH)**: 开启音乐创造力？生成式AI音乐系统中嵌入的意识形态 

**Authors**: Liam Pram, Fabio Morreale  

**Link**: [PDF](https://arxiv.org/pdf/2508.08805)  

**Abstract**: AI systems for music generation are increasingly common and easy to use, granting people without any musical background the ability to create music. Because of this, generative-AI has been marketed and celebrated as a means of democratizing music making. However, inclusivity often functions as marketable rhetoric rather than a genuine guiding principle in these industry settings. In this paper, we look at four generative-AI music making systems available to the public as of mid-2025 (AIVA, Stable Audio, Suno, and Udio) and track how they are rhetoricized by their developers, and received by users. Our aim is to investigate ideologies that are driving the early-stage development and adoption of generative-AI in music making, with a particular focus on democratization. A combination of autoethnography and digital ethnography is used to examine patterns and incongruities in rhetoric when positioned against product functionality. The results are then collated to develop a nuanced, contextual discussion. The shared ideology we map between producers and consumers is individualist, globalist, techno-liberal, and ethically evasive. It is a 'total ideology' which obfuscates individual responsibility, and through which the nature of music and musical practice is transfigured to suit generative outcomes. 

**Abstract (ZH)**: AI系统在音乐生成中的应用日益普遍并易于使用，使得没有音乐背景的人也能创作音乐。因此，生成式AI被宣传和推崇为音乐创作民主化的手段。然而，在这些行业环境中，包容性往往仅表现为可营销的 rhetoric，而非真正的指导原则。本文考察了截至2025年中期可公开使用的四种生成式AI音乐制作系统（AIVA、Stable Audio、Suno和Udio），分析它们的开发者如何进行宣传以及用户如何接收这些系统。我们的目标是研究驱动生成式AI在音乐制作早期开发和采用的意识形态，特别是关注民主化问题。我们采用自动民族志和数字民族志相结合的方法，研究宣传文本与产品功能之间的模式和不一致之处，然后汇集结果，进行细腻和情境化的讨论。我们映射的生产者和消费者共享的意识形态是个人主义、全球化、技术自由主义和道德规避主义。这是一种“全面意识形态”，模糊了个人责任，并通过这种方式将音乐的本质和实践改造成适合生成结果的形式。 

---
# Generative Modeling for Robust Deep Reinforcement Learning on the Traveling Salesman Problem 

**Title (ZH)**: 生成建模在旅行商问题上的鲁棒深度强化学习中的应用 

**Authors**: Michael Li, Eric Bae, Christopher Haberland, Natasha Jaques  

**Link**: [PDF](https://arxiv.org/pdf/2508.08718)  

**Abstract**: The Traveling Salesman Problem (TSP) is a classic NP-hard combinatorial optimization task with numerous practical applications. Classic heuristic solvers can attain near-optimal performance for small problem instances, but become computationally intractable for larger problems. Real-world logistics problems such as dynamically re-routing last-mile deliveries demand a solver with fast inference time, which has led researchers to investigate specialized neural network solvers. However, neural networks struggle to generalize beyond the synthetic data they were trained on. In particular, we show that there exist TSP distributions that are realistic in practice, which also consistently lead to poor worst-case performance for existing neural approaches. To address this issue of distribution robustness, we present Combinatorial Optimization with Generative Sampling (COGS), where training data is sampled from a generative TSP model. We show that COGS provides better data coverage and interpolation in the space of TSP training distributions. We also present TSPLib50, a dataset of realistically distributed TSP samples, which tests real-world generalization ability without conflating this issue with instance size. We evaluate our method on various synthetic datasets as well as TSPLib50, and compare to state-of-the-art neural baselines. We demonstrate that COGS improves distribution robustness, with most performance gains coming from worst-case scenarios. 

**Abstract (ZH)**: 旅行商问题（TSP）是一种经典的NP难组合优化任务，具有众多实际应用。经典的启发式求解器在小规模问题上可以取得接近最优的性能，但对于大规模问题则变得计算上不可行。现实世界物流问题如动态重新规划最后一英里交付需要快速的求解时间，这促使研究人员调查专门的神经网络求解器。然而，神经网络难以泛化到训练数据之外的数据。特别是在实践中，我们发现存在一些TSP分布，它们能够导致现有神经方法在最坏情况下的性能较差。为解决这一分布鲁棒性问题，我们提出了生成采样组合优化（COGS）方法，其中训练数据从生成的TSP模型中采样。我们展示了COGS在TSP训练分布空间中提供了更好的数据覆盖和插值性能。我们还提出了TSPLib50数据集，包含了现实分布的TSP样本，用于测试实际泛化能力，而不与实例规模混淆。我们在多种合成数据集以及TSPLib50上评估了我们的方法，并与最先进的神经基线进行了比较。我们展示了COGS提高了分布鲁棒性，大多数性能提升来自于最坏情况下的表现。 

---
# Doctor Sun: A Bilingual Multimodal Large Language Model for Biomedical AI 

**Title (ZH)**: 孙医生：一种双语多模态大型语言模型在生物医学AI中的应用 

**Authors**: Dong Xue, Ziyao Shao, Zhaoyang Duan, Fangzhou Liu, Bing Li, Zhongheng Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2508.08270)  

**Abstract**: Large multimodal models (LMMs) have demonstrated significant potential in providing innovative solutions for various biomedical tasks, including pathology analysis, radiology report generation, and biomedical assistance. However, the existing multimodal biomedical AI is typically based on foundation LLMs, thus hindering the understanding of intricate medical concepts with limited medical training data. Moreover, recent LLaVA-induced medical LMMs struggle to effectively capture the intricate relationship between the texts and the images. Therefore, we introduce Doctor Sun, a large multimodal generative model specialized in medicine, developed to encode, integrate, and interpret diverse biomedical data modalities such as text and images. In particular, Doctor Sun integrates a pre-trained vision encoder with a medical LLM and conducts two-stage training on various medical datasets, focusing on feature alignment and instruction tuning. Moreover, we release SunMed-VL, a wide-range bilingual medical multimodal dataset, along with all associated models, code, and resources, to freely support the advancement of biomedical multimodal research. 

**Abstract (ZH)**: 大型多模态模型（LMMs）在提供各种生物医药任务的创新解决方案方面展现了显著潜力，包括病理分析、放射学报告生成和生物医药辅助。然而，现有的多模态生物医药AI通常基于基础的LMMs，这限制了其在有限医学训练数据的情况下对复杂医学概念的理解。此外，最近由LLaVA引发的医学LMM难以有效地捕捉文本和图像之间的复杂关系。因此，我们提出了Doctor Sun这一专用于医学的大型多模态生成模型，旨在编码、整合和解释诸如文本和图像等多种生物医学数据模态。具体而言，Doctor Sun将预训练的视觉编码器与医学LLM集成，并在多种医学数据集上进行两阶段训练，重点关注特征对齐和指令调优。此外，我们发布了SunMed-VL这一广泛的双语医学多模态数据集，以及所有相关的模型、代码和资源，以自由支持生物医药多模态研究的发展。 

---
