{'arxiv_id': 'arXiv:2508.09071', 'title': 'GeoVLA: Empowering 3D Representations in Vision-Language-Action Models', 'authors': 'Lin Sun, Bin Xie, Yingfei Liu, Hao Shi, Tiancai Wang, Jiale Cao', 'link': 'https://arxiv.org/abs/2508.09071', 'abstract': 'Vision-Language-Action (VLA) models have emerged as a promising approach for enabling robots to follow language instructions and predict corresponding this http URL, current VLA models mainly rely on 2D visual inputs, neglecting the rich geometric information in the 3D physical world, which limits their spatial awareness and adaptability. In this paper, we present GeoVLA, a novel VLA framework that effectively integrates 3D information to advance robotic manipulation. It uses a vision-language model (VLM) to process images and language instructions,extracting fused vision-language embeddings. In parallel, it converts depth maps into point clouds and employs a customized point encoder, called Point Embedding Network, to generate 3D geometric embeddings independently. These produced embeddings are then concatenated and processed by our proposed spatial-aware action expert, called 3D-enhanced Action Expert, which combines information from different sensor modalities to produce precise action sequences. Through extensive experiments in both simulation and real-world environments, GeoVLA demonstrates superior performance and robustness. It achieves state-of-the-art results in the LIBERO and ManiSkill2 simulation benchmarks and shows remarkable robustness in real-world tasks requiring height adaptability, scale awareness and viewpoint invariance.', 'abstract_zh': '基于3D信息的视觉-语言-动作（GeoVLA）框架：增强机器人操作的几何意识', 'title_zh': 'GeoVLA：赋能视觉-语言-行动模型中的3D表示'}
{'arxiv_id': 'arXiv:2508.08257', 'title': 'Where is the Boundary: Multimodal Sensor Fusion Test Bench for Tissue Boundary Delineation', 'authors': 'Zacharias Chen, Alexa Cristelle Cahilig, Sarah Dias, Prithu Kolar, Ravi Prakash, Patrick J. Codd', 'link': 'https://arxiv.org/abs/2508.08257', 'abstract': "Robot-assisted neurological surgery is receiving growing interest due to the improved dexterity, precision, and control of surgical tools, which results in better patient outcomes. However, such systems often limit surgeons' natural sensory feedback, which is crucial in identifying tissues -- particularly in oncological procedures where distinguishing between healthy and tumorous tissue is vital. While imaging and force sensing have addressed the lack of sensory feedback, limited research has explored multimodal sensing options for accurate tissue boundary delineation. We present a user-friendly, modular test bench designed to evaluate and integrate complementary multimodal sensors for tissue identification. Our proposed system first uses vision-based guidance to estimate boundary locations with visual cues, which are then refined using data acquired by contact microphones and a force sensor. Real-time data acquisition and visualization are supported via an interactive graphical interface. Experimental results demonstrate that multimodal fusion significantly improves material classification accuracy. The platform provides a scalable hardware-software solution for exploring sensor fusion in surgical applications and demonstrates the potential of multimodal approaches in real-time tissue boundary delineation.", 'abstract_zh': '辅助神经外科手术的机器人系统因其实现了更高的灵巧性、精确度和控制性，从而改善了手术效果，正逐渐受到关注。然而，这样的系统往往限制了外科医生的自然感官反馈，这对于识别组织至关重要，尤其是在区分健康组织与肿瘤组织的肿瘤外科手术中尤为重要。虽然成像和力感知已弥补了感官反馈的不足，但鲜有研究探讨多模态感知选项以实现准确的组织边界识别。我们提出了一种用户友好、模块化的测试平台，用于评估和集成互补的多模态传感器以识别组织。该系统首先使用基于视觉的引导来通过视觉提示估计边界位置，然后利用接触麦克风和力传感器获取的数据进行细化。通过交互式图形界面实现了实时数据采集与可视化。实验结果表明，多模态融合显著提高了材料分类准确性。该平台提供了可扩展的硬件-软件解决方案，用于在手术应用中探索传感器融合，并展示了实时组织边界分割中多模态方法的潜力。', 'title_zh': '边界何在：多模态传感器融合测试平台用于组织边界勾画'}
{'arxiv_id': 'arXiv:2508.08926', 'title': 'Safe Semantics, Unsafe Interpretations: Tackling Implicit Reasoning Safety in Large Vision-Language Models', 'authors': 'Wei Cai, Jian Zhao, Yuchu Jiang, Tianle Zhang, Xuelong Li', 'link': 'https://arxiv.org/abs/2508.08926', 'abstract': 'Large Vision-Language Models face growing safety challenges with multimodal inputs. This paper introduces the concept of Implicit Reasoning Safety, a vulnerability in LVLMs. Benign combined inputs trigger unsafe LVLM outputs due to flawed or hidden reasoning. To showcase this, we developed Safe Semantics, Unsafe Interpretations, the first dataset for this critical issue. Our demonstrations show that even simple In-Context Learning with SSUI significantly mitigates these implicit multimodal threats, underscoring the urgent need to improve cross-modal implicit reasoning.', 'abstract_zh': '大规模多模态视觉语言模型面临日益增长的安全挑战。本文介绍了隐含推理安全性这一LVLMs的漏洞概念。 benign的多模态输入由于推理缺陷或隐藏原因触发了不安全的LVLM输出。为了展示这一点，我们开发了Safe Semantics和Unsafe Interpretations数据集，这是针对这一关键问题的第一个数据集。我们的实验表明，即使简单的基于上下文学习方法结合Safe Semantics也能显著减少这些隐含的多模态威胁，强调了提高跨模态隐含推理能力的迫切需求。', 'title_zh': '安全语义，不安全解释：大型视觉-语言模型中的隐式推理安全性挑战'}
{'arxiv_id': 'arXiv:2508.09085', 'title': 'Dynamic Uncertainty-aware Multimodal Fusion for Outdoor Health Monitoring', 'authors': 'Zihan Fang, Zheng Lin, Senkang Hu, Yihang Tao, Yiqin Deng, Xianhao Chen, Yuguang Fang', 'link': 'https://arxiv.org/abs/2508.09085', 'abstract': 'Outdoor health monitoring is essential to detect early abnormal health status for safeguarding human health and safety. Conventional outdoor monitoring relies on static multimodal deep learning frameworks, which requires extensive data training from scratch and fails to capture subtle health status changes. Multimodal large language models (MLLMs) emerge as a promising alternative, utilizing only small datasets to fine-tune pre-trained information-rich models for enabling powerful health status monitoring. Unfortunately, MLLM-based outdoor health monitoring also faces significant challenges: I) sensor data contains input noise stemming from sensor data acquisition and fluctuation noise caused by sudden changes in physiological signals due to dynamic outdoor environments, thus degrading the training performance; ii) current transformer based MLLMs struggle to achieve robust multimodal fusion, as they lack a design for fusing the noisy modality; iii) modalities with varying noise levels hinder accurate recovery of missing data from fluctuating distributions. To combat these challenges, we propose an uncertainty-aware multimodal fusion framework, named DUAL-Health, for outdoor health monitoring in dynamic and noisy environments. First, to assess the impact of noise, we accurately quantify modality uncertainty caused by input and fluctuation noise with current and temporal features. Second, to empower efficient muitimodal fusion with low-quality modalities,we customize the fusion weight for each modality based on quantified and calibrated uncertainty. Third, to enhance data recovery from fluctuating noisy modalities, we align modality distributions within a common semantic space. Extensive experiments demonstrate that our DUAL-Health outperforms state-of-the-art baselines in detection accuracy and robustness.', 'abstract_zh': '户外健康监测对于早期检测异常健康状态以保障人类健康和安全至关重要。基于传感器数据的户外健康监测面临显著挑战：I) 输入噪声和波动噪声影响模型训练效果；II) 当前基于变电站的多模态大语言模型难以实现稳健的多模态融合；III) 噪声水平各异的模态阻碍了从波动噪声中准确恢复缺失数据。为应对这些挑战，我们提出了一种不确定性意识的多模态融合框架——DUAL-Health，以应对动态和嘈杂环境下的户外健康监测。首先，通过准确量化由输入噪声和波动噪声引起的模态不确定性。其次，根据量化和校准后的不确定性为每个模态定制融合权重。最后，通过在公共语义空间中对齐模态分布来增强从波动噪声中恢复数据的能力。大量实验表明，我们的DUAL-Health在检测准确性和鲁棒性方面优于现有基线方法。', 'title_zh': '户外健康监测中的动态不确定性意识多模态融合'}
{'arxiv_id': 'arXiv:2508.08604', 'title': 'Transferable Model-agnostic Vision-Language Model Adaptation for Efficient Weak-to-Strong Generalization', 'authors': 'Jihwan Park, Taehoon song, Sanghyeok Lee, Miso Choi, Hyunwoo J. Kim', 'link': 'https://arxiv.org/abs/2508.08604', 'abstract': "Vision-Language Models (VLMs) have been widely used in various visual recognition tasks due to their remarkable generalization capabilities. As these models grow in size and complexity, fine-tuning becomes costly, emphasizing the need to reuse adaptation knowledge from 'weaker' models to efficiently enhance 'stronger' ones. However, existing adaptation transfer methods exhibit limited transferability across models due to their model-specific design and high computational demands. To tackle this, we propose Transferable Model-agnostic adapter (TransMiter), a light-weight adapter that improves vision-language models 'without backpropagation'. TransMiter captures the knowledge gap between pre-trained and fine-tuned VLMs, in an 'unsupervised' manner. Once trained, this knowledge can be seamlessly transferred across different models without the need for backpropagation. Moreover, TransMiter consists of only a few layers, inducing a negligible additional inference cost. Notably, supplementing the process with a few labeled data further yields additional performance gain, often surpassing a fine-tuned stronger model, with a marginal training cost. Experimental results and analyses demonstrate that TransMiter effectively and efficiently transfers adaptation knowledge while preserving generalization abilities across VLMs of different sizes and architectures in visual recognition tasks.", 'abstract_zh': 'Transferable Model-agnostic Adapter (TransMiter)：无需反向传播优化视觉语言模型的知识迁移', 'title_zh': '适用于高效弱到强泛化的可迁移模型agnostic视觉-语言模型适应方法'}
{'arxiv_id': 'arXiv:2508.08524', 'title': 'StreetViewAI: Making Street View Accessible Using Context-Aware Multimodal AI', 'authors': 'Jon E. Froehlich, Alexander Fiannaca, Nimer Jaber, Victor Tsara, Shaun Kane', 'link': 'https://arxiv.org/abs/2508.08524', 'abstract': 'Interactive streetscape mapping tools such as Google Street View (GSV) and Meta Mapillary enable users to virtually navigate and experience real-world environments via immersive 360° imagery but remain fundamentally inaccessible to blind users. We introduce StreetViewAI, the first-ever accessible street view tool, which combines context-aware, multimodal AI, accessible navigation controls, and conversational speech. With StreetViewAI, blind users can virtually examine destinations, engage in open-world exploration, or virtually tour any of the over 220 billion images and 100+ countries where GSV is deployed. We iteratively designed StreetViewAI with a mixed-visual ability team and performed an evaluation with eleven blind users. Our findings demonstrate the value of an accessible street view in supporting POI investigations and remote route planning. We close by enumerating key guidelines for future work.', 'abstract_zh': '无障碍街道视角工具StreetViewAI：结合上下文感知多模态AI、无障碍导航控制和会话语音的沉浸式360°影像虚拟导航工具', 'title_zh': 'StreetViewAI：使用基于上下文的多模态AI使街道视角更加 accessible'}
{'arxiv_id': 'arXiv:2508.08521', 'title': 'VISOR: Visual Input-based Steering for Output Redirection in Vision-Language Models', 'authors': 'Mansi Phute, Ravikumar Balakrishnan', 'link': 'https://arxiv.org/abs/2508.08521', 'abstract': "Vision Language Models (VLMs) are increasingly being used in a broad range of applications, bringing their security and behavioral control to the forefront. While existing approaches for behavioral control or output redirection, like system prompting in VLMs, are easily detectable and often ineffective, activation-based steering vectors require invasive runtime access to model internals--incompatible with API-based services and closed-source deployments. We introduce VISOR (Visual Input-based Steering for Output Redirection), a novel method that achieves sophisticated behavioral control through optimized visual inputs alone. By crafting universal steering images that induce target activation patterns, VISOR enables practical deployment across all VLM serving modalities while remaining imperceptible compared to explicit textual instructions. We validate VISOR on LLaVA-1.5-7B across three critical alignment tasks: refusal, sycophancy and survival instinct. A single 150KB steering image matches steering vector performance within 1-2% for positive behavioral shifts while dramatically exceeding it for negative steering--achieving up to 25% shifts from baseline compared to steering vectors' modest changes. Unlike system prompting (3-4% shifts), VISOR provides robust bidirectional control while maintaining 99.9% performance on 14,000 unrelated MMLU tasks. Beyond eliminating runtime overhead and model access requirements, VISOR exposes a critical security vulnerability: adversaries can achieve sophisticated behavioral manipulation through visual channels alone, bypassing text-based defenses. Our work fundamentally re-imagines multimodal model control and highlights the urgent need for defenses against visual steering attacks.", 'abstract_zh': '基于视觉输入的输出重定向行为控制 (VISOR)', 'title_zh': 'VISOR：基于视觉输入的输出重定向导向在视觉-语言模型中的应用'}
{'arxiv_id': 'arXiv:2508.08487', 'title': 'MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling', 'authors': 'Qian Wang, Ziqi Huang, Ruoxi Jia, Paul Debevec, Ning Yu', 'link': 'https://arxiv.org/abs/2508.08487', 'abstract': 'Despite recent advances, long-sequence video generation frameworks still suffer from significant limitations: poor assistive capability, suboptimal visual quality, and limited expressiveness. To mitigate these limitations, we propose MAViS, an end-to-end multi-agent collaborative framework for long-sequence video storytelling. MAViS orchestrates specialized agents across multiple stages, including script writing, shot designing, character modeling, keyframe generation, video animation, and audio generation. In each stage, agents operate under the 3E Principle -- Explore, Examine, and Enhance -- to ensure the completeness of intermediate outputs. Considering the capability limitations of current generative models, we propose the Script Writing Guidelines to optimize compatibility between scripts and generative tools. Experimental results demonstrate that MAViS achieves state-of-the-art performance in assistive capability, visual quality, and video expressiveness. Its modular framework further enables scalability with diverse generative models and tools. With just a brief user prompt, MAViS is capable of producing high-quality, expressive long-sequence video storytelling, enriching inspirations and creativity for users. To the best of our knowledge, MAViS is the only framework that provides multimodal design output -- videos with narratives and background music.', 'abstract_zh': '尽管近年来取得了进展，但长序列视频生成框架仍然存在显著的局限性：辅助能力差、视觉质量欠佳和表现力有限。为了缓解这些局限性，我们提出了一种端到端的多智能体协作框架MAViS，用于长序列视频叙事。MAViS在多个阶段协调专门的智能体，包括剧本写作、镜头设计、角色建模、关键帧生成、视频动画和音频生成。在每个阶段，智能体遵循3E原则——探索、审查和增强——以确保中间输出的完整性。考虑到当前生成模型的能力限制，我们提出剧本写作指南以优化剧本与生成工具之间的兼容性。实验结果表明，MAViS在辅助能力、视觉质量和视频表现力方面达到了最先进的性能。其模块化框架还使不同生成模型和工具的应用更具扩展性。只需简洁的用户提示，MAViS便能够生成高质量、有表现力的长序列视频叙事，丰富用户的灵感和创造力。据我们所知，MAViS是唯一提供多模态设计输出——包含叙述和背景音乐的视频——的框架。', 'title_zh': 'MAViS: 多Agent长序列视频叙述框架'}
