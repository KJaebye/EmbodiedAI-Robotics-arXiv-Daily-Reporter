{'arxiv_id': 'arXiv:2509.06191', 'title': 'Learning in ImaginationLand: Omnidirectional Policies through 3D Generative Models (OP-Gen)', 'authors': 'Yifei Ren, Edward Johns', 'link': 'https://arxiv.org/abs/2509.06191', 'abstract': 'Recent 3D generative models, which are capable of generating full object shapes from just a few images, now open up new opportunities in robotics. In this work, we show that 3D generative models can be used to augment a dataset from a single real-world demonstration, after which an omnidirectional policy can be learned within this imagined dataset. We found that this enables a robot to perform a task when initialised from states very far from those observed during the demonstration, including starting from the opposite side of the object relative to the real-world demonstration, significantly reducing the number of demonstrations required for policy learning. Through several real-world experiments across tasks such as grasping objects, opening a drawer, and placing trash into a bin, we study these omnidirectional policies by investigating the effect of various design choices on policy behaviour, and we show superior performance to recent baselines which use alternative methods for data augmentation.', 'abstract_zh': '近期的3D生成模型能够仅从少量图像中生成完整物体形状，为机器人技术开辟了新的机遇。在本文中，我们展示了3D生成模型可以用于扩展单个真实世界演示数据集，在此基础上可以在想象的数据集中学习全向策略。我们发现这种方法使机器人能够在初始状态与演示观察到的状态相距甚远的情况下执行任务，甚至可以从演示相对的物体位置开始，从而显著减少了所需的数据采集次数。通过在抓取物体、打开抽屉和将垃圾放入垃圾桶等任务中进行多项实际实验，我们研究了这些全向策略，并通过考察各种设计选择对策略行为的影响，展示了其优于使用替代方法进行数据扩展的近期基线方法的优越性能。', 'title_zh': '想象之地的learning：基于3D生成模型的全方位策略（OP-Gen）'}
{'arxiv_id': 'arXiv:2509.05599', 'title': 'MonoGlass3D: Monocular 3D Glass Detection with Plane Regression and Adaptive Feature Fusion', 'authors': 'Kai Zhang, Guoyang Zhao, Jianxing Shi, Bonan Liu, Weiqing Qi, Jun Ma', 'link': 'https://arxiv.org/abs/2509.05599', 'abstract': 'Detecting and localizing glass in 3D environments poses significant challenges for visual perception systems, as the optical properties of glass often hinder conventional sensors from accurately distinguishing glass surfaces. The lack of real-world datasets focused on glass objects further impedes progress in this field. To address this issue, we introduce a new dataset featuring a wide range of glass configurations with precise 3D annotations, collected from distinct real-world scenarios. On the basis of this dataset, we propose MonoGlass3D, a novel approach tailored for monocular 3D glass detection across diverse environments. To overcome the challenges posed by the ambiguous appearance and context diversity of glass, we propose an adaptive feature fusion module that empowers the network to effectively capture contextual information in varying conditions. Additionally, to exploit the distinct planar geometry of glass surfaces, we present a plane regression pipeline, which enables seamless integration of geometric properties within our framework. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in both glass segmentation and monocular glass depth estimation. Our results highlight the advantages of combining geometric and contextual cues for transparent surface understanding.', 'abstract_zh': '检测和定位3D环境中玻璃具有显著挑战性，因为玻璃的光学特性常常妨碍传统传感器准确区分玻璃表面。由于缺乏专注于玻璃物体的现实世界数据集，该领域的发展受到了阻碍。为解决这一问题，我们引入了一个新的数据集，该数据集包含多种精确3D注释的玻璃配置，并源自不同的现实场景。基于该数据集，我们提出了一种新颖的方法MonoGlass3D，专门用于在各种环境中进行单目3D玻璃检测。为了克服由玻璃的模糊外观和上下文多样性带来的挑战，我们提出了一种自适应特征融合模块，使网络能够在不同条件下有效地捕捉上下文信息。此外，为了利用玻璃表面的独特平面几何结构，我们提出了一种平面回归流水线，该流水线能够在我们的框架内无缝集成几何特性。广泛的实验表明，我们的方法在玻璃分割和单目玻璃深度估计方面均优于现有最先进的方法。我们的结果强调了结合几何和上下文线索对透明表面理解的优势。', 'title_zh': 'MonoGlass3D: 单目3D玻璃检测方法及其平面回归与自适应特征融合'}
{'arxiv_id': 'arXiv:2509.05368', 'title': 'Long-Horizon Visual Imitation Learning via Plan and Code Reflection', 'authors': 'Quan Chen, Chenrui Shi, Qi Chen, Yuwei Wu, Zhi Gao, Xintong Zhang, Rui Gao, Kun Wu, Yunde Jia', 'link': 'https://arxiv.org/abs/2509.05368', 'abstract': 'Learning from long-horizon demonstrations with complex action sequences presents significant challenges for visual imitation learning, particularly in understanding temporal relationships of actions and spatial relationships between objects. In this paper, we propose a new agent framework that incorporates two dedicated reflection modules to enhance both plan and code generation. The plan generation module produces an initial action sequence, which is then verified by the plan reflection module to ensure temporal coherence and spatial alignment with the demonstration video. The code generation module translates the plan into executable code, while the code reflection module verifies and refines the generated code to ensure correctness and consistency with the generated plan. These two reflection modules jointly enable the agent to detect and correct errors in both the plan generation and code generation, improving performance in tasks with intricate temporal and spatial dependencies. To support systematic evaluation, we introduce LongVILBench, a benchmark comprising 300 human demonstrations with action sequences of up to 18 steps. LongVILBench emphasizes temporal and spatial complexity across multiple task types. Experimental results demonstrate that existing methods perform poorly on this benchmark, whereas our new framework establishes a strong baseline for long-horizon visual imitation learning.', 'abstract_zh': '长时序复杂动作序列指导的视觉模仿学习中的计划与代码生成增强框架：时间与空间关系的联合理解与纠正', 'title_zh': '长时视觉模仿学习通过计划与代码反思'}
{'arxiv_id': 'arXiv:2509.06741', 'title': 'Event Spectroscopy: Event-based Multispectral and Depth Sensing using Structured Light', 'authors': 'Christian Geckeler, Niklas Neugebauer, Manasi Muglikar, Davide Scaramuzza, Stefano Mintchev', 'link': 'https://arxiv.org/abs/2509.06741', 'abstract': 'Uncrewed aerial vehicles (UAVs) are increasingly deployed in forest environments for tasks such as environmental monitoring and search and rescue, which require safe navigation through dense foliage and precise data collection. Traditional sensing approaches, including passive multispectral and RGB imaging, suffer from latency, poor depth resolution, and strong dependence on ambient light - especially under forest canopies. In this work, we present a novel event spectroscopy system that simultaneously enables high-resolution, low-latency depth reconstruction and multispectral imaging using a single sensor. Depth is reconstructed using structured light, and by modulating the wavelength of the projected structured light, our system captures spectral information in controlled bands between 650 nm and 850 nm. We demonstrate up to $60\\%$ improvement in RMSE over commercial depth sensors and validate the spectral accuracy against a reference spectrometer and commercial multispectral cameras, demonstrating comparable performance. A portable version limited to RGB (3 wavelengths) is used to collect real-world depth and spectral data from a Masoala Rainforest. We demonstrate the use of this prototype for color image reconstruction and material differentiation between leaves and branches using spectral and depth data. Our results show that adding depth (available at no extra effort with our setup) to material differentiation improves the accuracy by over $30\\%$ compared to color-only method. Our system, tested in both lab and real-world rainforest environments, shows strong performance in depth estimation, RGB reconstruction, and material differentiation - paving the way for lightweight, integrated, and robust UAV perception and data collection in complex natural environments.', 'abstract_zh': '无人驾驶航空器（UAV）越来越多地被部署在森林环境中执行环境监测和搜索救援等任务，需要在茂密植被中进行安全导航和精确数据收集。传统传感方法，包括被动多光谱和RGB成像，存在延迟大、深度分辨率差和对环境光线的强烈依赖，尤其是在森林树冠下。在本工作中，我们提出了一种新型事件光谱系统，该系统利用单个传感器同时实现高分辨率、低延迟的深度重建和多光谱成像。深度通过结构光进行重建，并通过调制投射结构光的波长，我们的系统在650 nm至850 nm的受控波段内捕获光谱信息。我们展示了在绝对均方根误差（RMSE）上相比商用深度传感器高达60%的提升，并通过参考光谱仪和商用多光谱相机验证光谱准确性，显示出类似的表现。用于实际森林环境（玛绍拉雨林）的便携版本仅限于RGB（3个波长）收集真实世界的深度和光谱数据。我们展示了该原型在使用光谱和深度数据进行彩色图像重建和叶片与枝条材料区分方面的应用。结果显示，将深度（在我们的方案中无需额外努力即可获得）加入材料区分可使准确率提高超过30%，相比仅使用颜色方法。在实验室和实际雨林环境中测试的系统在深度估计、RGB重建和材料区分方面表现出强劲性能，为在复杂自然环境中实现轻型、集成和稳健的无人机感知和数据收集铺平了道路。', 'title_zh': '事件光谱学：基于事件的多光谱和深度传感技术'}
{'arxiv_id': 'arXiv:2509.06660', 'title': 'Investigating Location-Regularised Self-Supervised Feature Learning for Seafloor Visual Imagery', 'authors': 'Cailei Liang, Adrian Bodenmann, Emma J Curtis, Samuel Simmons, Kazunori Nagano, Stan Brown, Adam Riese, Blair Thornton', 'link': 'https://arxiv.org/abs/2509.06660', 'abstract': 'High-throughput interpretation of robotically gathered seafloor visual imagery can increase the efficiency of marine monitoring and exploration. Although recent research has suggested that location metadata can enhance self-supervised feature learning (SSL), its benefits across different SSL strategies, models and seafloor image datasets are underexplored. This study evaluates the impact of location-based regularisation on six state-of-the-art SSL frameworks, which include Convolutional Neural Network (CNN) and Vision Transformer (ViT) models with varying latent-space dimensionality. Evaluation across three diverse seafloor image datasets finds that location-regularisation consistently improves downstream classification performance over standard SSL, with average F1-score gains of $4.9 \\pm 4.0%$ for CNNs and $6.3 \\pm 8.9%$ for ViTs, respectively. While CNNs pretrained on generic datasets benefit from high-dimensional latent representations, dataset-optimised SSL achieves similar performance across the high (512) and low (128) dimensional latent representations. Location-regularised SSL improves CNN performance over pre-trained models by $2.7 \\pm 2.7%$ and $10.1 \\pm 9.4%$ for high and low-dimensional latent representations, respectively. For ViTs, high-dimensionality benefits both pre-trained and dataset-optimised SSL. Although location-regularisation improves SSL performance compared to standard SSL methods, pre-trained ViTs show strong generalisation, matching the best-performing location-regularised SSL with F1-scores of $0.795 \\pm 0.075$ and $0.795 \\pm 0.077$, respectively. The findings highlight the value of location metadata for SSL regularisation, particularly when using low-dimensional latent representations, and demonstrate strong generalisation of high-dimensional ViTs for seafloor image analysis.', 'abstract_zh': '基于位置正则化的自监督学习在海底视觉图像高效监测与探索中的高 throughput 解释能够提高海洋监测与探索的效率。尽管最近的研究表明位置元数据可以增强自监督特征学习（SSL），但其在不同SSL策略、模型和海底图像数据集中的益处仍待探索。本研究评估了基于位置正则化对六种先进的SSL框架的影响，包括具有不同潜在空间维度的卷积神经网络（CNN）和视觉变换器（ViT）模型。通过三个不同的海底图像数据集的评估发现，基于位置的正则化一致性地提高了下游分类性能，与标准SSL相比，CNN的平均F1得分提升为4.9 ± 4.0%，ViT的平均F1得分提升为6.3 ± 8.9%。虽然预训练在通用数据集上的CNN受益于高维度的潜在表示，但针对数据集的SSL在高维度（512）和低维度（128）潜在表示上实现了相似的性能。基于位置正则化的SSL分别在高维度和低维度潜在表示中提高了预训练CNN性能的2.7 ± 2.7%和10.1 ± 9.4%。对于ViTs，高维度的潜在表示对其预训练和针对数据集的SSL都有益处。尽管基于位置的正则化提高了SSL性能，但预训练的ViTs表现出强大的泛化能力，其F1得分分别与基于位置正则化的SSL的最佳性能相匹配，为0.795 ± 0.075和0.795 ± 0.077。研究结果突显了低维度潜在表示中位置元数据对于SSL正则化的价值，并展示了高维度ViTs在海底图像分析中的强泛化能力。', 'title_zh': '探究位置正则化的自监督特征学习方法在海底视觉图像中的应用'}
{'arxiv_id': 'arXiv:2509.05645', 'title': 'Stereovision Image Processing for Planetary Navigation Maps with Semi-Global Matching and Superpixel Segmentation', 'authors': 'Yan-Shan Lu, Miguel Arana-Catania, Saurabh Upadhyay, Leonard Felicetti', 'link': 'https://arxiv.org/abs/2509.05645', 'abstract': "Mars exploration requires precise and reliable terrain models to ensure safe rover navigation across its unpredictable and often hazardous landscapes. Stereoscopic vision serves a critical role in the rover's perception, allowing scene reconstruction by generating precise depth maps through stereo matching. State-of-the-art Martian planetary exploration uses traditional local block-matching, aggregates cost over square windows, and refines disparities via smoothness constraints. However, this method often struggles with low-texture images, occlusion, and repetitive patterns because it considers only limited neighbouring pixels and lacks a wider understanding of scene context. This paper uses Semi-Global Matching (SGM) with superpixel-based refinement to mitigate the inherent block artefacts and recover lost details. The approach balances the efficiency and accuracy of SGM and adds context-aware segmentation to support more coherent depth inference. The proposed method has been evaluated in three datasets with successful results: In a Mars analogue, the terrain maps obtained show improved structural consistency, particularly in sloped or occlusion-prone regions. Large gaps behind rocks, which are common in raw disparity outputs, are reduced, and surface details like small rocks and edges are captured more accurately. Another two datasets, evaluated to test the method's general robustness and adaptability, show more precise disparity maps and more consistent terrain models, better suited for the demands of autonomous navigation on Mars, and competitive accuracy across both non-occluded and full-image error metrics. This paper outlines the entire terrain modelling process, from finding corresponding features to generating the final 2D navigation maps, offering a complete pipeline suitable for integration in future planetary exploration missions.", 'abstract_zh': '火星探索需要精确可靠的地形模型以确保漫游车在不可预测且经常具有危险性的地形上安全导航。立体视觉在漫游车感知中起着关键作用，通过生成精确的深度图来进行立体匹配，实现场景重建。现有的火星行星探索技术采用传统的局部块匹配方法，在方形窗口中聚合成本，并通过平滑性约束优化视差。然而，这种方法在低纹理图像、遮挡和重复模式面前往往表现不佳，因为这种方法仅考虑有限邻近像素，缺乏对场景上下文的广泛理解。本文利用基于超像素的半全局匹配（SGM）进行细化来缓解块状伪影并恢复丢失的细节。该方法平衡了SGM的效率和准确性，并添加了上下文感知分割以支持更连贯的深度推断。提出的方法已在三个数据集中进行了评估并取得了成功结果：在火星模拟中，获得的地形图在结构一致性方面得到了改进，尤其是在坡度或遮挡易发地区。岩石后的大型间隙在原始视差输出中很常见，被有效减少，表面对小岩石和边缘的细节描绘更为准确。另外两个数据集的评估旨在测试该方法的一般稳健性和适应性，显示出更精确的视差图和更具一致性的地形模型，更适合火星自主导航的需求，并在非遮挡和全图像误差指标上具有竞争力。本文概述了整个地形建模过程，从找到对应特征到生成最终的2D导航图，提供了一个适用于未来行星探索任务集成的完整管道。', 'title_zh': '行星导航地图的半全局匹配与超像素分割立体视觉图像处理'}
{'arxiv_id': 'arXiv:2509.05614', 'title': 'SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning', 'authors': 'Hanzhen Wang, Jiaming Xu, Jiayi Pan, Yongkang Zhou, Guohao Dai', 'link': 'https://arxiv.org/abs/2509.05614', 'abstract': 'Pruning accelerates compute-bound models by reducing computation. Recently applied to Vision-Language-Action (VLA) models, existing methods prune tokens using only local info from current action, ignoring global context from prior actions, causing >20% success rate drop and limited speedup. We observe high similarity across consecutive actions and propose leveraging both local (current) and global (past) info for smarter token selection. We introduce SpecPrune-VLA, a training-free method with two-level pruning and heuristic control: (1) Static pruning at action level: uses global history and local context to reduce visual tokens per action; (2) Dynamic pruning at layer level: prunes tokens per layer based on layer-specific importance; (3) Lightweight action-aware controller: classifies actions as coarse/fine-grained (by speed), adjusting pruning aggressiveness since fine-grained actions are pruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times speedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs. OpenVLA-OFT, with negligible success rate loss.', 'abstract_zh': 'Pruning加速计算约束模型通过减少计算量。我们提出SpecPrune-VLA：利用局部和全局信息进行智能_TOKEN_剪枝的训练-Free方法', 'title_zh': 'SpecPrune-VLA: 通过行动感知自推测剪枝加速视觉-语言-行动模型'}
{'arxiv_id': 'arXiv:2509.05512', 'title': 'Quaternion Approximation Networks for Enhanced Image Classification and Oriented Object Detection', 'authors': 'Bryce Grant, Peng Wang', 'link': 'https://arxiv.org/abs/2509.05512', 'abstract': 'This paper introduces Quaternion Approximate Networks (QUAN), a novel deep learning framework that leverages quaternion algebra for rotation equivariant image classification and object detection. Unlike conventional quaternion neural networks attempting to operate entirely in the quaternion domain, QUAN approximates quaternion convolution through Hamilton product decomposition using real-valued operations. This approach preserves geometric properties while enabling efficient implementation with custom CUDA kernels. We introduce Independent Quaternion Batch Normalization (IQBN) for training stability and extend quaternion operations to spatial attention mechanisms. QUAN is evaluated on image classification (CIFAR-10/100, ImageNet), object detection (COCO, DOTA), and robotic perception tasks. In classification tasks, QUAN achieves higher accuracy with fewer parameters and faster convergence compared to existing convolution and quaternion-based models. For objection detection, QUAN demonstrates improved parameter efficiency and rotation handling over standard Convolutional Neural Networks (CNNs) while establishing the SOTA for quaternion CNNs in this downstream task. These results highlight its potential for deployment in resource-constrained robotic systems requiring rotation-aware perception and application in other domains.', 'abstract_zh': '基于四元数近似网络（QUAN）的研究：利用四元数代数实现旋转不变的图像分类与目标检测', 'title_zh': '四元数近似网络以增强图像分类和定向物体检测'}
{'arxiv_id': 'arXiv:2509.06942', 'title': 'Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference', 'authors': 'Xiangwei Shen, Zhimin Li, Zhantao Yang, Shiyi Zhang, Yingfang Zhang, Donghao Li, Chunyu Wang, Qinglin Lu, Yansong Tang', 'link': 'https://arxiv.org/abs/2509.06942', 'abstract': 'Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the this http URL model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.', 'abstract_zh': '直接对齐：通过可微奖励直接将扩散模型与人类偏好对齐', 'title_zh': '直接对齐完整的扩散轨迹与精细粒度的人类偏好'}
{'arxiv_id': 'arXiv:2509.05469', 'title': 'From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation', 'authors': 'Chenguang Wang, Xiang Yan, Yilong Dai, Ziyi Wang, Susu Xu', 'link': 'https://arxiv.org/abs/2509.05469', 'abstract': 'Realistic visual renderings of street-design scenarios are essential for public engagement in active transportation planning. Traditional approaches are labor-intensive, hindering collective deliberation and collaborative decision-making. While AI-assisted generative design shows transformative potential by enabling rapid creation of design scenarios, existing generative approaches typically require large amounts of domain-specific training data and struggle to enable precise spatial variations of design/configuration in complex street-view scenes. We introduce a multi-agent system that edits and redesigns bicycle facilities directly on real-world street-view imagery. The framework integrates lane localization, prompt optimization, design generation, and automated evaluation to synthesize realistic, contextually appropriate designs. Experiments across diverse urban scenarios demonstrate that the system can adapt to varying road geometries and environmental conditions, consistently yielding visually coherent and instruction-compliant results. This work establishes a foundation for applying multi-agent pipelines to transportation infrastructure planning and facility design.', 'abstract_zh': '现实街景的视觉渲染对于促进公众参与主动交通规划至关重要。传统方法劳动密集型，阻碍了集体讨论和合作决策。虽然AI辅助生成设计展现出变革性的潜力，能够快速生成设计方案，但现有生成方法通常需要大量领域特定的训练数据，并难以在复杂街道视图场景中实现精确的空间变化。我们提出了一种多agent系统，可以直接在真实的街景图像上编辑和重新设计自行车设施。该框架集成了车道定位、提示优化、设计生成和自动化评估，以合成现实且上下文适当的设计。在多种城市场景的实验中证明，该系统能够适应不同的道路几何形状和环境条件，始终保持视觉一致性和指令一致性。本研究为将多agent流水线应用于交通基础设施规划和设施设计奠定了基础。', 'title_zh': '从图像生成到基础设施设计：面向街道设计生成的多智能体管道'}
{'arxiv_id': 'arXiv:2509.06956', 'title': 'H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers', 'authors': 'Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Shijian Lu, Nicu Sebe', 'link': 'https://arxiv.org/abs/2509.06956', 'abstract': 'Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a hierarchical plug-and-play pruning-and-recovering framework, called Hierarchical Hourglass Tokenizer (H$_{2}$OT), for efficient transformer-based 3D human pose estimation from videos. H$_{2}$OT begins with progressively pruning pose tokens of redundant frames and ends with recovering full-length sequences, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. It works with two key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module (TRM). TPM dynamically selects a few representative tokens to eliminate the redundancy of video frames, while TRM restores the detailed spatio-temporal information based on the selected tokens, thereby expanding the network output to the original full-length temporal resolution for fast inference. Our method is general-purpose: it can be easily incorporated into common VPT models on both seq2seq and seq2frame pipelines while effectively accommodating different token pruning and recovery strategies. In addition, our H$_{2}$OT reveals that maintaining the full pose sequence is unnecessary, and a few pose tokens of representative frames can achieve both high efficiency and estimation accuracy. Extensive experiments on multiple benchmark datasets demonstrate both the effectiveness and efficiency of the proposed method. Code and models are available at this https URL.', 'abstract_zh': '基于视频的3D人体姿态估计中高效Transformer的方法：分层即插即用剪裁与恢复框架（H$_{2}$OT）', 'title_zh': 'H$_{2}$OT：分层_hourglass_令牌化器用于高效的视频姿态变换器'}
{'arxiv_id': 'arXiv:2509.06885', 'title': 'Barlow-Swin: Toward a novel siamese-based segmentation architecture using Swin-Transformers', 'authors': 'Morteza Kiani Haftlang, Mohammadhossein Malmir, Foroutan Parand, Umberto Michelucci, Safouane El Ghazouali', 'link': 'https://arxiv.org/abs/2509.06885', 'abstract': "Medical image segmentation is a critical task in clinical workflows, particularly for the detection and delineation of pathological regions. While convolutional architectures like U-Net have become standard for such tasks, their limited receptive field restricts global context modeling. Recent efforts integrating transformers have addressed this, but often result in deep, computationally expensive models unsuitable for real-time use. In this work, we present a novel end-to-end lightweight architecture designed specifically for real-time binary medical image segmentation. Our model combines a Swin Transformer-like encoder with a U-Net-like decoder, connected via skip pathways to preserve spatial detail while capturing contextual information. Unlike existing designs such as Swin Transformer or U-Net, our architecture is significantly shallower and competitively efficient. To improve the encoder's ability to learn meaningful features without relying on large amounts of labeled data, we first train it using Barlow Twins, a self-supervised learning method that helps the model focus on important patterns by reducing unnecessary repetition in the learned features. After this pretraining, we fine-tune the entire model for our specific task. Experiments on benchmark binary segmentation tasks demonstrate that our model achieves competitive accuracy with substantially reduced parameter count and faster inference, positioning it as a practical alternative for deployment in real-time and resource-limited clinical environments. The code for our method is available at Github repository: this https URL.", 'abstract_zh': '实时医用图像二元分割的轻量级端到端架构', 'title_zh': 'Barlow-Swin：基于Swin Transformers的一种新型双目分割架构探究'}
{'arxiv_id': 'arXiv:2509.06854', 'title': 'Automated Radiographic Total Sharp Score (ARTSS) in Rheumatoid Arthritis: A Solution to Reduce Inter-Intra Reader Variation and Enhancing Clinical Practice', 'authors': 'Hajar Moradmand, Lei Ren', 'link': 'https://arxiv.org/abs/2509.06854', 'abstract': 'Assessing the severity of rheumatoid arthritis (RA) using the Total Sharp/Van Der Heijde Score (TSS) is crucial, but manual scoring is often time-consuming and subjective. This study introduces an Automated Radiographic Sharp Scoring (ARTSS) framework that leverages deep learning to analyze full-hand X-ray images, aiming to reduce inter- and intra-observer variability. The research uniquely accommodates patients with joint disappearance and variable-length image sequences. We developed ARTSS using data from 970 patients, structured into four stages: I) Image pre-processing and re-orientation using ResNet50, II) Hand segmentation using UNet.3, III) Joint identification using YOLOv7, and IV) TSS prediction using models such as VGG16, VGG19, ResNet50, DenseNet201, EfficientNetB0, and Vision Transformer (ViT). We evaluated model performance with Intersection over Union (IoU), Mean Average Precision (MAP), mean absolute error (MAE), Root Mean Squared Error (RMSE), and Huber loss. The average TSS from two radiologists was used as the ground truth. Model training employed 3-fold cross-validation, with each fold consisting of 452 training and 227 validation samples, and external testing included 291 unseen subjects. Our joint identification model achieved 99% accuracy. The best-performing model, ViT, achieved a notably low Huber loss of 0.87 for TSS prediction. Our results demonstrate the potential of deep learning to automate RA scoring, which can significantly enhance clinical practice. Our approach addresses the challenge of joint disappearance and variable joint numbers, offers timesaving benefits, reduces inter- and intra-reader variability, improves radiologist accuracy, and aids rheumatologists in making more informed decisions.', 'abstract_zh': '利用Total Sharp/Van Der Heijde评分（TSS）评估类风湿关节炎（RA）的严重程度至关重要，但手动评分往往耗时且具主观性。本研究提出了一种基于深度学习的自动放射学Sharp评分（ARTSS）框架，旨在减少观察者间和观察者内的变异。该研究独特地处理了关节消失和图像序列长度不一的患者。我们使用970名患者的资料构建了ARTSS，分为四个阶段：I）使用ResNet50进行图像预处理和重新定向；II）使用UNet.3进行手部分割；III）使用YOLOv7进行关节识别；IV）使用VGG16、VGG19、ResNet50、DenseNet201、EfficientNetB0和Vision Transformer（ViT）等模型进行TSS预测。我们使用Intersection over Union（IoU）、Mean Average Precision（MAP）、mean absolute error（MAE）、Root Mean Squared Error（RMSE）和Huber损失评估模型性能。两位放射科医生的平均TSS作为真实值。模型训练采用3折交叉验证，每折包含452个训练样本和227个验证样本，外部测试包括291个未见过的个体。我们的关节识别模型准确率达到99%。性能最佳的模型ViT在TSS预测中的Huber损失为0.87。我们的研究结果展示了深度学习在自动化RA评分方面具有巨大潜力，这将显著提升临床实践。本方法解决了关节消失和关节数目变化的挑战，提供了节省时间的好处，减少了观察者间和观察者内的变异，提高了放射科医生的准确性，并有助于风湿科医生做出更明智的决策。', 'title_zh': '基于放射影像的整体锐利度评分（ARTSS）在类风湿关节炎中的应用：减少阅片者间及阅片者内变异并提升临床实践'}
{'arxiv_id': 'arXiv:2509.06690', 'title': 'BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring', 'authors': 'Usman Haider, Lukasz Szemet, Daniel Kelly, Vasileios Sergis, Andrew C. Daly, Karl Mason', 'link': 'https://arxiv.org/abs/2509.06690', 'abstract': 'Bioprinting is a rapidly advancing field that offers a transformative approach to fabricating tissue and organ models through the precise deposition of cell-laden bioinks. Ensuring the fidelity and consistency of printed structures in real-time remains a core challenge, particularly under constraints imposed by limited imaging data and resource-constrained embedded hardware. Semantic segmentation of the extrusion process, differentiating between nozzle, extruded bioink, and surrounding background, enables in situ monitoring critical to maintaining print quality and biological viability. In this work, we introduce a lightweight semantic segmentation framework tailored for real-time bioprinting applications. We present a novel, manually annotated dataset comprising 787 RGB images captured during the bioprinting process, labeled across three classes: nozzle, bioink, and background. To achieve fast and efficient inference suitable for integration with bioprinting systems, we propose a BioLite U-Net architecture that leverages depthwise separable convolutions to drastically reduce computational load without compromising accuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based segmentation baselines using mean Intersection over Union (mIoU), Dice score, and pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess real-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85% and a Dice score of 96.17%, while being over 1300x smaller than MobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame, demonstrating near real-time capability. Compared to MobileNet baselines, BioLite U-Net offers a superior tradeoff between segmentation accuracy, efficiency, and deployability, making it highly suitable for intelligent, closed-loop bioprinting systems.', 'abstract_zh': '生物打印是一个迅速发展的领域，通过精确沉积细胞载生物墨水来制造组织和器官模型。在有限的成像数据和资源受限的嵌入式硬件约束下，实时确保打印结构的准确性和一致性仍然是一个核心挑战。通过对外挤出过程进行语义分割，区分喷嘴、挤出的生物墨水和背景，可以实现原位监测，这对于维持打印质量和生物活性至关重要。在这项工作中，我们介绍了一种轻量级的语义分割框架，适用于生物打印应用。我们提出了一种新型的手动标注数据集，包含787张RGB图像，这些图像捕捉了生物打印过程中的数据，并被标注为三个类别：喷嘴、生物墨水和背景。为了实现适用于生物打印系统集成的快速高效推理，我们提出了一种基于深度可分离卷积的BioLite U-Net架构，以大幅减少计算量而不牺牲准确性。我们的模型使用MobileNetV2和MobileNetV3基线进行基准测试，评估指标包括均值交并比（mIoU）、Dice分数和像素准确性。所有模型均在Raspberry Pi 4B上进行评估，以评估其实用性。所提出的BioLite U-Net在每帧上的推理时间为335毫秒，实现接近实时能力。与MobileNet基线相比，BioLite U-Net在分割准确性、效率和部署能力之间提供了更佳的权衡，使其非常适合智能的闭环生物打印系统。', 'title_zh': 'BioLite U-Net: 适用于就地生物打印监控的边缘部署语义分割'}
{'arxiv_id': 'arXiv:2509.06625', 'title': 'Improved Classification of Nitrogen Stress Severity in Plants Under Combined Stress Conditions Using Spatio-Temporal Deep Learning Framework', 'authors': 'Aswini Kumar Patra', 'link': 'https://arxiv.org/abs/2509.06625', 'abstract': "Plants in their natural habitats endure an array of interacting stresses, both biotic and abiotic, that rarely occur in isolation. Nutrient stress-particularly nitrogen deficiency-becomes even more critical when compounded with drought and weed competition, making it increasingly difficult to distinguish and address its effects. Early detection of nitrogen stress is therefore crucial for protecting plant health and implementing effective management strategies. This study proposes a novel deep learning framework to accurately classify nitrogen stress severity in a combined stress environment. Our model uses a unique blend of four imaging modalities-RGB, multispectral, and two infrared wavelengths-to capture a wide range of physiological plant responses from canopy images. These images, provided as time-series data, document plant health across three levels of nitrogen availability (low, medium, and high) under varying water stress and weed pressures. The core of our approach is a spatio-temporal deep learning pipeline that merges a Convolutional Neural Network (CNN) for extracting spatial features from images with a Long Short-Term Memory (LSTM) network to capture temporal dependencies. We also devised and evaluated a spatial-only CNN pipeline for comparison. Our CNN-LSTM pipeline achieved an impressive accuracy of 98%, impressively surpassing the spatial-only model's 80.45% and other previously reported machine learning method's 76%. These results bring actionable insights based on the power of our CNN-LSTM approach in effectively capturing the subtle and complex interactions between nitrogen deficiency, water stress, and weed pressure. This robust platform offers a promising tool for the timely and proactive identification of nitrogen stress severity, enabling better crop management and improved plant health.", 'abstract_zh': '植物在其自然生态环境中承受着多种交互性压力，包括生境压力和非生物压力，这些压力通常不会单独发生。氮素压力——特别是氮素缺乏——在与干旱和杂草竞争共存时变得更加关键，使其更加难以区分和应对。因此，早期检测氮素压力对于保护植物健康和实施有效的管理策略至关重要。本研究提出了一种新颖的深度学习框架，用于在共存压力环境中准确分类氮素压力严重程度。我们的模型使用四种成像模态的独特组合——RGB、多光谱和两种红外波长——来捕捉从冠层图像中获取的一系列生理植物响应。这些图像以时间序列数据的形式记录了在不同水分压力和杂草压力下，在低、中、高氮素可用性水平下植物的健康状况。我们方法的核心是一个时空深度学习管道，该管道结合了卷积神经网络(CNN)来从图像中提取空间特征，以及长短期记忆网络(LSTM)来捕捉时间依赖性。我们还设计并评估了一个仅空间CNN管道作为对比。我们的CNN-LSTM管道达到了令人印象深刻的98%的准确率，远超仅空间模型的80.45%和其他先前报道的机器学习方法的76%。这些结果基于我们CNN-LSTM方法的力量，提供了关于氮素缺乏、水分压力和杂草压力之间微妙而复杂的交互作用的可操作见解。这一强大的平台提供了一种及时和主动识别氮素压力严重程度的前景工具，有助于更好地作物管理和提高植物健康。', 'title_zh': '在联合作用力条件下基于时空深度学习框架的植物氮胁迫 severity 分类改善研究'}
{'arxiv_id': 'arXiv:2509.06461', 'title': "Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning", 'authors': 'Yuyao Ge, Shenghua Liu, Yiwei Wang, Lingrui Mei, Baolong Bi, Xuanshan Zhou, Jiayu Yao, Jiafeng Guo, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2509.06461', 'abstract': "Vision-Language Models (VLMs) have demonstrated remarkable success across diverse visual tasks, yet their performance degrades in complex visual environments. While existing enhancement approaches require additional training, rely on external segmentation tools, or operate at coarse-grained levels, they overlook the innate ability within VLMs. To bridge this gap, we investigate VLMs' attention patterns and discover that: (1) visual complexity strongly correlates with attention entropy, negatively impacting reasoning performance; (2) attention progressively refines from global scanning in shallow layers to focused convergence in deeper layers, with convergence degree determined by visual complexity. (3) Theoretically, we prove that the contrast of attention maps between general queries and task-specific queries enables the decomposition of visual signal into semantic signals and visual noise components. Building on these insights, we propose Contrastive Attention Refinement for Visual Enhancement (CARVE), a training-free method that extracts task-relevant visual signals through attention contrasting at the pixel level. Extensive experiments demonstrate that CARVE consistently enhances performance, achieving up to 75% improvement on open-source models. Our work provides critical insights into the interplay between visual complexity and attention mechanisms, offering an efficient pathway for improving visual reasoning with contrasting attention.", 'abstract_zh': 'Vision-Language模型(VLMs)在多样化的视觉任务中取得了显著的成功，但在复杂视觉环境中表现退化。现有增强方法需要额外训练、依赖外部分割工具或在粗粒度级别操作，忽略了VLMs本身的固有能力。为弥合这一差距，我们研究了VLMs的注意力模式，并发现：(1) 视觉复杂性与注意力熵呈强烈正相关，负面影响了推理性能；(2) 注意力从浅层的全局扫描逐步细化到深层的局部集中，集中程度由视觉复杂性决定；(3) 理论上，我们证明了通用查询与任务特定查询的注意力图对比能够将视觉信号分解为语义信号和视觉噪声成分。基于这些见解，我们提出了基于像素级别注意力对比的视觉增强方法(CARVE)，一种无需训练的方法，通过注意力对比提取任务相关视觉信号。大量实验证明，CARVE一致地提升了性能，开源模型最多可提升75%。我们的工作为理解视觉复杂性和注意力机制之间的互动提供了关键见解，提供了一条通过对比注意力提高视觉推理效率的途径。', 'title_zh': '基于对比注意力的聚焦增强：提升VLMs的视觉推理能力'}
{'arxiv_id': 'arXiv:2509.06336', 'title': 'Multi View Slot Attention Using Paraphrased Texts For Face Anti-Spoofing', 'authors': 'Jeongmin Yu, Susang Kim, Kisu Lee, Taekyoung Kwon, Won-Yong Shin, Ha Young Kim', 'link': 'https://arxiv.org/abs/2509.06336', 'abstract': "Recent face anti-spoofing (FAS) methods have shown remarkable cross-domain performance by employing vision-language models like CLIP. However, existing CLIP-based FAS models do not fully exploit CLIP's patch embedding tokens, failing to detect critical spoofing clues. Moreover, these models rely on a single text prompt per class (e.g., 'live' or 'fake'), which limits generalization. To address these issues, we propose MVP-FAS, a novel framework incorporating two key modules: Multi-View Slot attention (MVS) and Multi-Text Patch Alignment (MTPA). Both modules utilize multiple paraphrased texts to generate generalized features and reduce dependence on domain-specific text. MVS extracts local detailed spatial features and global context from patch embeddings by leveraging diverse texts with multiple perspectives. MTPA aligns patches with multiple text representations to improve semantic robustness. Extensive experiments demonstrate that MVP-FAS achieves superior generalization performance, outperforming previous state-of-the-art methods on cross-domain datasets. Code: this https URL.", 'abstract_zh': '最近的研究表明，通过使用像CLIP这样的视觉语言模型，面部防欺骗（Face Anti-Spoofing, FAS）方法在跨域性能上取得了显著成果。然而，现有的基于CLIP的FAS模型未能充分利用CLIP的 Patch嵌入令牌，不能检测到关键的欺骗线索。此外，这些模型依赖于每类单一的文字提示（例如，“live”或“fake”），这限制了它们的泛化能力。为了解决这些问题，我们提出了一种名为MVP-FAS的新颖框架，该框架包含两个关键模块：多视角槽注意（MVS）和多文本块对齐（MTPA）。这两个模块利用多个同义表达的文本来生成泛化的特征，并减少对特定领域文本的依赖。MVS通过利用多种视角的多样文本提取局部详细的空间特征和全局语境。MTPA通过改进语义稳健性来对齐带有多种文本表示的块。广泛的实验表明，MVP-FAS在泛化性能上表现出优越性，并在跨域数据集上超过了之前的方法。代码：this https URL。', 'title_zh': '基于 paraphrased 文本的多视图槽注意力-face 反冒充'}
{'arxiv_id': 'arXiv:2509.06165', 'title': 'UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning', 'authors': 'Huy Le, Nhat Chung, Tung Kieu, Jingkang Yang, Ngan Le', 'link': 'https://arxiv.org/abs/2509.06165', 'abstract': 'Video Scene Graph Generation (VidSGG) aims to represent dynamic visual content by detecting objects and modeling their temporal interactions as structured graphs. Prior studies typically target either coarse-grained box-level or fine-grained panoptic pixel-level VidSGG, often requiring task-specific architectures and multi-stage training pipelines. In this paper, we present UNO (UNified Object-centric VidSGG), a single-stage, unified framework that jointly addresses both tasks within an end-to-end architecture. UNO is designed to minimize task-specific modifications and maximize parameter sharing, enabling generalization across different levels of visual granularity. The core of UNO is an extended slot attention mechanism that decomposes visual features into object and relation slots. To ensure robust temporal modeling, we introduce object temporal consistency learning, which enforces consistent object representations across frames without relying on explicit tracking modules. Additionally, a dynamic triplet prediction module links relation slots to corresponding object pairs, capturing evolving interactions over time. We evaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Results demonstrate that UNO not only achieves competitive performance across both tasks but also offers improved efficiency through a unified, object-centric design.', 'abstract_zh': 'UNified Object-centric Video Scene Graph Generation', 'title_zh': 'UNO：通过对象中心的视觉表示学习统一的一阶段视频场景图生成'}
{'arxiv_id': 'arXiv:2509.06159', 'title': 'FASL-Seg: Anatomy and Tool Segmentation of Surgical Scenes', 'authors': 'Muraam Abdel-Ghani, Mahmoud Ali, Mohamed Ali, Fatmaelzahraa Ahmed, Mohamed Arsalan, Abdulaziz Al-Ali, Shidin Balakrishnan', 'link': 'https://arxiv.org/abs/2509.06159', 'abstract': 'The growing popularity of robotic minimally invasive surgeries has made deep learning-based surgical training a key area of research. A thorough understanding of the surgical scene components is crucial, which semantic segmentation models can help achieve. However, most existing work focuses on surgical tools and overlooks anatomical objects. Additionally, current state-of-the-art (SOTA) models struggle to balance capturing high-level contextual features and low-level edge features. We propose a Feature-Adaptive Spatial Localization model (FASL-Seg), designed to capture features at multiple levels of detail through two distinct processing streams, namely a Low-Level Feature Projection (LLFP) and a High-Level Feature Projection (HLFP) stream, for varying feature resolutions - enabling precise segmentation of anatomy and surgical instruments. We evaluated FASL-Seg on surgical segmentation benchmark datasets EndoVis18 and EndoVis17 on three use cases. The FASL-Seg model achieves a mean Intersection over Union (mIoU) of 72.71% on parts and anatomy segmentation in EndoVis18, improving on SOTA by 5%. It further achieves a mIoU of 85.61% and 72.78% in EndoVis18 and EndoVis17 tool type segmentation, respectively, outperforming SOTA overall performance, with comparable per-class SOTA results in both datasets and consistent performance in various classes for anatomy and instruments, demonstrating the effectiveness of distinct processing streams for varying feature resolutions.', 'abstract_zh': '基于深度学习的微创手术培训研究：一种适应性空间定位模型在解剖学和手术器械分割中的应用', 'title_zh': 'FASL-Seg: 手术场景中的解剖结构和工具分割'}
{'arxiv_id': 'arXiv:2509.06122', 'title': 'SpecSwin3D: Generating Hyperspectral Imagery from Multispectral Data via Transformer Networks', 'authors': 'Tang Sui, Songxi Yang, Qunying Huang', 'link': 'https://arxiv.org/abs/2509.06122', 'abstract': 'Multispectral and hyperspectral imagery are widely used in agriculture, environmental monitoring, and urban planning due to their complementary spatial and spectral characteristics. A fundamental trade-off persists: multispectral imagery offers high spatial but limited spectral resolution, while hyperspectral imagery provides rich spectra at lower spatial resolution. Prior hyperspectral generation approaches (e.g., pan-sharpening variants, matrix factorization, CNNs) often struggle to jointly preserve spatial detail and spectral fidelity. In response, we propose SpecSwin3D, a transformer-based model that generates hyperspectral imagery from multispectral inputs while preserving both spatial and spectral quality. Specifically, SpecSwin3D takes five multispectral bands as input and reconstructs 224 hyperspectral bands at the same spatial resolution. In addition, we observe that reconstruction errors grow for hyperspectral bands spectrally distant from the input bands. To address this, we introduce a cascade training strategy that progressively expands the spectral range to stabilize learning and improve fidelity. Moreover, we design an optimized band sequence that strategically repeats and orders the five selected multispectral bands to better capture pairwise relations within a 3D shifted-window transformer framework. Quantitatively, our model achieves a PSNR of 35.82 dB, SAM of 2.40°, and SSIM of 0.96, outperforming the baseline MHF-Net by +5.6 dB in PSNR and reducing ERGAS by more than half. Beyond reconstruction, we further demonstrate the practical value of SpecSwin3D on two downstream tasks, including land use classification and burnt area segmentation.', 'abstract_zh': '多光谱和高光谱影像由于其互补的空间和光谱特性，在农业、环境监测和城市规划中被广泛应用于高光谱影像生成。一种基本的权衡一直存在：多光谱影像提供高空间分辨率但光谱分辨率有限，而高光谱影像提供丰富的光谱信息但空间分辨率较低。先前的高光谱影像生成方法（如多尺度锐化变种、矩阵分解、CNNs）往往难以同时保留空间细节和光谱保真度。为此，我们提出了一种基于变压器的模型SpecSwin3D，它可以从前置的多光谱输入生成高光谱影像，同时保持空间和光谱质量。具体而言，SpecSwin3D 输入五个多光谱波段，并在相同的空间分辨率下重建 224 个高光谱波段。此外，我们观察到，对于和输入波段光谱距离较远的高光谱波段，重建误差会增加。为此，我们引入了一种级联训练策略，逐步扩展光谱范围以稳定学习并提高保真度。此外，我们设计了一种优化的波段序列，战略性地重复并排序五个选定的多光谱波段，以便更好地在 3D 移动窗口变压器框架中捕捉两两关系。定量结果表明，我们的模型达到了 35.82 dB 的 PSNR、2.40° 的 SAM 和 0.96 的 SSIM，PSNR 指标比基线 MHF-Net 高出 5.6 dB，且 ERGAS 降低了超过一半。除了重建任务外，我们还进一步展示了 SpecSwin3D 在两个下游任务（包括土地利用分类和烧伤面积分割）中的实用价值。', 'title_zh': 'SpecSwin3D: 通过变换器网络从多光谱数据生成高光谱图像'}
{'arxiv_id': 'arXiv:2509.06035', 'title': 'TinyDef-DETR:An Enhanced DETR Detector for UAV Power Line Defect Detection', 'authors': 'Jiaming Cui', 'link': 'https://arxiv.org/abs/2509.06035', 'abstract': 'Automated inspection of transmission lines using UAVs is hindered by the difficulty of detecting small and ambiguous defects against complex backgrounds. Conventional detectors often suffer from detail loss due to strided downsampling, weak boundary sensitivity in lightweight backbones, and insufficient integration of global context with local cues. To address these challenges, we propose TinyDef-DETR, a DETR-based framework designed for small-defect detection. The method introduces a stride-free space-to-depth module for lossless downsampling, an edge-enhanced convolution for boundary-aware feature extraction, a cross-stage dual-domain multi-scale attention module to jointly capture global and local information, and a Focaler-Wise-SIoU regression loss to improve localization of small objects. Experiments conducted on the CSG-ADCD dataset demonstrate that TinyDef-DETR achieves substantial improvements in both precision and recall compared to competitive baselines, with particularly notable gains on small-object subsets, while incurring only modest computational overhead. Further validation on the VisDrone benchmark confirms the generalization capability of the proposed approach. Overall, the results indicate that integrating detail-preserving downsampling, edge-sensitive representations, dual-domain attention, and difficulty-adaptive regression provides a practical and efficient solution for UAV-based small-defect inspection in power grids.', 'abstract_zh': '基于UAV的输电线路自动化检测受复杂背景中检测小而模棱两可缺陷的困难所制约。传统检测器常因跳跃下采样的细节损失、轻量级骨干网在边缘敏感度上的不足以及全球上下文与局部线索融合不足而受到影响。为应对这些挑战，我们提出了TinyDef-DETR，这是一种基于DETR的小缺陷检测框架。该方法引入了无跳跃的空间到深度模块进行无损下采样，边缘增强卷积进行边界感知特征提取，跨阶段双域多尺度注意力模块共同捕捉全局和局部信息，并采用焦点器-边缘-WSIoU回归损失以提高小目标的定位精度。在CSG-ADCD数据集上的实验表明，TinyDef-DETR在精确度和召回率方面均相对于竞争基线取得了显著提升，特别是在小目标子集上的提升尤为明显，同时仅带来了轻微的计算开销。进一步在VisDrone基准上的验证证实了所提出方法的泛化能力。总体而言，结果表明，结合细节保留下采样、边缘敏感表示、双域注意和难度适应性回归为基于UAV的电力网络中小缺陷检测提供了一种实用且高效的解决方案。', 'title_zh': 'TinyDef-DETR：一种用于无人机输电线路缺陷检测的增强DETR检测器'}
{'arxiv_id': 'arXiv:2509.05999', 'title': 'S-LAM3D: Segmentation-Guided Monocular 3D Object Detection via Feature Space Fusion', 'authors': 'Diana-Alexandra Sas, Florin Oniga', 'link': 'https://arxiv.org/abs/2509.05999', 'abstract': 'Monocular 3D Object Detection represents a challenging Computer Vision task due to the nature of the input used, which is a single 2D image, lacking in any depth cues and placing the depth estimation problem as an ill-posed one. Existing solutions leverage the information extracted from the input by using Convolutional Neural Networks or Transformer architectures as feature extraction backbones, followed by specific detection heads for 3D parameters prediction. In this paper, we introduce a decoupled strategy based on injecting precomputed segmentation information priors and fusing them directly into the feature space for guiding the detection, without expanding the detection model or jointly learning the priors. The focus is on evaluating the impact of additional segmentation information on existing detection pipelines without adding additional prediction branches. The proposed method is evaluated on the KITTI 3D Object Detection Benchmark, outperforming the equivalent architecture that relies only on RGB image features for small objects in the scene: pedestrians and cyclists, and proving that understanding the input data can balance the need for additional sensors or training data.', 'abstract_zh': '单目三维物体检测由于输入仅为单张缺乏深度线索的2D图像，是一个具有挑战性的计算机视觉任务，导致深度估计问题成为病态问题。现有解决方案通过使用卷积神经网络或变换器架构作为特征提取骨干，并结合特定的检测头进行3D参数预测来利用输入中的信息。本文提出了一种解耦策略，通过注入预先计算的分割信息先验并直接融合到特征空间中来指导检测，而无需扩展检测模型或共同学习先验。重点在于评估额外分割信息对现有检测管道的影响，而不增加额外的预测分支。所提出的方法在Kitti三维物体检测基准上进行了评估，优于仅依赖RGB图像特征的等效架构，特别是在场景中的小物体：行人和骑车人方面表现更优，证明了理解输入数据可以平衡对额外传感器或训练数据的需求。', 'title_zh': 'S-LAM3D: 基于分割引导的单目三维目标检测及其特征空间融合'}
{'arxiv_id': 'arXiv:2509.05892', 'title': 'Challenges in Deep Learning-Based Small Organ Segmentation: A Benchmarking Perspective for Medical Research with Limited Datasets', 'authors': 'Phongsakon Mark Konrad, Andrei-Alexandru Popa, Yaser Sabzehmeidani, Liang Zhong, Elisa A. Liehn, Serkan Ayvaz', 'link': 'https://arxiv.org/abs/2509.05892', 'abstract': 'Accurate segmentation of carotid artery structures in histopathological images is vital for advancing cardiovascular disease research and diagnosis. However, deep learning model development in this domain is constrained by the scarcity of annotated cardiovascular histopathological data. This study investigates a systematic evaluation of state-of-the-art deep learning segmentation models, including convolutional neural networks (U-Net, DeepLabV3+), a Vision Transformer (SegFormer), and recent foundation models (SAM, MedSAM, MedSAM+UNet), on a limited dataset of cardiovascular histology images. Despite employing an extensive hyperparameter optimization strategy with Bayesian search, our findings reveal that model performance is highly sensitive to data splits, with minor differences driven more by statistical noise than by true algorithmic superiority. This instability exposes the limitations of standard benchmarking practices in low-data clinical settings and challenges the assumption that performance rankings reflect meaningful clinical utility.', 'abstract_zh': '在心血管组织病理图像中准确分割颈动脉结构对于心血管疾病研究和诊断的推进至关重要。然而，该领域的深度学习模型开发受限于标注心血管组织病理数据的稀缺性。本研究探讨了在有限的心血管组织学图像数据集上，包括卷积神经网络（U-Net、DeepLabV3+）、视觉变换器（SegFormer）以及最近的基础模型（SAM、MedSAM、MedSAM+Unet）的顶级深度学习分割模型的系统评估。尽管采用了广泛的超参数优化策略（贝叶斯搜索），但研究发现模型性能高度依赖于数据划分，微小差异更多由统计噪声引起而非真正的算法优越性。这种不稳定性揭示了在数据匮乏的临床环境中标准基准测试实践的局限性，并挑战了性能排名反映实际临床效用的假设。', 'title_zh': '基于深度学习的小器官分割挑战：有限数据集条件下医学研究的基准视角'}
{'arxiv_id': 'arXiv:2509.05796', 'title': 'Dual-Mode Deep Anomaly Detection for Medical Manufacturing: Structural Similarity and Feature Distance', 'authors': 'Julio Zanon Diaz, Georgios Siogkas, Peter Corcoran', 'link': 'https://arxiv.org/abs/2509.05796', 'abstract': 'Automating visual inspection in medical device manufacturing remains challenging due to small and imbalanced datasets, high-resolution imagery, and stringent regulatory requirements. This work proposes two attention-guided autoencoder architectures for deep anomaly detection designed to address these constraints. The first employs a structural similarity-based anomaly score (4-MS-SSIM), offering lightweight and accurate real-time defect detection, yielding ACC 0.903 (unsupervised thresholding) and 0.931 (supervised thresholding) on the - Surface Seal Image - Test split with only 10% of defective samples. The second applies a feature-distance approach using Mahalanobis scoring on reduced latent features, providing high sensitivity to distributional shifts for supervisory monitoring, achieving ACC 0.722 with supervised thresholding. Together, these methods deliver complementary capabilities: the first supports reliable inline inspection, while the second enables scalable post-production surveillance and regulatory compliance monitoring. Experimental results demonstrate that both approaches surpass re-implemented baselines and provide a practical pathway for deploying deep anomaly detection in regulated manufacturing environments, aligning accuracy, efficiency, and the regulatory obligations defined for high-risk AI systems under the EU AI Act.', 'abstract_zh': '医疗设备制造中基于视觉检测的自动化 remains 挑战性由于小规模和不均衡的数据集、高分辨率图像以及严格的监管要求。本研究提出了两种基于注意力机制的自编码器架构，用于解决这些约束条件下的深度异常检测。第一个架构采用基于结构相似性的异常评分（4-MS-SSIM），提供轻量级且准确的实时缺陷检测，仅使用10%的缺陷样本在Surface Seal Image Test分割上达到无监督阈值 ACC 0.903 和监督阈值 ACC 0.931。第二个架构采用特征距离方法，使用 Mahalanobis 得分对降维后潜变量进行评分，为监督监控提供对分布偏移的高灵敏度，通过监督阈值实现 ACC 0.722。这两种方法互为补充：第一个支持可靠的在线检测，而第二个方法则可以实现可扩展的生产后监控和监管合规监控。实验结果表明，这两种方法均优于重新实现的基线方法，并提供了一条在受监管的制造环境中部署深度异常检测的实际路径，符合欧盟AI法案对高风险AI系统规定的准确度、效率和监管义务。', 'title_zh': '双模式深度异常检测在医疗制造中的应用：结构 similarity 和特征距离'}
{'arxiv_id': 'arXiv:2509.05604', 'title': 'Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization', 'authors': 'Jungin Park, Jiyoung Lee, Kwanghoon Sohn', 'link': 'https://arxiv.org/abs/2509.05604', 'abstract': 'Video summarization aims to select keyframes that are visually diverse and can represent the whole story of a given video. Previous approaches have focused on global interlinkability between frames in a video by temporal modeling. However, fine-grained visual entities, such as objects, are also highly related to the main content of the video. Moreover, language-guided video summarization, which has recently been studied, requires a comprehensive linguistic understanding of complex real-world videos. To consider how all the objects are semantically related to each other, this paper regards video summarization as a language-guided spatiotemporal graph modeling problem. We present recursive spatiotemporal graph networks, called VideoGraph, which formulate the objects and frames as nodes of the spatial and temporal graphs, respectively. The nodes in each graph are connected and aggregated with graph edges, representing the semantic relationships between the nodes. To prevent the edges from being configured with visual similarity, we incorporate language queries derived from the video into the graph node representations, enabling them to contain semantic knowledge. In addition, we adopt a recursive strategy to refine initial graphs and correctly classify each frame node as a keyframe. In our experiments, VideoGraph achieves state-of-the-art performance on several benchmarks for generic and query-focused video summarization in both supervised and unsupervised manners. The code is available at this https URL.', 'abstract_zh': '视频摘要旨在选择视觉上多样的关键帧，以代表给定视频的整个故事。先前的方法主要关注视频中帧之间的全局关联性，通过时间建模实现。然而，细粒度的视觉实体，如物体，也与视频的主要内容高度相关。此外，近年来研究的语言引导视频摘要需要对复杂的现实世界视频进行全面的语言理解。为了考虑所有物体之间的语义关联，本文将视频摘要视为一种语义引导的空间-temporal图建模问题。我们提出了递归空间-temporal图网络，称为VideoGraph，将物体和帧分别表示为空间和时间图的节点。每个图中的节点通过图边连接和聚合，表示节点之间的语义关系。为了防止边基于视觉相似性配置，我们通过将来自视频的语言查询纳入图节点表示中，使它们能够包含语义知识。此外，我们采用递归策略改进初始图，并正确分类每个帧节点为关键帧。在我们的实验中，VideoGraph在多种基准测试中实现了通用和查询导向视频摘要的最佳性能，无论是监督学习还是无监督学习。代码可在以下网址获取。', 'title_zh': '基于语言引导的递归时空图建模的视频摘要'}
{'arxiv_id': 'arXiv:2509.05490', 'title': 'An Analysis of Layer-Freezing Strategies for Enhanced Transfer Learning in YOLO Architectures', 'authors': 'Andrzej D. Dobrzycki, Ana M. Bernardos, José R. Casar', 'link': 'https://arxiv.org/abs/2509.05490', 'abstract': 'The You Only Look Once (YOLO) architecture is crucial for real-time object detection. However, deploying it in resource-constrained environments such as unmanned aerial vehicles (UAVs) requires efficient transfer learning. Although layer freezing is a common technique, the specific impact of various freezing configurations on contemporary YOLOv8 and YOLOv10 architectures remains unexplored, particularly with regard to the interplay between freezing depth, dataset characteristics, and training dynamics. This research addresses this gap by presenting a detailed analysis of layer-freezing strategies. We systematically investigate multiple freezing configurations across YOLOv8 and YOLOv10 variants using four challenging datasets that represent critical infrastructure monitoring. Our methodology integrates a gradient behavior analysis (L2 norm) and visual explanations (Grad-CAM) to provide deeper insights into training dynamics under different freezing strategies. Our results reveal that there is no universal optimal freezing strategy but, rather, one that depends on the properties of the data. For example, freezing the backbone is effective for preserving general-purpose features, while a shallower freeze is better suited to handling extreme class imbalance. These configurations reduce graphics processing unit (GPU) memory consumption by up to 28% compared to full fine-tuning and, in some cases, achieve mean average precision (mAP@50) scores that surpass those of full fine-tuning. Gradient analysis corroborates these findings, showing distinct convergence patterns for moderately frozen models. Ultimately, this work provides empirical findings and practical guidelines for selecting freezing strategies. It offers a practical, evidence-based approach to balanced transfer learning for object detection in scenarios with limited resources.', 'abstract_zh': 'YOLO架构在资源受限环境下高效迁移学习的研究：冻结策略的详细分析与应用', 'title_zh': 'YOLO架构中增强迁移学习的层冻结策略分析'}
{'arxiv_id': 'arXiv:2509.05431', 'title': 'Advanced Brain Tumor Segmentation Using EMCAD: Efficient Multi-scale Convolutional Attention Decoding', 'authors': 'GodsGift Uzor, Tania-Amanda Nkoyo Fredrick Eneye, Chukwuebuka Ijezue', 'link': 'https://arxiv.org/abs/2509.05431', 'abstract': 'Brain tumor segmentation is a critical pre-processing step in the medical image analysis pipeline that involves precise delineation of tumor regions from healthy brain tissue in medical imaging data, particularly MRI scans. An efficient and effective decoding mechanism is crucial in brain tumor segmentation especially in scenarios with limited computational resources. However these decoding mechanisms usually come with high computational costs. To address this concern EMCAD a new efficient multi-scale convolutional attention decoder designed was utilized to optimize both performance and computational efficiency for brain tumor segmentation on the BraTs2020 dataset consisting of MRI scans from 369 brain tumor patients. The preliminary result obtained by the model achieved a best Dice score of 0.31 and maintained a stable mean Dice score of 0.285 plus/minus 0.015 throughout the training process which is moderate. The initial model maintained consistent performance across the validation set without showing signs of over-fitting.', 'abstract_zh': '脑肿瘤分割是医疗图像分析管道中的一个关键预处理步骤，涉及精确界定肿瘤区域与健康脑组织的边界，特别是在MRI扫描等医学影像数据中。高效的解码机制在资源有限的场景下尤为关键，但这些机制通常伴随着高昂的计算成本。为了应对这一挑战，EMCAD一种新的高效多尺度卷积注意力解码器被用于优化Brats2020数据集（包含369例脑肿瘤患者MRI扫描）上的脑肿瘤分割性能和计算效率。模型的初步结果中，最高Dice评分为0.31，训练过程中保持了稳定的平均Dice得分0.285±0.015，效果适中。初始模型在验证集上保持了稳定的性能，没有表现出过拟合的迹象。', 'title_zh': '使用EMCAD的高效多尺度卷积注意力解码高级脑肿瘤分割'}
{'arxiv_id': 'arXiv:2509.05352', 'title': 'Unsupervised Instance Segmentation with Superpixels', 'authors': 'Cuong Manh Hoang', 'link': 'https://arxiv.org/abs/2509.05352', 'abstract': 'Instance segmentation is essential for numerous computer vision applications, including robotics, human-computer interaction, and autonomous driving. Currently, popular models bring impressive performance in instance segmentation by training with a large number of human annotations, which are costly to collect. For this reason, we present a new framework that efficiently and effectively segments objects without the need for human annotations. Firstly, a MultiCut algorithm is applied to self-supervised features for coarse mask segmentation. Then, a mask filter is employed to obtain high-quality coarse masks. To train the segmentation network, we compute a novel superpixel-guided mask loss, comprising hard loss and soft loss, with high-quality coarse masks and superpixels segmented from low-level image features. Lastly, a self-training process with a new adaptive loss is proposed to improve the quality of predicted masks. We conduct experiments on public datasets in instance segmentation and object detection to demonstrate the effectiveness of the proposed framework. The results show that the proposed framework outperforms previous state-of-the-art methods.', 'abstract_zh': '实例分割对于机器人技术、人机交互和自动驾驶等众多计算机视觉应用至关重要。目前，流行的模型通过大量人工标注训练，在实例分割上表现出色，但人工标注成本高昂。为解决这一问题，我们提出了一种新的框架，能够在无需人工标注的情况下高效且有效地进行对象分割。该框架首先应用MultiCut算法对自监督特征进行粗略掩膜分割，然后使用掩膜过滤器获得高质量的粗略掩膜。为训练分割网络，我们计算了一种基于超像素的新型掩膜损失，包含硬损失和软损失，并结合高质量的粗略掩膜和从低级图像特征中分割出的超像素。最后，提出了一种带有新自适应损失的自我训练过程，以提高预测掩膜的质量。我们在实例分割和对象检测的公开数据集上进行了实验，证明了所提框架的有效性。实验结果表明，所提框架优于之前的方法。', 'title_zh': '无监督实例分割方法：基于超像素技术'}
