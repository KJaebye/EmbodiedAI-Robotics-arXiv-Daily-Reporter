# Generate, Not Recommend: Personalized Multimodal Content Generation 

**Title (ZH)**: 生成，而非推荐：个性化多模态内容生成 

**Authors**: Jiongnan Liu, Zhicheng Dou, Ning Hu, Chenyan Xiong  

**Link**: [PDF](https://arxiv.org/pdf/2506.01704)  

**Abstract**: To address the challenge of information overload from massive web contents, recommender systems are widely applied to retrieve and present personalized results for users. However, recommendation tasks are inherently constrained to filtering existing items and lack the ability to generate novel concepts, limiting their capacity to fully satisfy user demands and preferences. In this paper, we propose a new paradigm that goes beyond content filtering and selecting: directly generating personalized items in a multimodal form, such as images, tailored to individual users. To accomplish this, we leverage any-to-any Large Multimodal Models (LMMs) and train them in both supervised fine-tuning and online reinforcement learning strategy to equip them with the ability to yield tailored next items for users. Experiments on two benchmark datasets and user study confirm the efficacy of the proposed method. Notably, the generated images not only align well with users' historical preferences but also exhibit relevance to their potential future interests. 

**Abstract (ZH)**: 为了应对海量网络内容带来的信息过载挑战，推荐系统被广泛应用于为用户提供个性化结果。然而，推荐任务本质上局限于过滤现有项目，缺乏生成新概念的能力，限制了其满足用户需求和偏好的能力。本文提出了一种新的 paradigm，超越了内容过滤和选择：直接生成个性化项目，如图像，且针对个别用户量身定制。为此，我们利用任意到任意的大规模多模态模型（LMMs），并采用监督微调和在线强化学习策略进行训练，使模型具备为用户提供定制化后续项目的 ability。在两个基准数据集上的实验和用户研究均证实了所提方法的有效性。值得注意的是，生成的图像不仅与用户的 histórico 偏好高度一致，还能反映他们潜在的未来兴趣。 

---
# Respond Beyond Language: A Benchmark for Video Generation in Response to Realistic User Intents 

**Title (ZH)**: 超越语言的回应：基于现实用户意图的视频生成基准 

**Authors**: Shuting Wang, Yunqi Liu, Zixin Yang, Ning Hu, Zhicheng Dou, Chenyan Xiong  

**Link**: [PDF](https://arxiv.org/pdf/2506.01689)  

**Abstract**: Querying generative AI models, e.g., large language models (LLMs), has become a prevalent method for information acquisition. However, existing query-answer datasets primarily focus on textual responses, making it challenging to address complex user queries that require visual demonstrations or explanations for better understanding. To bridge this gap, we construct a benchmark, RealVideoQuest, designed to evaluate the abilities of text-to-video (T2V) models in answering real-world, visually grounded queries. It identifies 7.5K real user queries with video response intents from Chatbot-Arena and builds 4.5K high-quality query-video pairs through a multistage video retrieval and refinement process. We further develop a multi-angle evaluation system to assess the quality of generated video answers. Experiments indicate that current T2V models struggle with effectively addressing real user queries, pointing to key challenges and future research opportunities in multimodal AI. 

**Abstract (ZH)**: 查询生成型AI模型，例如大规模语言模型（LLMs），已成为信息获取的主流方法。然而，现有的查询-回答数据集主要侧重于文本响应，这使得处理需要视觉演示或解释以更好地理解的复杂用户查询变得具有挑战性。为了弥合这一差距，我们构建了一个基准数据集RealVideoQuest，旨在评估文本到视频（T2V）模型在回答具有视觉基础的现实世界查询方面的能力。该数据集从Chatbot-Arena中识别出7,500个包含视频响应意图的真实用户查询，并通过多阶段的视频检索和精炼过程构建了4,500个高质量的查询-视频对。我们还开发了一个多角度评估系统来评估生成视频答案的质量。实验表明，当前的T2V模型在有效处理真实用户查询方面存在困难，指出了跨模态AI中的关键挑战和未来研究机会。 

---
# EgoBrain: Synergizing Minds and Eyes For Human Action Understanding 

**Title (ZH)**: EgoBrain: 结合心智与视觉的人类动作理解 

**Authors**: Nie Lin, Yansen Wang, Dongqi Han, Weibang Jiang, Jingyuan Li, Ryosuke Furuta, Yoichi Sato, Dongsheng Li  

**Link**: [PDF](https://arxiv.org/pdf/2506.01353)  

**Abstract**: The integration of brain-computer interfaces (BCIs), in particular electroencephalography (EEG), with artificial intelligence (AI) has shown tremendous promise in decoding human cognition and behavior from neural signals. In particular, the rise of multimodal AI models have brought new possibilities that have never been imagined before. Here, we present EgoBrain --the world's first large-scale, temporally aligned multimodal dataset that synchronizes egocentric vision and EEG of human brain over extended periods of time, establishing a new paradigm for human-centered behavior analysis. This dataset comprises 61 hours of synchronized 32-channel EEG recordings and first-person video from 40 participants engaged in 29 categories of daily activities. We then developed a muiltimodal learning framework to fuse EEG and vision for action understanding, validated across both cross-subject and cross-environment challenges, achieving an action recognition accuracy of 66.70%. EgoBrain paves the way for a unified framework for brain-computer interface with multiple modalities. All data, tools, and acquisition protocols are openly shared to foster open science in cognitive computing. 

**Abstract (ZH)**: 脑-机接口（BCI）特别是脑电图（EEG）与人工智能（AI）的整合在解码人类认知和行为的神经信号方面展现了巨大的潜力。特别是，多模态AI模型的兴起带来了前所未有的新可能性。在这里，我们介绍了EgoBrain——世界上首个大规模、时间对齐的多模态数据集，该数据集同步了人类大脑的自中心视觉和长时间段的脑电图记录，建立了以人为中心的行为分析新范式。该数据集包括40名参与者在29类日常活动中记录的61小时同步32通道脑电图和一人群体视角视频。我们随后开发了一种多模态学习框架，将脑电图和视觉信息融合以理解动作，验证通过跨被试和跨环境挑战，实现了动作识别准确率66.70%。EgoBrain为多模态脑-机接口整合框架铺平了道路。所有数据、工具和采集协议均公开共享，以促进认知计算中的开放科学。 

---
# Contra4: Evaluating Contrastive Cross-Modal Reasoning in Audio, Video, Image, and 3D 

**Title (ZH)**: Contra4：评估跨模态对比推理在音频、视频、图像和3D场景中的表现 

**Authors**: Artemis Panagopoulou, Le Xue, Honglu Zhou, silvio savarese, Ran Xu, Caiming Xiong, Chris Callison-Burch, Mark Yatskar, Juan Carlos Niebles  

**Link**: [PDF](https://arxiv.org/pdf/2506.01275)  

**Abstract**: Real-world decision-making often begins with identifying which modality contains the most relevant information for a given query. While recent multimodal models have made impressive progress in processing diverse inputs, it remains unclear whether they can reason contrastively across multiple modalities to select the one that best satisfies a natural language prompt. We argue this capability is foundational, especially in retrieval-augmented and decision-time contexts, where systems must evaluate multiple signals and identify which one conveys the relevant information. To evaluate this skill, we introduce Contra4, a dataset for contrastive cross-modal reasoning across four modalities: image, audio, video, and 3D. Each example presents a natural language question alongside multiple candidate modality instances, and the model must select the one that semantically aligns with the prompt. Contra4 combines human-annotated captions with a mixture-of-models round-trip-consistency filter to ensure high-quality supervision, resulting in 174k training examples and a manually verified test set of 2.3k samples. While task-specific fine-tuning improves performance by 56% relative to baseline, state-of-the-art models still achieve only 56% accuracy overall and 42% in four-modality settings, underscoring a significant limitation in current multimodal models. 

**Abstract (ZH)**: 实世界决策往往始于识别哪些模态包含了与给定查询最相关的信息。尽管最近的多模态模型在处理多种输入方面取得了显著进展，但仍然不清楚它们是否能够在多种模态之间进行对比推理，从而挑选出最能满足自然语言提示的那个模态。我们认为这一能力是基础性的，尤其是在检索增强和决策时刻的上下文中，系统必须评估多种信号并确定哪个信号传达了相关的信息。为了评估这一技能，我们引入了Contra4数据集，用于四模态（图像、音频、视频和3D）之间的对比跨模态推理。每个示例包含一个自然语言问题和多个候选模态实例，模型必须挑选出与提示语义上最对齐的那个。Contra4结合了人工标注的描述和模型互校一致性筛选，以确保高质量的监督，数据集包含174,000个训练样本和2,300个人工验证的测试样本。尽管特定任务的微调相比基线性能提高了56%，但最先进的模型整体准确率仅为56%，四模态设置下的准确率为42%，突显了当前多模态模型的重要局限性。 

---
# Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues 

**Title (ZH)**: 超越语言的表达：面向视频-grounded 对话学习非言语线索的大规模多模态数据集 

**Authors**: Youngmin Kim, Jiwan Chung, Jisoo Kim, Sunghyun Lee, Sangkyu Lee, Junhyeok Kim, Cheoljong Yang, Youngjae Yu  

**Link**: [PDF](https://arxiv.org/pdf/2506.00958)  

**Abstract**: Nonverbal communication is integral to human interaction, with gestures, facial expressions, and body language conveying critical aspects of intent and emotion. However, existing large language models (LLMs) fail to effectively incorporate these nonverbal elements, limiting their capacity to create fully immersive conversational experiences. We introduce MARS, a multimodal language model designed to understand and generate nonverbal cues alongside text, bridging this gap in conversational AI. Our key innovation is VENUS, a large-scale dataset comprising annotated videos with time-aligned text, facial expressions, and body language. Leveraging VENUS, we train MARS with a next-token prediction objective, combining text with vector-quantized nonverbal representations to achieve multimodal understanding and generation within a unified framework. Based on various analyses of the VENUS datasets, we validate its substantial scale and high effectiveness. Our quantitative and qualitative results demonstrate that MARS successfully generates text and nonverbal languages, corresponding to conversational input. 

**Abstract (ZH)**: 非言语沟通是人类互动的重要组成部分，手势、面部表情和身体语言传达着意图和情感的关键方面。然而，现有的大型语言模型（LLMs）未能有效地融入这些非言语元素，限制了它们创造沉浸式对话体验的能力。我们提出了MARS，一个能够理解和生成非言语暗示的多模态语言模型，从而弥合对话AI的这一空白。我们的核心创新是VENUS，一个包含注释视频的大型数据集，这些视频的时间对齐文本、面部表情和身体语言进行了标注。借助VENUS，我们使用下一个词预测目标训练MARS，将文本与矢量量化非言语表示结合，实现了统一框架内的多模态理解和生成。基于VENUS数据集的各种分析，我们验证了其庞大的规模和高度的有效性。我们的定量和定性结果表明，MARS能够生成与对话输入对应的文本和非言语语言。 

---
# GIA-MIC: Multimodal Emotion Recognition with Gated Interactive Attention and Modality-Invariant Learning Constraints 

**Title (ZH)**: GIA-MIC：基于门控交互注意力和模态不变学习约束的多模态情感识别 

**Authors**: Jiajun He, Jinyi Mi, Tomoki Toda  

**Link**: [PDF](https://arxiv.org/pdf/2506.00865)  

**Abstract**: Multimodal emotion recognition (MER) extracts emotions from multimodal data, including visual, speech, and text inputs, playing a key role in human-computer interaction. Attention-based fusion methods dominate MER research, achieving strong classification performance. However, two key challenges remain: effectively extracting modality-specific features and capturing cross-modal similarities despite distribution differences caused by modality heterogeneity. To address these, we propose a gated interactive attention mechanism to adaptively extract modality-specific features while enhancing emotional information through pairwise interactions. Additionally, we introduce a modality-invariant generator to learn modality-invariant representations and constrain domain shifts by aligning cross-modal similarities. Experiments on IEMOCAP demonstrate that our method outperforms state-of-the-art MER approaches, achieving WA 80.7% and UA 81.3%. 

**Abstract (ZH)**: 多模态情绪识别（MER）从视觉、语音和文本等多种模态数据中提取情绪，对人机交互起着关键作用。基于注意力的融合方法主导着MER研究，取得了强大的分类性能。然而，仍存在两个关键挑战：有效提取模态特定特征以及在模态异质性导致的分布差异下捕获跨模态相似性。为解决这些问题，我们提出了一种门控交互注意力机制，以适应性地提取模态特定特征并通过对等交互增强情感信息。此外，我们引入了一种模态不变生成器，学习模态不变表示，并通过对齐跨模态相似性来约束领域移位。实验结果表明，我们的方法在IEMOCAP数据集上优于最先进的MER方法，实现了加权平均准确率80.7%和未标定平均准确率81.3%。 

---
# Efficient Egocentric Action Recognition with Multimodal Data 

**Title (ZH)**: 基于多模态数据的高效自我中心动作识别 

**Authors**: Marco Calzavara, Ard Kastrati, Matteo Macchini, Dushan Vasilevski, Roger Wattenhofer  

**Link**: [PDF](https://arxiv.org/pdf/2506.01757)  

**Abstract**: The increasing availability of wearable XR devices opens new perspectives for Egocentric Action Recognition (EAR) systems, which can provide deeper human understanding and situation awareness. However, deploying real-time algorithms on these devices can be challenging due to the inherent trade-offs between portability, battery life, and computational resources. In this work, we systematically analyze the impact of sampling frequency across different input modalities - RGB video and 3D hand pose - on egocentric action recognition performance and CPU usage. By exploring a range of configurations, we provide a comprehensive characterization of the trade-offs between accuracy and computational efficiency. Our findings reveal that reducing the sampling rate of RGB frames, when complemented with higher-frequency 3D hand pose input, can preserve high accuracy while significantly lowering CPU demands. Notably, we observe up to a 3x reduction in CPU usage with minimal to no loss in recognition performance. This highlights the potential of multimodal input strategies as a viable approach to achieving efficient, real-time EAR on XR devices. 

**Abstract (ZH)**: 穿戴式XR设备数量的增加为自视点动作识别系统（EAR）提供了新的视角，这些系统可以提供更深入的人类理解和情况意识。然而，由于轻便性、电池寿命和计算资源之间的固有权衡，将实时算法部署在这些设备上可能会颇具挑战性。在这项工作中，我们系统地分析了不同输入模态——RGB视频和3D手部姿态——的采样频率对自视点动作识别性能和CPU使用率的影响。通过探索多种配置，我们提供了关于准确性和计算效率之间权衡关系的全面Characterization。我们的研究发现，当与更高频率的3D手部姿态输入相结合时，降低RGB帧的采样率可以保持高精度的同时显著降低CPU需求。值得注意的是，我们观察到CPU使用率最多可降低3倍，同时识别性能基本保持不变。这突显了多模态输入策略作为在XR设备上实现高效实时自视点动作识别的可行方法的潜力。 

---
# Synthesis of discrete-continuous quantum circuits with multimodal diffusion models 

**Title (ZH)**: 多模扩散模型合成离散-连续量子电路 

**Authors**: Florian Fürrutter, Zohim Chandani, Ikko Hamamura, Hans J. Briegel, Gorka Muñoz-Gil  

**Link**: [PDF](https://arxiv.org/pdf/2506.01666)  

**Abstract**: Efficiently compiling quantum operations remains a major bottleneck in scaling quantum computing. Today's state-of-the-art methods achieve low compilation error by combining search algorithms with gradient-based parameter optimization, but they incur long runtimes and require multiple calls to quantum hardware or expensive classical simulations, making their scaling prohibitive. Recently, machine-learning models have emerged as an alternative, though they are currently restricted to discrete gate sets. Here, we introduce a multimodal denoising diffusion model that simultaneously generates a circuit's structure and its continuous parameters for compiling a target unitary. It leverages two independent diffusion processes, one for discrete gate selection and one for parameter prediction. We benchmark the model over different experiments, analyzing the method's accuracy across varying qubit counts, circuit depths, and proportions of parameterized gates. Finally, by exploiting its rapid circuit generation, we create large datasets of circuits for particular operations and use these to extract valuable heuristics that can help us discover new insights into quantum circuit synthesis. 

**Abstract (ZH)**: 高效编译量子操作仍然是扩展量子计算的主要瓶颈。当前最先进的方法通过将搜索算法与基于梯度的参数优化结合来实现低编译误差，但这些方法存在较长的运行时间和需要多次调用量子硬件或昂贵的经典模拟，从而使其实现扩展变得不可行。最近，机器学习模型作为替代方法出现，尽管它们目前仅限于离散门集。在这里，我们介绍了一种多模态去噪扩散模型，该模型同时生成目标酉矩阵的电路结构及其连续参数。该模型利用了两个独立的扩散过程，一个用于离散门的选择，另一个用于参数预测。我们在不同的实验中对标记模型进行了基准测试，分析了该方法在不同量子比特数量、电路深度和参数化门比例下的准确性。最后，通过利用其快速电路生成能力，我们创建了特定操作的大型电路数据集，并使用这些数据集提取有价值的启发式方法，以帮助我们发现量子电路合成的新见解。 

---
# ViTA-PAR: Visual and Textual Attribute Alignment with Attribute Prompting for Pedestrian Attribute Recognition 

**Title (ZH)**: ViTA-PAR：基于属性提示的视觉和文本属性对齐的人体属性识别 

**Authors**: Minjeong Park, Hongbeen Park, Jinkyu Kim  

**Link**: [PDF](https://arxiv.org/pdf/2506.01411)  

**Abstract**: The Pedestrian Attribute Recognition (PAR) task aims to identify various detailed attributes of an individual, such as clothing, accessories, and gender. To enhance PAR performance, a model must capture features ranging from coarse-grained global attributes (e.g., for identifying gender) to fine-grained local details (e.g., for recognizing accessories) that may appear in diverse regions. Recent research suggests that body part representation can enhance the model's robustness and accuracy, but these methods are often restricted to attribute classes within fixed horizontal regions, leading to degraded performance when attributes appear in varying or unexpected body locations. In this paper, we propose Visual and Textual Attribute Alignment with Attribute Prompting for Pedestrian Attribute Recognition, dubbed as ViTA-PAR, to enhance attribute recognition through specialized multimodal prompting and vision-language alignment. We introduce visual attribute prompts that capture global-to-local semantics, enabling diverse attribute representations. To enrich textual embeddings, we design a learnable prompt template, termed person and attribute context prompting, to learn person and attributes context. Finally, we align visual and textual attribute features for effective fusion. ViTA-PAR is validated on four PAR benchmarks, achieving competitive performance with efficient inference. We release our code and model at this https URL. 

**Abstract (ZH)**: 基于视觉和文本属性对齐及属性提示的行人属性识别（ViTA-PAR） 

---
# FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal Contextual Fusion 

**Title (ZH)**: FusionAudio-1.2M：面向细粒度音频标注的多模态上下文融合 

**Authors**: Shunian Chen, Xinyuan Xie, Zheshu Chen, Liyan Zhao, Owen Lee, Zhan Su, Qilin Sun, Benyou Wang  

**Link**: [PDF](https://arxiv.org/pdf/2506.01111)  

**Abstract**: High-quality, large-scale audio captioning is crucial for advancing audio understanding, yet current automated methods often generate captions that lack fine-grained detail and contextual accuracy, primarily due to their reliance on limited unimodal or superficial multimodal information. Drawing inspiration from human auditory perception, which adeptly integrates cross-modal cues and performs sophisticated auditory scene analysis, we introduce a novel two-stage automated pipeline. This pipeline first employs specialized pretrained models to extract diverse contextual cues (e.g., speech, music, general sounds, and visual information from associated video). A large language model (LLM) then synthesizes these rich, multimodal inputs to generate detailed and context-aware audio captions. Key contributions of this work include: (1) the proposed scalable method for fine-grained audio caption generation; (2) FusionAudio, a new large-scale dataset comprising 1.2 million such detailed captions, combined with 6 million QA pairs; and (3) enhanced audio models developed using FusionAudio, specifically a CLAP-based audio encoder with superior audio-text alignment and instruction following. This paper paves the way for more nuanced and accurate automated understanding of complex audio environments. Code and data can be found in this https URL. 

**Abstract (ZH)**: 高质量、大规模的音频描述对于推进音频理解至关重要，然而当前的自动化方法往往生成缺乏细致细节和上下文准确性的描述，主要原因是它们依赖于有限的单模态或表面化的多模态信息。借鉴人类听觉感知能够巧妙整合跨模态线索并执行复杂的听觉场景分析，我们提出了一种新颖的两阶段自动化管道。该管道首先使用专门的预训练模型提取多样的上下文线索（例如，语音、音乐、一般声音以及关联视频中的视觉信息）。之后，大规模语言模型（LLM）综合这些丰富的多模态输入生成详细且上下文相关的音频描述。本文的关键贡献包括：（1）提出的一种 scalable 的细粒度音频描述生成方法；（2）FusionAudio，一个包含 120 万条此类详细描述的新大规模数据集，结合了 600 万对 QA；（3）利用 FusionAudio 开发的增强音频模型，特别是基于 CLAP 的音频编码器，其具有更优的音频-文本对齐和指令跟随能力。本文为更细致和准确地理解复杂音频环境开辟了路径。代码和数据可在此处获取。 

---
# GThinker: Towards General Multimodal Reasoning via Cue-Guided Rethinking 

**Title (ZH)**: GThinker: 向 towards 通用多模态推理 via 基于提示的重组 Cue-Guided Rethinking 

**Authors**: Yufei Zhan, Ziheng Wu, Yousong Zhu, Rongkun Xue, Ruipu Luo, Zhenghao Chen, Can Zhang, Yifan Li, Zhentao He, Zheming Yang, Ming Tang, Minghui Qiu, Jinqiao Wang  

**Link**: [PDF](https://arxiv.org/pdf/2506.01078)  

**Abstract**: Despite notable advancements in multimodal reasoning, leading Multimodal Large Language Models (MLLMs) still underperform on vision-centric multimodal reasoning tasks in general scenarios. This shortfall stems from their predominant reliance on logic- and knowledge-based slow thinking strategies, while effective for domains like math and science, fail to integrate visual information effectively during reasoning. Consequently, these models often fail to adequately ground visual cues, resulting in suboptimal performance in tasks that require multiple plausible visual interpretations and inferences. To address this, we present GThinker (General Thinker), a novel reasoning MLLM excelling in multimodal reasoning across general scenarios, mathematics, and science. GThinker introduces Cue-Rethinking, a flexible reasoning pattern that grounds inferences in visual cues and iteratively reinterprets these cues to resolve inconsistencies. Building on this pattern, we further propose a two-stage training pipeline, including pattern-guided cold start and incentive reinforcement learning, designed to enable multimodal reasoning capabilities across domains. Furthermore, to support the training, we construct GThinker-11K, comprising 7K high-quality, iteratively-annotated reasoning paths and 4K curated reinforcement learning samples, filling the data gap toward general multimodal reasoning. Extensive experiments demonstrate that GThinker achieves 81.5% on the challenging comprehensive multimodal reasoning benchmark M$^3$CoT, surpassing the latest O4-mini model. It also shows an average improvement of 2.1% on general scenario multimodal reasoning benchmarks, while maintaining on-par performance in mathematical reasoning compared to counterpart advanced reasoning models. The code, model, and data will be released soon at this https URL. 

**Abstract (ZH)**: 尽管在多模态推理方面取得了显著进展，主流的多模态大型语言模型（MLLMs）在一般场景下的视觉中心多模态推理任务中仍然表现不佳。这一缺陷源于它们主要依赖于逻辑和知识为基础的慢思考策略，这些策略在数学和科学等领域有效，但无法有效地整合推理过程中的视觉信息。因此，这些模型往往无法充分地将视觉线索纳入推理中，导致在需要多个合理的视觉解释和推理的任务中表现不佳。为了解决这一问题，我们提出了GThinker（通用思考者），一种在一般场景、数学和科学领域都擅长多模态推理的新颖推理MLLM。GThinker引入了基于线索重新思考（Cue-Rethinking）的灵活推理模式，该模式以视觉线索为基础，迭代重新解释这些线索以解决不一致问题。基于这一模式，我们进一步提出了一种两阶段训练管道，包括模式引导的冷启动和激励强化学习，旨在促进跨领域的多模态推理能力。此外，为了支持训练，我们构建了包含7000条高质量、迭代注释的推理路径和4000个精心挑选的强化学习样本的GThinker-11K，以填补通用多模态推理的数据缺口。广泛的经验表明，GThinker在具有挑战性的综合多模态推理基准M$^3$CoT上达到了81.5%的得分，超越了最新的O4-mini模型。同时，它在一般场景下的多模态推理基准测试中的平均表现提高了2.1%，而在数学推理方面与同类先进的推理模型保持了相当的性能。代码、模型和数据将在不久后在此网址发布。 

---
# QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training 

**Title (ZH)**: QoQ-Med: 建立具有领域意识GRPO训练的多模态临床基础模型 

**Authors**: Wei Dai, Peilin Chen, Chanakya Ekbote, Paul Pu Liang  

**Link**: [PDF](https://arxiv.org/pdf/2506.00711)  

**Abstract**: Clinical decision-making routinely demands reasoning over heterogeneous data, yet existing multimodal language models (MLLMs) remain largely vision-centric and fail to generalize across clinical specialties. To bridge this gap, we introduce QoQ-Med-7B/32B, the first open generalist clinical foundation model that jointly reasons across medical images, time-series signals, and text reports. QoQ-Med is trained with Domain-aware Relative Policy Optimization (DRPO), a novel reinforcement-learning objective that hierarchically scales normalized rewards according to domain rarity and modality difficulty, mitigating performance imbalance caused by skewed clinical data distributions. Trained on 2.61 million instruction tuning pairs spanning 9 clinical domains, we show that DRPO training boosts diagnostic performance by 43% in macro-F1 on average across all visual domains as compared to other critic-free training methods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation data, it is able to highlight salient regions related to the diagnosis, with an IoU 10x higher than open models while reaching the performance of OpenAI o4-mini. To foster reproducibility and downstream research, we release (i) the full model weights, (ii) the modular training pipeline, and (iii) all intermediate reasoning traces at this https URL. 

**Abstract (ZH)**: 临床决策需要综合处理异构数据，现有跨模态语言模型主要侧重视觉信息且在临床专科间难以泛化。为了弥合这一差距，我们引入了QoQ-Med-7B/32B，这是首个能够联合推理医学图像、时序信号和文本报告的开放通用临床基础模型。QoQ-Med 使用一种名为域意识相对策略优化 (DRPO) 的新型强化学习目标进行训练，该目标根据领域稀有性和模态难度逐级调整标准化奖励，从而缓解由临床数据分布偏斜引起的性能失衡问题。在包含9个临床领域的261万条指令调优对的数据上进行训练，我们展示了DRPO训练相比其他无批评家训练方法（如GRPO）在宏观F1的平均诊断性能上提升了43%。此外，利用QoQ-Med 对密集分割数据进行训练，能够突出显示与诊断相关的显著区域，并且与开源模型相比，交集分割指标高出10倍，同时达到OpenAI o4-mini的性能。为了促进可重复性和下游研究，我们在此公开发布了（i）完整模型权重，（ii）模块化训练流水线，以及（iii）所有中间推理轨迹。 

---
# Enabling Chatbots with Eyes and Ears: An Immersive Multimodal Conversation System for Dynamic Interactions 

**Title (ZH)**: 赋予聊天机器人以耳闻目见的能力：一种用于动态交互的沉浸式多模态对话系统 

**Authors**: Jihyoung Jang, Minwook Bae, Minji Kim, Dilek Hakkani-Tur, Hyounghun Kim  

**Link**: [PDF](https://arxiv.org/pdf/2506.00421)  

**Abstract**: As chatbots continue to evolve toward human-like, real-world, interactions, multimodality remains an active area of research and exploration. So far, efforts to integrate multimodality into chatbots have primarily focused on image-centric tasks, such as visual dialogue and image-based instructions, placing emphasis on the "eyes" of human perception while neglecting the "ears", namely auditory aspects. Moreover, these studies often center around static interactions that focus on discussing the modality rather than naturally incorporating it into the conversation, which limits the richness of simultaneous, dynamic engagement. Furthermore, while multimodality has been explored in multi-party and multi-session conversations, task-specific constraints have hindered its seamless integration into dynamic, natural conversations. To address these challenges, this study aims to equip chatbots with "eyes and ears" capable of more immersive interactions with humans. As part of this effort, we introduce a new multimodal conversation dataset, Multimodal Multi-Session Multi-Party Conversation ($M^3C$), and propose a novel multimodal conversation model featuring multimodal memory retrieval. Our model, trained on the $M^3C$, demonstrates the ability to seamlessly engage in long-term conversations with multiple speakers in complex, real-world-like settings, effectively processing visual and auditory inputs to understand and respond appropriately. Human evaluations highlight the model's strong performance in maintaining coherent and dynamic interactions, demonstrating its potential for advanced multimodal conversational agents. 

**Abstract (ZH)**: 随着聊天机器人的不断发展，趋向于实现自然真实的人类交互，多模态交互仍然是一个活跃的研究领域。目前，将多模态集成到聊天机器人中主要集中在以图像为中心的任务上，如视觉对话和图像指令，侧重于人类感知的“眼睛”，而忽视了“耳朵”，即听觉方面。此外，这些研究往往集中于静态交互，侧重于讨论模态本身而非自然地将其融入对话中，限制了同时动态交互的丰富性。进一步而言，虽然多模态已经在多轮和多会话对话中被探索，但特定任务的约束限制了其在自然、动态对话中的无缝集成。为了应对这些挑战，本研究旨在赋予聊天机器人“眼睛和耳朵”，以实现更沉浸式的交互。作为此努力的一部分，我们引入了一个新的多模态多会话多对话语音聊天数据集($M^3C$)，并提出了一种具有多模态记忆检索的新型多模态对话模型。该模型在$M^3C$数据集上训练，能够无缝参与复杂的、现实世界的长对话，有效处理视觉和听觉输入，以理解并适当地回应。人类评估表明，该模型在保持连贯和动态交互方面表现出色，展示了其作为先进多模态对话代理的潜力。 

---
