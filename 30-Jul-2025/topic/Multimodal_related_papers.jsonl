{'arxiv_id': 'arXiv:2507.21881', 'title': 'Multi-Representation Diagrams for Pain Recognition: Integrating Various Electrodermal Activity Signals into a Single Image', 'authors': 'Stefanos Gkikas, Ioannis Kyprakis, Manolis Tsiknakis', 'link': 'https://arxiv.org/abs/2507.21881', 'abstract': "Pain is a multifaceted phenomenon that affects a substantial portion of the population. Reliable and consistent evaluation benefits those experiencing pain and underpins the development of effective and advanced management strategies. Automatic pain-assessment systems deliver continuous monitoring, inform clinical decision-making, and aim to reduce distress while preventing functional decline. By incorporating physiological signals, these systems provide objective, accurate insights into an individual's condition. This study has been submitted to the \\textit{Second Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The proposed method introduces a pipeline that leverages electrodermal activity signals as input modality. Multiple representations of the signal are created and visualized as waveforms, and they are jointly visualized within a single multi-representation diagram. Extensive experiments incorporating various processing and filtering techniques, along with multiple representation combinations, demonstrate the effectiveness of the proposed approach. It consistently yields comparable, and in several cases superior, results to traditional fusion methods, establishing it as a robust alternative for integrating different signal representations or modalities.", 'abstract_zh': '疼痛是一种多维度的现象，影响着大量人群。可靠的且一致的评估有助于减轻疼痛患者的痛苦，并支撑有效且先进的疼痛管理策略的发展。自动疼痛评估系统提供连续监控，助力临床决策，并旨在减少痛苦同时防止功能衰退。通过整合生理信号，这些系统提供了关于个体状况的客观且准确的见解。本研究提交给了“下一代疼痛评估 multimodal 感知 grand challenge (AI4PAIN)”。所提出的方法引入了一种管道，利用电导率活动信号作为输入模态。创建了多种信号表示，并以波形的形式可视化，同时在单一多表示图中联合可视化。结合多种处理和滤波技术以及多种表示组合进行的大量实验表明，所提出的方法具有有效性，并在多种情况下优于传统融合方法，确立了其作为整合不同信号表示或模态的 robust 替代方案的地位。', 'title_zh': '多种表示图用于疼痛识别：整合多种电生理活动信号成单一图像'}
{'arxiv_id': 'arXiv:2507.21872', 'title': 'MultiEditor: Controllable Multimodal Object Editing for Driving Scenarios Using 3D Gaussian Splatting Priors', 'authors': 'Shouyi Lu, Zihan Lin, Chao Lu, Huanran Wang, Guirong Zhuo, Lianqing Zheng', 'link': 'https://arxiv.org/abs/2507.21872', 'abstract': 'Autonomous driving systems rely heavily on multimodal perception data to understand complex environments. However, the long-tailed distribution of real-world data hinders generalization, especially for rare but safety-critical vehicle categories. To address this challenge, we propose MultiEditor, a dual-branch latent diffusion framework designed to edit images and LiDAR point clouds in driving scenarios jointly. At the core of our approach is introducing 3D Gaussian Splatting (3DGS) as a structural and appearance prior for target objects. Leveraging this prior, we design a multi-level appearance control mechanism--comprising pixel-level pasting, semantic-level guidance, and multi-branch refinement--to achieve high-fidelity reconstruction across modalities. We further propose a depth-guided deformable cross-modality condition module that adaptively enables mutual guidance between modalities using 3DGS-rendered depth, significantly enhancing cross-modality consistency. Extensive experiments demonstrate that MultiEditor achieves superior performance in visual and geometric fidelity, editing controllability, and cross-modality consistency. Furthermore, generating rare-category vehicle data with MultiEditor substantially enhances the detection accuracy of perception models on underrepresented classes.', 'abstract_zh': '自主驾驶系统高度依赖多模态感知数据以理解复杂环境。然而，实际数据的长尾分布阻碍了泛化能力，尤其是在处理稀少但至关安全的车辆类别时。为应对这一挑战，我们提出了MultiEditor，这是一种设计用于联合编辑驾驶场景中图像和LiDAR点云的双分支潜扩散框架。我们方法的核心在于引入3D高斯斑点生成（3DGS）作为目标物体的结构和外观先验。基于此先验，我们设计了一种多层次的外观控制机制——包括像素级别拼接、语义级别指导以及多分支细化——以实现跨模态的高保真重构。此外，我们还提出了一个基于深度的可变形跨模态条件模块，利用3DGS渲染的深度信息实现模态间的自适应互指导，显著增强了跨模态一致性。大量实验表明，MultiEditor在视觉和几何保真度、编辑可控性以及跨模态一致性方面取得了优异性能。进一步使用MultiEditor生成稀有类别车辆数据显著提高了感知模型对未充分代表类别的检测精度。', 'title_zh': 'MultiEditor：基于3D高斯斑点先验的可控多模态对象编辑在驾驶场景中应用'}
{'arxiv_id': 'arXiv:2507.21588', 'title': 'Progressive Homeostatic and Plastic Prompt Tuning for Audio-Visual Multi-Task Incremental Learning', 'authors': 'Jiong Yin, Liang Li, Jiehua Zhang, Yuhan Gao, Chenggang Yan, Xichun Sheng', 'link': 'https://arxiv.org/abs/2507.21588', 'abstract': 'Audio-visual multi-task incremental learning aims to continuously learn from multiple audio-visual tasks without the need for joint training on all tasks. The challenge of the problem is how to preserve the old task knowledge while facilitating the learning of new task with previous experiences. To address these challenges, we introduce a three-stage Progressive Homeostatic and Plastic audio-visual prompt (PHP) method. In the shallow phase, we design the task-shared modality aggregating adapter to foster cross-task and cross-modal audio-visual representation learning to enhance shared understanding between tasks. In the middle phase, we propose the task-specific modality-shared dynamic generating adapter, which constructs prompts that are tailored to individual tasks while remaining general across modalities, which balances the models ability to retain knowledge against forgetting with its potential for versatile multi-task transferability. In the deep phase, we introduce the task-specific modality-independent prompts to further refine the understand ability by targeting individual information for each task and modality. By incorporating these three phases, PHP retains task-specific prompts while adapting shared parameters for new tasks to effectively balance knowledge sharing and specificity. Our method achieves SOTA performance in different orders of four tasks (AVE, AVVP, AVS and AVQA). Our code can be available at this https URL.', 'abstract_zh': '视听多任务增量学习旨在无需联合训练所有任务的情况下，持续从多个视听任务中学习。该问题的挑战在于如何在促进新任务学习的同时保留旧任务的知识。为解决这些挑战，我们引入了一种三级动态平衡视听提示（PHP）方法。在浅层阶段，我们设计了任务共享模态聚合适配器，以促进跨任务和跨模态的视听表示学习，增强任务间的共享理解。在中间阶段，我们提出了任务特定模态共享动态生成适配器，该适配器构建针对个别任务的定制提示，同时保持跨模态的一般性，从而在保留知识与遗忘之间的能力与多任务转移的灵活性之间取得平衡。在深层阶段，我们引入了任务特定独立模态的提示，进一步通过针对每个任务和模态的特定信息来精炼理解能力。通过结合这三个阶段，PHP 保留了任务特定的提示，同时适应新的任务的共享参数，以有效平衡知识共享和特定性。我们的方法在四种任务（AVE、AVVP、AVS 和 AVQA）中实现了最佳性能。我们的代码可在以下链接获取：this https URL。', 'title_zh': '渐进稳态与塑性提示调谐用于音频-视觉多任务增量学习'}
{'arxiv_id': 'arXiv:2507.21395', 'title': 'Sync-TVA: A Graph-Attention Framework for Multimodal Emotion Recognition with Cross-Modal Fusion', 'authors': 'Zeyu Deng, Yanhui Lu, Jiashu Liao, Shuang Wu, Chongfeng Wei', 'link': 'https://arxiv.org/abs/2507.21395', 'abstract': 'Multimodal emotion recognition (MER) is crucial for enabling emotionally intelligent systems that perceive and respond to human emotions. However, existing methods suffer from limited cross-modal interaction and imbalanced contributions across modalities. To address these issues, we propose Sync-TVA, an end-to-end graph-attention framework featuring modality-specific dynamic enhancement and structured cross-modal fusion. Our design incorporates a dynamic enhancement module for each modality and constructs heterogeneous cross-modal graphs to model semantic relations across text, audio, and visual features. A cross-attention fusion mechanism further aligns multimodal cues for robust emotion inference. Experiments on MELD and IEMOCAP demonstrate consistent improvements over state-of-the-art models in both accuracy and weighted F1 score, especially under class-imbalanced conditions.', 'abstract_zh': '多模态情感识别中的同步时间视图（Sync-TVA）：一种端到端的图注意力框架', 'title_zh': 'Sync-TVA: 一种基于图注意力的跨模态融合多模态情绪识别框架'}
{'arxiv_id': 'arXiv:2507.21260', 'title': 'Adaptive Multimodal Protein Plug-and-Play with Diffusion-Based Priors', 'authors': 'Amartya Banerjee, Xingyu Xu, Caroline Moosmüller, Harlin Lee', 'link': 'https://arxiv.org/abs/2507.21260', 'abstract': 'In an inverse problem, the goal is to recover an unknown parameter (e.g., an image) that has typically undergone some lossy or noisy transformation during measurement. Recently, deep generative models, particularly diffusion models, have emerged as powerful priors for protein structure generation. However, integrating noisy experimental data from multiple sources to guide these models remains a significant challenge. Existing methods often require precise knowledge of experimental noise levels and manually tuned weights for each data modality. In this work, we introduce Adam-PnP, a Plug-and-Play framework that guides a pre-trained protein diffusion model using gradients from multiple, heterogeneous experimental sources. Our framework features an adaptive noise estimation scheme and a dynamic modality weighting mechanism integrated into the diffusion process, which reduce the need for manual hyperparameter tuning. Experiments on complex reconstruction tasks demonstrate significantly improved accuracy using Adam-PnP.', 'abstract_zh': '逆问题的目标是恢复在测量过程中通常已经经历了某种失真或噪声变换的未知参数（例如，图像）。近年来，特别是扩散模型，已经作为强大的先验知识被用于蛋白质结构生成。然而，将多种来源的嘈杂实验数据整合以指导这些模型仍然是一项重大挑战。现有方法通常需要精确了解实验噪声水平，并手动调整每种数据模态的权重。在本工作中，我们引入了Adam-PnP，这是一种插即用框架，使用来自多种异质实验源的梯度来引导预训练的蛋白质扩散模型。我们的框架包含一种自适应噪声估计方案和一种集成到扩散过程中的动态模态权重机制，减少了手动超参数调整的需要。实验结果表明，在复杂重构任务中使用Adam-PnP可以显著提高准确性。', 'title_zh': '基于扩散先验的自适应多模态蛋白质插拔方法'}
{'arxiv_id': 'arXiv:2507.21246', 'title': 'On Explaining Visual Captioning with Hybrid Markov Logic Networks', 'authors': 'Monika Shah, Somdeb Sarkhel, Deepak Venugopal', 'link': 'https://arxiv.org/abs/2507.21246', 'abstract': 'Deep Neural Networks (DNNs) have made tremendous progress in multimodal tasks such as image captioning. However, explaining/interpreting how these models integrate visual information, language information and knowledge representation to generate meaningful captions remains a challenging problem. Standard metrics to measure performance typically rely on comparing generated captions with human-written ones that may not provide a user with a deep insights into this integration. In this work, we develop a novel explanation framework that is easily interpretable based on Hybrid Markov Logic Networks (HMLNs) - a language that can combine symbolic rules with real-valued functions - where we hypothesize how relevant examples from the training data could have influenced the generation of the observed caption. To do this, we learn a HMLN distribution over the training instances and infer the shift in distributions over these instances when we condition on the generated sample which allows us to quantify which examples may have been a source of richer information to generate the observed caption. Our experiments on captions generated for several state-of-the-art captioning models using Amazon Mechanical Turk illustrate the interpretability of our explanations, and allow us to compare these models along the dimension of explainability.', 'abstract_zh': '深度神经网络在多模态任务如图像 captioning 方面取得了显著进展。然而，解释这些模型如何结合视觉信息、语言信息和知识表示以生成有意义的 caption 仍然是一项具有挑战性的问题。标准的性能度量通常依赖于将生成的 caption 与人工撰写的 caption 进行比较，这可能无法为用户提供深入理解这种整合的信息。在本文中，我们基于混合马尔可夫逻辑网络（HMLNs）开发了一种新的解释框架，该框架将符号规则与实值函数结合起来，易于解释。我们假设训练数据中的相关示例如何影响观察到的 caption 的生成。为此，我们学习了一个 HMLN 分布，并在条件化生成样本的情况下推断这些实例分布的变化，从而量化哪些示例可能是生成观察到的 caption 的信息来源。我们在 Amazon Mechanical Turk 上对几种先进 captioning 模型生成的 caption 进行的实验展示了我们解释的可解释性，并允许我们从解释性的角度比较这些模型。', 'title_zh': '基于混合马尔可夫逻辑网络的视觉 captions 解释'}
{'arxiv_id': 'arXiv:2507.21167', 'title': 'ChartM$^3$: Benchmarking Chart Editing with Multimodal Instructions', 'authors': 'Danglu Yang, Liang Zhang, Zihao Yue, Liangyu Chen, Yichen Xu, Wenxuan Wang, Qin Jin', 'link': 'https://arxiv.org/abs/2507.21167', 'abstract': 'Charts are a fundamental visualization format widely used in data analysis across research and industry. While enabling users to edit charts based on high-level intentions is of great practical value, existing methods primarily rely on natural language instructions, which are often too ambiguous to support fine-grained editing. In this work, we introduce a novel paradigm for multimodal chart editing, where user intent is expressed through a combination of natural language and visual indicators that explicitly highlight the elements to be modified. To support this paradigm, we present Chart$\\text{M}^3$, a new benchmark for Multimodal chart editing with Multi-level complexity and Multi-perspective evaluation. Chart$\\text{M}^3$ contains 1,000 samples spanning four levels of editing difficulty. Each sample includes triplets in the form of (chart, code, multimodal instructions). To comprehensively evaluate chart editing models, Chart$\\text{M}^3$ provides metrics that assess both visual appearance and code correctness. Our benchmark reveals significant limitations in current multimodal large language models (MLLMs), including GPT-4o, particularly in their ability to interpret and act on visual indicators. To address this, we construct Chart$\\text{M}^3$-Train, a large-scale training set with 24,000 multimodal chart editing samples. Fine-tuning MLLMs on this dataset leads to substantial improvements, demonstrating the importance of multimodal supervision in building practical chart editing systems. Our datasets, codes, and evaluation tools are available at this https URL. %this https URL datasets, codes, and evaluation tools are available at this https URL.', 'abstract_zh': '多模态图表编辑的新范式：Chart$\\text{M}^3$基准chnerise', 'title_zh': 'ChartM$^3$: 基于多模态指令的图表编辑基准测试'}
{'arxiv_id': 'arXiv:2507.21161', 'title': 'Seeing Beyond Frames: Zero-Shot Pedestrian Intention Prediction with Raw Temporal Video and Multimodal Cues', 'authors': 'Pallavi Zambare, Venkata Nikhil Thanikella, Ying Liu', 'link': 'https://arxiv.org/abs/2507.21161', 'abstract': 'Pedestrian intention prediction is essential for autonomous driving in complex urban environments. Conventional approaches depend on supervised learning over frame sequences and require extensive retraining to adapt to new scenarios. Here, we introduce BF-PIP (Beyond Frames Pedestrian Intention Prediction), a zero-shot approach built upon Gemini 2.5 Pro. It infers crossing intentions directly from short, continuous video clips enriched with structured JAAD metadata. In contrast to GPT-4V based methods that operate on discrete frames, BF-PIP processes uninterrupted temporal clips. It also incorporates bounding-box annotations and ego-vehicle speed via specialized multimodal prompts. Without any additional training, BF-PIP achieves 73% prediction accuracy, outperforming a GPT-4V baseline by 18 %. These findings illustrate that combining temporal video inputs with contextual cues enhances spatiotemporal perception and improves intent inference under ambiguous conditions. This approach paves the way for agile, retraining-free perception module in intelligent transportation system.', 'abstract_zh': '基于Gemini 2.5 Pro的Beyond Frames行人意图预测', 'title_zh': '超越框架：利用原始时序视频和多模态线索进行零样本行人意图预测'}
{'arxiv_id': 'arXiv:2507.21100', 'title': 'A Tactical Behaviour Recognition Framework Based on Causal Multimodal Reasoning: A Study on Covert Audio-Video Analysis Combining GAN Structure Enhancement and Phonetic Accent Modelling', 'authors': 'Wei Meng', 'link': 'https://arxiv.org/abs/2507.21100', 'abstract': 'This paper introduces TACTIC-GRAPHS, a system that combines spectral graph theory and multimodal graph neural reasoning for semantic understanding and threat detection in tactical video under high noise and weak structure. The framework incorporates spectral embedding, temporal causal edge modeling, and discriminative path inference across heterogeneous modalities. A semantic-aware keyframe extraction method fuses visual, acoustic, and action cues to construct temporal graphs. Using graph attention and Laplacian spectral mapping, the model performs cross-modal weighting and causal signal analysis. Experiments on TACTIC-AVS and TACTIC-Voice datasets show 89.3 percent accuracy in temporal alignment and over 85 percent recognition of complete threat chains, with node latency within plus-minus 150 milliseconds. The approach enhances structural interpretability and supports applications in surveillance, defense, and intelligent security systems.', 'abstract_zh': '这篇论文介绍了TACTIC-GRAPHS系统，该系统结合了谱图理论和多模态图神经推理，以在高噪声和弱结构环境下进行战术视频中的语义理解和威胁检测。该框架融合了谱嵌入、时间因果边建模以及异构模态下的判别路径推断。一种语义aware的关键帧提取方法融合了视觉、声学和动作线索，构建时间图。通过图注意和拉普拉斯谱映射，模型进行跨模态加权和因果信号分析。在TACTIC-AVS和TACTIC-Voice数据集上的实验显示，时间对齐的准确率为89.3%，完整威胁链的识别率超过85%，节点延迟在±150毫秒以内。该方法增强了结构可解释性，并支持监视、防御和智能安全系统中的应用。', 'title_zh': '基于因果多模态推理的战术行为识别框架：结合GAN结构增强和音韵重音建模的隐蔽音视频分析研究'}
