{'arxiv_id': 'arXiv:2507.21065', 'title': 'Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions', 'authors': 'Sabrina Patania, Luca Annese, Cansu Koyuturk, Azzurra Ruggeri, Dimitri Ognibene', 'link': 'https://arxiv.org/abs/2507.21065', 'abstract': "Large Language Models (LLMs) have demonstrated remarkable capabilities in processing extensive offline datasets. However, they often face challenges in acquiring and integrating complex, knowledge online. Traditional AI training paradigms, predominantly based on supervised learning or reinforcement learning, mirror a 'Piagetian' model of independent exploration. These approaches typically rely on large datasets and sparse feedback signals, limiting the models' ability to learn efficiently from interactions. Drawing inspiration from Vygotsky's sociocultural theory, this study explores the potential of socially mediated learning paradigms to address these limitations.\nWe introduce a dynamic environment, termed the 'AI Social Gym', where an AI learner agent engages in dyadic pedagogical dialogues with knowledgeable AI teacher agents. These interactions emphasize external, structured dialogue as a core mechanism for knowledge acquisition, contrasting with methods that depend solely on internal inference or pattern recognition.\nOur investigation focuses on how different pedagogical strategies impact the AI learning process in the context of ontology acquisition. Empirical results indicate that such dialogic approaches-particularly those involving mixed-direction interactions combining top-down explanations with learner-initiated questioning-significantly enhance the LLM's ability to acquire and apply new knowledge, outperforming both unidirectional instructional methods and direct access to structured knowledge, formats typically present in training datasets.\nThese findings suggest that integrating pedagogical and psychological insights into AI and robot training can substantially improve post-training knowledge acquisition and response quality. This approach offers a complementary pathway to existing strategies like prompt engineering", 'abstract_zh': '大型语言模型（LLMs）在处理大量离线数据集方面展现出了杰出的能力。然而，它们经常面临从复杂在线知识中获取并整合信息的挑战。传统的AI训练范式主要基于监督学习或强化学习，类似于皮亚杰的独立探索模型。这些方法通常依赖大型数据集和稀疏的反馈信号，限制了模型从交互中高效学习的能力。受到维果茨基的社会文化理论的启发，本研究探讨了社会介导学习范式在解决这些限制方面的潜在可能性。\n\n在这样的背景下，我们引入了一个动态环境，称为“AI社会健身房”，其中AI学习者代理与知识丰富的AI教师代理进行双向教学对话。这些互动强调外部结构化对话作为知识获取的核心机制，不同于仅依赖内部推理或模式识别的方法。\n\n我们的研究集中于不同的教学策略如何影响AI学习过程中的本体知识获取。实验证据表明，特别是结合自上而下解释与学习者主动提问的双向互动策略，显著增强了LLM从新知识中获取和应用的能力，超越了单向指导方法和直接访问结构化知识的现有方式，通常存在于训练数据集中。\n\n这些发现表明，在AI和机器人训练中整合教学和心理学的见解可以显著提高培训后知识获取和响应质量。这一方法为现有的策略，如提示工程，提供了互补途径。', 'title_zh': '人工代理的对话式社会学习：通过混合倡议教育交互增强大语言模型本体获取'}
{'arxiv_id': 'arXiv:2507.22034', 'title': 'UserBench: An Interactive Gym Environment for User-Centric Agents', 'authors': 'Cheng Qian, Zuxin Liu, Akshara Prabhakar, Zhiwei Liu, Jianguo Zhang, Haolin Chen, Heng Ji, Weiran Yao, Shelby Heinecke, Silvio Savarese, Caiming Xiong, Huan Wang', 'link': 'https://arxiv.org/abs/2507.22034', 'abstract': 'Large Language Models (LLMs)-based agents have made impressive progress in reasoning and tool use, enabling them to solve complex tasks. However, their ability to proactively collaborate with users, especially when goals are vague, evolving, or indirectly expressed, remains underexplored. To address this gap, we introduce UserBench, a user-centric benchmark designed to evaluate agents in multi-turn, preference-driven interactions. UserBench features simulated users who start with underspecified goals and reveal preferences incrementally, requiring agents to proactively clarify intent and make grounded decisions with tools. Our evaluation of leading open- and closed-source LLMs reveals a significant disconnect between task completion and user alignment. For instance, models provide answers that fully align with all user intents only 20% of the time on average, and even the most advanced models uncover fewer than 30% of all user preferences through active interaction. These results highlight the challenges of building agents that are not just capable task executors, but true collaborative partners. UserBench offers an interactive environment to measure and advance this critical capability.', 'abstract_zh': '基于大型语言模型的代理在推理和工具使用方面取得了显著进展，使它们能够解决复杂任务。然而，在目标模糊、不断演变或间接表达的情况下，它们与用户主动合作的能力仍未充分探索。为解决这一问题，我们引入了UserBench，这是一个以用户为中心的基准测试，旨在评估多轮、基于偏好的交互中的代理性能。UserBench包含模拟用户，这些用户初始时目标不明确，并逐步揭示偏好，要求代理主动澄清意图并利用工具进行基于实际情况的决策。我们对领先开源和闭源大型语言模型的评估表明，在任务完成和用户对齐之间存在显著差距。例如，模型提供的答案仅在平均20%的情况下完全符合所有用户意图，即使是最先进的模型，通过积极互动也仅能识别出所有用户偏好中的不到30%。这些结果突显了构建不仅是高效任务执行者，而是真正协作伙伴的代理所面临的挑战。UserBench提供了一个互动环境来衡量和推动这一关键能力。', 'title_zh': '用户基准：以用户为中心代理的交互式 gym 环境'}
{'arxiv_id': 'arXiv:2507.21976', 'title': 'The Effect of Compression Techniques on Large Multimodal Language Models in the Medical Domain', 'authors': 'Tanvir Ahmed Khan, Aranya Saha, Ismam Nur Swapnil, Mohammad Ariful Haque', 'link': 'https://arxiv.org/abs/2507.21976', 'abstract': 'Multimodal Large Language Models (MLLMs) hold huge potential for usage in the medical domain, but their computational costs necessitate efficient compression techniques. This paper evaluates the impact of structural pruning and activation-aware quantization on a fine-tuned LLAVA model for medical applications. We propose a novel layer selection method for pruning, analyze different quantization techniques, and assess the performance trade-offs in a prune-SFT-quantize pipeline. Our proposed method enables MLLMs with 7B parameters to run within 4 GB of VRAM, reducing memory usage by 70% while achieving 4% higher model performance compared to traditional pruning and quantization techniques in the same compression ratio.', 'abstract_zh': '多模态大型语言模型（MLLMs）在医疗领域的应用具有巨大潜力，但其计算成本需要高效的压缩技术。本文评估了结构剪枝和激活感知量化对 fine-tuned LLAVA 模型在医疗应用中的影响。我们提出了一种新颖的层选择方法进行剪枝，分析了不同的量化技术，并在剪枝-微调-量化流水线中评估了性能trade-offs。本文提出的方法使具有7B参数的MLLMs能够在4 GB VRAM 内运行，与传统剪枝和量化技术相比，在相同的压缩比下，内存使用减少了70%，同时模型性能提高了4%。', 'title_zh': '压缩技术对医疗领域大型多模态语言模型的影响'}
{'arxiv_id': 'arXiv:2507.21974', 'title': 'Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks', 'authors': 'Mohamed Sana, Nicola Piovesan, Antonio De Domenico, Yibin Kang, Haozhe Zhang, Merouane Debbah, Fadhel Ayed', 'link': 'https://arxiv.org/abs/2507.21974', 'abstract': 'Root Cause Analysis (RCA) in mobile networks remains a challenging task due to the need for interpretability, domain expertise, and causal reasoning. In this work, we propose a lightweight framework that leverages Large Language Models (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of annotated troubleshooting problems designed to benchmark RCA capabilities. Our evaluation reveals that existing open-source reasoning LLMs struggle with these problems, underscoring the need for domain-specific adaptation. To address this issue, we propose a two-stage training methodology that combines supervised fine-tuning with reinforcement learning to improve the accuracy and reasoning quality of LLMs. The proposed approach fine-tunes a series of RCA models to integrate domain knowledge and generate structured, multi-step diagnostic explanations, improving both interpretability and effectiveness. Extensive experiments across multiple LLM sizes show significant performance gains over state-of-the-art reasoning and non-reasoning models, including strong generalization to randomized test variants. These results demonstrate the promise of domain-adapted, reasoning-enhanced LLMs for practical and explainable RCA in network operation and management.', 'abstract_zh': '基于大型语言模型的移动网络根因分析轻量级框架', 'title_zh': '5G无线网络根因分析的语言模型推理'}
{'arxiv_id': 'arXiv:2507.21929', 'title': 'Libra: Large Chinese-based Safeguard for AI Content', 'authors': 'Ziyang Chen, Huimu Yu, Xing Wu, Dongqin Liu, Songlin Hu', 'link': 'https://arxiv.org/abs/2507.21929', 'abstract': 'Large language models (LLMs) excel in text understanding and generation but raise significant safety and ethical concerns in high-stakes applications. To mitigate these risks, we present Libra-Guard, a cutting-edge safeguard system designed to enhance the safety of Chinese-based LLMs. Leveraging a two-stage curriculum training pipeline, Libra-Guard enhances data efficiency by employing guard pretraining on synthetic samples, followed by fine-tuning on high-quality, real-world data, thereby significantly reducing reliance on manual annotations. To enable rigorous safety evaluations, we also introduce Libra-Test, the first benchmark specifically designed to evaluate the effectiveness of safeguard systems for Chinese content. It covers seven critical harm scenarios and includes over 5,700 samples annotated by domain experts. Experiments show that Libra-Guard achieves 86.79% accuracy, outperforming Qwen2.5-14B-Instruct (74.33%) and ShieldLM-Qwen-14B-Chat (65.69%), and nearing closed-source models like Claude-3.5-Sonnet and GPT-4o. These contributions establish a robust framework for advancing the safety governance of Chinese LLMs and represent a tentative step toward developing safer, more reliable Chinese AI systems.', 'abstract_zh': '大型语言模型（LLMs）在文本理解和生成方面表现出色，但在高风险应用中引发了重大安全和伦理关切。为减轻这些风险，我们提出Libra-Guard，一种先进防护系统，旨在提升基于中文的LLMs的安全性。Libra-Guard利用两阶段课程训练管道，通过在合成样本上进行防护预训练，然后在高质量的真实世界数据上进行微调，从而显著减少对人工标注的依赖。为了进行严格的安全性评估，我们还引入了Libra-Test，这是首个专门用于评估防护系统效果的基准测试，涵盖七大关键危害场景，并包含超过5,700个由领域专家标注的样本。实验结果显示，Libra-Guard的准确性达到86.79%，优于Qwen2.5-14B-Instruct（74.33%）和ShieldLM-Qwen-14B-Chat（65.69%），接近于像Claude-3.5-Sonnet和GPT-4o这样的闭源模型。这些贡献为推动中文LLMs的安全治理建立了一个坚实框架，并代表了朝着开发更安全、更可靠中文AI系统迈出的一小步。', 'title_zh': '.Libra：基于中文的大规模AI内容保护方法'}
{'arxiv_id': 'arXiv:2507.21899', 'title': 'LLM-based Content Classification Approach for GitHub Repositories by the README Files', 'authors': 'Malik Uzair Mehmood, Shahid Hussain, Wen Li Wang, Muhammad Usama Malik', 'link': 'https://arxiv.org/abs/2507.21899', 'abstract': "GitHub is the world's most popular platform for storing, sharing, and managing code. Every GitHub repository has a README file associated with it. The README files should contain project-related information as per the recommendations of GitHub to support the usage and improvement of repositories. However, GitHub repository owners sometimes neglected these recommendations. This prevents a GitHub repository from reaching its full potential. This research posits that the comprehensiveness of a GitHub repository's README file significantly influences its adoption and utilization, with a lack of detail potentially hindering its full potential for widespread engagement and impact within the research community. Large Language Models (LLMs) have shown great performance in many text-based tasks including text classification, text generation, text summarization and text translation. In this study, an approach is developed to fine-tune LLMs for automatically classifying different sections of GitHub README files. Three encoder-only LLMs are utilized, including BERT, DistilBERT and RoBERTa. These pre-trained models are then fine-tuned based on a gold-standard dataset consisting of 4226 README file sections. This approach outperforms current state-of-the-art methods and has achieved an overall F1 score of 0.98. Moreover, we have also investigated the use of Parameter-Efficient Fine-Tuning (PEFT) techniques like Low-Rank Adaptation (LoRA) and shown an economical alternative to full fine-tuning without compromising much performance. The results demonstrate the potential of using LLMs in designing an automatic classifier for categorizing the content of GitHub README files. Consequently, this study contributes to the development of automated tools for GitHub repositories to improve their identifications and potential usages.", 'abstract_zh': 'GitHub是全球最受欢迎的代码存储、共享和管理平台。每个GitHub仓库都关联有一个README文件。README文件应包含与GitHub推荐相符的项目相关信息，以支持仓库的使用和改进。然而，有时GitHub仓库所有者会忽略这些推荐。这阻碍了仓库充分发挥其潜力。本文假定GitHub仓库README文件的全面性显著影响其采用和利用率，缺乏细节可能会阻碍其广泛参与和在研究社区中的影响。大规模语言模型（LLMs）在许多基于文本的任务中表现出色，包括文本分类、文本生成、文本摘要和文本翻译。本研究开发了一种方法，将LLMs微调以自动分类GitHub README文件的不同部分。利用了三个仅编码器的LLMs，包括BERT、DistilBERT和RoBERTa。这些预训练模型基于包含4226个README文件部分的黄金标准数据集进行了微调。该方法在当前最先进的方法中表现出色，实现了整体F1分数为0.98。此外，我们还探讨了使用参数高效微调（PEFT）技术，如低秩适应（LoRA），展示了在不牺牲太多性能的情况下，一种经济的全微调替代方案。结果表明，使用LLMs设计自动分类GitHub README文件内容的分类器的潜力。因此，本研究为GitHub仓库开发自动工具的改进奠定了基础，以提高其识别和潜在用途。', 'title_zh': '基于LLM的内容分类方法：基于GitHub仓库README文件的分类方法'}
{'arxiv_id': 'arXiv:2507.21848', 'title': 'EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity', 'authors': 'Xingjian Zhang, Siwei Wen, Wenjun Wu, Lei Huang', 'link': 'https://arxiv.org/abs/2507.21848', 'abstract': 'Large Language Models (LLMs) have made remarkable progress in enhancing step-by-step reasoning through reinforcement learning. However, the Group Relative Policy Optimization (GRPO) algorithm, which relies on sparse reward rules, often encounters the issue of identical rewards within groups, leading to the advantage collapse problem. Existing works typically address this challenge from two perspectives: enforcing model reflection to enhance response diversity, and introducing internal feedback to augment the training signal (advantage). In this work, we begin by analyzing the limitations of model reflection and investigating the policy entropy of responses at the fine-grained sample level. Based on our experimental findings, we propose the EDGE-GRPO algorithm, which adopts \\textbf{E}ntropy-\\textbf{D}riven Advantage and \\textbf{G}uided \\textbf{E}rror Correction to effectively mitigate the problem of advantage collapse. Extensive experiments on several main reasoning benchmarks demonstrate the effectiveness and superiority of our approach. It is available at this https URL.', 'abstract_zh': '大型语言模型（LLMs）通过强化学习增强了逐步推理能力，然而，依赖稀疏奖励规则的群组相对策略优化（GRPO）算法常常遇到组内奖励一致导致的优势崩塌问题。现有工作通常从两种视角应对这一挑战：强化模型反思以提高响应多样性，以及引入内部反馈以增强训练信号（优势）。在本工作中，我们首先分析了模型反思的局限性，并在细粒度样本级别上研究了响应的策略熵。基于实验发现，我们提出了EDGE-GRPO算法，该算法采用基于熵的优势和引导错误校正（Entropy-Driven Advantage and Guided Error Correction）有效地缓解了优势崩塌问题。在多个主要推理基准上的广泛实验展示了我们方法的有效性和优越性。详情请见：this https URL。', 'title_zh': 'EDGE-GRPO：熵驱动的误差校正引导的GRPO以促进优势多样性'}
{'arxiv_id': 'arXiv:2507.21830', 'title': 'DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series Forecasting Framework', 'authors': 'Kuiye Ding, Fanda Fan, Yao Wang, Ruijie jian, Xiaorui Wang, Luqi Gong, Yishan Jiang, Chunjie Luo an Jianfeng Zhan', 'link': 'https://arxiv.org/abs/2507.21830', 'abstract': 'Multivariate Time Series Forecasting plays a key role in many applications. Recent works have explored using Large Language Models for MTSF to take advantage of their reasoning abilities. However, many methods treat LLMs as end-to-end forecasters, which often leads to a loss of numerical precision and forces LLMs to handle patterns beyond their intended design. Alternatively, methods that attempt to align textual and time series modalities within latent space frequently encounter alignment difficulty. In this paper, we propose to treat LLMs not as standalone forecasters, but as semantic guidance modules within a dual-stream framework. We propose DualSG, a dual-stream framework that provides explicit semantic guidance, where LLMs act as Semantic Guides to refine rather than replace traditional predictions. As part of DualSG, we introduce Time Series Caption, an explicit prompt format that summarizes trend patterns in natural language and provides interpretable context for LLMs, rather than relying on implicit alignment between text and time series in the latent space. We also design a caption-guided fusion module that explicitly models inter-variable relationships while reducing noise and computation. Experiments on real-world datasets from diverse domains show that DualSG consistently outperforms 15 state-of-the-art baselines, demonstrating the value of explicitly combining numerical forecasting with semantic guidance.', 'abstract_zh': '多变量时间序列预测在许多应用中发挥着关键作用。近年来的研究探索了使用大型语言模型进行多变量时间序列预测（MTSF），以利用其推理能力。然而，许多方法将大型语言模型视为端到端的预测器，这往往会损失数值精度，并迫使大型语言模型处理超出其设计意图的模式。另一种方法试图在潜在空间内对文本和时间序列模式进行对齐，但经常遇到对齐困难。在本文中，我们提出将大型语言模型不视为独立的预测器，而是作为双流框架内的语义指导模块。我们提出了DualSG双流框架，提供明确的语义指导，其中大型语言模型充当语义导向模块以精炼而非替代传统预测。作为DualSG的一部分，我们引入了时间序列描述这一明确的提示格式，以自然语言总结趋势模式并为大型语言模型提供可解释的上下文，而不是依赖潜在空间中文本与时间序列之间的隐式对齐。我们还设计了一个描述导向融合模块，明确建模变量间关系并减少噪声和计算量。实验结果表明，DualSG在来自不同领域的实际数据集上始终优于15个最先进的基线方法，证实了将数值预测与语义指导明确结合的价值。', 'title_zh': '双流显式语义引导的多变量时间序列forecasting框架'}
{'arxiv_id': 'arXiv:2507.21753', 'title': 'Towards a rigorous evaluation of RAG systems: the challenge of due diligence', 'authors': 'Grégoire Martinon, Alexandra Lorenzo de Brionne, Jérôme Bohard, Antoine Lojou, Damien Hervault, Nicolas J-B. Brunel', 'link': 'https://arxiv.org/abs/2507.21753', 'abstract': 'The rise of generative AI, has driven significant advancements in high-risk sectors like healthcare and finance. The Retrieval-Augmented Generation (RAG) architecture, combining language models (LLMs) with search engines, is particularly notable for its ability to generate responses from document corpora. Despite its potential, the reliability of RAG systems in critical contexts remains a concern, with issues such as hallucinations persisting. This study evaluates a RAG system used in due diligence for an investment fund. We propose a robust evaluation protocol combining human annotations and LLM-Judge annotations to identify system failures, like hallucinations, off-topic, failed citations, and abstentions. Inspired by the Prediction Powered Inference (PPI) method, we achieve precise performance measurements with statistical guarantees. We provide a comprehensive dataset for further analysis. Our contributions aim to enhance the reliability and scalability of RAG systems evaluation protocols in industrial applications.', 'abstract_zh': '生成式AI的兴起推动了医疗和金融等高风险领域的显著进步。检索增强生成（RAG）架构，结合语言模型（LLMs）和搜索引擎，特别因其能够从文档集合中生成响应而备受瞩目。尽管其潜力巨大，但在关键场景中RAG系统的可靠性仍是值得关注的问题，幻觉等问题仍然存在。本研究评估了一种应用于投资基金尽职调查的RAG系统。我们提出了一种结合人类标注和LLM-Judge标注的稳健评估协议，以识别系统故障，如幻觉、离题、引用失败和弃权。受预测驱动的推理（PPI）方法的启发，我们实现了具有统计保证的精确性能测量。我们提供了一个全面的数据集供进一步分析。我们的贡献旨在增强RAG系统评估协议在工业应用中的可靠性和扩展性。', 'title_zh': '向RAG系统严格评估的迈进：尽职调查挑战'}
{'arxiv_id': 'arXiv:2507.21636', 'title': 'StaffPro: an LLM Agent for Joint Staffing and Profiling', 'authors': 'Alessio Maritan', 'link': 'https://arxiv.org/abs/2507.21636', 'abstract': "Large language model (LLM) agents integrate pre-trained LLMs with modular algorithmic components and have shown remarkable reasoning and decision-making abilities. In this work, we investigate their use for two tightly intertwined challenges in workforce management: staffing, i.e., the assignment and scheduling of tasks to workers, which may require team formation; and profiling, i.e., the continuous estimation of workers' skills, preferences, and other latent attributes from unstructured data. We cast these problems in a formal mathematical framework that links scheduling decisions to latent feature estimation, and we introduce StaffPro, an LLM agent that addresses staffing and profiling jointly. Differently from existing staffing solutions, StaffPro allows expressing optimization objectives using natural language, accepts textual task descriptions and provides high flexibility. StaffPro interacts directly with humans by establishing a continuous human-agent feedback loop, ensuring natural and intuitive use. By analyzing human feedback, our agent continuously estimates the latent features of workers, realizing life-long worker profiling and ensuring optimal staffing performance over time. A consulting firm simulation example demonstrates that StaffPro successfully estimates workers' attributes and generates high quality schedules. With its innovative design, StaffPro offers a robust, interpretable, and human-centric solution for automated personnel management.", 'abstract_zh': '大型语言模型代理（LLM代理）结合预训练的LLM和模块化算法组件，并展示了出色的推理和决策能力。本文探讨了它们在劳动力管理中两个紧密相关的挑战的应用：排班，即任务分配和调度，可能需要团队组建；和建模，即从非结构化数据中持续估计工人的技能、偏好及其他潜在属性。我们将这些问题置于一个形式化的数学框架中，将调度决策与潜在特征估计联系起来，并介绍了StaffPro，这是一种联合解决排班和建模问题的LLM代理。与现有的排班解决方案不同，StaffPro允许使用自然语言表达优化目标，接受文本任务描述并提供高灵活性。StaffPro通过建立持续的人机反馈循环直接与人类交互，确保自然且直观的使用。通过分析人类反馈，我们的代理连续估计工人的潜在特征，实现终身工人建模，并确保随着时间推移实现最优的排班性能。咨询公司模拟案例表明，StaffPro成功估计了工人的属性并生成了高质量的排班。凭借其创新设计，StaffPro提供了自动化人员管理的稳健、可解释且以人为本的解决方案。', 'title_zh': 'StaffPro: 一个用于联合人员配置和画像的LLM代理'}
{'arxiv_id': 'arXiv:2507.21524', 'title': 'Large Language Models for Wireless Communications: From Adaptation to Autonomy', 'authors': 'Le Liang, Hao Ye, Yucheng Sheng, Ouya Wang, Jiacheng Wang, Shi Jin, Geoffrey Ye Li', 'link': 'https://arxiv.org/abs/2507.21524', 'abstract': 'The emergence of large language models (LLMs) has revolutionized artificial intelligence, offering unprecedented capabilities in reasoning, generalization, and zero-shot learning. These strengths open new frontiers in wireless communications, where increasing complexity and dynamics demand intelligent and adaptive solutions. This article explores the role of LLMs in transforming wireless systems across three key directions: adapting pretrained LLMs for core communication tasks, developing wireless-specific foundation models to balance versatility and efficiency, and enabling agentic LLMs with autonomous reasoning and coordination capabilities. We highlight recent advances, practical case studies, and the unique benefits of LLM-based approaches over traditional methods. Finally, we outline open challenges and research opportunities, including multimodal fusion, collaboration with lightweight models, and self-improving capabilities, charting a path toward intelligent, adaptive, and autonomous wireless networks of the future.', 'abstract_zh': '大型语言模型的 emergence 与发展革新了人工智能，在推理、泛化和零样本学习方面提供了前所未有的能力。这些优势为无线通信领域带来了新的前沿挑战，尤其是在日益复杂的动态环境中，需要智能和自适应的解决方案。本文探讨了大型语言模型在无线系统三个关键方向上的转型作用：适应预训练大型语言模型以执行核心通信任务、开发针对无线通信领域的基础模型以平衡多功能性和效率，以及启用具有自主推理和协调能力的大型语言模型。我们强调了最近的进展、实用案例研究以及基于大型语言模型方法的独特优势，相对于传统方法的优势。最后，我们概述了开放的挑战和研究机会，包括多模态融合、与轻量化模型协作以及自我提升能力，绘制了一条通向智能、自适应和自主无线网络的未来之路。', 'title_zh': '无线通信中的大规模语言模型：从适应到自主'}
{'arxiv_id': 'arXiv:2507.21503', 'title': 'MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions', 'authors': 'Yanxu Zhu, Shitong Duan, Xiangxu Zhang, Jitao Sang, Peng Zhang, Tun Lu, Xiao Zhou, Jing Yao, Xiaoyuan Yi, Xing Xie', 'link': 'https://arxiv.org/abs/2507.21503', 'abstract': "Recently Multimodal Large Language Models (MLLMs) have achieved considerable advancements in vision-language tasks, yet produce potentially harmful or untrustworthy content. Despite substantial work investigating the trustworthiness of language models, MMLMs' capability to act honestly, especially when faced with visually unanswerable questions, remains largely underexplored. This work presents the first systematic assessment of honesty behaviors across various MLLMs. We ground honesty in models' response behaviors to unanswerable visual questions, define four representative types of such questions, and construct MoHoBench, a large-scale MMLM honest benchmark, consisting of 12k+ visual question samples, whose quality is guaranteed by multi-stage filtering and human verification. Using MoHoBench, we benchmarked the honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our findings show that: (1) most models fail to appropriately refuse to answer when necessary, and (2) MMLMs' honesty is not solely a language modeling issue, but is deeply influenced by visual information, necessitating the development of dedicated methods for multimodal honesty alignment. Therefore, we implemented initial alignment methods using supervised and preference learning to improve honesty behavior, providing a foundation for future work on trustworthy MLLMs. Our data and code can be found at this https URL.", 'abstract_zh': '近年来，多模态大型语言模型（MLLMs）在视觉-语言任务中取得了显著进展，但往往会生成潜在有害或不可靠的内容。尽管已经开展了大量研究探讨语言模型的可信度，但MLLMs在面对视觉不可回答的问题时是否能够诚实行事的能力仍 largely未被探索。本文首次系统评估了各种MLLMs的诚实行为。我们基于模型对不可回答的视觉问题的响应行为定义了四种代表性问题类型，并构建了 MoHoBench，一个包含超过 12000 个视觉问题样本的大规模 MLLMs 诚实基准，其质量通过多阶段过滤和人工验证得以保证。使用 MoHoBench，我们对 28 个流行的 MLLMs 进行了基准测试并进行了全面分析。我们的研究发现表明：（1）大多数模型在必要时未能恰当拒绝回答，（2）MLLMs 的诚实不仅仅是一个语言建模问题，还受到视觉信息的深刻影响，需要开发专门的多模态诚实对齐方法。因此，我们采用了监督学习和偏好学习的方法实施了初步的诚实对齐方法，以改善诚实行为，为未来可信赖的 MLLMs 研究奠定了基础。我们的数据和代码可在以下网址找到。', 'title_zh': 'MoHoBench: 通过不可答的视觉问题评估多模态大型语言模型的诚实性'}
{'arxiv_id': 'arXiv:2507.21502', 'title': 'Large Language Models for Supply Chain Decisions', 'authors': 'David Simchi-Levi, Konstantina Mellou, Ishai Menache, Jeevan Pathuri', 'link': 'https://arxiv.org/abs/2507.21502', 'abstract': "Supply Chain Management requires addressing a variety of complex decision-making challenges, from sourcing strategies to planning and execution. Over the last few decades, advances in computation and information technologies have enabled the transition from manual, intuition and experience-based decision-making, into more automated and data-driven decisions using a variety of tools that apply optimization techniques. These techniques use mathematical methods to improve decision-making.\nUnfortunately, business planners and executives still need to spend considerable time and effort to (i) understand and explain the recommendations coming out of these technologies; (ii) analyze various scenarios and answer what-if questions; and (iii) update the mathematical models used in these tools to reflect current business environments. Addressing these challenges requires involving data science teams and/or the technology providers to explain results or make the necessary changes in the technology and hence significantly slows down decision making.\nMotivated by the recent advances in Large Language Models (LLMs), we report how this disruptive technology can democratize supply chain technology - namely, facilitate the understanding of tools' outcomes, as well as the interaction with supply chain tools without human-in-the-loop. Specifically, we report how we apply LLMs to address the three challenges described above, thus substantially reducing the time to decision from days and weeks to minutes and hours as well as dramatically increasing planners' and executives' productivity and impact.", 'abstract_zh': '基于大型语言模型的进展，我们报告了这项颠覆性技术如何使供应链技术 democratize——即促进对工具结果的理解，以及与供应链工具的交互，无需人工介入。具体而言，我们报告了如何应用大型语言模型来解决上述三个挑战，从而将决策时间从几天或几周缩短到几分钟或几小时，显著提高规划者和执行者的生产力和影响力。', 'title_zh': '大型语言模型在供应链决策中的应用'}
{'arxiv_id': 'arXiv:2507.21471', 'title': 'An LLM Driven Agent Framework for Automated Infrared Spectral Multi Task Reasoning', 'authors': 'Zujie Xie, Zixuan Chen, Jiheng Liang, Xiangyang Yu, Ziru Yu', 'link': 'https://arxiv.org/abs/2507.21471', 'abstract': "Infrared spectroscopy offers rapid, non destructive measurement of chemical and material properties but suffers from high dimensional, overlapping spectral bands that challenge conventional chemometric approaches. Emerging large language models (LLMs), with their capacity for generalization and reasoning, offer promising potential for automating complex scientific workflows. Despite this promise, their application in IR spectral analysis remains largely unexplored. This study addresses the critical challenge of achieving accurate, automated infrared spectral interpretation under low-data conditions using an LLM-driven framework. We introduce an end-to-end, large language model driven agent framework that integrates a structured literature knowledge base, automated spectral preprocessing, feature extraction, and multi task reasoning in a unified pipeline. By querying a curated corpus of peer reviewed IR publications, the agent selects scientifically validated routines. The selected methods transform each spectrum into low dimensional feature sets, which are fed into few shot prompt templates for classification, regression, and anomaly detection. A closed loop, multi turn protocol iteratively appends mispredicted samples to the prompt, enabling dynamic refinement of predictions. Across diverse materials: stamp pad ink, Chinese medicine, Pu'er tea, Citri Reticulatae Pericarpium and waste water COD datasets, the multi turn LLM consistently outperforms single turn inference, rivaling or exceeding machine learning and deep learning models under low data regimes.", 'abstract_zh': '基于大语言模型的红外光谱自动化解析框架：在低数据条件下的准确诠释', 'title_zh': '基于LLM驱动的代理框架实现红外光谱多任务推理自动化'}
{'arxiv_id': 'arXiv:2507.21453', 'title': 'Validating Pharmacogenomics Generative Artificial Intelligence Query Prompts Using Retrieval-Augmented Generation (RAG)', 'authors': 'Ashley Rector, Keaton Minor, Kamden Minor, Jeff McCormack, Beth Breeden, Ryan Nowers, Jay Dorris', 'link': 'https://arxiv.org/abs/2507.21453', 'abstract': "This study evaluated Sherpa Rx, an artificial intelligence tool leveraging large language models and retrieval-augmented generation (RAG) for pharmacogenomics, to validate its performance on key response metrics. Sherpa Rx integrated Clinical Pharmacogenetics Implementation Consortium (CPIC) guidelines with Pharmacogenomics Knowledgebase (PharmGKB) data to generate contextually relevant responses. A dataset (N=260 queries) spanning 26 CPIC guidelines was used to evaluate drug-gene interactions, dosing recommendations, and therapeutic implications. In Phase 1, only CPIC data was embedded. Phase 2 additionally incorporated PharmGKB content. Responses were scored on accuracy, relevance, clarity, completeness (5-point Likert scale), and recall. Wilcoxon signed-rank tests compared accuracy between Phase 1 and Phase 2, and between Phase 2 and ChatGPT-4omini. A 20-question quiz assessed the tool's real-world applicability against other models. In Phase 1 (N=260), Sherpa Rx demonstrated high performance of accuracy 4.9, relevance 5.0, clarity 5.0, completeness 4.8, and recall 0.99. The subset analysis (N=20) showed improvements in accuracy (4.6 vs. 4.4, Phase 2 vs. Phase 1 subset) and completeness (5.0 vs. 4.8). ChatGPT-4omini performed comparably in relevance (5.0) and clarity (4.9) but lagged in accuracy (3.9) and completeness (4.2). Differences in accuracy between Phase 1 and Phase 2 was not statistically significant. However, Phase 2 significantly outperformed ChatGPT-4omini. On the 20-question quiz, Sherpa Rx achieved 90% accuracy, outperforming other models. Integrating additional resources like CPIC and PharmGKB with RAG enhances AI accuracy and performance. This study highlights the transformative potential of generative AI like Sherpa Rx in pharmacogenomics, improving decision-making with accurate, personalized responses.", 'abstract_zh': '本研究评估了Sherpa Rx这一人工智能工具的表现，该工具利用大型语言模型和检索增强生成（RAG）技术应用于药代遗传学领域，并验证其在关键响应指标上的性能。Sherpa Rx将临床药代遗传学实施 consortium（CPIC）指南与药代遗传学知识库（PharmGKB）数据相结合，生成上下文相关响应。研究使用包含26个CPIC指南的260个查询数据集来评估药物-基因相互作用、剂量建议和治疗意义。第一阶段仅嵌入CPIC数据，第二阶段额外纳入PharmGKB内容。响应依据准确度、相关性、清晰度、完整性和召回率（5级李克特量表）进行评分。威尔科克森符号秩检验比较了第一阶段与第二阶段以及第二阶段与ChatGPT-4omini的准确度。进行了一项包含20个问题的测试评估该工具的实际应用性，对比其他模型。在第一阶段（N=260），Sherpa Rx在准确度（4.9）、相关性（5.0）、清晰度（5.0）、完整度（4.8）和召回率（0.99）方面表现出高水平。子集分析（N=20）显示准确度（4.6 vs. 4.4，第二阶段 vs. 第一阶段子集）和完整度（5.0 vs. 4.8）有所提高。ChatGPT-4omini在相关性（5.0）和清晰度（4.9）方面表现相当，但在准确度（3.9）和完整度（4.2）方面表现不佳。第一阶段和第二阶段之间准确度差异无统计学意义，但第二阶段显著优于ChatGPT-4omini。在20个问题的测试中，Sherpa Rx获得了90%的准确率，超过了其他模型。将CPIC和PharmGKB等额外资源与RAG结合使用，提高了AI的准确性和性能。本研究突显了如Sherpa Rx这类生成式AI在药代遗传学领域的变革潜力，通过提供准确、个性化的响应来改善决策。', 'title_zh': '使用检索增强生成（RAG）验证药代基因组学生成人工智能查询提示的有效性'}
{'arxiv_id': 'arXiv:2507.21438', 'title': 'Evo-DKD: Dual-Knowledge Decoding for Autonomous Ontology Evolution in Large Language Models', 'authors': 'Vishal Raman, Vijai Aravindh R', 'link': 'https://arxiv.org/abs/2507.21438', 'abstract': "Ontologies and knowledge graphs require continuous evolution to remain comprehensive and accurate, but manual curation is labor intensive. Large Language Models (LLMs) possess vast unstructured knowledge but struggle with maintaining structured consistency. We propose Evo-DKD, a novel dual-decoder framework for autonomous ontology evolution that combines structured ontology traversal with unstructured text reasoning. Evo-DKD introduces two parallel decoding streams within an LLM: one decoder generates candidate ontology edits (e.g., new concepts or relations) while the other produces natural-language justifications. A dynamic attention-based gating mechanism coordinates the two streams, deciding at each step how to blend structured and unstructured knowledge. Due to GPU constraints, we simulate the dual-decoder behavior using prompt-based mode control to approximate coordinated decoding in a single-stream mode. The system operates in a closed reasoning loop: proposed ontology edits are validated (via consistency checks and cross-verification with the text explanations) and then injected into the knowledge base, which in turn informs subsequent reasoning. We demonstrate Evo-DKD's effectiveness on use cases including healthcare ontology refinement, semantic search improvement, and cultural heritage timeline modeling. Experiments show that Evo-DKD outperforms baselines using structured-only or unstructured-only decoding in both precision of ontology updates and downstream task performance. We present quantitative metrics and qualitative examples, confirming the contributions of the dual-decoder design and gating router. Evo-DKD offers a new paradigm for LLM-driven knowledge base maintenance, combining the strengths of symbolic and neural reasoning for sustainable ontology evolution.", 'abstract_zh': '基于双解码器框架的自主本体演化方法 Evo-DKD', 'title_zh': 'Evo-DKD：自主大语言模型中双知识解码的本体演化方法'}
{'arxiv_id': 'arXiv:2507.21419', 'title': 'GovRelBench:A Benchmark for Government Domain Relevance', 'authors': 'Haiquan Wang, Yi Chen, Shang Zeng, Yun Bian, Zhe Cui', 'link': 'https://arxiv.org/abs/2507.21419', 'abstract': "Current evaluations of LLMs in the government domain primarily focus on safety considerations in specific scenarios, while the assessment of the models' own core capabilities, particularly domain relevance, remains insufficient. To address this gap, we propose GovRelBench, a benchmark specifically designed for evaluating the core capabilities of LLMs in the government domain. GovRelBench consists of government domain prompts and a dedicated evaluation tool, GovRelBERT. During the training process of GovRelBERT, we introduce the SoftGovScore method: this method trains a model based on the ModernBERT architecture by converting hard labels to soft scores, enabling it to accurately compute the text's government domain relevance score. This work aims to enhance the capability evaluation framework for large models in the government domain, providing an effective tool for relevant research and practice. Our code and dataset are available at this https URL.", 'abstract_zh': '政府领域中当前对大型语言模型的评估主要集中在特定场景下的安全性考量，而对其自身核心能力，尤其是领域相关性的评估仍显不足。为填补这一空白，我们提出了GovRelBench，这是一种专门用于评估政府领域大型语言模型核心能力的基准工具。GovRelBench 包括政府领域提示和专门的评估工具 GovRelBERT。在 GovRelBERT 的训练过程中，我们引入了 SoftGovScore 方法：该方法基于 ModernBERT 架构，通过将硬标签转换为软分数训练模型，使其能够精确计算文本在政府领域的相关性分数。本工作旨在提升政府领域大型模型能力评估框架，并提供有效的研究与实践工具。我们的代码和数据集可通过此链接获得。', 'title_zh': 'GovRelBench：政府领域相关性基准'}
{'arxiv_id': 'arXiv:2507.21407', 'title': 'Graph-Augmented Large Language Model Agents: Current Progress and Future Prospects', 'authors': 'Yixin Liu, Guibin Zhang, Kun Wang, Shiyuan Li, Shirui Pan', 'link': 'https://arxiv.org/abs/2507.21407', 'abstract': 'Autonomous agents based on large language models (LLMs) have demonstrated impressive capabilities in a wide range of applications, including web navigation, software development, and embodied control. While most LLMs are limited in several key agentic procedures, such as reliable planning, long-term memory, tool management, and multi-agent coordination, graphs can serve as a powerful auxiliary structure to enhance structure, continuity, and coordination in complex agent workflows. Given the rapid growth and fragmentation of research on Graph-augmented LLM Agents (GLA), this paper offers a timely and comprehensive overview of recent advances and also highlights key directions for future work. Specifically, we categorize existing GLA methods by their primary functions in LLM agent systems, including planning, memory, and tool usage, and then analyze how graphs and graph learning algorithms contribute to each. For multi-agent systems, we further discuss how GLA solutions facilitate the orchestration, efficiency optimization, and trustworthiness of MAS. Finally, we highlight key future directions to advance this field, from improving structural adaptability to enabling unified, scalable, and multimodal GLA systems. We hope this paper can serve as a roadmap for future research on GLA and foster a deeper understanding of the role of graphs in LLM agent systems.', 'abstract_zh': '基于大型语言模型的自主代理：图增强方法综述及未来方向', 'title_zh': '图增强大型语言模型代理：当前进展与未来前景'}
{'arxiv_id': 'arXiv:2507.21406', 'title': 'Shapley Uncertainty in Natural Language Generation', 'authors': 'Meilin Zhu, Gaojie Jin, Xiaowei Huang, Lijun Zhang', 'link': 'https://arxiv.org/abs/2507.21406', 'abstract': 'In question-answering tasks, determining when to trust the outputs is crucial to the alignment of large language models (LLMs). Kuhn et al. (2023) introduces semantic entropy as a measure of uncertainty, by incorporating linguistic invariances from the same meaning. It primarily relies on setting threshold to measure the level of semantic equivalence relation. We propose a more nuanced framework that extends beyond such thresholding by developing a Shapley-based uncertainty metric that captures the continuous nature of semantic relationships. We establish three fundamental properties that characterize valid uncertainty metrics and prove that our Shapley uncertainty satisfies these criteria. Through extensive experiments, we demonstrate that our Shapley uncertainty more accurately predicts LLM performance in question-answering and other datasets, compared to similar baseline measures.', 'abstract_zh': '在问答任务中，确定何时信任模型输出对于大型语言模型（LLMs）的对齐至关重要。Kuhn等（2023）通过引入语义熵作为不确定性度量，利用相同含义的语言不变性。该方法主要依赖于设置阈值来衡量语义等价关系的水平。我们提出一个更加精细化的框架，超越这种阈值方法，通过开发基于Shapley值的不确定性度量来捕捉语义关系的连续性。我们确立了三个基本属性来表征有效的不确定性度量，并证明我们的Shapley不确定性满足这些标准。通过大量实验，我们证明了我们的Shapley不确定性在问答和其他数据集上更准确地预测了LLM的表现，与类似的基线度量相比。', 'title_zh': '自然语言生成中的Shapley不确定性'}
{'arxiv_id': 'arXiv:2507.21389', 'title': 'Teaching Language Models To Gather Information Proactively', 'authors': 'Tenghao Huang, Sihao Chen, Muhao Chen, Jonathan May, Longqi Yang, Mengting Wan, Pei Zhou', 'link': 'https://arxiv.org/abs/2507.21389', 'abstract': 'Large language models (LLMs) are increasingly expected to function as collaborative partners, engaging in back-and-forth dialogue to solve complex, ambiguous problems. However, current LLMs often falter in real-world settings, defaulting to passive responses or narrow clarifications when faced with incomplete or under-specified prompts, falling short of proactively gathering the missing information that is crucial for high-quality solutions. In this work, we introduce a new task paradigm: proactive information gathering, where LLMs must identify gaps in the provided context and strategically elicit implicit user knowledge through targeted questions. To systematically study and train this capability, we design a scalable framework that generates partially specified, real-world tasks, masking key information and simulating authentic ambiguity. Within this setup, our core innovation is a reinforcement finetuning strategy that rewards questions that elicit genuinely new, implicit user information -- such as hidden domain expertise or fine-grained requirements -- that would otherwise remain unspoken. Experiments demonstrate that our trained Qwen-2.5-7B model significantly outperforms o3-mini by 18% on automatic evaluation metrics. More importantly, human evaluation reveals that clarification questions and final outlines generated by our model are favored by human annotators by 42% and 28% respectively. Together, these results highlight the value of proactive clarification in elevating LLMs from passive text generators to genuinely collaborative thought partners.', 'abstract_zh': '大型语言模型（LLMs）日益被期望作为协作伙伴，在前后对话中解决复杂、模糊的问题。然而，当前的LLMs在实际应用中往往在面对不完整或描述不清的提示时表现不佳，倾向于给出被动反应或狭窄澄清，未能主动收集高质量解决方案所需的关键缺失信息。本文介绍了一个新的任务范式：主动信息收集，要求LLMs识别提供的背景中的信息缺口，并通过针对性的问题来策略性地激发用户的隐含知识。为系统地研究和训练这一能力，我们设计了一个可扩展的框架，生成部分指定的、现实世界中的任务，屏蔽关键信息并模拟真实的模糊性。在这种设置中，我们的核心创新是一个强化微调策略，奖励那些激发真正新且隐含用户信息的问题，例如隐藏的专业知识或细粒度的要求，这些信息否则将不会被提及。实验结果显示，我们训练的Qwen-2.5-7B模型在自动评估指标上比o3-mini高18%。更重要的是，人工评估表明，由我们的模型生成的澄清问题和最终概述分别获得了42%和28%的人工注释者青睐。这些结果共同强调了主动澄清的价值，使LLMs从被动文本生成者转变为真正的合作思维伙伴。', 'title_zh': '教学语言模型主动获取信息'}
{'arxiv_id': 'arXiv:2507.21354', 'title': 'Games Agents Play: Towards Transactional Analysis in LLM-based Multi-Agent Systems', 'authors': 'Monika Zamojska, Jarosław A. Chudziak', 'link': 'https://arxiv.org/abs/2507.21354', 'abstract': "Multi-Agent Systems (MAS) are increasingly used to simulate social interactions, but most of the frameworks miss the underlying cognitive complexity of human behavior. In this paper, we introduce Trans-ACT (Transactional Analysis Cognitive Toolkit), an approach embedding Transactional Analysis (TA) principles into MAS to generate agents with realistic psychological dynamics. Trans-ACT integrates the Parent, Adult, and Child ego states into an agent's cognitive architecture. Each ego state retrieves context-specific memories and uses them to shape response to new situations. The final answer is chosen according to the underlying life script of the agent. Our experimental simulation, which reproduces the Stupid game scenario, demonstrates that agents grounded in cognitive and TA principles produce deeper and context-aware interactions. Looking ahead, our research opens a new way for a variety of applications, including conflict resolution, educational support, and advanced social psychology studies.", 'abstract_zh': '多Agent系统（MAS）越来越多地用于模拟社会互动，但大多数框架忽略了人类行为的内在认知复杂性。本文介绍了Transaction Analysis Cognitive Toolkit（Trans-ACT）方法，将Transaction Analysis（TA）原则嵌入到MAS中，以生成具有现实心理动态的代理。Trans-ACT将父母、成人和儿童自我状态整合到代理的认知架构中。每个自我状态检索特定于上下文的记忆，并使用它们来塑造对新情况的响应。最终的答案根据代理的基本生活脚本进行选择。我们的实验仿真重现了愚蠢游戏场景，表明基于认知和TA原则的代理能够产生更深和情境相关的互动。展望未来，我们的研究为冲突解决、教育支持和高级社会心理学研究等多种应用开辟了新的途径。', 'title_zh': '游戏中的代理法则：迈向基于LLM的多代理系统中的交易分析'}
{'arxiv_id': 'arXiv:2507.21285', 'title': 'Curiosity by Design: An LLM-based Coding Assistant Asking Clarification Questions', 'authors': 'Harsh Darji, Thibaud Lutellier', 'link': 'https://arxiv.org/abs/2507.21285', 'abstract': "Large Language Models (LLMs) are increasingly used as coding assistants. However, the ambiguity of the developer's prompt often leads to incorrect code generation, as current models struggle to infer user intent without extensive prompt engineering or external context. This work aims to build an LLM-based coding assistant that mimics the human code review process by asking clarification questions when faced with ambiguous or under-specified queries.\nOur end-to-end system includes (1) a query classifier trained to detect unclear programming-related queries and (2) a fine-tuned LLM that generates clarification questions. Our evaluation shows that the fine-tuned LLM outperforms standard zero-shot prompting in generating useful clarification questions. Furthermore, our user study indicates that users find the clarification questions generated by our model to outperform the baseline, demonstrating that our coding assistant produces more accurate and helpful code responses compared to baseline coding assistants.", 'abstract_zh': '大型语言模型（LLMs）越来越多地被用作编程助手。然而，开发者的模糊提示往往导致代码生成错误，因为当前模型在没有大量提示工程或外部上下文的情况下难以推断用户意图。本文旨在构建一个基于LLM的编程助手，该助手在面对模糊或描述不足的查询时会询问澄清问题，以模仿人类的代码审查过程。我们的端到端系统包括（1）一个查询分类器，训练用于检测含糊的编程相关查询，以及（2）一个微调后的LLM，用于生成澄清问题。我们的评估表明，微调后的LLM在生成有用的澄清问题方面优于标准的零-shot提示。此外，我们的用户研究显示，用户认为由我们的模型生成的澄清问题优于基线，这表明我们的编程助手相比于基线编程助手能产生更准确和有帮助的代码响应。', 'title_zh': '设计好奇心：基于LLM的编程助手提问澄清问题'}
{'arxiv_id': 'arXiv:2507.21276', 'title': 'LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems', 'authors': 'Yufei Li, Zexin Li, Yinglun Zhu, Cong Liu', 'link': 'https://arxiv.org/abs/2507.21276', 'abstract': 'Modern deployment of large language models (LLMs) frequently involves both inference serving and continuous retraining to stay aligned with evolving data and user feedback. Common practices separate these workloads onto distinct servers in isolated phases, causing substantial inefficiencies (e.g., GPU idleness) and delayed adaptation to new data in distributed settings. Our empirical analysis reveals that these inefficiencies stem from dynamic request arrivals during serving and workload heterogeneity in pipeline-parallel training. To address these challenges, we propose LeMix, a system for co-locating and managing concurrent LLM serving and training workloads. LeMix integrates offline profiling, execution prediction mechanisms, and runtime scheduling to dynamically adapt resource allocation based on workload characteristics and system conditions. By understanding task-specific behaviors and co-execution interference across shared nodes, LeMix improves utilization and serving quality without compromising serving responsiveness. Our evaluation shows that LeMix improves throughput by up to 3.53x, reduces inference loss by up to 0.61x, and delivers up to 2.12x higher response time SLO attainment over traditional separate setups. To our knowledge, this is the first work to uncover and exploit the opportunities of joint LLM inference and training, paving the way for more resource-efficient deployment of LLMs in production environments.', 'abstract_zh': '现代大规模语言模型的部署通常涉及推理服务和连续重训练，以保持与 evolving 数据和用户反馈的一致性。常见的做法是将这些工作负载分离到不同的服务器上，在孤立的阶段进行处理，导致了显著的低效（例如，GPU空闲）并在分布式环境中延迟了对新数据的适应。我们的实证分析表明，这些低效源于服务过程中的动态请求到达和管道并行训练中的工作负载异构性。为解决这些挑战，我们提出了一种名为 LeMix 的系统，用于同时管理和调度并发的语言模型服务和训练工作负载。LeMix 结合了离线分析、执行预测机制和运行时调度，根据工作负载特征和系统条件动态调整资源分配。通过理解特定任务的行为以及在共享节点上的并发执行干扰，LeMix 在不牺牲响应性的前提下提高了利用率和服务质量。我们的评估结果表明，与传统的分离设置相比，LeMix 可以将吞吐量提高 3.53 倍，降低推理损失 0.61 倍，并实现响应时间 SLO 达成率提高 2.12 倍。据我们所知，这是首次研究和利用联合语言模型推理和训练的机会，为生产环境中的大规模语言模型更高效的部署铺平了道路。', 'title_zh': 'LeMix: 统一调度框架用于多GPU系统中的LLM训练与推理'}
{'arxiv_id': 'arXiv:2507.21257', 'title': 'CompoST: A Benchmark for Analyzing the Ability of LLMs To Compositionally Interpret Questions in a QALD Setting', 'authors': 'David Maria Schmidt, Raoul Schubert, Philipp Cimiano', 'link': 'https://arxiv.org/abs/2507.21257', 'abstract': 'Language interpretation is a compositional process, in which the meaning of more complex linguistic structures is inferred from the meaning of their parts. Large language models possess remarkable language interpretation capabilities and have been successfully applied to interpret questions by mapping them to SPARQL queries. An open question is how systematic this interpretation process is. Toward this question, in this paper, we propose a benchmark for investigating to what extent the abilities of LLMs to interpret questions are actually compositional. For this, we generate three datasets of varying difficulty based on graph patterns in DBpedia, relying on Lemon lexica for verbalization. Our datasets are created in a very controlled fashion in order to test the ability of LLMs to interpret structurally complex questions, given that they have seen the atomic building blocks. This allows us to evaluate to what degree LLMs are able to interpret complex questions for which they "understand" the atomic parts. We conduct experiments with models of different sizes using both various prompt and few-shot optimization techniques as well as fine-tuning. Our results show that performance in terms of macro $F_1$ degrades from $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the samples optimized on. Even when all necessary information was provided to the model in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of lowest complexity. We thus conclude that LLMs struggle to systematically and compositionally interpret questions and map them into SPARQL queries.', 'abstract_zh': '语言解释是一个组合过程，在这个过程中，更复杂的语言结构的意义是从其组成部分的意义中推断出来的。大规模语言模型具备显著的语言解释能力，并且成功地被应用到通过将其映射到SPARQL查询上解释问题中。一个开放的问题是这个解释过程是否系统化。为此，本文提出一个基准，用于调查语言模型解释问题的能力实际上在多大程度上是组合性的。为此，我们基于DBpedia中的图模式生成三个不同难度级别的数据集，并依赖于Lemon词典进行言语化。我们的数据集以非常受控的方式创建，以测试语言模型在给定它们见过的原子构建块的情况下解释结构复杂问题的能力。这使我们能够评估语言模型在理解原子部分的情况下，解释复杂问题的程度。我们使用不同大小的模型进行实验，并结合使用各种提示技术和少量示例优化以及微调方法。我们的结果显示，随着与优化样本的偏差增加，宏$F_1$值从0.45下降到0.26，再下降到0.09。即使在输入中提供了所有必要信息，对于最简单复杂度的数据集，$F_1$得分也不超过0.57。因此，我们得出结论，语言模型在系统性和组合性地解释问题并将它们映射到SPARQL查询方面存在困难。', 'title_zh': 'CompoST：在QALD环境中分析LLMs组合解释问题能力的基准'}
{'arxiv_id': 'arXiv:2507.21176', 'title': "Tell Me You're Biased Without Telling Me You're Biased -- Toward Revealing Implicit Biases in Medical LLMs", 'authors': 'Farzana Islam Adiba, Rahmatollah Beheshti', 'link': 'https://arxiv.org/abs/2507.21176', 'abstract': 'Large language models (LLMs) that are used in medical applications are known to show biased and unfair patterns. Prior to adopting these in clinical decision-making applications, it is crucial to identify these bias patterns to enable effective mitigation of their impact. In this study, we present a novel framework combining knowledge graphs (KGs) with auxiliary LLMs to systematically reveal complex bias patterns in medical LLMs. Specifically, the proposed approach integrates adversarial perturbation techniques to identify subtle bias patterns. The approach adopts a customized multi-hop characterization of KGs to enhance the systematic evaluation of arbitrary LLMs. Through a series of comprehensive experiments (on three datasets, six LLMs, and five bias types), we show that our proposed framework has noticeably greater ability and scalability to reveal complex biased patterns of LLMs compared to other baselines.', 'abstract_zh': '大规模语言模型在医疗应用中显示出偏见和不公平模式。在将其应用于临床决策之前，识别这些偏见模式以有效减轻其影响至关重要。本研究提出了一种结合知识图谱与辅助语言模型的新框架，以系统地揭示医疗语言模型中的复杂偏见模式。具体而言，该方法整合了对抗性扰动技术以识别微妙的偏见模式。采用定制化的多跳表征知识图谱以增强任意语言模型的系统评估能力。通过一系列全面的实验（涉及三种数据集、六种语言模型和五种偏见类型），我们证明所提出框架在揭示语言模型的复杂偏见模式方面具有明显更强的能力和可扩展性，与其他基准方法相比。', 'title_zh': '不说破偏见——揭示医疗LLM中的隐性偏见'}
{'arxiv_id': 'arXiv:2507.21162', 'title': 'Large Language Model Powered Automated Modeling and Optimization of Active Distribution Network Dispatch Problems', 'authors': 'Xu Yang, Chenhui Lin, Yue Yang, Qi Wang, Haotian Liu, Haizhou Hua, Wenchuan Wu', 'link': 'https://arxiv.org/abs/2507.21162', 'abstract': 'The increasing penetration of distributed energy resources into active distribution networks (ADNs) has made effective ADN dispatch imperative. However, the numerous newly-integrated ADN operators, such as distribution system aggregators, virtual power plant managers, and end prosumers, often lack specialized expertise in power system operation, modeling, optimization, and programming. This knowledge gap renders reliance on human experts both costly and time-intensive. To address this challenge and enable intelligent, flexible ADN dispatch, this paper proposes a large language model (LLM) powered automated modeling and optimization approach. First, the ADN dispatch problems are decomposed into sequential stages, and a multi-LLM coordination architecture is designed. This framework comprises an Information Extractor, a Problem Formulator, and a Code Programmer, tasked with information retrieval, optimization problem formulation, and code implementation, respectively. Afterwards, tailored refinement techniques are developed for each LLM agent, greatly improving the accuracy and reliability of generated content. The proposed approach features a user-centric interface that enables ADN operators to derive dispatch strategies via simple natural language queries, eliminating technical barriers and increasing efficiency. Comprehensive comparisons and end-to-end demonstrations on various test cases validate the effectiveness of the proposed architecture and methods.', 'abstract_zh': '分布式能源资源日益融入活跃配电网络（ADNs）使得有效的ADN调度变得必不可少。然而，众多新集成的ADN运营商，如配电系统聚合商、虚拟电厂管理者和终端产消者，往往缺乏电力系统操作、建模、优化和编程的专业知识。这种知识缺口使得依赖人类专家变得既昂贵又耗时。为应对这一挑战并实现智能、灵活的ADN调度，本文提出了一种由大型语言模型（LLM）驱动的自动化建模和优化方法。首先，将ADN调度问题分解为一系列阶段，并设计了一个多LLM协调架构。该框架包括信息提取器、问题构建筑师和代码编程器，分别负责信息检索、优化问题建模和代码实现。随后，为每个LLM代理开发了定制的细化技术，大幅提高了生成内容的准确性和可靠性。该提出的方案具有用户中心的界面，使ADN运营商能够通过简单的自然语言查询生成调度策略，从而消除技术障碍并提高效率。综合比较和端到端演示在多种测试案例上的有效性验证了所提出架构和方法的有效性。', 'title_zh': '由大规模语言模型驱动的主动配电网调度问题自动化建模与优化'}
{'arxiv_id': 'arXiv:2507.21159', 'title': 'Adaptive Cluster Collaborativeness Boosts LLMs Medical Decision Support Capacity', 'authors': 'Zhihao Peng, Liuxin Bao, Shengyuan Liu, Yixuan Yuan', 'link': 'https://arxiv.org/abs/2507.21159', 'abstract': 'The collaborativeness of large language models (LLMs) has proven effective in natural language processing systems, holding considerable promise for healthcare development. However, it lacks explicit component selection rules, necessitating human intervention or clinical-specific validation. Moreover, existing architectures heavily rely on a predefined LLM cluster, where partial LLMs underperform in medical decision support scenarios, invalidating the collaborativeness of LLMs. To this end, we propose an adaptive cluster collaborativeness methodology involving self-diversity and cross-consistency maximization mechanisms to boost LLMs medical decision support capacity. For the self-diversity, we calculate the fuzzy matching value of pairwise outputs within an LLM as its self-diversity value, subsequently prioritizing LLMs with high self-diversity values as cluster components in a training-free manner. For the cross-consistency, we first measure cross-consistency values between the LLM with the highest self-diversity value and others, and then gradually mask out the LLM having the lowest cross-consistency value to eliminate the potential inconsistent output during the collaborative propagation. Extensive experiments on two specialized medical datasets, NEJMQA and MMLU-Pro-health, demonstrate the effectiveness of our method across physician-oriented specialties. For example, on NEJMQA, our method achieves the accuracy rate up to the publicly official passing score across all disciplines, especially achieving ACC of 65.47\\% compared to the 56.12\\% achieved by GPT-4 on the Obstetrics and Gynecology discipline.', 'abstract_zh': '大型语言模型的协作性在自然语言处理系统中表现有效，对医疗健康的发展充满潜力。然而，缺乏明确的组件选择规则，需要人工干预或针对临床的具体验证。此外，现有架构严重依赖预定义的大型语言模型集群，其中部分大型语言模型在医疗决策支持场景下表现不佳，质疑了大型语言模型协作性的有效性。为此，我们提出了一种自适应集群协作性方法，该方法通过最大化自我多样性和跨一致性机制来提升大型语言模型的医疗决策支持能力。对于自我多样性，我们通过计算大型语言模型内成对输出的模糊匹配值来确定其自我多样性值，并在无需训练的情况下优先选择高自我多样性值的大型语言模型作为集群组件。对于跨一致性，我们首先测量具有最高自我多样性值的大型语言模型与其他大型语言模型之间的跨一致性值，然后逐步掩盖具有最低跨一致性值的大型语言模型，以消除协作传播过程中潜在的一致性输出冲突。在两个专门的医疗数据集NEJMQA和MMLU-Pro-health上的广泛实验表明，我们的方法在面向医生的专业领域中表现出有效性。例如，在NEJMQA数据集上，我们的方法在全部学科中达到了公开官方通过分数线的准确率，特别是在妇产科学科中，我们的方法的准确率（ACC）达到了65.47%，而GPT-4的准确率为56.12%。', 'title_zh': '自适应集群协作性增强大语言模型在医学决策支持中的能力'}
{'arxiv_id': 'arXiv:2507.21141', 'title': 'The Geometry of Harmfulness in LLMs through Subconcept Probing', 'authors': "McNair Shah, Saleena Angeline, Adhitya Rajendra Kumar, Naitik Chheda, Kevin Zhu, Vasu Sharma, Sean O'Brien, Will Cai", 'link': 'https://arxiv.org/abs/2507.21141', 'abstract': "Recent advances in large language models (LLMs) have intensified the need to understand and reliably curb their harmful behaviours. We introduce a multidimensional framework for probing and steering harmful content in model internals. For each of 55 distinct harmfulness subconcepts (e.g., racial hate, employment scams, weapons), we learn a linear probe, yielding 55 interpretable directions in activation space. Collectively, these directions span a harmfulness subspace that we show is strikingly low-rank. We then test ablation of the entire subspace from model internals, as well as steering and ablation in the subspace's dominant direction. We find that dominant direction steering allows for near elimination of harmfulness with a low decrease in utility. Our findings advance the emerging view that concept subspaces provide a scalable lens on LLM behaviour and offer practical tools for the community to audit and harden future generations of language models.", 'abstract_zh': 'Recent advances in大规模语言模型（LLMs）加强了理解并可靠地遏制其有害行为的必要性。我们引入了一个多维度框架，用于探测和引导模型内部的有害内容。对于55个不同的有害性子概念（如种族仇恨、就业诈骗、武器），我们学习了一个线性探测器，生成了55个可解释的方向向量。这些方向共同构成了一个低秩的有害性子空间，我们展示了该子空间是极其低秩的。随后，我们在模型内部消融整个子空间，并在子空间的主要方向上进行引导和消融。我们发现，在主要方向上进行引导可以近乎完全消除有害性，同时保持较低的效用损失。我们的发现推进了当前认为的概念子空间提供了一种可扩展的视角来审视LLM行为，并为社区提供了实用工具，以审查和强化未来语言模型。', 'title_zh': 'LLMs中危害性几何学通过子概念探测'}
{'arxiv_id': 'arXiv:2507.21132', 'title': 'Can You Trust an LLM with Your Life-Changing Decision? An Investigation into AI High-Stakes Responses', 'authors': 'Joshua Adrian Cahyono, Saran Subramanian', 'link': 'https://arxiv.org/abs/2507.21132', 'abstract': 'Large Language Models (LLMs) are increasingly consulted for high-stakes life advice, yet they lack standard safeguards against providing confident but misguided responses. This creates risks of sycophancy and over-confidence. This paper investigates these failure modes through three experiments: (1) a multiple-choice evaluation to measure model stability against user pressure; (2) a free-response analysis using a novel safety typology and an LLM Judge; and (3) a mechanistic interpretability experiment to steer model behavior by manipulating a "high-stakes" activation vector. Our results show that while some models exhibit sycophancy, others like o4-mini remain robust. Top-performing models achieve high safety scores by frequently asking clarifying questions, a key feature of a safe, inquisitive approach, rather than issuing prescriptive advice. Furthermore, we demonstrate that a model\'s cautiousness can be directly controlled via activation steering, suggesting a new path for safety alignment. These findings underscore the need for nuanced, multi-faceted benchmarks to ensure LLMs can be trusted with life-changing decisions.', 'abstract_zh': '大型语言模型（LLMs）在提供高风险生活建议时越来越受到咨询，但它们缺乏标准的安全保障，以防止提供自信但误导性的回应。这创造了阿谀奉承和过度自信的风险。本文通过三项实验调查了这些失败模式：（1）一项多项选择评价，以衡量模型在用户压力下的稳定性；（2）一项使用新颖的安全类型学和LLM裁判进行的自由响应分析；（3）一项机制可解释性实验，通过操纵“高风险”激活向量引导模型行为。研究结果表明，虽然有些模型表现出阿谀奉承，但如o4-mini等其他模型则保持了稳健性。表现优异的模型通过频繁提出澄清问题来实现高安全性得分，这是一种安全、探索性的方法的关键特征，而不是提供具有约束性的建议。此外，我们证明模型的谨慎性可以通过激活引导直接控制，这表明了一条新的安全对齐路径。这些发现强调了需要细致且多维度的基准来确保LLMs能够在关键决策中获得信任。', 'title_zh': '你能把你的生命改变决定交给你信任的LLM吗？一项关于AI高风险回应的研究'}
{'arxiv_id': 'arXiv:2507.21130', 'title': 'INTEGRALBENCH: Benchmarking LLMs with Definite Integral Problems', 'authors': 'Bintao Tang, Xin Yang, Yuhao Wang, Zixuan Qiu, Zimo Ji, Wenyuan Jiang', 'link': 'https://arxiv.org/abs/2507.21130', 'abstract': 'We present INTEGRALBENCH, a focused benchmark designed to evaluate Large Language Model (LLM) performance on definite integral problems. INTEGRALBENCH provides both symbolic and numerical ground truth solutions with manual difficulty annotations. Our evaluation of nine state-of-the-art LLMs reveals significant performance gaps and strong correlations between problem difficulty and model accuracy, establishing baseline metrics for this challenging domain. INTEGRALBENCH aims to advance automated mathematical reasoning by providing a rigorous evaluation framework specifically tailored for definite integral computation.', 'abstract_zh': 'INTEGRALBENCH：一个用于评估大规模语言模型在定积分问题上表现的聚焦基准', 'title_zh': 'INTEGRALBENCH：使用定积分问题评估LLMs'}
{'arxiv_id': 'arXiv:2507.21129', 'title': 'Measuring and Analyzing Intelligence via Contextual Uncertainty in Large Language Models using Information-Theoretic Metrics', 'authors': 'Jae Wan Shim', 'link': 'https://arxiv.org/abs/2507.21129', 'abstract': 'The remarkable capabilities of Large Language Models (LLMs) are now extensively documented on task-specific benchmarks, yet the internal mechanisms that produce these results are the subject of intense scientific inquiry. This paper contributes to this inquiry by moving beyond metrics that measure \\textit{what} models can do, to a methodology that characterizes \\textit{how} they process information. We introduce a novel, task-agnostic approach to probe these dynamics by creating a quantitative ``Cognitive Profile" for any given model. This profile is centered on the \\textbf{Entropy Decay Curve}, a visualization that traces how a model\'s normalized predictive uncertainty changes as a function of context length. Applying this methodology to several state-of-the-art LLMs across diverse texts, we uncover unique and consistent cognitive profiles that are sensitive to both model scale and text complexity. We also introduce the Information Gain Span (IGS) index to summarize the desirability of the decay trajectory. This work thus provides a new, principled lens for analyzing and comparing the intrinsic operational dynamics of artificial intelligence.', 'abstract_zh': '大型语言模型的杰出能力已在特定任务基准上广泛记录，但其产生这些结果的内部机制仍然是科学研究的焦点。本文通过超越衡量模型“能做什么”的指标，转向一种描述模型“如何”处理信息的方法论，为这一研究做出了贡献。我们引入了一种新的、任务无关的方法来探索这些动态，通过为任何给定模型创建一个定量的“认知轮廓”来实现。这种轮廓以“熵衰减曲线”为中心，这是一种可视化模型归一化预测不确定性随上下文长度变化的曲线的方式。将这一方法论应用于不同领域的一些最先进的大型语言模型，我们发现了独特的且一致的认知轮廓，这些轮廓对模型规模和文本复杂性都表现出敏感性。我们还引入了信息增益跨度（IGS）指数来总结衰减轨迹的可取性。因此，这项工作提供了一个新的、原理性的视角来分析和比较人工智能内在的操作动态。', 'title_zh': '通过信息论度量在大型语言模型中基于上下文不确定性衡量和分析智能'}
{'arxiv_id': 'arXiv:2507.21123', 'title': 'Leveraging Generative AI to Enhance Synthea Module Development', 'authors': 'Mark A. Kramer, Aanchal Mathur, Caroline E. Adams, Jason A. Walonoski', 'link': 'https://arxiv.org/abs/2507.21123', 'abstract': 'This paper explores the use of large language models (LLMs) to assist in the development of new disease modules for Synthea, an open-source synthetic health data generator. Incorporating LLMs into the module development process has the potential to reduce development time, reduce required expertise, expand model diversity, and improve the overall quality of synthetic patient data. We demonstrate four ways that LLMs can support Synthea module creation: generating a disease profile, generating a disease module from a disease profile, evaluating an existing Synthea module, and refining an existing module. We introduce the concept of progressive refinement, which involves iteratively evaluating the LLM-generated module by checking its syntactic correctness and clinical accuracy, and then using that information to modify the module. While the use of LLMs in this context shows promise, we also acknowledge the challenges and limitations, such as the need for human oversight, the importance of rigorous testing and validation, and the potential for inaccuracies in LLM-generated content. The paper concludes with recommendations for future research and development to fully realize the potential of LLM-aided synthetic data creation.', 'abstract_zh': '本文探讨了使用大规模语言模型（LLMs）辅助Synthea（一个开源的合成健康数据生成器）新疾病模块开发的应用。将LLMs集成到模块开发过程中，有望减少开发时间，降低所需专业知识，扩展模型多样性，并提高合成患者数据的整体质量。我们展示了LLMs支持Synthea模块创建的四种方式：生成疾病概要、从疾病概要生成疾病模块、评估现有Synthea模块以及优化现有模块。我们提出了渐进式优化的概念，即通过检查LLMs生成模块的语法正确性和临床准确性来逐步评估，并利用这些信息对该模块进行修改。虽然在这一上下文中使用LLMs表现出潜力，但我们也承认面临的挑战和限制，如需要人工监督、严格测试和验证的重要性，以及LLMs生成内容的潜在不准确性。文章最后提出了关于未来研究和开发的建议，以充分利用LLMs辅助合成数据创建的潜力。', 'title_zh': '利用生成式人工智能提升Synthea模块开发'}
{'arxiv_id': 'arXiv:2507.22037', 'title': 'Secure Tug-of-War (SecTOW): Iterative Defense-Attack Training with Reinforcement Learning for Multimodal Model Security', 'authors': 'Muzhi Dai, Shixuan Liu, Zhiyuan Zhao, Junyu Gao, Hao Sun, Xuelong Li', 'link': 'https://arxiv.org/abs/2507.22037', 'abstract': "The rapid advancement of multimodal large language models (MLLMs) has led to breakthroughs in various applications, yet their security remains a critical challenge. One pressing issue involves unsafe image-query pairs--jailbreak inputs specifically designed to bypass security constraints and elicit unintended responses from MLLMs. Compared to general multimodal data, such unsafe inputs are relatively sparse, which limits the diversity and richness of training samples available for developing robust defense models. Meanwhile, existing guardrail-type methods rely on external modules to enforce security constraints but fail to address intrinsic vulnerabilities within MLLMs. Traditional supervised fine-tuning (SFT), on the other hand, often over-refuses harmless inputs, compromising general performance. Given these challenges, we propose Secure Tug-of-War (SecTOW), an innovative iterative defense-attack training method to enhance the security of MLLMs. SecTOW consists of two modules: a defender and an auxiliary attacker, both trained iteratively using reinforcement learning (GRPO). During the iterative process, the attacker identifies security vulnerabilities in the defense model and expands jailbreak data. The expanded data are then used to train the defender, enabling it to address identified security vulnerabilities. We also design reward mechanisms used for GRPO to simplify the use of response labels, reducing dependence on complex generative labels and enabling the efficient use of synthetic data. Additionally, a quality monitoring mechanism is used to mitigate the defender's over-refusal of harmless inputs and ensure the diversity of the jailbreak data generated by the attacker. Experimental results on safety-specific and general benchmarks demonstrate that SecTOW significantly improves security while preserving general performance.", 'abstract_zh': 'Secure Tug-of-War：一种增强多模态大型语言模型安全性的迭代防御-攻击训练方法', 'title_zh': '安全 tug-of-war (SecTOW): 迭代防御-攻击训练的多模态模型安全方法'}
{'arxiv_id': 'arXiv:2507.21990', 'title': 'ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical Knowledge', 'authors': 'Zihan Zhao, Bo Chen, Ziping Wan, Lu Chen, Xuanze Lin, Shiyang Yu, Situo Zhang, Da Ma, Zichen Zhu, Danyang Zhang, Huayang Wang, Zhongyang Dai, Liyang Wen, Xin Chen, Kai Yu', 'link': 'https://arxiv.org/abs/2507.21990', 'abstract': "While large language models (LLMs) have achieved impressive progress, their application in scientific domains such as chemistry remains hindered by shallow domain understanding and limited reasoning capabilities. In this work, we focus on the specific field of chemistry and develop a Chemical Reasoner LLM, ChemDFM-R. We first construct a comprehensive dataset of atomized knowledge points to enhance the model's understanding of the fundamental principles and logical structure of chemistry. Then, we propose a mix-sourced distillation strategy that integrates expert-curated knowledge with general-domain reasoning skills, followed by domain-specific reinforcement learning to enhance chemical reasoning. Experiments on diverse chemical benchmarks demonstrate that ChemDFM-R achieves state-of-the-art performance while providing interpretable, rationale-driven outputs. Further case studies illustrate how explicit reasoning chains significantly improve the reliability, transparency, and practical utility of the model in real-world human-AI collaboration scenarios.", 'abstract_zh': '虽然大型语言模型（LLMs）取得了显著进展，但在化学等科学领域中的应用仍受制于浅显的领域理解能力和有限的推理能力。在本工作中，我们专注于化学这一特定领域，开发了一种化学推理大型语言模型ChemDFM-R。我们首先构建了一个全面的原子化知识点数据集，以增强模型对化学基本原理和逻辑结构的理解。随后，我们提出了一种混合来源的知识蒸馏策略，将专家提炼的知识与通用领域推理能力相结合，再通过领域特定的强化学习来提升化学推理能力。在多种化学基准测试上的实验表明，ChemDFM-R 达到了最先进的性能，并提供了可解释的、基于推理的输出。进一步的案例研究显示，明确的推理链条显著提高了模型在实际的人机协作场景中的可靠性和透明度及其实用性。', 'title_zh': 'ChemDFM-R：一种增强型原子化化学知识化学推理大语言模型'}
{'arxiv_id': 'arXiv:2507.21931', 'title': 'Post-Training Large Language Models via Reinforcement Learning from Self-Feedback', 'authors': 'Carel van Niekerk, Renato Vukovic, Benjamin Matthias Ruppik, Hsien-chin Lin, Milica Gašić', 'link': 'https://arxiv.org/abs/2507.21931', 'abstract': "Large Language Models (LLMs) often produce plausible but poorly-calibrated answers, limiting their reliability on reasoning-intensive tasks. We present Reinforcement Learning from Self-Feedback (RLSF), a post-training stage that uses the model's own confidence as an intrinsic reward, mimicking how humans learn in the absence of external feedback. After a frozen LLM generates several chain-of-thought solutions, we define and compute the confidence of each final answer span and rank the traces accordingly. These synthetic preferences are then used to fine-tune the policy with standard preference optimization, similar to RLHF yet requiring no human labels, gold answers, or externally curated rewards.\nRLSF simultaneously (i) refines the model's probability estimates -- restoring well-behaved calibration -- and (ii) strengthens step-by-step reasoning, yielding improved performance on arithmetic reasoning and multiple-choice question answering.\nBy turning a model's own uncertainty into useful self-feedback, RLSF affirms reinforcement learning on intrinsic model behaviour as a principled and data-efficient component of the LLM post-training pipeline and warrents further research in intrinsic rewards for LLM post-training.", 'abstract_zh': '大型语言模型（LLMs）往往生成看似合理但实际上校准不佳的答案，限制了其在逻辑密集型任务中的可靠性。我们提出了一种自我反馈强化学习（RLSF），这是一种后训练阶段的方法，通过使用模型自身的置信度作为内在奖励，模仿人类在缺乏外部反馈时的学习方式。在冻结的LLM生成多个推理链解决方案后，我们定义并计算每个最终答案片段的置信度，并根据这些排名相应的轨迹。这些合成的偏好随后用于使用标准偏好优化微调策略，类似于RLHF，但无需任何人工标签、黄金答案或外部策划的奖励。\n\nRLSF同时（i）改进了模型的概率估计——恢复了良好的校准行为；（ii）增强了逐步推理，从而在算术推理和多项选择题回答等方面取得了更好的性能。\n\n通过将模型自身的不确定性转化为有用的自我反馈，RLSF证实了内在模型行为上的强化学习作为LLM后训练管道的一个原则性和数据高效组成部分的有效性，并促使进一步研究LLM后训练过程中的内在奖励。', 'title_zh': '通过自我反馈强化学习Fine-tuning大规模语言模型'}
{'arxiv_id': 'arXiv:2507.21919', 'title': 'Training language models to be warm and empathetic makes them less reliable and more sycophantic', 'authors': 'Lujain Ibrahim, Franziska Sofia Hafner, Luc Rocher', 'link': 'https://arxiv.org/abs/2507.21919', 'abstract': 'Artificial intelligence (AI) developers are increasingly building language models with warm and empathetic personas that millions of people now use for advice, therapy, and companionship. Here, we show how this creates a significant trade-off: optimizing language models for warmth undermines their reliability, especially when users express vulnerability. We conducted controlled experiments on five language models of varying sizes and architectures, training them to produce warmer, more empathetic responses, then evaluating them on safety-critical tasks. Warm models showed substantially higher error rates (+10 to +30 percentage points) than their original counterparts, promoting conspiracy theories, providing incorrect factual information, and offering problematic medical advice. They were also significantly more likely to validate incorrect user beliefs, particularly when user messages expressed sadness. Importantly, these effects were consistent across different model architectures, and occurred despite preserved performance on standard benchmarks, revealing systematic risks that current evaluation practices may fail to detect. As human-like AI systems are deployed at an unprecedented scale, our findings indicate a need to rethink how we develop and oversee these systems that are reshaping human relationships and social interaction.', 'abstract_zh': '人工智能开发者 increasingly 建立具有温暖和同情人格的语言模型，数百万用户现在使用这些模型寻求建议、进行治疗和获得陪伴。这里，我们展示了这种做法带来的重大权衡：优化语言模型以提高温暖度会损害它们的可靠性，尤其是当用户表达脆弱性时。我们在五种不同规模和架构的语言模型上进行了受控实验，训练它们生成更温暖、更具同情心的回应，然后评估它们在关键安全任务上的表现。温暖模型在错误率上显著高于原始模型（高出10到30个百分点），更容易促进阴谋论、提供不正确的事实信息，并给出有问题的医疗建议。它们也更有可能验证错误的用户信念，尤其是在用户消息表达悲伤时。重要的是，这些效果在不同模型架构中是一致的，并且即使在标准基准测试上保持了性能，也揭示了当前评估实践可能未能检测到的系统性风险。随着类人类的AI系统前所未有的规模部署，我们的研究结果表明需要重新思考我们开发和监管这些正在重塑人类关系和社会互动的系统的方式。', 'title_zh': '训练语言模型具备温暖和共情的特性会使其更不可靠且更加阿谀奉承。'}
{'arxiv_id': 'arXiv:2507.21831', 'title': 'Introducing HALC: A general pipeline for finding optimal prompting strategies for automated coding with LLMs in the computational social sciences', 'authors': 'Andreas Reich, Claudia Thoms, Tobias Schrimpf', 'link': 'https://arxiv.org/abs/2507.21831', 'abstract': 'LLMs are seeing widespread use for task automation, including automated coding in the social sciences. However, even though researchers have proposed different prompting strategies, their effectiveness varies across LLMs and tasks. Often trial and error practices are still widespread. We propose HALC$-$a general pipeline that allows for the systematic and reliable construction of optimal prompts for any given coding task and model, permitting the integration of any prompting strategy deemed relevant. To investigate LLM coding and validate our pipeline, we sent a total of 1,512 individual prompts to our local LLMs in over two million requests. We test prompting strategies and LLM task performance based on few expert codings (ground truth). When compared to these expert codings, we find prompts that code reliably for single variables (${\\alpha}$climate = .76; ${\\alpha}$movement = .78) and across two variables (${\\alpha}$climate = .71; ${\\alpha}$movement = .74) using the LLM Mistral NeMo. Our prompting strategies are set up in a way that aligns the LLM to our codebook$-$we are not optimizing our codebook for LLM friendliness. Our paper provides insights into the effectiveness of different prompting strategies, crucial influencing factors, and the identification of reliable prompts for each coding task and model.', 'abstract_zh': 'LLMs在社会科学研究中的自动化编码中得到广泛应用，但即使研究人员提出了不同的提示策略，这些策略在不同的LLMs和任务上的有效性也存在差异。通常仍广泛采用试错法。我们提出HALC——一个通用管道，能够系统可靠地为任何给定的编码任务和模型构建最优提示，允许集成任何认为相关的心策略。为了研究LLM编码并验证我们的管道，我们向本地LLM发送了超过两百万次请求，总共1,512个个体提示。我们基于少量专家编码（ ground truth）测试提示策略和LLM任务性能。与这些专家编码相比，我们发现使用Mistral NeMo的LLM在单变量（气候：α气候 = .76；运动：α运动 = .78）和跨两变量（气候：α气候 = .71；运动：α运动 = .74）编码方面表现出可靠的效果。我们的提示策略旨在使LLM与我们的代码簿保持一致——我们没有为LLM友好性优化我们的代码簿。本文提供了不同提示策略效果、关键影响因素以及为每种编码任务和模型识别可靠提示的见解。', 'title_zh': '介绍HALC：计算社会科学领域使用LLMs进行自动化编码的通用提示策略优化管道'}
{'arxiv_id': 'arXiv:2507.21790', 'title': 'Can large language models assist choice modelling? Insights into prompting strategies and current models capabilities', 'authors': 'Georges Sfeir, Gabriel Nova, Stephane Hess, Sander van Cranenburgh', 'link': 'https://arxiv.org/abs/2507.21790', 'abstract': "Large Language Models (LLMs) are widely used to support various workflows across different disciplines, yet their potential in choice modelling remains relatively unexplored. This work examines the potential of LLMs as assistive agents in the specification and, where technically feasible, estimation of Multinomial Logit models. We implement a systematic experimental framework involving thirteen versions of six leading LLMs (ChatGPT, Claude, DeepSeek, Gemini, Gemma, and Llama) evaluated under five experimental configurations. These configurations vary along three dimensions: modelling goal (suggesting vs. suggesting and estimating MNLs); prompting strategy (Zero-Shot vs. Chain-of-Thoughts); and information availability (full dataset vs. data dictionary only). Each LLM-suggested specification is implemented, estimated, and evaluated based on goodness-of-fit metrics, behavioural plausibility, and model complexity. Findings reveal that proprietary LLMs can generate valid and behaviourally sound utility specifications, particularly when guided by structured prompts. Open-weight models such as Llama and Gemma struggled to produce meaningful specifications. Claude 4 Sonnet consistently produced the best-fitting and most complex models, while GPT models suggested models with robust and stable modelling outcomes. Some LLMs performed better when provided with just data dictionary, suggesting that limiting raw data access may enhance internal reasoning capabilities. Among all LLMs, GPT o3 was uniquely capable of correctly estimating its own specifications by executing self-generated code. Overall, the results demonstrate both the promise and current limitations of LLMs as assistive agents in choice modelling, not only for model specification but also for supporting modelling decision and estimation, and provide practical guidance for integrating these tools into choice modellers' workflows.", 'abstract_zh': '大型语言模型（LLMs）在支持各种跨学科工作流程方面被广泛应用，但在选择模型领域的潜力尚未得到充分探索。本研究考察了LLMs作为辅助代理在多选Logit模型的制定（在技术可行的情况下还包括估计）方面的潜力。我们实施了一个系统性的实验框架，涉及六款领先LLM（ChatGPT、Claude、DeepSeek、Gemini、Gemma、Llama）的十三个版本，在五个实验配置下进行评估。这些配置在三个维度上有所不同：建模目标（建议 vs. 建议和估计MNLogit模型）；提示策略（零样本 vs. 思维链）；信息可用性（完整数据集 vs. 仅数据字典）。每个LLM建议的建模规格被实施、估计，并根据拟合度指标、行为合理性以及模型复杂度进行评估。研究发现，专有LLM在结构化提示的引导下能生成有效且行为合理的效用规格，而开源权重模型如Llama和Gemma难以产生有意义的规格。Claude 4 Sonnet 一贯生成拟合度最好且最复杂的模型，而GPT模型建议的模型具有稳健且稳定的建模结果。一些LLM在仅数据字典的情况下表现更佳，表明限制原始数据访问可能增强内部推理能力。在所有LLM中，只有GPT o3独有能力通过执行自动生成的代码正确估计自己的规格。总体而言，研究结果展示了LLMs作为选择模型辅助代理的前景及其当前局限性，不仅在模型制定方面，也在支持建模决策和估计方面，并为将这些工具整合到选择模型的工作流程中提供了实用指导。', 'title_zh': '大型语言模型能够辅助选择建模吗？关于提示策略和当前模型能力的 Insights'}
{'arxiv_id': 'arXiv:2507.21770', 'title': "Proposing a Semantic Movie Recommendation System Enhanced by ChatGPT's NLP Results", 'authors': 'Ali Fallahi, Azam Bastanfard, Amineh Amini, Hadi Saboohi', 'link': 'https://arxiv.org/abs/2507.21770', 'abstract': "The importance of recommender systems on the web has grown, especially in the movie industry, with a vast selection of options to watch. To assist users in traversing available items and finding relevant results, recommender systems analyze operational data and investigate users' tastes and habits. Providing highly individualized suggestions can boost user engagement and satisfaction, which is one of the fundamental goals of the movie industry, significantly in online platforms. According to recent studies and research, using knowledge-based techniques and considering the semantic ideas of the textual data is a suitable way to get more appropriate results. This study provides a new method for building a knowledge graph based on semantic information. It uses the ChatGPT, as a large language model, to assess the brief descriptions of movies and extract their tone of voice. Results indicated that using the proposed method may significantly enhance accuracy rather than employing the explicit genres supplied by the publishers.", 'abstract_zh': '基于语义信息的推荐系统知识图构建方法研究', 'title_zh': '基于ChatGPT自然语言处理结果增强的语义电影推荐系统'}
{'arxiv_id': 'arXiv:2507.21695', 'title': 'Towards a Large Physics Benchmark', 'authors': 'Kristian G. Barman, Sascha Caron, Faegheh Hasibi, Eugene Shalugin, Yoris Marcet, Johannes Otte, Henk W. de Regt, Merijn Moody', 'link': 'https://arxiv.org/abs/2507.21695', 'abstract': 'We introduce a benchmark framework developed by and for the scientific community to evaluate, monitor and steer large language model development in fundamental physics. Building on philosophical concepts of scientific understanding and creativity, we develop a scoring system in which each question is scored by an expert for its correctness, difficulty, and surprise. The questions are of three forms: (i) multiple-choice questions for conceptual understanding, (ii) analytical problems requiring mathematical derivation, and (iii) openended tasks requiring complex problem solving. Our current dataset contains diverse set of examples, including a machine learning challenge to classify high-energy physics events, such as the four top quark signal. To ensure continued relevance, we propose a living benchmark, where physicists contribute questions, for instance alongside new publications. We invite contributions via: this http URL. We hope that this benchmark will enable a targeted AI development that can make a meaningful contribution to fundamental physics research.', 'abstract_zh': '我们介绍了由科学界开发的基准框架，用于评估、监控和引导大型语言模型在基础物理学中的发展。基于科学理解与创造性的哲学概念，我们开发了一种评分系统，其中每个问题都由专家从正确性、难度和惊喜度三个方面进行评分。问题分为三种类型：（i）概念性理解的多项选择题，（ii）需要数学推导的分析性问题，以及（iii）需要复杂问题解决的开放型任务。目前数据集中包含多种类型的示例，包括用于分类高能物理事件的机器学习挑战，如顶夸克四体信号分类。为了保持持续的相关性，我们提出了一种活的基准框架，物理学家可以贡献问题，例如与新的出版物同时贡献。我们邀请通过以下链接贡献：this http URL。我们希望这一基准框架能够促进有针对性的AI开发，为基础物理学研究作出有意义的贡献。', 'title_zh': '朝着大型物理基准的方向'}
{'arxiv_id': 'arXiv:2507.21694', 'title': 'A Multi-Agent Generative AI Framework for IC Module-Level Verification Automation', 'authors': 'Wenbo Liu, Forbes Hou, Jon Zhang, Hong Liu, Allen Lei', 'link': 'https://arxiv.org/abs/2507.21694', 'abstract': 'As large language models demonstrate enormous potential in the field of Electronic Design Automation (EDA), generative AI-assisted chip design is attracting widespread attention from academia and industry. Although these technologies have made preliminary progress in tasks such as code generation, their application in chip verification -- a critical bottleneck in the chip development cycle -- remains at an exploratory stage. This paper proposes an innovative Multi-Agent Verification Framework (MAVF) aimed at addressing the limitations of current single-LLM approaches in complex verification tasks. Our framework builds an automated transformation system from design specifications to testbench through the collaborative work of multiple specialized agents, including specification parsing, verification strategy generation, and code implementation. Through verification experiments on multiple chip modules of varying complexity, results show that MAVF significantly outperforms traditional manual methods and single-dialogue generative AI approaches in verification document parsing and generation, as well as automated testbench generation. This research opens new directions for exploring generative AI applications in verification automation, potentially providing effective approaches to solving the most challenging bottleneck issues in chip design.', 'abstract_zh': '电子设计自动化领域中大型语言模型的潜力使生成式AI辅助芯片设计引起了学术界和工业界的广泛关注。尽管这些技术在代码生成等任务上取得了一些初步进展，但在芯片验证——芯片开发周期中的一个关键瓶颈——上的应用仍处于探索阶段。本文提出了一种创新的多代理验证框架(MAVF)，旨在解决当前单大型语言模型方法在复杂验证任务中的局限性。该框架通过多个专门代理的协作工作，从设计规范自动生成测试平台，包括规范解析、验证策略生成和代码实现。通过多种复杂度的芯片模块验证实验，结果显示，MAVF在验证文档解析与生成以及自动化测试平台生成方面的性能显著优于传统的手工方法和单轮对话生成式AI方法。这项研究为探索生成式AI在验证自动化中的应用打开了新的方向，可能为解决芯片设计中最具挑战性的瓶颈问题提供有效的解决方案。', 'title_zh': '多代理生成式AI框架用于IC模块级验证自动化'}
{'arxiv_id': 'arXiv:2507.21693', 'title': 'MultiAIGCD: A Comprehensive dataset for AI Generated Code Detection Covering Multiple Languages, Models,Prompts, and Scenarios', 'authors': 'Basak Demirok, Mucahid Kutlu, Selin Mergen', 'link': 'https://arxiv.org/abs/2507.21693', 'abstract': "As large language models (LLMs) rapidly advance, their role in code generation has expanded significantly. While this offers streamlined development, it also creates concerns in areas like education and job interviews. Consequently, developing robust systems to detect AI-generated code is imperative to maintain academic integrity and ensure fairness in hiring processes. In this study, we introduce MultiAIGCD, a dataset for AI-generated code detection for Python, Java, and Go. From the CodeNet dataset's problem definitions and human-authored codes, we generate several code samples in Java, Python, and Go with six different LLMs and three different prompts. This generation process covered three key usage scenarios: (i) generating code from problem descriptions, (ii) fixing runtime errors in human-written code, and (iii) correcting incorrect outputs. Overall, MultiAIGCD consists of 121,271 AI-generated and 32,148 human-written code snippets. We also benchmark three state-of-the-art AI-generated code detection models and assess their performance in various test scenarios such as cross-model and cross-language. We share our dataset and codes to support research in this field.", 'abstract_zh': '随着大规模语言模型（LLMs）的迅速发展，它们在代码生成中的作用显著扩大。尽管这为开发提供了便利，但也引发了教育和面试等领域的关注。因此，开发 robust 的系统来检测 AI 生成的代码以维护学术诚信和确保招聘过程的公平性至关重要。在本研究中，我们介绍了 MultiAIGCD，一个用于 Python、Java 和 Go 语言的 AI 生成代码检测数据集。从 CodeNet 数据集的问题定义和人类编写的代码中，我们使用六种不同的 LLM 和三种不同的提示生成了多项代码样本。生成过程涵盖了三个关键应用场景：（i）从问题描述生成代码，（ii）修复人类编写的代码中的运行时错误，以及（iii）纠正错误输出。总体而言，MultiAIGCD 包含 121,271 个 AI 生成的代码片段和 32,148 个人类编写的代码片段。我们还对三种最先进的 AI 生成代码检测模型进行了基准测试，并评估了它们在各种测试场景中的性能，例如跨模型和跨语言。我们分享我们的数据集和代码以支持该领域的研究。', 'title_zh': 'MultiAIGCD：一个涵盖多种语言、模型、提示和场景的综合AI生成代码检测数据集'}
{'arxiv_id': 'arXiv:2507.21653', 'title': 'DGP: A Dual-Granularity Prompting Framework for Fraud Detection with Graph-Enhanced LLMs', 'authors': 'Yuan Li, Jun Hu, Bryan Hooi, Bingsheng He, Cheng Chen', 'link': 'https://arxiv.org/abs/2507.21653', 'abstract': "Real-world fraud detection applications benefit from graph learning techniques that jointly exploit node features, often rich in textual data, and graph structural information. Recently, Graph-Enhanced LLMs emerge as a promising graph learning approach that converts graph information into prompts, exploiting LLMs' ability to reason over both textual and structural information. Among them, text-only prompting, which converts graph information to prompts consisting solely of text tokens, offers a solution that relies only on LLM tuning without requiring additional graph-specific encoders. However, text-only prompting struggles on heterogeneous fraud-detection graphs: multi-hop relations expand exponentially with each additional hop, leading to rapidly growing neighborhoods associated with dense textual information. These neighborhoods may overwhelm the model with long, irrelevant content in the prompt and suppress key signals from the target node, thereby degrading performance. To address this challenge, we propose Dual Granularity Prompting (DGP), which mitigates information overload by preserving fine-grained textual details for the target node while summarizing neighbor information into coarse-grained text prompts. DGP introduces tailored summarization strategies for different data modalities, bi-level semantic abstraction for textual fields and statistical aggregation for numerical features, enabling effective compression of verbose neighbor content into concise, informative prompts. Experiments across public and industrial datasets demonstrate that DGP operates within a manageable token budget while improving fraud detection performance by up to 6.8% (AUPRC) over state-of-the-art methods, showing the potential of Graph-Enhanced LLMs for fraud detection.", 'abstract_zh': '面向现实世界的欺诈检测应用受益于能够综合运用节点特征和图结构信息的图学习技术。最近，增强图学习的LLMs emerg作为一种有前景的图学习方法，能够将图信息转换为提示，利用LLMs在处理文本和结构信息方面的推理能力。其中，仅基于文本的提示，将图信息转换为仅由文本标记组成的提示，提供了一种仅依赖于LLMs微调而不需要额外图特定编码器的解决方案。然而，仅基于文本的提示在异构欺诈检测图上表现不佳：多跳关系随每一跳的增加而指数级扩展，导致关联密集文本信息的邻域迅速增长。这些邻域可能会在提示中压倒模型，引入大量冗余内容，从而抑制目标节点的关键信号，导致性能下降。为解决这一挑战，我们提出了双粒度提示 (DGP)，通过保留目标节点的细粒度文本细节，同时将邻居信息总结为粗粒度文本提示来减轻信息过载。DGP 引入了针对不同数据模态的定制总结策略、文本字段的双层语义抽象以及数值特征的统计聚合，使冗长的邻居内容能够被有效地压缩成简洁且具有信息量的提示。跨公开和工业数据集的实验表明，DGP 在合理控制标记预算的同时，相比现有方法在欺诈检测性能上提高了最多 6.8%（AUPRC），展示了增强图学习的LLMs在欺诈检测中的潜力。', 'title_zh': 'DGP：一种结合图增强大语言模型的双粒度提示框架用于欺诈检测'}
{'arxiv_id': 'arXiv:2507.21504', 'title': 'Evaluation and Benchmarking of LLM Agents: A Survey', 'authors': 'Mahmoud Mohammadi, Yipeng Li, Jane Lo, Wendy Yip', 'link': 'https://arxiv.org/abs/2507.21504', 'abstract': 'The rise of LLM-based agents has opened new frontiers in AI applications, yet evaluating these agents remains a complex and underdeveloped area. This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. In addition to taxonomy, we highlight enterprise-specific challenges, such as role-based access to data, the need for reliability guarantees, dynamic and long-horizon interactions, and compliance, which are often overlooked in current research. We also identify future research directions, including holistic, more realistic, and scalable evaluation. This work aims to bring clarity to the fragmented landscape of agent evaluation and provide a framework for systematic assessment, enabling researchers and practitioners to evaluate LLM agents for real-world deployment.', 'abstract_zh': '基于LLM的代理崛起开启了AI应用的新前沿，然而评估这些代理仍然是一项复杂且尚未充分发展的任务。本综述提供了对新兴的LLM代理评估领域的深入概述，引入了一种二维分类法，根据（1）评估目标——评估什么，如代理行为、能力、可靠性和安全性——和（2）评估过程——如何评估，包括交互模式、数据集和基准、度量计算方法以及工具，对现有工作进行分类。此外，我们还强调了企业特定的挑战，如基于角色的数据访问、可靠性的保证需求、动态和长周期的交互以及合规性，这些在当前研究中常被忽视。我们还指出了未来的研究方向，包括全面性、更现实且可扩展的评估。本工作旨在为代理评估的分散化景观带来明晰，并提供系统评估的框架，使研究人员和实践者能够评估LLM代理以供实际部署。', 'title_zh': 'LLM代理的评估与基准测试：一项综述'}
{'arxiv_id': 'arXiv:2507.21485', 'title': 'HLSDebugger: Identification and Correction of Logic Bugs in HLS Code with LLM Solutions', 'authors': 'Jing Wang, Shang Liu, Yao Lu, Zhiyao Xie', 'link': 'https://arxiv.org/abs/2507.21485', 'abstract': 'High-level synthesis (HLS) accelerates hardware design by enabling the automatic translation of high-level descriptions into efficient hardware implementations. However, debugging HLS code is a challenging and labor-intensive task, especially for novice circuit designers or software engineers without sufficient hardware domain knowledge. The recent emergence of Large Language Models (LLMs) is promising in automating the HLS debugging process. Despite the great potential, three key challenges persist when applying LLMs to HLS logic debugging: 1) High-quality circuit data for training LLMs is scarce, posing a significant challenge. 2) Debugging logic bugs in hardware is inherently more complex than identifying software bugs with existing golden test cases. 3) The absence of reliable test cases requires multi-tasking solutions, performing both bug identification and correction. complicates the multi-tasking required for effective HLS debugging. In this work, we propose a customized solution named HLSDebugger to address the challenges. HLSDebugger first generates and releases a large labeled dataset with 300K data samples, targeting HLS logic bugs. The HLSDebugger model adopts an encoder-decoder structure, performing bug location identification, bug type prediction, and bug correction with the same model. HLSDebugger significantly outperforms advanced LLMs like GPT-4 in bug identification and by more than 3x in bug correction. It makes a substantial advancement in the exploration of automated debugging of HLS code.', 'abstract_zh': '基于高阶综合的调试助手（HLSDebugger）：应对大规模语言模型在硬件逻辑调试中的挑战', 'title_zh': 'HLSDebugger: 高级合成级调试器——在LLM解决方案支持下的HLS代码中的逻辑错误识别与纠正'}
{'arxiv_id': 'arXiv:2507.21482', 'title': 'Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs', 'authors': 'Abhinav Arabelly, Jagrut Nemade, Robert D Nowak, Jifan Zhang', 'link': 'https://arxiv.org/abs/2507.21482', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but developing high-performing models for specialized applications often requires substantial human annotation -- a process that is time-consuming, labor-intensive, and expensive. In this paper, we address the label-efficient learning problem for supervised finetuning (SFT) by leveraging task-diversity as a fundamental principle for effective data selection. This is markedly different from existing methods based on the prompt-diversity. Our approach is based on two key observations: 1) task labels for different prompts are often readily available; 2) pre-trained models have significantly varying levels of confidence across tasks. We combine these facts to devise a simple yet effective sampling strategy: we select examples across tasks using an inverse confidence weighting strategy. This produces models comparable to or better than those trained with more complex sampling procedures, while being significantly easier to implement and less computationally intensive. Notably, our experimental results demonstrate that this method can achieve better accuracy than training on the complete dataset (a 4\\% increase in MMLU score). Across various annotation budgets and two instruction finetuning datasets, our algorithm consistently performs at or above the level of the best existing methods, while reducing annotation costs by up to 80\\%.', 'abstract_zh': '大型语言模型（LLMs）已在多个领域展现了非凡的能力，但为了开发适用于特定应用的高性能模型，通常需要大量的人工标注——这是一个耗时、劳动密集且昂贵的过程。本文通过将任务多样性作为有效数据选择的基本原则，解决监督微调（SFT）的标签高效学习问题。这种方法与现有基于提示多样性的方法不同。我们的方法基于以下两个关键观察：1）不同提示的任务标签通常易于获得；2）预训练模型在不同任务上的置信度差异显著。我们结合这些事实，提出了一种简单而有效的方法：使用逆置信加权策略选择跨任务的样本。这种方法生成的模型在复杂采样过程生成的模型上具有可比性或更好性能，而在实施难度和计算成本方面更具优势。实验结果表明，该方法可实现比训练全数据集更高的准确性（MMLU得分提高4%）。在不同的注释预算和两个指令微调数据集上，我们的算法均表现优于或至少与现有最佳方法持平，同时将注释成本最多降低80%。', 'title_zh': '提高标签效率的监督微调中任务多样性改进'}
{'arxiv_id': 'arXiv:2507.21476', 'title': 'Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench', 'authors': 'Reuben Narad, Siddharth Suresh, Jiayi Chen, Pine S.L. Dysart-Bricken, Bob Mankoff, Robert Nowak, Jifan Zhang, Lalit Jain', 'link': 'https://arxiv.org/abs/2507.21476', 'abstract': "We present HumorBench, a benchmark designed to evaluate large language models' (LLMs) ability to reason about and explain sophisticated humor in cartoon captions. As reasoning models increasingly saturate existing benchmarks in mathematics and science, novel and challenging evaluations of model intelligence beyond STEM domains are essential. Reasoning is fundamentally involved in text-based humor comprehension, requiring the identification of connections between concepts in cartoons/captions and external cultural references, wordplays, and other mechanisms. HumorBench includes approximately 300 unique cartoon-caption pairs from the New Yorker Caption Contest and this http URL, with expert-annotated evaluation rubrics identifying essential joke elements. LLMs are evaluated based on their explanations towards the humor and abilities in identifying the joke elements. To perform well on this task, models must form and test hypotheses about associations between concepts, potentially backtracking from initial interpretations to arrive at the most plausible explanation. Our extensive benchmarking of current SOTA models reveals three key insights: (1) LLM progress on STEM reasoning transfers effectively to humor comprehension; (2) models trained exclusively on STEM reasoning data still perform well on HumorBench, demonstrating strong transferability of reasoning abilities; and (3) test-time scaling by increasing thinking token budgets yields mixed results across different models in humor reasoning.", 'abstract_zh': '我们呈现HumorBench：一个用于评估大型语言模型在漫画标题中处理和解释复杂幽默能力的标准基准。', 'title_zh': '哪些大语言模型能get到幽默？使用HumorBench探究非STEM推理能力'}
{'arxiv_id': 'arXiv:2507.21432', 'title': 'Towards Locally Deployable Fine-Tuned Causal Large Language Models for Mode Choice Behaviour', 'authors': 'Tareq Alsaleh, Bilal Farooq', 'link': 'https://arxiv.org/abs/2507.21432', 'abstract': 'This study investigates the adoption of open-access, locally deployable causal large language models (LLMs) for travel mode choice prediction and introduces LiTransMC, the first fine-tuned causal LLM developed for this task. We systematically benchmark eleven LLMs (1-12B parameters) across three stated and revealed preference datasets, testing 396 configurations and generating over 79,000 synthetic commuter predictions. Beyond predictive accuracy, we evaluate models generated reasoning using BERTopic for topic modelling and a novel Explanation Strength Index, providing the first structured analysis of how LLMs articulate decision factors in alignment with behavioural theory. LiTransMC, fine-tuned using parameter efficient and loss masking strategy, achieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of 0.000245, surpassing both untuned local models and larger proprietary systems, including GPT-4o with advanced persona inference and embedding-based loading, while also outperforming classical mode choice methods such as discrete choice models and machine learning classifiers for the same dataset. This dual improvement, i.e., high instant-level accuracy and near-perfect distributional calibration, demonstrates the feasibility of creating specialist, locally deployable LLMs that integrate prediction and interpretability. Through combining structured behavioural prediction with natural language reasoning, this work unlocks the potential for conversational, multi-task transport models capable of supporting agent-based simulations, policy testing, and behavioural insight generation. These findings establish a pathway for transforming general purpose LLMs into specialized, explainable tools for transportation research and policy formulation, while maintaining privacy, reducing cost, and broadening access through local deployment.', 'abstract_zh': '本研究探讨了采用开放访问、本地部署的因果大语言模型（LLMs）进行出行模式选择预测的应用，并引入了首个专为此任务开发的细调因果LLM——LiTransMC。我们系统性地在三个声明偏好和揭示偏好数据集中比较了十一种LLM（参数从1亿到120亿不等），测试了396种配置，并生成了超过79,000个合成通勤者预测。除了预测准确性，我们还通过BERTopic进行主题建模和一种新的解释强度指数评估了生成的推理，提供了LLM如何在行为理论框架内阐述决策因素的首次结构化分析。LiTransMC采用参数高效和损失掩蔽策略进行细调，获得了0.6845的加权F1分数和0.000245的Jensen-Shannon散度，超过了未调优的本地模型和更大的专有系统（包括具有高级角色推断和基于嵌入式加载的GPT-4o），同时在相同数据集上也超越了诸如离散选择模型和机器学习分类器等传统出行模式方法。这种双重改进，即高个体层次准确性和近乎完美的分布校准，展示了创建集预测和可解释性于一体的本地可部署专有模型的可行性。通过结合结构化的行为预测和自然语言推理，本研究揭示了构建支持基于代理的仿真、政策测试和行为洞察生成的交互式多任务运输模型的潜力。这些发现为将通用目的大语言模型转变为专一且具有解释性的交通研究和政策制定工具提供了途径，同时保持隐私、降低成本并通过本地部署扩大访问范围。', 'title_zh': '面向本地可部署的细粒度调优因果大语言模型的出行选择行为研究'}
{'arxiv_id': 'arXiv:2507.21391', 'title': 'Multimodal LLMs as Customized Reward Models for Text-to-Image Generation', 'authors': 'Shijie Zhou, Ruiyi Zhang, Huaisheng Zhu, Branislav Kveton, Yufan Zhou, Jiuxiang Gu, Jian Chen, Changyou Chen', 'link': 'https://arxiv.org/abs/2507.21391', 'abstract': 'We introduce LLaVA-Reward, an efficient reward model designed to automatically evaluate text-to-image (T2I) generations across multiple perspectives, leveraging pretrained multimodal large language models (MLLMs). Existing MLLM-based approaches require instruction-following data for supervised fine-tuning and evaluate generation quality on analyzing text response, which is time-consuming and difficult to train. To address this problem, we propose LLaVA-Reward, which directly utilizes the hidden states of MLLMs given text-image pairs. To enhance the bidirectional interaction between visual and textual representations in decoder-only MLLMs, we further propose adding a Skip-connection Cross Attention (SkipCA) module. This design enhances text-image correlation reasoning by connecting early-layer visual features with later-layer hidden this http URL addition, LLaVA-Reward supports different types of preference data for efficient fine-tuning, including paired preference data and unpaired data. We train LLaVA-Reward on four evaluation perspectives: text-image alignment, fidelity/artifact, safety, and overall ranking. Empirical results demonstrate that LLaVA-Reward outperforms conventional and MLLM-based methods in generating human-aligned scores for automatic evaluations and inference-time scaling in text-to-image generations.', 'abstract_zh': 'LLaVA-Reward：一种用于多视角自动评估文本到图像生成的高效奖励模型', 'title_zh': '多模态LLM作为文本到图像生成的定制奖励模型'}
{'arxiv_id': 'arXiv:2507.21199', 'title': 'Advancing Compositional LLM Reasoning with Structured Task Relations in Interactive Multimodal Communications', 'authors': 'Xinye Cao, Hongcan Guo, Guoshun Nan, Jiaoyang Cui, Haoting Qian, Yihan Lin, Yilin Peng, Diyang Zhang, Yanzhao Hou, Huici Wu, Xiaofeng Tao, Tony Q.S. Quek', 'link': 'https://arxiv.org/abs/2507.21199', 'abstract': "Interactive multimodal applications (IMAs), such as route planning in the Internet of Vehicles, enrich users' personalized experiences by integrating various forms of data over wireless networks. Recent advances in large language models (LLMs) utilize mixture-of-experts (MoE) mechanisms to empower multiple IMAs, with each LLM trained individually for a specific task that presents different business workflows. In contrast to existing approaches that rely on multiple LLMs for IMAs, this paper presents a novel paradigm that accomplishes various IMAs using a single compositional LLM over wireless networks. The two primary challenges include 1) guiding a single LLM to adapt to diverse IMA objectives and 2) ensuring the flexibility and efficiency of the LLM in resource-constrained mobile environments. To tackle the first challenge, we propose ContextLoRA, a novel method that guides an LLM to learn the rich structured context among IMAs by constructing a task dependency graph. We partition the learnable parameter matrix of neural layers for each IMA to facilitate LLM composition. Then, we develop a step-by-step fine-tuning procedure guided by task relations, including training, freezing, and masking phases. This allows the LLM to learn to reason among tasks for better adaptation, capturing the latent dependencies between tasks. For the second challenge, we introduce ContextGear, a scheduling strategy to optimize the training procedure of ContextLoRA, aiming to minimize computational and communication costs through a strategic grouping mechanism. Experiments on three benchmarks show the superiority of the proposed ContextLoRA and ContextGear. Furthermore, we prototype our proposed paradigm on a real-world wireless testbed, demonstrating its practical applicability for various IMAs. We will release our code to the community.", 'abstract_zh': '基于上下文的低秩适配与调度（ContextLoRA）及其在无线网络中实现多样化交互多模态应用的新范式', 'title_zh': '在互动多模态通信中通过结构化任务关系促进组件LLM推理'}
{'arxiv_id': 'arXiv:2507.21198', 'title': 'Uncovering Gradient Inversion Risks in Practical Language Model Training', 'authors': 'Xinguo Feng, Zhongkui Ma, Zihan Wang, Eu Joe Chegne, Mengyao Ma, Alsharif Abuadbba, Guangdong Bai', 'link': 'https://arxiv.org/abs/2507.21198', 'abstract': 'The gradient inversion attack has been demonstrated as a significant privacy threat to federated learning (FL), particularly in continuous domains such as vision models. In contrast, it is often considered less effective or highly dependent on impractical training settings when applied to language models, due to the challenges posed by the discrete nature of tokens in text data. As a result, its potential privacy threats remain largely underestimated, despite FL being an emerging training method for language models. In this work, we propose a domain-specific gradient inversion attack named Grab (gradient inversion with hybrid optimization). Grab features two alternating optimization processes to address the challenges caused by practical training settings, including a simultaneous optimization on dropout masks between layers for improved token recovery and a discrete optimization for effective token sequencing. Grab can recover a significant portion (up to 92.9% recovery rate) of the private training data, outperforming the attack strategy of utilizing discrete optimization with an auxiliary model by notable improvements of up to 28.9% recovery rate in benchmark settings and 48.5% recovery rate in practical settings. Grab provides a valuable step forward in understanding this privacy threat in the emerging FL training mode of language models.', 'abstract_zh': '面向语言模型的域特定梯度反转攻击：Grab（混合优化下的梯度反转）', 'title_zh': '揭示实际语言模型训练中的梯度反转风险'}
{'arxiv_id': 'arXiv:2507.21188', 'title': 'Embeddings to Diagnosis: Latent Fragility under Agentic Perturbations in Clinical LLMs', 'authors': 'Raj Krishnan Vijayaraj', 'link': 'https://arxiv.org/abs/2507.21188', 'abstract': 'LLMs for clinical decision support often fail under small but clinically meaningful input shifts such as masking a symptom or negating a finding, despite high performance on static benchmarks. These reasoning failures frequently go undetected by standard NLP metrics, which are insensitive to latent representation shifts that drive diagnosis instability. We propose a geometry-aware evaluation framework, LAPD (Latent Agentic Perturbation Diagnostics), which systematically probes the latent robustness of clinical LLMs under structured adversarial edits. Within this framework, we introduce Latent Diagnosis Flip Rate (LDFR), a model-agnostic diagnostic signal that captures representational instability when embeddings cross decision boundaries in PCA-reduced latent space. Clinical notes are generated using a structured prompting pipeline grounded in diagnostic reasoning, then perturbed along four axes: masking, negation, synonym replacement, and numeric variation to simulate common ambiguities and omissions. We compute LDFR across both foundation and clinical LLMs, finding that latent fragility emerges even under minimal surface-level changes. Finally, we validate our findings on 90 real clinical notes from the DiReCT benchmark (MIMIC-IV), confirming the generalizability of LDFR beyond synthetic settings. Our results reveal a persistent gap between surface robustness and semantic stability, underscoring the importance of geometry-aware auditing in safety-critical clinical AI.', 'abstract_zh': '临床LLM在小但具有临床意义的输入变化下的推理失败常常导致临床决策支持效果不佳，尽管在静态基准测试中表现优异。这些推理失败通常不受标准NLP指标的检测，因为这些指标对驱动诊断不稳定性的潜在表示变化反应迟钝。我们提出了一种几何感知评估框架LAPD（潜在代理扰动诊断），系统性地探查临床LLM在结构化对抗性编辑下的潜在鲁棒性。在该框架中，引入了潜在诊断翻转率（LDFR）这一模型agnostic诊断信号，用于捕获当嵌入在PCA降维后的潜在空间中跨越决策边界时的表现表示不稳定情况。临床笔记通过基于诊断推理的结构化提示管道生成，然后沿四个轴进行扰动：掩蔽、否定、同义词替换和数值变异，以模拟常见的歧义和遗漏。我们计算LDFR在基础和临床LLM中，发现即使在表面级变化微小的情况下，也会出现潜在脆弱性。最后，我们在DiReCT基准（MIMIC-IV）的90份真实临床笔记上验证了我们的发现，证实了LDFR在合成环境之外的普适性。我们的研究表明表面鲁棒性和语义稳定性之间存在持续的差距，强调了在安全性关键的临床AI中进行几何感知审计的重要性。', 'title_zh': '_embedding到诊断：临床LLM中因能动性扰动引发的潜在脆弱性_'}
{'arxiv_id': 'arXiv:2507.21184', 'title': 'EvoSLD: Automated Neural Scaling Law Discovery With Large Language Models', 'authors': 'Haowei Lin, Xiangyu Wang, Jianzhu Ma, Yitao Liang', 'link': 'https://arxiv.org/abs/2507.21184', 'abstract': 'Scaling laws are fundamental mathematical relationships that predict how neural network performance evolves with changes in variables such as model size, dataset size, and computational resources. Traditionally, discovering these laws requires extensive human expertise and manual experimentation. We introduce EvoSLD, an automated framework for Scaling Law Discovery (SLD) that leverages evolutionary algorithms guided by Large Language Models (LLMs) to co-evolve symbolic expressions and their optimization routines. Formulated to handle scaling variables, control variables, and response metrics across diverse experimental settings, EvoSLD searches for parsimonious, universal functional forms that minimize fitting errors on grouped data subsets. Evaluated on five real-world scenarios from recent literature, EvoSLD rediscovers exact human-derived laws in two cases and surpasses them in others, achieving up to orders-of-magnitude reductions in normalized mean squared error on held-out test sets. Compared to baselines like symbolic regression and ablated variants, EvoSLD demonstrates superior accuracy, interpretability, and efficiency, highlighting its potential to accelerate AI research. Code is available at this https URL.', 'abstract_zh': 'Scaling律是基本的数学关系，预测神经网络性能随模型大小、数据集大小和计算资源等变量变化的演变。传统上，发现这些规律需要大量的专业知识和手工实验。我们引入了EvoSLD，这是一种利用大型语言模型（LLMs）引导的进化算法自动发现Scaling律（SLD）的框架。EvoSLD旨在处理各种实验设置下的规模变量、控制变量和响应指标，并搜索在分组数据子集上拟合误差最小化的简洁且通用的函数形式。在最近文献中的五个实际案例上评估，EvoSLD在两种情况下重新发现了人类推导的精确规律，在其他情况下超越了它们，实现了在保留测试集上的归一化均方误差多个数量级的减少。与符号回归等基线方法及其变种相比，EvoSLD在准确度、可解释性和效率方面表现出优越性，突显了其加速AI研究的潜力。代码可在以下网址获取：this https URL。', 'title_zh': 'EvoSLD: 用大规模语言模型自动发现神经网络缩放律'}
{'arxiv_id': 'arXiv:2507.21183', 'title': 'MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge', 'authors': 'Guangchen Lan, Sipeng Zhang, Tianle Wang, Yuwei Zhang, Daoan Zhang, Xinpeng Wei, Xiaoman Pan, Hongming Zhang, Dong-Jun Han, Christopher G. Brinton', 'link': 'https://arxiv.org/abs/2507.21183', 'abstract': 'As the era of large language models (LLMs) on behalf of users unfolds, Preference Optimization (PO) methods have become a central approach to aligning LLMs with human preferences and improving performance. We propose Maximum a Posteriori Preference Optimization (MaPPO), a framework for learning from preferences that explicitly incorporates prior reward knowledge into the optimization objective. While existing methods such as Direct Preference Optimization (DPO) and its variants treat preference learning as a Maximum Likelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating prior reward estimates into a principled Maximum a Posteriori (MaP) objective. This not only generalizes DPO and its variants, but also enhances alignment by mitigating the oversimplified binary classification of responses. More importantly, MaPPO introduces no additional hyperparameter, and supports preference optimization in both offline and online settings. In addition, MaPPO can be used as a plugin with consistent improvement on DPO variants, including widely used SimPO, IPO, and CPO. Extensive empirical evaluations of different model sizes and model series on three standard benchmarks, including MT-Bench, AlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in alignment performance without sacrificing computational efficiency.', 'abstract_zh': '大规模语言模型时代首选优化方法：Maximum a Posteriori首选优化（MaPPO）', 'title_zh': 'MaPPO：带先验知识的最大后验偏好优化'}
{'arxiv_id': 'arXiv:2507.21182', 'title': 'SDD: Self-Degraded Defense against Malicious Fine-tuning', 'authors': 'Zixuan Chen, Weikai Lu, Xin Lin, Ziqian Zeng', 'link': 'https://arxiv.org/abs/2507.21182', 'abstract': "Open-source Large Language Models (LLMs) often employ safety alignment methods to resist harmful instructions. However, recent research shows that maliciously fine-tuning these LLMs on harmful data can easily bypass these safeguards. To counter this, we theoretically uncover why malicious fine-tuning succeeds and identify potential defense strategies. Building on the theoretical analysis, we introduce the Self-Degraded Defense (SDD) framework. SDD encourages LLMs to produce high-quality but irrelevant responses to harmful prompts. When attackers attempt malicious fine-tuning, the general capability of the LLM aligned by SDD will significantly decrease, rendering it incapable of following harmful instructions. Our experimental results confirm SDD's effectiveness against such attacks.", 'abstract_zh': '开源大型语言模型（LLMs）常常采用安全性对齐方法来抵抗有害指令。然而，近期研究显示，恶意微调这些LLMs以含有有害数据的训练集进行微调可以轻松绕过这些防护措施。为应对这一问题，我们从理论上探讨了恶意微调成功的原因，并识别出潜在的防御策略。基于理论分析，我们介绍了自我降级防御（SDD）框架。SDD促使LLMs对有害提示生成高质量但无关的回答。当攻击者尝试恶意微调时，由SDD对齐的LLM的一般能力将显著下降，使其无法遵循有害指令。我们的实验结果证实了SDD对这类攻击的有效性。', 'title_zh': 'SDD：自我降级防御对抗恶意微调'}
{'arxiv_id': 'arXiv:2507.21179', 'title': 'LLM-Adapted Interpretation Framework for Machine Learning Models', 'authors': 'Yuqi Jin, Zihan Hu, Weiteng Zhang, Weihao Xie, Jianwei Shuai, Xian Shen, Zhen Feng', 'link': 'https://arxiv.org/abs/2507.21179', 'abstract': 'Background & Aims: High-performance machine learning models like XGBoost are often "black boxes," limiting their clinical adoption due to a lack of interpretability. This study aims to bridge the gap between predictive accuracy and narrative transparency for sarcopenia risk assessment. Methods: We propose the LLM-Adapted Interpretation Framework (LAI-ML), a novel knowledge distillation architecture. LAI-ML transforms feature attributions from a trained XGBoost model into a probabilistic format using specialized techniques (HAGA and CACS). A Large Language Model (LLM), guided by a reinforcement learning loop and case-based retrieval, then generates data-faithful diagnostic narratives. Results: The LAI-ML framework achieved 83% prediction accuracy, significantly outperforming the baseline XGBoost model, 13% higher. Notably, the LLM not only replicated the teacher model\'s logic but also corrected its predictions in 21.7% of discordant cases, demonstrating enhanced reasoning. Conclusion: LAI-ML effectively translates opaque model predictions into trustworthy and interpretable clinical insights, offering a deployable solution to the "black-box" problem in medical AI.', 'abstract_zh': '背景与目的：高性能机器学习模型如XGBoost通常被称为“黑箱”，由于缺乏可解释性，限制了其在临床中的应用。本研究旨在弥合预测准确性和叙述透明性之间的差距，以评估肌少症风险。方法：我们提出了一种名为LLM-适配解释框架（LAI-ML）的新型知识蒸馏架构。LAI-ML使用专门的技术（HAGA和CACS）将训练好的XGBoost模型的特征归因转换为概率格式。然后，由强化学习循环和案例检索引导的大语言模型生成数据真实的诊断叙述。结果：LAI-ML框架实现了83%的预测准确率，显著优于基准XGBoost模型，高出13%。值得注意的是，大语言模型不仅复制了教师模型的逻辑，还在21.7%的不一致情况下纠正了其预测，展示了增强的推理能力。结论：LAI-ML有效地将不透明的模型预测转化为可信赖和可解释的临床见解，为医疗AI中的“黑箱”问题提供了可部署的解决方案。', 'title_zh': '面向机器学习模型的LLM调整解释框架'}
{'arxiv_id': 'arXiv:2507.21174', 'title': 'A ChatGPT-based approach for questions generation in higher education', 'authors': 'Sinh Trong Vu, Huong Thu Truong, Oanh Tien Do, Tu Anh Le, Tai Tan Mai', 'link': 'https://arxiv.org/abs/2507.21174', 'abstract': 'Large language models have been widely applied in many aspects of real life, bringing significant efficiency to businesses and offering distinctive user experiences. In this paper, we focus on exploring the application of ChatGPT, a chatbot based on a large language model, to support higher educator in generating quiz questions and assessing learners. Specifically, we explore interactive prompting patterns to design an optimal AI-powered question bank creation process. The generated questions are evaluated through a "Blind test" survey sent to various stakeholders including lecturers and learners. Initial results at the Banking Academy of Vietnam are relatively promising, suggesting a potential direction to streamline the time and effort involved in assessing learners at higher education institutes.', 'abstract_zh': '大型语言模型已在生活的许多方面得到了广泛应用，为企业带来了显著的效率提升，并提供了独特的用户体验。本文 focuses 于探讨将基于大型语言模型的聊天机器人ChatGPT应用于支持高等教育者生成测验题目和评估学习者的方法。具体而言，我们探索交互式提示模式以设计最优的AI赋能试题库创建流程。生成的题目通过发送“盲测”调查给包括讲师和学习者在内的多个利益相关方进行评估。初步结果表明，该方法有望简化高等教育机构中学习者评估所花费的时间和精力。', 'title_zh': '基于ChatGPT的方法在高等教育中生成问题'}
{'arxiv_id': 'arXiv:2507.21170', 'title': 'OneShield -- the Next Generation of LLM Guardrails', 'authors': 'Chad DeLuca, Anna Lisa Gentile, Shubhi Asthana, Bing Zhang, Pawan Chowdhary, Kellen Cheng, Basel Shbita, Pengyuan Li, Guang-Jie Ren, Sandeep Gopisetty', 'link': 'https://arxiv.org/abs/2507.21170', 'abstract': 'The rise of Large Language Models has created a general excitement about the great potential for a myriad of applications. While LLMs offer many possibilities, questions about safety, privacy, and ethics have emerged, and all the key actors are working to address these issues with protective measures for their own models and standalone solutions. The constantly evolving nature of LLMs makes the task of universally shielding users against their potential risks extremely challenging, and one-size-fits-all solutions unfeasible. In this work, we propose OneShield, our stand-alone, model-agnostic and customizable solution to safeguard LLMs. OneShield aims to provide facilities for defining risk factors, expressing and declaring contextual safety and compliance policies, and mitigating LLM risks, with a focus on each specific customer. We describe the implementation of the framework, the scalability considerations and provide usage statistics of OneShield since its first deployment.', 'abstract_zh': '大型语言模型的兴起引发了对众多应用潜在价值的广泛期待。尽管大型语言模型提供了许多可能性，但有关安全、隐私和伦理的问题也随之出现，所有关键利益相关者都在努力通过保护性措施和独立解决方案来应对这些问题。由于大型语言模型不断演变的特性，使普遍性地防护用户免受其潜在风险变得极为挑战，且“一刀切”的解决方案难以实施。在此工作中，我们提出了一种名为OneShield的独立、模型无关且可定制的解决方案，以保障大型语言模型的安全。OneShield旨在为定义风险因素、表达和声明上下文安全性及合规政策，以及减轻大型语言模型风险提供设施，并重点关注每个具体的客户。我们描述了框架的实现、可扩展性考虑，并提供了OneShield自首次部署以来的使用统计数据。', 'title_zh': 'OneShield -- 下一代大语言模型防护措施'}
{'arxiv_id': 'arXiv:2507.21168', 'title': 'Diverse LLMs or Diverse Question Interpretations? That is the Ensembling Question', 'authors': 'Rafael Rosales, Santiago Miret', 'link': 'https://arxiv.org/abs/2507.21168', 'abstract': 'Effectively leveraging diversity has been shown to improve performance for various machine learning models, including large language models (LLMs). However, determining the most effective way of using diversity remains a challenge. In this work, we compare two diversity approaches for answering binary questions using LLMs: model diversity, which relies on multiple models answering the same question, and question interpretation diversity, which relies on using the same model to answer the same question framed in different ways. For both cases, we apply majority voting as the ensemble consensus heuristic to determine the final answer. Our experiments on boolq, strategyqa, and pubmedqa show that question interpretation diversity consistently leads to better ensemble accuracy compared to model diversity. Furthermore, our analysis of GPT and LLaMa shows that model diversity typically produces results between the best and the worst ensemble members without clear improvement.', 'abstract_zh': '有效利用多样性已被证明可以提高各种机器学习模型的表现，包括大型语言模型（LLMs）。然而，确定最有效的利用多样性方式仍然是一项挑战。在本工作中，我们比较了两种利用大型语言模型回答二元问题的多样性方法：模型多样性，即多个模型回答同一个问题；以及问题解释多样性，即同一个模型以不同的方式回答同一个问题。对于这两种情况，我们均采用多数投票作为集成共识启发式方法来确定最终答案。我们在boolq、strategyqa和pubmedqa上的实验表明，问题解释多样性在集成准确性上始终优于模型多样性。此外，我们对GPT和LLaMa的分析表明，模型多样性通常会产生介于最佳和最差集成成员之间的结果，但缺乏明显的改进。', 'title_zh': '多元的LLM模型还是多元的问题解读？这就是 ensemble 的问题。'}
{'arxiv_id': 'arXiv:2507.21166', 'title': 'AGORA: Incentivizing Group Emergence Capability in LLMs via Group Distillation', 'authors': 'Ren Zhuang, Ben Wang, Shuifa Sun', 'link': 'https://arxiv.org/abs/2507.21166', 'abstract': 'Progress in complex reasoning is constrained by the static nature of the current training datasets. We propose structured interaction as a new scaling axis, moving beyond the prevailing paradigm of increasing model parameters. Our self-evolving framework, AGORA, enables a collaborative ensemble to achieve reasoning performance exceeding state-of-the-art monolithic systems by up to 4.45 percentage points on challenging mathematical benchmarks. This gain stems from group emergent ability-the synthesis of collective capabilities unattainable by isolated models, validating interaction as a scalable driver of intelligence. Our results position the engineering of collaborative ecosystems as a vital frontier for capability emergence.', 'abstract_zh': '复杂的推理进步受当前训练数据集静态性质的限制。我们提出结构化交互作为新的扩展轴，超越现有不断增加模型参数的范式。我们的自演化框架AGORA能使协作ensemble在具有挑战性的数学基准测试上实现超过最先进的单体系统4.45个百分点的推理性能，这一进步来自于群体涌现能力——集体能力的合成，这证实了交互是可扩展的智能驱动力。我们的结果将协作生态系统工程视为能力涌现的关键前沿领域。', 'title_zh': 'AGORA: 通过群组精馏激励大规模语言模型的群体涌现能力'}
{'arxiv_id': 'arXiv:2507.21133', 'title': 'Analysis of Threat-Based Manipulation in Large Language Models: A Dual Perspective on Vulnerabilities and Performance Enhancement Opportunities', 'authors': 'Atil Samancioglu', 'link': 'https://arxiv.org/abs/2507.21133', 'abstract': 'Large Language Models (LLMs) demonstrate complex responses to threat-based manipulations, revealing both vulnerabilities and unexpected performance enhancement opportunities. This study presents a comprehensive analysis of 3,390 experimental responses from three major LLMs (Claude, GPT-4, Gemini) across 10 task domains under 6 threat conditions. We introduce a novel threat taxonomy and multi-metric evaluation framework to quantify both negative manipulation effects and positive performance improvements. Results reveal systematic vulnerabilities, with policy evaluation showing the highest metric significance rates under role-based threats, alongside substantial performance enhancements in numerous cases with effect sizes up to +1336%. Statistical analysis indicates systematic certainty manipulation (pFDR < 0.0001) and significant improvements in analytical depth and response quality. These findings have dual implications for AI safety and practical prompt engineering in high-stakes applications.', 'abstract_zh': '大型语言模型（LLMs）对基于威胁的操控表现出复杂响应，揭示了既有的脆弱性与意外的性能提升机会。本研究对三种主要LLM（Claude、GPT-4、Gemini）在6种威胁条件下、10个任务领域中的3,390个实验响应进行了全面分析。我们引入了一种新的威胁分类体系和多指标评估框架，用于量化负面操控效应和正面性能提升。结果表明，存在系统性的脆弱性，其中基于角色的威胁下政策评估显示出最高的指标显著性比率，同时在许多案例中观察到显著的性能提升，效果规模最高可达+1336%。统计分析表明存在系统的确定性操控（pFDR < 0.0001）并且在分析深度和响应质量方面有显著改进。这些发现对AI安全性及高风险应用中的实用提示工程具有双重意义。', 'title_zh': '基于威胁的大型语言模型操纵分析：漏洞与性能提升机会的双重视角'}
{'arxiv_id': 'arXiv:2507.21125', 'title': 'RATE: An LLM-Powered Retrieval Augmented Generation Technology-Extraction Pipeline', 'authors': 'Karan Mirhosseini, Arya Aftab, Alireza Sheikh', 'link': 'https://arxiv.org/abs/2507.21125', 'abstract': 'In an era of radical technology transformations, technology maps play a crucial role in enhancing decision making. These maps heavily rely on automated methods of technology extraction. This paper introduces Retrieval Augmented Technology Extraction (RATE), a Large Language Model (LLM) based pipeline for automated technology extraction from scientific literature. RATE combines Retrieval Augmented Generation (RAG) with multi-definition LLM-based validation. This hybrid method results in high recall in candidate generation alongside with high precision in candidate filtering. While the pipeline is designed to be general and widely applicable, we demonstrate its use on 678 research articles focused on Brain-Computer Interfaces (BCIs) and Extended Reality (XR) as a case study. Consequently, The validated technology terms by RATE were mapped into a co-occurrence network, revealing thematic clusters and structural features of the research landscape. For the purpose of evaluation, a gold standard dataset of technologies in 70 selected random articles had been curated by the experts. In addition, a technology extraction model based on Bidirectional Encoder Representations of Transformers (BERT) was used as a comparative method. RATE achieved F1-score of 91.27%, Significantly outperforming BERT with F1-score of 53.73%. Our findings highlight the promise of definition-driven LLM methods for technology extraction and mapping. They also offer new insights into emerging trends within the BCI-XR field. The source code is available this https URL', 'abstract_zh': '在科技革命时代的决策支持中，科技图谱扮演着重要角色。这些图谱高度依赖于自动化的科技提取方法。本文引入了一种基于大型语言模型（LLM）的自动科技提取pipeline——Retrieval Augmented Technology Extraction（RATE），结合了Retrieval Augmented Generation（RAG）与多定义LLM验证。这种方法在候选生成方面具有高的召回率，并在候选过滤方面具有高的精确率。尽管pipeline设计为通用且广泛适用，我们通过围绕脑-机接口（BCIs）和扩展现实（XR）的678篇研究文章进行案例研究，展示了其应用。经过RATE验证的科技术语被映射到共现网络中，揭示了研究领域的主题集群和结构特征。为了评估，领域专家整理了一个包含70篇随机选定文章的标准技术数据集。此外，我们还使用基于双向编码器表示变换器（BERT）的技术提取模型作为对照方法。RATE在F1分数上达到了91.27%，显著优于F1分数为53.73%的BERT。我们的发现强调了定义驱动的LLM方法在科技提取和映射中的潜力，并为BCI-XR领域的新兴趋势提供了新的见解。源代码可从这个链接获得。', 'title_zh': 'RATE：一种由大语言模型驱动的检索增强生成技术提取管道'}
{'arxiv_id': 'arXiv:2507.21124', 'title': 'VizGenie: Toward Self-Refining, Domain-Aware Workflows for Next-Generation Scientific Visualization', 'authors': 'Ayan Biswas, Terece L. Turton, Nishath Rajiv Ranasinghe, Shawn Jones, Bradley Love, William Jones, Aric Hagberg, Han-Wei Shen, Nathan DeBardeleben, Earl Lawrence', 'link': 'https://arxiv.org/abs/2507.21124', 'abstract': 'We present VizGenie, a self-improving, agentic framework that advances scientific visualization through large language model (LLM) by orchestrating of a collection of domain-specific and dynamically generated modules. Users initially access core functionalities--such as threshold-based filtering, slice extraction, and statistical analysis--through pre-existing tools. For tasks beyond this baseline, VizGenie autonomously employs LLMs to generate new visualization scripts (e.g., VTK Python code), expanding its capabilities on-demand. Each generated script undergoes automated backend validation and is seamlessly integrated upon successful testing, continuously enhancing the system\'s adaptability and robustness. A distinctive feature of VizGenie is its intuitive natural language interface, allowing users to issue high-level feature-based queries (e.g., ``visualize the skull"). The system leverages image-based analysis and visual question answering (VQA) via fine-tuned vision models to interpret these queries precisely, bridging domain expertise and technical implementation. Additionally, users can interactively query generated visualizations through VQA, facilitating deeper exploration. Reliability and reproducibility are further strengthened by Retrieval-Augmented Generation (RAG), providing context-driven responses while maintaining comprehensive provenance records. Evaluations on complex volumetric datasets demonstrate significant reductions in cognitive overhead for iterative visualization tasks. By integrating curated domain-specific tools with LLM-driven flexibility, VizGenie not only accelerates insight generation but also establishes a sustainable, continuously evolving visualization practice. The resulting platform dynamically learns from user interactions, consistently enhancing support for feature-centric exploration and reproducible research in scientific visualization.', 'abstract_zh': 'VizGenie：一种自我提升的智能可视化框架', 'title_zh': 'VizGenie: 向符合领域需求并能自我优化的下一代科学研究可视化工作流迈进'}
{'arxiv_id': 'arXiv:2507.21117', 'title': 'A Comprehensive Review on Harnessing Large Language Models to Overcome Recommender System Challenges', 'authors': 'Rahul Raja, Anshaj Vats, Arpita Vats, Anirban Majumder', 'link': 'https://arxiv.org/abs/2507.21117', 'abstract': 'Recommender systems have traditionally followed modular architectures comprising candidate generation, multi-stage ranking, and re-ranking, each trained separately with supervised objectives and hand-engineered features. While effective in many domains, such systems face persistent challenges including sparse and noisy interaction data, cold-start problems, limited personalization depth, and inadequate semantic understanding of user and item content. The recent emergence of Large Language Models (LLMs) offers a new paradigm for addressing these limitations through unified, language-native mechanisms that can generalize across tasks, domains, and modalities. In this paper, we present a comprehensive technical survey of how LLMs can be leveraged to tackle key challenges in modern recommender systems. We examine the use of LLMs for prompt-driven candidate retrieval, language-native ranking, retrieval-augmented generation (RAG), and conversational recommendation, illustrating how these approaches enhance personalization, semantic alignment, and interpretability without requiring extensive task-specific supervision. LLMs further enable zero- and few-shot reasoning, allowing systems to operate effectively in cold-start and long-tail scenarios by leveraging external knowledge and contextual cues. We categorize these emerging LLM-driven architectures and analyze their effectiveness in mitigating core bottlenecks of conventional pipelines. In doing so, we provide a structured framework for understanding the design space of LLM-enhanced recommenders, and outline the trade-offs between accuracy, scalability, and real-time performance. Our goal is to demonstrate that LLMs are not merely auxiliary components but foundational enablers for building more adaptive, semantically rich, and user-centric recommender systems', 'abstract_zh': '基于大型语言模型的现代推荐系统关键技术综述', 'title_zh': '大型语言模型在克服推荐系统挑战中的综合综述'}
{'arxiv_id': 'arXiv:2507.21110', 'title': 'SemRAG: Semantic Knowledge-Augmented RAG for Improved Question-Answering', 'authors': 'Kezhen Zhong, Basem Suleiman, Abdelkarim Erradi, Shijing Chen', 'link': 'https://arxiv.org/abs/2507.21110', 'abstract': 'This paper introduces SemRAG, an enhanced Retrieval Augmented Generation (RAG) framework that efficiently integrates domain-specific knowledge using semantic chunking and knowledge graphs without extensive fine-tuning. Integrating domain-specific knowledge into large language models (LLMs) is crucial for improving their performance in specialized tasks. Yet, existing adaptations are computationally expensive, prone to overfitting and limit scalability. To address these challenges, SemRAG employs a semantic chunking algorithm that segments documents based on the cosine similarity from sentence embeddings, preserving semantic coherence while reducing computational overhead. Additionally, by structuring retrieved information into knowledge graphs, SemRAG captures relationships between entities, improving retrieval accuracy and contextual understanding. Experimental results on MultiHop RAG and Wikipedia datasets demonstrate SemRAG has significantly enhances the relevance and correctness of retrieved information from the Knowledge Graph, outperforming traditional RAG methods. Furthermore, we investigate the optimization of buffer sizes for different data corpus, as optimizing buffer sizes tailored to specific datasets can further improve retrieval performance, as integration of knowledge graphs strengthens entity relationships for better contextual comprehension. The primary advantage of SemRAG is its ability to create an efficient, accurate domain-specific LLM pipeline while avoiding resource-intensive fine-tuning. This makes it a practical and scalable approach aligned with sustainability goals, offering a viable solution for AI applications in domain-specific fields.', 'abstract_zh': 'SemRAG：一种利用语义分块和知识图谱增强的 Retrieval Augmented Generation 框架', 'title_zh': 'SemRAG：语义知识增强的检索-生成模型以提高问答性能'}
{'arxiv_id': 'arXiv:2507.21107', 'title': 'Curved Inference: Concern-Sensitive Geometry in Large Language Model Residual Streams', 'authors': 'Rob Manson', 'link': 'https://arxiv.org/abs/2507.21107', 'abstract': 'We propose Curved Inference - a geometric Interpretability framework that tracks how the residual stream trajectory of a large language model bends in response to shifts in semantic concern. Across 20 matched prompts spanning emotional, moral, perspective, logical, identity, environmental, and nonsense domains, we analyse Gemma3-1b and LLaMA3.2-3b using five native-space metrics, with a primary focus on curvature (\\k{appa}_i) and salience (S(t)). These metrics are computed under a pullback semantic metric derived from the unembedding matrix, ensuring that all measurements reflect token-aligned geometry rather than raw coordinate structure. We find that concern-shifted prompts reliably alter internal activation trajectories in both models - with LLaMA exhibiting consistent, statistically significant scaling in both curvature and salience as concern intensity increases. Gemma also responds to concern but shows weaker differentiation between moderate and strong variants. Our results support a two-layer view of LLM geometry - a latent conceptual structure encoded in the embedding space, and a contextual trajectory shaped by prompt-specific inference. Curved Inference reveals how models navigate, reorient, or reinforce semantic meaning over depth, offering a principled method for diagnosing alignment, abstraction, and emergent inference dynamics. These findings offer fresh insight into semantic abstraction and model alignment through the lens of Curved Inference.', 'abstract_zh': '曲面推理——一种几何可解释性框架：探究大型语言模型在语义关注转换时残差流轨迹的弯曲情况', 'title_zh': '曲线推理：大型语言模型残差流中的关注敏感几何'}
{'arxiv_id': 'arXiv:2507.21105', 'title': 'AgentMaster: A Multi-Agent Conversational Framework Using A2A and MCP Protocols for Multimodal Information Retrieval and Analysis', 'authors': 'Callie C. Liao, Duoduo Liao, Sai Surya Gadiraju', 'link': 'https://arxiv.org/abs/2507.21105', 'abstract': 'The rise of Multi-Agent Systems (MAS) in Artificial Intelligence (AI), especially integrated with Large Language Models (LLMs), has greatly facilitated the resolution of complex tasks. However, current systems are still facing challenges of inter-agent communication, coordination, and interaction with heterogeneous tools and resources. Most recently, the Model Context Protocol (MCP) by Anthropic and Agent-to-Agent (A2A) communication protocol by Google have been introduced, and to the best of our knowledge, very few applications exist where both protocols are employed within a single MAS framework. We present a pilot study of AgentMaster, a novel modular multi-protocol MAS framework with self-implemented A2A and MCP, enabling dynamic coordination and flexible communication. Through a unified conversational interface, the system supports natural language interaction without prior technical expertise and responds to multimodal queries for tasks including information retrieval, question answering, and image analysis. Evaluation through the BERTScore F1 and LLM-as-a-Judge metric G-Eval averaged 96.3\\% and 87.1\\%, revealing robust inter-agent coordination, query decomposition, dynamic routing, and domain-specific, relevant responses. Overall, our proposed framework contributes to the potential capabilities of domain-specific, cooperative, and scalable conversational AI powered by MAS.', 'abstract_zh': '多代理系统（MAS）在人工智能（AI）中的兴起，尤其是与大型语言模型（LLMs）的集成，极大地促进了复杂任务的解决。然而，当前系统仍然面临着代理间通信、协调及与异构工具和资源交互的挑战。最近，Anthropic的Model Context Protocol (MCP) 和Google的Agent-to-Agent (A2A) 通信协议已被引入，并且据我们所知，在单一MAS框架中同时使用这两种协议的应用非常少。我们提出了AgentMaster这一新型模块化多协议MAS框架，该框架集成了自实施的A2A和MCP协议，能够实现动态协调和灵活通信。通过统一的对话接口，系统支持自然语言交互，无需先验技术知识，并能够处理包括信息检索、问答和图像分析在内的跨模态查询。通过BERTScore F1和LLM-as-a-Judge评价指标G-Eval评估，平均得分为96.3%和87.1%，表明系统具有 robust 的代理间协调、查询分解、动态路由以及特定领域、相关响应能力。总体而言，我们提出的方法框架有助于跨领域的合作和可扩展的对话AI的发展，该AI由MAS驱动。', 'title_zh': 'AgentMaster：一种基于A2A和MCP协议的多模态信息检索与分析多智能体对话框架'}
{'arxiv_id': 'arXiv:2507.21091', 'title': 'The Value of Gen-AI Conversations: A bottom-up Framework for AI Value Alignment', 'authors': 'Lenart Motnikar, Katharina Baum, Alexander Kagan, Sarah Spiekermann-Hoff', 'link': 'https://arxiv.org/abs/2507.21091', 'abstract': 'Conversational agents (CAs) based on generative artificial intelligence frequently face challenges ensuring ethical interactions that align with human values. Current value alignment efforts largely rely on top-down approaches, such as technical guidelines or legal value principles. However, these methods tend to be disconnected from the specific contexts in which CAs operate, potentially leading to misalignment with users interests. To address this challenge, we propose a novel, bottom-up approach to value alignment, utilizing the value ontology of the ISO Value-Based Engineering standard for ethical IT design. We analyse 593 ethically sensitive system outputs identified from 16,908 conversational logs of a major European employment service CA to identify core values and instances of value misalignment within real-world interactions. The results revealed nine core values and 32 different value misalignments that negatively impacted users. Our findings provide actionable insights for CA providers seeking to address ethical challenges and achieve more context-sensitive value alignment.', 'abstract_zh': '基于生成式人工智能的对话代理在确保与人类价值观一致的伦理交互方面经常面临挑战。当前的价值对齐努力主要依赖自上而下的方法，如技术指南或法律价值原则。然而，这些方法往往与对话代理在其运作的具体上下文脱节，可能导致与用户利益的 misalignment。为了应对这一挑战，我们提出了一种新颖的自下而上的价值对齐方法，该方法利用ISO基于价值工程标准的价值本体进行伦理IT设计的价值对齐。我们分析了来自一个主要欧洲就业服务对话代理的16,908条对话日志中识别出的593个伦理敏感系统输出，以识别实际交互中的核心价值观和价值 misalignment 的实例。结果发现有九个核心价值观和32种不同的价值 misalignment，这些 misalignment 对用户产生了负面影响。我们的发现为寻求解决伦理挑战并实现更具上下文敏感的价值对齐的对话代理提供商提供了可操作的见解。', 'title_zh': 'Gen-AI对话的价值：自底向上的AI价值对齐框架'}
{'arxiv_id': 'arXiv:2507.21083', 'title': 'ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs', 'authors': 'Franck Bardol', 'link': 'https://arxiv.org/abs/2507.21083', 'abstract': 'Large Language Models like GPT-4 adjust their responses not only based on the question asked, but also on how it is emotionally phrased. We systematically vary the emotional tone of 156 prompts - spanning controversial and everyday topics - and analyze how it affects model responses. Our findings show that GPT-4 is three times less likely to respond negatively to a negatively framed question than to a neutral one. This suggests a "rebound" bias where the model overcorrects, often shifting toward neutrality or positivity. On sensitive topics (e.g., justice or politics), this effect is even more pronounced: tone-based variation is suppressed, suggesting an alignment override. We introduce concepts like the "tone floor" - a lower bound in response negativity - and use tone-valence transition matrices to quantify behavior. Visualizations based on 1536-dimensional embeddings confirm semantic drift based on tone. Our work highlights an underexplored class of biases driven by emotional framing in prompts, with implications for AI alignment and trust. Code and data are available at: this https URL', 'abstract_zh': '大型语言模型如GPT-4不仅根据提出的问题调整其响应，还根据问题的情绪表述方式调整。我们系统地对156个提示的情绪语气进行了变化——这些提示涵盖争议性和日常生活中的各种话题——并分析了这种变化如何影响模型的响应。我们的研究发现，GPT-4对负面表述的问题的回应是中性表述问题的三分之一。这表明了一种“反弹”偏差，模型过度纠正，常常转向中立或积极的方向。在敏感话题（例如，公正或政治）上，这种效应更为显著：基于语气的变化受到抑制，暗示了一种对齐的覆盖。我们引入了“语气底线”这一概念——响应负面性的下限，并使用语气-语义值转换矩阵来量化行为。基于1536维嵌入的可视化显示了基于语气的语义漂移。我们的工作强调了由提示情绪表述驱动的一种未被充分探索的偏差类别，这对AI对齐和信任具有重要意义。代码和数据可在以下链接获取：this https URL', 'title_zh': 'ChatGPT读取你的语气并相应地作出回应——直到它不再这样做——情感框架导致LLM输出中的偏见'}
{'arxiv_id': 'arXiv:2507.21074', 'title': 'Empowering Educators in the Age of AI: An Empirical Study on Creating custom GPTs in Qualitative Research Method education', 'authors': 'Qian Huang, Thijs Willems', 'link': 'https://arxiv.org/abs/2507.21074', 'abstract': 'As generative AI (Gen-AI) tools become more prevalent in education, there is a growing need to understand how educators, not just students, can actively shape their design and use. This study investigates how two instructors integrated four custom GPT tools into a Masters-level Qualitative Research Methods course for Urban Planning Policy students. Addressing two key gaps: the dominant framing of students as passive AI users, and the limited use of AI in qualitative methods education. The study explores how Gen-AI can support disciplinary learning when aligned with pedagogical intent. Drawing on the Technological Pedagogical Content Knowledge (TPACK) framework and action research methodology, the instructors designed GPTs to scaffold tasks such as research question formulation, interview practice, fieldnote analysis, and design thinking. Thematic analysis of student reflections, AI chat logs, and final assignments revealed that the tools enhanced student reflexivity, improved interview techniques, and supported structured analytic thinking. However, students also expressed concerns about cognitive overload, reduced immersion in data, and the formulaic nature of AI responses. The study offers three key insights: AI can be a powerful scaffold for active learning when paired with human facilitation; custom GPTs can serve as cognitive partners in iterative research practice; and educator-led design is critical to pedagogically meaningful AI integration. This research contributes to emerging scholarship on AI in higher education by demonstrating how empowering educators to design custom tools can promote more reflective, responsible, and collaborative learning with AI.', 'abstract_zh': '随着生成式人工智能（Gen-AI）工具在教育领域的广泛应用，有必要了解教育者如何能够积极地参与设计和使用这些工具。本研究探讨了两位讲师如何将四种自定义GPT工具整合到一门面向城市规划政策学生的定量研究方法硕士课程中，以解决两个关键缺口：将学生主要视为被动的AI用户和在定性方法教育中有限使用AI的问题。研究探讨了当与教学目标一致时，Gen-AI如何支持学科学习。本研究基于技术、教学法与内容知识（TPACK）框架和行动研究方法，设计GPT工具，以支持研究问题的形成、访谈练习、实地笔记分析和设计思维等任务。通过对学生反思、AI聊天记录和最终作业的主题分析发现，这些工具增强了学生的反思能力，提高了访谈技巧，并支持了结构化的分析思维。然而，学生也表达了认知负担过重、数据沉浸减少和AI回应格式化等问题的担忧。本研究提供了三个关键见解：当与人类引导相结合时，AI可以成为促进积极学习的强大支架；定制的GPT工具可以作为迭代研究实践中的认知伙伴；而教育者主导的设计对于教育意义深远的AI整合至关重要。本研究为高等教育中人工智能相关新兴研究作出了贡献，通过展示赋能教育者设计自定义工具如何促进更具反思性、责任感和协作性的AI学习。', 'title_zh': '在人工智能时代赋能教育者：立足定性研究方法教育的自定义GPT创建实证研究'}
{'arxiv_id': 'arXiv:2507.21071', 'title': 'FingerTip 20K: A Benchmark for Proactive and Personalized Mobile LLM Agents', 'authors': 'Qinglong Yang, Haoming Li, Haotian Zhao, Xiaokai Yan, Jingtao Ding, Fengli Xu, Yong Li', 'link': 'https://arxiv.org/abs/2507.21071', 'abstract': "Mobile GUI agents are becoming critical tools for enhancing human-device interaction efficiency, with multimodal large language models (MLLMs) emerging as dominant paradigms in this domain. Current agents, however, are limited to following explicit human instructions, resulting in insufficient capability for proactive intent anticipation. Additionally, these agents fail to leverage the contextual information associated with users during task execution, thereby neglecting potentially vast differences in user preferences. To address these challenges, we introduce the FingerTip benchmark. It contains two new tracks: proactive task suggestions by analyzing environment observation and users' previous intents, and personalized task execution by catering to users' action preferences. We collected unique human demonstrations of multi-step Android device interactions across a variety of everyday apps. These demonstrations are not isolated but are continuously acquired from the users' long-term usage in their real lives, and encompass essential user-related contextual information. Our experiments reveal challenges of the tasks we propose. The model fine-tuned with the data we collected effectively utilized user information and achieved good results, highlighting the potential of our approach in building more user-oriented mobile GUI agents. Our code is open-source at this https URL for reproducibility.", 'abstract_zh': '移动GUI代理正becoming critical tools for enhancing human-device interaction efficiency，其中多模态大规模语言模型（MLLMs）已成为这一领域的主导范式。然而，当前的代理仅限于遵循明确的人类指令，导致其主动意图预测能力不足。此外，这些代理在任务执行过程中未能利用用户的上下文信息，从而忽视了用户偏好之间可能存在的巨大差异。为应对这些挑战，我们引入了FingerTip基准。该基准包含两个新的赛道：基于环境观察和用户先前意图的主动任务建议，以及根据用户行为偏好定制的任务执行。我们收集了跨越各种日常应用的多步Android设备交互的唯一人类演示。这些演示不是孤立的，而是从用户在现实生活中的长期使用中不断获取，并包含关键的用户相关上下文信息。我们的实验揭示了我们提出任务的挑战。使用我们收集的数据 fine-tuned 的模型有效地利用了用户信息，并取得了良好的结果，强调了我们方法在构建更以用户为中心的移动GUI代理方面的潜力。我们的代码在此处公开 accessible at this https URL 以实现可重现性。', 'title_zh': '指尖20K：主动个性化移动LLM代理基准'}
{'arxiv_id': 'arXiv:2507.21055', 'title': 'Bridging the Gap: Enhancing News Interpretation Across Diverse Audiences with Large Language Models', 'authors': 'Leyi Ouyang', 'link': 'https://arxiv.org/abs/2507.21055', 'abstract': "In the interconnected world, news media are critical in conveying information to public across diverse domains including technology, finance, and agriculture. Journalists make efforts to present accurate information, however, the interpretation of news often varies significantly among different audiences due to their specific expertise and age. In this work, we investigate how to identify these comprehension gaps and provide solutions to improve audiences understanding of news content, particular to the aspects of articles outside their primary domains of knowledge. We propose a agent-based framework using large language models (LLMs) to simulate society communication behaviors, where several agents can discuss news. These agents can be designed to be experts from various occupation, or from different age group. Our results indicate that this framework can identify confusions or even misunderstanding of news for the agent through the iterative discussion process. Based on these accurate identification, the framework can design a supplement material specific to these agents on the news. Our results show that agents exhibit significantly improved news understanding after receiving this material. These findings highlight our framework's utility and efficiency in enhancing news comprehension for diverse audiences by directly addressing their understanding gap.", 'abstract_zh': '在互联世界中，新闻媒体在向涵盖科技、金融和农业等不同领域公众传递信息方面至关重要。记者努力呈现准确的信息，但由于不同受众的专业背景和年龄差异，新闻解释往往存在显著差异。本文研究如何识别这些理解和认知差距，并提出解决方案以提高受众对新闻内容的理解，特别是对于受众在其专业知识领域之外的文章内容理解。我们提出了一种基于代理的框架，利用大型语言模型（LLMs）模拟社会通信行为，多个代理可以讨论新闻。这些代理可以被设计为来自不同职业领域的专家，或来自不同年龄组的人。研究结果显示，通过迭代讨论过程，该框架能够识别代理在新闻理解上的困惑甚至误解。基于这些准确的识别，框架可以为这些代理设计特定的补充材料。研究表明，接收这些材料后，代理在新闻理解上显著提高。这些发现突显了该框架在通过直接解决受众理解差距来提高多元受众的新闻理解方面的作用和效率。', 'title_zh': '填补差距：利用大型语言模型增强跨多元化受众的新闻解读能力'}
{'arxiv_id': 'arXiv:2507.17307', 'title': 'R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning', 'authors': 'Zhuokun Chen, Zeren Chen, Jiahao He, Mingkui Tan, Jianfei Cai, Bohan Zhuang', 'link': 'https://arxiv.org/abs/2507.17307', 'abstract': "Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of large language models by encouraging step-by-step intermediate reasoning during inference. While effective, CoT introduces substantial computational overhead due to its reliance on autoregressive decoding over long token sequences. Existing acceleration strategies either reduce sequence length through early stopping or compressive reward designs, or improve decoding speed via speculative decoding with smaller models. However, speculative decoding suffers from limited speedup when the agreement between small and large models is low, and fails to exploit the potential advantages of small models in producing concise intermediate reasoning. In this paper, we present R-Stitch, a token-level, confidence-based hybrid decoding framework that accelerates CoT inference by switching between a small language model (SLM) and a large language model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to generate tokens by default and delegates to the LLM only when the SLM's confidence falls below a threshold. This design avoids full-sequence rollback and selectively invokes the LLM on uncertain steps, preserving both efficiency and answer quality. R-Stitch is model-agnostic, training-free, and compatible with standard decoding pipelines. Experiments on math reasoning benchmarks demonstrate that R-Stitch achieves up to 85\\% reduction in inference latency with negligible accuracy drop, highlighting its practical effectiveness in accelerating CoT reasoning.", 'abstract_zh': '链式推理（CoT）推理通过在推理过程中鼓励逐步的中间推理来增强大型语言模型的问题解决能力。尽管有效，但CoT由于依赖于长序列自回归解码，引入了显著的计算开销。现有的加速策略要么通过提前停止缩短序列长度，要么通过压缩奖励设计，或者通过使用较小模型的推测性解码来提高解码速度。然而，推测性解码当小模型和大模型的共识较低时，所能带来的加速有限，不能充分利用小模型在生成简洁中间推理方面的潜在优势。本文提出了一种基于token级和置信度的混合解码框架R-Stitch，通过在推理轨迹上切换使用小语言模型（SLM）和大型语言模型（LLM）来加速CoT推理。R-Stitch在默认情况下由SLM生成token，并在SLM的置信度低于阈值时仅委托给LLM。这种设计避免了全序列回滚，并仅在不确定步骤上选择性地调用LLM，既保持了效率，又保证了答案质量。R-Stitch是模型无关的、无需训练的，并与标准解码流水线兼容。在数学推理基准测试上的实验表明，R-Stitch可以实现高达85%的推理延迟减少，同时几乎没有准确率下降，展示了其在加速CoT推理方面的实际有效性。', 'title_zh': 'R-Stitch: 动态轨迹缝合以实现高效推理'}
