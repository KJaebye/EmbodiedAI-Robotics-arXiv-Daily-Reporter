{'arxiv_id': 'arXiv:2503.22541', 'title': 'SafeCast: Risk-Responsive Motion Forecasting for Autonomous Vehicles', 'authors': 'Haicheng Liao, Hanlin Kong, Bin Rao, Bonan Wang, Chengyue Wang, Guyang Yu, Yuming Huang, Ruru Tang, Chengzhong Xu, Zhenning Li', 'link': 'https://arxiv.org/abs/2503.22541', 'abstract': 'Accurate motion forecasting is essential for the safety and reliability of autonomous driving (AD) systems. While existing methods have made significant progress, they often overlook explicit safety constraints and struggle to capture the complex interactions among traffic agents, environmental factors, and motion dynamics. To address these challenges, we present SafeCast, a risk-responsive motion forecasting model that integrates safety-aware decision-making with uncertainty-aware adaptability. SafeCast is the first to incorporate the Responsibility-Sensitive Safety (RSS) framework into motion forecasting, encoding interpretable safety rules--such as safe distances and collision avoidance--based on traffic norms and physical principles. To further enhance robustness, we introduce the Graph Uncertainty Feature (GUF), a graph-based module that injects learnable noise into Graph Attention Networks, capturing real-world uncertainties and enhancing generalization across diverse scenarios. We evaluate SafeCast on four real-world benchmark datasets--Next Generation Simulation (NGSIM), Highway Drone (HighD), ApolloScape, and the Macao Connected Autonomous Driving (MoCAD)--covering highway, urban, and mixed-autonomy traffic environments. Our model achieves state-of-the-art (SOTA) accuracy while maintaining a lightweight architecture and low inference latency, underscoring its potential for real-time deployment in safety-critical AD systems.', 'abstract_zh': '准确的运动预测对于自主驾驶（AD）系统的安全性和可靠性至关重要。现有方法虽取得了显著进展，但往往忽视了明确的安全约束，并难以捕捉交通代理、环境因素和运动动力学之间的复杂交互。为应对这些挑战，我们提出了SafeCast，一种响应风险的运动预测模型，将安全意识决策与不确定性意识适应性相结合。SafeCast是首次将责任敏感安全（RSS）框架应用于运动预测，基于交通规范和物理原理编码可解释的安全规则，如安全距离和碰撞避免。为了进一步增强鲁棒性，我们引入了基于图的不确定性特征（GUF），这是一种基于图的模块，向图注意网络中注入可学习的噪声，捕捉现实世界的不确定性，提升不同场景下的泛化能力。我们在Next Generation Simulation（NGSIM）、Highway Drone（HighD）、ApolloScape和Macao Connected Autonomous Driving（MoCAD）四个真实世界基准数据集上评估了SafeCast，涵盖了高速公路、城市和混合自主交通环境。我们的模型在保持轻量级架构和低推理延迟的同时达到了最先进的（SOTA）准确度，突显了其在安全关键型AD系统中实时部署的潜力。', 'title_zh': 'SafeCast：响应风险的自主车辆运动预测'}
{'arxiv_id': 'arXiv:2503.22177', 'title': '3D Acetabular Surface Reconstruction from 2D Pre-operative X-ray Images using SRVF Elastic Registration and Deformation Graph', 'authors': 'Shuai Zhang, Jinliang Wang, Sujith Konandetails, Xu Wang, Danail Stoyanov, Evangelos B.Mazomenos', 'link': 'https://arxiv.org/abs/2503.22177', 'abstract': 'Accurate and reliable selection of the appropriate acetabular cup size is crucial for restoring joint biomechanics in total hip arthroplasty (THA). This paper proposes a novel framework that integrates square-root velocity function (SRVF)-based elastic shape registration technique with an embedded deformation (ED) graph approach to reconstruct the 3D articular surface of the acetabulum by fusing multiple views of 2D pre-operative pelvic X-ray images and a hemispherical surface model. The SRVF-based elastic registration establishes 2D-3D correspondences between the parametric hemispherical model and X-ray images, and the ED framework incorporates the SRVF-derived correspondences as constraints to optimize the 3D acetabular surface reconstruction using nonlinear least-squares optimization. Validations using both simulation and real patient datasets are performed to demonstrate the robustness and the potential clinical value of the proposed algorithm. The reconstruction result can assist surgeons in selecting the correct acetabular cup on the first attempt in primary THA, minimising the need for revision surgery.', 'abstract_zh': '基于平方根速度函数的弹性形状注册与嵌入变形图结合的股骨头臼杯大小精确选择框架：融合二维术前骨盆X射线多视角图像和半球面表面模型重构三维髋臼关节表面', 'title_zh': '基于SRVF弹性注册与变形图的术前X-ray图像三维髋臼表面重建'}
{'arxiv_id': 'arXiv:2503.22060', 'title': 'Deep Depth Estimation from Thermal Image: Dataset, Benchmark, and Challenges', 'authors': 'Ukcheol Shin, Jinsun Park', 'link': 'https://arxiv.org/abs/2503.22060', 'abstract': 'Achieving robust and accurate spatial perception under adverse weather and lighting conditions is crucial for the high-level autonomy of self-driving vehicles and robots. However, existing perception algorithms relying on the visible spectrum are highly affected by weather and lighting conditions. A long-wave infrared camera (i.e., thermal imaging camera) can be a potential solution to achieve high-level robustness. However, the absence of large-scale datasets and standardized benchmarks remains a significant bottleneck to progress in active research for robust visual perception from thermal images. To this end, this manuscript provides a large-scale Multi-Spectral Stereo (MS$^2$) dataset that consists of stereo RGB, stereo NIR, stereo thermal, stereo LiDAR data, and GNSS/IMU information along with semi-dense depth ground truth. MS$^2$ dataset includes 162K synchronized multi-modal data pairs captured across diverse locations (e.g., urban city, residential area, campus, and high-way road) at different times (e.g., morning, daytime, and nighttime) and under various weather conditions (e.g., clear-sky, cloudy, and rainy). Secondly, we conduct a thorough evaluation of monocular and stereo depth estimation networks across RGB, NIR, and thermal modalities to establish standardized benchmark results on MS$^2$ depth test sets (e.g., day, night, and rainy). Lastly, we provide in-depth analyses and discuss the challenges revealed by the benchmark results, such as the performance variability for each modality under adverse conditions, domain shift between different sensor modalities, and potential research direction for thermal perception. Our dataset and source code are publicly available at this https URL and this https URL.', 'abstract_zh': '在恶劣天气和光照条件下的稳健准确的空间知觉对于自动驾驶车辆和机器人的高级自主至关重要。然而，依赖可见光谱的现有感知算法在受到天气和光照条件的影响下表现较差。长波红外相机（即热成像相机）可能是实现高度稳健性的潜在解决方案。然而，大型数据集和标准化基准的缺乏仍然是稳健热成像感知研究中的重要瓶颈。为此，本文提供了一个大规模的多光谱立体（MS$^2$）数据集，该数据集包含立体RGB、立体近红外、立体热成像、立体LiDAR数据以及GNSS/IMU信息，并附有半密集深度地面真值。MS$^2$数据集包括在不同地点（例如城市、住宅区、校园、高速公路）和不同时段（例如早晨、白天、夜间）以及各种天气条件下（例如晴朗、多云、雨天）同步采集的162,000多组多模态数据对。其次，我们在RGB、近红外和热成像模态下对单目和立体深度估计网络进行了全面评估，以在MS$^2$深度测试集（例如白天、夜晚和雨天）上确立标准化基准结果。最后，我们进行了深入分析，并讨论了基准结果揭示的挑战，例如在不利条件下每个模态的性能变异性、不同传感器模态之间的领域转移以及热感知研究潜在的研究方向。我们的数据集和源代码可在以下网址获取：this https URL 和 this https URL。', 'title_zh': '基于热成像的深度估计：数据集、基准和挑战'}
{'arxiv_id': 'arXiv:2503.22673', 'title': 'ActionStudio: A Lightweight Framework for Data and Training of Action Models', 'authors': 'Jianguo Zhang, Thai Hoang, Ming Zhu, Zuxin Liu, Shiyu Wang, Tulika Awalgaonkar, Akshara Prabhakar, Haolin Chen, Weiran Yao, Zhiwei Liu, Juntao Tan, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Silvio Savarese, Caiming Xiong', 'link': 'https://arxiv.org/abs/2503.22673', 'abstract': 'Action models are essential for enabling autonomous agents to perform complex tasks. However, training large action models remains challenging due to the diversity of agent environments and the complexity of agentic data. Despite growing interest, existing infrastructure provides limited support for scalable, agent-specific fine-tuning. We present ActionStudio, a lightweight and extensible data and training framework designed for action models. ActionStudio unifies heterogeneous agent trajectories through a standardized format, supports diverse training paradigms including LoRA, full fine-tuning, and distributed setups, and integrates robust preprocessing and verification tools. We validate its effectiveness across both public and realistic industry benchmarks, demonstrating strong performance and practical scalability. We open-sourced code and data at this https URL to facilitate research in the community.', 'abstract_zh': '行动模型对于使自主代理能够执行复杂任务是必不可少的。然而，由于代理环境的多样性和代理数据的复杂性，训练大规模行动模型仍然具有挑战性。尽管现有兴趣不断增长，现有基础设施对可扩展的、针对代理特定的微调支持有限。我们提出了ActionStudio，一个轻量级且可扩展的数据和训练框架，专门设计用于行动模型。ActionStudio 通过标准化格式统一了异构代理轨迹，支持包括LoRA、全程微调和分布式设置在内的多种训练范式，并集成了稳健的预处理和验证工具。我们在公共和现实行业的基准测试中验证了其有效性，展示了强大的性能和实际的可扩展性。我们已在此<https://>URL 开放了代码和数据，以促进社区中的研究。', 'title_zh': 'ActionStudio: 一种轻量级的动作模型数据与训练框架'}
{'arxiv_id': 'arXiv:2503.22658', 'title': 'Evaluation of Machine-generated Biomedical Images via A Tally-based Similarity Measure', 'authors': 'Frank J. Brooks, Rucha Deshpande', 'link': 'https://arxiv.org/abs/2503.22658', 'abstract': "Super-resolution, in-painting, whole-image generation, unpaired style-transfer, and network-constrained image reconstruction each include an aspect of machine-learned image synthesis where the actual ground truth is not known at time of use. It is generally difficult to quantitatively and authoritatively evaluate the quality of synthetic images; however, in mission-critical biomedical scenarios robust evaluation is paramount. In this work, all practical image-to-image comparisons really are relative qualifications, not absolute difference quantifications; and, therefore, meaningful evaluation of generated image quality can be accomplished using the Tversky Index, which is a well-established measure for assessing perceptual similarity. This evaluation procedure is developed and then demonstrated using multiple image data sets, both real and simulated. The main result is that when the subjectivity and intrinsic deficiencies of any feature-encoding choice are put upfront, Tversky's method leads to intuitive results, whereas traditional methods based on summarizing distances in deep feature spaces do not.", 'abstract_zh': '超分辨率、 inpainting、整图生成、无配对风格转移以及网络约束图像重建各自包含机器学习生成图像的一个方面，其中实际的ground truth在使用时未知。通常难以定量和权威性地评估合成图像的质量；然而，在关键的生物医学场景中，稳健的评估至关重要。在本工作中，所有实际的图像到图像比较实际上是相对的评价，而不是绝对差异的量化；因此，可以使用Tversky指数来评估生成图像质量，该指数是一个已建立的感知相似性评估措施。该评估程序通过多种实际和模拟图像数据集得到开发和验证。主要结果是，当将任何特征编码选择的主观性和内在缺陷摆到台面上时，Tversky的方法会产生直观的结果，而基于深入特征空间距离汇总的传统方法则不会。', 'title_zh': '基于计数的相似度量评价机器生成的生物医学图像'}
{'arxiv_id': 'arXiv:2503.22592', 'title': 'KEVS: Enhancing Segmentation of Visceral Adipose Tissue in Pre-Cystectomy CT with Gaussian Kernel Density Estimation', 'authors': 'Thomas Boucher, Nicholas Tetlow, Annie Fung, Amy Dewar, Pietro Arina, Sven Kerneis, John Whittle, Evangelos B. Mazomenos', 'link': 'https://arxiv.org/abs/2503.22592', 'abstract': 'Purpose: The distribution of visceral adipose tissue (VAT) in cystectomy patients is indicative of the incidence of post-operative complications. Existing VAT segmentation methods for computed tomography (CT) employing intensity thresholding have limitations relating to inter-observer variability. Moreover, the difficulty in creating ground-truth masks limits the development of deep learning (DL) models for this task. This paper introduces a novel method for VAT prediction in pre-cystectomy CT, which is fully automated and does not require ground-truth VAT masks for training, overcoming aforementioned limitations. Methods: We introduce the Kernel density Enhanced VAT Segmentator ( KEVS), combining a DL semantic segmentation model, for multi-body feature prediction, with Gaussian kernel density estimation analysis of predicted subcutaneous adipose tissue to achieve accurate scan-specific predictions of VAT in the abdominal cavity. Uniquely for a DL pipeline, KEVS does not require ground-truth VAT masks. Results: We verify the ability of KEVS to accurately segment abdominal organs in unseen CT data and compare KEVS VAT segmentation predictions to existing state-of-the-art (SOTA) approaches in a dataset of 20 pre-cystectomy CT scans, collected from University College London Hospital (UCLH-Cyst), with expert ground-truth annotations. KEVS presents a 4.80% and 6.02% improvement in Dice Coefficient over the second best DL and thresholding-based VAT segmentation techniques respectively when evaluated on UCLH-Cyst. Conclusion: This research introduces KEVS; an automated, SOTA method for the prediction of VAT in pre-cystectomy CT which eliminates inter-observer variability and is trained entirely on open-source CT datasets which do not contain ground-truth VAT masks.', 'abstract_zh': '目的：膀胱切除术患者腹内脂肪组织（VAT）的分布是术后并发症发生的指示器。现有的基于CT的VAT分割方法使用强度阈值分割存在因观察者间差异而导致的局限性。此外，创建ground-truth掩模的难度限制了该任务深度学习（DL）模型的发展。本文提出了一种新的用于膀胱切除术前CT中VAT预测的方法，该方法完全自动化且无需训练时使用ground-truth VAT掩模，克服了上述局限性。方法：我们引入了一种名为Kernel density Enhanced VAT Segmentator（KEVS）的方法，结合了一个基于DL的语义分割模型用于多体素特征预测，并通过高斯核密度估计分析预测的皮下脂肪组织以在腹腔中实现准确的扫描特定VAT分割。唯一不同的是，KEVS无需ground-truth VAT掩模。结果：我们验证了KEVS在未见过的CT数据中准确分割腹部器官的能力，并将KEVS的VAT分割预测与20例来自University College London Hospital（UCLH-Cyst）的膀胱切除术前CT扫描数据集中的现有最佳方法进行了比较，该数据集具有专家ground-truth注释。当在UCLH-Cyst数据集上评估时，KEVS分别在第二好的DL方法和基于阈值的VAT分割技术上分别取得了4.80%和6.02%的Dice系数改进。结论：本文介绍了一种自动化的、基于开源CT数据集训练、无需ground-truth VAT掩模的最新方法KEVS，用于膀胱切除术前CT中VAT的预测，消除了观察者间差异。', 'title_zh': 'KEVS: 用高斯核密度估计增强肾切除术CT前期内脏脂肪组织分割'}
{'arxiv_id': 'arXiv:2503.22537', 'title': 'LIM: Large Interpolator Model for Dynamic Reconstruction', 'authors': 'Remy Sabathier, Niloy J. Mitra, David Novotny', 'link': 'https://arxiv.org/abs/2503.22537', 'abstract': 'Reconstructing dynamic assets from video data is central to many in computer vision and graphics tasks. Existing 4D reconstruction approaches are limited by category-specific models or slow optimization-based methods. Inspired by the recent Large Reconstruction Model (LRM), we present the Large Interpolation Model (LIM), a transformer-based feed-forward solution, guided by a novel causal consistency loss, for interpolating implicit 3D representations across time. Given implicit 3D representations at times $t_0$ and $t_1$, LIM produces a deformed shape at any continuous time $t\\in[t_0,t_1]$, delivering high-quality interpolated frames in seconds. Furthermore, LIM allows explicit mesh tracking across time, producing a consistently uv-textured mesh sequence ready for integration into existing production pipelines. We also use LIM, in conjunction with a diffusion-based multiview generator, to produce dynamic 4D reconstructions from monocular videos. We evaluate LIM on various dynamic datasets, benchmarking against image-space interpolation methods (e.g., FiLM) and direct triplane linear interpolation, and demonstrate clear advantages. In summary, LIM is the first feed-forward model capable of high-speed tracked 4D asset reconstruction across diverse categories.', 'abstract_zh': '从视频数据中重构动态资产是计算机视觉和图形任务中的核心问题。现有的4D重构方法受限于类别特定模型或慢速的优化方法。受到最近提出的大型重构模型（LRM）的启发，我们提出了大型插值模型（LIM），这是一种基于变压器的前馈解决方案，并由一个新的因果一致性损失引导，用于在时间上插值隐式的3D表示。给定时间$t_0$和$t_1$的隐式3D表示，LIM可在任意连续时间$t\\in[t_0,t_1]$生成变形的形状，并在秒内提供高质量的插值帧。此外，LIM允许在时间上的显式网格追踪，生成可用于现有生产流水线的连续uv纹理网格序列。我们也使用LIM结合基于扩散的多视图生成器，从单目视频中生成动态4D重构。我们对各种动态数据集评估了LIM，将其与图像空间插值方法（例如，FiLM）和直接三平面线性插值进行基准测试，并展示了明显的优越性。总结来说，LIM是首个能够高速跨类别进行跟踪的4D资产重构的前馈模型。', 'title_zh': '大插值模型用于动态重建'}
{'arxiv_id': 'arXiv:2503.22394', 'title': 'Endo-TTAP: Robust Endoscopic Tissue Tracking via Multi-Facet Guided Attention and Hybrid Flow-point Supervision', 'authors': 'Rulin Zhou, Wenlong He, An Wang, Qiqi Yao, Haijun Hu, Jiankun Wang, Xi Zhang an Hongliang Ren', 'link': 'https://arxiv.org/abs/2503.22394', 'abstract': 'Accurate tissue point tracking in endoscopic videos is critical for robotic-assisted surgical navigation and scene understanding, but remains challenging due to complex deformations, instrument occlusion, and the scarcity of dense trajectory annotations. Existing methods struggle with long-term tracking under these conditions due to limited feature utilization and annotation dependence. We present Endo-TTAP, a novel framework addressing these challenges through: (1) A Multi-Facet Guided Attention (MFGA) module that synergizes multi-scale flow dynamics, DINOv2 semantic embeddings, and explicit motion patterns to jointly predict point positions with uncertainty and occlusion awareness; (2) A two-stage curriculum learning strategy employing an Auxiliary Curriculum Adapter (ACA) for progressive initialization and hybrid supervision. Stage I utilizes synthetic data with optical flow ground truth for uncertainty-occlusion regularization, while Stage II combines unsupervised flow consistency and semi-supervised learning with refined pseudo-labels from off-the-shelf trackers. Extensive validation on two MICCAI Challenge datasets and our collected dataset demonstrates that Endo-TTAP achieves state-of-the-art performance in tissue point tracking, particularly in scenarios characterized by complex endoscopic conditions. The source code and dataset will be available at this https URL.', 'abstract_zh': '精确的内窥镜视频组织点跟踪对于机器人辅助手术导航和场景理解至关重要，但由于复杂的变形、器械遮挡和密集轨迹注解稀缺的限制，这一任务仍然具有挑战性。现有方法在这些条件下难以实现长期跟踪，因为它们在特征利用和注解依赖方面存在局限。我们提出了Endo-TTAP，这是一种通过以下方式解决这些挑战的新框架：(1) 多尺度流动态、DINOv2语义嵌入和显式运动模式协同作用的多面引导注意力(MFGA)模块，用于联合预测点位置及其不确定性和遮挡感知；(2) 采用辅助课程适配器(ACA)的两阶段课程学习策略，实现渐进初始化和混合监督。第一阶段利用具有光学流 ground truth 的合成数据进行不确定性和遮挡正则化，而第二阶段结合了无监督流一致性监督与改进的离线成品跟踪器伪标签的半监督学习。在两个MICCAI挑战数据集和我们收集的数据集上的广泛验证表明，Endo-TTAP 在组织点跟踪方面达到了最先进的性能，特别是在复杂内窥镜条件下。源代码和数据集将在此处提供。', 'title_zh': '内窥镜组织跟踪器：基于多面引导注意力和混合流点监督的稳健内窥镜组织跟踪'}
{'arxiv_id': 'arXiv:2503.22374', 'title': 'ViSketch-GPT: Collaborative Multi-Scale Feature Extraction for Sketch Recognition and Generation', 'authors': 'Giulio Federico, Giuseppe Amato, Fabio Carrara, Claudio Gennaro, Marco Di Benedetto', 'link': 'https://arxiv.org/abs/2503.22374', 'abstract': 'Understanding the nature of human sketches is challenging because of the wide variation in how they are created. Recognizing complex structural patterns improves both the accuracy in recognizing sketches and the fidelity of the generated sketches. In this work, we introduce ViSketch-GPT, a novel algorithm designed to address these challenges through a multi-scale context extraction approach. The model captures intricate details at multiple scales and combines them using an ensemble-like mechanism, where the extracted features work collaboratively to enhance the recognition and generation of key details crucial for classification and generation tasks.\nThe effectiveness of ViSketch-GPT is validated through extensive experiments on the QuickDraw dataset. Our model establishes a new benchmark, significantly outperforming existing methods in both classification and generation tasks, with substantial improvements in accuracy and the fidelity of generated sketches.\nThe proposed algorithm offers a robust framework for understanding complex structures by extracting features that collaborate to recognize intricate details, enhancing the understanding of structures like sketches and making it a versatile tool for various applications in computer vision and machine learning.', 'abstract_zh': '理解人类草图的本质因创作方式的广泛差异而具有挑战性。识别复杂的结构模式可以提高草图识别的准确性和生成草图的保真度。在本工作中，我们提出了一种名为ViSketch-GPT的新型算法，通过多尺度上下文提取方法来应对这些挑战。该模型在多尺度上捕捉复杂的细微特征，并通过类似集成的机制将它们结合在一起，提取的特征协同工作以增强关键细节的识别和生成，这些细节对于分类和生成任务至关重要。\n\nViSketch-GPT的有效性通过在QuickDraw数据集上的大量实验得到验证。我们的模型在分类和生成任务中均建立了新的基准，显著优于现有方法，在准确性和生成草图的保真度方面取得了显着改进。\n\n提出的算法提供了一种稳健的框架，用于通过提取协同工作的特征来理解复杂结构，增强了对如草图等结构的理解，并使其成为计算机视觉和机器学习中各种应用的多功能工具。', 'title_zh': 'ViSketch-GPT: 协同多尺度特征提取的草图识别与生成'}
{'arxiv_id': 'arXiv:2503.22363', 'title': 'ForcePose: A Deep Learning Approach for Force Calculation Based on Action Recognition Using MediaPipe Pose Estimation Combined with Object Detection', 'authors': 'Nandakishor M, Vrinda Govind V, Anuradha Puthalath, Anzy L, Swathi P S, Aswathi R, Devaprabha A R, Varsha Raj, Midhuna Krishnan K, Akhila Anilkumar T V, Yamuna P V', 'link': 'https://arxiv.org/abs/2503.22363', 'abstract': "Force estimation in human-object interactions is crucial for various fields like ergonomics, physical therapy, and sports science. Traditional methods depend on specialized equipment such as force plates and sensors, which makes accurate assessments both expensive and restricted to laboratory settings. In this paper, we introduce ForcePose, a novel deep learning framework that estimates applied forces by combining human pose estimation with object detection. Our approach leverages MediaPipe for skeletal tracking and SSD MobileNet for object recognition to create a unified representation of human-object interaction. We've developed a specialized neural network that processes both spatial and temporal features to predict force magnitude and direction without needing any physical sensors. After training on our dataset of 850 annotated videos with corresponding force measurements, our model achieves a mean absolute error of 5.83 N in force magnitude and 7.4 degrees in force direction. When compared to existing computer vision approaches, our method performs 27.5% better while still offering real-time performance on standard computing hardware. ForcePose opens up new possibilities for force analysis in diverse real-world scenarios where traditional measurement tools are impractical or intrusive. This paper discusses our methodology, the dataset creation process, evaluation metrics, and potential applications across rehabilitation, ergonomics assessment, and athletic performance analysis.", 'abstract_zh': '人体与物体交互中的力估计对于人机工程学、物理治疗和运动科学等领域至关重要。传统方法依赖于力板和传感器等专用设备，这使得准确评估既昂贵又局限于实验室环境。本文介绍了一种新颖的深度学习框架ForcePose，通过结合人体姿态估计和物体检测来估计施加的力。我们的方法利用MediaPipe进行骨骼跟踪，SSD-MobileNet进行物体识别，从而创建人体-物体交互的统一表示。我们开发了一个专用的神经网络，处理空间和时间特征以预测力的大小和方向，而无需任何物理传感器。在包含850个标注视频及其相应力测量值的训练集上训练后，我们的模型在力的大小上实现了5.83 N的平均绝对误差，在力的方向上实现了7.4度的误差。与现有的计算机视觉方法相比，我们的方法在标准计算硬件上仍能实现实时性能，且性能提高了27.5%。ForcePose为在传统测量工具不切实际或侵入性的多种现实场景中进行力分析开辟了新途径。本文讨论了我们的方法论、数据集创建过程、评估指标以及在康复、人机工程学评估和运动表现分析中的潜在应用。', 'title_zh': '基于MediaPipe姿态估计与物体检测结合的动作识别的力计算深度学习方法：ForcePose'}
{'arxiv_id': 'arXiv:2503.22328', 'title': 'VoteFlow: Enforcing Local Rigidity in Self-Supervised Scene Flow', 'authors': 'Yancong Lin, Shiming Wang, Liangliang Nan, Julian Kooij, Holger Caesar', 'link': 'https://arxiv.org/abs/2503.22328', 'abstract': 'Scene flow estimation aims to recover per-point motion from two adjacent LiDAR scans. However, in real-world applications such as autonomous driving, points rarely move independently of others, especially for nearby points belonging to the same object, which often share the same motion. Incorporating this locally rigid motion constraint has been a key challenge in self-supervised scene flow estimation, which is often addressed by post-processing or appending extra regularization. While these approaches are able to improve the rigidity of predicted flows, they lack an architectural inductive bias for local rigidity within the model structure, leading to suboptimal learning efficiency and inferior performance. In contrast, we enforce local rigidity with a lightweight add-on module in neural network design, enabling end-to-end learning. We design a discretized voting space that accommodates all possible translations and then identify the one shared by nearby points by differentiable voting. Additionally, to ensure computational efficiency, we operate on pillars rather than points and learn representative features for voting per pillar. We plug the Voting Module into popular model designs and evaluate its benefit on Argoverse 2 and Waymo datasets. We outperform baseline works with only marginal compute overhead. Code is available at this https URL.', 'abstract_zh': '场景流估计旨在从两个相邻的LiDAR扫描中恢复每个点的运动。然而，在自动驾驶等实际应用中，点之间通常不是独立移动的，尤其是属于同一物体的附近点，它们往往共享相同的运动。将这种局部刚性运动约束纳入自监督场景流估计中一直是一个关键挑战，通常通过后处理或附加额外正则化来解决。虽然这些方法能够提高预测流的刚性，但它们缺乏用于局部刚性的架构归纳偏差，导致学习效率低下且性能欠佳。相比之下，我们通过神经网络设计中的轻量级附加模块来强制实施局部刚性，从而实现端到端学习。我们设计了一个离散化投票空间，包含所有可能的平移，并通过可微投票识别附近点共有的平移。此外，为了确保计算效率，我们基于柱状结构进行操作，并为每个柱状结构学习代表特征用于投票。我们将投票模块插入流行模型设计中，并在Argoverse 2和Waymo数据集上评估其益处。我们仅以微小的计算成本超出了基线工作。代码可在以下链接获取。', 'title_zh': 'VoteFlow: 强化自监督场景流中的局部刚性约束'}
{'arxiv_id': 'arXiv:2503.22324', 'title': 'AH-GS: Augmented 3D Gaussian Splatting for High-Frequency Detail Representation', 'authors': 'Chenyang Xu, XingGuo Deng, Rui Zhong', 'link': 'https://arxiv.org/abs/2503.22324', 'abstract': "The 3D Gaussian Splatting (3D-GS) is a novel method for scene representation and view synthesis. Although Scaffold-GS achieves higher quality real-time rendering compared to the original 3D-GS, its fine-grained rendering of the scene is extremely dependent on adequate viewing angles. The spectral bias of neural network learning results in Scaffold-GS's poor ability to perceive and learn high-frequency information in the scene. In this work, we propose enhancing the manifold complexity of input features and using network-based feature map loss to improve the image reconstruction quality of 3D-GS models. We introduce AH-GS, which enables 3D Gaussians in structurally complex regions to obtain higher-frequency encodings, allowing the model to more effectively learn the high-frequency information of the scene. Additionally, we incorporate high-frequency reinforce loss to further enhance the model's ability to capture detailed frequency information. Our result demonstrates that our model significantly improves rendering fidelity, and in specific scenarios (e.g., MipNeRf360-garden), our method exceeds the rendering quality of Scaffold-GS in just 15K iterations.", 'abstract_zh': '3D高斯喷涂增强的复杂流形输入特征和基于网络的特征图损失在场景表示和视图合成中的应用', 'title_zh': 'AH-GS: 增强的3D高斯点云表示方法用于高频细节表达'}
{'arxiv_id': 'arXiv:2503.22182', 'title': 'Sell It Before You Make It: Revolutionizing E-Commerce with Personalized AI-Generated Items', 'authors': 'Jianghao Lin, Peng Du, Jiaqi Liu, Weite Li, Yong Yu, Weinan Zhang, Yang Cao', 'link': 'https://arxiv.org/abs/2503.22182', 'abstract': 'E-commerce has revolutionized retail, yet its traditional workflows remain inefficient, with significant time and resource costs tied to product design and manufacturing inventory. This paper introduces a novel system deployed at Alibaba that leverages AI-generated items (AIGI) to address these challenges with personalized text-to-image generation for e-commercial product design. AIGI enables an innovative business mode called "sell it before you make it", where merchants can design fashion items and generate photorealistic images with digital models based on textual descriptions. Only when the items have received a certain number of orders, do the merchants start to produce them, which largely reduces reliance on physical prototypes and thus accelerates time to market. For such a promising application, we identify the underlying key scientific challenge, i.e., capturing the users\' group-level personalized preferences towards multiple generated candidate images. To this end, we propose a Personalized Group-Level Preference Alignment Framework for Diffusion Models (i.e., PerFusion). We first design PerFusion Reward Model for user preference estimation with a feature-crossing-based personalized plug-in. Then we develop PerFusion with a personalized adaptive network to model diverse preferences across users, and meanwhile derive the group-level preference optimization objective to capture the comparative behaviors among multiple candidates. Both offline and online experiments demonstrate the effectiveness of our proposed algorithm. The AI-generated items have achieved over 13% relative improvements for both click-through rate and conversion rate compared to their human-designed counterparts, validating the revolutionary potential of AI-generated items for e-commercial platforms.', 'abstract_zh': '电商平台已经重塑了零售业，但其传统的 workflows仍然效率低下，产品设计和制造库存涉及大量时间和资源成本。本文介绍了一种部署在阿里巴巴的新系统，该系统利用AI生成的物品（AIGI）通过个性化文本到图像生成解决电商平台产品设计中的挑战，实现了“先卖后制”的创新商业模式。商家可以根据文本描述设计时尚商品并生成逼真的图像。只有当商品收到一定数量的订单后，商家才开始生产，这大大减少了对实物原型的依赖，从而加快了市场时间。针对这一有前景的应用，我们确定了其背后的科学挑战，即捕捉用户群体级个性化偏好对多个生成候选图像的理解。为此，我们提出了基于扩散模型的个性化群体级偏好对齐框架（即PerFusion）。我们首先设计了基于特征交叉的个性化插件的PerFusion奖励模型以进行用户偏好估计，然后开发了个性化自适应网络以建模用户的多样偏好，并同时推导出群体级偏好优化目标以捕捉多个候选者的相对行为。离线和在线实验均证明了我们提出算法的有效性。AI生成的物品在点击率和转换率上相对于人类设计的商品分别实现了超过13%的相对提升，验证了AI生成的商品在电商平台上具有革命性的潜力。', 'title_zh': '在制造之前就卖掉它：以个性化AI生成物品革新电子商务'}
{'arxiv_id': 'arXiv:2503.22093', 'title': "How Well Can Vison-Language Models Understand Humans' Intention? An Open-ended Theory of Mind Question Evaluation Benchmark", 'authors': 'Ximing Wen, Mallika Mainali, Anik Sen', 'link': 'https://arxiv.org/abs/2503.22093', 'abstract': "Vision Language Models (VLMs) have demonstrated strong reasoning capabilities in Visual Question Answering (VQA) tasks; However, their ability to perform Theory of Mind (ToM) tasks such as accurately inferring human intentions, beliefs, and other mental states remains underexplored. In this work, we propose an open-ended question framework to comprehensively evaluate VLMs' performance across diverse categories of ToM tasks. We curated and annotated a benchmark dataset composed of 30 images. We then assessed the performance of four VLMs of varying sizes on this dataset. Our experimental results show that the GPT-4 model outperformed all others, with only one smaller model, GPT-4o-mini, achieving comparable performance. Additionally, we observed that VLMs often struggle to accurately infer intentions in complex scenarios such as bullying or cheating. Moreover, our findings also reveal that smaller models can sometimes infer correct intentions despite relying on incorrect visual cues.", 'abstract_zh': '视觉语言模型在同理心任务中的表现：一个开放性问题框架的探索', 'title_zh': '视觉-语言模型能多准确地理解人类的意图？一个开放性理论思维问题评价基准'}
{'arxiv_id': 'arXiv:2503.22069', 'title': 'Contrasting Low and High-Resolution Features for HER2 Scoring using Deep Learning', 'authors': 'Ekansh Chauhan, Anila Sharma, Amit Sharma, Vikas Nishadham, Asha Ghughtyal, Ankur Kumar, Gurudutt Gupta, Anurag Mehta, C.V. Jawahar, P.K. Vinod', 'link': 'https://arxiv.org/abs/2503.22069', 'abstract': "Breast cancer, the most common malignancy among women, requires precise detection and classification for effective treatment. Immunohistochemistry (IHC) biomarkers like HER2, ER, and PR are critical for identifying breast cancer subtypes. However, traditional IHC classification relies on pathologists' expertise, making it labor-intensive and subject to significant inter-observer variability. To address these challenges, this study introduces the India Pathology Breast Cancer Dataset (IPD-Breast), comprising of 1,272 IHC slides (HER2, ER, and PR) aimed at automating receptor status classification. The primary focus is on developing predictive models for HER2 3-way classification (0, Low, High) to enhance prognosis. Evaluation of multiple deep learning models revealed that an end-to-end ConvNeXt network utilizing low-resolution IHC images achieved an AUC, F1, and accuracy of 91.79%, 83.52%, and 83.56%, respectively, for 3-way classification, outperforming patch-based methods by over 5.35% in F1 score. This study highlights the potential of simple yet effective deep learning techniques to significantly improve accuracy and reproducibility in breast cancer classification, supporting their integration into clinical workflows for better patient outcomes.", 'abstract_zh': '乳腺癌，女性最常见的恶性肿瘤，要求精确检测和分类以实现有效的治疗。免疫组织化学（IHC）标志物如HER2、ER和PR对于识别乳腺癌亚型至关重要。然而，传统的IHC分类依赖于病理学家的专业知识，这使得其劳动密集且具有显著的观察者间变异。为了应对这些挑战，本研究引入了印度病理学乳腺癌数据集（IPD-Breast），包含1,272张IHC切片（HER2、ER和PR），旨在实现受体状态的自动化分类。主要重点是开发HER2三分类（0、低、高）的预测模型，以提高预后。多种深度学习模型的评估表明，使用低分辨率IHC图像的端到端ConvNeXt网络在三分类中的AUC、F1和准确率分别为91.79%、83.52%和83.56%，其F1得分比基于patch的方法高出5.35%以上。本研究突显了简单有效的深度学习技术在提高乳腺癌分类的准确性和可重复性方面的潜力，支持其集成到临床工作流程中以改善患者预后。', 'title_zh': '低分辨率与高分辨率特征在使用深度学习进行HER2评分中的对比'}
{'arxiv_id': 'arXiv:2503.21991', 'title': 'BOOTPLACE: Bootstrapped Object Placement with Detection Transformers', 'authors': 'Hang Zhou, Xinxin Zuo, Rui Ma, Li Cheng', 'link': 'https://arxiv.org/abs/2503.21991', 'abstract': "In this paper, we tackle the copy-paste image-to-image composition problem with a focus on object placement learning. Prior methods have leveraged generative models to reduce the reliance for dense supervision. However, this often limits their capacity to model complex data distributions. Alternatively, transformer networks with a sparse contrastive loss have been explored, but their over-relaxed regularization often leads to imprecise object placement. We introduce BOOTPLACE, a novel paradigm that formulates object placement as a placement-by-detection problem. Our approach begins by identifying suitable regions of interest for object placement. This is achieved by training a specialized detection transformer on object-subtracted backgrounds, enhanced with multi-object supervisions. It then semantically associates each target compositing object with detected regions based on their complementary characteristics. Through a boostrapped training approach applied to randomly object-subtracted images, our model enforces meaningful placements through extensive paired data augmentation. Experimental results on established benchmarks demonstrate BOOTPLACE's superior performance in object repositioning, markedly surpassing state-of-the-art baselines on Cityscapes and OPA datasets with notable improvements in IOU scores. Additional ablation studies further showcase the compositionality and generalizability of our approach, supported by user study evaluations.", 'abstract_zh': '本文介绍了BOOTPLACE，一种新颖的物体放置学习范式，将物体放置问题形式化为检测问题。通过在物体减背景上训练专门的检测变压器，并结合多物体监督，我们的方法首先识别适合放置物体的感兴趣区域。然后基于检测到的区域和目标组成物体的互补特征进行语义关联。通过应用于随机物体减背景图像的自助训练方法，我们的模型通过广泛的配对数据增强强制实现有意义的放置。在建立的基线上的实验结果表明，BOOTPLACE在物体重新定位任务中的性能显著优于现有最先进的基线，特别是在Cityscapes和OPA数据集上，IOU分数有显著提高。额外的消融研究进一步展示了我们方法的组合性和泛化能力，并得到了用户研究的验证。', 'title_zh': 'BOOTPLACE: 基于检测变换器的自举对象放置'}
{'arxiv_id': 'arXiv:2503.21943', 'title': 'Parametric Shadow Control for Portrait Generationin Text-to-Image Diffusion Models', 'authors': 'Haoming Cai, Tsung-Wei Huang, Shiv Gehlot, Brandon Y. Feng, Sachin Shah, Guan-Ming Su, Christopher Metzler', 'link': 'https://arxiv.org/abs/2503.21943', 'abstract': 'Text-to-image diffusion models excel at generating diverse portraits, but lack intuitive shadow control. Existing editing approaches, as post-processing, struggle to offer effective manipulation across diverse styles. Additionally, these methods either rely on expensive real-world light-stage data collection or require extensive computational resources for training. To address these limitations, we introduce Shadow Director, a method that extracts and manipulates hidden shadow attributes within well-trained diffusion models. Our approach uses a small estimation network that requires only a few thousand synthetic images and hours of training-no costly real-world light-stage data needed. Shadow Director enables parametric and intuitive control over shadow shape, placement, and intensity during portrait generation while preserving artistic integrity and identity across diverse styles. Despite training only on synthetic data built on real-world identities, it generalizes effectively to generated portraits with diverse styles, making it a more accessible and resource-friendly solution.', 'abstract_zh': '基于文本描述的图像扩散模型在生成多样化的肖像方面表现出色，但缺乏直观的阴影控制。现有的编辑方法作为后处理手段，在不同风格下难以实现有效的操作。此外，这些方法要么依赖昂贵的现实世界光源采集数据，要么需要大量的计算资源进行训练。为解决这些局限性，我们引入了Shadow Director方法，该方法从well-trained扩散模型中提取和操控隐藏的阴影属性。我们的方法使用一个小的估计网络，仅需少量（几千张）合成图像和几小时的训练时间，无需昂贵的现实世界光源采集数据。Shadow Director在肖像生成过程中提供了参数化和直观的阴影形状、位置和强度控制，同时保持了不同风格下的艺术完整性和身份特征。尽管仅在基于真实身份的合成数据上进行训练，但其能够有效泛化到具有不同风格的生成肖像，使其更具可访问性和资源友好性。', 'title_zh': '面向肖像生成的参数化阴影控制在文本到图像扩散模型中'}
{'arxiv_id': 'arXiv:2503.21910', 'title': 'JEEM: Vision-Language Understanding in Four Arabic Dialects', 'authors': 'Karima Kadaoui, Hanin Atwany, Hamdan Al-Ali, Abdelrahman Mohamed, Ali Mekky, Sergei Tilga, Natalia Fedorova, Ekaterina Artemova, Hanan Aldarmaki, Yova Kementchedjhieva', 'link': 'https://arxiv.org/abs/2503.21910', 'abstract': "We introduce JEEM, a benchmark designed to evaluate Vision-Language Models (VLMs) on visual understanding across four Arabic-speaking countries: Jordan, The Emirates, Egypt, and Morocco. JEEM includes the tasks of image captioning and visual question answering, and features culturally rich and regionally diverse content. This dataset aims to assess the ability of VLMs to generalize across dialects and accurately interpret cultural elements in visual contexts. In an evaluation of five prominent open-source Arabic VLMs and GPT-4V, we find that the Arabic VLMs consistently underperform, struggling with both visual understanding and dialect-specific generation. While GPT-4V ranks best in this comparison, the model's linguistic competence varies across dialects, and its visual understanding capabilities lag behind. This underscores the need for more inclusive models and the value of culturally-diverse evaluation paradigms.", 'abstract_zh': 'JEEM：一种用于评估视觉-语言模型在阿拉伯语地区视觉理解能力的基准测试', 'title_zh': 'JEEM: 四种阿拉伯方言的视觉-语言理解'}
{'arxiv_id': 'arXiv:2503.21889', 'title': 'StarFlow: Generating Structured Workflow Outputs From Sketch Images', 'authors': 'Patrice Bechard, Chao Wang, Amirhossein Abaskohi, Juan Rodriguez, Christopher Pal, David Vazquez, Spandana Gella, Sai Rajeswar, Perouz Taslakian', 'link': 'https://arxiv.org/abs/2503.21889', 'abstract': 'Workflows are a fundamental component of automation in enterprise platforms, enabling the orchestration of tasks, data processing, and system integrations. Despite being widely used, building workflows can be complex, often requiring manual configuration through low-code platforms or visual programming tools. To simplify this process, we explore the use of generative foundation models, particularly vision-language models (VLMs), to automatically generate structured workflows from visual inputs. Translating hand-drawn sketches or computer-generated diagrams into executable workflows is challenging due to the ambiguity of free-form drawings, variations in diagram styles, and the difficulty of inferring execution logic from visual elements. To address this, we introduce StarFlow, a framework for generating structured workflow outputs from sketches using vision-language models. We curate a diverse dataset of workflow diagrams -- including synthetic, manually annotated, and real-world samples -- to enable robust training and evaluation. We finetune and benchmark multiple vision-language models, conducting a series of ablation studies to analyze the strengths and limitations of our approach. Our results show that finetuning significantly enhances structured workflow generation, outperforming large vision-language models on this task.', 'abstract_zh': '基于视觉语言模型的草图到结构化工作流生成：StarFlow框架', 'title_zh': '星流：从素描图像生成结构化工作流输出'}
{'arxiv_id': 'arXiv:2503.21854', 'title': 'Foveated Instance Segmentation', 'authors': 'Hongyi Zeng, Wenxuan Liu, Tianhua Xia, Jinhui Chen, Ziyun Li, Sai Qian Zhang', 'link': 'https://arxiv.org/abs/2503.21854', 'abstract': 'Instance segmentation is essential for augmented reality and virtual reality (AR/VR) as it enables precise object recognition and interaction, enhancing the integration of virtual and real-world elements for an immersive experience. However, the high computational overhead of segmentation limits its application on resource-constrained AR/VR devices, causing large processing latency and degrading user experience. In contrast to conventional scenarios, AR/VR users typically focus on only a few regions within their field of view before shifting perspective, allowing segmentation to be concentrated on gaze-specific areas. This insight drives the need for efficient segmentation methods that prioritize processing instance of interest, reducing computational load and enhancing real-time performance. In this paper, we present a foveated instance segmentation (FovealSeg) framework that leverages real-time user gaze data to perform instance segmentation exclusively on instance of interest, resulting in substantial computational savings. Evaluation results show that FSNet achieves an IoU of 0.56 on ADE20K and 0.54 on LVIS, notably outperforming the baseline. The code is available at this https URL', 'abstract_zh': '注视点导向实例分割（FovealSeg）框架：基于实时用户注视数据的实例分割方法及其性能评估', 'title_zh': '注视点实例分割'}
{'arxiv_id': 'arXiv:2503.21848', 'title': 'Comparative Analysis of Image, Video, and Audio Classifiers for Automated News Video Segmentation', 'authors': 'Jonathan Attard, Dylan Seychell', 'link': 'https://arxiv.org/abs/2503.21848', 'abstract': 'News videos require efficient content organisation and retrieval systems, but their unstructured nature poses significant challenges for automated processing. This paper presents a comprehensive comparative analysis of image, video, and audio classifiers for automated news video segmentation. This work presents the development and evaluation of multiple deep learning approaches, including ResNet, ViViT, AST, and multimodal architectures, to classify five distinct segment types: advertisements, stories, studio scenes, transitions, and visualisations. Using a custom-annotated dataset of 41 news videos comprising 1,832 scene clips, our experiments demonstrate that image-based classifiers achieve superior performance (84.34\\% accuracy) compared to more complex temporal models. Notably, the ResNet architecture outperformed state-of-the-art video classifiers while requiring significantly fewer computational resources. Binary classification models achieved high accuracy for transitions (94.23\\%) and advertisements (92.74\\%). These findings advance the understanding of effective architectures for news video segmentation and provide practical insights for implementing automated content organisation systems in media applications. These include media archiving, personalised content delivery, and intelligent video search.', 'abstract_zh': '新闻视频需要高效的內容组织和检索系统，但由于其非结构化特性，为自动化处理带来了巨大挑战。本文对图像、视频和音频分类器在新闻视频自动分割中的应用进行了全面的比较分析。本文还阐述了多种深度学习方法的发展与评估，包括ResNet、ViViT、AST以及多模态架构，用于分类五大不同段落类型：广告、故事、演播室场景、过渡和可视化。使用包含41条新闻视频和1,832个场景片段的自标注数据集，我们的实验显示，基于图像的分类器在性能上优于更复杂的时间模型（准确率84.34%）。值得注意的是，ResNet架构在计算资源消耗显著减少的情况下，优于最先进的视频分类器。二元分类模型在过渡（94.23%）和广告（92.74%）分类上获得了高准确率。这些发现推进了对新闻视频分割有效架构的理解，并为媒体应用中的自动化内容组织系统实施提供了实用洞察，包括媒体归档、个性化内容交付和智能视频搜索。', 'title_zh': '图像、视频和音频分类器的自动新闻视频分割比较分析'}
{'arxiv_id': 'arXiv:2503.21847', 'title': 'ReCoM: Realistic Co-Speech Motion Generation with Recurrent Embedded Transformer', 'authors': 'Yong Xie, Yunlian Sun, Hongwen Zhang, Yebin Liu, Jinhui Tang', 'link': 'https://arxiv.org/abs/2503.21847', 'abstract': "We present ReCoM, an efficient framework for generating high-fidelity and generalizable human body motions synchronized with speech. The core innovation lies in the Recurrent Embedded Transformer (RET), which integrates Dynamic Embedding Regularization (DER) into a Vision Transformer (ViT) core architecture to explicitly model co-speech motion dynamics. This architecture enables joint spatial-temporal dependency modeling, thereby enhancing gesture naturalness and fidelity through coherent motion synthesis. To enhance model robustness, we incorporate the proposed DER strategy, which equips the model with dual capabilities of noise resistance and cross-domain generalization, thereby improving the naturalness and fluency of zero-shot motion generation for unseen speech inputs. To mitigate inherent limitations of autoregressive inference, including error accumulation and limited self-correction, we propose an iterative reconstruction inference (IRI) strategy. IRI refines motion sequences via cyclic pose reconstruction, driven by two key components: (1) classifier-free guidance improves distribution alignment between generated and real gestures without auxiliary supervision, and (2) a temporal smoothing process eliminates abrupt inter-frame transitions while ensuring kinematic continuity. Extensive experiments on benchmark datasets validate ReCoM's effectiveness, achieving state-of-the-art performance across metrics. Notably, it reduces the Fréchet Gesture Distance (FGD) from 18.70 to 2.48, demonstrating an 86.7% improvement in motion realism. Our project page is this https URL.", 'abstract_zh': '我们提出ReCoM，一种高效框架，用于生成与语音同步的高保真和可泛化的_human_body_动作。核心创新在于循环嵌入变换器（RET），它将动态嵌入正则化（DER）集成到Vision Transformer（ViT）核心架构中，以显式建模共言语动动态。该架构能够实现空间-时间依赖性建模，从而通过一致的动作合成增强手势的自然性和保真度。为了增强模型的鲁棒性，我们引入了提出的DER策略，使模型具备噪声抵抗和跨域泛化的双重能力，从而提高对未见过的语音输入的零样本动作生成的自然流畅度。为缓解自回归推理的固有限制，包括误差累积和自我纠正能力有限，我们提出了一种迭代重建推理（IRI）策略。IRI通过循环姿态重建细化动作序列，由两个关键组件驱动：（1）无分类引导提高生成手势和真实手势之间的分布对齐，无需辅助监督；（2）时间平滑过程消除帧间突变过渡，同时确保运动连贯性。基准数据集上的广泛实验证明了ReCoM的有效性，多项指标上取得了最佳性能。值得注意的是，它将Fréchet 动作距离（FGD）从18.70降低到2.48，显示出86.7%的动作真实感改进。我们的项目页面在此：https://xxxxxx。', 'title_zh': 'ReCoM: 基于递归嵌入变压器的现实主义语音同步运动生成'}
{'arxiv_id': 'arXiv:2503.20258', 'title': 'Mamba-3D as Masked Autoencoders for Accurate and Data-Efficient Analysis of Medical Ultrasound Videos', 'authors': 'Jiaheng Zhou, Yanfeng Zhou, Wei Fang, Yuxing Tang, Le Lu, Ge Yang', 'link': 'https://arxiv.org/abs/2503.20258', 'abstract': 'Ultrasound videos are an important form of clinical imaging data, and deep learning-based automated analysis can improve diagnostic accuracy and clinical efficiency. However, the scarcity of labeled data and the inherent challenges of video analysis have impeded the advancement of related methods. In this work, we introduce E-ViM$^3$, a data-efficient Vision Mamba network that preserves the 3D structure of video data, enhancing long-range dependencies and inductive biases to better model space-time correlations. With our design of Enclosure Global Tokens (EGT), the model captures and aggregates global features more effectively than competing methods. To further improve data efficiency, we employ masked video modeling for self-supervised pre-training, with the proposed Spatial-Temporal Chained (STC) masking strategy designed to adapt to various video scenarios. Experiments demonstrate that E-ViM$^3$ performs as the state-of-the-art in two high-level semantic analysis tasks across four datasets of varying sizes: EchoNet-Dynamic, CAMUS, MICCAI-BUV, and WHBUS. Furthermore, our model achieves competitive performance with limited labels, highlighting its potential impact on real-world clinical applications.', 'abstract_zh': '基于超声视频的E-ViM$^3$数据高效视网膜网络及其在空间-时间相关性建模中的应用', 'title_zh': 'Mamba-3D作为遮蔽自编码器用于医学超声视频的准确和数据高效分析'}
