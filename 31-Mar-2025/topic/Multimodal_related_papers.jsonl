{'arxiv_id': 'arXiv:2503.22610', 'title': 'Evaluating Multimodal Language Models as Visual Assistants for Visually Impaired Users', 'authors': 'Antonia Karamolegkou, Malvina Nikandrou, Georgios Pantazopoulos, Danae Sanchez Villegas, Phillip Rust, Ruchira Dhar, Daniel Hershcovich, Anders Søgaard', 'link': 'https://arxiv.org/abs/2503.22610', 'abstract': 'This paper explores the effectiveness of Multimodal Large Language models (MLLMs) as assistive technologies for visually impaired individuals. We conduct a user survey to identify adoption patterns and key challenges users face with such technologies. Despite a high adoption rate of these models, our findings highlight concerns related to contextual understanding, cultural sensitivity, and complex scene understanding, particularly for individuals who may rely solely on them for visual interpretation. Informed by these results, we collate five user-centred tasks with image and video inputs, including a novel task on Optical Braille Recognition. Our systematic evaluation of twelve MLLMs reveals that further advancements are necessary to overcome limitations related to cultural context, multilingual support, Braille reading comprehension, assistive object recognition, and hallucinations. This work provides critical insights into the future direction of multimodal AI for accessibility, underscoring the need for more inclusive, robust, and trustworthy visual assistance technologies.', 'abstract_zh': '本文探讨了多模态大型语言模型（MLLMs）作为盲人辅助技术的有效性。我们开展了一项用户调查，以识别用户采用模式和他们使用此类技术所面临的key挑战。尽管这些模型的采用率很高，但我们的研究结果突显了与语境理解、文化敏感性和复杂场景理解相关的问题，尤其是对于那些可能完全依赖它们进行视觉解释的个人。根据这些结果，我们汇总了五个以图像和视频为输入的用户中心任务，其中包括一项新颖的光学盲文识别任务。对十二种MLLMs的系统性评估表明，为了克服与文化背景、多语言支持、盲文阅读理解、辅助对象识别和幻觉相关的问题，还需要进一步的发展。本研究为多模态AI在无障碍领域的未来方向提供了关键见解，强调了需要更多包容性、稳健性和可信度的视觉辅助技术。', 'title_zh': '评估多模态语言模型作为视觉辅助工具用于视觉障碍用户'}
{'arxiv_id': 'arXiv:2503.22577', 'title': 'Breaking Language Barriers in Visual Language Models via Multilingual Textual Regularization', 'authors': 'Iñigo Pikabea, Iñaki Lacunza, Oriol Pareras, Carlos Escolano, Aitor Gonzalez-Agirre, Javier Hernando, Marta Villegas', 'link': 'https://arxiv.org/abs/2503.22577', 'abstract': "Rapid advancements in Visual Language Models (VLMs) have transformed multimodal understanding but are often constrained by generating English responses regardless of the input language. This phenomenon has been termed as Image-induced Fidelity Loss (IFL) and stems from limited multimodal multilingual training data. To address this, we propose a continuous multilingual integration strategy that injects text-only multilingual data during visual instruction tuning, preserving the language model's original multilingual capabilities. Extensive evaluations demonstrate that our approach significantly improves linguistic fidelity across languages without degradation in visual performance. We also explore model merging, which improves language fidelity but comes at the cost of visual performance. In contrast, our core method achieves robust multilingual alignment without trade-offs, offering a scalable and effective path to mitigating IFL for global VLM adoption.", 'abstract_zh': '快速发展的视觉语言模型(VLMs)已 transforming 多模态理解，但通常受限于无论输入语言为何种，生成英语响应的现象。这一现象被称为图像引发的忠实度损失(IFL)，源自于多模态多语言训练数据的限制。为解决此问题，我们提出了一种连续的多语言集成策略，在视觉指令调整期间注入仅文本的多语言数据，从而保留语言模型原有的多语言能力。广泛评估表明，我们的方法显著提升了多种语言的语义忠实度，同时不牺牲视觉性能。我们还探讨了模型合并，这可以提升语言忠实度，但会牺牲视觉性能。相比之下，我们的核心方法实现了稳健的多语言对齐，无需权衡，提供了一条可扩展且有效的减轻IFL的道路，促进全球VLM的采用。', 'title_zh': '通过多语言文本正则化打破语言壁垒的视觉语言模型方法'}
{'arxiv_id': 'arXiv:2503.22020', 'title': 'CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models', 'authors': 'Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, Ankur Handa, Ming-Yu Liu, Donglai Xiang, Gordon Wetzstein, Tsung-Yi Lin', 'link': 'https://arxiv.org/abs/2503.22020', 'abstract': 'Vision-language-action models (VLAs) have shown potential in leveraging pretrained vision-language models and diverse robot demonstrations for learning generalizable sensorimotor control. While this paradigm effectively utilizes large-scale data from both robotic and non-robotic sources, current VLAs primarily focus on direct input--output mappings, lacking the intermediate reasoning steps crucial for complex manipulation tasks. As a result, existing VLAs lack temporal planning or reasoning capabilities. In this paper, we introduce a method that incorporates explicit visual chain-of-thought (CoT) reasoning into vision-language-action models (VLAs) by predicting future image frames autoregressively as visual goals before generating a short action sequence to achieve these goals. We introduce CoT-VLA, a state-of-the-art 7B VLA that can understand and generate visual and action tokens. Our experimental results demonstrate that CoT-VLA achieves strong performance, outperforming the state-of-the-art VLA model by 17% in real-world manipulation tasks and 6% in simulation benchmarks. Project website: this https URL', 'abstract_zh': '基于视觉-语言-动作模型（VLAs）的显式视觉链式思考推理方法', 'title_zh': 'CoT-VLA: 视觉链式思考推理方法在视语动模型中的应用'}
{'arxiv_id': 'arXiv:2503.21843', 'title': 'CMD-HAR: Cross-Modal Disentanglement for Wearable Human Activity Recognition', 'authors': 'Hanyu Liu, Siyao Li, Ying Yu, Yixuan Jiang, Hang Xiao, Jingxi Long, Haotian Tang', 'link': 'https://arxiv.org/abs/2503.21843', 'abstract': 'Human Activity Recognition (HAR) is a fundamental technology for numerous human - centered intelligent applications. Although deep learning methods have been utilized to accelerate feature extraction, issues such as multimodal data mixing, activity heterogeneity, and complex model deployment remain largely unresolved. The aim of this paper is to address issues such as multimodal data mixing, activity heterogeneity, and complex model deployment in sensor-based human activity recognition. We propose a spatiotemporal attention modal decomposition alignment fusion strategy to tackle the problem of the mixed distribution of sensor data. Key discriminative features of activities are captured through cross-modal spatio-temporal disentangled representation, and gradient modulation is combined to alleviate data heterogeneity. In addition, a wearable deployment simulation system is constructed. We conducted experiments on a large number of public datasets, demonstrating the effectiveness of the model.', 'abstract_zh': '基于传感器的人体活动识别中的多模态数据混叠、活动异质性和复杂模型部署问题研究', 'title_zh': 'CMD-HAR: 跨模态解耦的人体活动识别'}
