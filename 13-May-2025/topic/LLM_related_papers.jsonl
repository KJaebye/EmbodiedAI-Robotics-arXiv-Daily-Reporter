{'arxiv_id': 'arXiv:2505.07084', 'title': 'DriveSOTIF: Advancing Perception SOTIF Through Multimodal Large Language Models', 'authors': 'Shucheng Huang, Freda Shi, Chen Sun, Jiaming Zhong, Minghao Ning, Yufeng Yang, Yukun Lu, Hong Wang, Amir Khajepour', 'link': 'https://arxiv.org/abs/2505.07084', 'abstract': 'Human drivers naturally possess the ability to perceive driving scenarios, predict potential hazards, and react instinctively due to their spatial and causal intelligence, which allows them to perceive, understand, predict, and interact with the 3D world both spatially and temporally. Autonomous vehicles, however, lack these capabilities, leading to challenges in effectively managing perception-related Safety of the Intended Functionality (SOTIF) risks, particularly in complex and unpredictable driving conditions. To address this gap, we propose an approach that fine-tunes multimodal language models (MLLMs) on a customized dataset specifically designed to capture perception-related SOTIF scenarios. Model benchmarking demonstrates that this tailored dataset enables the models to better understand and respond to these complex driving situations. Additionally, in real-world case studies, the proposed method correctly handles challenging scenarios that even human drivers may find difficult. Real-time performance tests further indicate the potential for the models to operate efficiently in live driving environments. This approach, along with the dataset generation pipeline, shows significant promise for improving the identification, cognition, prediction, and reaction to SOTIF-related risks in autonomous driving systems. The dataset and information are available: this https URL', 'abstract_zh': '人类驾驶员天生具备感知驾驶场景、预测潜在风险并本能反应的能力，这得益于他们的空间和因果智能，使他们能够从空间和时间上感知、理解、预测和交互3D世界。然而，自动驾驶车辆缺乏这些能力，导致在复杂和不可预测的驾驶条件下难以有效管理感知相关的意图功能安全（SOTIF）风险。为解决这一问题，我们提出了一种方法，即在为捕捉感知相关的SOTIF场景而定制的数据集上细调多模态语言模型（MLLMs）。模型基准测试表明，这种定制数据集使模型能够更好地理解和应对这些复杂的驾驶情况。此外，在实际案例研究中，所提议的方法正确处理了即使是人类驾驶员也可能难以应对的挑战性场景。实时性能测试进一步表明，这些模型有可能在实际驾驶环境中高效运行。该方法结合数据集生成管道，在自动驾驶系统中显著提高了对SOTIF相关风险的识别、认知、预测和反应能力。数据集和信息可在以下链接获取：this https URL。', 'title_zh': 'DriveSOTIF：通过多模态大语言模型促进感知SOTIF的研究'}
{'arxiv_id': 'arXiv:2505.06875', 'title': 'Towards Human-Centric Autonomous Driving: A Fast-Slow Architecture Integrating Large Language Model Guidance with Reinforcement Learning', 'authors': 'Chengkai Xu, Jiaqi Liu, Yicheng Guo, Yuhang Zhang, Peng Hang, Jian Sun', 'link': 'https://arxiv.org/abs/2505.06875', 'abstract': 'Autonomous driving has made significant strides through data-driven techniques, achieving robust performance in standardized tasks. However, existing methods frequently overlook user-specific preferences, offering limited scope for interaction and adaptation with users. To address these challenges, we propose a "fast-slow" decision-making framework that integrates a Large Language Model (LLM) for high-level instruction parsing with a Reinforcement Learning (RL) agent for low-level real-time decision. In this dual system, the LLM operates as the "slow" module, translating user directives into structured guidance, while the RL agent functions as the "fast" module, making time-critical maneuvers under stringent latency constraints. By decoupling high-level decision making from rapid control, our framework enables personalized user-centric operation while maintaining robust safety margins. Experimental evaluations across various driving scenarios demonstrate the effectiveness of our method. Compared to baseline algorithms, the proposed architecture not only reduces collision rates but also aligns driving behaviors more closely with user preferences, thereby achieving a human-centric mode. By integrating user guidance at the decision level and refining it with real-time control, our framework bridges the gap between individual passenger needs and the rigor required for safe, reliable driving in complex traffic environments.', 'abstract_zh': '基于大数据驱动技术的自动驾驶已取得显著进展，实现了标准化任务中的稳健性能。然而，现有方法往往忽略了用户的个性化偏好，限制了与用户的互动和适应性。为解决这些问题，我们提出了一种“快慢”决策框架，将大型语言模型（LLM）用于高层次指令解析，将强化学习（RL）代理用于低层次实时决策。在该双系统中，LLM 作为“慢”模块，将用户指令转换为结构化指导，而 RL 代理作为“快”模块，在严格的时间延迟约束条件下进行快速机动。通过将高层次决策与快速控制解耦，我们的框架能够实现个性化用户中心的操作，同时保持 robust 的安全边际。在多种驾驶场景下的实验评估证明了我们方法的有效性。与基准算法相比，所提出架构不仅降低了碰撞率，还使驾驶行为更加符合用户的偏好，从而实现以人为中心的模式。通过在决策级别整合用户指导并在实时控制中进行优化，我们的框架填补了个体乘客需求与复杂交通环境中安全可靠驾驶所需的严格要求之间的差距。', 'title_zh': '面向以人为中心的自主驾驶：一种结合大型语言模型指导与 reinforcement 学习的快慢架构'}
{'arxiv_id': 'arXiv:2505.06513', 'title': 'LLM-Flock: Decentralized Multi-Robot Flocking via Large Language Models and Influence-Based Consensus', 'authors': 'Peihan Li, Lifeng Zhou', 'link': 'https://arxiv.org/abs/2505.06513', 'abstract': 'Large Language Models (LLMs) have advanced rapidly in recent years, demonstrating strong capabilities in problem comprehension and reasoning. Inspired by these developments, researchers have begun exploring the use of LLMs as decentralized decision-makers for multi-robot formation control. However, prior studies reveal that directly applying LLMs to such tasks often leads to unstable and inconsistent behaviors, where robots may collapse to the centroid of their positions or diverge entirely due to hallucinated reasoning, logical inconsistencies, and limited coordination awareness. To overcome these limitations, we propose a novel framework that integrates LLMs with an influence-based plan consensus protocol. In this framework, each robot independently generates a local plan toward the desired formation using its own LLM. The robots then iteratively refine their plans through a decentralized consensus protocol that accounts for their influence on neighboring robots. This process drives the system toward a coherent and stable flocking formation in a fully decentralized manner. We evaluate our approach through comprehensive simulations involving both state-of-the-art closed-source LLMs (e.g., o3-mini, Claude 3.5) and open-source models (e.g., Llama3.1-405b, Qwen-Max, DeepSeek-R1). The results show notable improvements in stability, convergence, and adaptability over previous LLM-based methods. We further validate our framework on a physical team of Crazyflie drones, demonstrating its practical viability and effectiveness in real-world multi-robot systems.', 'abstract_zh': '大型语言模型（LLMs）近年来取得了快速进展，展示了在问题理解和推理方面的强大能力。受这些进展的启发，研究人员开始探索将LLMs用于多机器人编队控制的分散决策制定。然而，前期研究显示，直接将LLMs应用于此类任务往往会引发不稳定的且不一致的行为，机器人可能会坍缩到其位置的质心，或者完全发散，这归因于幻觉推理、逻辑不一致和有限的协调意识。为克服这些限制，我们提出了一种将LLMs与基于影响的计划一致性协议相结合的新型框架。在该框架中，每个机器人使用自己的LLM独立生成通往期望编队的局部计划。然后，通过一个考虑机器人对其邻近机器人影响的分散一致性协议，机器人逐步细化其计划。这一过程在完全分散的方式下将系统导向协同且稳定的 flocking 编队。我们通过综合模拟评估了我们的方法，模拟中包括最先进的闭源LLMs（如o3-mini，Claude 3.5）和开源模型（如Llama3.1-405b，Qwen-Max，DeepSeek-R1）。结果表明，与之前的基于LLM的方法相比，在稳定性、收敛性和适应性方面有所提升。进一步地，我们在实际的疯狂飞机无人机团队上验证了我们的框架，证明了其在实际多机器人系统中的实用性和有效性。', 'title_zh': 'LLM-Flock: 基于大型语言模型和基于影响的共识的去中心化多机器人群集算法'}
{'arxiv_id': 'arXiv:2505.06402', 'title': 'Camera Control at the Edge with Language Models for Scene Understanding', 'authors': 'Alexiy Buynitsky, Sina Ehsani, Bhanu Pallakonda, Pragyana Mishra', 'link': 'https://arxiv.org/abs/2505.06402', 'abstract': "In this paper, we present Optimized Prompt-based Unified System (OPUS), a framework that utilizes a Large Language Model (LLM) to control Pan-Tilt-Zoom (PTZ) cameras, providing contextual understanding of natural environments. To achieve this goal, the OPUS system improves cost-effectiveness by generating keywords from a high-level camera control API and transferring knowledge from larger closed-source language models to smaller ones through Supervised Fine-Tuning (SFT) on synthetic data. This enables efficient edge deployment while maintaining performance comparable to larger models like GPT-4. OPUS enhances environmental awareness by converting data from multiple cameras into textual descriptions for language models, eliminating the need for specialized sensory tokens. In benchmark testing, our approach significantly outperformed both traditional language model techniques and more complex prompting methods, achieving a 35% improvement over advanced techniques and a 20% higher task accuracy compared to closed-source models like Gemini Pro. The system demonstrates OPUS's capability to simplify PTZ camera operations through an intuitive natural language interface. This approach eliminates the need for explicit programming and provides a conversational method for interacting with camera systems, representing a significant advancement in how users can control and utilize PTZ camera technology.", 'abstract_zh': '本文提出了优化提示统一系统（OPUS），这是一种利用大规模语言模型（LLM）控制PTZ摄像头的框架，提供对自然环境的上下文理解。为了实现这一目标，OPUS系统通过从高级摄像头控制API生成关键词，并通过监督微调（SFT）在合成数据上的训练，将大型专有语言模型的知识转移到较小的模型上，从而提高成本效益，实现高效的边缘部署，同时保持与GPT-4等大型模型相当的性能。OPUS通过将多个摄像头的数据转换为文本描述以供语言模型使用，增强了环境意识，消除了对专门感知识别标记的需求。在基准测试中，我们的方法显著优于传统的语言模型技术以及更为复杂的提示方法，相较于高级技术实现了35%的改进，并且与Gemini Pro等专有模型相比，任务准确率提高了20%。该系统展示了OPUS通过直观的自然语言界面简化PTZ摄像头操作的能力，这一方法消除了显式编程的需求，并提供了一种与摄像头系统交互的对话方式，代表了用户控制和利用PTZ摄像头技术的一个重要进步。', 'title_zh': '边缘节点上的相机控制以语言模型实现场景理解'}
{'arxiv_id': 'arXiv:2505.06399', 'title': 'LLM-Land: Large Language Models for Context-Aware Drone Landing', 'authors': 'Siwei Cai, Yuwei Wu, Lifeng Zhou', 'link': 'https://arxiv.org/abs/2505.06399', 'abstract': 'Autonomous landing is essential for drones deployed in emergency deliveries, post-disaster response, and other large-scale missions. By enabling self-docking on charging platforms, it facilitates continuous operation and significantly extends mission endurance. However, traditional approaches often fall short in dynamic, unstructured environments due to limited semantic awareness and reliance on fixed, context-insensitive safety margins. To address these limitations, we propose a hybrid framework that integrates large language model (LLMs) with model predictive control (MPC). Our approach begins with a vision-language encoder (VLE) (e.g., BLIP), which transforms real-time images into concise textual scene descriptions. These descriptions are processed by a lightweight LLM (e.g., Qwen 2.5 1.5B or LLaMA 3.2 1B) equipped with retrieval-augmented generation (RAG) to classify scene elements and infer context-aware safety buffers, such as 3 meters for pedestrians and 5 meters for vehicles. The resulting semantic flags and unsafe regions are then fed into an MPC module, enabling real-time trajectory replanning that avoids collisions while maintaining high landing precision. We validate our framework in the ROS-Gazebo simulator, where it consistently outperforms conventional vision-based MPC baselines. Our results show a significant reduction in near-miss incidents with dynamic obstacles, while preserving accurate landings in cluttered environments.', 'abstract_zh': '自主着陆对于部署在紧急配送、灾后响应和其他大规模任务中的无人机至关重要。通过在充电平台上实现自主对接，它促进了连续操作并显著延长了任务续航能力。然而，传统的Approach often falls short in dynamic, unstructured environments due to limited semantic awareness and reliance on fixed, context-insensitive safety margins. To address these limitations, we propose a hybrid framework that integrates large language model (LLMs) with model predictive control (MPC). Our approach begins with a vision-language encoder (VLE) (e.g., BLIP), which transforms real-time images into concise textual scene descriptions. These descriptions are processed by a lightweight LLM (e.g., Qwen 2.5 1.5B or LLaMA 3.2 1B) equipped with retrieval-augmented generation (RAG) to classify scene elements and infer context-aware safety buffers, such as 3 meters for pedestrians and 5 meters for vehicles. The resulting semantic flags and unsafe regions are then fed into an MPC module, enabling real-time trajectory replanning that avoids collisions while maintaining high landing precision. We validate our framework in the ROS-Gazebo simulator, where it consistently outperforms conventional vision-based MPC baselines. Our results show a significant reduction in near-miss incidents with dynamic obstacles, while preserving accurate landings in cluttered environments.\n\n标题：一种结合大规模语言模型和模型预测控制的自主着陆框架', 'title_zh': 'LLM-陆地：基于上下文感知的大规模语言模型应用于无人机降落'}
{'arxiv_id': 'arXiv:2505.07219', 'title': 'Language-Driven Dual Style Mixing for Single-Domain Generalized Object Detection', 'authors': 'Hongda Qin, Xiao Lu, Zhiyong Wei, Yihong Cao, Kailun Yang, Ningjiang Chen', 'link': 'https://arxiv.org/abs/2505.07219', 'abstract': "Generalizing an object detector trained on a single domain to multiple unseen domains is a challenging task. Existing methods typically introduce image or feature augmentation to diversify the source domain to raise the robustness of the detector. Vision-Language Model (VLM)-based augmentation techniques have been proven to be effective, but they require that the detector's backbone has the same structure as the image encoder of VLM, limiting the detector framework selection. To address this problem, we propose Language-Driven Dual Style Mixing (LDDS) for single-domain generalization, which diversifies the source domain by fully utilizing the semantic information of the VLM. Specifically, we first construct prompts to transfer style semantics embedded in the VLM to an image translation network. This facilitates the generation of style diversified images with explicit semantic information. Then, we propose image-level style mixing between the diversified images and source domain images. This effectively mines the semantic information for image augmentation without relying on specific augmentation selections. Finally, we propose feature-level style mixing in a double-pipeline manner, allowing feature augmentation to be model-agnostic and can work seamlessly with the mainstream detector frameworks, including the one-stage, two-stage, and transformer-based detectors. Extensive experiments demonstrate the effectiveness of our approach across various benchmark datasets, including real to cartoon and normal to adverse weather tasks. The source code and pre-trained models will be publicly available at this https URL.", 'abstract_zh': '单域到多未见域的物体检测通用化是一项具有挑战性的工作。现有方法通常通过图像或特征增强来多样化源域，以提高检测器的鲁棒性。基于视觉-语言模型（VLM）的增强技术已被证明有效，但它们要求检测器的骨干结构与VLM的图像编码器结构相同，限制了检测器框架的选择。为了解决这一问题，我们提出了语言驱动的双风格混合（LDDS）方法，通过充分利用VLM的语义信息来多样化源域。具体而言，我们首先构建提示，将VLM中嵌入的风格语义传递给图像翻译网络，促进了具有明确语义信息的风格多样化图像的生成。然后，我们提出了风格混合方法，在多样化图像与源域图像的图像级别之间进行风格混合，有效地挖掘用于图像增强的语义信息，而不依赖于特定的增强选择。最后，我们以双重管道的方式提出了特征级别风格混合，使得特征增强模型无关，并可以与主流的检测器框架无缝集成，包括单阶段、双阶段和基于变换器的检测器。广泛的经验表明，我们的方法在各种基准数据集中有效，包括从现实到卡通、从正常到不良天气的任务。源代码和预训练模型将在该网址公开。', 'title_zh': '语言驱动的双风格混合单域泛化对象检测'}
{'arxiv_id': 'arXiv:2505.07773', 'title': 'Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving', 'authors': 'Xinji Mai, Haotian Xu, Xing W, Weinong Wang, Yingying Zhang, Wenqiang Zhang', 'link': 'https://arxiv.org/abs/2505.07773', 'abstract': 'Large Language Models (LLMs) often struggle with mathematical reasoning tasks requiring precise, verifiable computation. While Reinforcement Learning (RL) from outcome-based rewards enhances text-based reasoning, understanding how agents autonomously learn to leverage external tools like code execution remains crucial. We investigate RL from outcome-based rewards for Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously generate and execute Python code for mathematical problems without supervised tool-use examples. Our central contribution is we demonstrate that as RL training progresses, key metrics scale predictably. Specifically, we observe strong positive correlations where increased training steps lead to increases in the spontaneous code execution frequency, the average response length, and, critically, the final task accuracy. This suggests a quantifiable relationship between computational effort invested in training and the emergence of effective, tool-augmented reasoning strategies. We implement a robust framework featuring a decoupled code execution environment and validate our findings across standard RL algorithms and frameworks. Experiments show ZeroTIR significantly surpasses non-tool ZeroRL baselines on challenging math benchmarks. Our findings provide a foundational understanding of how autonomous tool use is acquired and scales within Agent RL, offering a reproducible benchmark for future studies. Code is released at \\href{this https URL}{this https URL}.', 'abstract_zh': '大型语言模型（LLMs）在需要精确可验证计算的数学推理任务上常常表现不佳。基于结果奖励的强化学习（RL）虽然提升了基于文本的推理能力，但理解智能体如何自主学习利用外部工具（如代码执行）仍至关重要。我们研究了基于结果奖励的工具集成推理（ZeroTIR），训练基础语言模型自发生成并执行Python代码解决数学问题，而无需监督工具使用示例。我们的主要贡献是证明了随着RL训练的进行，关键指标可预测地增长。具体而言，我们观察到，随着训练步骤的增加，自发代码执行的频率、平均响应长度以及最终任务准确性都呈强烈正相关。这表明了在训练中投入的计算努力与有效工具增强推理策略的涌现之间存在定量关系。我们实现了一个稳健的框架，其中包括解耦的代码执行环境，并在标准RL算法和框架上验证了我们的发现。实验表明ZeroTIR在具有挑战性的数学基准测试中显著超越了非工具ZeroRL基线。我们的研究结果提供了关于自主工具使用如何获得及其扩展的基石性理解，并为未来的研究提供了一个可复现实验基准。代码发布在\\href{this https URL}{this https URL}。', 'title_zh': 'Agent RL扩展律：自发代码执行的代理强化学习及其在数学问题求解中的应用'}
{'arxiv_id': 'arXiv:2505.07686', 'title': 'S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models', 'authors': 'Muzhi Dai, Chenxu Yang, Qingyi Si', 'link': 'https://arxiv.org/abs/2505.07686', 'abstract': "As Test-Time Scaling emerges as an active research focus in the large language model community, advanced post-training methods increasingly emphasize extending chain-of-thought (CoT) generation length, thereby enhancing reasoning capabilities to approach Deepseek R1-like reasoning models. However, recent studies reveal that reasoning models (even Qwen3) consistently exhibit excessive thought redundancy in CoT generation. This overthinking problem stems from conventional outcome-reward reinforcement learning's systematic neglect in regulating intermediate reasoning steps. This paper proposes Serial-Group Decaying-Reward Policy Optimization (namely S-GRPO), a novel reinforcement learning method that empowers models with the capability to determine the sufficiency of reasoning steps, subsequently triggering early exit of CoT generation. Specifically, unlike GRPO, which samples multiple possible completions (parallel group) in parallel, we select multiple temporal positions in the generation of one CoT to allow the model to exit thinking and instead generate answers (serial group), respectively. For the correct answers in a serial group, we assign rewards that decay according to positions, with lower rewards towards the later ones, thereby reinforcing the model's behavior to generate higher-quality answers at earlier phases with earlier exits of thinking. Empirical evaluations demonstrate compatibility with state-of-the-art reasoning models, including Qwen3 and Deepseek-distill models, achieving 35.4% ~ 61.1\\% sequence length reduction with 0.72% ~ 6.08% accuracy improvements across GSM8K, AIME 2024, AMC 2023, MATH-500, and GPQA Diamond benchmarks.", 'abstract_zh': 'As 测试时缩放 研究成为大规模语言模型领域的一个活跃研究焦点，先进的后训练方法 increasingly 强调扩展链式思考（CoT）生成长度，从而提高推理能力以接近 Deepseek R1 类型的推理模型。然而，最近的研究表明，即使对于 Qwen3，推理模型在 CoT 生成中也一致地表现出过度的思考冗余。这一过度思考问题源于传统的结果奖励强化学习对中间推理步骤的系统忽视。本文提出了一种新颖的强化学习方法 S-GRPO（Serial-Group Decaying-Reward Policy Optimization），该方法赋予模型判断推理步骤充分性的能力，进而触发 CoT 生成的早期退出。具体来说，与 GRPO 不同，S-GRPO 在生成一个 CoT 的过程中选择多个时间位置，允许模型退出思考而生成答案（串行组），分别对待。对于串行组中的正确答案，我们按照位置分配递减奖励，后期奖励较低，从而强化模型在早期阶段生成高质量答案并尽早退出思考的行为。实证评估表明，该方法与最先进的推理模型（包括 Qwen3 和 Deepseek-distill 模型）兼容，并在 GSM8K、AIME 2024、AMC 2023、MATH-500 和 GPQA Diamond 标准测试中实现了 35.4%~61.1% 的序列长度减少，同时在准确率上提高了 0.72%~6.08%。', 'title_zh': 'S-GRPO：推理模型中的early exit通过强化学习实现'}
{'arxiv_id': 'arXiv:2505.07581', 'title': 'YuLan-OneSim: Towards the Next Generation of Social Simulator with Large Language Models', 'authors': 'Lei Wang, Heyang Gao, Xiaohe Bo, Xu Chen, Ji-Rong Wen', 'link': 'https://arxiv.org/abs/2505.07581', 'abstract': 'Leveraging large language model (LLM) based agents to simulate human social behaviors has recently gained significant attention. In this paper, we introduce a novel social simulator called YuLan-OneSim. Compared to previous works, YuLan-OneSim distinguishes itself in five key aspects: (1) Code-free scenario construction: Users can simply describe and refine their simulation scenarios through natural language interactions with our simulator. All simulation code is automatically generated, significantly reducing the need for programming expertise. (2) Comprehensive default scenarios: We implement 50 default simulation scenarios spanning 8 domains, including economics, sociology, politics, psychology, organization, demographics, law, and communication, broadening access for a diverse range of social researchers. (3) Evolvable simulation: Our simulator is capable of receiving external feedback and automatically fine-tuning the backbone LLMs, significantly enhancing the simulation quality. (4) Large-scale simulation: By developing a fully responsive agent framework and a distributed simulation architecture, our simulator can handle up to 100,000 agents, ensuring more stable and reliable simulation results. (5) AI social researcher: Leveraging the above features, we develop an AI social researcher. Users only need to propose a research topic, and the AI researcher will automatically analyze the input, construct simulation environments, summarize results, generate technical reports, review and refine the reports--completing the social science research loop. To demonstrate the advantages of YuLan-OneSim, we conduct experiments to evaluate the quality of the automatically generated scenarios, the reliability, efficiency, and scalability of the simulation process, as well as the performance of the AI social researcher.', 'abstract_zh': '利用基于大规模语言模型的代理模拟人类社会行为 recently gained significant attention. 本文介绍了一种新颖的社会仿真器 YuLan-OneSim。与以往工作相比，YuLan-OneSim 在五个关键方面具有优势：（1）无需代码的场景构建：用户可以通过自然语言与仿真器交互来简单描述和细化仿真场景，所有仿真代码均自动生成，显著降低了编程技能的需求。（2）全面的默认场景：我们实现了涵盖经济学、社会学、政治学、心理学、组织学、人口学、法律和通讯等 8 个领域的 50 个默认仿真场景，为多样化的社会研究者提供了更广泛的访问渠道。（3）可进化的仿真：该仿真器能够接收外部反馈并自动精细调整骨干语言模型，显著提高了仿真质量。（4）大规模仿真：通过开发全响应式代理框架和分布式仿真架构，该仿真器能够处理多达 100,000 个代理，确保仿真结果更加稳定可靠。（5）AI 社会研究者：利用上述特性，我们开发了 AI 社会研究者。用户只需提出研究主题，AI 研究者将自动分析输入、构建仿真环境、总结结果、生成技术报告、审阅和优化报告——完成社会科学研究的全过程。为了展示 YuLan-OneSim 的优势，我们进行了实验，评估了自动生成场景的质量、仿真的可靠性和效率以及 AI 社会研究者的表现。', 'title_zh': 'YuLan-OneSim：迈向新一代社会模拟器的大语言模型技术'}
{'arxiv_id': 'arXiv:2505.07531', 'title': 'QuantX: A Framework for Hardware-Aware Quantization of Generative AI Workloads', 'authors': 'Khurram Mazher, Saad Bin Nasir', 'link': 'https://arxiv.org/abs/2505.07531', 'abstract': 'We present QuantX: a tailored suite of recipes for LLM and VLM quantization. It is capable of quantizing down to 3-bit resolutions with minimal loss in performance. The quantization strategies in QuantX take into account hardware-specific constraints to achieve efficient dequantization during inference ensuring flexible trade-off between runtime speed, memory requirement and model accuracy. Our results demonstrate that QuantX achieves performance within 6% of the unquantized model for LlaVa-v1.6 quantized down to 3-bits for multiple end user tasks and outperforms recently published state-of-the-art quantization techniques. This manuscript provides insights into the LLM quantization process that motivated the range of recipes and options that are incorporated in QuantX.', 'abstract_zh': '我们呈现QuantX：针对LLM和VLM量化的一套定制化方案。它能够在保持最低性能损失的情况下将量化精度降至3位。QuantX中的量化策略考虑了硬件特异性限制，以实现高效推理时的去量化，确保在运行时速度、内存需求和模型准确性之间灵活权衡。我们的结果表明，QuantX在将LlaVa-v1.6量化至3位后，跨多个终端用户任务的性能与未量化模型的差距不超过6%，并优于最近公布的最先进的量化技术。本文提供了关于LLM量化过程的见解，这些见解激发了QuantX中所集成的各种方案和选项。', 'title_zh': 'QuantX: 一种面向硬件的生成型AI工作负载量化框架'}
{'arxiv_id': 'arXiv:2505.07473', 'title': 'Web-Bench: A LLM Code Benchmark Based on Web Standards and Frameworks', 'authors': 'Kai Xu, YiWei Mao, XinYi Guan, ZiLong Feng', 'link': 'https://arxiv.org/abs/2505.07473', 'abstract': "The application of large language models (LLMs) in the field of coding is evolving rapidly: from code assistants, to autonomous coding agents, and then to generating complete projects through natural language. Early LLM code benchmarks primarily focused on code generation accuracy, but these benchmarks have gradually become saturated. Benchmark saturation weakens their guiding role for LLMs. For example, HumanEval Pass@1 has reached 99.4% and MBPP 94.2%. Among various attempts to address benchmark saturation, approaches based on software engineering have stood out, but the saturation of existing software engineering benchmarks is rapidly increasing. To address this, we propose a new benchmark, Web-Bench, which contains 50 projects, each consisting of 20 tasks with sequential dependencies. The tasks implement project features in sequence, simulating real-world human development workflows. When designing Web-Bench, we aim to cover the foundational elements of Web development: Web Standards and Web Frameworks. Given the scale and complexity of these projects, which were designed by engineers with 5 to 10 years of experience, each presents a significant challenge. On average, a single project takes 4 to 8 hours for a senior engineer to complete. On our given benchmark agent (Web-Agent), SOTA (Claude 3.7 Sonnet) achieves only 25.1% Pass@1, significantly lower (better) than SWE-Bench's Verified (65.4%) and Full (33.8%) scores. Finally, we discuss that in any development field, Standards and Frameworks represent foundational knowledge and efficiency tools, respectively, and LLMs require optimization tailored to them.", 'abstract_zh': '大型语言模型在编码领域的应用不断进化：从代码辅助到自主编码代理，再到通过自然语言生成完整的项目。早期的大型语言模型代码基准主要关注代码生成准确性，但这些基准逐渐饱和。基准饱和削弱了它们对大型语言模型的指导作用。为此，我们提出一个新的基准——Web-Bench，包含50个项目，每个项目由20个具有序列依赖性的任务组成，模拟现实世界的人类开发工作流。我们设计Web-Bench的目标是覆盖Web开发的基础要素：Web标准和Web框架。由于这些项目由具有5到10年经验的工程师设计，每个项目都构成了重大挑战。平均而言，一个项目需要资深工程师4到8小时才能完成。在我们提供的基准代理（Web-Agent）上，当前最先进的模型Claude 3.7 Sonnet的Pass@1得分为25.1%，显著低于SWE-Bench的Verified（65.4%）和Full（33.8%）得分。最后，我们讨论在任何开发领域，标准和框架分别代表基础知识和效率工具，大型语言模型需要针对它们进行优化。', 'title_zh': 'Web-Bench：基于Web标准和框架的LLM代码基准'}
{'arxiv_id': 'arXiv:2505.07460', 'title': 'A Survey on Collaborative Mechanisms Between Large and Small Language Models', 'authors': 'Yi Chen, JiaHao Zhao, HaoHao Han', 'link': 'https://arxiv.org/abs/2505.07460', 'abstract': 'Large Language Models (LLMs) deliver powerful AI capabilities but face deployment challenges due to high resource costs and latency, whereas Small Language Models (SLMs) offer efficiency and deployability at the cost of reduced performance. Collaboration between LLMs and SLMs emerges as a crucial paradigm to synergistically balance these trade-offs, enabling advanced AI applications, especially on resource-constrained edge devices. This survey provides a comprehensive overview of LLM-SLM collaboration, detailing various interaction mechanisms (pipeline, routing, auxiliary, distillation, fusion), key enabling technologies, and diverse application scenarios driven by on-device needs like low latency, privacy, personalization, and offline operation. While highlighting the significant potential for creating more efficient, adaptable, and accessible AI, we also discuss persistent challenges including system overhead, inter-model consistency, robust task allocation, evaluation complexity, and security/privacy concerns. Future directions point towards more intelligent adaptive frameworks, deeper model fusion, and expansion into multimodal and embodied AI, positioning LLM-SLM collaboration as a key driver for the next generation of practical and ubiquitous artificial intelligence.', 'abstract_zh': '大型语言模型（LLMs）提供了强大的AI能力，但由于高昂的资源成本和延迟问题面临部署挑战，而小型语言模型（SLMs）虽然在性能上有所降低，但提供了高效性和可部署性。LLMs与SLMs的合作成为一种关键范式，旨在协同平衡这些权衡，从而实现先进的AI应用，特别是在资源受限的边缘设备上。这篇综述提供了关于LLM-SLM合作的全面概述，详细介绍了各种交互机制（管道、路由、辅助、蒸馏、融合）、关键技术以及各种由设备端需求（如低延迟、隐私、个性化和离线操作）驱动的应用场景。尽管突显了创建更高效、灵活和可访问的AI的巨大潜力，我们还讨论了系统开销、模型间一致性、鲁棒任务分配、评估复杂性和安全/隐私问题等持续挑战。未来方向包括更加智能的自适应框架、更深层次的模型融合，并扩展到多模态和实体AI，将LLM-SLM合作定位为推动下一代实用且普及的AI的关键驱动力。', 'title_zh': '大型和小型语言模型之间的协作机制综述'}
{'arxiv_id': 'arXiv:2505.07453', 'title': 'How well do LLMs reason over tabular data, really?', 'authors': 'Cornelius Wolff, Madelon Hulsebos', 'link': 'https://arxiv.org/abs/2505.07453', 'abstract': "Large Language Models (LLMs) excel in natural language tasks, but less is known about their reasoning capabilities over tabular data. Prior analyses devise evaluation strategies that poorly reflect an LLM's realistic performance on tabular queries. Moreover, we have a limited understanding of the robustness of LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can general-purpose LLMs reason over tabular data, really?, and focus on two questions 1) are tabular reasoning capabilities of general-purpose LLMs robust to real-world characteristics of tabular inputs, and 2) how can we realistically evaluate an LLM's performance on analytical tabular queries? Building on a recent tabular reasoning benchmark, we first surface shortcomings of its multiple-choice prompt evaluation strategy, as well as commonly used free-form text metrics such as SacreBleu and BERT-score. We show that an LLM-as-a-judge procedure yields more reliable performance insights and unveil a significant deficit in tabular reasoning performance of LLMs. We then extend the tabular inputs reflecting three common characteristics in practice: 1) missing values, 2) duplicate entities, and 3) structural variations. Experiments show that the tabular reasoning capabilities of general-purpose LLMs suffer from these variations, stressing the importance of improving their robustness for realistic tabular inputs.", 'abstract_zh': '大型语言模型在表格数据上的推理能力：现状与挑战', 'title_zh': 'LLM在处理表格数据时真的能进行有效的推理吗？'}
{'arxiv_id': 'arXiv:2505.07087', 'title': 'Architectural Precedents for General Agents using Large Language Models', 'authors': 'Robert E. Wray, James R. Kirk, John E. Laird', 'link': 'https://arxiv.org/abs/2505.07087', 'abstract': 'One goal of AI (and AGI) is to identify and understand specific mechanisms and representations sufficient for general intelligence. Often, this work manifests in research focused on architectures and many cognitive architectures have been explored in AI/AGI. However, different research groups and even different research traditions have somewhat independently identified similar/common patterns of processes and representations or cognitive design patterns that are manifest in existing architectures. Today, AI systems exploiting large language models (LLMs) offer a relatively new combination of mechanism and representation available for exploring the possibilities of general intelligence. In this paper, we summarize a few recurring cognitive design patterns that have appeared in various pre-transformer AI architectures. We then explore how these patterns are evident in systems using LLMs, especially for reasoning and interactive ("agentic") use cases. By examining and applying these recurring patterns, we can also predict gaps or deficiencies in today\'s Agentic LLM Systems and identify likely subjects of future research towards general intelligence using LLMs and other generative foundation models.', 'abstract_zh': '人工智能（及超人工智能）的一个目标是识别和理解足够支持一般智能的具体机制和表示。目前，利用大规模语言模型（LLMs）的AI系统为探索一般智能的可能性提供了一种相对较新的机制和表示的组合。本文总结了几种在各种预转子AI架构中反复出现的认知设计模式，并探讨了这些模式在使用LLMs的系统中，特别是在推理和互动（“代理”）应用场景中的表现。通过研究和应用这些反复出现的模式，我们还可以预测当前代理LLM系统的空白或不足，并识别未来研究方向，以利用LLMs及其他生成型基础模型向一般智能前进的研究主题。', 'title_zh': '大型语言模型驱动的通用代理建筑范式'}
{'arxiv_id': 'arXiv:2505.07049', 'title': 'DialogueReason: Rule-Based RL Sparks Dialogue Reasoning in LLMs', 'authors': 'Yubo Shu, Zhewei Huang, Xin Wu, Chen Hu, Shuchang Zhou, Daxin Jiang', 'link': 'https://arxiv.org/abs/2505.07049', 'abstract': 'We propose DialogueReason, a reasoning paradigm that uncovers the lost roles in monologue-style reasoning models, aiming to boost diversity and coherency of the reasoning process. Recent advances in RL-based large reasoning models have led to impressive long CoT capabilities and high performance on math and science benchmarks. However, these reasoning models rely mainly on monologue-style reasoning, which often limits reasoning diversity and coherency, frequently recycling fixed strategies or exhibiting unnecessary shifts in attention. Our work consists of an analysis of monologue reasoning patterns and the development of a dialogue-based reasoning approach. We first introduce the Compound-QA task, which concatenates multiple problems into a single prompt to assess both diversity and coherency of reasoning. Our analysis shows that Compound-QA exposes weaknesses in monologue reasoning, evidenced by both quantitative metrics and qualitative reasoning traces. Building on the analysis, we propose a dialogue-based reasoning, named DialogueReason, structured around agents, environment, and interactions. Using PPO with rule-based rewards, we train open-source LLMs (Qwen-QWQ and Qwen-Base) to adopt dialogue reasoning. We evaluate trained models on MATH, AIME, and GPQA datasets, showing that the dialogue reasoning model outperforms monologue models under more complex compound questions. Additionally, we discuss how dialogue-based reasoning helps enhance interpretability, facilitate more intuitive human interaction, and inspire advances in multi-agent system design.', 'abstract_zh': 'DialogueReason：揭示独白式推理模型中丢失的角色，促进推理过程的多样性和连贯性', 'title_zh': 'DialogueReason: 规则基于的RL激发大语言模型的对话推理能力'}
{'arxiv_id': 'arXiv:2505.07027', 'title': 'LLM-Augmented Chemical Synthesis and Design Decision Programs', 'authors': 'Haorui Wang, Jeff Guo, Lingkai Kong, Rampi Ramprasad, Philippe Schwaller, Yuanqi Du, Chao Zhang', 'link': 'https://arxiv.org/abs/2505.07027', 'abstract': 'Retrosynthesis, the process of breaking down a target molecule into simpler precursors through a series of valid reactions, stands at the core of organic chemistry and drug development. Although recent machine learning (ML) research has advanced single-step retrosynthetic modeling and subsequent route searches, these solutions remain restricted by the extensive combinatorial space of possible pathways. Concurrently, large language models (LLMs) have exhibited remarkable chemical knowledge, hinting at their potential to tackle complex decision-making tasks in chemistry. In this work, we explore whether LLMs can successfully navigate the highly constrained, multi-step retrosynthesis planning problem. We introduce an efficient scheme for encoding reaction pathways and present a new route-level search strategy, moving beyond the conventional step-by-step reactant prediction. Through comprehensive evaluations, we show that our LLM-augmented approach excels at retrosynthesis planning and extends naturally to the broader challenge of synthesizable molecular design.', 'abstract_zh': '逆合成分析，通过一系列有效的反应将目标分子分解为更简单的前体，是有机化学和药物开发的核心。尽管最近的机器学习研究已经在单步逆合成模型和后续路径搜索方面取得了进展，但这些解决方案仍然受到潜在路径组合空间的限制。同时，大型语言模型（LLMs）展现了显著的化学知识，暗示它们在化学中的复杂决策任务中具有潜在的应用价值。在这项工作中，我们探索LLMs是否能够成功解决高度受限的多步逆合成规划问题。我们提出了一种高效的方法来编码反应路径，并提出了一种新的路线级搜索策略，超越了传统的逐步反应物预测。通过全面的评估，我们展示了我们的LLM增强方法在逆合成分析规划方面的优越性能，并自然地扩展到合成可及分子设计的更广泛挑战。', 'title_zh': 'LLM增强的化学合成与设计决策程序'}
{'arxiv_id': 'arXiv:2505.06964', 'title': 'From Knowledge to Reasoning: Evaluating LLMs for Ionic Liquids Research in Chemical and Biological Engineering', 'authors': 'Gaurab Sarkar, Sougata Saha', 'link': 'https://arxiv.org/abs/2505.06964', 'abstract': "Although Large Language Models (LLMs) have achieved remarkable performance in diverse general knowledge and reasoning tasks, their utility in the scientific domain of Chemical and Biological Engineering (CBE) is unclear. Hence, it necessitates challenging evaluation benchmarks that can measure LLM performance in knowledge- and reasoning-based tasks, which is lacking. As a foundational step, we empirically measure the reasoning capabilities of LLMs in CBE. We construct and share an expert-curated dataset of 5,920 examples for benchmarking LLMs' reasoning capabilities in the niche domain of Ionic Liquids (ILs) for carbon sequestration, an emergent solution to reducing global warming. The dataset presents different difficulty levels by varying along the dimensions of linguistic and domain-specific knowledge. Benchmarking three less than 10B parameter open-source LLMs on the dataset suggests that while smaller general-purpose LLMs are knowledgeable about ILs, they lack domain-specific reasoning capabilities. Based on our results, we further discuss considerations for leveraging LLMs for carbon capture research using ILs. Since LLMs have a high carbon footprint, gearing them for IL research can symbiotically benefit both fields and help reach the ambitious carbon neutrality target by 2050. Dataset link: this https URL", 'abstract_zh': '尽管大型语言模型（LLMs）在多种一般知识和推理任务中取得了显著性能，它们在化学和生物工程（CBE）科学领域的实用性尚不清楚。因此，迫切需要具有挑战性的评估基准来衡量LLMs在基于知识和推理的任务中的性能，而这方面尚缺乏。作为基础步骤，我们实证测量了LLMs在CBE领域的推理能力。我们构建并共享了一个由专家精心挑选的5,920个案例组成的语料库，以评估LLMs在离子液体（ILs）用于碳捕获这一新兴减缓全球变暖解决方案的专业领域的推理能力。此语料库通过在语言和领域特定知识维度上的变化呈现了不同难度级别。在该语料库上对三个少于10B参数的开源LLMs进行基准测试表明，虽然更小的通用LLMs对ILs有所了解，但在领域特定的推理能力上却存在不足。基于我们的研究结果，我们进一步讨论了利用ILs进行碳捕获研究的LLMs考虑因素。由于LLMs具有高碳足迹，为IL研究调整它们可以互利地造福两个领域，并有助于在2050年达成雄心勃勃的碳中和目标。数据集链接：this https URL', 'title_zh': '从知识到推理：评估在化学与生物工程领域离子液体研究中的LLM能力'}
{'arxiv_id': 'arXiv:2505.06907', 'title': 'Towards Artificial General or Personalized Intelligence? A Survey on Foundation Models for Personalized Federated Intelligence', 'authors': 'Yu Qiao, Huy Q. Le, Avi Deb Raha, Phuong-Nam Tran, Apurba Adhikary, Mengchun Zhang, Loc X. Nguyen, Eui-Nam Huh, Dusit Niyato, Choong Seon Hong', 'link': 'https://arxiv.org/abs/2505.06907', 'abstract': 'The rise of large language models (LLMs), such as ChatGPT, DeepSeek, and Grok-3, has reshaped the artificial intelligence landscape. As prominent examples of foundational models (FMs) built on LLMs, these models exhibit remarkable capabilities in generating human-like content, bringing us closer to achieving artificial general intelligence (AGI). However, their large-scale nature, sensitivity to privacy concerns, and substantial computational demands present significant challenges to personalized customization for end users. To bridge this gap, this paper presents the vision of artificial personalized intelligence (API), focusing on adapting these powerful models to meet the specific needs and preferences of users while maintaining privacy and efficiency. Specifically, this paper proposes personalized federated intelligence (PFI), which integrates the privacy-preserving advantages of federated learning (FL) with the zero-shot generalization capabilities of FMs, enabling personalized, efficient, and privacy-protective deployment at the edge. We first review recent advances in both FL and FMs, and discuss the potential of leveraging FMs to enhance federated systems. We then present the key motivations behind realizing PFI and explore promising opportunities in this space, including efficient PFI, trustworthy PFI, and PFI empowered by retrieval-augmented generation (RAG). Finally, we outline key challenges and future research directions for deploying FM-powered FL systems at the edge with improved personalization, computational efficiency, and privacy guarantees. Overall, this survey aims to lay the groundwork for the development of API as a complement to AGI, with a particular focus on PFI as a key enabling technique.', 'abstract_zh': '大型语言模型的兴起：从ChatGPT到Grok-3，这些模型重塑了人工智能格局。作为基于大型语言模型的基础模型（FMs）的杰出示例，这些模型展示了生成人类like内容的显著能力，使我们更接近实现人工通用智能（AGI）。然而，它们的规模化性质、对隐私关注的敏感性和巨大的计算需求为个性化定制带来重大挑战。为弥补这一差距，本文提出了人工个性化智能（API）的愿景，旨在将这些强大模型适应以满足用户的具体需求和偏好，同时保持隐私和效率。具体而言，本文提出个性化联邦智能（PFI），将联邦学习（FL）的隐私保护优势与基础模型（FM）的零样本泛化能力结合起来，以实现边缘上的个性化、高效且隐私保护的部署。我们首先回顾了联邦学习和基础模型的最新进展，并讨论了利用基础模型增强联邦系统的机会。然后，我们探讨了实现PFI的关键动力和这一领域中前景广阔的机会，包括高效的PFI、可信赖的PFI以及由检索增强生成（RAG）赋能的PFI。最后，我们概述了部署支持增强个性化、计算效率和隐私保障的基础模型驱动的联邦学习系统的关键挑战和未来研究方向。总之，本文旨在为API的发展奠定基础，特别是在PFI作为一种关键使能技术方面。', 'title_zh': '通向通用或个性化人工智能？一种关于个性化联邦智能基础模型的综述'}
{'arxiv_id': 'arXiv:2505.06507', 'title': 'Text-to-CadQuery: A New Paradigm for CAD Generation with Scalable Large Model Capabilities', 'authors': 'Haoyang Xie, Feng Ju', 'link': 'https://arxiv.org/abs/2505.06507', 'abstract': 'Computer-aided design (CAD) is fundamental to modern engineering and manufacturing, but creating CAD models still requires expert knowledge and specialized software. Recent advances in large language models (LLMs) open up the possibility of generative CAD, where natural language is directly translated into parametric 3D models. However, most existing methods generate task-specific command sequences that pretrained models cannot directly handle. These sequences must be converted into CAD representations such as CAD vectors before a 3D model can be produced, which requires training models from scratch and adds unnecessary complexity. To tackle this issue, we propose generating CadQuery code directly from text, leveraging the strengths of pretrained LLMs to produce 3D models without intermediate representations, using this Python-based scripting language. Since LLMs already excel at Python generation and spatial reasoning, fine-tuning them on Text-to-CadQuery data proves highly effective. Given that these capabilities typically improve with scale, we hypothesize that larger models will perform better after fine-tuning. To enable this, we augment the Text2CAD dataset with 170,000 CadQuery annotations. We fine-tune six open-source LLMs of varying sizes and observe consistent improvements. Our best model achieves a top-1 exact match of 69.3%, up from 58.8%, and reduces Chamfer Distance by 48.6%. Project page: this https URL.', 'abstract_zh': '基于文本直接生成CadQuery代码的预训练大型语言模型驱动的生成式CAD', 'title_zh': '文本到CAD查询：具有可扩展大型模型能力的CAD生成新范式'}
{'arxiv_id': 'arXiv:2505.06469', 'title': 'KCluster: An LLM-based Clustering Approach to Knowledge Component Discovery', 'authors': 'Yumou Wei, Paulo Carvalho, John Stamper', 'link': 'https://arxiv.org/abs/2505.06469', 'abstract': 'Educators evaluate student knowledge using knowledge component (KC) models that map assessment questions to KCs. Still, designing KC models for large question banks remains an insurmountable challenge for instructors who need to analyze each question by hand. The growing use of Generative AI in education is expected only to aggravate this chronic deficiency of expert-designed KC models, as course engineers designing KCs struggle to keep up with the pace at which questions are generated. In this work, we propose KCluster, a novel KC discovery algorithm based on identifying clusters of congruent questions according to a new similarity metric induced by a large language model (LLM). We demonstrate in three datasets that an LLM can create an effective metric of question similarity, which a clustering algorithm can use to create KC models from questions with minimal human effort. Combining the strengths of LLM and clustering, KCluster generates descriptive KC labels and discovers KC models that predict student performance better than the best expert-designed models available. In anticipation of future work, we illustrate how KCluster can reveal insights into difficult KCs and suggest improvements to instruction.', 'abstract_zh': '基于大型语言模型诱导相似度度量的K集群知识组件发现算法', 'title_zh': 'KCluster：一种基于大语言模型的知识组件发现聚类方法'}
{'arxiv_id': 'arXiv:2505.06438', 'title': 'Reliable Collaborative Conversational Agent System Based on LLMs and Answer Set Programming', 'authors': 'Yankai Zeng, Gopal Gupta', 'link': 'https://arxiv.org/abs/2505.06438', 'abstract': "As the Large-Language-Model-driven (LLM-driven) Artificial Intelligence (AI) bots became popular, people realized their strong potential in Task-Oriented Dialogue (TOD). However, bots relying wholly on LLMs are unreliable in their knowledge, and whether they can finally produce a correct result for the task is not guaranteed. The collaboration among these agents also remains a challenge, since the necessary information to convey is unclear, and the information transfer is by prompts -- unreliable, and malicious knowledge is easy to inject. With the help of logic programming tools such as Answer Set Programming (ASP), conversational agents can be built safely and reliably, and communication among the agents made more efficient and secure. We proposed an Administrator-Assistant Dual-Agent paradigm, where the two ASP-driven bots share the same knowledge base and complete their tasks independently, while the information can be passed by a Collaborative Rule Set (CRS). The knowledge and information conveyed are encapsulated and invisible to the users, ensuring the security of information transmission. We have constructed AutoManager, a dual-agent system for managing the drive-through window of a fast-food restaurant such as Taco Bell in the US. In AutoManager, the assistant bot takes the customer's order while the administrator bot manages the menu and food supply. We evaluated our AutoManager and compared it with the real-world Taco Bell Drive-Thru AI Order Taker, and the results show that our method is more reliable.", 'abstract_zh': '基于大型语言模型的基于逻辑编程的对话代理双Agent框架：以快餐餐厅为例的安全可靠对话代理设计与应用', 'title_zh': '基于LLMs和回答集编程的可靠协作会话代理系统'}
{'arxiv_id': 'arXiv:2505.07796', 'title': 'Learning Dynamics in Continual Pre-Training for Large Language Models', 'authors': 'Xingjin Wang, Howe Tissue, Lu Wang, Linjing Li, Daniel Dajun Zeng', 'link': 'https://arxiv.org/abs/2505.07796', 'abstract': 'Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.', 'abstract_zh': '持续预训练（CPT）已成为将强基础模型应用于特定下游任务的有效方法。在本文中，我们探索大规模语言模型在整个CPT过程中的学习动态。我们具体关注每一步训练中一般性能和下游领域性能的变化，通过验证损失衡量领域性能。我们观察到，CPT损失曲线从根本上描述了一条曲线到另一条隐含曲线的转变过程，并可通过解除分布偏移效应和学习率退火效应的影响来描述。我们推导出一个结合这两种因素的CPT缩放定律，使我们能够预测任何（持续）训练步骤和CPT中不同学习率调度下的损失。我们的公式对CPT中几个关键因素提供了全面的理解，包括损失潜力、峰值学习率、训练步骤、回放比例等。此外，我们的方法可以适应不同的CPT目标定制训练超参数，如平衡一般性能和领域特定性能。大量实验表明，我们的缩放定律在各种CPT数据集和训练超参数下均适用。', 'title_zh': '连续预训练中大型语言模型的学习动力学'}
{'arxiv_id': 'arXiv:2505.07793', 'title': 'Overflow Prevention Enhances Long-Context Recurrent LLMs', 'authors': 'Assaf Ben-Kish, Itamar Zimerman, M. Jehanzeb Mirza, James Glass, Leonid Karlinsky, Raja Giryes', 'link': 'https://arxiv.org/abs/2505.07793', 'abstract': 'A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are trained for extended contexts, their use of long contexts remains underutilized. Specifically, we demonstrate that a chunk-based inference procedure, which identifies and processes only the most relevant portion of the input can mitigate recurrent memory failures and be effective for many long-context tasks: On LongBench, our method improves the overall performance of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%, RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this simple approach also leads to state-of-the-art results in the challenging LongBench v2 benchmark, showing competitive performance with equivalent size Transformers. Furthermore, our findings raise questions about whether recurrent models genuinely exploit long-range dependencies, as our single-chunk strategy delivers stronger performance - even in tasks that presumably require cross-context relations.', 'abstract_zh': '近年来，大规模语言模型的趋势是开发亚二次递归模型以提高长上下文处理效率。我们研究了主要的大型长上下文模型，重点关注它们固定大小的递归记忆对其性能的影响。我们的实验表明，即使这些模型在长上下文中进行了长时间的训练，它们对长上下文的利用仍然不足。具体来说，我们展示了一种基于片段的推理过程，该过程仅识别并处理输入中最相关部分，可以缓解递归记忆故障并有效应用于许多长上下文任务：在LongBench上，我们的方法分别将Falcon3-Mamba-Inst-7B的整体性能提高了14%、Falcon-Mamba-Inst-7B提高了28%、RecurrentGemma-IT-9B提高了50%、RWKV6-Finch-7B提高了51%。令人惊讶的是，这种简单的approach在具有挑战性的LongBench v2基准测试中也达到了最先进的结果，显示出与同等规模的Transformers相当的性能。此外，我们的研究结果引发了一个问题，即递归模型是否真正利用了长范围依赖性，因为我们的单片段策略即使在显然需要跨上下文关系的任务中也表现出了更强的性能。', 'title_zh': '溢出预防增强长时间上下文循环生成模型'}
{'arxiv_id': 'arXiv:2505.07768', 'title': 'Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding', 'authors': 'Yifeng Di, Tianyi Zhang', 'link': 'https://arxiv.org/abs/2505.07768', 'abstract': 'Large Language Models (LLMs) have demonstrated unprecedented capability in code generation. However, LLM-generated code is still plagued with a wide range of functional errors, especially for complex programming tasks that LLMs have not seen before. Recent studies have shown that developers often struggle with inspecting and fixing incorrect code generated by LLMs, diminishing their productivity and trust in LLM-based code generation. Inspired by the mutual grounding theory in communication, we propose an interactive approach that leverages code comments as a medium for developers and LLMs to establish a shared understanding. Our approach facilitates iterative grounding by interleaving code generation, inline comment generation, and contextualized user feedback through editable comments to align generated code with developer intent. We evaluated our approach on two popular benchmarks and demonstrated that our approach significantly improved multiple state-of-the-art LLMs, e.g., 17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we conducted a user study with 12 participants in comparison to two baselines: (1) interacting with GitHub Copilot, and (2) interacting with a multi-step code generation paradigm called Multi-Turn Program Synthesis. Participants completed the given programming tasks 16.7% faster and with 10.5% improvement in task success rate when using our approach. Both results show that interactively refining code comments enables the collaborative establishment of mutual grounding, leading to more accurate code generation and higher developer confidence.', 'abstract_zh': '大型语言模型在代码生成方面展现了前所未有的能力，但由于生成的代码仍然存在各种功能错误，特别是在LLMs未见过的复杂编程任务中，这一问题尤为突出。最近的研究表明，开发者往往难以检查和修复LLMs生成的错误代码，这降低了他们的生产力和对LLM驱动的代码生成的信任。受交流中的相互接地理论启发，我们提出了一种交互式方法，利用代码注释作为开发者和LLMs建立共同理解的媒介。该方法通过嵌入代码生成、行内注释生成及上下文化用户反馈，促进迭代接地，使生成的代码与开发者的意图保持一致。我们在两个流行的基准测试上评估了该方法，并证明了该方法显著提高了多种最先进的LLMs的表现，例如，在HumanEval上的code-davinci-002上取得了17.1%的pass@1改进。此外，我们还进行了一项包含12名参与者的研究，与两种基线方法进行了比较：（1）与GitHub Copilot交互，（2）与一种称为多轮程序合成的多步代码生成范式交互。结果显示，使用该方法，参与者完成给定编程任务的速度提高了16.7%，任务成功率提高了10.5%。这两项结果表明，交互式精炼代码注释能够促进协作相互接地，从而实现更准确的代码生成并提高开发者的信心。', 'title_zh': '通过双向代码注释级互信息接地增强代码生成'}
{'arxiv_id': 'arXiv:2505.07711', 'title': 'Circuit Partitioning Using Large Language Models for Quantum Compilation and Simulations', 'authors': 'Pranav Sinha, Sumit Kumar Jha, Sunny Raj', 'link': 'https://arxiv.org/abs/2505.07711', 'abstract': 'We are in the midst of the noisy intermediate-scale quantum (NISQ) era, where quantum computers are limited by noisy gates, some of which are more error-prone than others and can render the final computation incomprehensible. Quantum circuit compilation algorithms attempt to minimize these noisy gates when mapping quantum algorithms onto quantum hardware but face computational challenges that restrict their application to circuits with no more than 5-6 qubits, necessitating the need to partition large circuits before the application of noisy quantum gate minimization algorithms. The existing generation of these algorithms is heuristic in nature and does not account for downstream gate minimization tasks. Large language models (LLMs) have the potential to change this and help improve quantum circuit partitions. This paper investigates the use of LLMs, such as Llama and Mistral, for partitioning quantum circuits by capitalizing on their abilities to understand and generate code, including QASM. Specifically, we teach LLMs to partition circuits using the quick partition approach of the Berkeley Quantum Synthesis Toolkit. Through experimental evaluations, we show that careful fine-tuning of open source LLMs enables us to obtain an accuracy of 53.4% for the partition task while over-the-shelf LLMs are unable to correctly partition circuits, using standard 1-shot and few-shot training approaches.', 'abstract_zh': '我们正处于嘈杂的中尺度量子（NISQ）时代，其中量子计算机受限于嘈杂的门操作，有些门操作比其他门操作更容易出错，可能导致最终计算结果难以理解。量子电路编译算法试图在将量子算法映射到量子硬件时最小化这些嘈杂的门操作，但由于计算挑战的限制，这些算法的应用仅限于不超过5-6个量子位的电路，从而需要在应用嘈杂的量子门操作最小化算法之前将大电路进行分区。现有的这些算法具有启发式性质，并未考虑到后续的门操作最小化任务。大规模语言模型（LLMs）有可能改变这一现状并帮助改进量子电路的分区。本文研究了利用Llama和Mistral等大规模语言模型进行量子电路分区的可能性，利用它们理解和生成代码的能力，包括QASM。具体地，我们教导LLMs使用伯克利量子合成工具包中的快速分区方法来分区电路。通过实验评估，我们表明对开源LLMs进行仔细的微调使我们能够在分区任务中获得53.4%的准确率，而标准的单 Shot 和少 Shot 训练方法无法使即用的大规模语言模型正确地分区电路。', 'title_zh': '使用大规模语言模型进行量子编译和模拟的电路分区'}
{'arxiv_id': 'arXiv:2505.07683', 'title': 'Multimodal Survival Modeling in the Age of Foundation Models', 'authors': 'Steven Song, Morgan Borjigin-Wang, Irene Madejski, Robert L. Grossman', 'link': 'https://arxiv.org/abs/2505.07683', 'abstract': 'The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a large-scale reference through its harmonized genomics, clinical, and image data. Prior studies have trained bespoke cancer survival prediction models from unimodal or multimodal TCGA data. A modern paradigm in biomedical deep learning is the development of foundation models (FMs) to derive meaningful feature embeddings, agnostic to a specific modeling task. Biomedical text especially has seen growing development of FMs. While TCGA contains free-text data as pathology reports, these have been historically underutilized. Here, we investigate the feasibility of training classical, multimodal survival models over zero-shot embeddings extracted by FMs. We show the ease and additive effect of multimodal fusion, outperforming unimodal models. We demonstrate the benefit of including pathology report text and rigorously evaluate the effect of model-based text summarization and hallucination. Overall, we modernize survival modeling by leveraging FMs and information extraction from pathology reports.', 'abstract_zh': 'The Cancer Genome Atlas (TCGA)通过协调的基因组、临床和图像数据，实现了新型发现并成为大规模参考。先前的研究从单模态或跨模态TCGA数据中训练定制的癌症生存预测模型。生物医学深度学习中的一种现代范式是开发基础模型(FMs)以提取无特定模型任务之别的有意义特征嵌入。生物医学文本尤其见证了FMs的发展。尽管TCGA包含病理报告等自由文本数据，但这些数据历来未被充分利用。我们研究了使用FMs提取零样本嵌入训练传统跨模态生存模型的可能性。我们展示了跨模态融合的便捷性和附加效应，优于单模态模型。我们展示了包含病理报告文本的益处，并严格评估基于模型的文本摘要和幻觉效果。总体而言，我们通过利用FMs和病理报告信息提取来现代化生存模型。', 'title_zh': '基础模型时代多模态生存模型研究'}
{'arxiv_id': 'arXiv:2505.07672', 'title': 'OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit', 'authors': 'Arun S. Maiya', 'link': 'https://arxiv.org/abs/2505.07672', 'abstract': 'We present this http URL, a Python-based toolkit for applying large language models (LLMs) to sensitive, non-public data in offline or restricted environments. The system is designed for privacy-preserving use cases and provides prebuilt pipelines for document processing and storage, retrieval-augmented generation (RAG), information extraction, summarization, classification, and prompt/output processing with minimal configuration. this http URL supports multiple LLM backends -- including this http URL, Ollama, vLLM, and Hugging Face Transformers -- with quantized model support, GPU acceleration, and seamless backend switching. Although designed for fully local execution, this http URL also supports integration with a wide range of cloud LLM providers when permitted, enabling hybrid deployments that balance performance with data control. A no-code web interface extends accessibility to non-technical users.', 'abstract_zh': '我们提供了一个基于Python的工具包this http URL，用于在离线或受限环境中应用大语言模型（LLMs）处理敏感的非公开数据。该系统设计用于保护隐私的应用场景，并提供了预构建的文档处理和存储、检索增强生成（RAG）、信息提取、总结、分类以及提示/输出处理管道，配置简单。this http URL支持多种LLM后端，包括this http URL、Ollama、vLLM和Hugging Face Transformers，支持量化模型、GPU加速，并且可以无缝切换后端。尽管设计用于本地执行，但当允许时，this http URL也可以与广泛的云LLM提供商集成，从而实现性能与数据控制之间的平衡。无代码Web界面使非技术用户也能访问。', 'title_zh': 'OnPrem.LLM：一种注重隐私的文档智能工具包'}
{'arxiv_id': 'arXiv:2505.07671', 'title': 'Benchmarking Retrieval-Augmented Generation for Chemistry', 'authors': 'Xianrui Zhong, Bowen Jin, Siru Ouyang, Yanzhen Shen, Qiao Jin, Yin Fang, Zhiyong Lu, Jiawei Han', 'link': 'https://arxiv.org/abs/2505.07671', 'abstract': 'Retrieval-augmented generation (RAG) has emerged as a powerful framework for enhancing large language models (LLMs) with external knowledge, particularly in scientific domains that demand specialized and dynamic information. Despite its promise, the application of RAG in the chemistry domain remains underexplored, primarily due to the lack of high-quality, domain-specific corpora and well-curated evaluation benchmarks. In this work, we introduce ChemRAG-Bench, a comprehensive benchmark designed to systematically assess the effectiveness of RAG across a diverse set of chemistry-related tasks. The accompanying chemistry corpus integrates heterogeneous knowledge sources, including scientific literature, the PubChem database, PubMed abstracts, textbooks, and Wikipedia entries. In addition, we present ChemRAG-Toolkit, a modular and extensible RAG toolkit that supports five retrieval algorithms and eight LLMs. Using ChemRAG-Toolkit, we demonstrate that RAG yields a substantial performance gain -- achieving an average relative improvement of 17.4% over direct inference methods. We further conduct in-depth analyses on retriever architectures, corpus selection, and the number of retrieved passages, culminating in practical recommendations to guide future research and deployment of RAG systems in the chemistry domain. The code and data is available at this https URL.', 'abstract_zh': '检索增强生成（RAG）已成为一种增强大规模语言模型（LLMs）的有力框架，特别是在需要专门和动态信息的科学领域。尽管前景广阔，但在化学领域的应用仍然未被充分探索，主要原因在于缺乏高质量的领域特定语料库和完善的评估基准。在本文中，我们引入了ChemRAG-Bench，这是一个全面的基准，旨在系统评估RAG在一系列化学相关任务中的有效性。伴随的化学语料库整合了异构知识源，包括科学文献、PubChem数据库、PubMed摘要、教科书和维基百科条目。此外，我们介绍了ChemRAG-Toolkit，这是一个模块化和可扩展的RAG工具包，支持五种检索算法和八种LLM。利用ChemRAG-Toolkit，我们展示了RAG取得显著性能提升——相对于直接推理方法，平均相对改进率为17.4%。我们还进行深入分析，探讨了检索器架构、语料库选择和检索段落数量等方面，最终提出指导未来RAG系统在化学领域研究和部署的实际建议。代码和数据可在以下链接获取。', 'title_zh': '化学领域的检索增强生成基准研究'}
{'arxiv_id': 'arXiv:2505.07664', 'title': 'A Case Study Investigating the Role of Generative AI in Quality Evaluations of Epics in Agile Software Development', 'authors': 'Werner Geyer, Jessica He, Daita Sarkar, Michelle Brachman, Chris Hammond, Jennifer Heins, Zahra Ashktorab, Carlos Rosemberg, Charlie Hill', 'link': 'https://arxiv.org/abs/2505.07664', 'abstract': 'The broad availability of generative AI offers new opportunities to support various work domains, including agile software development. Agile epics are a key artifact for product managers to communicate requirements to stakeholders. However, in practice, they are often poorly defined, leading to churn, delivery delays, and cost overruns. In this industry case study, we investigate opportunities for large language models (LLMs) to evaluate agile epic quality in a global company. Results from a user study with 17 product managers indicate how LLM evaluations could be integrated into their work practices, including perceived values and usage in improving their epics. High levels of satisfaction indicate that agile epics are a new, viable application of AI evaluations. However, our findings also outline challenges, limitations, and adoption barriers that can inform both practitioners and researchers on the integration of such evaluations into future agile work practices.', 'abstract_zh': '生成式人工智能的广泛availability为各种工作领域提供了新的机会，包括敏捷软件开发。在实践中，敏捷史诗往往定义不足，导致返工、交付延迟和成本超支。在这一行业案例研究中，我们调查了大型语言模型（LLMs）在一家全球公司中评估敏捷史诗质量的机会。用户研究（17名产品经理参与）的结果表明，LLM评估如何能够整合到他们的工作实践中，包括在提高其史诗方面感知到的价值和使用情况。高度的满意度表明，敏捷史诗是AI评估的一项新且可行的应用。然而，我们的研究成果也指出了挑战、限制和采纳障碍，这些信息可以为从业者和研究人员提供关于将此类评估整合到未来敏捷工作实践中的指导。', 'title_zh': '一项探究生成式AI在敏捷软件开发中史诗质量评估作用的案例研究'}
{'arxiv_id': 'arXiv:2505.07610', 'title': 'Concept-Level Explainability for Auditing & Steering LLM Responses', 'authors': 'Kenza Amara, Rita Sevastjanova, Mennatallah El-Assady', 'link': 'https://arxiv.org/abs/2505.07610', 'abstract': "As large language models (LLMs) become widely deployed, concerns about their safety and alignment grow. An approach to steer LLM behavior, such as mitigating biases or defending against jailbreaks, is to identify which parts of a prompt influence specific aspects of the model's output. Token-level attribution methods offer a promising solution, but still struggle in text generation, explaining the presence of each token in the output separately, rather than the underlying semantics of the entire LLM response. We introduce ConceptX, a model-agnostic, concept-level explainability method that identifies the concepts, i.e., semantically rich tokens in the prompt, and assigns them importance based on the outputs' semantic similarity. Unlike current token-level methods, ConceptX also offers to preserve context integrity through in-place token replacements and supports flexible explanation goals, e.g., gender bias. ConceptX enables both auditing, by uncovering sources of bias, and steering, by modifying prompts to shift the sentiment or reduce the harmfulness of LLM responses, without requiring retraining. Across three LLMs, ConceptX outperforms token-level methods like TokenSHAP in both faithfulness and human alignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for random edits and lower attack success rates from 0.463 to 0.242, outperforming attribution and paraphrasing baselines. While prompt engineering and self-explaining methods sometimes yield safer responses, ConceptX offers a transparent and faithful alternative for improving LLM safety and alignment, demonstrating the practical value of attribution-based explainability in guiding LLM behavior.", 'abstract_zh': '随着大型语言模型（LLMs）的广泛应用，对其安全性和一致性方面的担忧日益增加。一种引导LLM行为的方法，如缓解偏见或防御逃逸攻击，是确定提示中哪些部分影响模型输出的具体方面。基于令牌级别的归因方法提供了一种有前途的解决方案，但仍然在文本生成中挣扎，难以单独解释输出中每个令牌的存在，而不是整个LLM响应的底层语义。我们引入了ConceptX，这是一种模型无关的概念级解释方法，它识别出概念，即提示中的语义丰富的令牌，并基于输出的语义相似度赋予它们重要性。与现有的基于令牌的方法不同，ConceptX 还可以通过就地令牌替换来保持上下文完整性，并支持灵活的解释目标，例如性别偏见。ConceptX 既能用于审计，通过揭示偏见来源，也能用于引导，通过修改提示来改变情感或减少LLM响应的危害性，而无需重新训练。在三种LLM中，ConceptX 在忠实度和人类一致性方面均优于基于令牌的方法（如TokenSHAP）。在转向任务中，概念X使情感转变提高了0.252，而随机编辑为0.131，并将攻击成功率从0.463降低到0.242，超越了归因和改写基线。虽然提示工程和自我解释方法有时会带来更安全的响应，但ConceptX 提供了一种透明且忠实的替代方案，用于提高LLM的安全性和一致性，展示了基于归因的解释在引导LLM行为方面的实际价值。', 'title_zh': '概念级解释性审计与引导LLM响应'}
{'arxiv_id': 'arXiv:2505.07608', 'title': 'MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining', 'authors': 'Xiaomi LLM-Core Team, Bingquan Xia, Bowen Shen, Cici, Dawei Zhu, Di Zhang, Gang Wang, Hailin Zhang, Huaqiu Liu, Jiebao Xiao, Jinhao Dong, Liang Zhao, Peidian Li, Peng Wang, Shihua Yu, Shimao Chen, Weikun Wang, Wenhan Ma, Xiangwei Deng, Yi Huang, Yifan Song, Zihan Jiang, Bowen Ye, Can Cai, Chenhong He, Dong Zhang, Duo Zhang, Guoan Wang, Hao Tian, Haochen Zhao, Heng Qu, Hongshen Xu, Jun Shi, Kainan Bao, QingKai Fang, Kang Zhou, Kangyang Zhou, Lei Li, Menghang Zhu, Nuo Chen, Qiantong Wang, Shaohui Liu, Shicheng Li, Shuhao Gu, Shuhuai Ren, Shuo Liu, Sirui Deng, Weiji Zhuang, Weiwei Lv, Wenyu Yang, Xin Zhang, Xing Yong, Xing Zhang, Xingchen Song, Xinzhe Xu, Xu Wang, Yihan Yan, Yu Tu, Yuanyuan Tian, Yudong Wang, Yue Yu, Zhenru Lin, Zhichao Song, Zihao Yue', 'link': 'https://arxiv.org/abs/2505.07608', 'abstract': "We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at this https URL.", 'abstract_zh': 'MiMo-7B：一种生于推理任务的大型语言模型，经过前后训练阶段的优化', 'title_zh': 'MiMo：解锁语言模型的推理潜力——从预训练到后训练'}
{'arxiv_id': 'arXiv:2505.07596', 'title': 'Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent', 'authors': 'Ziyang Huang, Xiaowei Yuan, Yiming Ju, Jun Zhao, Kang Liu', 'link': 'https://arxiv.org/abs/2505.07596', 'abstract': 'Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead to redundant retrievals, potential harmful knowledge conflicts, and increased inference latency. To address these limitations, an efficient and adaptive search agent capable of discerning optimal retrieval timing and synergistically integrating parametric (internal) and retrieved (external) knowledge is in urgent need. This paper introduces the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could indentify its own knowledge boundary and prioritize the utilization of internal knowledge, resorting to external search only when internal knowledge is deemed insufficient. This is achieved using a novel knowledge-boundary aware reward function and a knowledge-boundary aware training dataset. These are designed for internal-external knowledge synergy oriented RL, incentivizing the model to deliver accurate answers, minimize unnecessary retrievals, and encourage appropriate external searches when its own knowledge is lacking. Evaluations across multiple knowledge reasoning tasks demonstrate that IKEA significantly outperforms baseline methods, reduces retrieval frequency significantly, and exhibits robust generalization capabilities.', 'abstract_zh': '基于检索增强生成的强化内部-外部知识协同推理代理（IKEA）', 'title_zh': '强化内外知识协同推理以实现高效自适应搜索代理'}
{'arxiv_id': 'arXiv:2505.07591', 'title': 'A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models', 'authors': 'Junjie Ye, Caishuang Huang, Zhuohan Chen, Wenjie Fu, Chenyuan Yang, Leyi Yang, Yilong Wu, Peng Wang, Meng Zhou, Xiaolong Yang, Tao Gui, Qi Zhang, Zhongchao Shi, Jianping Fan, Xuanjing Huang', 'link': 'https://arxiv.org/abs/2505.07591', 'abstract': "Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessment. To fill this gap, we propose a multi-dimensional constraint framework encompassing three constraint patterns, four constraint categories, and four difficulty levels. Building on this framework, we develop an automated instruction generation pipeline that performs constraint expansion, conflict detection, and instruction rewriting, yielding 1,200 code-verifiable instruction-following test samples. We evaluate 19 LLMs across seven model families and uncover substantial variation in performance across constraint forms. For instance, average performance drops from 77.67% at Level I to 32.96% at Level IV. Furthermore, we demonstrate the utility of our approach by using it to generate data for reinforcement learning, achieving substantial gains in instruction following without degrading general performance. In-depth analysis indicates that these gains stem primarily from modifications in the model's attention modules parameters, which enhance constraint recognition and adherence. Code and data are available in this https URL.", 'abstract_zh': '大规模语言模型指令遵循的多维度约束框架及应用研究', 'title_zh': '大型语言模型中指令跟随评价与改进的多维度约束框架'}
{'arxiv_id': 'arXiv:2505.07553', 'title': 'Towards Requirements Engineering for RAG Systems', 'authors': 'Tor Sporsem, Rasmus Ulfsnes', 'link': 'https://arxiv.org/abs/2505.07553', 'abstract': 'This short paper explores how a maritime company develops and integrates large-language models (LLM). Specifically by looking at the requirements engineering for Retrieval Augmented Generation (RAG) systems in expert settings. Through a case study at a maritime service provider, we demonstrate how data scientists face a fundamental tension between user expectations of AI perfection and the correctness of the generated outputs. Our findings reveal that data scientists must identify context-specific "retrieval requirements" through iterative experimentation together with users because they are the ones who can determine correctness. We present an empirical process model describing how data scientists practically elicited these "retrieval requirements" and managed system limitations. This work advances software engineering knowledge by providing insights into the specialized requirements engineering processes for implementing RAG systems in complex domain-specific applications.', 'abstract_zh': '这篇短论文探讨了海洋运输公司如何开发和整合大型语言模型（LLM）。通过具体分析专家环境中检索增强生成（RAG）系统的开发工程需求，论文展示了数据科学家在用户对人工智能完美性的期望与生成输出的准确性之间的基本张力。研究发现，数据科学家必须通过与用户的迭代实验来识别具体的“检索要求”，因为他们才能确定准确性。论文呈现了描述数据科学家如何实际获取这些“检索要求”以及管理系统限制的经验过程模型。这项工作通过提供关于在复杂领域特定应用中实现RAG系统的专门开发工程流程的洞察，推进了软件工程知识。', 'title_zh': '面向RAG系统的 Requirements Engineering 研究'}
{'arxiv_id': 'arXiv:2505.07546', 'title': 'GRADA: Graph-based Reranker against Adversarial Documents Attack', 'authors': 'Jingjie Zheng, Aryo Pradipta Gema, Giwon Hong, Xuanli He, Pasquale Minervini, Youcheng Sun, Qiongkai Xu', 'link': 'https://arxiv.org/abs/2505.07546', 'abstract': "Retrieval Augmented Generation (RAG) frameworks improve the accuracy of large language models (LLMs) by integrating external knowledge from retrieved documents, thereby overcoming the limitations of models' static intrinsic knowledge. However, these systems are susceptible to adversarial attacks that manipulate the retrieval process by introducing documents that are adversarial yet semantically similar to the query. Notably, while these adversarial documents resemble the query, they exhibit weak similarity to benign documents in the retrieval set. Thus, we propose a simple yet effective Graph-based Reranking against Adversarial Document Attacks (GRADA) framework aiming at preserving retrieval quality while significantly reducing the success of adversaries. Our study evaluates the effectiveness of our approach through experiments conducted on five LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b, Llama3.1-70b, and Qwen2.5-7b. We use three datasets to assess performance, with results from the Natural Questions dataset demonstrating up to an 80% reduction in attack success rates while maintaining minimal loss in accuracy.", 'abstract_zh': '基于图的对抗文档攻击重排序框架（GRADA）提高大型语言模型的检索质量并显著降低攻击成功率', 'title_zh': '基于图的对抗文档攻击重排序器（GRADA）'}
{'arxiv_id': 'arXiv:2505.07512', 'title': 'ToolACE-DEV: Self-Improving Tool Learning via Decomposition and EVolution', 'authors': 'Xu Huang, Weiwen Liu, Xingshan Zeng, Yuefeng Huang, Xinlong Hao, Yuxian Wang, Yirong Zeng, Chuhan Wu, Yasheng Wang, Ruiming Tang, Defu Lian', 'link': 'https://arxiv.org/abs/2505.07512', 'abstract': 'The tool-using capability of large language models (LLMs) enables them to access up-to-date external information and handle complex tasks. Current approaches to enhancing this capability primarily rely on distilling advanced models by data synthesis. However, this method incurs significant costs associated with advanced model usage and often results in data compatibility issues, led by the high discrepancy in the knowledge scope between the advanced model and the target model. To address these challenges, we propose ToolACE-DEV, a self-improving framework for tool learning. First, we decompose the tool-learning objective into sub-tasks that enhance basic tool-making and tool-using abilities. Then, we introduce a self-evolving paradigm that allows lightweight models to self-improve, reducing reliance on advanced LLMs. Extensive experiments validate the effectiveness of our approach across models of varying scales and architectures.', 'abstract_zh': '大型语言模型（LLMs）的工具使用能力使它们能够访问最新的外部信息并处理复杂任务。当前增强这种能力的方法主要依赖于通过数据合成蒸馏先进模型。然而，这种方法会产生与先进模型使用相关的高昂成本，并且经常导致数据兼容性问题，原因是先进模型和目标模型的知识范围存在巨大差异。为了解决这些挑战，我们提出了一种自提高框架ToolACE-DEV，用于工具学习。首先，我们将工具学习目标分解为增强基本工具制造和使用能力的子任务。然后，我们引入了一种自我进化的范式，使轻量级模型能够自我改进，从而减少对先进LLM的依赖。广泛实验证明了该方法在不同规模和架构模型上的有效性。', 'title_zh': 'ToolACE-DEV: 自我提升的工具学习通过分解与进化'}
{'arxiv_id': 'arXiv:2505.07457', 'title': 'Can Generative AI agents behave like humans? Evidence from laboratory market experiments', 'authors': 'R. Maria del Rio-Chanona, Marco Pangallo, Cars Hommes', 'link': 'https://arxiv.org/abs/2505.07457', 'abstract': "We explore the potential of Large Language Models (LLMs) to replicate human behavior in economic market experiments. Compared to previous studies, we focus on dynamic feedback between LLM agents: the decisions of each LLM impact the market price at the current step, and so affect the decisions of the other LLMs at the next step. We compare LLM behavior to market dynamics observed in laboratory settings and assess their alignment with human participants' behavior. Our findings indicate that LLMs do not adhere strictly to rational expectations, displaying instead bounded rationality, similarly to human participants. Providing a minimal context window i.e. memory of three previous time steps, combined with a high variability setting capturing response heterogeneity, allows LLMs to replicate broad trends seen in human experiments, such as the distinction between positive and negative feedback markets. However, differences remain at a granular level--LLMs exhibit less heterogeneity in behavior than humans. These results suggest that LLMs hold promise as tools for simulating realistic human behavior in economic contexts, though further research is needed to refine their accuracy and increase behavioral diversity.", 'abstract_zh': '我们探讨大型语言模型（LLMs）在经济市场实验中复制人类行为的潜力。与以往研究相比，我们关注LLM代理之间的动态反馈：每个LLM的决策会影响当前步骤的市场价格，从而影响其他LLM在下一步骤的决策。我们将LLM的行为与实验室环境中观察到的市场动态进行比较，并评估其与人类参与者行为的契合度。研究发现，LLMs并不严格遵守理性预期，而是表现出局限性理性，类似于人类参与者。提供一个最小的上下文窗口即三步之前的记忆，结合一个高变异设置以捕捉反应异质性，使LLMs能够复制人类实验中看到的广泛趋势，如正反馈市场和负反馈市场的区别。然而，在细微层面上仍存在差异——LLMs在行为上的异质性低于人类。这些结果表明，LLMs有潜力成为模拟经济背景下现实人类行为的工具，但仍需进一步研究以提高其准确性并增加行为多样性。', 'title_zh': '生成式AI代理能否像人类一样行为？来自实验室市场实验的证据'}
{'arxiv_id': 'arXiv:2505.07437', 'title': 'LEAD: Iterative Data Selection for Efficient LLM Instruction Tuning', 'authors': 'Xiaotian Lin, Yanlin Qi, Yizhang Zhu, Themis Palpanas, Chengliang Chai, Nan Tang, Yuyu Luo', 'link': 'https://arxiv.org/abs/2505.07437', 'abstract': 'Instruction tuning has emerged as a critical paradigm for improving the capabilities and alignment of large language models (LLMs). However, existing iterative model-aware data selection methods incur significant computational overhead, as they rely on repeatedly performing full-dataset model inference to estimate sample utility for subsequent training iterations, creating a fundamental efficiency bottleneck. In this paper, we propose LEAD, an efficient iterative data selection framework that accurately estimates sample utility entirely within the standard training loop, eliminating the need for costly additional model inference. At its core, LEAD introduces Instance-Level Dynamic Uncertainty (IDU), a theoretically grounded utility function combining instantaneous training loss, gradient-based approximation of loss changes, and exponential smoothing of historical loss signals. To further scale efficiently to large datasets, LEAD employs a two-stage, coarse-to-fine selection strategy, adaptively prioritizing informative clusters through a multi-armed bandit mechanism, followed by precise fine-grained selection of high-utility samples using IDU. Extensive experiments across four diverse benchmarks show that LEAD significantly outperforms state-of-the-art methods, improving average model performance by 6.1%-10.8% while using only 2.5% of the training data and reducing overall training time by 5-10x.', 'abstract_zh': 'LEAD：一种高效的数据选择框架，用于大型语言模型的指令调优', 'title_zh': '迭代数据选择以实现高效的大规模语言模型指令调优'}
{'arxiv_id': 'arXiv:2505.07393', 'title': 'AI in Money Matters', 'authors': 'Nadine Sandjo Tchatchoua, Richard Harper', 'link': 'https://arxiv.org/abs/2505.07393', 'abstract': 'In November 2022, Europe and the world by and large were stunned by the birth of a new large language model : ChatGPT. Ever since then, both academic and populist discussions have taken place in various public spheres such as LinkedIn and X(formerly known as Twitter) with the view to both understand the tool and its benefits for the society. The views of real actors in professional spaces, especially in regulated industries such as finance and law have been largely missing. We aim to begin to close this gap by presenting results from an empirical investigation conducted through interviews with professional actors in the Fintech industry. The paper asks the question, how and to what extent are large language models in general and ChatGPT in particular being adopted and used in the Fintech industry? The results show that while the fintech experts we spoke with see a potential in using large language models in the future, a lot of questions marks remain concerning how they are policed and therefore might be adopted in a regulated industry such as Fintech. This paper aims to add to the existing academic discussing around large language models, with a contribution to our understanding of professional viewpoints.', 'abstract_zh': '欧洲和世界在2022年11月对一个新的大型语言模型ChatGPT的诞生感到震惊。自此之后，学术界和普通民众在LinkedIn和X（原Twitter）等公共领域展开了讨论，试图理解这一工具及其对社会的好处。来自金融和法律等受监管行业的专业人员的观点在这些讨论中 largely 缺席。我们希望通过访谈金融科技行业专业人士进行实证调查的结果，开始缩小这一缺口。本文探讨了大型语言模型，特别是ChatGPT，在金融科技行业中被采纳和使用的具体情况和程度。结果显示，尽管我们访谈的金融科技专家看到了未来使用大型语言模型的潜力，但在一个如金融科技这样的受监管行业中，它们如何被监管以及可能会如何被采纳仍存在许多疑问。本文旨在为现有关于大型语言模型的学术讨论做出贡献，并增进我们对专业视角的理解。', 'title_zh': 'AI在金融事务中的应用'}
{'arxiv_id': 'arXiv:2505.07377', 'title': 'Examining the Role of LLM-Driven Interactions on Attention and Cognitive Engagement in Virtual Classrooms', 'authors': 'Suleyman Ozdel, Can Sarpkaya, Efe Bozkir, Hong Gao, Enkelejda Kasneci', 'link': 'https://arxiv.org/abs/2505.07377', 'abstract': 'Transforming educational technologies through the integration of large language models (LLMs) and virtual reality (VR) offers the potential for immersive and interactive learning experiences. However, the effects of LLMs on user engagement and attention in educational environments remain open questions. In this study, we utilized a fully LLM-driven virtual learning environment, where peers and teachers were LLM-driven, to examine how students behaved in such settings. Specifically, we investigate how peer question-asking behaviors influenced student engagement, attention, cognitive load, and learning outcomes and found that, in conditions where LLM-driven peer learners asked questions, students exhibited more targeted visual scanpaths, with their attention directed toward the learning content, particularly in complex subjects. Our results suggest that peer questions did not introduce extraneous cognitive load directly, as the cognitive load is strongly correlated with increased attention to the learning material. Considering these findings, we provide design recommendations for optimizing VR learning spaces.', 'abstract_zh': '通过将大型语言模型（LLMs）和虚拟现实（VR）整合以转变教育技术提供了沉浸式和交互式学习体验的潜力。然而，LLMs 对用户在教育环境中的参与度和注意力的影响仍然存在疑问。在本研究中，我们利用了一个完全由LLM驱动的虚拟学习环境，其中同伴和教师都是由LLM驱动的，以考察学生在这种环境中的行为。具体而言，我们研究了同伴提问行为如何影响学生参与度、注意力、认知负荷和学习成果，发现当LLM驱动的同伴学习者提问时，学生表现出更定向的视线扫描路径，注意力集中在学习内容上，尤其是在复杂科目中。研究结果表明，同伴问题并没有直接引入额外的认知负荷，因为认知负荷与对学习材料的注意力增加有很强的相关性。基于这些发现，我们提供了优化VR学习空间的设计建议。', 'title_zh': '考查LLM驱动交互在虚拟课堂中对注意力和认知参与的作用'}
{'arxiv_id': 'arXiv:2505.07372', 'title': 'Synthetic Code Surgery: Repairing Bugs and Vulnerabilities with LLMs and Synthetic Data', 'authors': 'David de-Fitero-Dominguez, Antonio Garcia-Cabot, Eva Garcia-Lopez', 'link': 'https://arxiv.org/abs/2505.07372', 'abstract': "This paper presents a novel methodology for enhancing Automated Program Repair (APR) through synthetic data generation utilizing Large Language Models (LLMs). Current APR systems are constrained by the limited availability of high-quality training data encompassing diverse bug types across multiple programming languages. The proposed approach addresses this limitation through a two-phase process: a synthetic sample generation followed by a rigorous quality assessment. Multiple state-of-the-art LLMs were employed to generate approximately 30,000 paired examples of buggy and fixed code across 12 programming languages and 13 bug categories. Subsequently, these samples underwent cross-model evaluation against five criteria: correctness, code quality, security, performance, and completeness. Experimental evaluation on the VulRepair test set dataset showed statistically significant improvements in Perfect Prediction rates, with the quality-filtered synthetic dataset outperforming both baseline and real-world commit data configurations in certain scenarios. The methodology was validated through rigorous statistical testing, including ANOVA and post-hoc Tukey's Honest Significant Difference analysis. Furthermore, the best-performing configurations surpassed existing systems despite using a less computationally intensive decoding strategy. This research establishes a self-bootstrapping paradigm in which LLMs generate and evaluate their own training data, potentially transforming approaches to data scarcity across software engineering tasks and advancing the development of robust, adaptable tools for automated code maintenance.", 'abstract_zh': '本研究提出了一种通过大型语言模型生成合成数据以增强自动化程序修复的新方法。当前的自动化程序修复系统受限于高质量训练数据的有限可用性，这些数据涵盖了多种编程语言中的不同类型的错误。所提出的方法通过两阶段过程来解决这一限制：合成样本生成和严格的质量评估。采用了多种最先进的大型语言模型，生成了约30,000个跨12种编程语言和13种错误类别的代码错误和修复配对示例。随后，这些样本经过针对正确性、代码质量、安全性、性能和完整性五个标准的多模型评估。在VulRepair测试集数据集上进行的实验评估显示，在某些场景下，经过质量过滤的合成数据集在完美预测率方面取得了统计学上的显著改善，优于基线和实际提交数据配置。该方法通过严格的统计测试得到了验证，包括ANOVA和事后Tukey’s诚实显著差异分析。此外，最优配置在使用较不计算密集型解码策略的情况下仍然超过了现有系统。这项研究确立了一种自我启动范式，其中大型语言模型自动生成并评估自己的训练数据，有可能在软件工程任务中改变数据稀缺性问题，并推动开发更加鲁棒和适应性强的自动化代码维护工具。', 'title_zh': '合成代码手术：使用大规模语言模型和合成数据修复漏洞与错误'}
{'arxiv_id': 'arXiv:2505.07345', 'title': 'QUPID: Quantified Understanding for Enhanced Performance, Insights, and Decisions in Korean Search Engines', 'authors': 'Ohjoon Kwon, Changsu Lee, Jihye Back, Lim Sun Suk, Inho Kang, Donghyeon Jeon', 'link': 'https://arxiv.org/abs/2505.07345', 'abstract': "Large language models (LLMs) have been widely used for relevance assessment in information retrieval. However, our study demonstrates that combining two distinct small language models (SLMs) with different architectures can outperform LLMs in this task. Our approach -- QUPID -- integrates a generative SLM with an embedding-based SLM, achieving higher relevance judgment accuracy while reducing computational costs compared to state-of-the-art LLM solutions. This computational efficiency makes QUPID highly scalable for real-world search systems processing millions of queries daily. In experiments across diverse document types, our method demonstrated consistent performance improvements (Cohen's Kappa of 0.646 versus 0.387 for leading LLMs) while offering 60x faster inference times. Furthermore, when integrated into production search pipelines, QUPID improved nDCG@5 scores by 1.9%. These findings underscore how architectural diversity in model combinations can significantly enhance both search relevance and operational efficiency in information retrieval systems.", 'abstract_zh': '大型语言模型（LLMs）在信息检索中的相关性评估中已广泛应用。然而，我们的研究表明，结合两种具有不同架构的独立小型语言模型（SLMs）可以在这一任务上超越LLMs。我们的方法——QUPID——将生成型SLM与基于嵌入的SLM集成，相比于最先进的LLM解决方案，能够在提高相关性判断准确率的同时降低计算成本。这种计算效率使QUPID在处理每天数百万查询的实际搜索系统中具有高度的可扩展性。在针对不同文档类型的实验中，我们的方法展示了一致性的性能改进（科恩κ系数为0.646，而领先的LLM为0.387），同时提供60倍更快的推理时间。此外，当集成到生产搜索管道中时，QUPID将nDCG@5分数提高了1.9%。这些发现强调了在模型组合中架构多样性可以显著提升信息检索系统中的搜索相关性和操作效率。', 'title_zh': 'QUPID: 量化理解以提升韩语搜索引擎的性能、洞察与决策能力'}
{'arxiv_id': 'arXiv:2505.07313', 'title': 'Towards Multi-Agent Reasoning Systems for Collaborative Expertise Delegation: An Exploratory Design Study', 'authors': 'Baixuan Xu, Chunyang Li, Weiqi Wang, Wei Fan, Tianshi Zheng, Haochen Shi, Tao Fan, Yangqiu Song, Qiang Yang', 'link': 'https://arxiv.org/abs/2505.07313', 'abstract': 'Designing effective collaboration structure for multi-agent LLM systems to enhance collective reasoning is crucial yet remains under-explored. In this paper, we systematically investigate how collaborative reasoning performance is affected by three key design dimensions: (1) Expertise-Domain Alignment, (2) Collaboration Paradigm (structured workflow vs. diversity-driven integration), and (3) System Scale. Our findings reveal that expertise alignment benefits are highly domain-contingent, proving most effective for contextual reasoning tasks. Furthermore, collaboration focused on integrating diverse knowledge consistently outperforms rigid task decomposition. Finally, we empirically explore the impact of scaling the multi-agent system with expertise specialization and study the computational trade off, highlighting the need for more efficient communication protocol design. This work provides concrete guidelines for configuring specialized multi-agent system and identifies critical architectural trade-offs and bottlenecks for scalable multi-agent reasoning. The code will be made available upon acceptance.', 'abstract_zh': '设计有效的多智能体LLM系统协作结构以增强集体推理至关重要但仍未充分探索。本论文系统地探索了协作推理性能受三个关键设计维度的影响：（1）专业知识-领域对齐，（2）合作范式（结构化工作流程 vs. 知识多样性集成），（3）系统规模。我们的研究发现，专业知识对齐的效果高度依赖于领域，证明在上下文推理任务中最为有效。此外，注重集成多样化知识的合作方式始终优于刚性任务分解。最后，我们实证探讨了专业知识专门化扩展多智能体系统的影响，并研究了计算权衡，强调了更高效通信协议设计的必要性。本工作为配置专业化的多智能体系统提供了具体的指南，并指出了可扩展多智能体推理的关键架构权衡和瓶颈。接受后将提供代码。', 'title_zh': '面向协作知识委托的多智能体推理系统设计研究：一项探索性研究'}
{'arxiv_id': 'arXiv:2505.07289', 'title': 'Semantic Retention and Extreme Compression in LLMs: Can We Have Both?', 'authors': 'Stanislas Laborde, Martin Cousseau, Antoun Yaacoub, Lionel Prevost', 'link': 'https://arxiv.org/abs/2505.07289', 'abstract': 'The exponential growth in Large Language Model (LLM) deployment has intensified the need for efficient model compression techniques to reduce computational and memory costs. While pruning and quantization have shown promise, their combined potential remains largely unexplored. In this paper, we examine joint compression and how strategically combining pruning and quantization could yield superior performance-to-compression ratios compared to single-method approaches. Recognizing the challenges in accurately assessing LLM performance, we address key limitations of previous evaluation frameworks and introduce the Semantic Retention Compression Rate (SrCr), a novel metric that quantifies the trade-off between model compression and semantic preservation, facilitating the optimization of pruning-quantization configurations. Experiments demonstrate that our recommended combination achieves, on average, a 20% performance increase compared to an equivalent quantization-only model at the same theoretical compression rate.', 'abstract_zh': '大型语言模型部署的指数增长加剧了对高效模型压缩技术的需求以降低计算和内存成本。虽然剪枝和量化显示了潜力，但它们的联合潜力尚未充分探索。本文探讨了联合压缩，并研究了如何战略性地结合剪枝和量化以获得优于单一方法的性能-压缩比。鉴于准确评估大型语言模型性能的挑战，本文解决了先前评价框架的关键局限性，并引入了语义保留压缩率（SrCr），这是一种新的度量标准，用于量化模型压缩与语义保留之间的权衡，以便优化剪枝-量化配置。实验表明，我们建议的组合在相同理论压缩率下，平均可实现20%的性能提升，相比于仅量化模型。', 'title_zh': 'LLMs中的语义保留与极端压缩：两者可以兼得吗？'}
{'arxiv_id': 'arXiv:2505.07271', 'title': 'On the Robustness of Reward Models for Language Model Alignment', 'authors': 'Jiwoo Hong, Noah Lee, Eunki Kim, Guijin Son, Woojin Chung, Aman Gupta, Shao Tang, James Thorne', 'link': 'https://arxiv.org/abs/2505.07271', 'abstract': 'The Bradley-Terry (BT) model is widely practiced in reward modeling for reinforcement learning with human feedback (RLHF). Despite its effectiveness, reward models (RMs) trained with BT model loss are prone to over-optimization, losing generalizability to unseen input distributions. In this paper, we study the cause of over-optimization in RM training and its downstream effects on the RLHF procedure, accentuating the importance of distributional robustness of RMs in unseen data. First, we show that the excessive dispersion of hidden state norms is the main source of over-optimization. Then, we propose batch-wise sum-to-zero regularization (BSR) to enforce zero-centered reward sum per batch, constraining the rewards with extreme magnitudes. We assess the impact of BSR in improving robustness in RMs through four scenarios of over-optimization, where BSR consistently manifests better robustness. Subsequently, we compare the plain BT model and BSR on RLHF training and empirically show that robust RMs better align the policy to the gold preference model. Finally, we apply BSR to high-quality data and models, which surpasses state-of-the-art RMs in the 8B scale by adding more than 5% in complex preference prediction tasks. By conducting RLOO training with 8B RMs, AlpacaEval 2.0 reduces generation length by 40% while adding a 7% increase in win rate, further highlighting that robustness in RMs induces robustness in RLHF training. We release the code, data, and models: this https URL.', 'abstract_zh': '布拉德利-特里(BT)模型在具有人类反馈的强化学习(RLHF)中的奖励模型训练中广泛应用。尽管其有效，但使用BT模型损失训练的奖励模型(RMs)容易过度优化，失去对未见输入分布的泛化能力。本文研究了RMs训练中的过度优化原因及其对RLHF流程的下游影响，强调了RMs对未见数据分布鲁棒性的的重要性。首先，我们表明隐藏状态范数的过度分散是过度优化的主要来源。然后，我们提出了批次累和为零正则化(BSR)，以确保每批次的奖励总和为中心，限制极端幅度的奖励。通过在四种过度优化情景下评估BSR在提高RMs鲁棒性方面的影响，我们发现BSR表现出更好的鲁棒性。接着，我们将BSR与原始BT模型在RLHF训练中进行比较，并实验证明鲁棒的RMs更好地对齐了策略与黄金偏好模型。最后，我们将BSR应用于高质量的数据和模型，在8B规模的任务中实现了超过5%的复杂偏好预测性能提升，并通过RLOO训练AlpacaEval 2.0减少了生成长度40%的同时增加了7%的胜率，进一步强调了RMs的鲁棒性对RLHF训练的鲁棒性所起的作用。我们开源了代码、数据和模型：https://github.com/alibaba/Qwen', 'title_zh': '关于奖励模型在语言模型对齐中的健壮性研究'}
{'arxiv_id': 'arXiv:2505.07258', 'title': 'No Query, No Access', 'authors': 'Wenqiang Wang, Siyuan Liang, Yangshijie Zhang, Xiaojun Jia, Hao Lin, Xiaochun Cao', 'link': 'https://arxiv.org/abs/2505.07258', 'abstract': 'Textual adversarial attacks mislead NLP models, including Large Language Models (LLMs), by subtly modifying text. While effective, existing attacks often require knowledge of the victim model, extensive queries, or access to training data, limiting real-world feasibility. To overcome these constraints, we introduce the \\textbf{Victim Data-based Adversarial Attack (VDBA)}, which operates using only victim texts. To prevent access to the victim model, we create a shadow dataset with publicly available pre-trained models and clustering methods as a foundation for developing substitute models. To address the low attack success rate (ASR) due to insufficient information feedback, we propose the hierarchical substitution model design, generating substitute models to mitigate the failure of a single substitute model at the decision boundary.\nConcurrently, we use diverse adversarial example generation, employing various attack methods to generate and select the adversarial example with better similarity and attack effectiveness. Experiments on the Emotion and SST5 datasets show that VDBA outperforms state-of-the-art methods, achieving an ASR improvement of 52.08\\% while significantly reducing attack queries to 0. More importantly, we discover that VDBA poses a significant threat to LLMs such as Qwen2 and the GPT family, and achieves the highest ASR of 45.99% even without access to the API, confirming that advanced NLP models still face serious security risks. Our codes can be found at this https URL', 'abstract_zh': '基于 Victim 数据的对抗攻击（VDBA）：仅使用 Victim 文本误导 NLP 模型', 'title_zh': '无查询，无访问。'}
{'arxiv_id': 'arXiv:2505.07247', 'title': 'SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring with Large Language Models', 'authors': 'Peichao Lai, Kexuan Zhang, Yi Lin, Linyihan Zhang, Feiyang Ye, Jinhao Yan, Yanwei Xu, Conghui He, Yilei Wang, Wentao Zhang, Bin Cui', 'link': 'https://arxiv.org/abs/2505.07247', 'abstract': 'Subjective Answer Grading (SAG) plays a crucial role in education, standardized testing, and automated assessment systems, particularly for evaluating short-form responses in Short Answer Scoring (SAS). However, existing approaches often produce coarse-grained scores and lack detailed reasoning. Although large language models (LLMs) have demonstrated potential as zero-shot evaluators, they remain susceptible to bias, inconsistencies with human judgment, and limited transparency in scoring decisions. To overcome these limitations, we introduce SAS-Bench, a benchmark specifically designed for LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring, expert-annotated error categories, and a diverse range of question types derived from real-world subject-specific exams. This benchmark facilitates detailed evaluation of model reasoning processes and explainability. We also release an open-source dataset containing 1,030 questions and 4,109 student responses, each annotated by domain experts. Furthermore, we conduct comprehensive experiments with various LLMs, identifying major challenges in scoring science-related questions and highlighting the effectiveness of few-shot prompting in improving scoring accuracy. Our work offers valuable insights into the development of more robust, fair, and educationally meaningful LLM-based evaluation systems.', 'abstract_zh': '主观回答评分（SAG）在教育、标准化测试和自动化评估系统中起着重要作用，特别是在简短答案评分（SAS）中评估简短形式的响应方面。虽然现有的方法通常产生粗粒度的评分并且缺乏详细的理由说明，尽管大型语言模型（LLMs）在零样本评估中显示出潜在能力，但仍易受偏见、人类判断不一致和评分决策透明度低的影响。为克服这些局限性，我们引入了SAS-Bench，一个专门针对基于LLM的SAS任务的基准测试。SAS-Bench提供了细粒度、分步骤的评分、专家标注的错误类别以及源自真实世界学科特定考试的多样化问题类型。该基准测试促进了对模型推理过程和可解释性的详细评估。我们还发布了包含1,030个问题和4,109个学生回应的开源数据集，每个问题和回应都由领域专家标注。此外，我们进行了全面的实验，使用了多种LLM，确定了在评分科学相关问题时的主要挑战，并强调了少样本提示在提高评分准确性方面的有效性。我们的工作为开发更稳健、公平且教育意义更强的基于LLM的评估系统提供了宝贵的洞察。', 'title_zh': 'SAS-Bench：一种细粒度的短答评分评估基准（Large Language Models版本）'}
{'arxiv_id': 'arXiv:2505.07239', 'title': 'Comet: Accelerating Private Inference for Large Language Model by Predicting Activation Sparsity', 'authors': 'Guang Yan, Yuhui Zhang, Zimu Guo, Lutan Zhao, Xiaojun Chen, Chen Wang, Wenhao Wang, Dan Meng, Rui Hou', 'link': 'https://arxiv.org/abs/2505.07239', 'abstract': 'With the growing use of large language models (LLMs) hosted on cloud platforms to offer inference services, privacy concerns about the potential leakage of sensitive information are escalating. Secure multi-party computation (MPC) is a promising solution to protect the privacy in LLM inference. However, MPC requires frequent inter-server communication, causing high performance overhead.\nInspired by the prevalent activation sparsity of LLMs, where most neuron are not activated after non-linear activation functions, we propose an efficient private inference system, Comet. This system employs an accurate and fast predictor to predict the sparsity distribution of activation function output. Additionally, we introduce a new private inference protocol. It efficiently and securely avoids computations involving zero values by exploiting the spatial locality of the predicted sparse distribution. While this computation-avoidance approach impacts the spatiotemporal continuity of KV cache entries, we address this challenge with a low-communication overhead cache refilling strategy that merges miss requests and incorporates a prefetching mechanism. Finally, we evaluate Comet on four common LLMs and compare it with six state-of-the-art private inference systems. Comet achieves a 1.87x-2.63x speedup and a 1.94x-2.64x communication reduction.', 'abstract_zh': '基于云平台的大语言模型推理服务中，隐私泄露担忧加剧，安全多方计算是保护隐私的 promising 解决方案。然而，安全多方计算需要频繁的服务器间通信，导致高性能开销。受大语言模型中普遍存在的激活稀疏性启发，大多数神经元在非线性激活函数后未被激活，我们提出了一种高效的隐私推理系统 Comet。该系统采用准确且快速的预测器来预测激活函数输出的稀疏性分布。此外，我们引入了一种新的隐私推理协议。该协议通过利用预测稀疏分布的空间局部性高效且安全地避免涉及零值的计算。尽管这种计算避免方法影响 KV 缓存条目的时空间连续性，我们通过一种低通信开销的缓存补充策略解决了这一挑战，该策略合并了缺失请求并结合了一个预取机制。最后，我们在四种常见的大语言模型上评估了 Comet，并将其与六种最先进的隐私推理系统进行了比较。Comet 实现了 1.87-2.63 倍的加速和 1.94-2.64 倍的通信量减少。', 'title_zh': 'Comet: 通过预测激活稀疏性加速大型语言模型的隐私推理'}
{'arxiv_id': 'arXiv:2505.07233', 'title': 'DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation', 'authors': 'Jiashuo Sun, Xianrui Zhong, Sizhe Zhou, Jiawei Han', 'link': 'https://arxiv.org/abs/2505.07233', 'abstract': 'Retrieval-augmented generation (RAG) systems combine large language models (LLMs) with external knowledge retrieval, making them highly effective for knowledge-intensive tasks. A crucial but often under-explored component of these systems is the reranker, which refines retrieved documents to enhance generation quality and explainability. The challenge of selecting the optimal number of documents (k) remains unsolved: too few may omit critical information, while too many introduce noise and inefficiencies. Although recent studies have explored LLM-based rerankers, they primarily leverage internal model knowledge and overlook the rich supervisory signals that LLMs can provide, such as using response quality as feedback for optimizing reranking decisions. In this paper, we propose DynamicRAG, a novel RAG framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query. We model the reranker as an agent optimized through reinforcement learning (RL), using rewards derived from LLM output quality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates superior performance, achieving state-of-the-art results. The model, data and code are available at this https URL', 'abstract_zh': '检索增强生成（RAG）系统结合了大规模语言模型（LLMs）与外部知识检索，使其在知识密集型任务中表现出色。这些系统中的关键但常常被忽视的组件是再排序器，它通过对检索到的文档进行细化来提升生成质量和可解释性。如何选择最优的文档数量（k）这一挑战仍未解决：数量过少可能导致重要信息缺失，而数量过多则会引入噪音和低效。尽管近期研究已经探索了基于LLM的再排序器，但这些研究主要利用了模型内部的知识，而忽视了LLM能够提供的丰富监督信号，如使用响应质量作为反馈来优化再排序决策。在本文中，我们提出了DynamicRAG，这是一种新颖的RAG框架，其中再排序器能够根据查询动态调整检索到的文档的数量和顺序。我们通过强化学习（RL）将再排序器建模为一个优化代理，并使用来源于LLM输出质量的奖励。在七个知识密集型数据集上，DynamicRAG展示了优越的性能，达到了最先进的成果。模型、数据和代码可在以下链接获取。', 'title_zh': '动态RAG：将大规模语言模型的输出作为反馈用于检索增强生成的动态重排名'}
{'arxiv_id': 'arXiv:2505.07078', 'title': 'Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?', 'authors': 'Weixian Waylon Li, Hyeonjun Kim, Mihai Cucuringu, Tiejun Ma', 'link': 'https://arxiv.org/abs/2505.07078', 'abstract': 'Large Language Models (LLMs) have recently been leveraged for asset pricing tasks and stock trading applications, enabling AI agents to generate investment decisions from unstructured financial data. However, most evaluations of LLM timing-based investing strategies are conducted on narrow timeframes and limited stock universes, overstating effectiveness due to survivorship and data-snooping biases. We critically assess their generalizability and robustness by proposing FINSABER, a backtesting framework evaluating timing-based strategies across longer periods and a larger universe of symbols. Systematic backtests over two decades and 100+ symbols reveal that previously reported LLM advantages deteriorate significantly under broader cross-section and over a longer-term evaluation. Our market regime analysis further demonstrates that LLM strategies are overly conservative in bull markets, underperforming passive benchmarks, and overly aggressive in bear markets, incurring heavy losses. These findings highlight the need to develop LLM strategies that are able to prioritise trend detection and regime-aware risk controls over mere scaling of framework complexity.', 'abstract_zh': '大规模语言模型（LLMs） recently has been利用于资产定价任务和股票交易应用，使AI代理能够从非结构化的财务数据中生成投资决策。然而，大多数基于时间的大规模语言模型投资策略评估都在较窄的时间框架和有限的股票 universes 中进行，这夸大了其效果，原因在于生存偏差和数据淘金偏见。我们通过提出FINSABER回测框架，系统地评估这些策略在更长时期和更大股票 universes 中的一般化能力和稳健性。二十年和100多种股票的系统回测揭示，之前报告的大规模语言模型的优势在更广泛的横截面和更长时间的评估下大幅减弱。进一步的市场环境分析表明，大规模语言模型策略在牛市中过于保守，表现不及被动基准，在熊市中则过于激进，造成重大损失。这些发现强调了开发能够优先考虑趋势检测和环境感知风险控制的大规模语言模型策略的重要性，而不仅仅是提升框架复杂性。', 'title_zh': '基于LLM的金融投资策略能否在长期内超越市场？'}
{'arxiv_id': 'arXiv:2505.07062', 'title': 'Seed1.5-VL Technical Report', 'authors': 'Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, Jingji Chen, Jingjia Huang, Kang Lei, Liping Yuan, Lishu Luo, Pengfei Liu, Qinghao Ye, Rui Qian, Shen Yan, Shixiong Zhao, Shuai Peng, Shuangye Li, Sihang Yuan, Sijin Wu, Tianheng Cheng, Weiwei Liu, Wenqian Wang, Xianhan Zeng, Xiao Liu, Xiaobo Qin, Xiaohan Ding, Xiaojun Xiao, Xiaoying Zhang, Xuanwei Zhang, Xuehan Xiong, Yanghua Peng, Yangrui Chen, Yanwei Li, Yanxu Hu, Yi Lin, Yiyuan Hu, Yiyuan Zhang, Youbin Wu, Yu Li, Yudong Liu, Yue Ling, Yujia Qin, Zanbo Wang, Zhiwu He, Aoxue Zhang, Bairen Yi, Bencheng Liao, Can Huang, Can Zhang, Chaorui Deng, Chaoyi Deng, Cheng Lin, Cheng Yuan, Chenggang Li, Chenhui Gou, Chenwei Lou, Chengzhi Wei, Chundian Liu, Chunyuan Li, Deyao Zhu, Donghong Zhong, Feng Li, Feng Zhang, Gang Wu, Guodong Li, Guohong Xiao, Haibin Lin, Haihua Yang, Haoming Wang, Heng Ji, Hongxiang Hao, Hui Shen, Huixia Li, Jiahao Li, Jialong Wu, Jianhua Zhu, Jianpeng Jiao, Jiashi Feng, Jiaze Chen, Jianhui Duan, Jihao Liu, Jin Zeng, Jingqun Tang, Jingyu Sun, Joya Chen, Jun Long, Junda Feng, Junfeng Zhan, Junjie Fang, Junting Lu, Kai Hua, Kai Liu, Kai Shen, Kaiyuan Zhang, Ke Shen', 'link': 'https://arxiv.org/abs/2505.07062', 'abstract': 'We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at this https URL (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)', 'abstract_zh': '我们介绍Seed1.5-VL，一个设计用于推进通用多模态理解与推理的视觉-语言基础模型。Seed1.5-VL由一个包含532M参数的视觉编码器和一个具有20B激活参数的专家混排（MoE）大型语言模型组成。尽管其架构相对紧凑，但在广泛公共VLM基准测试和内部评估套件中均展示了卓越的性能，共在60个公共基准中的38个上达到最佳性能。此外，在以代理为中心的任务，如GUI控制和游戏玩法中，Seed1.5-VL也优于包括OpenAI CUA和Claude 3.7在内的其他多模态系统。超越视觉和视频理解，它还展示了强大的推理能力，对于多模态推理挑战如视觉谜题尤其有效。我们相信这些能力将促进跨多种任务的应用。在本报告中，我们主要提供在模型设计、数据构建和各个阶段训练过程中的全面经验回顾，希望这份报告能够激励进一步的研究。Seed1.5-VL现在可通过以下链接访问：https://volcanoengine.com/model/doubao-1-5-thinking-vision-pro-250428。', 'title_zh': 'Seed1.5-VL 技术报告'}
{'arxiv_id': 'arXiv:2505.06987', 'title': 'Convert Language Model into a Value-based Strategic Planner', 'authors': 'Xiaoyu Wang, Yue Zhao, Qingqing Gu, Zhonglin Jiang, Xiaokai Chen, Yong Chen, Luo Ji', 'link': 'https://arxiv.org/abs/2505.06987', 'abstract': 'Emotional support conversation (ESC) aims to alleviate the emotional distress of individuals through effective conversations. Although large language models (LLMs) have obtained remarkable progress on ESC, most of these studies might not define the diagram from the state model perspective, therefore providing a suboptimal solution for long-term satisfaction. To address such an issue, we leverage the Q-learning on LLMs, and propose a framework called straQ*. Our framework allows a plug-and-play LLM to bootstrap the planning during ESC, determine the optimal strategy based on long-term returns, and finally guide the LLM to response. Substantial experiments on ESC datasets suggest that straQ* outperforms many baselines, including direct inference, self-refine, chain of thought, finetuning, and finite state machines.', 'abstract_zh': '情绪支持对话（ESC）旨在通过有效的对话缓解个体的情绪困扰。尽管大型语言模型（LLMs）在ESC方面取得了显著进展，但大多数研究可能并未从状态模型的角度定义该过程，因此可能无法提供长期满意的解决方案。为解决这一问题，我们利用Q-learning技术，提出了一个名为straQ*的框架。该框架允许插拔式的LLMs在ESC过程中进行规划，基于长期回报确定最优策略，并最终指导LLM进行响应。在ESC数据集上的大量实验表明，straQ*在多种基线方法（包括直接推理、自我完善、逻辑推理、微调和有限状态机）中表现更优。', 'title_zh': '将语言模型转换为价值为基础的战略规划器'}
{'arxiv_id': 'arXiv:2505.06913', 'title': 'RedTeamLLM: an Agentic AI framework for offensive security', 'authors': 'Brian Challita, Pierre Parrend', 'link': 'https://arxiv.org/abs/2505.06913', 'abstract': 'From automated intrusion testing to discovery of zero-day attacks before software launch, agentic AI calls for great promises in security engineering. This strong capability is bound with a similar threat: the security and research community must build up its models before the approach is leveraged by malicious actors for cybercrime. We therefore propose and evaluate RedTeamLLM, an integrated architecture with a comprehensive security model for automatization of pentest tasks. RedTeamLLM follows three key steps: summarizing, reasoning and act, which embed its operational capacity. This novel framework addresses four open challenges: plan correction, memory management, context window constraint, and generality vs. specialization. Evaluation is performed through the automated resolution of a range of entry-level, but not trivial, CTF challenges. The contribution of the reasoning capability of our agentic AI framework is specifically evaluated.', 'abstract_zh': '从自动化入侵测试到软件发布前发现零日攻击，自主人工智能在安全工程领域寄予厚望。这一强大能力伴随着相似的威胁：安全与研究领域必须在该方法被恶意行为者用于网络犯罪之前建立健全其模型。因此，我们提出并评估了RedTeamLLM，这是一种集成架构，具有全面的安全模型以自动化渗透测试任务。RedTeamLLM 遵循三个关键步骤：总结、推理和执行，这嵌入了其操作能力。该新颖框架解决了四个开放挑战：计划修正、内存管理、上下文窗口约束以及通用性与专门化之间的权衡。通过自动化解决一系列入门级但不简单的CTF挑战来对其实验评估。特别评估了我们自主人工智能框架的推理能力贡献。', 'title_zh': 'RedTeamLLM: 一种用于进攻性安全的自主AI框架'}
{'arxiv_id': 'arXiv:2505.06889', 'title': 'IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method', 'authors': 'Mihyeon Kim, Juhyoung Park, Youngbin Kim', 'link': 'https://arxiv.org/abs/2505.06889', 'abstract': "Pre-trained Language Models (PLMs) have achieved remarkable performance on diverse NLP tasks through pre-training and fine-tuning. However, fine-tuning the model with a large number of parameters on limited downstream datasets often leads to vulnerability to adversarial attacks, causing overfitting of the model on standard datasets.\nTo address these issues, we propose IM-BERT from the perspective of a dynamic system by conceptualizing a layer of BERT as a solution of Ordinary Differential Equations (ODEs). Under the situation of initial value perturbation, we analyze the numerical stability of two main numerical ODE solvers: the explicit and implicit Euler approaches.\nBased on these analyses, we introduce a numerically robust IM-connection incorporating BERT's layers. This strategy enhances the robustness of PLMs against adversarial attacks, even in low-resource scenarios, without introducing additional parameters or adversarial training strategies.\nExperimental results on the adversarial GLUE (AdvGLUE) dataset validate the robustness of IM-BERT under various conditions. Compared to the original BERT, IM-BERT exhibits a performance improvement of approximately 8.3\\%p on the AdvGLUE dataset. Furthermore, in low-resource scenarios, IM-BERT outperforms BERT by achieving 5.9\\%p higher accuracy.", 'abstract_zh': '预训练语言模型（PLMs）通过预训练和微调在多种NLP任务中取得了显著性能。然而，使用大量参数对下游数据集进行微调往往会导致模型对 adversarial 攻击的脆弱性，造成模型在标准数据集上的过拟合。\n为解决这些问题，我们从动态系统的角度提出了 IM-BERT，将其一层 BERT 视作常微分方程（ODEs）的解。在初始值扰动的情况下，我们分析了两种主要的数值 ODE 解算器——显式和隐式欧拉方法的数值稳定性。\n基于这些分析，我们引入了一种数值稳健的 IM 连接，该策略增强了 PLMs 对 adversarial 攻击的鲁棒性，即使在低资源场景下，也不会引入额外参数或 adversarial 训练策略。\n在 adversarial GLUE（AdvGLUE）数据集上的实验结果验证了 IM-BERT 在各种条件下的鲁棒性。与原始 BERT 相比，IM-BERT 在 AdvGLUE 数据集上的性能提高了约 8.3%p。此外，在低资源场景中，IM-BERT 获得了 5.9%p 的更高准确率。', 'title_zh': 'IM-BERT：通过隐式欧拉方法增强BERT的鲁棒性'}
{'arxiv_id': 'arXiv:2505.06841', 'title': 'Optimizing Recommendations using Fine-Tuned LLMs', 'authors': 'Prabhdeep Cheema, Erhan Guven', 'link': 'https://arxiv.org/abs/2505.06841', 'abstract': "As digital media platforms strive to meet evolving user expectations, delivering highly personalized and intuitive movies and media recommendations has become essential for attracting and retaining audiences. Traditional systems often rely on keyword-based search and recommendation techniques, which limit users to specific keywords and a combination of keywords. This paper proposes an approach that generates synthetic datasets by modeling real-world user interactions, creating complex chat-style data reflective of diverse preferences. This allows users to express more information with complex preferences, such as mood, plot details, and thematic elements, in addition to conventional criteria like genre, title, and actor-based searches. In today's search space, users cannot write queries like ``Looking for a fantasy movie featuring dire wolves, ideally set in a harsh frozen world with themes of loyalty and survival.''\nBuilding on these contributions, we evaluate synthetic datasets for diversity and effectiveness in training and benchmarking models, particularly in areas often absent from traditional datasets. This approach enhances personalization and accuracy by enabling expressive and natural user queries. It establishes a foundation for the next generation of conversational AI-driven search and recommendation systems in digital entertainment.", 'abstract_zh': '随着数字媒体平台努力满足不断演变的用户期望，提供高度个性化和直观的内容推荐已成为吸引和保留观众的关键。传统系统通常依赖于基于关键词的搜索和推荐技术，这限制了用户只能使用特定的关键词及其组合。本文提出了一种方法，通过模拟真实世界的用户交互来生成合成数据集，创建反映多样化偏好的复杂聊天风格数据。这使用户能够通过复杂偏好表达更多信息，包括情绪、情节细节和主题元素，而不仅仅是传统的类别搜索，如类型的、标题的和演员的搜索。在当今的搜索空间中，用户不能编写类似于“寻找一部特色为Direwolves的奇幻电影，理想情况下设定在一个严酷的冰冻世界中，主题涉及忠诚和生存”的查询。\n基于这些贡献，我们评估合成数据集在多样性和有效性方面的表现，特别是在传统数据集中经常缺乏的领域，用以训练和基准测试模型。这种方法通过启用富有表现力和自然的用户查询来增强个性化和准确性。它为下一代基于对话AI的搜索和推荐系统在数字娱乐中的应用奠定了基础。', 'title_zh': '使用微调后的大语言模型优化推荐系统'}
{'arxiv_id': 'arXiv:2505.06821', 'title': 'ThreatLens: LLM-guided Threat Modeling and Test Plan Generation for Hardware Security Verification', 'authors': 'Dipayan Saha, Hasan Al Shaikh, Shams Tarek, Farimah Farahmandi', 'link': 'https://arxiv.org/abs/2505.06821', 'abstract': 'Current hardware security verification processes predominantly rely on manual threat modeling and test plan generation, which are labor-intensive, error-prone, and struggle to scale with increasing design complexity and evolving attack methodologies. To address these challenges, we propose ThreatLens, an LLM-driven multi-agent framework that automates security threat modeling and test plan generation for hardware security verification. ThreatLens integrates retrieval-augmented generation (RAG) to extract relevant security knowledge, LLM-powered reasoning for threat assessment, and interactive user feedback to ensure the generation of practical test plans. By automating these processes, the framework reduces the manual verification effort, enhances coverage, and ensures a structured, adaptable approach to security verification. We evaluated our framework on the NEORV32 SoC, demonstrating its capability to automate security verification through structured test plans and validating its effectiveness in real-world scenarios.', 'abstract_zh': '当前的硬件安全验证过程主要依赖于手工威胁建模和测试计划生成，这既费时又容易出错，并且难以适应日益复杂的设计和不断演变的攻击方法。为应对这些挑战，我们提出了一种基于LLM的多agent框架——ThreatLens，该框架自动进行硬件安全验证中的威胁建模和测试计划生成。ThreatLens通过检索增强生成（RAG）提取相关安全知识，利用LLM进行威胁评估，并结合交互式用户反馈来确保生成的测试计划具有实际操作性。通过自动化这些过程，该框架减少了手工验证的工作量，提升了覆盖率，并确保了一种结构化且适应性强的安全验证方法。我们通过对NEORV32 SoC的评估，展示了该框架通过结构化测试计划实现自动化安全验证的能力，并验证了其在实际应用中的有效性。', 'title_zh': 'ThreatLens: LLM引导的硬件安全验证中的威胁建模与测试计划生成'}
{'arxiv_id': 'arXiv:2505.06652', 'title': 'Enfoque Odychess: Un método dialéctico, constructivista y adaptativo para la enseñanza del ajedrez con inteligencias artificiales generativas', 'authors': 'Ernesto Giralt Hernandez, Lazaro Antonio Bueno Perez', 'link': 'https://arxiv.org/abs/2505.06652', 'abstract': 'Chess teaching has evolved through different approaches, however, traditional methodologies, often based on memorization, contrast with the new possibilities offered by generative artificial intelligence, a technology still little explored in this field. This study seeks to empirically validate the effectiveness of the Odychess Approach in improving chess knowledge, strategic understanding, and metacognitive skills in students. A quasi-experimental study was conducted with a pre-test/post-test design and a control group (N=60). The experimental intervention implemented the Odychess Approach, incorporating a Llama 3.3 language model that was specifically adapted using Parameter-Efficient Fine-Tuning (PEFT) techniques to act as a Socratic chess tutor. Quantitative assessment instruments were used to measure chess knowledge, strategic understanding, and metacognitive skills before and after the intervention. The results of the quasi-experimental study showed significant improvements in the experimental group compared to the control group in the three variables analyzed: chess knowledge, strategic understanding, and metacognitive skills. The complementary qualitative analysis revealed greater analytical depth, more developed dialectical reasoning, and increased intrinsic motivation in students who participated in the Odychess method-based intervention. The Odychess Approach represents an effective pedagogical methodology for teaching chess, demonstrating the potential of the synergistic integration of constructivist and dialectical principles with generative artificial intelligence. The implications of this work are relevant for educators and institutions interested in adopting innovative pedagogical technologies and for researchers in the field of AI applied to education, highlighting the transferability of the language model adaptation methodology to other educational domains.', 'abstract_zh': '棋类教学通过不同的方法演化，然而，传统的基于记忆的方法与生成式人工智能技术提供的新可能性形成对比，而该技术在这一领域仍较少被探索。本研究旨在通过定量实验证实Odychess方法在提高学生棋类知识、战略理解以及元认知技能方面的有效性。采用准实验设计，包括预测试/后测试以及对照组（N=60），实验组实施了结合Parameter-Efficient Fine-Tuning (PEFT) 技术适配的Llama 3.3语言模型，以此作为苏格拉底式棋类导师进行干预。使用定量评估工具在干预前和干预后分别测量棋类知识、战略理解以及元认知技能。准实验研究的结果显示，与对照组相比，实验组在三个分析变量上均表现出显著改善：棋类知识、战略理解以及元认知技能。补充的定性分析表明，采用Odychess方法为基础的干预措施的学生表现出更深层次的分析能力、更发达的辩证推理能力和更高的内在动机。Odychess方法代表了一种有效的教学方法，展示了结合建构主义和辩证原则与生成式人工智能的协同集成的潜力。该研究对关注采用创新教学技术的教育工作者和机构以及研究人工智能在教育应用领域的研究人员具有重要意义，强调了语言模型适配方法在其他教育领域中的可转移性。', 'title_zh': 'Odychess 方法：一种辩证、建构主义和适应性的象棋教学方法，涉及生成性人工智能'}
{'arxiv_id': 'arXiv:2505.06569', 'title': 'MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG', 'authors': 'Woosang Lim, Zekun Li, Gyuwan Kim, Sungyoung Ji, HyeonJung Kim, Kyuri Choi, Jin Hyuk Lim, Kyungpyo Park, William Yang Wang', 'link': 'https://arxiv.org/abs/2505.06569', 'abstract': 'Long-context (LC) Large Language Models (LLMs) combined with Retrieval-Augmented Generation (RAG) hold strong potential for complex multi-hop and large-document tasks. However, existing RAG systems often suffer from imprecise retrieval, incomplete context coverage under constrained context windows, and fragmented information caused by suboptimal context construction. We introduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical retrieval framework that compresses and partitions documents into coarse-to-fine granularities, then adaptively merges relevant contexts through chunk- and document-level expansions in real time. By starting from the finest-level retrieval and progressively incorporating higher-level and broader context, MacRAG constructs effective query-specific long contexts, optimizing both precision and coverage. Evaluations on the challenging LongBench expansions of HotpotQA, 2WikiMultihopQA, and Musique confirm that MacRAG consistently surpasses baseline RAG pipelines on single- and multi-step generation with Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o. Our results establish MacRAG as an efficient, scalable solution for real-world long-context, multi-hop reasoning. Our code is available at this https URL.', 'abstract_zh': '多尺度自适应上下文检索增强生成（MacRAG）：一种高效的复杂多跳推理解决方案', 'title_zh': 'MacRAG: 压缩、切片和扩展以实现多尺度自适应上下文检索增强生成'}
{'arxiv_id': 'arXiv:2505.06493', 'title': 'System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection', 'authors': 'Jiawei Guo, Haipeng Cai', 'link': 'https://arxiv.org/abs/2505.06493', 'abstract': 'Large language models (LLMs) have gained widespread adoption across diverse applications due to their impressive generative capabilities. Their plug-and-play nature enables both developers and end users to interact with these models through simple prompts. However, as LLMs become more integrated into various systems in diverse domains, concerns around their security are growing. Existing studies mainly focus on threats arising from user prompts (e.g. prompt injection attack) and model output (e.g. model inversion attack), while the security of system prompts remains largely overlooked. This work bridges the critical gap. We introduce system prompt poisoning, a new attack vector against LLMs that, unlike traditional user prompt injection, poisons system prompts hence persistently impacts all subsequent user interactions and model responses. We systematically investigate four practical attack strategies in various poisoning scenarios. Through demonstration on both generative and reasoning LLMs, we show that system prompt poisoning is highly feasible without requiring jailbreak techniques, and effective across a wide range of tasks, including those in mathematics, coding, logical reasoning, and natural language processing. Importantly, our findings reveal that the attack remains effective even when user prompts employ advanced prompting techniques like chain-of-thought (CoT). We also show that such techniques, including CoT and retrieval-augmentation-generation (RAG), which are proven to be effective for improving LLM performance in a wide range of tasks, are significantly weakened in their effectiveness by system prompt poisoning.', 'abstract_zh': '大型语言模型（LLMs）因其强大的生成能力在各种应用中得到了广泛应用。它们的即插即用特性使得开发者和最终用户可以通过简单的提示与这些模型进行交互。然而，随着LLMs在各个领域中越来越多地集成到各种系统中，对其安全性的担忧也在增长。现有研究主要关注来自用户提示（如提示注入攻击）和模型输出（如模型反向工程攻击）的威胁，而系统提示的安全性则被很大程度上忽视。本工作填补了这一关键空白。我们介绍了系统提示投毒这一新的攻击向量，与传统的用户提示注入不同，系统提示投毒攻击旨在持久影响所有后续的用户交互和模型响应。我们系统地研究了四种实用的攻击策略在各种投毒场景下的表现。通过在生成和推理两种大型语言模型上的演示，我们表明，系统提示投毒在无需使用越狱技术的情况下是高度可行的，并且在广泛的任务中具有有效性，包括数学、编程、逻辑推理和自然语言处理任务。重要的是，我们的结果揭示出，即使用户提示使用了链式思考（CoT）等高级提示技术，攻击仍然有效。我们还展示了这些技术，包括CoT和检索增强生成（RAG），在许多任务中已被证明能够显著提高大型语言模型的性能，但通过系统提示投毒却显著削弱了其有效性。', 'title_zh': '系统提示中毒：超出用户注入的大语言模型持续攻击'}
{'arxiv_id': 'arXiv:2505.06413', 'title': 'Natural Reflection Backdoor Attack on Vision Language Model for Autonomous Driving', 'authors': 'Ming Liu, Siyuan Liang, Koushik Howlader, Liwen Wang, Dacheng Tao, Wensheng Zhang', 'link': 'https://arxiv.org/abs/2505.06413', 'abstract': 'Vision-Language Models (VLMs) have been integrated into autonomous driving systems to enhance reasoning capabilities through tasks such as Visual Question Answering (VQA). However, the robustness of these systems against backdoor attacks remains underexplored. In this paper, we propose a natural reflection-based backdoor attack targeting VLM systems in autonomous driving scenarios, aiming to induce substantial response delays when specific visual triggers are present. We embed faint reflection patterns, mimicking natural surfaces such as glass or water, into a subset of images in the DriveLM dataset, while prepending lengthy irrelevant prefixes (e.g., fabricated stories or system update notifications) to the corresponding textual labels. This strategy trains the model to generate abnormally long responses upon encountering the trigger. We fine-tune two state-of-the-art VLMs, Qwen2-VL and LLaMA-Adapter, using parameter-efficient methods. Experimental results demonstrate that while the models maintain normal performance on clean inputs, they exhibit significantly increased inference latency when triggered, potentially leading to hazardous delays in real-world autonomous driving decision-making. Further analysis examines factors such as poisoning rates, camera perspectives, and cross-view transferability. Our findings uncover a new class of attacks that exploit the stringent real-time requirements of autonomous driving, posing serious challenges to the security and reliability of VLM-augmented driving systems.', 'abstract_zh': '基于自然反射的视觉语言模型后门攻击：针对自动驾驶场景的响应延迟诱导', 'title_zh': '自然反射后门攻击对自动驾驶视觉语言模型的影响'}
{'arxiv_id': 'arXiv:2505.06394', 'title': 'Towards AI-Driven Human-Machine Co-Teaming for Adaptive and Agile Cyber Security Operation Centers', 'authors': 'Massimiliano Albanese, Xinming Ou, Kevin Lybarger, Daniel Lende, Dmitry Goldgof', 'link': 'https://arxiv.org/abs/2505.06394', 'abstract': 'Security Operations Centers (SOCs) face growing challenges in managing cybersecurity threats due to an overwhelming volume of alerts, a shortage of skilled analysts, and poorly integrated tools. Human-AI collaboration offers a promising path to augment the capabilities of SOC analysts while reducing their cognitive overload. To this end, we introduce an AI-driven human-machine co-teaming paradigm that leverages large language models (LLMs) to enhance threat intelligence, alert triage, and incident response workflows. We present a vision in which LLM-based AI agents learn from human analysts the tacit knowledge embedded in SOC operations, enabling the AI agents to improve their performance on SOC tasks through this co-teaming. We invite SOCs to collaborate with us to further develop this process and uncover replicable patterns where human-AI co-teaming yields measurable improvements in SOC productivity.', 'abstract_zh': '安全运营中心（SOC）面临日益严峻的网络安全威胁管理挑战，由于警报量巨大、熟练分析师短缺以及工具集成不良。人机协作为增强SOC分析师能力、减轻其认知负担提供了前景。为此，我们提出了一种基于AI的人机协同训练 paradigm，利用大型语言模型（LLMs）提升威胁情报、警报分诊和事件响应工作流程。我们提出了一种愿景：基于LLM的AI代理从SOC分析师那里学习隐含在SOC运营中的 tacit 知识，使AI代理能够通过这种人机协作提高其在SOC任务上的表现。我们邀请SOC与我们合作，进一步开发这一过程，并发现人机协同训练在提高SOC生产率方面可复制的模式。', 'title_zh': '面向人工智能驱动的人机协同操作的自适应敏捷网络安全运营中心'}
{'arxiv_id': 'arXiv:2505.06347', 'title': 'Quantum State Preparation via Large-Language-Model-Driven Evolution', 'authors': 'Qing-Hong Cao, Zong-Yue Hou, Ying-Ying Li, Xiaohui Liu, Zhuo-Yang Song, Liang-Qi Zhang, Shutao Zhang, Ke Zhao', 'link': 'https://arxiv.org/abs/2505.06347', 'abstract': 'We propose an automated framework for quantum circuit design by integrating large-language models (LLMs) with evolutionary optimization to overcome the rigidity, scalability limitations, and expert dependence of traditional ones in variational quantum algorithms. Our approach (FunSearch) autonomously discovers hardware-efficient ansätze with new features of scalability and system-size-independent number of variational parameters entirely from scratch. Demonstrations on the Ising and XY spin chains with n = 9 qubits yield circuits containing 4 parameters, achieving near-exact energy extrapolation across system sizes. Implementations on quantum hardware (Zuchongzhi chip) validate practicality, where two-qubit quantum gate noises can be effectively mitigated via zero-noise extrapolations for a spin chain system as large as 20 sites. This framework bridges algorithmic design and experimental constraints, complementing contemporary quantum architecture search frameworks to advance scalable quantum simulations.', 'abstract_zh': '一种将大型语言模型与进化优化集成的自动化量子电路设计框架：FunSearch方法及其在可扩展量子模拟中的应用', 'title_zh': '大型语言模型驱动的量子态准备方法'}
{'arxiv_id': 'arXiv:2505.06330', 'title': 'Prompting Large Language Models for Training-Free Non-Intrusive Load Monitoring', 'authors': 'Junyu Xue, Xudong Wang, Xiaoling He, Shicheng Liu, Yi Wang, Guoming Tang', 'link': 'https://arxiv.org/abs/2505.06330', 'abstract': 'Non-intrusive Load Monitoring (NILM) aims to disaggregate aggregate household electricity consumption into individual appliance usage, enabling more effective energy management. While deep learning has advanced NILM, it remains limited by its dependence on labeled data, restricted generalization, and lack of interpretability. In this paper, we introduce the first prompt-based NILM framework that leverages Large Language Models (LLMs) with in-context learning. We design and evaluate prompt strategies that integrate appliance features, timestamps and contextual information, as well as representative time-series examples, using the REDD dataset. With optimized prompts, LLMs achieve competitive state detection accuracy, reaching an average F1-score of 0.676 on unseen households, and demonstrate robust generalization without the need for fine-tuning. LLMs also enhance interpretability by providing clear, human-readable explanations for their predictions. Our results show that LLMs can reduce data requirements, improve adaptability, and provide transparent energy disaggregation in NILM applications.', 'abstract_zh': '基于提示的非侵入式负荷监测框架：利用大型语言模型实现高效能源管理', 'title_zh': 'prompting大型语言模型进行无需训练的非侵入式负荷监测'}
{'arxiv_id': 'arXiv:2505.06324', 'title': 'Document Attribution: Examining Citation Relationships using Large Language Models', 'authors': 'Vipula Rawte, Ryan A. Rossi, Franck Dernoncourt, Nedim Lipka', 'link': 'https://arxiv.org/abs/2505.06324', 'abstract': "As Large Language Models (LLMs) are increasingly applied to document-based tasks - such as document summarization, question answering, and information extraction - where user requirements focus on retrieving information from provided documents rather than relying on the model's parametric knowledge, ensuring the trustworthiness and interpretability of these systems has become a critical concern. A central approach to addressing this challenge is attribution, which involves tracing the generated outputs back to their source documents. However, since LLMs can produce inaccurate or imprecise responses, it is crucial to assess the reliability of these citations.\nTo tackle this, our work proposes two techniques. (1) A zero-shot approach that frames attribution as a straightforward textual entailment task. Our method using flan-ul2 demonstrates an improvement of 0.27% and 2.4% over the best baseline of ID and OOD sets of AttributionBench, respectively. (2) We also explore the role of the attention mechanism in enhancing the attribution process. Using a smaller LLM, flan-t5-small, the F1 scores outperform the baseline across almost all layers except layer 4 and layers 8 through 11.", 'abstract_zh': '大规模语言模型（LLMs）在文档任务中的应用与信任与可解释性的保障：一种新颖的归因方法及其关注点', 'title_zh': '文档归属性分析：使用大规模语言模型探究引用关系'}
{'arxiv_id': 'arXiv:2505.06321', 'title': 'Learn to Think: Bootstrapping LLM Reasoning Capability Through Graph Learning', 'authors': 'Hang Gao, Chenhao Zhang, Tie Wang, Junsuo Zhao, Fengge Wu, Changwen Zheng, Huaping Liu', 'link': 'https://arxiv.org/abs/2505.06321', 'abstract': 'Large Language Models (LLMs) have achieved remarkable success across various domains. However, they still face significant challenges, including high computational costs for training and limitations in solving complex reasoning problems. Although existing methods have extended the reasoning capabilities of LLMs through structured paradigms, these approaches often rely on task-specific prompts and predefined reasoning processes, which constrain their flexibility and generalizability. To address these limitations, we propose a novel framework that leverages graph learning to enable more flexible and adaptive reasoning capabilities for LLMs. Specifically, this approach models the reasoning process of a problem as a graph and employs LLM-based graph learning to guide the adaptive generation of each reasoning step. To further enhance the adaptability of the model, we introduce a Graph Neural Network (GNN) module to perform representation learning on the generated reasoning process, enabling real-time adjustments to both the model and the prompt. Experimental results demonstrate that this method significantly improves reasoning performance across multiple tasks without requiring additional training or task-specific prompt design. Code can be found in this https URL.', 'abstract_zh': '大型语言模型（LLMs）已在多个领域取得了显著成功。然而，它们仍然面临重大挑战，包括高昂的训练计算成本以及解决复杂推理问题的局限性。尽管现有的方法通过结构化范式扩展了LLMs的推理能力，但这些方法往往依赖于特定任务的提示和预定义的推理过程，这限制了它们的灵活性和泛化能力。为解决这些局限性，我们提出了一种新的框架，利用图学习来增强LLMs的更灵活和自适应的推理能力。具体而言，该方法将问题的推理过程建模为图，并利用基于LLM的图学习来指导每一步推理的自适应生成。为进一步增强模型的适应性，我们引入了一个图神经网络（GNN）模块，对生成的推理过程进行表示学习，从而能够实时调整模型和提示。实验结果表明，该方法在多个任务上显著提高了推理性能，无需额外训练或特定任务的提示设计。代码可在以下链接找到：https://...。', 'title_zh': '基于图学习提升大语言模型推理能力：学会思考'}
{'arxiv_id': 'arXiv:2505.06311', 'title': 'Defending against Indirect Prompt Injection by Instruction Detection', 'authors': 'Tongyu Wen, Chenglong Wang, Xiyuan Yang, Haoyu Tang, Yueqi Xie, Lingjuan Lyu, Zhicheng Dou, Fangzhao Wu', 'link': 'https://arxiv.org/abs/2505.06311', 'abstract': 'The integration of Large Language Models (LLMs) with external sources is becoming increasingly common, with Retrieval-Augmented Generation (RAG) being a prominent example. However, this integration introduces vulnerabilities of Indirect Prompt Injection (IPI) attacks, where hidden instructions embedded in external data can manipulate LLMs into executing unintended or harmful actions. We recognize that the success of IPI attacks fundamentally relies in the presence of instructions embedded within external content, which can alter the behavioral state of LLMs. Can effectively detecting such state changes help us defend against IPI attacks? In this paper, we propose a novel approach that takes external data as input and leverages the behavioral state of LLMs during both forward and backward propagation to detect potential IPI attacks. Specifically, we demonstrate that the hidden states and gradients from intermediate layers provide highly discriminative features for instruction detection. By effectively combining these features, our approach achieves a detection accuracy of 99.60\\% in the in-domain setting and 96.90\\% in the out-of-domain setting, while reducing the attack success rate to just 0.12\\% on the BIPIA benchmark.', 'abstract_zh': '大型语言模型与外部数据源的集成变得越来越普遍，检索增强生成（RAG）是其中的一个典型例子。然而，这种集成引入了间接提示注入（IPI）攻击的安全漏洞，隐藏在外部数据中的指令可以操控大型语言模型执行未预期或有害的操作。我们认识到，IPI攻击的成功与否从根本上依赖于存在于外部内容中的隐蔽指令，这些指令可以改变大型语言模型的行为状态。有效检测这种状态变化是否能帮助我们抵御IPI攻击？在本文中，我们提出了一种新的方法，将外部数据作为输入，并利用大型语言模型在前向和反向传播过程中的行为状态，来检测潜在的IPI攻击。具体而言，我们证明了中间层的隐藏状态和梯度提供了一种高度区分性的特征，用于指令检测。通过有效结合这些特征，我们的方法在领域内设置实现了99.60%的检测准确率，在领域外设置实现了96.90%的检测准确率，并将BIPIA基准测试中的攻击成功率降低到仅0.12%。', 'title_zh': '防御间接提示注入攻击：指令检测方法'}
{'arxiv_id': 'arXiv:2505.06307', 'title': 'Large Language Model-driven Security Assistant for Internet of Things via Chain-of-Thought', 'authors': 'Mingfei Zeng, Ming Xie, Xixi Zheng, Chunhai Li, Chuan Zhang, Liehuang Zhu', 'link': 'https://arxiv.org/abs/2505.06307', 'abstract': "The rapid development of Internet of Things (IoT) technology has transformed people's way of life and has a profound impact on both production and daily activities. However, with the rapid advancement of IoT technology, the security of IoT devices has become an unavoidable issue in both research and applications. Although some efforts have been made to detect or mitigate IoT security vulnerabilities, they often struggle to adapt to the complexity of IoT environments, especially when dealing with dynamic security scenarios. How to automatically, efficiently, and accurately understand these vulnerabilities remains a challenge. To address this, we propose an IoT security assistant driven by Large Language Model (LLM), which enhances the LLM's understanding of IoT security vulnerabilities and related threats. The aim of the ICoT method we propose is to enable the LLM to understand security issues by breaking down the various dimensions of security vulnerabilities and generating responses tailored to the user's specific needs and expertise level. By incorporating ICoT, LLM can gradually analyze and reason through complex security scenarios, resulting in more accurate, in-depth, and personalized security recommendations and solutions. Experimental results show that, compared to methods relying solely on LLM, our proposed LLM-driven IoT security assistant significantly improves the understanding of IoT security issues through the ICoT approach and provides personalized solutions based on the user's identity, demonstrating higher accuracy and reliability.", 'abstract_zh': '物联网技术的快速发展改变了人们的生活方式，并对生产和日常活动产生了深远影响。然而，随着物联网技术的快速进步，物联网设备的安全性已经成为研究和应用中无法回避的问题，尤其是在处理动态安全场景时更为明显。如何自动、高效、准确地理解和应对这些安全漏洞仍然是一项挑战。为此，我们提出了由大规模语言模型（Large Language Model，LLM）驱动的物联网安全助手，以增强LLM对物联网安全漏洞及其相关威胁的理解。我们所提出的ICoT方法旨在通过分解安全漏洞的各种维度并生成针对用户具体需求和专业水平的响应，使LLM能够理解安全问题。通过整合ICoT方法，LLM可以逐步分析和推理复杂的安全场景，从而提供更准确、深入且个性化的安全建议和解决方案。实验结果表明，与仅依赖于LLM的方法相比，我们提出的大规模语言模型驱动的物联网安全助手通过ICoT方法显著提高了对物联网安全问题的理解，并基于用户的身份提供个性化的解决方案，显示出更高的准确性和可靠性。', 'title_zh': '大型语言模型驱动的物联网安全助手通过思维链'}
{'arxiv_id': 'arXiv:2505.06305', 'title': 'User Behavior Analysis in Privacy Protection with Large Language Models: A Study on Privacy Preferences with Limited Data', 'authors': 'Haowei Yang, Qingyi Lu, Yang Wang, Sibei Liu, Jiayun Zheng, Ao Xiang', 'link': 'https://arxiv.org/abs/2505.06305', 'abstract': 'With the widespread application of large language models (LLMs), user privacy protection has become a significant research topic. Existing privacy preference modeling methods often rely on large-scale user data, making effective privacy preference analysis challenging in data-limited environments. This study explores how LLMs can analyze user behavior related to privacy protection in scenarios with limited data and proposes a method that integrates Few-shot Learning and Privacy Computing to model user privacy preferences. The research utilizes anonymized user privacy settings data, survey responses, and simulated data, comparing the performance of traditional modeling approaches with LLM-based methods. Experimental results demonstrate that, even with limited data, LLMs significantly improve the accuracy of privacy preference modeling. Additionally, incorporating Differential Privacy and Federated Learning further reduces the risk of user data exposure. The findings provide new insights into the application of LLMs in privacy protection and offer theoretical support for advancing privacy computing and user behavior analysis.', 'abstract_zh': '随着大规模语言模型（LLMs）的广泛应用，用户隐私保护已成为一个重要研究课题。现有的隐私偏好建模方法往往依赖大量用户数据，这在数据受限环境中使得有效的隐私偏好分析变得具有挑战性。本研究探索了在数据有限的情景下，LLM 如何分析与隐私保护相关的用户行为，并提出了一种结合少样本学习和隐私计算的用户隐私偏好建模方法。研究利用匿名化的用户隐私设置数据、调查响应和模拟数据，比较了传统建模方法与基于LLM的方法的性能。实验结果表明，即使在数据有限的情况下，LLM 也显著提高了隐私偏好建模的准确性。此外，结合差分隐私和联邦学习进一步降低了用户数据暴露的风险。研究结果为 LLM 在隐私保护中的应用提供了新的见解，并为推进隐私计算和用户行为分析提供了理论支持。', 'title_zh': '使用大规模语言模型保护隐私中的用户行为分析：基于有限数据的隐私偏好研究'}
{'arxiv_id': 'arXiv:2505.06302', 'title': 'QiMeng-TensorOp: Automatically Generating High-Performance Tensor Operators with Hardware Primitives', 'authors': 'Xuzhi Zhang, Shaohui Peng, Qirui Zhou, Yuanbo Wen, Qi Guo, Ruizhi Chen, Xinguo Zhu, Weiqiang Xiong, Haixin Chen, Congying Ma, Ke Gao, Chen Zhao, Yanjun Wu, Yunji Chen, Ling Li', 'link': 'https://arxiv.org/abs/2505.06302', 'abstract': 'Computation-intensive tensor operators constitute over 90\\% of the computations in Large Language Models (LLMs) and Deep Neural this http URL and efficiently generating high-performance tensor operators with hardware primitives is crucial for diverse and ever-evolving hardware architectures like RISC-V, ARM, and GPUs, as manually optimized implementation takes at least months and lacks this http URL excel at generating high-level language codes, but they struggle to fully comprehend hardware characteristics and produce high-performance tensor operators. We introduce a tensor-operator auto-generation framework with a one-line user prompt (QiMeng-TensorOp), which enables LLMs to automatically exploit hardware characteristics to generate tensor operators with hardware primitives, and tune parameters for optimal performance across diverse hardware. Experimental results on various hardware platforms, SOTA LLMs, and typical tensor operators demonstrate that QiMeng-TensorOp effectively unleashes the computing capability of various hardware platforms, and automatically generates tensor operators of superior performance. Compared with vanilla LLMs, QiMeng-TensorOp achieves up to $1291 \\times$ performance improvement. Even compared with human experts, QiMeng-TensorOp could reach $251 \\%$ of OpenBLAS on RISC-V CPUs, and $124 \\%$ of cuBLAS on NVIDIA GPUs. Additionally, QiMeng-TensorOp also significantly reduces development costs by $200 \\times$ compared with human experts.', 'abstract_zh': '计算密集型张量操作构成了大规模语言模型（LLMs）和深度神经网络中超过90%的计算量，高效地使用硬件 primitives 生成高性能张量操作对于如RISC-V、ARM和GPU等多样和不断演进的硬件架构至关重要。由于手工优化实现需要至少几个月的时间且缺乏一致性，虽然现有的LLMs在生成高级语言代码方面表现出色，但它们在完全理解硬件特性并产生高性能张量操作方面存在困难。我们引入了一种带有单一用户提示（QiMeng-TensorOp）的张量操作自动生成框架，使LLMs能够自动利用硬件特性，使用硬件 primitives 生成张量操作，并针对多种硬件进行参数调整以实现最优性能。实验结果表明，QiMeng-TensorOp 有效地释放了各种硬件平台的计算能力，并自动生成了高性能的张量操作。与 vanilla LLMs 相比，QiMeng-TensorOp 实现了高达1291倍的性能改进。即使与人类专家相比，QiMeng-TensorOp 在RISC-V CPU上的性能也可达OpenBLAS的251%，在NVIDIA GPU上的性能则可达cuBLAS的124%。此外，与人类专家相比，QiMeng-TensorOp 还将开发成本降低了200倍。', 'title_zh': 'QiMeng-TensorOp: 通过硬件 primitives 自动生成高性能张量操作符'}
{'arxiv_id': 'arXiv:2505.06274', 'title': 'PARM: Multi-Objective Test-Time Alignment via Preference-Aware Autoregressive Reward Model', 'authors': 'Baijiong Lin, Weisen Jiang, Yuancheng Xu, Hao Chen, Ying-Cong Chen', 'link': 'https://arxiv.org/abs/2505.06274', 'abstract': 'Multi-objective test-time alignment aims to adapt large language models (LLMs) to diverse multi-dimensional user preferences during inference while keeping LLMs frozen. Recently, GenARM (Xu et al., 2025) first independently trains Autoregressive Reward Models (ARMs) for each preference dimension without awareness of each other, then combines their outputs based on user-specific preference vectors during inference to achieve multi-objective test-time alignment, leading to two key limitations: the need for \\textit{multiple} ARMs increases the inference cost, and the separate training of ARMs causes the misalignment between the guided generation and the user preferences. To address these issues, we propose Preference-aware ARM (PARM), a single unified ARM trained across all preference dimensions. PARM uses our proposed Preference-Aware Bilinear Low-Rank Adaptation (PBLoRA), which employs a bilinear form to condition the ARM on preference vectors, enabling it to achieve precise control over preference trade-offs during inference. Experiments demonstrate that PARM reduces inference costs and achieves better alignment with preference vectors compared with existing methods. Additionally, PARM enables weak-to-strong guidance, allowing a smaller PARM to guide a larger frozen LLM without expensive training, making multi-objective alignment accessible with limited computing resources. The code is available at this https URL.', 'abstract_zh': '多目标测试时对齐旨在适应大规模语言模型（LLMs）在推理过程中多元的多维度用户偏好，同时保持LLMs冻结。近期，GenARM（Xu等，2025）率先独立训练每个偏好维度的自回归奖励模型（ARMs），而彼此之间无意识关系，然后在推理过程中基于用户的特定偏好向量结合它们的输出以实现多目标测试时对齐，导致两个关键限制：需要多个ARM增加了推理成本，独立训练ARM导致导向生成与用户偏好之间的错位。为解决这些问题，我们提出了一种偏好意识ARM（PARM），这是一种在所有偏好维度上统一训练的ARM。PARM使用我们提出的偏好意识双线性低秩适应（PBLoRA），该方法采用双线性形式在ARM上条件化偏好向量，使其能够在推理过程中实现精确的偏好权衡控制。实验表明，PARM减少了推理成本并比现有方法更好地与偏好向量对齐。此外，PARM支持从弱到强的指导，允许较小的PARM在不进行昂贵训练的情况下引导较大的冻结LLM，从而在有限计算资源下实现多目标对齐。代码可在以下链接获取：this https URL。', 'title_zh': 'PARM：基于偏好意识自回归奖励模型的多目标测试时对齐方法'}
{'arxiv_id': 'arXiv:2505.06267', 'title': 'AKD : Adversarial Knowledge Distillation For Large Language Models Alignment on Coding tasks', 'authors': 'Ilyas Oulkadda, Julien Perez', 'link': 'https://arxiv.org/abs/2505.06267', 'abstract': 'The widespread adoption of Large Language Models (LLMs) for code generation, exemplified by GitHub Copilot\\footnote{A coding extension powered by a Code-LLM to assist in code completion tasks} surpassing a million users, highlights the transformative potential of these tools in improving developer productivity. However, this rapid growth also underscores critical concerns regarding the quality, safety, and reliability of the code they generate. As Code-LLMs evolve, they face significant challenges, including the diminishing returns of model scaling and the scarcity of new, high-quality training data. To address these issues, this paper introduces Adversarial Knowledge Distillation (AKD), a novel approach that leverages adversarially generated synthetic datasets to distill the capabilities of larger models into smaller, more efficient ones. By systematically stress-testing and refining the reasoning capabilities of Code-LLMs, AKD provides a framework for enhancing model robustness, reliability, and security while improving their parameter-efficiency. We believe this work represents a critical step toward ensuring dependable automated code generation within the constraints of existing data and the cost-efficiency of model execution.', 'abstract_zh': '大型语言模型在代码生成中的广泛采用，以GitHub Copilot为例，用户数超过百万，凸显了这些工具在提升开发者生产力方面的变革潜力。然而，这种快速增长也引起了对其生成的代码质量、安全性和可靠性的关键关注。随着代码LLM的发展，它们面临着模型扩展回报递减和高质量训练数据稀缺的重大挑战。为了解决这些问题，本文提出了对抗性知识蒸馏（AKD）这一新颖方法，利用对抗生成的合成数据集将大模型的能力提炼至更小、更高效的模型中。通过系统性地压力测试和优化代码LLM的推理能力，AKD提供了一种框架，用于增强模型的鲁棒性、可靠性和安全性，同时提高其参数效率。我们认为，这项工作代表了确保在现有数据约束和模型执行成本效益下的可靠自动化代码生成的一个关键步骤。', 'title_zh': 'AKD：面向编码任务的大语言模型对抗知识精炼iếc\nuser\nTransformer模型的工作原理及在自然语言处理任务中的应用。'}
{'arxiv_id': 'arXiv:2505.06262', 'title': 'Dialz: A Python Toolkit for Steering Vectors', 'authors': 'Zara Siddique, Liam D. Turner, Luis Espinosa-Anke', 'link': 'https://arxiv.org/abs/2505.06262', 'abstract': "We introduce Dialz, a framework for advancing research on steering vectors for open-source LLMs, implemented in Python. Steering vectors allow users to modify activations at inference time to amplify or weaken a 'concept', e.g. honesty or positivity, providing a more powerful alternative to prompting or fine-tuning. Dialz supports a diverse set of tasks, including creating contrastive pair datasets, computing and applying steering vectors, and visualizations. Unlike existing libraries, Dialz emphasizes modularity and usability, enabling both rapid prototyping and in-depth analysis. We demonstrate how Dialz can be used to reduce harmful outputs such as stereotypes, while also providing insights into model behaviour across different layers. We release Dialz with full documentation, tutorials, and support for popular open-source models to encourage further research in safe and controllable language generation. Dialz enables faster research cycles and facilitates insights into model interpretability, paving the way for safer, more transparent, and more reliable AI systems.", 'abstract_zh': 'Dialz：一种用于开源LLM引导向量研究的框架', 'title_zh': 'Dialz: 一个用于引导向量的Python工具包'}
