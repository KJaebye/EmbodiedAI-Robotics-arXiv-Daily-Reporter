# Robust Understanding of Human-Robot Social Interactions through Multimodal Distillation 

**Title (ZH)**: 通过多模态蒸馏实现人类-机器人社会互动的稳健理解 

**Authors**: Tongfei Bian, Mathieu Chollet, Tanaya Guha  

**Link**: [PDF](https://arxiv.org/pdf/2505.06278)  

**Abstract**: The need for social robots and agents to interact and assist humans is growing steadily. To be able to successfully interact with humans, they need to understand and analyse socially interactive scenes from their (robot's) perspective. Works that model social situations between humans and agents are few; and even those existing ones are often too computationally intensive to be suitable for deployment in real time or on real world scenarios with limited available information. We propose a robust knowledge distillation framework that models social interactions through various multimodal cues, yet is robust against incomplete and noisy information during inference. Our teacher model is trained with multimodal input (body, face and hand gestures, gaze, raw images) that transfers knowledge to a student model that relies solely on body pose. Extensive experiments on two publicly available human-robot interaction datasets demonstrate that the our student model achieves an average accuracy gain of 14.75\% over relevant baselines on multiple downstream social understanding task even with up to 51\% of its input being corrupted. The student model is highly efficient: it is $<1$\% in size of the teacher model in terms of parameters and uses $\sim 0.5$\textperthousand~FLOPs of that in the teacher model. Our code will be made public during publication. 

**Abstract (ZH)**: 社会机器人和代理与人类交互和辅助的需求正在稳步增长。为了能够成功地与人类交互，它们需要从自身（机器人）的视角理解并分析社会互动场景。现有模型中对人类与代理之间社会情境的建模较少，而现有的模型往往因计算量大而不适合在实时或信息有限的真实场景中部署。我们提出了一种鲁棒的知识蒸馏框架，通过多种多模态线索建模社会互动，且在推理过程中能够应对不完整和噪声信息。我们的教师模型通过多模态输入（身体、面部和手部动作、视线、原始图像）训练，并将知识传递给仅依赖于身体姿态的学生模型。在两个公开的交互式人类-机器人数据集上的广泛实验表明，即使有高达51%的输入被破坏，学生模型在多个下游社会理解任务中的平均准确率也比相关基准高14.75%。该学生模型极为高效：参数量仅为教师模型的不到1%，运行开销约为教师模型的0.5‰。我们将在发表时公开代码。 

---
# SynSHRP2: A Synthetic Multimodal Benchmark for Driving Safety-critical Events Derived from Real-world Driving Data 

**Title (ZH)**: SynSHRP2：来自真实驾驶数据的安全关键事件合成多模态基准 

**Authors**: Liang Shi, Boyu Jiang, Zhenyuan Yuan, Miguel A. Perez, Feng Guo  

**Link**: [PDF](https://arxiv.org/pdf/2505.06276)  

**Abstract**: Driving-related safety-critical events (SCEs), including crashes and near-crashes, provide essential insights for the development and safety evaluation of automated driving systems. However, two major challenges limit their accessibility: the rarity of SCEs and the presence of sensitive privacy information in the data. The Second Strategic Highway Research Program (SHRP 2) Naturalistic Driving Study (NDS), the largest NDS to date, collected millions of hours of multimodal, high-resolution, high-frequency driving data from thousands of participants, capturing thousands of SCEs. While this dataset is invaluable for safety research, privacy concerns and data use restrictions significantly limit public access to the raw data. To address these challenges, we introduce SynSHRP2, a publicly available, synthetic, multimodal driving dataset containing over 1874 crashes and 6924 near-crashes derived from the SHRP 2 NDS. The dataset features de-identified keyframes generated using Stable Diffusion and ControlNet, ensuring the preservation of critical safety-related information while eliminating personally identifiable data. Additionally, SynSHRP2 includes detailed annotations on SCE type, environmental and traffic conditions, and time-series kinematic data spanning 5 seconds before and during each event. Synchronized keyframes and narrative descriptions further enhance its usability. This paper presents two benchmarks for event attribute classification and scene understanding, demonstrating the potential applications of SynSHRP2 in advancing safety research and automated driving system development. 

**Abstract (ZH)**: 驾驶相关的安全关键事件（SCEs），包括碰撞和接近碰撞事件，为自动驾驶系统的发展和安全评估提供了重要的见解。然而，两个主要挑战限制了这些事件的访问性：SCEs的稀有性和数据中的敏感隐私信息。第二代战略公路研究计划（SHRP 2）自然驾驶研究（NDS），迄今为止最大的NDS，收集了数千名参与者数百万小时的多模态、高分辨率、高频率驾驶数据，捕捉了数千个SCEs。尽管该数据集对安全研究至关重要，但隐私问题和数据使用限制显著限制了原始数据的公开访问。为应对这些挑战，我们引入了SynSHRP2，这是一个包含超过1874起碰撞和6924起接近碰撞事件的公开可用、合成的多模态驾驶数据集，源自SHRP 2 NDS。该数据集通过使用Stable Diffusion和ControlNet生成脱敏关键帧，确保保留了关键安全相关的信息，同时消除了可识别的个人数据。此外，SynSHRP2还包括关于事件类型、环境和交通条件以及每个事件前5秒和期间的时间序列动力学数据的详细标注。同步的关键帧和叙述性描述进一步提高了其可用性。本文提出了两种事件属性分类和场景理解的基准，展示了SynSHRP2在推进安全研究和自动驾驶系统开发中潜在应用的可能性。 

---
# Exploring Multimodal Foundation AI and Expert-in-the-Loop for Sustainable Management of Wild Salmon Fisheries in Indigenous Rivers 

**Title (ZH)**: 探索多模态基础人工智能和专家在环可持续管理原住民河流野生鲑鱼渔场的方法 

**Authors**: Chi Xu, Yili Jin, Sami Ma, Rongsheng Qian, Hao Fang, Jiangchuan Liu, Xue Liu, Edith C.H. Ngai, William I. Atlas, Katrina M. Connors, Mark A. Spoljaric  

**Link**: [PDF](https://arxiv.org/pdf/2505.06637)  

**Abstract**: Wild salmon are essential to the ecological, economic, and cultural sustainability of the North Pacific Rim. Yet climate variability, habitat loss, and data limitations in remote ecosystems that lack basic infrastructure support pose significant challenges to effective fisheries management. This project explores the integration of multimodal foundation AI and expert-in-the-loop frameworks to enhance wild salmon monitoring and sustainable fisheries management in Indigenous rivers across Pacific Northwest. By leveraging video and sonar-based monitoring, we develop AI-powered tools for automated species identification, counting, and length measurement, reducing manual effort, expediting delivery of results, and improving decision-making accuracy. Expert validation and active learning frameworks ensure ecological relevance while reducing annotation burdens. To address unique technical and societal challenges, we bring together a cross-domain, interdisciplinary team of university researchers, fisheries biologists, Indigenous stewardship practitioners, government agencies, and conservation organizations. Through these collaborations, our research fosters ethical AI co-development, open data sharing, and culturally informed fisheries management. 

**Abstract (ZH)**: 野生鲑鱼对北太平洋地区的生态、经济和文化可持续性至关重要。然而，气候变化、栖息地丧失以及缺乏基本基础设施支持的偏远生态系统的数据限制，给有效的渔业管理带来了重大挑战。本项目探索多模态基础AI与专家在环框架的集成，以增强对太平洋西北地区原住民河流中野生鲑鱼的监测，并推动可持续渔业管理。通过利用视频和声纳监测，我们开发了基于AI的工具，以实现自动物种识别、计数和长度测量，减少人工操作，加快结果交付速度，并提高决策准确性。专家验证和主动学习框架确保生态相关性，同时减少标注负担。为应对独特的技术和社会挑战，我们汇聚了一个跨领域、跨学科的研究团队，包括高校研究人员、渔业生物学家、原住民管理实践者、政府部门和保护组织。通过这些合作，我们的研究促进了伦理AI协同开发、开放数据共享，并推动了文化导向的渔业管理。 

---
# Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge 

**Title (ZH)**: NLPCC 2025 共享任务 4 概览：多模态、多语言、多跳医疗教学视频问答挑战 

**Authors**: Bin Li, Shenxi Liu, Yixuan Weng, Yue Du, Yuhang Tian, Shoujun Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2505.06814)  

**Abstract**: Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the 2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has been introduced to further advance research in multi-modal, multilingual, and multi-hop medical instructional question answering (M4IVQA) systems, with a specific focus on medical instructional videos. The M4IVQA challenge focuses on evaluating models that integrate information from medical instructional videos, understand multiple languages, and answer multi-hop questions requiring reasoning over various modalities. This task consists of three tracks: multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Single Video (M4TAGSV), multi-modal, multilingual, and multi-hop Video Corpus Retrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected to develop algorithms capable of processing both video and text data, understanding multilingual queries, and providing relevant answers to multi-hop medical questions. We believe the newly introduced M4IVQA challenge will drive innovations in multimodal reasoning systems for healthcare scenarios, ultimately contributing to smarter emergency response systems and more effective medical education platforms in multilingual communities. Our official website is this https URL 

**Abstract (ZH)**: 在成功举办了第一届（NLPCC 2023 福州）CMIVQA和第二届（NLPCC 2024 杭州）MMIVQA挑战赛后，今年引入了一个新的任务，旨在进一步推动多模态、多语言、多跳医疗指导性问答（M4IVQA）系统的研究，特别关注医疗指导性视频。M4IVQA挑战关注的是评估能够整合医疗指导性视频信息、理解多种语言并回答需要在多种模态上进行推理的多跳问题的模型。该任务包含三个赛道：多模态、多语言、多跳单视频时间答案定位（M4TAGSV）、多模态、多语言、多跳视频数据集检索（M4VCR）和多模态、多语言、多跳视频数据集时间答案定位（M4TAGVC）。M4IVQA参赛者需开发能够处理视频和文本数据、理解多语言查询并提供相关答案以应对多跳医疗问题的算法。我们相信新引入的M4IVQA挑战将推动医疗场景中多模态推理系统的创新，最终为多语言社区贡献更智能的应急响应系统和更有效的医疗教育平台。我们的官方网站是这个 https URL。 

---
# A Short Overview of Multi-Modal Wi-Fi Sensing 

**Title (ZH)**: 多模Wi-Fi传感简要概述 

**Authors**: Zijian Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2505.06682)  

**Abstract**: Wi-Fi sensing has emerged as a significant technology in wireless sensing and Integrated Sensing and Communication (ISAC), offering benefits such as low cost, high penetration, and enhanced privacy. Currently, it is widely utilized in various applications, including action recognition, human localization, and crowd counting. However, Wi-Fi sensing also faces challenges, such as low robustness and difficulties in data collection. Recently, there has been an increasing focus on multi-modal Wi-Fi sensing, where other modalities can act as teachers, providing ground truth or robust features for Wi-Fi sensing models to learn from, or can be directly fused with Wi-Fi for enhanced sensing capabilities. Although these methods have demonstrated promising results and substantial value in practical applications, there is a lack of comprehensive surveys reviewing them. To address this gap, this paper reviews the multi-modal Wi-Fi sensing literature \textbf{from the past 24 months} and highlights the current limitations, challenges and future directions in this field. 

**Abstract (ZH)**: Wi-Fi sensing从多模态角度在过去24个月内的发展：当前限制、挑战与未来方向 

---
# TACFN: Transformer-based Adaptive Cross-modal Fusion Network for Multimodal Emotion Recognition 

**Title (ZH)**: 基于Transformer的自适应跨模态融合网络多模态情感识别 

**Authors**: Feng Liu, Ziwang Fu, Yunlong Wang, Qijian Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2505.06536)  

**Abstract**: The fusion technique is the key to the multimodal emotion recognition task. Recently, cross-modal attention-based fusion methods have demonstrated high performance and strong robustness. However, cross-modal attention suffers from redundant features and does not capture complementary features well. We find that it is not necessary to use the entire information of one modality to reinforce the other during cross-modal interaction, and the features that can reinforce a modality may contain only a part of it. To this end, we design an innovative Transformer-based Adaptive Cross-modal Fusion Network (TACFN). Specifically, for the redundant features, we make one modality perform intra-modal feature selection through a self-attention mechanism, so that the selected features can adaptively and efficiently interact with another modality. To better capture the complementary information between the modalities, we obtain the fused weight vector by splicing and use the weight vector to achieve feature reinforcement of the modalities. We apply TCAFN to the RAVDESS and IEMOCAP datasets. For fair comparison, we use the same unimodal representations to validate the effectiveness of the proposed fusion method. The experimental results show that TACFN brings a significant performance improvement compared to other methods and reaches the state-of-the-art. All code and models could be accessed from this https URL. 

**Abstract (ZH)**: 多模态情感识别任务中的融合技术是关键。近年来，跨模态注意力基融合方法显示出高性能和强鲁棒性。然而，跨模态注意力存在冗余特征问题，并不能很好地捕捉互补特征。我们发现，在跨模态交互过程中，并非必须使用另一种模态的全部信息来强化自身，能够强化一种模态的特征可能仅包含其部分信息。为此，我们设计了一种创新的基于Transformer的自适应跨模态融合网络(TACFN)。具体来说，对于冗余特征，使一种模态通过自我注意力机制进行模内特征选择，从而使选定的特征能够适应性和高效地与另一种模态交互。为了更好地捕捉模态间的互补信息，我们通过拼接获得融合权向量，并利用此权向量实现模态特征的强化。我们将TACFN应用于RAVDESS和IEMOCAP数据集。为了公平比较，我们使用相同的单模态表示验证所提融合方法的有效性。实验结果表明，TACFN相比其他方法带来了显著的性能提升，并达到当前最佳水平。所有代码和模型均可从此链接访问。 

---
# NSF-MAP: Neurosymbolic Multimodal Fusion for Robust and Interpretable Anomaly Prediction in Assembly Pipelines 

**Title (ZH)**: NSF-MAP: 基于神经符号多模态融合的装配流水线稳健可解释异常预测 

**Authors**: Chathurangi Shyalika, Renjith Prasad, Fadi El Kalach, Revathy Venkataramanan, Ramtin Zand, Ramy Harik, Amit Sheth  

**Link**: [PDF](https://arxiv.org/pdf/2505.06333)  

**Abstract**: In modern assembly pipelines, identifying anomalies is crucial in ensuring product quality and operational efficiency. Conventional single-modality methods fail to capture the intricate relationships required for precise anomaly prediction in complex predictive environments with abundant data and multiple modalities. This paper proposes a neurosymbolic AI and fusion-based approach for multimodal anomaly prediction in assembly pipelines. We introduce a time series and image-based fusion model that leverages decision-level fusion techniques. Our research builds upon three primary novel approaches in multimodal learning: time series and image-based decision-level fusion modeling, transfer learning for fusion, and knowledge-infused learning. We evaluate the novel method using our derived and publicly available multimodal dataset and conduct comprehensive ablation studies to assess the impact of our preprocessing techniques and fusion model compared to traditional baselines. The results demonstrate that a neurosymbolic AI-based fusion approach that uses transfer learning can effectively harness the complementary strengths of time series and image data, offering a robust and interpretable approach for anomaly prediction in assembly pipelines with enhanced performance. \noindent The datasets, codes to reproduce the results, supplementary materials, and demo are available at this https URL. 

**Abstract (ZH)**: 现代装配生产线中多模态异常检测的关键性及其神经符号AI和融合方法研究 

---
# Collaborative Multi-LoRA Experts with Achievement-based Multi-Tasks Loss for Unified Multimodal Information Extraction 

**Title (ZH)**: 基于成就导向多任务损失的协作多LoRA专家统一多模态信息提取 

**Authors**: Li Yuan, Yi Cai, Xudong Shen, Qing Li, Qingbao Huang, Zikun Deng, Tao Wang  

**Link**: [PDF](https://arxiv.org/pdf/2505.06303)  

**Abstract**: Multimodal Information Extraction (MIE) has gained attention for extracting structured information from multimedia sources. Traditional methods tackle MIE tasks separately, missing opportunities to share knowledge across tasks. Recent approaches unify these tasks into a generation problem using instruction-based T5 models with visual adaptors, optimized through full-parameter fine-tuning. However, this method is computationally intensive, and multi-task fine-tuning often faces gradient conflicts, limiting performance. To address these challenges, we propose collaborative multi-LoRA experts with achievement-based multi-task loss (C-LoRAE) for MIE tasks. C-LoRAE extends the low-rank adaptation (LoRA) method by incorporating a universal expert to learn shared multimodal knowledge from cross-MIE tasks and task-specific experts to learn specialized instructional task features. This configuration enhances the model's generalization ability across multiple tasks while maintaining the independence of various instruction tasks and mitigating gradient conflicts. Additionally, we propose an achievement-based multi-task loss to balance training progress across tasks, addressing the imbalance caused by varying numbers of training samples in MIE tasks. Experimental results on seven benchmark datasets across three key MIE tasks demonstrate that C-LoRAE achieves superior overall performance compared to traditional fine-tuning methods and LoRA methods while utilizing a comparable number of training parameters to LoRA. 

**Abstract (ZH)**: 多模态信息提取中的协作多LoRA专家与成就导向的多任务损失（C-LoRAE） 

---
