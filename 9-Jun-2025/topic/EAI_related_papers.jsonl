{'arxiv_id': 'arXiv:2506.06221', 'title': 'BiAssemble: Learning Collaborative Affordance for Bimanual Geometric Assembly', 'authors': 'Yan Shen, Ruihai Wu, Yubin Ke, Xinyuan Song, Zeyi Li, Xiaoqi Li, Hongwei Fan, Haoran Lu, Hao dong', 'link': 'https://arxiv.org/abs/2506.06221', 'abstract': 'Shape assembly, the process of combining parts into a complete whole, is a crucial robotic skill with broad real-world applications. Among various assembly tasks, geometric assembly--where broken parts are reassembled into their original form (e.g., reconstructing a shattered bowl)--is particularly challenging. This requires the robot to recognize geometric cues for grasping, assembly, and subsequent bimanual collaborative manipulation on varied fragments. In this paper, we exploit the geometric generalization of point-level affordance, learning affordance aware of bimanual collaboration in geometric assembly with long-horizon action sequences. To address the evaluation ambiguity caused by geometry diversity of broken parts, we introduce a real-world benchmark featuring geometric variety and global reproducibility. Extensive experiments demonstrate the superiority of our approach over both previous affordance-based and imitation-based methods. Project page: this https URL.', 'abstract_zh': '几何组装：一种将破碎部件重新组合成原形的机器人技能，在广泛的实际应用中至关重要。在各类组装任务中，几何组装尤其具有挑战性——需要机器人识别几何线索以进行抓取、组装及后续的双臂协作操作。本文利用点级 affordance 的几何泛化，在长时序行动序列中学习具有双臂协作意识的 affordance，以应对碎片几何多样性带来的评估模糊性问题。我们引入了一个包含几何多样性和全局可复现性的实际环境基准。大量实验表明，本文方法在与基于 affordance 和基于模仿的方法的对比中均表现出优越性。项目页面：这个 https://url.cn/3Jn8XhK。', 'title_zh': 'BiAssemble: 学习双手几何装配的合作功能'}
{'arxiv_id': 'arXiv:2506.06205', 'title': 'Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal Learning', 'authors': 'Sheng Chen, Peiyu He, Jiaxin Hu, Ziyang Liu, Yansheng Wang, Tao Xu, Chi Zhang, Chongchong Zhang, Chao An, Shiyu Cai, Duo Cao, Kangping Chen, Shuai Chu, Tianwei Chu, Mingdi Dan, Min Du, Weiwei Fang, Pengyou Fu, Junkai Hu, Xiaowei Jiang, Zhaodi Jiang, Fuxuan Li, Jun Li, Minghui Li, Mingyao Li, Yanchang Li, Zhibin Li, Guangming Liu, Kairui Liu, Lihao Liu, Weizhi Liu, Xiaoshun Liu, Yufei Liu, Yunfei Liu, Qiang Lu, Yuanfei Luo, Xiang Lv, Hongying Ma, Sai Ma, Lingxian Mi, Sha Sa, Hongxiang Shu, Lei Tian, Chengzhi Wang, Jiayu Wang, Kaijie Wang, Qingyi Wang, Renwen Wang, Tao Wang, Wei Wang, Xirui Wang, Chao Wei, Xuguang Wei, Zijun Xia, Zhaohao Xiao, Tingshuai Yan, Liyan Yang, Yifan Yang, Zhikai Yang, Zhong Yin, Li Yuan, Liuchun Yuan, Chi Zhang, Jinyang Zhang, Junhui Zhang, Linge Zhang, Zhenyi Zhang, Zheyu Zhang, Dongjie Zhu, Hang Li, Yangang Zhang', 'link': 'https://arxiv.org/abs/2506.06205', 'abstract': 'Modern robot navigation systems encounter difficulties in diverse and complex indoor environments. Traditional approaches rely on multiple modules with small models or rule-based systems and thus lack adaptability to new environments. To address this, we developed Astra, a comprehensive dual-model architecture, Astra-Global and Astra-Local, for mobile robot navigation. Astra-Global, a multimodal LLM, processes vision and language inputs to perform self and goal localization using a hybrid topological-semantic graph as the global map, and outperforms traditional visual place recognition methods. Astra-Local, a multitask network, handles local path planning and odometry estimation. Its 4D spatial-temporal encoder, trained through self-supervised learning, generates robust 4D features for downstream tasks. The planning head utilizes flow matching and a novel masked ESDF loss to minimize collision risks for generating local trajectories, and the odometry head integrates multi-sensor inputs via a transformer encoder to predict the relative pose of the robot. Deployed on real in-house mobile robots, Astra achieves high end-to-end mission success rate across diverse indoor environments.', 'abstract_zh': '现代机器人导航系统在多样复杂的室内环境中面临挑战。传统的 approach 依赖多个模块和小规模模型或基于规则的系统，因而缺乏对新环境的适应性。为了解决这一问题，我们开发了 Astra，一种综合双模型架构，包括 Astra-Global 和 Astra-Local，用于移动机器人导航。Astra-Global 是一个多模态大语言模型，处理视觉和语言输入，使用混合拓扑-语义图作为全局地图进行自我和目标定位，并优于传统视觉场所识别方法。Astra-Local 是一个多任务网络，处理局部路径规划和里程计估计。其通过自监督学习训练的 4D 空间-时间编码器生成鲁棒的 4D 特征以供下游任务使用。规划头利用流匹配和一种新颖的掩码 ESDF 损失来最小化碰撞风险以生成局部轨迹，里程计头通过变压器编码整合多传感器输入以预测机器人的相对姿态。在实际室内移动机器人上部署的 Astra 实现了高端到端任务成功率，适用于多种室内环境。', 'title_zh': 'Astra: 基于层次多模态学习的通用型移动机器人研究'}
{'arxiv_id': 'arXiv:2506.06199', 'title': '3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World Model', 'authors': 'Hongyan Zhi, Peihao Chen, Siyuan Zhou, Yubo Dong, Quanxi Wu, Lei Han, Mingkui Tan', 'link': 'https://arxiv.org/abs/2506.06199', 'abstract': 'Manipulation has long been a challenging task for robots, while humans can effortlessly perform complex interactions with objects, such as hanging a cup on the mug rack. A key reason is the lack of a large and uniform dataset for teaching robots manipulation skills. Current robot datasets often record robot action in different action spaces within a simple scene. This hinders the robot to learn a unified and robust action representation for different robots within diverse scenes. Observing how humans understand a manipulation task, we find that understanding how the objects should move in the 3D space is a critical clue for guiding actions. This clue is embodiment-agnostic and suitable for both humans and different robots. Motivated by this, we aim to learn a 3D flow world model from both human and robot manipulation data. This model predicts the future movement of the interacting objects in 3D space, guiding action planning for manipulation. Specifically, we synthesize a large-scale 3D optical flow dataset, named ManiFlow-110k, through a moving object auto-detect pipeline. A video diffusion-based world model then learns manipulation physics from these data, generating 3D optical flow trajectories conditioned on language instructions. With the generated 3D object optical flow, we propose a flow-guided rendering mechanism, which renders the predicted final state and leverages GPT-4o to assess whether the predicted flow aligns with the task description. This equips the robot with a closed-loop planning ability. Finally, we consider the predicted 3D optical flow as constraints for an optimization policy to determine a chunk of robot actions for manipulation. Extensive experiments demonstrate strong generalization across diverse robotic manipulation tasks and reliable cross-embodiment adaptation without hardware-specific training.', 'abstract_zh': '基于人类和机器人数据的学习三维流动世界模型：引导操作规划与通用化', 'title_zh': '3DFlowAction：从3D流世界模型学习跨身体操纵技能'}
{'arxiv_id': 'arXiv:2506.06196', 'title': 'Bridging Perception and Action: Spatially-Grounded Mid-Level Representations for Robot Generalization', 'authors': 'Jonathan Yang, Chuyuan Kelly Fu, Dhruv Shah, Dorsa Sadigh, Fei Xia, Tingnan Zhang', 'link': 'https://arxiv.org/abs/2506.06196', 'abstract': 'In this work, we investigate how spatially grounded auxiliary representations can provide both broad, high-level grounding as well as direct, actionable information to improve policy learning performance and generalization for dexterous tasks. We study these mid-level representations across three critical dimensions: object-centricity, pose-awareness, and depth-awareness. We use these interpretable mid-level representations to train specialist encoders via supervised learning, then feed them as inputs to a diffusion policy to solve dexterous bimanual manipulation tasks in the real world. We propose a novel mixture-of-experts policy architecture that combines multiple specialized expert models, each trained on a distinct mid-level representation, to improve policy generalization. This method achieves an average success rate that is 11% higher than a language-grounded baseline and 24 percent higher than a standard diffusion policy baseline on our evaluation tasks. Furthermore, we find that leveraging mid-level representations as supervision signals for policy actions within a weighted imitation learning algorithm improves the precision with which the policy follows these representations, yielding an additional performance increase of 10%. Our findings highlight the importance of grounding robot policies not only with broad perceptual tasks but also with more granular, actionable representations. For further information and videos, please visit this https URL.', 'abstract_zh': '本工作中，我们探讨了基于空间的辅助表示如何提供广泛的高层次 grounding 以及直接的可操作信息，以提高灵巧任务的策略学习性能和泛化能力。我们从三个关键维度研究这些中层表示：对象中心性、姿态感知和深度感知。我们使用这些可解释的中层表示，通过监督学习训练专业编码器，然后将它们作为输入馈入扩散策略，以解决真实世界中的双臂灵巧操作任务。我们提出了一种新颖的专家混合策略架构，该架构结合了多个训练在不同中层表示上的专业专家模型，以提高策略的泛化能力。该方法在我们的评估任务中，平均成功率比基于语言的基线高11%，比标准的扩散策略基线高24%。此外，我们发现，在加权模仿学习算法中利用中层表示作为策略行动的监督信号，可以提高策略跟随这些表示的精确度，从而进一步提高10%的性能。我们的研究结果强调，不仅需要通过广泛的感知任务来地化机器人策略，还需要通过更细粒度的、可操作的表示来地化策略。欲了解更多信息和视频，请访问此链接：https://xxxxxx。', 'title_zh': '感知与行动的桥梁：基于空间的中间表示形式促进机器人泛化'}
{'arxiv_id': 'arXiv:2506.06094', 'title': 'On-board Mission Replanning for Adaptive Cooperative Multi-Robot Systems', 'authors': 'Elim Kwan, Rehman Qureshi, Liam Fletcher, Colin Laganier, Victoria Nockles, Richard Walters', 'link': 'https://arxiv.org/abs/2506.06094', 'abstract': 'Cooperative autonomous robotic systems have significant potential for executing complex multi-task missions across space, air, ground, and maritime domains. But they commonly operate in remote, dynamic and hazardous environments, requiring rapid in-mission adaptation without reliance on fragile or slow communication links to centralised compute. Fast, on-board replanning algorithms are therefore needed to enhance resilience. Reinforcement Learning shows strong promise for efficiently solving mission planning tasks when formulated as Travelling Salesperson Problems (TSPs), but existing methods: 1) are unsuitable for replanning, where agents do not start at a single location; 2) do not allow cooperation between agents; 3) are unable to model tasks with variable durations; or 4) lack practical considerations for on-board deployment. Here we define the Cooperative Mission Replanning Problem as a novel variant of multiple TSP with adaptations to overcome these issues, and develop a new encoder/decoder-based model using Graph Attention Networks and Attention Models to solve it effectively and efficiently. Using a simple example of cooperative drones, we show our replanner consistently (90% of the time) maintains performance within 10% of the state-of-the-art LKH3 heuristic solver, whilst running 85-370 times faster on a Raspberry Pi. This work paves the way for increased resilience in autonomous multi-agent systems.', 'abstract_zh': '合作自主机器人系统在空间、空气、地面和 maritime 领域执行复杂多任务使命具有巨大的潜力，但它们通常在远程、动态和危险的环境中操作，需要快速进行在任务中的适应而不依赖于脆弱或慢速的通信链路与集中计算。因此需要快速的机载重规划算法以增强系统的韧性。强化学习在将使命规划任务表述为旅行商问题（TSP）时显示出强大的潜力，但现有方法：1) 不适用于重规划，因代理不从单一位置开始；2) 不允许代理之间的合作；3) 无法建模具有变化持续时间的任务；或 4) 缺乏针对机载部署的实际考虑。在这里，我们将合作使命重规划问题定义为多TSP的新型变体，并通过Graph Attention Networks和注意力模型开发了一种新的编码器/解码器模型，以有效地解决该问题。以合作无人机的简单示例为例，我们展示了我们的重规划器在90%的时间内保持与最先进的LKH3启发式求解器相当的性能，同时在Raspberry Pi上运行速度快85-370倍。这项工作为自主多代理系统增加了韧性奠定了基础。', 'title_zh': '在分支任务规划适应性合作多机器人系统中的车载任务重规划'}
{'arxiv_id': 'arXiv:2506.06028', 'title': 'End-to-End Framework for Robot Lawnmower Coverage Path Planning using Cellular Decomposition', 'authors': 'Nikunj Shah, Utsav Dey, Kenji Nishimiya', 'link': 'https://arxiv.org/abs/2506.06028', 'abstract': 'Efficient Coverage Path Planning (CPP) is necessary for autonomous robotic lawnmowers to effectively navigate and maintain lawns with diverse and irregular shapes. This paper introduces a comprehensive end-to-end pipeline for CPP, designed to convert user-defined boundaries on an aerial map into optimized coverage paths seamlessly. The pipeline includes user input extraction, coordinate transformation, area decomposition and path generation using our novel AdaptiveDecompositionCPP algorithm, preview and customization through an interactive coverage path visualizer, and conversion to actionable GPS waypoints. The AdaptiveDecompositionCPP algorithm combines cellular decomposition with an adaptive merging strategy to reduce non-mowing travel thereby enhancing operational efficiency. Experimental evaluations, encompassing both simulations and real-world lawnmower tests, demonstrate the effectiveness of the framework in coverage completeness and mowing efficiency.', 'abstract_zh': '高效的覆盖路径规划（CPP）对于自主割草机器人有效导航和维护具有多样化和不规则形状的草坪是必要的。本文介绍了用于CPP的全面端到端管道，旨在将用户定义的边界无缝转换为优化的覆盖路径。该管道包括用户输入提取、坐标变换、区域分解和路径生成（使用我们提出的AdaptiveDecompositionCPP算法）、通过交互式的覆盖路径可视化器进行预览和定制，以及转换为可执行的GPS航点。AdaptiveDecompositionCPP算法结合了细胞分解与自适应合并策略，以减少非割草行驶，从而提升操作效率。实验评估，包括模拟和实际割草机测试，证明了该框架在覆盖完整性和割草效率方面的有效性。', 'title_zh': '基于细胞分解的无人驾驶割草机器人全场路径规划端到端框架'}
{'arxiv_id': 'arXiv:2506.05997', 'title': 'Improving Long-Range Navigation with Spatially-Enhanced Recurrent Memory via End-to-End Reinforcement Learning', 'authors': 'Fan Yang, Per Frivik, David Hoeller, Chen Wang, Cesar Cadena, Marco Hutter', 'link': 'https://arxiv.org/abs/2506.05997', 'abstract': 'Recent advancements in robot navigation, especially with end-to-end learning approaches like reinforcement learning (RL), have shown remarkable efficiency and effectiveness. Yet, successful navigation still relies on two key capabilities: mapping and planning, whether explicit or implicit. Classical approaches use explicit mapping pipelines to register ego-centric observations into a coherent map frame for the planner. In contrast, end-to-end learning achieves this implicitly, often through recurrent neural networks (RNNs) that fuse current and past observations into a latent space for planning. While architectures such as LSTM and GRU capture temporal dependencies, our findings reveal a key limitation: their inability to perform effective spatial memorization. This skill is essential for transforming and integrating sequential observations from varying perspectives to build spatial representations that support downstream planning. To address this, we propose Spatially-Enhanced Recurrent Units (SRUs), a simple yet effective modification to existing RNNs, designed to enhance spatial memorization capabilities. We introduce an attention-based architecture with SRUs, enabling long-range navigation using a single forward-facing stereo camera. Regularization techniques are employed to ensure robust end-to-end recurrent training via RL. Experimental results show our approach improves long-range navigation by 23.5% compared to existing RNNs. Furthermore, with SRU memory, our method outperforms the RL baseline with explicit mapping and memory modules, achieving a 29.6% improvement in diverse environments requiring long-horizon mapping and memorization. Finally, we address the sim-to-real gap by leveraging large-scale pretraining on synthetic depth data, enabling zero-shot transfer to diverse and complex real-world environments.', 'abstract_zh': '近期机器人导航的进展，尤其是端到端学习方法（如强化学习RL），显示出了显著的效率和效果。然而，成功的导航仍依赖于两个关键能力：建图和规划，无论是显式的还是隐式的。经典方法使用显式的建图管道，将第一人称观测注册为规划器所需的连贯的地图框架。相比之下，端到端学习隐式地实现了这一点，通常通过融合当前和过去观测的递归神经网络（RNN），在潜在空间中进行规划。虽然LSTM和GRU等架构捕捉了时间依赖性，但我们的研究揭示了一个关键限制：它们无法有效地进行空间记忆。这种技能对于将不同视角下的序列观测转换和整合以构建支持下游规划的空间表示是必不可少的。为了应对这一挑战，我们提出了一种增强空间记忆能力的简单而有效的递归单元改进方案——空间增强递归单元（SRUs）。我们介绍了一种基于注意力机制的SRU架构，使得仅用一个前方双目摄像头实现远程导航成为可能。通过正则化技术，我们确保了通过RL实现端到端递归训练的鲁棒性。实验结果显示，与现有RNN相比，我们的方法在远程导航性能上提高了23.5%。此外，借助SRU记忆，我们的方法在需要长期映射和记忆的多样化环境中优于带有显式建图和记忆模块的RL基线，取得了29.6%的性能提升。最后，我们通过大规模预训练合成深度数据，解决了仿真到现实世界的差距，使方法能够零样本传输到多样化和复杂的实际环境。', 'title_zh': '基于端到端强化学习的时空增强递归记忆在长距离导航中的改进'}
{'arxiv_id': 'arXiv:2506.05896', 'title': 'Object Navigation with Structure-Semantic Reasoning-Based Multi-level Map and Multimodal Decision-Making LLM', 'authors': 'Chongshang Yan, Jiaxuan He, Delun Li, Yi Yang, Wenjie Song', 'link': 'https://arxiv.org/abs/2506.05896', 'abstract': 'The zero-shot object navigation (ZSON) in unknown open-ended environments coupled with semantically novel target often suffers from the significant decline in performance due to the neglect of high-dimensional implicit scene information and the long-range target searching task. To address this, we proposed an active object navigation framework with Environmental Attributes Map (EAM) and MLLM Hierarchical Reasoning module (MHR) to improve its success rate and efficiency. EAM is constructed by reasoning observed environments with SBERT and predicting unobserved ones with Diffusion, utilizing human space regularities that underlie object-room correlations and area adjacencies. MHR is inspired by EAM to perform frontier exploration decision-making, avoiding the circuitous trajectories in long-range scenarios to improve path efficiency. Experimental results demonstrate that the EAM module achieves 64.5\\% scene mapping accuracy on MP3D dataset, while the navigation task attains SPLs of 28.4\\% and 26.3\\% on HM3D and MP3D benchmarks respectively - representing absolute improvements of 21.4\\% and 46.0\\% over baseline methods.', 'abstract_zh': '基于环境属性图和多级逻辑记忆模块的零样本物体导航', 'title_zh': '基于结构语义推理的多层级地图与多模态决策导航'}
{'arxiv_id': 'arXiv:2506.05808', 'title': 'Where Do We Look When We Teach? Analyzing Human Gaze Behavior Across Demonstration Devices in Robot Imitation Learning', 'authors': 'Yutaro Ishida, Takamitsu Matsubara, Takayuki Kanai, Kazuhiro Shintani, Hiroshi Bito', 'link': 'https://arxiv.org/abs/2506.05808', 'abstract': "Imitation learning for acquiring generalizable policies often requires a large volume of demonstration data, making the process significantly costly. One promising strategy to address this challenge is to leverage the cognitive and decision-making skills of human demonstrators with strong generalization capability, particularly by extracting task-relevant cues from their gaze behavior. However, imitation learning typically involves humans collecting data using demonstration devices that emulate a robot's embodiment and visual condition. This raises the question of how such devices influence gaze behavior. We propose an experimental framework that systematically analyzes demonstrators' gaze behavior across a spectrum of demonstration devices. Our experimental results indicate that devices emulating (1) a robot's embodiment or (2) visual condition impair demonstrators' capability to extract task-relevant cues via gaze behavior, with the extent of impairment depending on the degree of emulation. Additionally, gaze data collected using devices that capture natural human behavior improves the policy's task success rate from 18.8% to 68.8% under environmental shifts.", 'abstract_zh': '基于认知和决策技能的人类演示者在获取可泛化策略中的模仿学习往往需要大量示例数据，使这一过程变得成本高昂。一种有潜力的策略是利用具有强泛化能力的人类演示者认知和决策技能，特别是通过提取与任务相关的线索来利用他们的注视行为。然而，模仿学习通常涉及人类使用模拟机器人身体和视觉条件的设备来收集数据。这引发了这些设备如何影响注视行为的问题。我们提出了一种实验框架，系统分析不同演示设备下演示者注视行为的变化。实验结果表明，模拟（1）机器人身体或（2）视觉条件的设备会妨碍演示者通过注视行为提取与任务相关的线索，这种妨碍的程度取决于模拟的程度。此外，在环境变化下，使用捕捉自然人类行为的设备收集的眼动数据可以使策略的任务成功率从18.8%提高到68.8%。', 'title_zh': '我们教学时看向何处？在机器人模仿学习中演示设备的人类凝视行为分析'}
{'arxiv_id': 'arXiv:2506.06016', 'title': 'Equivariant Filter for Relative Attitude and Target Angular Velocity Estimation', 'authors': 'Gil Serrano, Bruno J. Guerreiro, Pedro Lourenço, Rita Cunha', 'link': 'https://arxiv.org/abs/2506.06016', 'abstract': "Accurate estimation of the relative attitude and angular velocity between two rigid bodies is fundamental in aerospace applications such as spacecraft rendezvous and docking. In these scenarios, a chaser vehicle must determine the orientation and angular velocity of a target object using onboard sensors. This work addresses the challenge of designing an Equivariant Filter (EqF) that can reliably estimate both the relative attitude and the target angular velocity using noisy observations of two known, non-collinear vectors fixed in the target frame. To derive the EqF, a symmetry for the system is proposed and an equivariant lift onto the symmetry group is calculated. Observability and convergence properties are analyzed. Simulations demonstrate the filter's performance, with Monte Carlo runs yielding statistically significant results. The impact of low-rate measurements is also examined and a strategy to mitigate this effect is proposed. Experimental results, using fiducial markers and both conventional and event cameras for measurement acquisition, further validate the approach, confirming its effectiveness in a realistic setting.", 'abstract_zh': '在航天航空应用中基于两刚体之间相对姿态和角速度的准确估计方法', 'title_zh': '相对姿态和目标角速度 estimation 的协变滤波器'}
{'arxiv_id': 'arXiv:2506.05985', 'title': 'Dynamic Mixture of Progressive Parameter-Efficient Expert Library for Lifelong Robot Learning', 'authors': 'Yuheng Lei, Sitong Mao, Shunbo Zhou, Hongyuan Zhang, Xuelong Li, Ping Luo', 'link': 'https://arxiv.org/abs/2506.05985', 'abstract': 'A generalist agent must continuously learn and adapt throughout its lifetime, achieving efficient forward transfer while minimizing catastrophic forgetting. Previous work within the dominant pretrain-then-finetune paradigm has explored parameter-efficient fine-tuning for single-task adaptation, effectively steering a frozen pretrained model with a small number of parameters. However, in the context of lifelong learning, these methods rely on the impractical assumption of a test-time task identifier and restrict knowledge sharing among isolated adapters. To address these limitations, we propose Dynamic Mixture of Progressive Parameter-Efficient Expert Library (DMPEL) for lifelong robot learning. DMPEL progressively learn a low-rank expert library and employs a lightweight router to dynamically combine experts into an end-to-end policy, facilitating flexible behavior during lifelong adaptation. Moreover, by leveraging the modular structure of the fine-tuned parameters, we introduce coefficient replay to guide the router in accurately retrieving frozen experts for previously encountered tasks, thereby mitigating catastrophic forgetting. This method is significantly more storage- and computationally-efficient than applying demonstration replay to the entire policy. Extensive experiments on the lifelong manipulation benchmark LIBERO demonstrate that our framework outperforms state-of-the-art lifelong learning methods in success rates across continual adaptation, while utilizing minimal trainable parameters and storage.', 'abstract_zh': '一种通用智能体必须在其生命周期内持续学习和适应，实现高效的前向迁移同时尽量减少灾难性遗忘。在占主导地位的预训练-然后微调范式中，先前工作探索了单任务适应的参数高效微调方法，有效利用少量参数引导冻结的预训练模型。然而，在终身学习的背景下，这些方法依赖于在测试时需任务标识符这一 impractical 的假设，并限制了隔离适配器之间知识的共享。为了解决这些限制，我们提出了一种动态渐进参数高效专家库（DMPEL）方法，用于终身机器人学习。DMPEL 逐步学习一个低秩专家库，并采用一个轻量级路由器动态组合专家，促进终身适应过程中灵活的行为。此外，利用微调参数的模块化结构，我们引入了系数重播，指导路由器准确检索之前遇到的任务的冻结专家，从而减轻灾难性遗忘。该方法在整体策略上应用演示重播方面更为存储和计算高效。在终身操作基准 LIBERO 上的广泛实验表明，我们的框架在持续适应过程中成功率达到最新水平，同时使用最少的可训练参数和存储空间。', 'title_zh': 'lifelong 机器人学习中渐进参数高效专家库的动态混合'}
{'arxiv_id': 'arXiv:2506.05797', 'title': 'EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator', 'authors': 'Qianyi Chen, Tianrun Gao, Chenbo Jiang, Tailin Wu', 'link': 'https://arxiv.org/abs/2506.05797', 'abstract': 'Simulating collisions of deformable objects is a fundamental yet challenging task due to the complexity of modeling solid mechanics and multi-body interactions. Existing data-driven methods often suffer from lack of equivariance to physical symmetries, inadequate handling of collisions, and limited scalability. Here we introduce EqCollide, the first end-to-end equivariant neural fields simulator for deformable objects and their collisions. We propose an equivariant encoder to map object geometry and velocity into latent control points. A subsequent equivariant Graph Neural Network-based Neural Ordinary Differential Equation models the interactions among control points via collision-aware message passing. To reconstruct velocity fields, we query a neural field conditioned on control point features, enabling continuous and resolution-independent motion predictions. Experimental results show that EqCollide achieves accurate, stable, and scalable simulations across diverse object configurations, and our model achieves 24.34% to 35.82% lower rollout MSE even compared with the best-performing baseline model. Furthermore, our model could generalize to more colliding objects and extended temporal horizons, and stay robust to input transformed with group action.', 'abstract_zh': '基于等变神经场的可变形物体及其碰撞的端到端模拟', 'title_zh': 'EqCollide: 具有equivariance和碰撞意识的可变形对象神经模拟器'}
{'arxiv_id': 'arXiv:2506.05616', 'title': 'Toward Greater Autonomy in Materials Discovery Agents: Unifying Planning, Physics, and Scientists', 'authors': 'Lianhao Zhou, Hongyi Ling, Keqiang Yan, Kaiji Zhao, Xiaoning Qian, Raymundo Arróyave, Xiaofeng Qian, Shuiwang Ji', 'link': 'https://arxiv.org/abs/2506.05616', 'abstract': 'We aim at designing language agents with greater autonomy for crystal materials discovery. While most of existing studies restrict the agents to perform specific tasks within predefined workflows, we aim to automate workflow planning given high-level goals and scientist intuition. To this end, we propose Materials Agent unifying Planning, Physics, and Scientists, known as MAPPS. MAPPS consists of a Workflow Planner, a Tool Code Generator, and a Scientific Mediator. The Workflow Planner uses large language models (LLMs) to generate structured and multi-step workflows. The Tool Code Generator synthesizes executable Python code for various tasks, including invoking a force field foundation model that encodes physics. The Scientific Mediator coordinates communications, facilitates scientist feedback, and ensures robustness through error reflection and recovery. By unifying planning, physics, and scientists, MAPPS enables flexible and reliable materials discovery with greater autonomy, achieving a five-fold improvement in stability, uniqueness, and novelty rates compared with prior generative models when evaluated on the MP-20 data. We provide extensive experiments across diverse tasks to show that MAPPS is a promising framework for autonomous materials discovery.', 'abstract_zh': '我们旨在设计更具自主性的语言代理以发现晶体材料。现有大多数研究限制代理在预定义工作流中执行特定任务，而我们的目标是在高层目标和科学家直觉的指导下自动化工作流规划。为此，我们提出了一种统合规划、物理和科学家的Materials Agent，简称MAPPS。MAPPS包括工作流规划器、工具代码生成器和科学调解器。工作流规划器使用大规模语言模型（LLMs）生成结构化和多步工作流。工具代码生成器合成可执行的Python代码以执行各种任务，包括调用编码物理学的力场基础模型。科学调解器协调沟通、促进科学家反馈，并通过错误反思和恢复确保鲁棒性。通过统合规划、物理和科学家，MAPPS能够实现更具自主性的灵活且可靠的材料发现，并在MP-20数据上评估时，与先前的生成模型相比，在稳定性和新颖性方面取得了五倍的改进。我们提供了涵盖多种任务的广泛实验，证明了MAPPS是自主材料发现的一个有前景的框架。', 'title_zh': '向着材料发现代理更大的自主权：统一规划、物理和科学家'}
{'arxiv_id': 'arXiv:2506.05422', 'title': 'Constructive Symbolic Reinforcement Learning via Intuitionistic Logic and Goal-Chaining Inference', 'authors': 'Andrei T. Patrascu', 'link': 'https://arxiv.org/abs/2506.05422', 'abstract': 'We introduce a novel learning and planning framework that replaces traditional reward-based optimisation with constructive logical inference. In our model, actions, transitions, and goals are represented as logical propositions, and decision-making proceeds by building constructive proofs under intuitionistic logic. This method ensures that state transitions and policies are accepted only when supported by verifiable preconditions -- eschewing probabilistic trial-and-error in favour of guaranteed logical validity. We implement a symbolic agent operating in a structured gridworld, where reaching a goal requires satisfying a chain of intermediate subgoals (e.g., collecting keys to open doors), each governed by logical constraints. Unlike conventional reinforcement learning agents, which require extensive exploration and suffer from unsafe or invalid transitions, our constructive agent builds a provably correct plan through goal chaining, condition tracking, and knowledge accumulation. Empirical comparison with Q-learning demonstrates that our method achieves perfect safety, interpretable behaviour, and efficient convergence with no invalid actions, highlighting its potential for safe planning, symbolic cognition, and trustworthy AI. This work presents a new direction for reinforcement learning grounded not in numeric optimisation, but in constructive logic and proof theory.', 'abstract_zh': '一种基于构造逻辑推断的新型学习与规划框架：从数值优化到证明理论', 'title_zh': '基于直觉逻辑和目标链推理的建设性符号强化学习'}
{'arxiv_id': 'arXiv:2506.06006', 'title': 'Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models', 'authors': 'Yifu Qiu, Yftah Ziser, Anna Korhonen, Shay B. Cohen, Edoardo M. Ponti', 'link': 'https://arxiv.org/abs/2506.06006', 'abstract': 'To what extent do vision-and-language foundation models possess a realistic world model (observation $\\times$ action $\\rightarrow$ observation) and a dynamics model (observation $\\times$ observation $\\rightarrow$ action), when actions are expressed through language? While open-source foundation models struggle with both, we find that fine-tuning them to acquire a dynamics model through supervision is significantly easier than acquiring a world model. In turn, dynamics models can be used to bootstrap world models through two main strategies: 1) weakly supervised learning from synthetic data and 2) inference time verification. Firstly, the dynamics model can annotate actions for unlabelled pairs of video frame observations to expand the training data. We further propose a new objective, where image tokens in observation pairs are weighted by their importance, as predicted by a recognition model. Secondly, the dynamics models can assign rewards to multiple samples of the world model to score them, effectively guiding search at inference time. We evaluate the world models resulting from both strategies through the task of action-centric image editing on Aurora-Bench. Our best model achieves a performance competitive with state-of-the-art image editing models, improving on them by a margin of $15\\%$ on real-world subsets according to GPT4o-as-judge, and achieving the best average human evaluation across all subsets of Aurora-Bench.', 'abstract_zh': '视觉和语言基础模型在多模态观察（观察×动作→观察）和动力学模型（观察×观察→动作）方面具备多现实世界的程度，尤其是在动作通过语言表达时？尽管开源基础模型在两者方面都存在挑战，我们发现通过监督细调它们以获得动力学模型比获得世界模型要容易得多。反过来，动力学模型可以通过两种主要策略来增强世界模型：1) 从合成数据中进行弱监督学习；2) 推断时间验证。首先，动力学模型可以为未标记的视频帧观察对添加动作标签，以扩展训练数据。我们进一步提出一个新的目标，其中在观察对中的图像标记根据识别模型的预测重要性加权。其次，动力学模型可以为世界模型的多个样本分配奖励，以评估它们，在推理时间有效引导搜索。我们通过奥罗拉平台上的以动作为中心的图像编辑任务评估通过这两种策略生成的世界模型。我们的最佳模型在GPT4o-as-judge评测上与最先进的图像编辑模型具有竞争力，在现实世界的子集上提高了15%的性能，并在奥罗拉平台的所有子集上实现了最高的平均人类评估得分。', 'title_zh': '从多模态基础模型的动态模型中bootstrapping世界模型'}
{'arxiv_id': 'arXiv:2506.05883', 'title': 'HMVLM: Multistage Reasoning-Enhanced Vision-Language Model for Long-Tailed Driving Scenarios', 'authors': 'Daming Wang, Yuhao Song, Zijian He, Kangliang Chen, Xing Pan, Lu Deng, Weihao Gu', 'link': 'https://arxiv.org/abs/2506.05883', 'abstract': 'We present HaoMo Vision-Language Model (HMVLM), an end-to-end driving framework that implements the slow branch of a cognitively inspired fast-slow architecture. A fast controller outputs low-level steering, throttle, and brake commands, while a slow planner-a large vision-language model-generates high-level intents such as "yield to pedestrian" or "merge after the truck" without compromising latency. HMVLM introduces three upgrades: (1) selective five-view prompting with an embedded 4s history of ego kinematics, (2) multi-stage chain-of-thought (CoT) prompting that enforces a Scene Understanding -> Driving Decision -> Trajectory Inference reasoning flow, and (3) spline-based trajectory post-processing that removes late-stage jitter and sharp turns. Trained on the Waymo Open Dataset, these upgrades enable HMVLM to achieve a Rater Feedback Score (RFS) of 7.7367, securing 2nd place in the 2025 Waymo Vision-based End-to-End (E2E) Driving Challenge and surpassing the public baseline by 2.77%.', 'abstract_zh': '基于视觉-语言模型的 HaoMo 驱动框架：一种认知启发式快速-缓慢架构的端到端驾驶框架', 'title_zh': 'HMVLM：多阶段推理增强的视觉-语言模型用于长尾驾驶场景'}
{'arxiv_id': 'arXiv:2506.05437', 'title': 'A MARL-based Approach for Easing MAS Organization Engineering', 'authors': 'Julien Soulé, Jean-Paul Jamont, Michel Occello, Louis-Marie Traonouez, Paul Théron', 'link': 'https://arxiv.org/abs/2506.05437', 'abstract': "Multi-Agent Systems (MAS) have been successfully applied in industry for their ability to address complex, distributed problems, especially in IoT-based systems. Their efficiency in achieving given objectives and meeting design requirements is strongly dependent on the MAS organization during the engineering process of an application-specific MAS. To design a MAS that can achieve given goals, available methods rely on the designer's knowledge of the deployment environment. However, high complexity and low readability in some deployment environments make the application of these methods to be costly or raise safety concerns. In order to ease the MAS organization design regarding those concerns, we introduce an original Assisted MAS Organization Engineering Approach (AOMEA). AOMEA relies on combining a Multi-Agent Reinforcement Learning (MARL) process with an organizational model to suggest relevant organizational specifications to help in MAS engineering.", 'abstract_zh': '基于多Agent系统组织辅助工程的方法（AOMEA）：一种结合多Agent强化学习的过程', 'title_zh': '基于MARL的方法用于简化MAS组织工程'}
{'arxiv_id': 'arXiv:2506.05428', 'title': 'Diffusion with a Linguistic Compass: Steering the Generation of Clinically Plausible Future sMRI Representations for Early MCI Conversion Prediction', 'authors': 'Zhihao Tang, Chaozhuo Li, Litian Zhang, Xi Zhang', 'link': 'https://arxiv.org/abs/2506.05428', 'abstract': 'Early prediction of Mild Cognitive Impairment (MCI) conversion is hampered by a trade-off between immediacy--making fast predictions from a single baseline sMRI--and accuracy--leveraging longitudinal scans to capture disease progression. We propose MCI-Diff, a diffusion-based framework that synthesizes clinically plausible future sMRI representations directly from baseline data, achieving both real-time risk assessment and high predictive performance. First, a multi-task sequence reconstruction strategy trains a shared denoising network on interpolation and extrapolation tasks to handle irregular follow-up sampling and learn robust latent trajectories. Second, an LLM-driven "linguistic compass" is introduced for clinical plausibility sampling: generated feature candidates are quantized, tokenized, and scored by a fine-tuned language model conditioned on expected structural biomarkers, guiding autoregressive generation toward realistic disease patterns. Experiments on ADNI and AIBL cohorts show that MCI-Diff outperforms state-of-the-art baselines, improving early conversion accuracy by 5-12%.', 'abstract_zh': '基于扩散的MCI转换早期预测框架MCI-Diff：实现实时风险评估与高预测性能', 'title_zh': '以语言为指南的扩散：引导临床合理未来sMRI表示以预测早期MCI转换'}
{'arxiv_id': 'arXiv:2506.05426', 'title': 'Mixture-of-Experts Meets In-Context Reinforcement Learning', 'authors': 'Wenhao Wu, Fuhong Liu, Haoru Li, Zican Hu, Daoyi Dong, Chunlin Chen, Zhi Wang', 'link': 'https://arxiv.org/abs/2506.05426', 'abstract': 'In-context reinforcement learning (ICRL) has emerged as a promising paradigm for adapting RL agents to downstream tasks through prompt conditioning. However, two notable challenges remain in fully harnessing in-context learning within RL domains: the intrinsic multi-modality of the state-action-reward data and the diverse, heterogeneous nature of decision tasks. To tackle these challenges, we propose \\textbf{T2MIR} (\\textbf{T}oken- and \\textbf{T}ask-wise \\textbf{M}oE for \\textbf{I}n-context \\textbf{R}L), an innovative framework that introduces architectural advances of mixture-of-experts (MoE) into transformer-based decision models. T2MIR substitutes the feedforward layer with two parallel layers: a token-wise MoE that captures distinct semantics of input tokens across multiple modalities, and a task-wise MoE that routes diverse tasks to specialized experts for managing a broad task distribution with alleviated gradient conflicts. To enhance task-wise routing, we introduce a contrastive learning method that maximizes the mutual information between the task and its router representation, enabling more precise capture of task-relevant information. The outputs of two MoE components are concatenated and fed into the next layer. Comprehensive experiments show that T2MIR significantly facilitates in-context learning capacity and outperforms various types of baselines. We bring the potential and promise of MoE to ICRL, offering a simple and scalable architectural enhancement to advance ICRL one step closer toward achievements in language and vision communities. Our code is available at this https URL.', 'abstract_zh': '基于令牌和任务的MoE的上下文强化学习（T2MIR）：一种将MoE架构引入基于变换器的决策模型的创新框架', 'title_zh': 'Experts混合体遇上了基于文本的强化学习'}
{'arxiv_id': 'arXiv:2506.05419', 'title': 'Dream to Generalize: Zero-Shot Model-Based Reinforcement Learning for Unseen Visual Distractions', 'authors': 'Jeongsoo Ha, Kyungsoo Kim, Yusung Kim', 'link': 'https://arxiv.org/abs/2506.05419', 'abstract': 'Model-based reinforcement learning (MBRL) has been used to efficiently solve vision-based control tasks in highdimensional image observations. Although recent MBRL algorithms perform well in trained observations, they fail when faced with visual distractions in observations. These task-irrelevant distractions (e.g., clouds, shadows, and light) may be constantly present in real-world scenarios. In this study, we propose a novel self-supervised method, Dream to Generalize (Dr. G), for zero-shot MBRL. Dr. G trains its encoder and world model with dual contrastive learning which efficiently captures task-relevant features among multi-view data augmentations. We also introduce a recurrent state inverse dynamics model that helps the world model to better understand the temporal structure. The proposed methods can enhance the robustness of the world model against visual distractions. To evaluate the generalization performance, we first train Dr. G on simple backgrounds and then test it on complex natural video backgrounds in the DeepMind Control suite, and the randomizing environments in Robosuite. Dr. G yields a performance improvement of 117% and 14% over prior works, respectively. Our code is open-sourced and available at this https URL', 'abstract_zh': '基于模型的强化学习（MBRL）在高维图像观察下的视觉控制任务中已被用于高效求解。尽管近期的MBRL算法在训练观察中表现良好，但在面对观察中的视觉干扰时却会失效。这些与任务无关的干扰（如云、阴影和光线）在现实世界场景中可能会持续存在。在本研究中，我们提出了一种新颖的自监督方法Dr. G，用于零样本MBRL。Dr. G 使用双对比学习训练其编码器和世界模型，该方法能有效地在多视图数据增强中捕获任务相关特征。我们还引入了一个递归状态逆动力学模型，帮助世界模型更好地理解时间结构。所提出的方法可以提高世界模型对视觉干扰的鲁棒性。为了评估泛化性能，我们首先在简单背景上训练Dr. G，然后在DeepMind Control套件和Robosuite中的随机环境下的复杂自然视频背景上进行测试。与先前的工作相比，Dr. G 的性能分别提高了117%和14%。我们的代码已开源，可在以下链接获取。', 'title_zh': '梦中泛化：针对未见过的视觉干扰的零样本模型驱动 reinforcement 学习'}
{'arxiv_id': 'arXiv:2506.05368', 'title': 'Speaking images. A novel framework for the automated self-description of artworks', 'authors': 'Valentine Bernasconi, Gustavo Marfia', 'link': 'https://arxiv.org/abs/2506.05368', 'abstract': 'Recent breakthroughs in generative AI have opened the door to new research perspectives in the domain of art and cultural heritage, where a large number of artifacts have been digitized. There is a need for innovation to ease the access and highlight the content of digital collections. Such innovations develop into creative explorations of the digital image in relation to its malleability and contemporary interpretation, in confrontation to the original historical object. Based on the concept of the autonomous image, we propose a new framework towards the production of self-explaining cultural artifacts using open-source large-language, face detection, text-to-speech and audio-to-animation models. The goal is to start from a digitized artwork and to automatically assemble a short video of the latter where the main character animates to explain its content. The whole process questions cultural biases encapsulated in large-language models, the potential of digital images and deepfakes of artworks for educational purposes, along with concerns of the field of art history regarding such creative diversions.', 'abstract_zh': '近期生成式人工智能的突破为艺术和文化遗产领域的新研究视角打开了大门，其中大量文物已被数字化。需要创新以简化访问并突出数字收藏的内容。这些创新发展成为对数字图像及其可塑性和当代诠释的创造性探索，与原始历史物体相对。基于自主图像的概念，我们提出了一种新的框架，用于生成自我解释的文化艺术品，利用开源的大型语言模型、面部检测、文本转语音和音频到动画模型。目标是从一件数字化的艺术作品开始，自动组装一个简短视频，其中的主要角色会解释其内容。整个过程质疑大型语言模型中嵌入的文化偏见，数字图像和艺术品的深伪技术在教育目的上的潜力，以及艺术史领域对此类创造性的关切。', 'title_zh': '描绘图像：一种自动化艺术作品自我描述的新框架'}
{'arxiv_id': 'arXiv:2506.05171', 'title': 'Towards provable probabilistic safety for scalable embodied AI systems', 'authors': 'Linxuan He, Qing-Shan Jia, Ang Li, Hongyan Sang, Ling Wang, Jiwen Lu, Tao Zhang, Jie Zhou, Yi Zhang, Yisen Wang, Peng Wei, Zhongyuan Wang, Henry X. Liu, Shuo Feng', 'link': 'https://arxiv.org/abs/2506.05171', 'abstract': 'Embodied AI systems, comprising AI models and physical plants, are increasingly prevalent across various applications. Due to the rarity of system failures, ensuring their safety in complex operating environments remains a major challenge, which severely hinders their large-scale deployment in safety-critical domains, such as autonomous vehicles, medical devices, and robotics. While achieving provable deterministic safety--verifying system safety across all possible scenarios--remains theoretically ideal, the rarity and complexity of corner cases make this approach impractical for scalable embodied AI systems. To address this challenge, we introduce provable probabilistic safety, which aims to ensure that the residual risk of large-scale deployment remains below a predefined threshold. Instead of attempting exhaustive safety proof across all corner cases, this paradigm establishes a probabilistic safety boundary on overall system performance, leveraging statistical methods to enhance feasibility and scalability. A well-defined probabilistic safety boundary enables embodied AI systems to be deployed at scale while allowing for continuous refinement of safety guarantees. Our work focuses on three core questions: what is provable probabilistic safety, how to prove the probabilistic safety, and how to achieve the provable probabilistic safety. By bridging the gap between theoretical safety assurance and practical deployment, our work offers a pathway toward safer, large-scale adoption of embodied AI systems in safety-critical applications.', 'abstract_zh': '具身AI系统的可验证概率安全：理论与实践的桥梁', 'title_zh': '可验证的概率安全性朝着可扩展的具身AI系统的方向研究'}
