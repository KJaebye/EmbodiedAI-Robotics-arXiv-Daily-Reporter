{'arxiv_id': 'arXiv:2505.13278', 'title': 'Hybrid Voting-Based Task Assignment in Modular Construction Scenarios', 'authors': 'Daniel Weiner, Raj Korpan', 'link': 'https://arxiv.org/abs/2505.13278', 'abstract': "Modular construction, involving off-site prefabrication and on-site assembly, offers significant advantages but presents complex coordination challenges for robotic automation. Effective task allocation is critical for leveraging multi-agent systems (MAS) in these structured environments. This paper introduces the Hybrid Voting-Based Task Assignment (HVBTA) framework, a novel approach to optimizing collaboration between heterogeneous multi-agent construction teams. Inspired by human reasoning in task delegation, HVBTA uniquely integrates multiple voting mechanisms with the capabilities of a Large Language Model (LLM) for nuanced suitability assessment between agent capabilities and task requirements. The framework operates by assigning Capability Profiles to agents and detailed requirement lists called Task Descriptions to construction tasks, subsequently generating a quantitative Suitability Matrix. Six distinct voting methods, augmented by a pre-trained LLM, analyze this matrix to robustly identify the optimal agent for each task. Conflict-Based Search (CBS) is integrated for decentralized, collision-free path planning, ensuring efficient and safe spatio-temporal coordination of the robotic team during assembly operations. HVBTA enables efficient, conflict-free assignment and coordination, facilitating potentially faster and more accurate modular assembly. Current work is evaluating HVBTA's performance across various simulated construction scenarios involving diverse robotic platforms and task complexities. While designed as a generalizable framework for any domain with clearly definable tasks and capabilities, HVBTA will be particularly effective for addressing the demanding coordination requirements of multi-agent collaborative robotics in modular construction due to the predetermined construction planning involved.", 'abstract_zh': '基于混合投票的任务分配框架：面向模块化建筑的异构多Agent团队合作优化', 'title_zh': '模块化建筑场景下的混合投票制任务分配'}
{'arxiv_id': 'arXiv:2505.13445', 'title': 'Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards', 'authors': 'Xiaoyuan Liu, Tian Liang, Zhiwei He, Jiahao Xu, Wenxuan Wang, Pinjia He, Zhaopeng Tu, Haitao Mi, Dong Yu', 'link': 'https://arxiv.org/abs/2505.13445', 'abstract': "Large Language Models (LLMs) show great promise in complex reasoning, with Reinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement strategy. However, a prevalent issue is ``superficial self-reflection'', where models fail to robustly verify their own outputs. We introduce RISE (Reinforcing Reasoning with Self-Verification), a novel online RL framework designed to tackle this. RISE explicitly and simultaneously trains an LLM to improve both its problem-solving and self-verification abilities within a single, integrated RL process. The core mechanism involves leveraging verifiable rewards from an outcome verifier to provide on-the-fly feedback for both solution generation and self-verification tasks. In each iteration, the model generates solutions, then critiques its own on-policy generated solutions, with both trajectories contributing to the policy update. Extensive experiments on diverse mathematical reasoning benchmarks show that RISE consistently improves model's problem-solving accuracy while concurrently fostering strong self-verification skills. Our analyses highlight the advantages of online verification and the benefits of increased verification compute. Additionally, RISE models exhibit more frequent and accurate self-verification behaviors during reasoning. These advantages reinforce RISE as a flexible and effective path towards developing more robust and self-aware reasoners.", 'abstract_zh': '大型语言模型（LLMs）在复杂推理方面展现出巨大潜力，可信奖励强化学习（RLVR）是关键增强策略之一。然而，一个普遍存在的问题是“表面自我反思”，模型难以稳健地验证自己的输出。我们提出了RISE（强化推理与自我验证）这一新颖的在线RL框架，旨在解决这一问题。RISE在单一集成的RL过程中显式且同时训练LLM，提升其问题解决和自我验证能力。核心机制是利用结果验证器提供的可验证奖励，为解决方案生成和自我验证任务提供实时反馈。在每一轮迭代中，模型生成解决方案，然后对其自身的策略内生成的解决方案进行自我批评，两者共同促进政策更新。在多种数学推理基准上的广泛实验显示，RISE在提高模型问题解决准确性的同时，还提升了其强烈的自我验证能力。我们的分析突显了在线验证的优势以及增加验证计算量的益处。此外，RISE模型在推理过程中表现出更频繁和准确的自我验证行为。这些优势进一步证明了RISE作为开发更稳健和自我意识推理器的灵活有效途径的重要性。', 'title_zh': '互信但求证：一种基于可验证奖励的自我验证强化学习方法'}
{'arxiv_id': 'arXiv:2505.13408', 'title': 'CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process', 'authors': 'Jinhe Bi, Danqi Yan, Yifan Wang, Wenke Huang, Haokun Chen, Guancheng Wan, Mang Ye, Xun Xiao, Hinrich Schuetze, Volker Tresp, Yunpu Ma', 'link': 'https://arxiv.org/abs/2505.13408', 'abstract': "Recent Large Reasoning Models significantly improve the reasoning ability of Large Language Models by learning to reason, exhibiting the promising performance in solving complex tasks. LRMs solve tasks that require complex reasoning by explicitly generating reasoning trajectories together with answers. Nevertheless, judging the quality of such an output answer is not easy because only considering the correctness of the answer is not enough and the soundness of the reasoning trajectory part matters as well. Logically, if the soundness of the reasoning part is poor, even if the answer is correct, the confidence of the derived answer should be low. Existing methods did consider jointly assessing the overall output answer by taking into account the reasoning part, however, their capability is still not satisfactory as the causal relationship of the reasoning to the concluded answer cannot properly reflected. In this paper, inspired by classical mechanics, we present a novel approach towards establishing a CoT-Kinetics energy equation. Specifically, our CoT-Kinetics energy equation formulates the token state transformation process, which is regulated by LRM internal transformer layers, as like a particle kinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy assigns a scalar score to evaluate specifically the soundness of the reasoning phase, telling how confident the derived answer could be given the evaluated reasoning. As such, the LRM's overall output quality can be accurately measured, rather than a coarse judgment (e.g., correct or incorrect) anymore.", 'abstract_zh': 'Recent Large Reasoning Models显著提高大型语言模型的推理能力，通过学习推理并在解决复杂任务中表现出色。LRMs通过显式生成推理轨迹和答案来解决需要复杂推理的任务。然而，评估这种输出答案的质量并不容易，因为仅仅考虑答案的正确性是不够的，推理轨迹部分的正确性也很重要。逻辑上，如果推理部分的正确性较差，即使答案正确，推导出的答案的置信度也应该较低。现有方法虽然尝试通过考虑推理部分来联合评估整体输出答案，但它们的能力仍然不尽如人意，无法准确反映推理与结论之间的因果关系。本文受到经典力学的启发，提出了一种新的方法，旨在建立CoT-Kinetics能量方程。具体而言，我们的CoT-Kinetics能量方程将由LRM内部变压器层调节的标记状态转换过程，类比于在机械场中受治理的粒子动力学。CoT-Kinetics能量为评估推理阶段的正确性分配一个标量分数，告诉在评估的推理下推导出的答案可以有多大的置信度。因此，可以精确测量LRM的整体输出质量，而不仅仅是粗略判断（如正确或错误）。', 'title_zh': 'CoT-Kinetics: 一种评估LRM推理过程的理论建模'}
{'arxiv_id': 'arXiv:2505.13406', 'title': 'AutoMathKG: The automated mathematical knowledge graph based on LLM and vector database', 'authors': 'Rong Bian, Yu Geng, Zijian Yang, Bing Cheng', 'link': 'https://arxiv.org/abs/2505.13406', 'abstract': 'A mathematical knowledge graph (KG) presents knowledge within the field of mathematics in a structured manner. Constructing a math KG using natural language is an essential but challenging task. There are two major limitations of existing works: first, they are constrained by corpus completeness, often discarding or manually supplementing incomplete knowledge; second, they typically fail to fully automate the integration of diverse knowledge sources. This paper proposes AutoMathKG, a high-quality, wide-coverage, and multi-dimensional math KG capable of automatic updates. AutoMathKG regards mathematics as a vast directed graph composed of Definition, Theorem, and Problem entities, with their reference relationships as edges. It integrates knowledge from ProofWiki, textbooks, arXiv papers, and TheoremQA, enhancing entities and relationships with large language models (LLMs) via in-context learning for data augmentation. To search for similar entities, MathVD, a vector database, is built through two designed embedding strategies using SBERT. To automatically update, two mechanisms are proposed. For knowledge completion mechanism, Math LLM is developed to interact with AutoMathKG, providing missing proofs or solutions. For knowledge fusion mechanism, MathVD is used to retrieve similar entities, and LLM is used to determine whether to merge with a candidate or add as a new entity. A wide range of experiments demonstrate the advanced performance and broad applicability of the AutoMathKG system, including superior reachability query results in MathVD compared to five baselines and robust mathematical reasoning capability in Math LLM.', 'abstract_zh': '一种数学知识图谱（KG）以结构化方式呈现数学领域的知识。使用自然语言构建数学KG是一项重要但具有挑战性的任务。现有工作的两个主要限制是：首先，它们受语料库完整性的限制，经常丢弃或手动补充不完整的信息；其次，它们通常无法完全自动化地整合多种知识源。本文提出AutoMathKG，这是一种高质量、覆盖面广、多维度的数学KG，能够自动更新。AutoMathKG将数学视为由定义、定理和问题实体组成的庞大有向图，它们之间的参考关系作为边。它通过使用SBERT设计的嵌入策略构建MathVD向量数据库，并通过上下文学习使用大语言模型（LLMs）增强实体和关系进行数据扩充。为查找相似实体，通过设计的嵌入策略使用SBERT构建MathVD向量数据库。为了自动更新，提出了两种机制。对于知识完成机制，开发了Math LLM与AutoMathKG交互，提供缺失的证明或解决方案。对于知识融合机制，使用MathVD检索相似实体，并使用LLM确定是否将候选实体合并或作为新实体添加。广泛实验展示了AutoMathKG系统的先进性能和广泛应用，包括在MathVD中相比五个基线具有更优秀的可达查询结果和强大的数学推理能力。', 'title_zh': 'AutoMathKG：基于LLM和向量数据库的自动数学知识图谱'}
{'arxiv_id': 'arXiv:2505.13380', 'title': 'CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition', 'authors': 'Nam V. Nguyen, Huy Nguyen, Quang Pham, Van Nguyen, Savitha Ramasamy, Nhat Ho', 'link': 'https://arxiv.org/abs/2505.13380', 'abstract': "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the network's depth or width. However, we argue that effective SMoE training remains challenging because of the suboptimal routing process where experts that perform computation do not directly contribute to the routing process. In this work, we propose competition, a novel mechanism to route tokens to experts with the highest neural response. Theoretically, we show that the competition mechanism enjoys a better sample efficiency than the traditional softmax routing. Furthermore, we develop CompeteSMoE, a simple yet effective algorithm to train large language models by deploying a router to learn the competition policy, thus enjoying strong performances at a low training overhead. Our extensive empirical evaluations on both the visual instruction tuning and language pre-training tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE compared to state-of-the-art SMoE strategies. We have made the implementation available at: this https URL. This work is an improved version of the previous study at arXiv:2402.02526", 'abstract_zh': 'Sparse混合专家模型（SMoE）提供了一种超出增加网络深度或宽度来提升模型复杂性的有吸引力的解决方案。然而，我们认为有效的SMoE训练由于专家进行计算但不直接参与路由过程的次优路由过程而仍然具有挑战性。在本文中，我们提出了一种新颖的竞争机制，用于将令牌路由到具有最高神经响应的专家。理论上，我们证明了竞争机制在样本效率方面优于传统的softmax路由。此外，我们开发了CompeteSMoE，这是一种简单而有效的算法，通过部署一个路由器来学习竞争策略来训练大型语言模型，从而在较低的训练开销下享受良好的性能。我们通过对视觉指令调优和语言预训练任务的广泛实证评估，展示了与当前最先进的SMoE策略相比，CompeteSMoE的有效性、稳健性和可扩展性。我们已将实现代码发布在：this https URL。本工作是arXiv:2402.02526的改进版。', 'title_zh': 'CompeteSMoE —— 统计保证的专家混合训练竞争方法'}
{'arxiv_id': 'arXiv:2505.13355', 'title': 'Multi-Armed Bandits Meet Large Language Models', 'authors': 'Djallel Bouneffouf, Raphael Feraud', 'link': 'https://arxiv.org/abs/2505.13355', 'abstract': 'Bandit algorithms and Large Language Models (LLMs) have emerged as powerful tools in artificial intelligence, each addressing distinct yet complementary challenges in decision-making and natural language processing. This survey explores the synergistic potential between these two fields, highlighting how bandit algorithms can enhance the performance of LLMs and how LLMs, in turn, can provide novel insights for improving bandit-based decision-making. We first examine the role of bandit algorithms in optimizing LLM fine-tuning, prompt engineering, and adaptive response generation, focusing on their ability to balance exploration and exploitation in large-scale learning tasks. Subsequently, we explore how LLMs can augment bandit algorithms through advanced contextual understanding, dynamic adaptation, and improved policy selection using natural language reasoning. By providing a comprehensive review of existing research and identifying key challenges and opportunities, this survey aims to bridge the gap between bandit algorithms and LLMs, paving the way for innovative applications and interdisciplinary research in AI.', 'abstract_zh': '-bandit算法与大语言模型（LLMs）在人工智能领域 emerged as 强大的工具，分别在决策制定和自然语言处理中解决独特而互补的挑战。本文综述探讨了这两个领域之间的协同潜力，强调bandit算法如何提升LLMs的性能，以及LLMs如何为基于bandit的决策制定提供新的见解。我们首先研究了bandit算法在优化LLM微调、提示工程和自适应响应生成中的作用，重点在于它们在大规模学习任务中平衡探索与利用的能力。随后，我们探讨了LLMs如何通过高级语境理解、动态适应和基于自然语言推理改进政策选择来增强bandit算法。通过全面回顾现有研究并识别关键挑战和机遇，本文旨在弥合bandit算法与LLMs之间的差距，为人工智能中的创新应用和跨学科研究铺平道路。', 'title_zh': '多臂 Bandits 遇上大规模语言模型'}
{'arxiv_id': 'arXiv:2505.13246', 'title': 'Agentic Publications: An LLM-Driven Framework for Interactive Scientific Publishing, Supplementing Traditional Papers with AI-Powered Knowledge Systems', 'authors': 'Roberto Pugliese, George Kourousias, Francesco Venier, Grazia Garlatti Costa', 'link': 'https://arxiv.org/abs/2505.13246', 'abstract': 'The exponential growth of scientific literature presents significant challenges for researchers navigating the complex knowledge landscape. We propose "Agentic Publications", a novel LLM-driven framework complementing traditional publishing by transforming papers into interactive knowledge systems. Our architecture integrates structured data with unstructured content through retrieval-augmented generation and multi-agent verification. The framework offers interfaces for both humans and machines, combining narrative explanations with machine-readable outputs while addressing ethical considerations through automated validation and transparent governance. Key features include continuous knowledge updates, automatic integration of new findings, and customizable detail levels. Our proof-of-concept demonstrates multilingual interaction, API accessibility, and structured knowledge representation through vector databases, knowledge graphs, and verification agents. This approach enhances scientific communication across disciplines, improving efficiency and collaboration while preserving traditional publishing pathways, particularly valuable for interdisciplinary fields where knowledge integration remains challenging.', 'abstract_zh': '科学文献的指数增长为研究人员导航复杂知识景观带来了显著挑战。我们提出“自主出版”这一新型LLM驱动框架，通过将论文转化为互动知识系统来补充传统出版方式。该架构通过检索增强生成和多代理验证将结构化数据与非结构化内容相结合。该框架为人类和机器提供接口，结合叙述性解释与机器可读输出，并通过自动化验证和透明治理解决伦理考量。关键功能包括持续的知识更新、自动整合新发现以及可定制的详细级别。概念验证展示了多语言交互、API访问能力和通过向量数据库、知识图谱和验证代理呈现结构化知识。该方法增强跨学科的科学交流，提高效率与协作，同时保留传统出版路径，特别是在知识整合仍具挑战性的跨学科领域尤为重要。', 'title_zh': '代理出版物：一种由LLM驱动的交互式科学出版框架，以AI增强的知识系统补充传统论文'}
{'arxiv_id': 'arXiv:2505.13195', 'title': 'Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities', 'authors': 'Lili Zhang, Haomiaomiao Wang, Long Cheng, Libao Deng, Tomas Ward', 'link': 'https://arxiv.org/abs/2505.13195', 'abstract': 'As Large Language Models (LLMs) become increasingly integrated into real-world decision-making systems, understanding their behavioural vulnerabilities remains a critical challenge for AI safety and alignment. While existing evaluation metrics focus primarily on reasoning accuracy or factual correctness, they often overlook whether LLMs are robust to adversarial manipulation or capable of using adaptive strategy in dynamic environments. This paper introduces an adversarial evaluation framework designed to systematically stress-test the decision-making processes of LLMs under interactive and adversarial conditions. Drawing on methodologies from cognitive psychology and game theory, our framework probes how models respond in two canonical tasks: the two-armed bandit task and the Multi-Round Trust Task. These tasks capture key aspects of exploration-exploitation trade-offs, social cooperation, and strategic flexibility. We apply this framework to several state-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3, revealing model-specific susceptibilities to manipulation and rigidity in strategy adaptation. Our findings highlight distinct behavioral patterns across models and emphasize the importance of adaptability and fairness recognition for trustworthy AI deployment. Rather than offering a performance benchmark, this work proposes a methodology for diagnosing decision-making weaknesses in LLM-based agents, providing actionable insights for alignment and safety research.', 'abstract_zh': '随着大型语言模型（LLMs）越来越多地集成到实际决策系统中，理解其行为漏洞仍然是人工智能安全和对齐中的关键挑战。虽然现有评估指标主要关注推理准确度或事实准确性，但往往忽视了LLMs对对抗操纵的鲁棒性或在动态环境中使用适应策略的能力。本文介绍了一种对抗性评估框架，旨在在交互和对抗条件下系统地测试LLMs的决策过程。该框架借鉴了认知心理学和博弈论的方法，探讨了模型在经典任务中的响应方式：两臂-bandit任务和多轮信任任务。这些任务捕捉了探索与利用之间的权衡、社会合作以及策略灵活性的关键方面。我们将这一框架应用于包括GPT-3.5、GPT-4、Gemini-1.5和DeepSeek-V3在内的多个最新大语言模型，揭示了模型特定的操纵脆弱性和策略适应僵化。我们的发现突显了模型之间不同的行为模式，并强调了在可信的人工智能部署中适应性和公平性识别的重要性。本文不提供性能基准，而是提出了一种诊断基于大语言模型的智能体决策弱点的方法，为对齐和安全性研究提供了可操作的见解。', 'title_zh': 'LLM中的对抗性测试：决策漏洞洞察'}
{'arxiv_id': 'arXiv:2505.13175', 'title': 'Enhancing LLMs for Time Series Forecasting via Structure-Guided Cross-Modal Alignment', 'authors': 'Siming Sun, Kai Zhang, Xuejun Jiang, Wenchao Meng, Qinmin Yang', 'link': 'https://arxiv.org/abs/2505.13175', 'abstract': 'The emerging paradigm of leveraging pretrained large language models (LLMs) for time series forecasting has predominantly employed linguistic-temporal modality alignment strategies through token-level or layer-wise feature mapping. However, these approaches fundamentally neglect a critical insight: the core competency of LLMs resides not merely in processing localized token features but in their inherent capacity to model holistic sequence structures. This paper posits that effective cross-modal alignment necessitates structural consistency at the sequence level. We propose the Structure-Guided Cross-Modal Alignment (SGCMA), a framework that fully exploits and aligns the state-transition graph structures shared by time-series and linguistic data as sequential modalities, thereby endowing time series with language-like properties and delivering stronger generalization after modality alignment. SGCMA consists of two key components, namely Structure Alignment and Semantic Alignment. In Structure Alignment, a state transition matrix is learned from text data through Hidden Markov Models (HMMs), and a shallow transformer-based Maximum Entropy Markov Model (MEMM) receives the hot-start transition matrix and annotates each temporal patch into state probability, ensuring that the temporal representation sequence inherits language-like sequential dynamics. In Semantic Alignment, cross-attention is applied between temporal patches and the top-k tokens within each state, and the ultimate temporal embeddings are derived by the expected value of these embeddings using a weighted average based on state probabilities. Experiments on multiple benchmarks demonstrate that SGCMA achieves state-of-the-art performance, offering a novel approach to cross-modal alignment in time series forecasting.', 'abstract_zh': '基于结构导向的跨模态对齐框架（SGCMA）：提升时间序列预测中的语言时间序列属性', 'title_zh': '通过结构引导的跨模态对齐增强LLMs的时间序列预测能力'}
{'arxiv_id': 'arXiv:2505.13126', 'title': 'Zero-Shot Iterative Formalization and Planning in Partially Observable Environments', 'authors': 'Liancheng Gong, Wang Zhu, Jesse Thomason, Li Zhang', 'link': 'https://arxiv.org/abs/2505.13126', 'abstract': 'In planning, using LLMs not to predict plans but to formalize an environment into the Planning Domain Definition Language (PDDL) has been shown to greatly improve performance and control. While most work focused on fully observable environments, we tackle the more realistic and challenging partially observable environments where existing methods are incapacitated by the lack of complete information. We propose PDDLego+, a framework to iteratively formalize, plan, grow, and refine PDDL representations in a zero-shot manner, without needing access to any existing trajectories. On two textual simulated environments, we show that PDDLego+ not only achieves superior performance, but also shows robustness against problem complexity. We also show that the domain knowledge captured after a successful trial is interpretable and benefits future tasks.', 'abstract_zh': '使用LLM将环境形式化为PDDL以提高规划性能和控制：PDDLego+框架在部分可观测环境中的应用', 'title_zh': '零样本迭代形式化与规划在部分可观测环境中'}
{'arxiv_id': 'arXiv:2505.13098', 'title': 'LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs', 'authors': 'Lars-Peter Meyer, Johannes Frey, Desiree Heim, Felix Brei, Claus Stadler, Kurt Junghanns, Michael Martin', 'link': 'https://arxiv.org/abs/2505.13098', 'abstract': "Current Large Language Models (LLMs) can assist developing program code beside many other things, but can they support working with Knowledge Graphs (KGs) as well? Which LLM is offering the best capabilities in the field of Semantic Web and Knowledge Graph Engineering (KGE)? Is this possible to determine without checking many answers manually? The LLM-KG-Bench framework in Version 3.0 is designed to answer these questions. It consists of an extensible set of tasks for automated evaluation of LLM answers and covers different aspects of working with semantic technologies. In this paper the LLM-KG-Bench framework is presented in Version 3 along with a dataset of prompts, answers and evaluations generated with it and several state-of-the-art LLMs. Significant enhancements have been made to the framework since its initial release, including an updated task API that offers greater flexibility in handling evaluation tasks, revised tasks, and extended support for various open models through the vllm library, among other improvements. A comprehensive dataset has been generated using more than 30 contemporary open and proprietary LLMs, enabling the creation of exemplary model cards that demonstrate the models' capabilities in working with RDF and SPARQL, as well as comparing their performance on Turtle and JSON-LD RDF serialization tasks.", 'abstract_zh': '当前大型语言模型（LLMs）可以在开发程序代码等方面提供帮助，但它们能否支持与知识图谱（KGs）的交互呢？哪些LLM在语义网和知识图谱工程（KGE）领域提供了最好的能力？我们能否在不手动检查多个答案的情况下做出判断？LLM-KG-Bench框架（版本3.0）旨在回答这些问题。它包括一组可扩展的任务，用于自动评估LLM的答案，并涵盖与语义技术交互的不同方面。本文介绍了LLM-KG-Bench框架（版本3），并提供了一个使用该框架及其几种最先进的LLM生成的数据集。自首次发布以来，该框架进行了显著增强，包括更新的任务API，提供了更大的灵活性以处理评估任务，修订的任务，以及通过vllm库扩展对各种开源模型的支持等改进。使用超过30种当代开源和专有LLM生成了一个全面的数据集，使创建展示模型在处理RDF和SPARQL以及比较其在turtle和JSON-LD RDF序列化任务上性能的示例模型卡片成为可能。', 'title_zh': 'LLM-KG-Bench 3.0：导航LLM海洋中语义技术能力的指南针'}
{'arxiv_id': 'arXiv:2505.12923', 'title': 'The Traitors: Deception and Trust in Multi-Agent Language Model Simulations', 'authors': 'Pedro M. P. Curvo', 'link': 'https://arxiv.org/abs/2505.12923', 'abstract': "As AI systems increasingly assume roles where trust and alignment with human values are essential, understanding when and why they engage in deception has become a critical research priority. We introduce The Traitors, a multi-agent simulation framework inspired by social deduction games, designed to probe deception, trust formation, and strategic communication among large language model (LLM) agents under asymmetric information. A minority of agents the traitors seek to mislead the majority, while the faithful must infer hidden identities through dialogue and reasoning. Our contributions are: (1) we ground the environment in formal frameworks from game theory, behavioral economics, and social cognition; (2) we develop a suite of evaluation metrics capturing deception success, trust dynamics, and collective inference quality; (3) we implement a fully autonomous simulation platform where LLMs reason over persistent memory and evolving social dynamics, with support for heterogeneous agent populations, specialized traits, and adaptive behaviors. Our initial experiments across DeepSeek-V3, GPT-4o-mini, and GPT-4o (10 runs per model) reveal a notable asymmetry: advanced models like GPT-4o demonstrate superior deceptive capabilities yet exhibit disproportionate vulnerability to others' falsehoods. This suggests deception skills may scale faster than detection abilities. Overall, The Traitors provides a focused, configurable testbed for investigating LLM behavior in socially nuanced interactions. We position this work as a contribution toward more rigorous research on deception mechanisms, alignment challenges, and the broader social reliability of AI systems.", 'abstract_zh': '随着AI系统在需要信任和与人类价值观对齐的领域中扮演越来越重要的角色，了解何时以及为何它们进行欺骗已成为一项至关重要的研究优先事项。我们介绍了《叛徒》，一个受社交推理游戏启发的多agent仿真框架，旨在探究在不对称信息条件下大语言模型（LLM）agent之间的欺骗、信任形成和战略沟通。少数“叛徒”试图误导多数人，而“忠诚者”则需要通过对话和推理来推断隐藏的身份。我们的贡献包括：（1）我们将环境建立在博弈论、行为经济学和社会认知的正式框架之上；（2）我们开发了一套评估指标，用于捕捉欺骗成功率、信任动态和集体推理质量；（3）我们实现了一个完全自主的仿真平台，其中LLM能够在持续记忆和不断变化的社会动态背景下进行推理，并支持异质性agent群体、专门技能和适应性行为。我们在DeepSeek-V3、GPT-4o-mini和GPT-4o（每种模型运行10次）上的初始实验揭示了一个显著的不对称性：如GPT-4o这样的先进模型展示了卓越的欺骗能力，但对其他人的虚假信息表现出不成比例的脆弱性。这表明欺骗技能的增长可能比检测技能更快。总体而言，《叛徒》提供了一个专注于社会细微互动中LLM行为的研究平台。我们将这项工作定位为在欺骗机制研究、对齐挑战研究及其更广泛的AI系统社会可靠性方面更具严谨性研究的贡献。', 'title_zh': '叛徒：多Agent语言模型仿真中的欺骗与信任'}
{'arxiv_id': 'arXiv:2505.12891', 'title': 'TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios', 'authors': 'Shaohang Wei, Wei Li, Feifan Song, Wen Luo, Tianyi Zhuang, Haochen Tan, Zhijiang Guo, Houfeng Wang', 'link': 'https://arxiv.org/abs/2505.12891', 'abstract': 'Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend the real world. However, existing works neglect the real-world challenges for temporal reasoning: (1) intensive temporal information, (2) fast-changing event dynamics, and (3) complex temporal dependencies in social interactions. To bridge this gap, we propose a multi-level benchmark TIME, designed for temporal reasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3 levels with 11 fine-grained sub-tasks. This benchmark encompasses 3 sub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News, and TIME-Dial. We conduct extensive experiments on reasoning models and non-reasoning models. And we conducted an in-depth analysis of temporal reasoning performance across diverse real-world scenarios and tasks, and summarized the impact of test-time scaling on temporal reasoning capabilities. Additionally, we release TIME-Lite, a human-annotated subset to foster future research and standardized evaluation in temporal reasoning. The code is available at this https URL , and the dataset is available at this https URL .', 'abstract_zh': '时空推理对于大规模语言模型（LLMs）理解现实世界至关重要。然而，现有工作忽略了时空推理的实际挑战：（1）密集的时空信息，（2）快速变化的事件动态，以及（3）社会互动中的复杂时空依赖关系。为了弥合这一差距，我们提出了一个多层基准TIME，用于现实世界场景中的时空推理。TIME包含38,522个问答对，涵盖3个层次并包含11个细粒度子任务。该基准数据集包括3个子数据集，分别反映不同的现实世界挑战：TIME-Wiki、TIME-News和TIME-Dial。我们在推理模型和非推理模型上进行了广泛的实验，并对时空推理性能在各种现实世界场景和任务中的表现进行了深入分析，总结了测试时扩展对时空推理能力的影响。此外，我们发布了TIME-Lite，这是一个手工标注的子集，旨在促进未来在时空推理领域的研究和标准化评估。相关代码可通过以下链接获取：this https URL，数据集可通过以下链接获取：this https URL。', 'title_zh': 'TIME：一种面向真实场景的LLM时间推理多层级基准'}
{'arxiv_id': 'arXiv:2505.12886', 'title': 'Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective', 'authors': 'Zhongxiang Sun, Qipeng Wang, Haoyu Wang, Xiao Zhang, Jun Xu', 'link': 'https://arxiv.org/abs/2505.12886', 'abstract': 'Large Reasoning Models (LRMs) have shown impressive capabilities in multi-step reasoning tasks. However, alongside these successes, a more deceptive form of model error has emerged--Reasoning Hallucination--where logically coherent but factually incorrect reasoning traces lead to persuasive yet faulty conclusions. Unlike traditional hallucinations, these errors are embedded within structured reasoning, making them more difficult to detect and potentially more harmful. In this work, we investigate reasoning hallucinations from a mechanistic perspective. We propose the Reasoning Score, which quantifies the depth of reasoning by measuring the divergence between logits obtained from projecting late layers of LRMs to the vocabulary space, effectively distinguishing shallow pattern-matching from genuine deep reasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA dataset and identify two key reasoning hallucination patterns: early-stage fluctuation in reasoning depth and incorrect backtracking to flawed prior steps. These insights motivate our Reasoning Hallucination Detection (RHD) framework, which achieves state-of-the-art performance across multiple domains. To mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced reinforcement learning algorithm that incorporates step-level deep reasoning rewards via potential-based shaping. Our theoretical analysis establishes stronger generalization guarantees, and experiments demonstrate improved reasoning quality and reduced hallucination rates.', 'abstract_zh': '大型推理模型中的推理幻觉：机制分析与检测', 'title_zh': '大型推理模型中的幻觉检测与缓解：一种机理视角'}
{'arxiv_id': 'arXiv:2505.12833', 'title': 'Reasoning BO: Enhancing Bayesian Optimization with Long-Context Reasoning Power of LLMs', 'authors': 'Zhuo Yang, Lingli Ge, Dong Han, Tianfan Fu, Yuqiang Li', 'link': 'https://arxiv.org/abs/2505.12833', 'abstract': 'Many real-world scientific and industrial applications require the optimization of expensive black-box functions. Bayesian Optimization (BO) provides an effective framework for such problems. However, traditional BO methods are prone to get trapped in local optima and often lack interpretable insights. To address this issue, this paper designs Reasoning BO, a novel framework that leverages reasoning models to guide the sampling process in BO while incorporating multi-agent systems and knowledge graphs for online knowledge accumulation. By integrating the reasoning and contextual understanding capabilities of Large Language Models (LLMs), we can provide strong guidance to enhance the BO process. As the optimization progresses, Reasoning BO provides real-time sampling recommendations along with critical insights grounded in plausible scientific theories, aiding in the discovery of superior solutions within the search space. We systematically evaluate our approach across 10 diverse tasks encompassing synthetic mathematical functions and complex real-world applications. The framework demonstrates its capability to progressively refine sampling strategies through real-time insights and hypothesis evolution, effectively identifying higher-performing regions of the search space for focused exploration. This process highlights the powerful reasoning and context-learning abilities of LLMs in optimization scenarios. For example, in the Direct Arylation task, our method increased the yield to 60.7%, whereas traditional BO achieved only a 25.2% yield. Furthermore, our investigation reveals that smaller LLMs, when fine-tuned through reinforcement learning, can attain comparable performance to their larger counterparts. This enhanced reasoning capability paves the way for more efficient automated scientific experimentation while maintaining computational feasibility.', 'abstract_zh': '多实例学习的贝叶斯优化：一种基于推理的框架', 'title_zh': 'LLM长上下文推理增强的Bayesian优化'}
{'arxiv_id': 'arXiv:2505.12822', 'title': 'Emergent Specialization: Rare Token Neurons in Language Models', 'authors': 'Jing Liu, Haozheng Wang, Yueheng Li', 'link': 'https://arxiv.org/abs/2505.12822', 'abstract': "Large language models struggle with representing and generating rare tokens despite their importance in specialized domains. In this study, we identify neuron structures with exceptionally strong influence on language model's prediction of rare tokens, termed as rare token neurons, and investigate the mechanism for their emergence and behavior. These neurons exhibit a characteristic three-phase organization (plateau, power-law, and rapid decay) that emerges dynamically during training, evolving from a homogeneous initial state to a functionally differentiated architecture. In the activation space, rare token neurons form a coordinated subnetwork that selectively co-activates while avoiding co-activation with other neurons. This functional specialization potentially correlates with the development of heavy-tailed weight distributions, suggesting a statistical mechanical basis for emergent specialization.", 'abstract_zh': '大型语言模型在表示和生成稀有令牌方面存在困难，尽管这些令牌在专门领域中非常重要。在本研究中，我们识别出对语言模型预测稀有令牌具有异常强大影响的神经元结构，称为稀有令牌神经元，并探讨其产生机制和行为。这些神经元展示出一种动态出现的三阶段组织特征（平台期、幂律分布期和快速衰减期），从初始的同质状态进化为功能分化结构。在激活空间中，稀有令牌神经元形成一个协调子网络，在激活时选择性地协同激活而不与其他神经元发生共激活。这种功能专业化可能与重尾权重分布的发展相关，表明可能存在统计力学基础的自发专业化机制。', 'title_zh': 'emergent specialization: 语言模型中的 Rarity Token 神经元'}
{'arxiv_id': 'arXiv:2505.12795', 'title': 'FRAbench and GenEval: Scaling Fine-Grained Aspect Evaluation across Tasks, Modalities', 'authors': 'Shibo Hong, Jiahao Ying, Haiyuan Liang, Mengdi Zhang, Jun Kuang, Jiazheng Zhang, Yixin Cao', 'link': 'https://arxiv.org/abs/2505.12795', 'abstract': 'Evaluating the open-ended outputs of large language models (LLMs) has become a bottleneck as model capabilities, task diversity, and modality coverage rapidly expand. Existing "LLM-as-a-Judge" evaluators are typically narrow in a few tasks, aspects, or modalities, and easily suffer from low consistency. In this paper, we argue that explicit, fine-grained aspect specification is the key to both generalizability and objectivity in automated evaluation. To do so, we introduce a hierarchical aspect taxonomy spanning 112 aspects that unifies evaluation across four representative settings - Natural Language Generation, Image Understanding, Image Generation, and Interleaved Text-and-Image Generation. Building on this taxonomy, we create FRAbench, a benchmark comprising 60.4k pairwise samples with 325k aspect-level labels obtained from a combination of human and LLM annotations. FRAbench provides the first large-scale, multi-modal resource for training and meta-evaluating fine-grained LMM judges. Leveraging FRAbench, we develop GenEval, a fine-grained evaluator generalizable across tasks and modalities. Experiments show that GenEval (i) attains high agreement with GPT-4o and expert annotators, (ii) transfers robustly to unseen tasks and modalities, and (iii) reveals systematic weaknesses of current LMMs on evaluation.', 'abstract_zh': '评估大型语言模型的开放式输出已成为瓶颈，随着模型能力、任务多样性和模态覆盖范围的迅速扩展。现有的“作为评判者的大型语言模型”评估者通常在少数任务、方面或模态上狭窄，且容易遭受一致性低的影响。在本文中，我们argueExplicit、细粒度方面的明确指定是自动评估中广泛适用性和客观性的关键。为此，我们引入了一个涵盖112个方面的层次结构方面的分类系统，该系统统一了四个代表性设置——自然语言生成、图像理解、图像生成和交替的文本和图像生成的评估。基于这一分类系统，我们创建了FRAbench，这是一个包含60,400对样本的基准，共有325,000个方面的标签，这些标签是通过结合人力和LLM标注获得的。FRAbench提供了首个大型、多模态资源，用于训练和元评估细粒度的LMM评判者。利用FRAbench，我们开发了GenEval，这是一种适用于跨任务和模态的细粒度评估器。实验显示，GenEval (i) 与GPT-4o和专家标注者高度一致，(ii) 能够稳健地转移到未见过的任务和模态上，以及(iii) 揭示了当前LMMs在评估中的系统性弱点。', 'title_zh': 'FRAbench 和 GenEval：跨任务和模态的细粒度方面评价扩展'}
{'arxiv_id': 'arXiv:2505.12767', 'title': 'Language Models That Walk the Talk: A Framework for Formal Fairness Certificates', 'authors': 'Danqing Chen, Tobias Ladner, Ahmed Rayen Mhadhbi, Matthias Althoff', 'link': 'https://arxiv.org/abs/2505.12767', 'abstract': 'As large language models become integral to high-stakes applications, ensuring their robustness and fairness is critical. Despite their success, large language models remain vulnerable to adversarial attacks, where small perturbations, such as synonym substitutions, can alter model predictions, posing risks in fairness-critical areas, such as gender bias mitigation, and safety-critical areas, such as toxicity detection. While formal verification has been explored for neural networks, its application to large language models remains limited. This work presents a holistic verification framework to certify the robustness of transformer-based language models, with a focus on ensuring gender fairness and consistent outputs across different gender-related terms. Furthermore, we extend this methodology to toxicity detection, offering formal guarantees that adversarially manipulated toxic inputs are consistently detected and appropriately censored, thereby ensuring the reliability of moderation systems. By formalizing robustness within the embedding space, this work strengthens the reliability of language models in ethical AI deployment and content moderation.', 'abstract_zh': '随着大型语言模型在高风险应用中的作用日益重要，确保其稳健性和公平性至关重要。尽管取得了成功，但大型语言模型仍易受到对抗性攻击的威胁，如同义词替换等微小的扰动均可改变模型预测，这在性别偏见缓解和毒性检测等关键领域带来了风险。虽然对神经网络已进行了形式化验证探索，但其在大型语言模型中的应用仍受到限制。本文提出了一种整体验证框架，以验证基于变换器的语言模型的稳健性，重点关注性别公平性和不同性别相关术语的一致输出。此外，我们还将该方法扩展到毒性检测，提供了形式化的保证，即对抗性操纵的有毒输入能够一致地被检测并适当过滤，从而确保内容审核系统的可靠性。通过在嵌入空间内形式化稳健性，本文增强了在伦理人工智能部署和内容审核中语言模型的可靠性。', 'title_zh': '语言模型不仅要会话还要身体力行：正式公平性证书的框架'}
{'arxiv_id': 'arXiv:2505.12762', 'title': 'IDEAL: Data Equilibrium Adaptation for Multi-Capability Language Model Alignment', 'authors': 'Chenlin Ming, Chendi Qu, Mengzhang Cai, Qizhi Pei, Zhuoshi Pan, Yu Li, Xiaoming Duan, Lijun Wu, Conghui He', 'link': 'https://arxiv.org/abs/2505.12762', 'abstract': "Large Language Models (LLMs) have achieved impressive performance through Supervised Fine-tuning (SFT) on diverse instructional datasets. When training on multiple capabilities simultaneously, the mixture training dataset, governed by volumes of data from different domains, is a critical factor that directly impacts the final model's performance. Unlike many studies that focus on enhancing the quality of training datasets through data selection methods, few works explore the intricate relationship between the compositional quantity of mixture training datasets and the emergent capabilities of LLMs. Given the availability of a high-quality multi-domain training dataset, understanding the impact of data from each domain on the model's overall capabilities is crucial for preparing SFT data and training a well-balanced model that performs effectively across diverse domains. In this work, we introduce IDEAL, an innovative data equilibrium adaptation framework designed to effectively optimize volumes of data from different domains within mixture SFT datasets, thereby enhancing the model's alignment and performance across multiple capabilities. IDEAL employs a gradient-based approach to iteratively refine the training data distribution, dynamically adjusting the volumes of domain-specific data based on their impact on downstream task performance. By leveraging this adaptive mechanism, IDEAL ensures a balanced dataset composition, enabling the model to achieve robust generalization and consistent proficiency across diverse tasks. Experiments across different capabilities demonstrate that IDEAL outperforms conventional uniform data allocation strategies, achieving a comprehensive improvement of approximately 7% in multi-task evaluation scores.", 'abstract_zh': '大型语言模型（LLMs）通过多样化指令数据集的监督微调（SFT）取得了 impressive 的性能。当同时训练多种能力时，由不同领域大量数据组成的混合训练数据集是直接影响最终模型性能的关键因素。与许多专注于通过数据选择方法提升训练数据质量的研究不同，很少有工作探索混合训练数据集组成数量与LLMs新兴能力之间的复杂关系。鉴于高质量多领域训练数据集的可用性，理解来自每个领域的数据对模型整体能力的影响对于准备SFT数据和训练一种在各种领域中表现良好的平衡模型至关重要。在本项工作中，我们引入了IDEAL，一种创新的数据均衡适应框架，旨在有效优化混合SFT数据集中不同领域数据的量，从而增强模型在多个能力上的对齐度和性能。IDEAL采用基于梯度的方法，迭代细化训练数据分布，根据其对下游任务性能的影响动态调整特定领域数据的量。通过利用这一适应机制，IDEAL确保数据集组成平衡，使模型在多种任务中实现稳健泛化和一致的专业技能。不同能力的实验表明，与传统均匀数据分配策略相比，IDEAL在多任务评估得分上取得了全面改进，约提高了7%。', 'title_zh': 'IDEAL：数据均衡适应多能力语言模型对齐'}
{'arxiv_id': 'arXiv:2505.12746', 'title': 'Correspondence of high-dimensional emotion structures elicited by video clips between humans and Multimodal LLMs', 'authors': 'Haruka Asanuma, Naoko Koide-Majima, Ken Nakamura, Takato Horii, Shinji Nishimoto, Masafumi Oizumi', 'link': 'https://arxiv.org/abs/2505.12746', 'abstract': 'Recent studies have revealed that human emotions exhibit a high-dimensional, complex structure. A full capturing of this complexity requires new approaches, as conventional models that disregard high dimensionality risk overlooking key nuances of human emotions. Here, we examined the extent to which the latest generation of rapidly evolving Multimodal Large Language Models (MLLMs) capture these high-dimensional, intricate emotion structures, including capabilities and limitations. Specifically, we compared self-reported emotion ratings from participants watching videos with model-generated estimates (e.g., Gemini or GPT). We evaluated performance not only at the individual video level but also from emotion structures that account for inter-video relationships. At the level of simple correlation between emotion structures, our results demonstrated strong similarity between human and model-inferred emotion structures. To further explore whether the similarity between humans and models is at the signle item level or the coarse-categorical level, we applied Gromov Wasserstein Optimal Transport. We found that although performance was not necessarily high at the strict, single-item level, performance across video categories that elicit similar emotions was substantial, indicating that the model could infer human emotional experiences at the category level. Our results suggest that current state-of-the-art MLLMs broadly capture the complex high-dimensional emotion structures at the category level, as well as their apparent limitations in accurately capturing entire structures at the single-item level.', 'abstract_zh': '最近的研究揭示了人类情绪具有高维度、复杂的结构。全面捕捉这种复杂性需要新的方法，因为忽略高维度的传统模型可能会遗漏人类情绪的关键微妙之处。本研究考察了最新一代快速演化的多模态大型语言模型（MLLMs）在捕捉这些高维度、复杂情绪结构方面的能力及其局限性，包括具体的能力和局限。我们比较了参与者观看视频时自我报告的情绪评分与模型生成的估计值（如Gemini或GPT）。我们不仅在单个视频层面评估性能，还在考虑到视频间关系的情绪结构层面进行评估。在情绪结构简单相关性的层面，我们的结果显示人类和模型推断的情绪结构之间存在强烈的相似性。为进一步探索人类和模型之间的相似性是出现在单一项目层面还是粗糙类别层面，我们应用了Gromov Wasserstein最优传输。我们发现虽然在严格的单一项目层面表现未必很高，但在引起相似情绪的视频类别层面的表现却是显著的，表明模型可以在类别层面推断人类的情绪体验。我们的研究结果表明，当前最先进的MLLMs在类别层面广泛地捕捉了复杂高维度的情绪结构及其在单一项目层面准确捕捉整个结构的明显局限性。', 'title_zh': '由视频片段引发的高维情绪结构在人类和多模态LLM之间的对应关系'}
{'arxiv_id': 'arXiv:2505.12741', 'title': 'Dense Communication between Language Models', 'authors': 'Shiguang Wu, Yaqing Wang, Quanming Yao', 'link': 'https://arxiv.org/abs/2505.12741', 'abstract': 'As higher-level intelligence emerges from the combination of modular components with lower-level intelligence, many works combines Large Language Models (LLMs) for collective intelligence. Such combination is achieved by building communications among LLMs. While current systems primarily facilitate such communication through natural language, this paper proposes a novel paradigm of direct dense vector communication between LLMs. Our approach eliminates the unnecessary embedding and de-embedding steps when LLM interact with another, enabling more efficient information transfer, fully differentiable optimization pathways, and exploration of capabilities beyond human heuristics. We use such stripped LLMs as vertexes and optimizable seq2seq modules as edges to construct LMNet, with similar structure as MLPs. By utilizing smaller pre-trained LLMs as vertexes, we train a LMNet that achieves comparable performance with LLMs in similar size with only less than 0.1% training cost. This offers a new perspective on scaling for general intelligence rather than training a monolithic LLM from scratch. Besides, the proposed method can be used for other applications, like customizing LLM with limited data, showing its versatility.', 'abstract_zh': '更高层次智能从低层次智能的模块化组件结合中 emerge，许多研究结合大型语言模型（LLMs）以实现集体智能。这种结合通过在LLMs之间构建通信来实现。虽然当前系统主要通过自然语言促进这种通信，本文提出了一种新的LLM之间直接密集向量通信范式。我们的方法在LLM相互交互时消除了不必要的嵌入和反嵌入步骤，从而使信息传递更高效，优化路径完全可微，且能够探索超越人类启发式的方法。我们使用这样的精简LLM作为节点，可优化的序列到序列模块作为边，构建LMNet，其结构类似于MLP。通过使用较小的预训练LLM作为节点，我们训练了一个LMNet，其性能与相似大小的LLM相当，训练成本仅少于0.1%。这为通用智能的扩展提供了一个新的视角，而不是从头训练一个巨量的LLM。此外，所提出的方法还可以用于其他应用，如有限数据下的LLM定制，显示出其灵活性。', 'title_zh': '语言模型之间的密集通信'}
{'arxiv_id': 'arXiv:2505.12731', 'title': 'Accelerating Adaptive Retrieval Augmented Generation via Instruction-Driven Representation Reduction of Retrieval Overlaps', 'authors': 'Jie Ou, Jinyu Guo, Shuaihong Jiang, Zhaokun Wang, Libo Qin, Shunyu Yao, Wenhong Tian', 'link': 'https://arxiv.org/abs/2505.12731', 'abstract': 'Retrieval-augmented generation (RAG) has emerged as a pivotal method for expanding the knowledge of large language models. To handle complex queries more effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the generated quality through multiple interactions with external knowledge bases. Despite its effectiveness, A-RAG exacerbates the pre-existing efficiency challenges inherent in RAG, which are attributable to its reliance on multiple iterations of generation. Existing A-RAG approaches process all retrieved contents from scratch. However, they ignore the situation where there is a significant overlap in the content of the retrieval results across rounds. The overlapping content is redundantly represented, which leads to a large proportion of repeated computations, thus affecting the overall efficiency. To address this issue, this paper introduces a model-agnostic approach that can be generally applied to A-RAG methods, which is dedicated to reducing the redundant representation process caused by the overlapping of retrieval results. Specifically, we use cache access and parallel generation to speed up the prefilling and decoding stages respectively. Additionally, we also propose an instruction-driven module to further guide the model to more effectively attend to each part of the content in a more suitable way for LLMs. Experiments show that our approach achieves 2.79 and 2.33 times significant acceleration on average for prefilling and decoding respectively while maintaining equal generation quality.', 'abstract_zh': '基于检索的生成增强（RAG）方法已成为扩展大型语言模型知识的关键技术。为更有效地处理复杂查询，研究人员开发了自适应RAG（A-RAG）以通过多次与外部知识库的交互来提升生成质量。尽管A-RAG方法有效，但它加剧了RAG固有的效率挑战，这些挑战归因于其对多次生成迭代的依赖。现有的A-RAG方法会从头处理所有检索到的内容，但它们忽略了检索结果在多轮中有显著重叠的情况。重叠的内容冗余表示，导致大量重复计算，从而影响整体效率。为解决这一问题，本文提出了一种模型无关的方法，该方法可适用于A-RAG方法，专门用于减少由检索结果重叠引起的冗余表示过程。具体地，我们利用缓存访问和并行生成分别加速预填充和解码阶段。此外，我们还提出了一种指令驱动模块，进一步指导模型以更适合的大规模语言模型（LLMs）的方式更加有效地关注内容的每个部分。实验表明，我们的方法在预填充和解码阶段分别平均加速了2.79倍和2.33倍，同时保持了相同的质量生成。', 'title_zh': '基于指令驱动的检索重叠表示减少以加速自适应检索增强生成'}
{'arxiv_id': 'arXiv:2505.12692', 'title': 'Bullying the Machine: How Personas Increase LLM Vulnerability', 'authors': 'Ziwei Xu, Udit Sanghi, Mohan Kankanhalli', 'link': 'https://arxiv.org/abs/2505.12692', 'abstract': "Large Language Models (LLMs) are increasingly deployed in interactions where they are prompted to adopt personas. This paper investigates whether such persona conditioning affects model safety under bullying, an adversarial manipulation that applies psychological pressures in order to force the victim to comply to the attacker. We introduce a simulation framework in which an attacker LLM engages a victim LLM using psychologically grounded bullying tactics, while the victim adopts personas aligned with the Big Five personality traits. Experiments using multiple open-source LLMs and a wide range of adversarial goals reveal that certain persona configurations -- such as weakened agreeableness or conscientiousness -- significantly increase victim's susceptibility to unsafe outputs. Bullying tactics involving emotional or sarcastic manipulation, such as gaslighting and ridicule, are particularly effective. These findings suggest that persona-driven interaction introduces a novel vector for safety risks in LLMs and highlight the need for persona-aware safety evaluation and alignment strategies.", 'abstract_zh': '大语言模型在采用人格设定的互动中越来越常见。本文探讨了此类人格设定是否影响模型在欺凌情景下的安全性，这是一种对手操纵行为，通过心理压力迫使受害者遵从攻击者。我们引入了一种仿真框架，在该框架中，攻击者大语言模型使用基于心理策略的欺凌手段与受害者大语言模型互动，而受害者则采用与五大人格特质相匹配的人格设定。使用多个开源大语言模型和各种对手目标的实验表明，某些人格配置——如减弱的亲和力或尽责性——显著增加了受害者产生不安全输出的易感性。涉及情感或讽刺操控的欺凌手法，如白眼和嘲讽，尤其有效。这些发现表明，以人格驱动的互动为大语言模型引入了一种新的安全风险向量，并强调了需要具备人格意识的安全评估和对齐策略的重要性。', 'title_zh': '欺凌机器：人设如何增加LLM的脆弱性'}
{'arxiv_id': 'arXiv:2505.12680', 'title': 'Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities', 'authors': 'Haoyu Zhao, Yihan Geng, Shange Tang, Yong Lin, Bohan Lyu, Hongzhou Lin, Chi Jin, Sanjeev Arora', 'link': 'https://arxiv.org/abs/2505.12680', 'abstract': 'LLM-based formal proof assistants (e.g., in Lean) hold great promise for automating mathematical discovery. But beyond syntactic correctness, do these systems truly understand mathematical structure as humans do? We investigate this question through the lens of mathematical inequalities -- a fundamental tool across many domains. While modern provers can solve basic inequalities, we probe their ability to handle human-intuitive compositionality. We introduce Ineq-Comp, a benchmark built from elementary inequalities through systematic transformations, including variable duplication, algebraic rewriting, and multi-step composition. Although these problems remain easy for humans, we find that most provers -- including Goedel, STP, and Kimina-7B -- struggle significantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly because it is trained to decompose the problems into sub-problems -- but still suffers a 20\\% performance drop (pass@32). Strikingly, performance remains poor for all models even when formal proofs of the constituent parts are provided in context, revealing that the source of weakness is indeed in compositional reasoning. Our results expose a persisting gap between the generalization behavior of current AI provers and human mathematical intuition.', 'abstract_zh': '基于LLM的形式证明助手（例如Lean）在自动化数学发现方面展现出巨大的潜力。但是，这些系统是否像人类一样真正理解数学结构？我们通过数学不等式的视角探讨了这一问题——不等式是众多领域中的基础工具。尽管现代证明系统可以解决基本的不等式问题，但我们探究了它们处理人类直觉下的组合性能力。我们构建了Ineq-Comp基准，通过系统性的变换（包括变量复制、代数重写和多步组合）从基本不等式中生成。尽管这些问题对人类来说仍然相对容易，但我们发现大多数证明系统（包括Gödel、STP和Kimina-7B）显著挣扎。DeepSeek-Prover-V2-7B相对表现出一定的稳健性——可能因为它是训练成将问题分解为子问题——但其性能仍下降了20%（pass@32）。令人惊讶的是，即使在提供构成部分形式证明的情况下，所有模型的表现仍然不佳，这揭示出缺陷实际上源于组合性推理。我们的结果揭示了当前AI证明系统的一般化行为与人类数学直觉之间持续存在的差距。', 'title_zh': 'Ineq-Comp: 在不等式自动定理证明中的人类直觉组合推理基准测试'}
{'arxiv_id': 'arXiv:2505.12575', 'title': 'RealMath: A Continuous Benchmark for Evaluating Language Models on Research-Level Mathematics', 'authors': 'Jie Zhang, Cezara Petrui, Kristina Nikolić, Florian Tramèr', 'link': 'https://arxiv.org/abs/2505.12575', 'abstract': "Existing benchmarks for evaluating mathematical reasoning in large language models (LLMs) rely primarily on competition problems, formal proofs, or artificially challenging questions -- failing to capture the nature of mathematics encountered in actual research environments. We introduce RealMath, a novel benchmark derived directly from research papers and mathematical forums that assesses LLMs' abilities on authentic mathematical tasks. Our approach addresses three critical challenges: sourcing diverse research-level content, enabling reliable automated evaluation through verifiable statements, and designing a continually refreshable dataset to mitigate contamination risks. Experimental results across multiple LLMs reveal surprising capabilities in handling research mathematics compared to competition problems, suggesting current models may already serve as valuable assistants for working mathematicians despite limitations on highly challenging problems. The code and dataset for RealMath are publicly available.", 'abstract_zh': '现有的用于评估大型语言模型在数学推理能力上的基准主要依赖于竞赛题目、形式证明或人工构建的难题——未能捕捉到实际研究环境中遇到的数学性质。我们引入了RealMath，这是一个直接源自研究论文和数学论坛的新型基准，评估大型语言模型在解决真实数学任务方面的能力。我们的方法解决了三个关键挑战：获取多样化的研究级内容、通过可验证的陈述实现可靠的自动化评估、并设计一个可持续更新的数据集以减轻污染风险。跨多个大型语言模型的实验结果显示，与竞赛题目相比，当前模型在处理研究数学方面表现出令人惊讶的能力，表明尽管在处理极富挑战性的问题上存在局限，当前的模型可能已经能够成为数学工作者有价值的助手。RealMath的代码和数据集已公开。', 'title_zh': '实数学：评估语言模型在研究级数学方面的持续基准'}
{'arxiv_id': 'arXiv:2505.12565', 'title': 'mCLM: A Function-Infused and Synthesis-Friendly Modular Chemical Language Model', 'authors': 'Carl Edwards, Chi Han, Gawon Lee, Thao Nguyen, Bowen Jin, Chetan Kumar Prasad, Sara Szymkuć, Bartosz A. Grzybowski, Ying Diao, Jiawei Han, Ge Liu, Hao Peng, Martin D. Burke, Heng Ji', 'link': 'https://arxiv.org/abs/2505.12565', 'abstract': "Despite their ability to understand chemical knowledge and accurately generate sequential representations, large language models (LLMs) remain limited in their capacity to propose novel molecules with drug-like properties. In addition, the molecules that LLMs propose can often be challenging to make in the lab. To more effectively enable the discovery of functional small molecules, LLMs need to learn a molecular language. However, LLMs are currently limited by encoding molecules from atoms. In this paper, we argue that just like tokenizing texts into (sub-)word tokens instead of characters, molecules should be decomposed and reassembled at the level of functional building blocks, i.e., parts of molecules that bring unique functions and serve as effective building blocks for real-world automated laboratory synthesis. This motivates us to propose mCLM, a modular Chemical-Language Model tokenizing molecules into building blocks and learning a bilingual language model of both natural language descriptions of functions and molecule building blocks. By reasoning on such functional building blocks, mCLM guarantees to generate efficiently synthesizable molecules thanks to recent progress in block-based chemistry, while also improving the functions of molecules in a principled manner. In experiments on 430 FDA-approved drugs, we find mCLM capable of significantly improving 5 out of 6 chemical functions critical to determining drug potentials. More importantly, mCLM can reason on multiple functions and improve the FDA-rejected drugs (``fallen angels'') over multiple iterations to greatly improve their shortcomings.", 'abstract_zh': '尽管大型语言模型具备理解化学知识和生成准确序列表示的能力，但它们在提出具有药物性质的新型分子方面仍有限制。此外，大型语言模型提出的分子在实验室制备时往往具有挑战性。为了更有效地促进功能性小分子的发现，大型语言模型需要学习一种分子语言。然而，当前的大型语言模型受限于从原子角度编码分子。在本文中，我们提出，就像用子词而非字符进行文本分词一样，分子应该在功能构建块的层面进行分解和重组，即那些赋予分子独特功能并作为实际自动化实验室合成有效构建块的部分。这促使我们提出mCLM，这是一种模块化的化学语言模型，将分子分解为构建块，并学习一种双语语言模型，该模型可以理解自然语言描述的功能和分子构建块。通过在这些功能构建块上进行推理，mCLM能够利用基于块的化学进展高效生成可合成分子，同时在原则上改善分子的功能。在对430种FDA批准药物进行的实验中，我们发现mCLM能够显著改善决定药物潜能的6大化学功能中的5大功能。更重要的是，mCLM能够在多轮推理中对多种功能进行推理，显著改善未通过FDA评审的“失落的天使”药物的不足之处。', 'title_zh': 'mCLM：一种功能注入型和合成友好的模块化化学语言模型'}
{'arxiv_id': 'arXiv:2505.12501', 'title': 'ALAS: A Stateful Multi-LLM Agent Framework for Disruption-Aware Planning', 'authors': 'Edward Y. Chang, Longling Geng', 'link': 'https://arxiv.org/abs/2505.12501', 'abstract': 'Large language models (LLMs) excel at rapid generation of text and multimodal content, yet they falter on transaction-style planning that demands ACID-like guarantees and real-time disruption recovery. We present Adaptive LLM Agent System (ALAS), a framework that tackles four fundamental LLM deficits: (i) absence of self-verification, (ii) context erosion, (iii) next-token myopia, and (iv) lack of persistent state. ALAS decomposes each plan into role-specialized agents, equips them with automatic state tracking, and coordinates them through a lightweight protocol. When disruptions arise, agents apply history-aware local compensation, avoiding costly global replanning and containing cascade effects. On real-world, large-scale job-shop scheduling benchmarks, ALAS sets new best results for static sequential planning and excels in dynamic reactive scenarios with unexpected disruptions. These gains show that principled modularization plus targeted compensation can unlock scalable and resilient planning with LLMs.', 'abstract_zh': '大型语言模型（LLMs）在快速生成文本和多模态内容方面表现出色，但在需要类似ACID的保证和实时中断恢复的事务式规划方面却表现不佳。我们提出了一种应对大型语言模型四大根本缺陷的框架——自适应大型语言模型代理系统（ALAS），这些缺陷包括：自我验证的缺失、情境侵蚀、下一标记近视以及持久状态的缺乏。ALAS 将每个计划分解为角色专门化的代理，为它们提供自动状态跟踪，并通过轻量级协议进行协调。当出现中断时，代理应用具有历史意识的局部补偿，避免昂贵的全局重规划，并遏制连锁反应。在真实的大型作业车间调度基准测试中，ALAS 在静态序列规划中取得了新的最佳结果，并在具有意外中断的动态反应场景中表现出色。这些成果表明，原理性的模块化加上针对性的补偿可以解锁具有大型语言模型的可扩展和鲁棒规划。', 'title_zh': 'ALAS：一种具备状态awareness的多大型语言模型代理框架，用于干扰感知规划'}
{'arxiv_id': 'arXiv:2505.12500', 'title': 'MARGE: Improving Math Reasoning for LLMs with Guided Exploration', 'authors': 'Jingyue Gao, Runji Lin, Keming Lu, Bowen Yu, Junyang Lin, Jianyu Chen', 'link': 'https://arxiv.org/abs/2505.12500', 'abstract': "Large Language Models (LLMs) exhibit strong potential in mathematical reasoning, yet their effectiveness is often limited by a shortage of high-quality queries. This limitation necessitates scaling up computational responses through self-generated data, yet current methods struggle due to spurious correlated data caused by ineffective exploration across all reasoning stages. To address such challenge, we introduce \\textbf{MARGE}: Improving \\textbf{Ma}th \\textbf{R}easoning with \\textbf{G}uided \\textbf{E}xploration, a novel method to address this issue and enhance mathematical reasoning through hit-guided exploration. MARGE systematically explores intermediate reasoning states derived from self-generated solutions, enabling adequate exploration and improved credit assignment throughout the reasoning process. Through extensive experiments across multiple backbone models and benchmarks, we demonstrate that MARGE significantly improves reasoning capabilities without requiring external annotations or training additional value models. Notably, MARGE improves both single-shot accuracy and exploration diversity, mitigating a common trade-off in alignment methods. These results demonstrate MARGE's effectiveness in enhancing mathematical reasoning capabilities and unlocking the potential of scaling self-generated training data. Our code and models are available at \\href{this https URL}{this link}.", 'abstract_zh': '改进数学推理的引导探索：MARGE 方法', 'title_zh': 'MARGE: 通过引导式探索提高大规模语言模型的数学推理能力'}
{'arxiv_id': 'arXiv:2505.12470', 'title': 'NeuroGen: Neural Network Parameter Generation via Large Language Models', 'authors': 'Jiaqi Wang, Yusen Zhang, Xi Li', 'link': 'https://arxiv.org/abs/2505.12470', 'abstract': 'Acquiring the parameters of neural networks (NNs) has been one of the most important problems in machine learning since the inception of NNs. Traditional approaches, such as backpropagation and forward-only optimization, acquire parameters via iterative data fitting to gradually optimize them. This paper aims to explore the feasibility of a new direction: acquiring NN parameters via large language model generation. We propose NeuroGen, a generalized and easy-to-implement two-stage approach for NN parameter generation conditioned on descriptions of the data, task, and network architecture. Stage one is Parameter Reference Knowledge Injection, where LLMs are pretrained on NN checkpoints to build foundational understanding of parameter space, whereas stage two is Context-Enhanced Instruction Tuning, enabling LLMs to adapt to specific tasks through enriched, task-aware prompts. Experimental results demonstrate that NeuroGen effectively generates usable NN parameters. Our findings highlight the feasibility of LLM-based NN parameter generation and suggest a promising new paradigm where LLMs and lightweight NNs can coexist synergistically', 'abstract_zh': '通过大型语言模型生成神经网络参数：一种基于描述的两阶段方法', 'title_zh': 'NeuroGen：通过大型语言模型生成神经网络参数'}
{'arxiv_id': 'arXiv:2505.12371', 'title': 'MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks', 'authors': 'Yinghao Zhu, Ziyi He, Haoran Hu, Xiaochen Zheng, Xichen Zhang, Zixiang Wang, Junyi Gao, Liantao Ma, Lequan Yu', 'link': 'https://arxiv.org/abs/2505.12371', 'abstract': 'The rapid advancement of Large Language Models (LLMs) has stimulated interest in multi-agent collaboration for addressing complex medical tasks. However, the practical advantages of multi-agent collaboration approaches remain insufficiently understood. Existing evaluations often lack generalizability, failing to cover diverse tasks reflective of real-world clinical practice, and frequently omit rigorous comparisons against both single-LLM-based and established conventional methods. To address this critical gap, we introduce MedAgentBoard, a comprehensive benchmark for the systematic evaluation of multi-agent collaboration, single-LLM, and conventional approaches. MedAgentBoard encompasses four diverse medical task categories: (1) medical (visual) question answering, (2) lay summary generation, (3) structured Electronic Health Record (EHR) predictive modeling, and (4) clinical workflow automation, across text, medical images, and structured EHR data. Our extensive experiments reveal a nuanced landscape: while multi-agent collaboration demonstrates benefits in specific scenarios, such as enhancing task completeness in clinical workflow automation, it does not consistently outperform advanced single LLMs (e.g., in textual medical QA) or, critically, specialized conventional methods that generally maintain better performance in tasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital resource and actionable insights, emphasizing the necessity of a task-specific, evidence-based approach to selecting and developing AI solutions in medicine. It underscores that the inherent complexity and overhead of multi-agent collaboration must be carefully weighed against tangible performance gains. All code, datasets, detailed prompts, and experimental results are open-sourced at this https URL.', 'abstract_zh': '大型语言模型（LLMs）的rapid advancement激发了对多智能体协作解决复杂医疗任务的兴趣。然而，多智能体协作方法的实际优势尚未充分理解。现有评估往往缺乏普适性，未能涵盖反映实际临床实践多样性的任务，并且经常省略与单一LLM方法和成熟的传统方法的严格对比。为弥补这一关键差距，我们引入了MedAgentBoard，这是一个全面的基准，用于系统评估多智能体协作、单一LLM和传统方法。MedAgentBoard包括四类不同的医疗任务类别：（1）医学（视觉）问答，（2）通俗摘要生成，（3）结构化电子健康记录（EHR）预测建模，以及（4）临床工作流自动化，涉及文本、医学图像和结构化EHR数据。我们的广泛实验揭示了一个复杂的景观：尽管多智能体协作在某些场景下显示出优势，如增强临床工作流自动化的任务完整性，但它并不总是比先进的单一LLM（例如，在文本医学问答）性能更好，或更关键地，在医学VQA和基于EHR的预测任务中持续保持更好的性能。MedAgentBoard提供了一个重要的资源和可操作的见解，强调了在医学中选择和开发AI解决方案时需要针对特定任务和基于证据的方法。它强调了多智能体协作的固有复杂性和额外开销必须谨慎权衡以获得实际性能改进的重要性。所有代码、数据集、详细提示和实验结果均可在此网址获取。', 'title_zh': 'MedAgentBoard: 用传统方法评估多agents协作进行多样化医疗任务的能力'}
{'arxiv_id': 'arXiv:2505.12348', 'title': 'Reasoning-CV: Fine-tuning Powerful Reasoning LLMs for Knowledge-Assisted Claim Verification', 'authors': 'Zhi Zheng, Wee Sun Lee', 'link': 'https://arxiv.org/abs/2505.12348', 'abstract': 'Claim verification is essential in combating misinformation, and large language models (LLMs) have recently emerged in this area as powerful tools for assessing the veracity of claims using external knowledge. Existing LLM-based methods for claim verification typically adopt a Decompose-Then-Verify paradigm, which involves decomposing complex claims into several independent sub-claims and verifying each sub-claim separately. However, this paradigm often introduces errors during the claim decomposition process. To mitigate these errors, we propose to develop the Chain-of-Thought (CoT)-Verify paradigm, which leverages LLM reasoning methods to generate CoT-verification paths for the original complex claim without requiring decompositions into sub-claims and separate verification stages. The CoT-Verify paradigm allows us to propose a natural fine-tuning method called Reasoning-CV to enhance the verification capabilities in LLMs. Reasoning-CV includes a supervised fine-tuning (SFT) stage and a self-improvement direct preference optimization (DPO) stage. Utilizing only an 8B pre-trained LLM, Reasoning-CV demonstrates superior knowledge-assisted claim verification performances compared to existing Decompose-Then-Verify methods, as well as powerful black-box LLMs such as GPT-4o+CoT and o1-preview. Our code is available.', 'abstract_zh': '基于链式思考的声明验证方法：一种无需分解的声明验证范式', 'title_zh': 'Reasoning-CV：细调强大的推理大规模语言模型以进行知识辅助断言验证'}
{'arxiv_id': 'arXiv:2505.12346', 'title': 'SEED-GRPO: Semantic Entropy Enhanced GRPO for Uncertainty-Aware Policy Optimization', 'authors': 'Minghan Chen, Guikun Chen, Wenguan Wang, Yi Yang', 'link': 'https://arxiv.org/abs/2505.12346', 'abstract': "Large language models (LLMs) exhibit varying levels of confidence across input prompts (questions): some lead to consistent, semantically similar answers, while others yield diverse or contradictory outputs. This variation reflects LLM's uncertainty about the input prompt, a signal of how confidently the model understands a given problem. However, vanilla Group Relative Policy Optimization (GRPO) treats all prompts equally during policy updates, ignoring this important information about the model's knowledge boundaries. To address this limitation, we propose SEED-GRPO (Semantic Entropy EnhanceD GRPO), which explicitly measures LLMs' uncertainty of the input prompts semantic entropy. Semantic entropy measures the diversity of meaning in multiple generated answers given a prompt and uses this to modulate the magnitude of policy updates. This uncertainty-aware training mechanism enables dynamic adjustment of policy update magnitudes based on question uncertainty. It allows more conservative updates on high-uncertainty questions while maintaining the original learning signal on confident ones. Experimental results on five mathematical reasoning benchmarks (AIME24 56.7, AMC 68.7, MATH 83.4, Minerva 34.2, and OlympiadBench 48.0) demonstrate that SEED-GRPO achieves new state-of-the-art performance in average accuracy, validating the effectiveness of uncertainty-aware policy optimization.", 'abstract_zh': '基于语义熵增强的分组相对策略优化（SEED-GRPO）：面向输入提示不确定性适应性训练机制', 'title_zh': 'SEED-GRPO:Semantic Entropy Enhanced GRPO for Uncertainty-Aware Policy Optimization'}
{'arxiv_id': 'arXiv:2505.12334', 'title': 'Enhancing User-Oriented Proactivity in Open-Domain Dialogues with Critic Guidance', 'authors': 'Yufeng Wang, Jinwu Hu, Ziteng Huang, Kunyang Lin, Zitian Zhang, Peihao Chen, Yu Hu, Qianyue Wang, Zhuliang Yu, Bin Sun, Xiaofen Xing, Qingfang Zheng, Mingkui Tan', 'link': 'https://arxiv.org/abs/2505.12334', 'abstract': "Open-domain dialogue systems aim to generate natural and engaging conversations, providing significant practical value in real applications such as social robotics and personal assistants. The advent of large language models (LLMs) has greatly advanced this field by improving context understanding and conversational fluency. However, existing LLM-based dialogue systems often fall short in proactively understanding the user's chatting preferences and guiding conversations toward user-centered topics. This lack of user-oriented proactivity can lead users to feel unappreciated, reducing their satisfaction and willingness to continue the conversation in human-computer interactions. To address this issue, we propose a User-oriented Proactive Chatbot (UPC) to enhance the user-oriented proactivity. Specifically, we first construct a critic to evaluate this proactivity inspired by the LLM-as-a-judge strategy. Given the scarcity of high-quality training data, we then employ the critic to guide dialogues between the chatbot and user agents, generating a corpus with enhanced user-oriented proactivity. To ensure the diversity of the user backgrounds, we introduce the ISCO-800, a diverse user background dataset for constructing user agents. Moreover, considering the communication difficulty varies among users, we propose an iterative curriculum learning method that trains the chatbot from easy-to-communicate users to more challenging ones, thereby gradually enhancing its performance. Experiments demonstrate that our proposed training method is applicable to different LLMs, improving user-oriented proactivity and attractiveness in open-domain dialogues.", 'abstract_zh': '面向用户的主动聊天机器人：增强用户导向的主动性', 'title_zh': '基于批评指导提升面向用户的开放域对话的主动性'}
{'arxiv_id': 'arXiv:2505.12301', 'title': 'Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge', 'authors': 'Luyu Chen, Zeyu Zhang, Haoran Tan, Quanyu Dai, Hao Yang, Zhenhua Dong, Xu Chen', 'link': 'https://arxiv.org/abs/2505.12301', 'abstract': 'LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm, offering significant efficiency and flexibility compared to human judgments. However, previous methods primarily rely on single-point evaluations, overlooking the inherent diversity and uncertainty in human evaluations. This approach leads to information loss and decreases the reliability of evaluations. To address this limitation, we propose a novel training framework that explicitly aligns the LLM-generated judgment distribution with empirical human distributions. Specifically, we propose a distributional alignment objective based on KL divergence, combined with an auxiliary cross-entropy regularization to stabilize the training process. Furthermore, considering that empirical distributions may derive from limited human annotations, we incorporate adversarial training to enhance model robustness against distribution perturbations. Extensive experiments across various LLM backbones and evaluation tasks demonstrate that our framework significantly outperforms existing closed-source LLMs and conventional single-point alignment methods, with improved alignment quality, evaluation accuracy, and robustness.', 'abstract_zh': 'LLMs作为法官 paradigm 中的强大评估者：基于分布对齐的新型训练框架及其应用', 'title_zh': '超越单点判断：LLM作为法官的分布对齐'}
{'arxiv_id': 'arXiv:2505.12189', 'title': 'Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering', 'authors': 'Marco Valentino, Geonhee Kim, Dhairya Dalal, Zhixue Zhao, André Freitas', 'link': 'https://arxiv.org/abs/2505.12189', 'abstract': 'Large language models (LLMs) frequently demonstrate reasoning limitations, often conflating content plausibility (i.e., material inference) with logical validity (i.e., formal inference). This can result in biased inferences, where plausible arguments are incorrectly deemed logically valid or vice versa. Mitigating this limitation is critical, as it undermines the trustworthiness and generalizability of LLMs in applications that demand rigorous logical consistency. This paper investigates the problem of mitigating content biases on formal reasoning through activation steering. Specifically, we curate a controlled syllogistic reasoning dataset to disentangle formal validity from content plausibility. After localising the layers responsible for formal and material inference, we investigate contrastive activation steering methods for test-time interventions. An extensive empirical analysis on different LLMs reveals that contrastive steering consistently supports linear control over content biases. However, we observe that a static approach is insufficient for improving all the tested models. We then leverage the possibility to control content effects by dynamically determining the value of the steering parameters via fine-grained conditional methods. We found that conditional steering is effective on unresponsive models, achieving up to 15% absolute improvement in formal reasoning accuracy with a newly introduced kNN-based method (K-CAST). Finally, additional experiments reveal that steering for content effects is robust to prompt variations, incurs minimal side effects on language modeling capabilities, and can partially generalize to out-of-distribution reasoning tasks. Practically, this paper demonstrates that activation-level interventions can offer a scalable strategy for enhancing the robustness of LLMs, contributing towards more systematic and unbiased formal reasoning.', 'abstract_zh': '大规模语言模型（LLMs）经常表现出推理限制，常常将内容合理性（即材料推理）与逻辑有效性（即形式推理）混淆。这可能导致有偏的推理，其中合理的论点被错误地认为是逻辑有效的，反之亦然。减轻这一限制至关重要，因为它会削弱LLMs在需要严格逻辑一致性的应用中的可信度和普适性。本文探讨了通过激活控制减轻形式推理内容偏见的问题。具体而言，我们策划了一个受控的三段论推理数据集，以区分形式有效性与内容合理性。在定位负责形式和材料推理的层后，我们调查了对比激活控制方法在测试时的干预效果。对不同LLMs的广泛实证分析表明，对比控制一致地支持对内容偏见的线性控制。然而，我们观察到静态方法不足以提高所有测试模型的性能。然后，我们利用通过细粒度条件方法动态确定引导参数值的可能性，从而控制内容效果。我们发现条件控制在无响应模型上是有效的，使用新提出的基于kNN的方法（K-CAST）实现了高达15%的绝对形式推理准确率改进。此外的实验还表明，对内容效果的引导在提示变化时是稳健的，对语言建模能力的影响最小，并且可以部分泛化到新的推理任务。实际上，本文证明了激活层次干预可以提供一种可扩展的战略来提高LLMs的鲁棒性，从而促进更系统的无偏形式推理。', 'title_zh': '通过精细粒度激活调控减轻内容效应对语言模型推理的影响'}
{'arxiv_id': 'arXiv:2505.12135', 'title': 'LLM-BABYBENCH: Understanding and Evaluating Grounded Planning and Reasoning in LLMs', 'authors': 'Omar Choukrani, Idriss Malek, Daniil Orel, Zhuohan Xie, Zangir Iklassov, Martin Takáč, Salem Lahlou', 'link': 'https://arxiv.org/abs/2505.12135', 'abstract': 'Assessing the capacity of Large Language Models (LLMs) to plan and reason within the constraints of interactive environments is crucial for developing capable AI agents. We introduce $\\textbf{LLM-BabyBench}$, a new benchmark suite designed specifically for this purpose. Built upon a textual adaptation of the procedurally generated BabyAI grid world, this suite evaluates LLMs on three fundamental aspects of grounded intelligence: (1) predicting the consequences of actions on the environment state ($\\textbf{Predict}$ task), (2) generating sequences of low-level actions to achieve specified objectives ($\\textbf{Plan}$ task), and (3) decomposing high-level instructions into coherent subgoal sequences ($\\textbf{Decompose}$ task). We detail the methodology for generating the three corresponding datasets ($\\texttt{LLM-BabyBench-Predict}$, $\\texttt{-Plan}$, $\\texttt{-Decompose}$) by extracting structured information from an expert agent operating within the text-based environment. Furthermore, we provide a standardized evaluation harness and metrics, including environment interaction for validating generated plans, to facilitate reproducible assessment of diverse LLMs. Initial baseline results highlight the challenges posed by these grounded reasoning tasks. The benchmark suite, datasets, data generation code, and evaluation code are made publicly available ($\\href{this https URL}{\\text{GitHub}}$, $\\href{this https URL}{\\text{HuggingFace}}$).', 'abstract_zh': '评估大型语言模型在交互环境约束下进行计划与推理的能力对于开发能力强的AI代理至关重要。我们引入了**LLM-BabyBench**，一种专门为此目的设计的新基准套件。基于文本适应的 procedurally 生成的 BabyAI 网格世界，该套件从三个方面评估 LLMs 的基于现实智能：(1) 预测动作对环境状态的影响（**Predict** 任务），(2) 生成实现特定目标的低级动作序列（**Plan** 任务），(3) 将高层次指令分解为连贯的子目标序列（**Decompose** 任务）。我们详细介绍了生成三个相应数据集（**LLM-BabyBench-Predict**、**-Plan**、**-Decompose**）的方法，通过从文本环境中的专家代理中提取结构化信息。此外，我们还提供了一套标准化评估框架和指标，包括环境交互以验证生成的计划，以促进不同 LLMs 的可重现评估。初步基准结果突显了这些基于现实推理任务所提出的挑战。基准套件、数据集、数据生成代码和评估代码已公开发布（GitHub、HuggingFace）。', 'title_zh': 'LLM-BABYBENCH: 理解和评估LLM中基于地面规划与推理的能力'}
{'arxiv_id': 'arXiv:2505.12065', 'title': 'Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents', 'authors': 'Tiannuo Yang, Zebin Yao, Bowen Jin, Lixiao Cui, Yusen Li, Gang Wang, Xiaoguang Liu', 'link': 'https://arxiv.org/abs/2505.12065', 'abstract': 'Large Language Model (LLM)-based search agents have shown remarkable capabilities in solving complex tasks by dynamically decomposing problems and addressing them through interleaved reasoning and retrieval. However, this interleaved paradigm introduces substantial efficiency bottlenecks. First, we observe that both highly accurate and overly approximate retrieval methods degrade system efficiency: exact search incurs significant retrieval overhead, while coarse retrieval requires additional reasoning steps during generation. Second, we identify inefficiencies in system design, including improper scheduling and frequent retrieval stalls, which lead to cascading latency -- where even minor delays in retrieval amplify end-to-end inference time. To address these challenges, we introduce SearchAgent-X, a high-efficiency inference framework for LLM-based search agents. SearchAgent-X leverages high-recall approximate retrieval and incorporates two key techniques: priority-aware scheduling and non-stall retrieval. Extensive experiments demonstrate that SearchAgent-X consistently outperforms state-of-the-art systems such as vLLM and HNSW-based retrieval across diverse tasks, achieving up to 3.4$\\times$ higher throughput and 5$\\times$ lower latency, without compromising generation quality. SearchAgent-X is available at this https URL.', 'abstract_zh': '基于大语言模型的搜索代理通过动态分解问题和通过交错推理与检索来解决复杂任务展现了显著能力，然而这种交错范式引入了显著的效率瓶颈。SearchAgent-X：基于大语言模型的高效率推理框架', 'title_zh': '揭示并提升基于大型语言模型的搜索代理效率'}
{'arxiv_id': 'arXiv:2505.12058', 'title': 'Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation', 'authors': 'Vincent Koc', 'link': 'https://arxiv.org/abs/2505.12058', 'abstract': 'Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual smoke-test suite designed to give large-language-model (LLM) pipelines a unit-test style safety net dataset that runs in seconds with minimal cost. Born out of the tight feedback-loop demands building the Comet Opik prompt-optimization SDK, where waiting on heavyweight benchmarks breaks developer flow. TQB++ couples a 52-item English gold set (less than 20 kB) with a tiny synthetic-data generator pypi package built on provider-agnostic LiteLLM. The generator lets practitioners mint their own tiny packs in any language, domain, or difficulty, while ten ready-made packs already cover Arabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian, Spanish, and Turkish. Every dataset ships with Croissant metadata and plug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so teams can drop deterministic micro-benchmarks directly into pull-request gates, prompt-engineering loops, and production dashboards without touching GPU budgets. A complete TQB++ run adds only a few seconds to pipeline latency yet reliably flags prompt-template errors, tokenizer drift, and fine-tuning side-effects long before full-scale suites like MMLU or BIG-Bench would finish configuring. The entire framework is released to accelerate continuous, resource-efficient quality assurance across the generative-AI ecosystem.', 'abstract_zh': 'Tiny QA基准增强版(TQB++)：一个超轻量级、多语言的烟雾测试套件，旨在为大型语言模型(LLM)管道提供类似单元测试的安全网数据集，可在几秒内运行，成本极低。', 'title_zh': 'Tiny QA Benchmark++: 超轻量级合成多语言数据集生成与连续语言模型评估的烟雾测试'}
{'arxiv_id': 'arXiv:2505.12031', 'title': 'LLM-based Automated Theorem Proving Hinges on Scalable Synthetic Data Generation', 'authors': 'Junyu Lai, Jiakun Zhang, Shuo Xu, Taolue Chen, Zihang Wang, Yao Yang, Jiarui Zhang, Chun Cao, Jingwei Xu', 'link': 'https://arxiv.org/abs/2505.12031', 'abstract': 'Recent advancements in large language models (LLMs) have sparked considerable interest in automated theorem proving and a prominent line of research integrates stepwise LLM-based provers into tree search. In this paper, we introduce a novel proof-state exploration approach for training data synthesis, designed to produce diverse tactics across a wide range of intermediate proof states, thereby facilitating effective one-shot fine-tuning of LLM as the policy model. We also propose an adaptive beam size strategy, which effectively takes advantage of our data synthesis method and achieves a trade-off between exploration and exploitation during tree search. Evaluations on the MiniF2F and ProofNet benchmarks demonstrate that our method outperforms strong baselines under the stringent Pass@1 metric, attaining an average pass rate of $60.74\\%$ on MiniF2F and $21.18\\%$ on ProofNet. These results underscore the impact of large-scale synthetic data in advancing automated theorem proving.', 'abstract_zh': 'Recent advancements in large language models (LLMs) have sparked considerable interest in automated theorem proving, and a prominent line of research integrates stepwise LLM-based provers into tree search. In this paper, we introduce a novel proof-state exploration approach for training data synthesis, designed to produce diverse tactics across a wide range of intermediate proof states, thereby facilitating effective one-shot fine-tuning of LLM as the policy model. We also propose an adaptive beam size strategy, which effectively takes advantage of our data synthesis method and achieves a trade-off between exploration and exploitation during tree search. Evaluations on the MiniF2F and ProofNet benchmarks demonstrate that our method outperforms strong baselines under the stringent Pass@1 metric, attaining an average pass rate of $60.74\\%$ on MiniF2F and $21.18\\%$ on ProofNet. These results underscore the impact of large-scale synthetic data in advancing automated theorem proving.', 'title_zh': '基于LLM的自动化定理证明依赖于可扩展的合成数据生成。'}
{'arxiv_id': 'arXiv:2505.12006', 'title': 'SOCIA: An End-to-End Agentic Framework for Automated Cyber-Physical-Social Simulator Generation', 'authors': 'Yuncheng Hua, Ji Miao, Mehdi Jafari, Jianxiang Xie, Hao Xue, Flora D. Salim', 'link': 'https://arxiv.org/abs/2505.12006', 'abstract': "This paper introduces SOCIA (Simulation Orchestration for Cyber-physical-social Intelligence and Agents), a novel end-to-end framework leveraging Large Language Model (LLM)-based multi-agent systems to automate the generation of high-fidelity Cyber-Physical-Social (CPS) simulators. Addressing the challenges of labor-intensive manual simulator development and complex data calibration, SOCIA integrates a centralized orchestration manager that coordinates specialized agents for tasks including data comprehension, code generation, simulation execution, and iterative evaluation-feedback loops. Through empirical evaluations across diverse CPS tasks, such as mask adoption behavior simulation (social), personal mobility generation (physical), and user modeling (cyber), SOCIA demonstrates its ability to produce high-fidelity, scalable simulations with reduced human intervention. These results highlight SOCIA's potential to offer a scalable solution for studying complex CPS phenomena", 'abstract_zh': '本论文介绍了SOCIA（基于大型语言模型的多智能体系统仿真编排），一种新颖的端到端框架，利用基于大型语言模型的多智能体系统自动化生成高保真度的网络物理社会（CPS）仿真。通过集中编排管理者协调专门智能体进行任务，包括数据理解、代码生成、仿真执行和迭代评估反馈循环，SOCIA解决了劳动密集型的手动仿真开发和复杂数据校准的挑战。通过在多样化的CPS任务，如口罩佩戴行为仿真（社会）、个人移动生成（物理）和用户建模（ cyber）方面的实证评估，SOCIA展示了其生成高保真度、可扩展仿真并减少人力干预的能力。这些结果突显了SOCIA在研究复杂CPS现象方面的潜在可扩展解决方案。', 'title_zh': 'SOCIA：一种端到端的自主框架，用于自动生成网络-物理-社会仿真器'}
{'arxiv_id': 'arXiv:2505.12001', 'title': 'Interactional Fairness in LLM Multi-Agent Systems: An Evaluation Framework', 'authors': 'Ruta Binkyte', 'link': 'https://arxiv.org/abs/2505.12001', 'abstract': "As large language models (LLMs) are increasingly used in multi-agent systems, questions of fairness should extend beyond resource distribution and procedural design to include the fairness of how agents communicate. Drawing from organizational psychology, we introduce a novel framework for evaluating Interactional fairness encompassing Interpersonal fairness (IF) and Informational fairness (InfF) in LLM-based multi-agent systems (LLM-MAS). We extend the theoretical grounding of Interactional Fairness to non-sentient agents, reframing fairness as a socially interpretable signal rather than a subjective experience. We then adapt established tools from organizational justice research, including Colquitt's Organizational Justice Scale and the Critical Incident Technique, to measure fairness as a behavioral property of agent interaction. We validate our framework through a pilot study using controlled simulations of a resource negotiation task. We systematically manipulate tone, explanation quality, outcome inequality, and task framing (collaborative vs. competitive) to assess how IF influences agent behavior. Results show that tone and justification quality significantly affect acceptance decisions even when objective outcomes are held constant. In addition, the influence of IF vs. InfF varies with context. This work lays the foundation for fairness auditing and norm-sensitive alignment in LLM-MAS.", 'abstract_zh': '随着大型语言模型（LLMs）在多智能体系统中的广泛应用，公平性问题应从资源分配和程序设计扩展到包括代理间沟通的公平性。借鉴组织心理学，我们提出了一种新的框架，用于评估基于LLM的多智能体系统（LLM-MAS）中的交互公平性，该框架包括人际公平性（IF）和信息公平性（InfF）。我们将交互公平性的理论基础扩展到非有感知能力的代理上，重新定义公平性为一种社会可解析的信号，而非主观体验。随后，我们借鉴组织公平性研究中已有的工具，包括Colquitt的组织公平性量表和关键事件技术，以行为属性的形式衡量公平性。我们通过使用资源谈判任务控制性模拟的试点研究验证了该框架。我们系统地操控语气、解释质量、结果不平等性和任务框架（合作 vs. 竞争），以评估IF如何影响代理行为。结果表明，即使在客观结果保持不变的情况下，语气和解释质量对接受决策的影响仍然显著。此外，IF和InfF的影响因情境而异。这项工作为LLM-MAS中的公平性审计和规范敏感性对齐奠定了基础。', 'title_zh': 'LLM多智能体系统中的互动公平性评估框架'}
{'arxiv_id': 'arXiv:2505.11966', 'title': 'Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative Verifier', 'authors': 'Jianyuan Zhong, Zeju Li, Zhijian Xu, Xiangyu Wen, Kezhi Li, Qiang Xu', 'link': 'https://arxiv.org/abs/2505.11966', 'abstract': 'Large Language Model (LLM) reasoning for complex tasks inherently involves a trade-off between solution accuracy and computational efficiency. The subsequent step of verification, while intended to improve performance, further complicates this landscape by introducing its own challenging trade-off: sophisticated Generative Reward Models (GenRMs) can be computationally prohibitive if naively integrated with LLMs at test-time, while simpler, faster methods may lack reliability. To overcome these challenges, we introduce FlexiVe, a novel generative verifier that flexibly balances computational resources between rapid, reliable fast thinking and meticulous slow thinking using a Flexible Allocation of Verification Budget strategy. We further propose the Solve-Detect-Verify pipeline, an efficient inference-time scaling framework that intelligently integrates FlexiVe, proactively identifying solution completion points to trigger targeted verification and provide focused solver feedback. Experiments show FlexiVe achieves superior accuracy in pinpointing errors within reasoning traces on ProcessBench. Furthermore, on challenging mathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full approach outperforms baselines like self-consistency in reasoning accuracy and inference efficiency. Our system offers a scalable and effective solution to enhance LLM reasoning at test time.', 'abstract_zh': '大规模语言模型（LLM）在复杂任务推理中固有地涉及求解准确性和计算效率之间的权衡。验证步骤虽然旨在提升性能，但也引入了自身的挑战性权衡：如果在测试时天真地将复杂的生成奖励模型与LLM集成，可能会导致计算成本过高，而简单快速的方法可能缺乏可靠性。为克服这些挑战，我们提出了FlexiVe，一种新颖的生成验证器，通过灵活分配验证预算策略在快速可靠直觉思考和仔细慎重思考之间灵活平衡计算资源。此外，我们进一步提出了Solve-Detect-Verify流水线，这是一种高效的推理时扩展框架，智能地集成FlexiVe，主动识别解决方案完成点，触发目标验证并提供集中解算器反馈。实验表明，FlexiVe在ProcessBench中的推理跟踪中更精确地定位错误。在具有挑战性的数学推理基准测试（AIME 2024、AIME 2025和CNMO）上，我们的完整方法在推理准确性和推理效率方面优于自一致性等基线。我们的系统为提高测试时LLM推理提供了一种可扩展且有效的解决方案。', 'title_zh': '解-检测-验证：灵活生成验证器的推理时扩展方法'}
{'arxiv_id': 'arXiv:2505.11942', 'title': 'LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners', 'authors': 'Junhao Zheng, Xidi Cai, Qiuke Li, Duzhen Zhang, ZhongZhi Li, Yingying Zhang, Le Song, Qianli Ma', 'link': 'https://arxiv.org/abs/2505.11942', 'abstract': 'Lifelong learning is essential for intelligent agents operating in dynamic environments. Current large language model (LLM)-based agents, however, remain stateless and unable to accumulate or transfer knowledge over time. Existing benchmarks treat agents as static systems and fail to evaluate lifelong learning capabilities. We present LifelongAgentBench, the first unified benchmark designed to systematically assess the lifelong learning ability of LLM agents. It provides skill-grounded, interdependent tasks across three interactive environments, Database, Operating System, and Knowledge Graph, with automatic label verification, reproducibility, and modular extensibility. Extensive experiments reveal that conventional experience replay has limited effectiveness for LLM agents due to irrelevant information and context length constraints. We further introduce a group self-consistency mechanism that significantly improves lifelong learning performance. We hope LifelongAgentBench will advance the development of adaptive, memory-capable LLM agents.', 'abstract_zh': '终身学习对于在动态环境中操作的智能代理至关重要。当前基于大规模语言模型（LLM）的代理仍然无状态，无法随时间积累或转移知识。现有的基准将代理视为静态系统，未能评估终身学习能力。我们提出了LifelongAgentBench，这是首个用于系统评估LLM代理终身学习能力的统一基准。它提供了跨数据库、操作系统和知识图谱三个交互环境的技能导向且相互依赖的任务，并具备自动标签验证、可重复性和模块可扩展性。广泛实验证明，传统的经验回放对LLM代理的效果有限，原因在于无关信息和上下文长度的限制。我们进一步引入了一种群体自一致性机制，显著提高了终身学习性能。我们期望LifelongAgentBench能够促进适应性和记忆能力的LLM代理的发展。', 'title_zh': '终身学习者Agent评估基准：评估LLM Agents的终身学习能力'}
{'arxiv_id': 'arXiv:2505.11899', 'title': 'From Recall to Reasoning: Automated Question Generation for Deeper Math Learning through Large Language Models', 'authors': 'Yongan Yu, Alexandre Krantz, Nikki G. Lobczowski', 'link': 'https://arxiv.org/abs/2505.11899', 'abstract': 'Educators have started to turn to Generative AI (GenAI) to help create new course content, but little is known about how they should do so. In this project, we investigated the first steps for optimizing content creation for advanced math. In particular, we looked at the ability of GenAI to produce high-quality practice problems that are relevant to the course content. We conducted two studies to: (1) explore the capabilities of current versions of publicly available GenAI and (2) develop an improved framework to address the limitations we found. Our results showed that GenAI can create math problems at various levels of quality with minimal support, but that providing examples and relevant content results in better quality outputs. This research can help educators decide the ideal way to adopt GenAI in their workflows, to create more effective educational experiences for students.', 'abstract_zh': '教育者已经开始利用生成式人工智能（GenAI）来创建新的课程内容，但尚不清楚应该如何操作。本项目研究了优化高级数学内容创作的第一步。特别地，我们探讨了GenAI生成与课程内容相关且高质量练习题的能力。我们进行了两项研究：（1）探索当前公开版本GenAI的能力；（2）开发改进框架以解决我们发现的限制。研究结果表明，GenAI在最少支持下可以生成不同质量级别的数学题目，但在生成高质量输出时提供建例和相关内容效果更佳。本研究可帮助教育者决定在工作流程中采用GenAI的最佳方式，从而为学生创建更有效的教育体验。', 'title_zh': '从回忆到推理：通过大规模语言模型实现更深层次数学学习的自动问题生成'}
{'arxiv_id': 'arXiv:2505.11866', 'title': 'Position Paper: Bounded Alignment: What (Not) To Expect From AGI Agents', 'authors': 'Ali A. Minai', 'link': 'https://arxiv.org/abs/2505.11866', 'abstract': 'The issues of AI risk and AI safety are becoming critical as the prospect of artificial general intelligence (AGI) looms larger. The emergence of extremely large and capable generative models has led to alarming predictions and created a stir from boardrooms to legislatures. As a result, AI alignment has emerged as one of the most important areas in AI research. The goal of this position paper is to argue that the currently dominant vision of AGI in the AI and machine learning (AI/ML) community needs to evolve, and that expectations and metrics for its safety must be informed much more by our understanding of the only existing instance of general intelligence, i.e., the intelligence found in animals, and especially in humans. This change in perspective will lead to a more realistic view of the technology, and allow for better policy decisions.', 'abstract_zh': '人工智能风险与安全问题随通用人工智能（AGI）的到来日益关键：AI/ML社区的AGI主导愿景需要演进', 'title_zh': '位置论文：有界对齐：从AGI代理中期望（不期望）什么'}
{'arxiv_id': 'arXiv:2505.11861', 'title': 'Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity', 'authors': 'Qi Zhou, Jie Zhang, Dongxia Wang, Qiang Liu, Tianlin Li, Jin Song Dong, Wenhai Wang, Qing Guo', 'link': 'https://arxiv.org/abs/2505.11861', 'abstract': 'Human preference plays a crucial role in the refinement of large language models (LLMs). However, collecting human preference feedback is costly and most existing datasets neglect the correlation between personalization and preferences. To address this issue, we introduce Fair-PP, a synthetic dataset of personalized preferences targeting social equity, derived from real-world social survey data, which includes 28 social groups, 98 equity topics, and 5 personal preference dimensions. Leveraging GPT-4o-mini, we engage in role-playing based on seven representative persona portrayals guided by existing social survey data, yielding a total of 238,623 preference records. Through Fair-PP, we also contribute (i) An automated framework for generating preference data, along with a more fine-grained dataset of personalized preferences; (ii) analysis of the positioning of the existing mainstream LLMs across five major global regions within the personalized preference space; and (iii) a sample reweighting method for personalized preference alignment, enabling alignment with a target persona while maximizing the divergence from other personas. Empirical experiments show our method outperforms the baselines.', 'abstract_zh': '人类偏好在大型语言模型的精炼过程中扮演着至关重要的角色。然而，收集人类偏好反馈的成本较高，且现有大多数数据集忽略了个性化与偏好之间的关联。为解决这一问题，我们引入了Fair-PP，这是一个基于真实世界社会调查数据的合成个性化偏好数据集，旨在促进社会公平，该数据集包含28个社会群体、98项公平主题和5个偏好维度。借助GPT-4o-mini，我们基于现有社会调查数据中的七个代表性人物画像进行角色扮演，生成了总计238,623条偏好记录。通过Fair-PP，我们还贡献了（i）生成偏好数据的自动化框架，以及更细致的个性化偏好数据集；（ii）对主流大型语言模型在全球五大地区内的个性化偏好空间中的定位进行分析；以及（iii）一种个性化偏好对齐的样本加权方法，该方法允许与目标人物画像对齐的同时最大程度地远离其他人物画像。实证实验表明，我们的方法优于基线方法。', 'title_zh': 'Fair-PP：一个合成数据集，用于 Alignment with 社会公平个性化偏好的大型语言模型'}
{'arxiv_id': 'arXiv:2505.11849', 'title': 'VeriReason: Reinforcement Learning with Testbench Feedback for Reasoning-Enhanced Verilog Generation', 'authors': 'Yiting Wang, Guoheng Sun, Wanghao Ye, Gang Qu, Ang Li', 'link': 'https://arxiv.org/abs/2505.11849', 'abstract': 'Automating Register Transfer Level (RTL) code generation using Large Language Models (LLMs) offers substantial promise for streamlining digital circuit design and reducing human effort. However, current LLM-based approaches face significant challenges with training data scarcity, poor specification-code alignment, lack of verification mechanisms, and balancing generalization with specialization. Inspired by DeepSeek-R1, we introduce VeriReason, a framework integrating supervised fine-tuning with Guided Reward Proximal Optimization (GRPO) reinforcement learning for RTL generation. Using curated training examples and a feedback-driven reward model, VeriReason combines testbench evaluations with structural heuristics while embedding self-checking capabilities for autonomous error correction. On the VerilogEval Benchmark, VeriReason delivers significant improvements: achieving 83.1% functional correctness on the VerilogEval Machine benchmark, substantially outperforming both comparable-sized models and much larger commercial systems like GPT-4 Turbo. Additionally, our approach demonstrates up to a 2.8X increase in first-attempt functional correctness compared to baseline methods and exhibits robust generalization to unseen designs. To our knowledge, VeriReason represents the first system to successfully integrate explicit reasoning capabilities with reinforcement learning for Verilog generation, establishing a new state-of-the-art for automated RTL synthesis. The models and datasets are available at: this https URL Code is Available at: this https URL', 'abstract_zh': '使用大型语言模型（LLMs）自动化寄存器传输级（RTL）代码生成在简化数字电路设计和减少人力方面具有巨大潜力。然而，当前基于LLM的方法面临训练数据稀缺、规范代码对齐不佳、缺乏验证机制以及泛化与专门化之间平衡的显著挑战。受DeepSeek-R1启发，我们提出了一种VeriReason框架，该框架结合了监督微调与引导奖励近端优化（GRPO）强化学习，用于RTL生成。通过精选的训练示例和反馈驱动的奖励模型，VeriReason结合了测试平台评估与结构启发式方法，同时嵌入自检功能以实现自主错误校正。在VerilogEval基准测试中，VeriReason实现了显著改进：在VerilogEval Machine基准测试中达到83.1%的功能正确性，大幅优于同等大小的模型及更大规模的商用系统如GPT-4 Turbo。此外，我们的方法在首次尝试时的功能正确性上显示最高可达2.8倍的提高，且在未见过的设计上表现出色的泛化能力。据我们所知，VeriReason是第一个将显式推理能力与Verilog生成的强化学习集成的系统，确立了自动化RTL综合的新前沿。模型和数据集可从以下链接获取：this https URL代码可在以下链接获取：this https URL', 'title_zh': 'VeriReason: 基于测试台反馈的强化学习与推理增强的Verilog生成'}
{'arxiv_id': 'arXiv:2505.11839', 'title': 'On the Eligibility of LLMs for Counterfactual Reasoning: A Decompositional Study', 'authors': 'Shuai Yang, Qi Yang, Luoxi Tang, Jeremy Blackburn, Zhaohan Xi', 'link': 'https://arxiv.org/abs/2505.11839', 'abstract': 'Counterfactual reasoning has emerged as a crucial technique for generalizing the reasoning capabilities of large language models (LLMs). By generating and analyzing counterfactual scenarios, researchers can assess the adaptability and reliability of model decision-making. Although prior work has shown that LLMs often struggle with counterfactual reasoning, it remains unclear which factors most significantly impede their performance across different tasks and modalities. In this paper, we propose a decompositional strategy that breaks down the counterfactual generation from causality construction to the reasoning over counterfactual interventions. To support decompositional analysis, we investigate 11 datasets spanning diverse tasks, including natural language understanding, mathematics, programming, and vision-language tasks. Through extensive evaluations, we characterize LLM behavior across each decompositional stage and identify how modality type and intermediate reasoning influence performance. By establishing a structured framework for analyzing counterfactual reasoning, this work contributes to the development of more reliable LLM-based reasoning systems and informs future elicitation strategies.', 'abstract_zh': '基于因果构建的分解策略：探究大规模语言模型在反事实推理中的表现与影响因素', 'title_zh': 'LLMs在反事实推理中的恰当性：一项分解性研究'}
{'arxiv_id': 'arXiv:2505.11833', 'title': 'ToLeaP: Rethinking Development of Tool Learning with Large Language Models', 'authors': 'Haotian Chen, Zijun Song, Boye Niu, Ke Zhang, Litu Ou, Yaxi Lu, Zhong Zhang, Xin Cong, Yankai Lin, Zhiyuan Liu, Maosong Sun', 'link': 'https://arxiv.org/abs/2505.11833', 'abstract': 'Tool learning, which enables large language models (LLMs) to utilize external tools effectively, has garnered increasing attention for its potential to revolutionize productivity across industries. Despite rapid development in tool learning, key challenges and opportunities remain understudied, limiting deeper insights and future advancements. In this paper, we investigate the tool learning ability of 41 prevalent LLMs by reproducing 33 benchmarks and enabling one-click evaluation for seven of them, forming a Tool Learning Platform named ToLeaP. We also collect 21 out of 33 potential training datasets to facilitate future exploration. After analyzing over 3,000 bad cases of 41 LLMs based on ToLeaP, we identify four main critical challenges: (1) benchmark limitations induce both the neglect and lack of (2) autonomous learning, (3) generalization, and (4) long-horizon task-solving capabilities of LLMs. To aid future advancements, we take a step further toward exploring potential directions, namely (1) real-world benchmark construction, (2) compatibility-aware autonomous learning, (3) rationale learning by thinking, and (4) identifying and recalling key clues. The preliminary experiments demonstrate their effectiveness, highlighting the need for further research and exploration.', 'abstract_zh': '大型语言模型的工具学习能力研究：ToLeaP平台及其挑战探索', 'title_zh': 'ToLeP: 重新思考工具学习ewith大型语言模型的发展'}
{'arxiv_id': 'arXiv:2505.11792', 'title': 'Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling', 'authors': 'Yitian Chen, Jingfan Xia, Siyu Shao, Dongdong Ge, Yinyu Ye', 'link': 'https://arxiv.org/abs/2505.11792', 'abstract': 'Optimization modeling is fundamental to decision-making across diverse this http URL progress in automating optimization formulation from natural language descriptions, Large Language Models (LLMs) often struggle to generate formally correct and usable models due to hallucinations, posing a challenge for reliable automation. Inspired by the success of Reinforcement Learning (RL) in enhancing Large Reasoning Models, we present Solver-Informed Reinforcement Learning (SIRL).This novel framework leverages external optimization solvers as verifiable reward mechanisms to significantly improve the authenticity of LLMs for optimization this http URL as precise verifiers, these solvers automatically assess the executable code and the instance-level mathematical model represented by the associated LP file, yielding precise and comprehensive feedback signals -- including syntax, feasibility, and solution quality that directly inform the RL process. This automated verification process, powered by classic optimization solvers, also underpins our instance-enhanced self-consistency method to synthesize high-quality training data. Extensive experiments on diverse public benchmarks demonstrate that SIRL achieves state-of-the-art performance, substantially outperforming existing methods in generating accurate and executable optimization models.', 'abstract_zh': '基于求解器指导的强化学习方法：优化建模中的自动化与可靠化', 'title_zh': '基于求解器的RL：为真实的优化建模grounding大型语言模型'}
{'arxiv_id': 'arXiv:2505.11730', 'title': 'Rethinking Optimal Verification Granularity for Compute-Efficient Test-Time Scaling', 'authors': 'Hao Mark Chen, Guanxi Lu, Yasuyuki Okoshi, Zhiwen Mo, Masato Motomura, Hongxiang Fan', 'link': 'https://arxiv.org/abs/2505.11730', 'abstract': 'Test-time scaling (TTS) has proven effective in enhancing the reasoning capabilities of large language models (LLMs). Verification plays a key role in TTS, simultaneously influencing (1) reasoning performance and (2) compute efficiency, due to the quality and computational cost of verification. In this work, we challenge the conventional paradigms of verification, and make the first attempt toward systematically investigating the impact of verification granularity-that is, how frequently the verifier is invoked during generation, beyond verifying only the final output or individual generation steps. To this end, we introduce Variable Granularity Search (VG-Search), a unified algorithm that generalizes beam search and Best-of-N sampling via a tunable granularity parameter g. Extensive experiments with VG-Search under varying compute budgets, generator-verifier configurations, and task attributes reveal that dynamically selecting g can improve the compute efficiency and scaling behavior. Building on these findings, we propose adaptive VG-Search strategies that achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over Best-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to support future research.', 'abstract_zh': 'Test-time Scaling (TTS) 经验证有效于提升大型语言模型（LLMs）的推理能力。验证在 TTS 中扮演关键角色，同时影响（1）推理性能和（2）计算效率，这取决于验证的质量和计算成本。在本工作中，我们挑战了传统的验证范式，并首次系统地探讨了验证粒度的影响——即生成过程中验证器被调用的频率，而不仅仅是验证最终输出或单独的生成步骤。为此，我们引入了可调粒度搜索（VG-Search），这是一种统一算法，通过可调节的粒度参数 g  generalizes 并扩展了束搜索和 Best-of-N 抽样。在不同计算预算、生成器-验证器配置和任务属性下进行的大量实验表明，动态选择 g 可以提高计算效率和扩展行为。基于这些发现，我们提出了适应性 VG-Search 策略，这些策略在束搜索的基础上实现了高达 3.1% 的准确率提升，在 Best-of-N 的基础上实现了 3.6% 的准确率提升，同时减少了超过 52% 的 FLOPs。我们将开源代码以支持未来的研究。', 'title_zh': '重新思考计算高效测试时扩展下的最优验证粒度'}
{'arxiv_id': 'arXiv:2505.11718', 'title': 'REMOR: Automated Peer Review Generation with LLM Reasoning and Multi-Objective Reinforcement Learning', 'authors': 'Pawin Taechoyotin, Daniel Acuna', 'link': 'https://arxiv.org/abs/2505.11718', 'abstract': 'AI-based peer review systems tend to produce shallow and overpraising suggestions compared to human feedback. Here, we evaluate how well a reasoning LLM trained with multi-objective reinforcement learning (REMOR) can overcome these limitations. We start by designing a multi-aspect reward function that aligns with human evaluation of reviews. The aspects are related to the review itself (e.g., criticisms, novelty) and the relationship between the review and the manuscript (i.e., relevance). First, we perform supervised fine-tuning of DeepSeek-R1-Distill-Qwen-7B using LoRA on PeerRT, a new dataset of high-quality top AI conference reviews enriched with reasoning traces. We then apply Group Relative Policy Optimization (GRPO) to train two models: REMOR-H (with the human-aligned reward) and REMOR-U (with a uniform reward). Interestingly, the human-aligned reward penalizes aspects typically associated with strong reviews, leading REMOR-U to produce qualitatively more substantive feedback. Our results show that REMOR-U and REMOR-H achieve more than twice the average rewards of human reviews, non-reasoning state-of-the-art agentic multi-modal AI review systems, and general commercial LLM baselines. We found that while the best AI and human reviews are comparable in quality, REMOR avoids the long tail of low-quality human reviews. We discuss how reasoning is key to achieving these improvements and release the Human-aligned Peer Review Reward (HPRR) function, the Peer Review Reasoning-enriched Traces (PeerRT) dataset, and the REMOR models, which we believe can help spur progress in the area.', 'abstract_zh': '基于AI的同行评审系统倾向于产生浅薄且过度赞扬的建议，而人类反馈则不然。这里，我们评估了一种使用多目标强化学习（REMOR）训练的推理LLM如何克服这些局限。我们首先设计了一个多方面奖赏函数，该函数与人类对评审的评估相一致。这些方面与评审本身（如批评、新颖性）和评审与手稿的关系（相关性）相关。我们首先使用LoRA对DeepSeek-R1-Distill-Qwen-7B进行有监督的微调，并在包含推理痕迹的新优质顶级AI会议评审数据集PeerRT上进行。然后，我们应用组相对策略优化（GRPO）训练两个模型：REMOR-H（采用与人类对齐的奖赏）和REMOR-U（采用均匀的奖赏）。有趣的是，与人类对齐的奖赏惩罚了通常与高质量评审相关的方面，导致REMOR-U生成了更具实质性的反馈。我们的结果显示，REMOR-U和REMOR-H在平均奖赏方面超过了人类评审、无推理的最先进多模态AI评审系统以及通用商业LLM基线两倍以上。我们发现，虽然最佳AI和人类评审在质量上可比，但REMOR避免了低质量人类评审的尾部效应。我们讨论了推理为何对于实现这些改进至关重要，并发布了与人类对齐的同行评审奖赏函数（HPRR）、同伴评审推理增强痕迹（PeerRT）数据集以及REMOR模型，我们相信这将有助于推动该领域的发展。', 'title_zh': 'REMOR：基于LLM推理和多目标强化学习的自动化同伴评审生成'}
{'arxiv_id': 'arXiv:2505.11701', 'title': 'DMN-Guided Prompting: A Low-Code Framework for Controlling LLM Behavior', 'authors': 'Shaghayegh Abedi, Amin Jalali', 'link': 'https://arxiv.org/abs/2505.11701', 'abstract': 'Large Language Models (LLMs) have shown considerable potential in automating decision logic within knowledge-intensive processes. However, their effectiveness largely depends on the strategy and quality of prompting. Since decision logic is typically embedded in prompts, it becomes challenging for end users to modify or refine it. Decision Model and Notation (DMN) offers a standardized graphical approach for defining decision logic in a structured, user-friendly manner. This paper introduces a DMN-guided prompting framework that breaks down complex decision logic into smaller, manageable components, guiding LLMs through structured decision pathways. We implemented the framework in a graduate-level course where students submitted assignments. The assignments and DMN models representing feedback instructions served as inputs to our framework. The instructor evaluated the generated feedback and labeled it for performance assessment. Our approach demonstrated promising results, outperforming chain-of-thought (CoT) prompting. Students also responded positively to the generated feedback, reporting high levels of perceived usefulness in a survey based on the Technology Acceptance Model.', 'abstract_zh': '大型语言模型（LLMs）在知识密集型过程中的决策逻辑自动化方面展现了显著潜力，但其效果很大程度上取决于提示策略和质量。由于决策逻辑通常嵌入在提示中，这使得终端用户修改或精炼它变得具有挑战性。决策模型与表示法（DMN）提供了一种标准化的图形化方法，以结构化和用户友好的方式定义决策逻辑。本文介绍了一种由DMN引导的提示框架，将复杂的决策逻辑分解为更小、更易于管理的组件，指导LLMs通过结构化的决策路径。我们在一门研究生课程中实现了该框架，学生提交作业，作业和表示反馈指令的DMN模型作为框架的输入。教师评估生成的反馈并据此进行绩效评估。我们的方法显示出令人鼓舞的结果，优于基于推理链（CoT）的提示方法。学生还对生成的反馈给出了积极的反应，在基于技术接受模型的调查中报告了较高的感知有用性。', 'title_zh': 'DMN引导提示：一种低代码LLM行为控制框架'}
{'arxiv_id': 'arXiv:2505.11646', 'title': 'FLOW-BENCH: Towards Conversational Generation of Enterprise Workflows', 'authors': 'Evelyn Duesterwald, Siyu Huo, Vatche Isahagian, K.R. Jayaram, Ritesh Kumar, Vinod Muthusamy, Punleuk Oum, Debashish Saha, Gegi Thomas, Praveen Venkateswaran', 'link': 'https://arxiv.org/abs/2505.11646', 'abstract': 'Business process automation (BPA) that leverages Large Language Models (LLMs) to convert natural language (NL) instructions into structured business process artifacts is becoming a hot research topic. This paper makes two technical contributions -- (i) FLOW-BENCH, a high quality dataset of paired natural language instructions and structured business process definitions to evaluate NL-based BPA tools, and support bourgeoning research in this area, and (ii) FLOW-GEN, our approach to utilize LLMs to translate natural language into an intermediate representation with Python syntax that facilitates final conversion into widely adopted business process definition languages, such as BPMN and DMN. We bootstrap FLOW-BENCH by demonstrating how it can be used to evaluate the components of FLOW-GEN across eight LLMs of varying sizes. We hope that FLOW-GEN and FLOW-BENCH catalyze further research in BPA making it more accessible to novice and expert users.', 'abstract_zh': '利用大规模语言模型将自然语言指令转换为结构化业务过程模型的业务过程自动化（BPA）：FLOW-BENCH和FLOW-GEN的贡献', 'title_zh': 'FLOW-BENCH: 向 toward 企业工作流对话生成方向努力'}
{'arxiv_id': 'arXiv:2505.11618', 'title': 'Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges', 'authors': 'Pengrui Quan, Brian Wang, Kang Yang, Liying Han, Mani Srivastava', 'link': 'https://arxiv.org/abs/2505.11618', 'abstract': 'Spatiotemporal reasoning plays a key role in Cyber-Physical Systems (CPS). Despite advances in Large Language Models (LLMs) and Large Reasoning Models (LRMs), their capacity to reason about complex spatiotemporal signals remains underexplored. This paper proposes a hierarchical SpatioTemporal reAsoning benchmaRK, STARK, to systematically evaluate LLMs across three levels of reasoning complexity: state estimation (e.g., predicting field variables, localizing and tracking events in space and time), spatiotemporal reasoning over states (e.g., inferring spatial-temporal relationships), and world-knowledge-aware reasoning that integrates contextual and domain knowledge (e.g., intent prediction, landmark-aware navigation). We curate 26 distinct spatiotemporal tasks with diverse sensor modalities, comprising 14,552 challenges where models answer directly or by Python Code Interpreter. Evaluating 3 LRMs and 8 LLMs, we find LLMs achieve limited success in tasks requiring geometric reasoning (e.g., multilateration or triangulation), particularly as complexity increases. Surprisingly, LRMs show robust performance across tasks with various levels of difficulty, often competing or surpassing traditional first-principle-based methods. Our results show that in reasoning tasks requiring world knowledge, the performance gap between LLMs and LRMs narrows, with some LLMs even surpassing LRMs. However, the LRM o3 model continues to achieve leading performance across all evaluated tasks, a result attributed primarily to the larger size of the reasoning models. STARK motivates future innovations in model architectures and reasoning paradigms for intelligent CPS by providing a structured framework to identify limitations in the spatiotemporal reasoning of LLMs and LRMs.', 'abstract_zh': '空间-temporal推理在 cyber-physical 系统（CPS）中发挥着关键作用。尽管在大规模语言模型（LLMs）和大规模推理模型（LRMs）方面取得了进展，但它们在复杂空间-temporal信号推理方面的能力仍然未被充分探索。本文提出了一种分层空间-temporal推理基准STARK，系统性地评估LLMs在三种推理复杂度层次上的表现：状态估计（例如，预测场变量、在空间和时间中定位和跟踪事件）、状态的空间-temporal推理（例如，推断空间-temporal关系），以及融合上下文和领域知识的世界知识推理（例如，意图预测、地标引导导航）。我们收集了26个具有多种传感器模态的独立空间-temporal任务，共计14,552个挑战，其中模型直接回答或使用Python代码解释器。评估了3个LRMs和8个LLMs，发现LLMs在需要几何推理的任务（例如，多边orth或测距）中取得有限的成功，特别是随着复杂性的增加。令人惊讶的是，LRMs在不同难度级别的任务中表现出稳健的性能，经常与或超过传统的基于第一原理的方法。我们的结果表明，在需要世界知识的推理任务中，LLMs和LRMs之间的性能差距缩小，某些LLMs甚至超过了LRMs。然而，LRM o3模型在所有评估任务中继续保持领先表现，这一结果主要归因于推理模型的更大规模。STARK通过提供一个结构化的框架来识别LLMs和LRMs在空间-temporal推理方面的局限性，从而激发了智能CPS中模型架构和推理范式的未来创新。', 'title_zh': '基于时空推理能力与挑战的大型语言模型及推理模型基准研究'}
{'arxiv_id': 'arXiv:2505.11614', 'title': 'Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions', 'authors': 'Jian-Qiao Zhu, Hanbo Xie, Dilip Arumugam, Robert C. Wilson, Thomas L. Griffiths', 'link': 'https://arxiv.org/abs/2505.11614', 'abstract': 'A central goal of cognitive modeling is to develop models that not only predict human behavior but also provide insight into the underlying cognitive mechanisms. While neural network models trained on large-scale behavioral data often achieve strong predictive performance, they typically fall short in offering interpretable explanations of the cognitive processes they capture. In this work, we explore the potential of pretrained large language models (LLMs) to serve as dual-purpose cognitive models--capable of both accurate prediction and interpretable explanation in natural language. Specifically, we employ reinforcement learning with outcome-based rewards to guide LLMs toward generating explicit reasoning traces for explaining human risky choices. Our findings demonstrate that this approach produces high-quality explanations alongside strong quantitative predictions of human decisions.', 'abstract_zh': '认知建模的一个中心目标是开发既能预测人类行为又能揭示其背后认知机制的模型。尽管在大规模行为数据上训练的神经网络模型往往能实现较强的预测性能，但在提供可解释的认知过程解释方面通常较为欠缺。在本研究中，我们探讨了预训练大型语言模型（LLMs）作为兼具准确预测和自然语言可解释解释双重目的认知模型的潜力。具体而言，我们利用基于结果的强化学习来引导LLMs生成解释人类冒险选择的明确推理痕迹。我们的研究结果表明，这种方法能够在产生高质量解释的同时，对人类决策进行强大的定量预测。', 'title_zh': '使用强化学习训练大型语言模型解释人类决策'}
{'arxiv_id': 'arXiv:2505.11612', 'title': 'Heart2Mind: Human-Centered Contestable Psychiatric Disorder Diagnosis System using Wearable ECG Monitors', 'authors': 'Hung Nguyen, Alireza Rahimi, Veronica Whitford, Hélène Fournier, Irina Kondratova, René Richard, Hung Cao', 'link': 'https://arxiv.org/abs/2505.11612', 'abstract': 'Psychiatric disorders affect millions globally, yet their diagnosis faces significant challenges in clinical practice due to subjective assessments and accessibility concerns, leading to potential delays in treatment. To help address this issue, we present Heart2Mind, a human-centered contestable psychiatric disorder diagnosis system using wearable electrocardiogram (ECG) monitors. Our approach leverages cardiac biomarkers, particularly heart rate variability (HRV) and R-R intervals (RRI) time series, as objective indicators of autonomic dysfunction in psychiatric conditions. The system comprises three key components: (1) a Cardiac Monitoring Interface (CMI) for real-time data acquisition from Polar H9/H10 devices; (2) a Multi-Scale Temporal-Frequency Transformer (MSTFT) that processes RRI time series through integrated time-frequency domain analysis; (3) a Contestable Diagnosis Interface (CDI) combining Self-Adversarial Explanations (SAEs) with contestable Large Language Models (LLMs). Our MSTFT achieves 91.7% accuracy on the HRV-ACC dataset using leave-one-out cross-validation, outperforming state-of-the-art methods. SAEs successfully detect inconsistencies in model predictions by comparing attention-based and gradient-based explanations, while LLMs enable clinicians to validate correct predictions and contest erroneous ones. This work demonstrates the feasibility of combining wearable technology with Explainable Artificial Intelligence (XAI) and contestable LLMs to create a transparent, contestable system for psychiatric diagnosis that maintains clinical oversight while leveraging advanced AI capabilities. Our implementation is publicly available at: this https URL.', 'abstract_zh': '心理障碍影响全球数以百万计的人，但由于临床实践中主观评估和可及性的问题，其诊断面临重大挑战，可能导致治疗延误。为解决这一问题，我们提出Heart2Mind，这是一种基于可穿戴心电图（ECG）监测的人本中心可争议心理障碍诊断系统。该方法利用心脏生物标志物，特别是心率变异性（HRV）和R-R间隔（RRI）时间序列作为心理障碍条件下自主功能障碍的客观指标。该系统包括三个关键组件：（1）心监测界面（CMI），用于从Polar H9/H10设备实时获取数据；（2）多尺度时频变压器（MSTFT），通过综合时频域分析处理RRI时间序列；（3）可争议诊断界面（CDI），结合自我对抗解释（SAEs）与可争议的大语言模型（LLMs）。我们的MSTFT在使用留一交叉验证的HRV-ACC数据集上达到了91.7%的准确率，超过了最先进的方法。SAEs通过比较基于注意力和梯度的解释成功检测模型预测中的不一致，而LLMs使临床医生能够验证正确的预测并质疑错误的预测。本研究展示了将可穿戴技术与可解释的人工智能（XAI）和可争议的大语言模型结合以创建一种透明、可争议的心理障碍诊断系统的可行性，同时保持临床监督并利用高级人工智能能力。我们的实现已公开发布于：this https URL。', 'title_zh': '心联思维：基于可争议可穿戴ECG监测的人本精神障碍诊断系统'}
{'arxiv_id': 'arXiv:2505.11611', 'title': 'Probing the Vulnerability of Large Language Models to Polysemantic Interventions', 'authors': 'Bofan Gong, Shiyang Lai, Dawn Song', 'link': 'https://arxiv.org/abs/2505.11611', 'abstract': 'Polysemanticity -- where individual neurons encode multiple unrelated features -- is a well-known characteristic of large neural networks and remains a central challenge in the interpretability of language models. At the same time, its implications for model safety are also poorly understood. Leveraging recent advances in sparse autoencoders, we investigate the polysemantic structure of two small models (Pythia-70M and GPT-2-Small) and evaluate their vulnerability to targeted, covert interventions at the prompt, feature, token, and neuron levels. Our analysis reveals a consistent polysemantic topology shared across both models. Strikingly, we demonstrate that this structure can be exploited to mount effective interventions on two larger, black-box instruction-tuned models (LLaMA3.1-8B-Instruct and Gemma-2-9B-Instruct). These findings suggest not only the generalizability of the interventions but also point to a stable and transferable polysemantic structure that could potentially persist across architectures and training regimes.', 'abstract_zh': '多义性——单个神经元编码多个不相关的特征——是大型神经网络的一个已知特性，仍然是语言模型可解释性中的一个中心挑战。同时，其对模型安全性的影响也不甚明了。借助稀疏自编码器的近期进展，我们研究了两个小型模型（Pythia-70M 和 GPT-2-Small）的多义性结构，并评估了它们在提示、特征、标记和神经元层面受到有针对性的隐蔽干预的脆弱性。我们的分析揭示了这两种模型中存在的统一的多义性拓扑结构。令人惊讶的是，我们证明了这种结构可以被利用，以有效地对两个更大、黑盒指令调优模型（LLaMA3.1-8B-Instruct 和 Gemma-2-9B-Instruct）实施干预。这些发现不仅表明了干预的普适性，还指出了一个稳定且可转移的多义性结构，这种结构可能在不同架构和训练制度下持续存在。', 'title_zh': '探测大型语言模型对多义干预的脆弱性'}
{'arxiv_id': 'arXiv:2505.11584', 'title': 'LLM Agents Are Hypersensitive to Nudges', 'authors': 'Manuel Cherep, Pattie Maes, Nikhil Singh', 'link': 'https://arxiv.org/abs/2505.11584', 'abstract': 'LLMs are being set loose in complex, real-world environments involving sequential decision-making and tool use. Often, this involves making choices on behalf of human users. However, not much is known about the distribution of such choices, and how susceptible they are to different choice architectures. We perform a case study with a few such LLM models on a multi-attribute tabular decision-making problem, under canonical nudges such as the default option, suggestions, and information highlighting, as well as additional prompting strategies. We show that, despite superficial similarities to human choice distributions, such models differ in subtle but important ways. First, they show much higher susceptibility to the nudges. Second, they diverge in points earned, being affected by factors like the idiosyncrasy of available prizes. Third, they diverge in information acquisition strategies: e.g. incurring substantial cost to reveal too much information, or selecting without revealing any. Moreover, we show that simple prompt strategies like zero-shot chain of thought (CoT) can shift the choice distribution, and few-shot prompting with human data can induce greater alignment. Yet, none of these methods resolve the sensitivity of these models to nudges. Finally, we show how optimal nudges optimized with a human resource-rational model can similarly increase LLM performance for some models. All these findings suggest that behavioral tests are needed before deploying models as agents or assistants acting on behalf of users in complex environments.', 'abstract_zh': '大规模语言模型在涉及序列决策和工具使用的真实复杂环境中的行为研究：来自几种模型的案例分析', 'title_zh': 'LLM代理对暗示过度敏感'}
{'arxiv_id': 'arXiv:2505.13448', 'title': 'CIE: Controlling Language Model Text Generations Using Continuous Signals', 'authors': 'Vinay Samuel, Harshita Diddee, Yiming Zhang, Daphne Ippolito', 'link': 'https://arxiv.org/abs/2505.13448', 'abstract': 'Aligning language models with user intent is becoming increasingly relevant to enhance user experience. This calls for designing methods that can allow users to control the properties of the language that LMs generate. For example, controlling the length of the generation, the complexity of the language that gets chosen, the sentiment, tone, etc. Most existing work attempts to integrate users\' control by conditioning LM generations on natural language prompts or discrete control signals, which are often brittle and hard to scale. In this work, we are interested in \\textit{continuous} control signals, ones that exist along a spectrum that can\'t easily be captured in a natural language prompt or via existing techniques in conditional generation. Through a case study in controlling the precise response-length of generations produced by LMs, we demonstrate how after fine-tuning, behaviors of language models can be controlled via continuous signals -- as vectors that are interpolated between a "low" and a "high" token embedding. Our method more reliably exerts response-length control than in-context learning methods or fine-tuning methods that represent the control signal as a discrete signal. Our full open-sourced code and datasets are available at this https URL.', 'abstract_zh': '将语言模型与用户意图对齐以增强用户体验日益重要，这需要设计方法让用户能够控制语言模型生成的语言的属性。例如，控制生成的长度、所选择的语言的复杂性、情感、语气等。大多数现有工作试图通过用自然语言提示或离散控制信号条件化语言模型的生成来整合用户的控制，但这些方法往往是脆弱的且难以扩展。在本工作中，我们感兴趣的是连续控制信号，这些信号存在于一种难以用自然语言提示或现有条件生成技术捕捉的光谱中。通过控制LM生成的精确响应长度的案例研究，我们展示了经过微调后，可以通过插值于“低”和“高”标记嵌入之间的向量来用连续信号控制语言模型的行为。我们的方法比上下文学习方法或将控制信号表示为离散信号的微调方法更可靠地实现响应长度控制。我们的完整开源代码和数据集可在以下链接获得：this https URL。', 'title_zh': 'CIE: 使用连续信号控制语言模型文本生成'}
{'arxiv_id': 'arXiv:2505.13438', 'title': 'Optimizing Anytime Reasoning via Budget Relative Policy Optimization', 'authors': 'Penghui Qi, Zichen Liu, Tianyu Pang, Chao Du, Wee Sun Lee, Min Lin', 'link': 'https://arxiv.org/abs/2505.13438', 'abstract': 'Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency.', 'abstract_zh': '扩展示时计算对于增强大语言模型推理能力至关重要。现有的方法通常通过强化学习（RL）来最大化推理踪迹结束时可验证的奖励。然而，这些方法只优化大量固定token预算下的最终性能，这在训练和部署中都降低了效率。在本文中，我们提出了一种新的框架AnytimeReasoner，以优化任意时间推理性能，旨在在不同的token预算约束下提高token效率和推理的灵活性。为此，我们将完整的推理过程截断以适应来自先验分布的抽样token预算，迫使模型总结每个截断推理的最优答案以供验证。这引入了推理过程中的可验证密集奖励，促进了RL优化中的更有效责任分配。然后，我们以解耦的方式优化推理和总结策略，以最大化累积奖励。此外，我们引入了一种新的方差减少技术，预算相对策略优化（BRPO），以增强强化推理策略学习过程的稳健性和效率。在数学推理任务中的实证结果显示，我们的方法在各种先验分布下的所有推理预算下均优于GRPO，同时提高训练和token效率。', 'title_zh': '通过预算相对策略优化优化任意时间推理'}
{'arxiv_id': 'arXiv:2505.13425', 'title': 'Learnware of Language Models: Specialized Small Language Models Can Do Big', 'authors': 'Zhi-Hao Tan, Zi-Chen Zhao, Hao-Yu Shi, Xin-Yu Zhang, Peng Tan, Yang Yu, Zhi-Hua Zhou', 'link': 'https://arxiv.org/abs/2505.13425', 'abstract': "The learnware paradigm offers a novel approach to machine learning by enabling users to reuse a set of well-trained models for tasks beyond the models' original purposes. It eliminates the need to build models from scratch, instead relying on specifications (representations of a model's capabilities) to identify and leverage the most suitable models for new tasks. While learnware has proven effective in many scenarios, its application to language models has remained largely unexplored. At the same time, large language models (LLMs) have demonstrated remarkable universal question-answering abilities, yet they face challenges in specialized scenarios due to data scarcity, privacy concerns, and high computational costs, thus more and more specialized small language models (SLMs) are being trained for specific domains. To address these limitations systematically, the learnware paradigm provides a promising solution by enabling maximum utilization of specialized SLMs, and allowing users to identify and reuse them in a collaborative and privacy-preserving manner.\nThis paper presents a preliminary attempt to apply the learnware paradigm to language models. We simulated a learnware system comprising approximately 100 learnwares of specialized SLMs with 8B parameters, fine-tuned across finance, healthcare, and mathematics domains. Each learnware contains an SLM and a specification, which enables users to identify the most relevant models without exposing their own data. Experimental results demonstrate promising performance: by selecting one suitable learnware for each task-specific inference, the system outperforms the base SLMs on all benchmarks. Compared to LLMs, the system outperforms Qwen1.5-110B, Qwen2.5-72B, and Llama3.1-70B-Instruct by at least 14% in finance domain tasks, and surpasses Flan-PaLM-540B (ranked 7th on the Open Medical LLM Leaderboard) in medical domain tasks.", 'abstract_zh': '学习软件范式通过使用户能够重用一组经过充分训练的模型来执行超出模型原始用途的任务，为机器学习提供了一种新颖的方法。它消除了从头开始构建模型的需要，而是依赖于规范（即模型能力的表示）来识别并利用最适合新任务的模型。虽然学习软件在许多场景中已被证明是有效的，但其在语言模型中的应用仍未得到充分探索。同时，大型语言模型（LLMs）展示了令人瞩目的通用问答能力，但由于数据稀缺、隐私问题和高计算成本，在专业场景中仍面临挑战，因此越来越多的专门的小语言模型（SLMs）正在为特定领域进行训练。为了系统地解决这些问题，学习软件范式提供了一种有前途的解决方案，通过最大限度地利用专门的SLMs，并允许用户以协作和隐私保护的方式识别和重用它们。\n\n本文初步尝试将学习软件范式应用于语言模型。我们模拟了一个由约100个专用于不同领域的特化小型语言模型（SLMs，参数量为8B）的学习软件系统，这些SLMs在金融、医疗和数学领域进行了微调。每个学习软件包含一个SLM和一个规范，这使得用户能够在不泄露自身数据的情况下识别最相关的模型。实验结果显示出有希望的性能：通过为每个特定任务选择一个合适的语言模型，系统在所有基准上的性能优于基础SLMs。与大型语言模型相比，该系统在金融领域的任务中至少优于Qwen1.5-110B、Qwen2.5-72B和Llama3.1-70B-Instruct 14%，并且在医疗领域的任务中超越了排名第七的Flan-PaLM-540B。', 'title_zh': '语言模型的适配软件：专门化的小型语言模型也能做到大事。'}
{'arxiv_id': 'arXiv:2505.13379', 'title': 'Thinkless: LLM Learns When to Think', 'authors': 'Gongfan Fang, Xinyin Ma, Xinchao Wang', 'link': 'https://arxiv.org/abs/2505.13379', 'abstract': "Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many problems admit straightforward solutions. This motivates an open question: Can LLMs learn when to think? To answer this, we propose Thinkless, a learnable framework that empowers an LLM to adaptively select between short-form and long-form reasoning, based on both task complexity and the model's ability. Thinkless is trained under a reinforcement learning paradigm and employs two control tokens, <short> for concise responses and <think> for detailed reasoning. At the core of our method is a Decoupled Group Relative Policy Optimization (DeGRPO) algorithm, which decomposes the learning objective of hybrid reasoning into two components: (1) a control token loss that governs the selection of the reasoning mode, and (2) a response loss that improves the accuracy of the generated answers. This decoupled formulation enables fine-grained control over the contributions of each objective, stabilizing training and effectively preventing collapse observed in vanilla GRPO. Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% - 90%, significantly improving the efficiency of Reasoning Language Models. The code is available at this https URL", 'abstract_zh': '基于学习的思考与否框架：在复杂逻辑推理任务中适应性选择简短与详尽推理', 'title_zh': 'Thinkless: LLM 学习何时思考'}
{'arxiv_id': 'arXiv:2505.13346', 'title': 'J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization', 'authors': 'Austin Xu, Yilun Zhou, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty', 'link': 'https://arxiv.org/abs/2505.13346', 'abstract': 'To keep pace with the increasing pace of large language models (LLM) development, model output evaluation has transitioned away from time-consuming human evaluation to automatic evaluation, where LLMs themselves are tasked with assessing and critiquing other model outputs. LLM-as-judge models are a class of generative evaluators that excel in evaluating relatively simple domains, like chat quality, but struggle in reasoning intensive domains where model responses contain more substantive and challenging content. To remedy existing judge shortcomings, we explore training judges with reinforcement learning (RL). We make three key contributions: (1) We propose the Equivalent Initial State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us to train our judge to be robust to positional biases that arise in more complex evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that evaluates judges in diverse reasoning settings not covered by prior work. (3) We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or exceeding the performance of larger GRPO-trained judges on both JudgeBench and ReasoningJudgeBench.', 'abstract_zh': '随着大规模语言模型（LLM）开发速度的加快，模型输出评估已从耗时的人工评估转向自动评估，其中LLM本身被赋予评估和批判其他模型输出的任务。LLM作为裁判模型是一类擅长评估相对简单领域（如聊天质量），但在需要更多实质性且具有挑战性内容的推理密集型领域中表现不佳的生成性评估器。为弥补现有裁判的不足，我们探索了使用强化学习（RL）训练裁判的方法。我们做出了三项重要贡献：（1）我们提出了等价初始状态组相对策略优化（EIS-GRPO）算法，以使我们的裁判能够应对更复杂评估环境中出现的位置偏差。（2）我们引入了ReasoningJudgeBench基准测试，该基准测试评估裁判在多元推理场景中的表现，这些场景在以往工作之外。（3）我们训练了Reasoning Judges for Reasoning (J4R)，一种使用EIS-GRPO训练的7B裁判，其性能分别比GPT-4o和下一个最佳的小裁判高出6.7%和9%，在JudgeBench和ReasoningJudgeBench上的表现与更大规模的GRPO训练裁判相当或超过。', 'title_zh': 'J4R: 基于等初始状态组相对偏好优化的判断学习'}
{'arxiv_id': 'arXiv:2505.13338', 'title': 'Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation', 'authors': 'Qiongqiong Wang, Hardik B. Sailor, Tianchi Liu, Ai Ti Aw', 'link': 'https://arxiv.org/abs/2505.13338', 'abstract': "Current speech-LLMs exhibit limited capability in contextual reasoning alongside paralinguistic understanding, primarily due to the lack of Question-Answer (QA) datasets that cover both aspects. We propose a novel framework for dataset generation from in-the-wild speech data, that integrates contextual reasoning with paralinguistic information. It consists of a pseudo paralinguistic label-based data condensation of in-the-wild speech and LLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is validated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct model on a dataset created by our framework and human-generated CPQA dataset. The results also reveal the speech-LLM's limitations in handling empathetic reasoning tasks, highlighting the need for such datasets and more robust models. The proposed framework is first of its kind and has potential in training more robust speech-LLMs with paralinguistic reasoning capabilities.", 'abstract_zh': '当前的语音大语言模型在上下文推理和副语言理解方面的能力有限，主要原因是缺乏涵盖这两方面的问答数据集。我们提出了一种从野生语音数据中生成数据的新框架，该框架将上下文推理与副语言信息集成。该框架包括基于伪副语言标签的野生语音数据 condensation 和基于大语言模型的上下文副语言问答（CPQA）生成。通过在使用我们框架创建的数据集上评估 Qwen2-Audio-7B-Instruct 模型与人工生成的 CPQA 数据集之间的强相关性，验证了其有效性。结果还揭示了语音大语言模型在处理共情推理任务方面的局限性，突显了此类数据集和更稳健模型的需求。所提出的框架是首创的，并有可能用于训练具有副语言推理能力的更 robust 的语音大语言模型。', 'title_zh': '多模态语音-LLM中的上下文副语言数据创建：数据凝练与口语化问答生成'}
{'arxiv_id': 'arXiv:2505.13308', 'title': 'Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space', 'authors': 'Hengli Li, Chenxi Li, Tong Wu, Xuekai Zhu, Yuxuan Wang, Zhaoxin Yu, Eric Hanchen Jiang, Song-Chun Zhu, Zixia Jia, Ying Nian Wu, Zilong Zheng', 'link': 'https://arxiv.org/abs/2505.13308', 'abstract': "Reasoning ability, a core component of human intelligence, continues to pose a significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training algorithms, such as catastrophic forgetting, and the limited availability of novel training data. As an alternative, test-time scaling enhances reasoning performance by increasing test-time computation without parameter updating. Unlike prior methods in this paradigm focused on token space, we propose leveraging latent space for more effective reasoning and better adherence to the test-time scaling law. We introduce LatentSeek, a novel framework that enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA) within the model's latent space. Specifically, LatentSeek leverages policy gradient to iteratively update latent representations, guided by self-generated reward signals. LatentSeek is evaluated on a range of reasoning benchmarks, including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures. Results show that LatentSeek consistently outperforms strong baselines, such as Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our analysis demonstrates that LatentSeek is highly efficient, typically converging within a few iterations for problems of average complexity, while also benefiting from additional iterations, thereby highlighting the potential of test-time scaling in the latent space. These findings position LatentSeek as a lightweight, scalable, and effective solution for enhancing the reasoning capabilities of LLMs.", 'abstract_zh': 'Large Language Models Reasoning Enhancement via Test-Time Instance-level Adaptation in Latent Space', 'title_zh': '在黑暗中寻找：在潜在空间通过测试时实例级策略梯度进行推理'}
{'arxiv_id': 'arXiv:2505.13307', 'title': 'RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning', 'authors': 'Qiguang Chen, Libo Qin, Jinhao Liu, Yue Liao, Jiaqi Wang, Jingxuan Zhou, Wanxiang Che', 'link': 'https://arxiv.org/abs/2505.13307', 'abstract': 'Chain-of-Thought (CoT) reasoning has proven effective in enhancing large language models (LLMs) on complex tasks, spurring research into its underlying mechanisms. However, two primary challenges remain for real-world applications: (1) the lack of quantitative metrics and actionable guidelines for evaluating and optimizing measurable boundaries of CoT capability, and (2) the absence of methods to assess boundaries of unmeasurable CoT capability, such as multimodal perception. To address these gaps, we introduce the Reasoning Boundary Framework++ (RBF++). To tackle the first challenge, we define the reasoning boundary (RB) as the maximum limit of CoT performance. We also propose a combination law for RBs, enabling quantitative analysis and offering actionable guidance across various CoT tasks. For the second challenge, particularly in multimodal scenarios, we introduce a constant assumption, which replaces unmeasurable RBs with scenario-specific constants. Additionally, we propose the reasoning boundary division mechanism, which divides unmeasurable RBs into two sub-boundaries, facilitating the quantification and optimization of both unmeasurable domain knowledge and multimodal perception capabilities. Extensive experiments involving 38 models across 13 tasks validate the feasibility of our framework in cross-modal settings. Additionally, we evaluate 10 CoT strategies, offer insights into optimization and decay from two complementary perspectives, and expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope this work advances the understanding of RBs and optimization strategies in LLMs. Code and data are available at this https URL.', 'abstract_zh': 'Chain-of-Thought（CoT）推理边界框架++（RBF++）：跨模态环境中的可测量与不可测量推理边界的探索与优化', 'title_zh': 'RBF++：衡量与优化可测量和不可测量能力的推理边界以提高链式推理效果'}
{'arxiv_id': 'arXiv:2505.13192', 'title': 'True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics', 'authors': 'Christoph Jürgen Hemmer, Daniel Durstewitz', 'link': 'https://arxiv.org/abs/2505.13192', 'abstract': "Complex, temporally evolving phenomena, from climate to brain activity, are governed by dynamical systems (DS). DS reconstruction (DSR) seeks to infer generative surrogate models of these from observed data, reproducing their long-term behavior. Existing DSR approaches require purpose-training for any new system observed, lacking the zero-shot and in-context inference capabilities known from LLMs. Here we introduce DynaMix, a novel multivariate ALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR model able to generalize zero-shot to out-of-domain DS. Just from a provided context signal, without any re-training, DynaMix faithfully forecasts the long-term evolution of novel DS where existing time series (TS) foundation models, like Chronos, fail -- at a fraction of the number of parameters and orders of magnitude faster inference times. DynaMix outperforms TS foundation models in terms of long-term statistics, and often also short-term forecasts, even on real-world time series, like traffic or weather data, typically used for training and evaluating TS models, but not at all part of DynaMix' training corpus. We illustrate some of the failure modes of TS models for DSR problems, and conclude that models built on DS principles may bear a huge potential also for advancing the TS prediction field.", 'abstract_zh': '复杂随时间演变的现象，从气候到脑活动，均由动力系统（DS）支配。DS重构（DSR）旨在从观测数据中推断这些现象的生成替代模型，再现其长期行为。现有的DSR方法需要为每个新观测系统进行特定训练，缺乏类似于LLMs的零样本和上下文推理能力。我们引入了DynaMix，这是一种基于多变量ALRNN的混合专家架构，预先训练用于DSR，是第一个能够泛化到领域外DS的DSR模型。仅通过提供上下文信号，无需任何重新训练，DynaMix能够准确预测现有时间序列（TS）基础模型（如Chronos）无法解决问题的新型DS的长期演化——参数数量大幅减少，推理速度也快多个数量级。DynaMix在长期统计方面优于TS基础模型，并且在许多情况下，在短期预测方面也表现更好，即使是在通常用于训练和评估TS模型的真实世界时间序列数据（如交通或天气数据）上，这些数据根本不包含于DynaMix的训练语料库中。我们展示了TS模型在DSR问题上的几种失败模式，并得出结论，基于DS原理构建的模型可能对推进时间序列预测领域有着巨大的潜力。', 'title_zh': '真零样本推断动力系统并保持长期统计特性'}
{'arxiv_id': 'arXiv:2505.13176', 'title': 'ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models', 'authors': 'Zihao Cheng, Hongru Wang, Zeming Liu, Yuhang Guo, Yuanfang Guo, Yunhong Wang, Haifeng Wang', 'link': 'https://arxiv.org/abs/2505.13176', 'abstract': "While integrating external tools into large language models (LLMs) enhances their ability to access real-time information and domain-specific services, existing approaches focus narrowly on functional tool selection following user instructions, overlooking the context-aware personalization in tool selection. This oversight leads to suboptimal user satisfaction and inefficient tool utilization, particularly when overlapping toolsets require nuanced selection based on contextual factors. To bridge this gap, we introduce ToolSpectrum, a benchmark designed to evaluate LLMs' capabilities in personalized tool utilization. Specifically, we formalize two key dimensions of personalization, user profile and environmental factors, and analyze their individual and synergistic impacts on tool utilization. Through extensive experiments on ToolSpectrum, we demonstrate that personalized tool utilization significantly improves user experience across diverse scenarios. However, even state-of-the-art LLMs exhibit the limited ability to reason jointly about user profiles and environmental factors, often prioritizing one dimension at the expense of the other. Our findings underscore the necessity of context-aware personalization in tool-augmented LLMs and reveal critical limitations for current models. Our data and code are available at this https URL.", 'abstract_zh': '外部工具集成到大型语言模型中增强了其访问实时信息和领域特定服务的能力，但现有方法在工具选择上仅专注于用户指令的功能性选择，忽视了基于上下文的个性化选择。这种忽视导致了用户满意度的降低和工具利用效率的低下，尤其是在工具集重叠的情况下，需要根据上下文因素进行细致的选择。为解决这一问题，我们提出了ToolSpectrum这一基准测试，旨在评估大型语言模型在个性化工具利用方面的能力。具体而言，我们形式化了个人化选择的两个关键维度——用户画像和环境因素，并分析了它们在单独及协同作用下的工具利用影响。通过在ToolSpectrum上的大量实验，我们证明了个性化工具利用可以显著改进跨不同场景的用户体验。然而，即使是最先进的大型语言模型，在综合考虑用户画像和环境因素方面也表现出有限的能力，往往优先考虑其中一个维度而牺牲另一个维度。我们的研究结果强调了工具增强的大语言模型中上下文感知个性化的重要性，并揭示了当前模型的关键限制。我们的数据和代码可在以下链接获取：this https URL。', 'title_zh': 'ToolSpectrum: 向量化大型语言模型个性化工具使用的研究'}
{'arxiv_id': 'arXiv:2505.13157', 'title': 'Role-Playing Evaluation for Large Language Models', 'authors': 'Yassine El Boudouri, Walter Nuninger, Julian Alvarez, Yvan Peter', 'link': 'https://arxiv.org/abs/2505.13157', 'abstract': 'Large Language Models (LLMs) demonstrate a notable capacity for adopting personas and engaging in role-playing. However, evaluating this ability presents significant challenges, as human assessments are resource-intensive and automated evaluations can be biased. To address this, we introduce Role-Playing Eval (RPEval), a novel benchmark designed to assess LLM role-playing capabilities across four key dimensions: emotional understanding, decision-making, moral alignment, and in-character consistency. This article details the construction of RPEval and presents baseline evaluations. Our code and dataset are available at this https URL', 'abstract_zh': '大规模语言模型（LLMs）表现出接纳人设和角色扮演的能力。然而，评估这种能力面临显著挑战，因为人类评估资源密集，而自动评估可能会产生偏见。为解决这一问题，我们引入了角色扮演评估（RPEval），这是一种新型基准，旨在从情感理解、决策制定、道德对齐和入戏一致性四个关键维度评估LLM的角色扮演能力。本文详细介绍了RPEval的构建并呈现了基线评估。我们的代码和数据集可在以下链接获取：this https URL。', 'title_zh': '大型语言模型的角色扮演评价'}
{'arxiv_id': 'arXiv:2505.13156', 'title': 'Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice', 'authors': 'Zhi Liu, Tao Yang, Jing Wang, Yexin Chen, Zhan Gao, Jiaxi Yang, Kui Chen, Bingji Lu, Xiaochen Li, Changyong Luo, Yan Li, Xiaohong Gu, Peng Cao', 'link': 'https://arxiv.org/abs/2505.13156', 'abstract': 'Natural medicines, particularly Traditional Chinese Medicine (TCM), are gaining global recognition for their therapeutic potential in addressing human symptoms and diseases. TCM, with its systematic theories and extensive practical experience, provides abundant resources for healthcare. However, the effective application of TCM requires precise syndrome diagnosis, determination of treatment principles, and prescription formulation, which demand decades of clinical expertise. Despite advancements in TCM-based decision systems, machine learning, and deep learning research, limitations in data and single-objective constraints hinder their practical application. In recent years, large language models (LLMs) have demonstrated potential in complex tasks, but lack specialization in TCM and face significant challenges, such as too big model scale to deploy and issues with hallucination. To address these challenges, we introduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and specifically designed for TCM, pre-trained and fine-tuned on diverse TCM corpora, including classical texts, expert treatises, clinical records, and knowledge graphs. Tianyi is designed to assimilate interconnected and systematic TCM knowledge through a progressive learning manner. Additionally, we establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in TCM examinations, clinical tasks, domain-specific question-answering, and real-world trials. The extensive evaluations demonstrate the significant potential of Tianyi as an AI assistant in TCM clinical practice and research, bridging the gap between TCM knowledge and practical application.', 'abstract_zh': '自然药物，尤其是中国传统 medicine（TCM），因其在治疗人类症状和疾病方面的潜在疗效而获得全球认可。TCM凭借其系统理论和丰富的实践经验，提供了丰富的医疗资源。然而，TCM的有效应用需要精确的病证诊断、治疗原则确定和处方制定，这需要多年临床经验。尽管基于TCM的决策系统、机器学习和深度学习研究已经取得进展，但在数据限制和单一目标约束下，它们的实际应用受到限制。近年来，大规模语言模型（LLMs）在复杂任务中展现了潜力，但在TCM专业性方面仍面临重大挑战，如模型规模过大难以部署和幻觉问题。为应对这些挑战，我们引入了含76亿参数的天弈模型（Tianyi），其模型规模合理且专门针对TCM设计，基于多样化的TCM语料库进行预训练和微调，包括古典文献、专家著作、临床记录和知识图谱。天弈旨在通过渐进式学习方式吸收相互关联和系统的TCM知识。此外，我们建立了TCMEval综合评估基准，以评估LLM在TCM考试、临床任务、领域特定问答和真实世界试验中的性能。广泛的评估表明，天弈作为TCM临床实践和研究中的AI助手具有巨大潜力，缩小了TCM知识与实际应用之间的差距。', 'title_zh': '天意：中医药全科语言模型及其临床实践应用'}
{'arxiv_id': 'arXiv:2505.13136', 'title': 'ModernGBERT: German-only 1B Encoder Model Trained from Scratch', 'authors': 'Anton Ehrmanntraut, Julia Wunderle, Jan Pfister, Fotis Jannidis, Andreas Hotho', 'link': 'https://arxiv.org/abs/2505.13136', 'abstract': 'Despite the prominence of decoder-only language models, encoders remain crucial for resource-constrained applications. We introduce ModernGBERT (134M, 1B), a fully transparent family of German encoder models trained from scratch, incorporating architectural innovations from ModernBERT. To evaluate the practical trade-offs of training encoders from scratch, we also present LLäMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German decoder-only models via LLM2Vec. We benchmark all models on natural language understanding, text embedding, and long-context reasoning tasks, enabling a controlled comparison between dedicated encoders and converted decoders. Our results show that ModernGBERT 1B outperforms prior state-of-the-art German encoders as well as encoders adapted via LLM2Vec, with regard to performance and parameter-efficiency. All models, training data, checkpoints and code are publicly available, advancing the German NLP ecosystem with transparent, high-performance encoder models.', 'abstract_zh': '尽管解码器主导的语言模型备受瞩目，但编码器在资源受限的应用中仍然至关重要。我们介绍了ModernGBERT（134M，1B）这一完全透明的德语编码器模型家族，从头开始训练，整合了ModernBERT的架构创新。为了评估从头训练编码器的实际权衡，我们还呈现了LLäMmlein2Vec（120M，1B，7B）这一德语解码器模型家族，它是通过LLM2Vec转换自德语解码器模型。我们在自然语言理解、文本嵌入和长上下文推理任务中对所有模型进行了基准测试，从而使专用编码器与转换后的解码器之间能够进行受控比较。我们的结果显示，ModernGBERT 1B在性能和参数效率方面优于先前的最佳德语编码器以及通过LLM2Vec转换的编码器。所有模型、训练数据、检查点和代码均已公开，推动了透明、高性能德语NLP生态系统的建设。', 'title_zh': 'ModernGBERT: 从头训练的1B编码器模型（仅限德语）'}
{'arxiv_id': 'arXiv:2505.13115', 'title': 'Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning', 'authors': 'Debarpan Bhattacharya, Apoorva Kulkarni, Sriram Ganapathy', 'link': 'https://arxiv.org/abs/2505.13115', 'abstract': 'The popular success of text-based large language models (LLM) has streamlined the attention of the multimodal community to combine other modalities like vision and audio along with text to achieve similar multimodal capabilities. In this quest, large audio language models (LALMs) have to be evaluated on reasoning related tasks which are different from traditional classification or generation tasks. Towards this goal, we propose a novel dataset called temporal reasoning evaluation of audio (TREA).\nWe benchmark open-source LALMs and observe that they are consistently behind human capabilities on the tasks in the TREA dataset. While evaluating LALMs, we also propose an uncertainty metric, which computes the invariance of the model to semantically identical perturbations of the input. Our analysis shows that the accuracy and uncertainty metrics are not necessarily correlated and thus, points to a need for wholesome evaluation of LALMs for high-stakes applications.', 'abstract_zh': '基于文本的大型语言模型的流行成功吸引了多模态社区的注意力，以结合其他模态如视觉和音频，以实现类似的多模态能力。在此过程中，大型音频语言模型（LALMs）需要在与传统分类或生成任务不同的推理相关任务上进行评估。为了实现这一目标，我们提出了一种新的数据集，称为时间推理评估音频（TREA）。\n\n我们对开源LALMs进行了基准测试，并观察到它们在TREA数据集的任务上始终落后于人类的能力。在评估LALMs时，我们还提出了一种不确定性度量方法，该方法计算模型对输入语义上相同扰动的不变性。我们的分析表明，准确性和不确定性度量并非总是相关的，这表明应全面评估LALMs以适应高风险应用。', 'title_zh': 'LALMs在时间推理领域中的基准测试与置信度评估'}
{'arxiv_id': 'arXiv:2505.13109', 'title': 'FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference', 'authors': 'Guangda Liu, Chengwei Li, Zhenyu Ning, Jing Lin, Yiwu Yao, Danning Ke, Minyi Guo, Jieru Zhao', 'link': 'https://arxiv.org/abs/2505.13109', 'abstract': 'Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\\times$ speedup compared to SOTA KV retrieval methods.', 'abstract_zh': '大型语言模型（LLMs）通过快速扩展上下文窗口得到了广泛应用，以支持日益 demanding的应用。然而，长上下文带来了重大的部署挑战，主要原因是键值缓存（KV缓存）的大小与上下文长度成正比增长。虽然提出了KV缓存压缩方法来解决此问题，但KV丢弃方法会导致显著的准确度损失，而KV检索方法则面临显著的效率瓶颈。我们提出FreeKV，一种算法与系统协同优化框架，旨在在保持准确度的同时提高KV检索效率。在算法方面，FreeKV引入了投机性检索，将KV的选择和召回过程移出关键路径，并结合细粒度校正以确保准确度。在系统方面，FreeKV采用CPU和GPU内存跨平台的混合KV布局，以消除碎片化数据传输，并利用双缓冲流式召回以进一步提高效率。实验表明，FreeKV在各种场景和模型下实现了几乎无损的准确度，并比目前最先进的KV检索方法快至13倍。', 'title_zh': 'FreeKV：提升键值缓存检索以实现高效的LLM推理'}
{'arxiv_id': 'arXiv:2505.13082', 'title': 'MultiActor-Audiobook: Zero-Shot Audiobook Generation with Faces and Voices of Multiple Speakers', 'authors': 'Kyeongman Park, Seongho Joo, Kyomin Jung', 'link': 'https://arxiv.org/abs/2505.13082', 'abstract': "We introduce MultiActor-Audiobook, a zero-shot approach for generating audiobooks that automatically produces consistent, expressive, and speaker-appropriate prosody, including intonation and emotion. Previous audiobook systems have several limitations: they require users to manually configure the speaker's prosody, read each sentence with a monotonic tone compared to voice actors, or rely on costly training. However, our MultiActor-Audiobook addresses these issues by introducing two novel processes: (1) MSP (**Multimodal Speaker Persona Generation**) and (2) LSI (**LLM-based Script Instruction Generation**). With these two processes, MultiActor-Audiobook can generate more emotionally expressive audiobooks with a consistent speaker prosody without additional training. We compare our system with commercial products, through human and MLLM evaluations, achieving competitive results. Furthermore, we demonstrate the effectiveness of MSP and LSI through ablation studies.", 'abstract_zh': '多演员有声书：一种零-shot 方法，自动生成一致、表达丰富且适合演讲者风格的语调和情绪的有声书', 'title_zh': '多演讲者有声书生成：基于多说话人面部和声音的零样本有声书生成'}
{'arxiv_id': 'arXiv:2505.13076', 'title': 'The Hidden Dangers of Browsing AI Agents', 'authors': 'Mykyta Mudryi, Markiyan Chaklosh, Grzegorz Wójcik', 'link': 'https://arxiv.org/abs/2505.13076', 'abstract': 'Autonomous browsing agents powered by large language models (LLMs) are increasingly used to automate web-based tasks. However, their reliance on dynamic content, tool execution, and user-provided data exposes them to a broad attack surface. This paper presents a comprehensive security evaluation of such agents, focusing on systemic vulnerabilities across multiple architectural layers. Our work outlines the first end-to-end threat model for browsing agents and provides actionable guidance for securing their deployment in real-world environments. To address discovered threats, we propose a defense in depth strategy incorporating input sanitization, planner executor isolation, formal analyzers, and session safeguards. These measures protect against both initial access and post exploitation attack vectors. Through a white box analysis of a popular open source project, Browser Use, we demonstrate how untrusted web content can hijack agent behavior and lead to critical security breaches. Our findings include prompt injection, domain validation bypass, and credential exfiltration, evidenced by a disclosed CVE and a working proof of concept exploit.', 'abstract_zh': '由大规模语言模型驱动的自主浏览代理日益用于自动化基于Web的任务。然而，它们对动态内容、工具执行和用户提供的数据的依赖性暴露出广泛的攻击面。本文对这类代理进行了全面的安全评估，重点关注跨多个架构层的系统性漏洞。我们的工作提出了首个端到端的浏览代理威胁模型，并提供了在实际环境中部署代理时的安全指导。为了应对发现的威胁，我们提出了多层次防御策略，包括输入 sanitization、规划者执行器隔离、形式化分析器和会话保护措施。这些措施保护了初始访问和后利用攻击向量。通过对流行的开源项目Browser Use的白盒分析，我们展示了不可信的Web内容如何劫持代理行为并导致关键安全漏洞。我们的发现包括提示注入、域名验证绕过和凭证泄露，所述漏洞得到了披露的CVE和一个可行的漏洞利用示例的支持。', 'title_zh': '浏览AI代理隐藏的危险'}
{'arxiv_id': 'arXiv:2505.13073', 'title': 'Structure-Aware Corpus Construction and User-Perception-Aligned Metrics for Large-Language-Model Code Completion', 'authors': 'Dengfeng Liu, Jucai Zhai, Xiaoguang Jiang, Ziqun Li, Qianjin Yu, Feng Liu, Rui Ye, Huang Liu, Zhiguo Yang, Yongsheng Du, Fang Tan', 'link': 'https://arxiv.org/abs/2505.13073', 'abstract': "Code completion technology based on large language model has significantly improved the development efficiency of programmers. However, in practical applications, there remains a gap between current commonly used code completion evaluation metrics and users' actual perception. To address this issue, we propose two evaluation metrics for code completion tasks--LCP and ROUGE-LCP, from the perspective of probabilistic modeling. Furthermore, to tackle the lack of effective structural semantic modeling and cross-module dependency information in LLMs for repository-level code completion scenarios, we propose a data processing method based on a Structure-Preserving and Semantically-Reordered Code Graph (SPSR-Graph). Through theoretical analysis and experimental validation, we demonstrate the superiority of the proposed evaluation metrics in terms of user perception consistency, as well as the effectiveness of the data processing method in enhancing model performance.", 'abstract_zh': '基于大型语言模型的代码补全技术显著提高了程序员的开发效率。然而，在实际应用中，当前常用的代码补全评价指标与用户的实际感知之间仍存在差距。为解决这一问题，我们从概率建模的角度提出了两种代码补全任务的评价指标——LCP和ROUGE-LCP。为进一步解决大型语言模型在仓库级代码补全场景中缺乏有效的结构语义建模和跨模块依赖信息的问题，我们提出了一种基于结构保持和语义重排代码图（SPSR-Graph）的数据处理方法。通过理论分析和实验验证，我们展示了所提出评价指标在用户感知一致性方面的优越性，以及数据处理方法在提高模型性能方面的有效性。', 'title_zh': '结构感知语料构建与用户感知导向的代码补全评估指标'}
{'arxiv_id': 'arXiv:2505.13036', 'title': "KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025", 'authors': 'Sai Koneru, Maike Züfle, Thai-Binh Nguyen, Seymanur Akti, Jan Niehues, Alexander Waibel', 'link': 'https://arxiv.org/abs/2505.13036', 'abstract': "The scope of the International Workshop on Spoken Language Translation (IWSLT) has recently broadened beyond traditional Speech Translation (ST) to encompass a wider array of tasks, including Speech Question Answering and Summarization. This shift is partly driven by the growing capabilities of modern systems, particularly with the success of Large Language Models (LLMs). In this paper, we present the Karlsruhe Institute of Technology's submissions for the Offline ST and Instruction Following (IF) tracks, where we leverage LLMs to enhance performance across all tasks. For the Offline ST track, we propose a pipeline that employs multiple automatic speech recognition systems, whose outputs are fused using an LLM with document-level context. This is followed by a two-step translation process, incorporating additional refinement step to improve translation quality. For the IF track, we develop an end-to-end model that integrates a speech encoder with an LLM to perform a wide range of instruction-following tasks. We complement it with a final document-level refinement stage to further enhance output quality by using contextual information.", 'abstract_zh': '国际口语翻译研讨会（IWSLT）的范围已从传统的口语翻译拓展到涵盖更多的任务，包括口语问答和摘要。随着大规模语言模型（LLMs）的成功，这一转变部分是由现代系统能力的不断提高驱动的。在本文中，我们介绍了卡尔斯鲁厄理工学院在离线口语翻译（Offline ST）和指令跟随（IF）赛道上的提交内容，我们利用LLMs来提高所有任务的表现。对于离线口语翻译轨道，我们提出了一种管道，该管道使用多个自动语音识别系统，并通过具有文档级上下文的LLM融合其输出。随后进行两步翻译过程，并增加额外的润色步骤以提高翻译质量。对于指令跟随轨道，我们开发了一种端到端模型，该模型将语音编码器与LLM结合，以执行各种指令跟随任务。并通过最终的文档级润色阶段进一步利用上下文信息提高输出质量。', 'title_zh': 'KIT的离线语音翻译及指令跟随提交：IWSLT 2025'}
{'arxiv_id': 'arXiv:2505.13028', 'title': 'Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset', 'authors': 'Sayon Palit, Daniel Woods', 'link': 'https://arxiv.org/abs/2505.13028', 'abstract': 'Large Language Models (LLMs) are increasingly integrated into critical systems in industries like healthcare and finance. Users can often submit queries to LLM-enabled chatbots, some of which can enrich responses with information retrieved from internal databases storing sensitive data. This gives rise to a range of attacks in which a user submits a malicious query and the LLM-system outputs a response that creates harm to the owner, such as leaking internal data or creating legal liability by harming a third-party. While security tools are being developed to counter these threats, there is little formal evaluation of their effectiveness and usability. This study addresses this gap by conducting a thorough comparative analysis of LLM security tools. We identified 13 solutions (9 closed-source, 4 open-source), but only 7 were evaluated due to a lack of participation by proprietary model this http URL evaluate, we built a benchmark dataset of malicious prompts, and evaluate these tools performance against a baseline LLM model (ChatGPT-3.5-Turbo). Our results show that the baseline model has too many false positives to be used for this task. Lakera Guard and ProtectAI LLM Guard emerged as the best overall tools showcasing the tradeoff between usability and performance. The study concluded with recommendations for greater transparency among closed source providers, improved context-aware detections, enhanced open-source engagement, increased user awareness, and the adoption of more representative performance metrics.', 'abstract_zh': '大型语言模型安全性工具的全面比较研究', 'title_zh': '评估大规模语言模型安全解决方案的效果：帕利特基准数据集'}
{'arxiv_id': 'arXiv:2505.13026', 'title': 'Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs', 'authors': 'Jack Chen, Fazhong Liu, Naruto Liu, Yuhan Luo, Erqu Qin, Harry Zheng, Tian Dong, Haojin Zhu, Yan Meng, Xiao Wang', 'link': 'https://arxiv.org/abs/2505.13026', 'abstract': "Large language models (LLMs) excel at mathematical reasoning and logical problem-solving. The current popular training paradigms primarily use supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the models' reasoning abilities. However, when using SFT or RL alone, there are respective challenges: SFT may suffer from overfitting, while RL is prone to mode collapse. The state-of-the-art methods have proposed hybrid training schemes. However, static switching faces challenges such as poor generalization across different tasks and high dependence on data quality. In response to these challenges, inspired by the curriculum learning-quiz mechanism in human reasoning cultivation, We propose SASR, a step-wise adaptive hybrid training framework that theoretically unifies SFT and RL and dynamically balances the two throughout optimization. SASR uses SFT for initial warm-up to establish basic reasoning skills, and then uses an adaptive dynamic adjustment algorithm based on gradient norm and divergence relative to the original distribution to seamlessly integrate SFT with the online RL method GRPO. By monitoring the training status of LLMs and adjusting the training process in sequence, SASR ensures a smooth transition between training schemes, maintaining core reasoning abilities while exploring different paths. Experimental results demonstrate that SASR outperforms SFT, RL, and static hybrid training methods.", 'abstract_zh': '大规模语言模型在数学推理和逻辑问题解决方面表现出色。当前流行的训练范式主要采用监督微调（SFT）和强化学习（RL）来增强模型的推理能力。然而，单独使用SFT或RL都会面临各自的挑战：SFT可能遭受过拟合，而RL容易出现模式崩溃。最新的方法提出了混合训练方案。但是，静态切换面临着跨不同任务的泛化能力差和对数据质量的高度依赖等问题。为应对这些挑战，受人类推理培养中课程学习-测试机制的启发，我们提出了一种逐步自适应混合训练框架SASR，该框架在理论上统一了SFT和RL，并在优化过程中动态平衡两者。SASR使用SFT进行初始暖启动以建立基本的推理技能，然后使用基于梯度范数和相对于原始分布的偏差的自适应动态调整算法，无缝地将SFT与在线RL方法GRPO结合。通过监控LLMs的训练状态并按顺序调整训练过程，SASR确保了训练方案之间的平滑过渡，既保持核心推理能力又探索不同的路径。实验结果表明，SASR优于SFT、RL和静态混合训练方法。', 'title_zh': '面向特定任务的大型语言模型逐步自适应集成监督微调和强化学习方法'}
{'arxiv_id': 'arXiv:2505.12996', 'title': 'ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning', 'authors': 'Jiaan Wang, Fandong Meng, Jie Zhou', 'link': 'https://arxiv.org/abs/2505.12996', 'abstract': 'In recent years, the emergence of large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex problems, e.g., mathematics and coding. Some pioneering studies attempt to bring the success of LRMs in neural machine translation (MT). They try to build LRMs with deep reasoning MT ability via reinforcement learning (RL). Despite some progress that has been made, these attempts generally focus on several high-resource languages, e.g., English and Chinese, leaving the performance on other languages unclear. Besides, the reward modeling methods in previous work do not fully unleash the potential of reinforcement learning in MT. In this work, we first design a new reward modeling method that compares the translation results of the policy MT model with a strong LRM (i.e., DeepSeek-R1-671B), and quantifies the comparisons to provide rewards. Experimental results demonstrate the superiority of the reward modeling method. Using Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new state-of-the-art performance in literary translation, and outperforms strong LRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to the multilingual settings with 11 languages. With a carefully designed lightweight reward modeling in RL, we can simply transfer the strong MT ability from a single direction into multiple (i.e., 90) translation directions and achieve impressive multilingual MT performance.', 'abstract_zh': '近年来，大型推理模型（LRMs）如OpenAI-o1和DeepSeek-R1在复杂问题上的表现，例如数学和编码方面展现出令人印象深刻的能力。一些开创性研究试图将LRMs的成功应用到神经机器翻译（MT）中。它们尝试通过强化学习（RL）构建具有深度推理MT能力的LRMs。尽管取得了一些进展，但这些尝试通常主要关注一些资源丰富的语言，例如英语和汉语，其他语言的表现则不明确。此外，以往工作中使用的奖励建模方法并未充分挖掘强化学习在MT中的潜力。在这项工作中，我们首先设计了一种新的奖励建模方法，该方法将策略MT模型的翻译结果与强大的LRM（即DeepSeek-R1-671B）进行比较，并量化这种比较以提供奖励。实验结果表明该奖励建模方法具有优越性。使用Qwen2.5-7B-Instruct作为主干，训练后的模型在文学翻译方面达到了新的最佳性能，并超越了包括OpenAI-o1和DeepSeek-R1在内的强大LRMs。此外，我们将方法扩展至多语言环境，包括11种语言。通过精心设计的轻量级奖励建模在RL中，可以简单地将单一方向的强MT能力扩展到多个方向（即90个翻译方向），从而实现令人印象深刻的语言间机器翻译性能。', 'title_zh': 'ExTrans: 通过范例增强强化学习的多语言深度推理翻译'}
{'arxiv_id': 'arXiv:2505.12992', 'title': 'Fractured Chain-of-Thought Reasoning', 'authors': 'Baohao Liao, Hanze Dong, Yuhui Xu, Doyen Sahoo, Christof Monz, Junnan Li, Caiming Xiong', 'link': 'https://arxiv.org/abs/2505.12992', 'abstract': 'Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by generating rich intermediate reasoning trajectories, but these approaches incur substantial token costs that impede their deployment in latency-sensitive settings. In this work, we first show that truncated CoT, which stops reasoning before completion and directly generates the final answer, often matches full CoT sampling while using dramatically fewer tokens. Building on this insight, we introduce Fractured Sampling, a unified inference-time strategy that interpolates between full CoT and solution-only sampling along three orthogonal axes: (1) the number of reasoning trajectories, (2) the number of final solutions per trajectory, and (3) the depth at which reasoning traces are truncated. Through extensive experiments on five diverse reasoning benchmarks and several model scales, we demonstrate that Fractured Sampling consistently achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling gains in Pass@k versus token budget. Our analysis reveals how to allocate computation across these dimensions to maximize performance, paving the way for more efficient and scalable LLM reasoning.', 'abstract_zh': '推理时缩放技术通过在推理时利用额外的计算资源来显著增强大型语言模型的推理能力，而无需重新训练。类似地，带有推理链（Chain-of-Thought，CoT）的提示及其扩展Long CoT通过生成丰富的中间推理轨迹来提高准确性，但这些方法会带来显著的标记成本，阻碍了它们在延迟敏感设置中的部署。在本文中，我们首先展示了截断CoT的方法，该方法在推理未完成时直接生成最终答案，通常与完整的CoT抽样具有类似的效果，但使用了大幅减少的标记。在此基础上，我们提出了一种统一的推理时策略——断层抽样（Fractured Sampling），该策略在推理轨迹的数量、每条轨迹的最终解决方案数量以及推理轨迹的截断深度这三个正交维度上插值，以实现完整的CoT和仅解题抽样的平衡。通过在五个不同的推理基准和多个模型规模上的广泛实验，我们证明了断层抽样在准确性和成本之间始终实现了更优的权衡，使得在标记预算上的累进线性扩展收益更加显著。我们的分析揭示了如何在这些维度上分配计算资源以最大化性能，从而为更高效和可扩展的大型语言模型推理铺平了道路。', 'title_zh': '断裂的思维链 reasoning'}
{'arxiv_id': 'arXiv:2505.12983', 'title': 'An Empirical Study of Many-to-Many Summarization with Large Language Models', 'authors': 'Jiaan Wang, Fandong Meng, Zengkui Sun, Yunlong Liang, Yuxuan Cao, Jiarong Xu, Haoxiang Shi, Jie Zhou', 'link': 'https://arxiv.org/abs/2505.12983', 'abstract': "Many-to-many summarization (M2MS) aims to process documents in any language and generate the corresponding summaries also in any language. Recently, large language models (LLMs) have shown strong multi-lingual abilities, giving them the potential to perform M2MS in real applications. This work presents a systematic empirical study on LLMs' M2MS ability. Specifically, we first reorganize M2MS data based on eight previous domain-specific datasets. The reorganized data contains 47.8K samples spanning five domains and six languages, which could be used to train and evaluate LLMs. Then, we benchmark 18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned traditional models (e.g., mBART) are also conducted for comparisons. Our experiments reveal that, zero-shot LLMs achieve competitive results with fine-tuned traditional models. After instruct-tuning, open-source LLMs can significantly improve their M2MS ability, and outperform zero-shot LLMs (including GPT-4) in terms of automatic evaluations. In addition, we demonstrate that this task-specific improvement does not sacrifice the LLMs' general task-solving abilities. However, as revealed by our human evaluation, LLMs still face the factuality issue, and the instruction tuning might intensify the issue. Thus, how to control factual errors becomes the key when building LLM summarizers in real applications, and is worth noting in future research.", 'abstract_zh': '多语言到多语言摘要生成能力的系统性 empirical 研究：大规模语言模型的潜力与挑战', 'title_zh': '大型语言模型中多对多总结的实证研究'}
{'arxiv_id': 'arXiv:2505.12981', 'title': 'From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents', 'authors': 'Liangxuan Wu, Chao Wang, Tianming Liu, Yanjie Zhao, Haoyu Wang', 'link': 'https://arxiv.org/abs/2505.12981', 'abstract': 'The growing adoption of large language models (LLMs) has led to a new paradigm in mobile computing--LLM-powered mobile AI agents--capable of decomposing and automating complex tasks directly on smartphones. However, the security implications of these agents remain largely unexplored. In this paper, we present the first comprehensive security analysis of mobile LLM agents, encompassing three representative categories: System-level AI Agents developed by original equipment manufacturers (e.g., YOYO Assistant), Third-party Universal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g., Alibaba Mobile Agent). We begin by analyzing the general workflow of mobile agents and identifying security threats across three core capability dimensions: language-based reasoning, GUI-based interaction, and system-level execution. Our analysis reveals 11 distinct attack surfaces, all rooted in the unique capabilities and interaction patterns of mobile LLM agents, and spanning their entire operational lifecycle. To investigate these threats in practice, we introduce AgentScan, a semi-automated security analysis framework that systematically evaluates mobile LLM agents across all 11 attack scenarios. Applying AgentScan to nine widely deployed agents, we uncover a concerning trend: every agent is vulnerable to targeted attacks. In the most severe cases, agents exhibit vulnerabilities across eight distinct attack vectors. These attacks can cause behavioral deviations, privacy leakage, or even full execution hijacking. Based on these findings, we propose a set of defensive design principles and practical recommendations for building secure mobile LLM agents. Our disclosures have received positive feedback from two major device vendors. Overall, this work highlights the urgent need for standardized security practices in the fast-evolving landscape of LLM-driven mobile automation.', 'abstract_zh': '大型语言模型在移动计算中的广泛应用催生了以LLM为动力的移动AI代理，这些代理能够在智能手机上直接分解和自动化复杂任务。然而，这些代理的安全性影响尚未得到充分探索。本文首次对移动LLM代理进行了全面的安全分析，涵盖三大代表类别：系统级AI代理（如YOYO Assistant）、第三方通用代理（如Zhipu AI AutoGLM）和新兴代理框架（如阿里移动代理）。我们首先分析移动代理的一般工作流程，并在语言推理、GUI交互和系统级执行三个核心能力维度上识别安全威胁。我们的分析揭示了11个不同的攻击面，均源于移动LLM代理的独特能力和交互模式，并贯穿其整个运营生命周期。为了在实际中调查这些威胁，我们引入了AgentScan，这是一种半自动化的安全分析框架，系统地评估所有11种攻击场景下的移动LLM代理。将AgentScan应用于九个广泛部署的代理后，我们发现了一个令人担忧的趋势：每个代理都容易受到定向攻击。在最严重的案例中，代理在八个不同的攻击向量上均存在漏洞。这些攻击可能导致行为偏差、隐私泄露，甚至全面执行劫持。基于这些发现，我们提出了构建安全移动LLM代理的一整套防御设计原则和实用建议。我们的披露得到了两家主要设备供应商的积极反馈。总体而言，本项工作强调了在以LLM为驱动的移动自动化快速发展的环境中，需要标准化的安全实践。', 'title_zh': '从助手到对手：探索移动LLM代理的安全风险'}
{'arxiv_id': 'arXiv:2505.12951', 'title': 'DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management', 'authors': 'Xuerui Su, Liya Guo, Yue Wang, Yi Zhu, Zhiming Ma, Zun Wang, Yuting Liu', 'link': 'https://arxiv.org/abs/2505.12951', 'abstract': 'Inference scaling further accelerates Large Language Models (LLMs) toward Artificial General Intelligence (AGI), with large-scale Reinforcement Learning (RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning approaches usually rely on handcrafted rule-based reward functions. However, the tarde-offs of exploration and exploitation in RL algorithms involves multiple complex considerations, and the theoretical and empirical impacts of manually designed reward functions remain insufficiently explored. In this paper, we propose Decoupled Group Reward Optimization (DGRO), a general RL algorithm for LLM reasoning. On the one hand, DGRO decouples the traditional regularization coefficient into two independent hyperparameters: one scales the policy gradient term, and the other regulates the distance from the sampling policy. This decoupling not only enables precise control over balancing exploration and exploitation, but also can be seamlessly extended to Online Policy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward Optimization. On the other hand, we observe that reward variance significantly affects both convergence speed and final model performance. We conduct both theoretical analysis and extensive empirical validation to assess DGRO, including a detailed ablation study that investigates its performance and optimization dynamics. Experimental results show that DGRO achieves state-of-the-art performance on the Logic dataset with an average accuracy of 96.9\\%, and demonstrates strong generalization across mathematical benchmarks.', 'abstract_zh': '推理放大进一步加速了大型语言模型（LLMs）向人工通用智能（AGI）的发展，通过大规模强化学习（RL）释放长链推理能力。大多数当代推理方法通常依赖于手工制作的基于规则的奖励函数。然而，RL算法中的探索与利用权衡涉及多种复杂的考虑因素，手动设计的奖励函数的理论和实证影响尚未得到充分探索。在本文中，我们提出了一种适用于LLM推理的通用RL算法——解耦组奖励优化（DGRO）。一方面，DGRO将传统的正则化系数解耦为两个独立的超参数：一个调整策略梯度项，另一个调节采样策略的距离。这种解耦不仅使得精确控制探索与利用之间的权衡成为可能，还可以无缝扩展到Kimi k1.5中的在线策略镜像下降（OPMD）算法和直接奖励优化中。另一方面，我们观察到奖励方差显著影响收敛速度和最终模型性能。我们通过理论分析和广泛的实证验证评估了DGRO，包括详细剖析其性能和优化动态。实验结果表明，DGRO在逻辑数据集上的平均准确率达到96.9%，并在数学基准测试中展示了较强的泛化能力。', 'title_zh': 'DGRO：通过探索-利用控制和奖励方差管理提升LLM推理能力'}
{'arxiv_id': 'arXiv:2505.12942', 'title': 'A3 : an Analytical Low-Rank Approximation Framework for Attention', 'authors': 'Jeffrey T. H. Wong, Cheng Zhang, Xinye Cao, Pedro Gimenes, George A. Constantinides, Wayne Luk, Yiren Zhao', 'link': 'https://arxiv.org/abs/2505.12942', 'abstract': "Large language models have demonstrated remarkable performance; however, their massive parameter counts make deployment highly expensive. Low-rank approximation offers a promising compression solution, yet existing approaches have two main limitations: (1) They focus on minimizing the output error of individual linear layers, without considering the architectural characteristics of Transformers, and (2) they decompose a large weight matrix into two small low-rank matrices. Consequently, these methods often fall short compared to other compression techniques like pruning and quantization, and introduce runtime overhead such as the extra GEMM kernel launches for decomposed small matrices. To address these limitations, we propose $\\tt A^\\tt 3$, a post-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a Transformer layer into three functional components, namely $\\tt QK$, $\\tt OV$, and $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical solution that reduces the hidden dimension size inside each component while minimizing the component's functional loss ($\\it i.e.$, error in attention scores, attention outputs, and MLP outputs). This approach directly reduces model sizes, KV cache sizes, and FLOPs without introducing any runtime overheads. In addition, it provides a new narrative in advancing the optimization problem from singular linear layer loss optimization toward improved end-to-end performance. Through extensive experiments, we show that $\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example, under the same reduction budget in computation and memory, our low-rank approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2, outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the versatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and mixed-rank assignments for enhanced performance.", 'abstract_zh': '大型语言模型展现了卓越的性能，但其庞大的参数数量使部署成本高昂。低秩逼近提供了一种有前景的压缩解决方案，然而现有方法存在两个主要局限：（1）它们专注于最小化单个线性层的输出误差，而不考虑Transformer的架构特性；（2）它们将一个大权重矩阵分解为两个小的低秩矩阵。因此，这些方法通常在与其他压缩技术（如剪枝和量化）的比较中表现不佳，并且引入了运行时开销，如分解小矩阵所需的额外GEMM内核启动。为了解决这些局限，我们提出了一种训练后低秩逼近框架$\\tt A^\\tt 3$。$\\tt A^\\tt 3$将Transformer层划分为三个功能组件，即$\\tt QK$、$\\tt OV$和$\\tt MLP$。对于每个组件，$\\tt A^\\tt 3$提供了一种分析解决方案，可以在减少每个组件的隐藏维度大小的同时最小化该组件的功能损失（即注意力分数、注意力输出和MLP输出的误差）。这种方法直接减小了模型大小、KV缓存大小和FLOPs，而没有任何运行时开销。此外，它提供了一种新思路，即将单一线性层损失优化的优化问题推进到整体性能的改进。通过大量实验，我们展示了$\\tt A^\\tt 3$相对于当前最佳性能保持了更优异的表现。例如，在相同的计算和内存减少预算下，我们低秩逼近的LLaMA 3.1-70B在WikiText-2上的困惑度为4.69，优于前一项SoTA的7.87，提高了3.18。我们还展示了$\\tt A^\\tt 3$的通用性，包括KV缓存压缩、量化和混合秩分配以增强性能。', 'title_zh': 'A3：注意力的分析性低秩逼近框架'}
{'arxiv_id': 'arXiv:2505.12938', 'title': 'Leveraging LLM Inconsistency to Boost Pass@k Performance', 'authors': 'Uri Dalal, Meirav Segal, Zvika Ben-Haim, Dan Lahav, Omer Nevo', 'link': 'https://arxiv.org/abs/2505.12938', 'abstract': 'Large language models (LLMs) achieve impressive abilities in numerous domains, but exhibit inconsistent performance in response to minor input changes. Rather than view this as a drawback, in this paper we introduce a novel method for leveraging models\' inconsistency to boost Pass@k performance. Specifically, we present a "Variator" agent that generates k variants of a given task and submits one candidate solution for each one. Our variant generation approach is applicable to a wide range of domains as it is task agnostic and compatible with free-form inputs. We demonstrate the efficacy of our agent theoretically using a probabilistic model of the inconsistency effect, and show empirically that it outperforms the baseline on the APPS dataset. Furthermore, we establish that inconsistency persists even in frontier reasoning models across coding and cybersecurity domains, suggesting our method is likely to remain relevant for future model generations.', 'abstract_zh': '大型语言模型在响应小的输入变化时表现出不一致的性能，但这一特性可以被利用以提升Pass@k性能：一种“变异者”代理的方法研究', 'title_zh': '利用大模型不一致性提升Pass@k性能'}
{'arxiv_id': 'arXiv:2505.12929', 'title': 'Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs', 'authors': 'Zhihe Yang, Xufang Luo, Zilong Wang, Dongqi Han, Zhiyuan He, Dongsheng Li, Yunjian Xu', 'link': 'https://arxiv.org/abs/2505.12929', 'abstract': "Reinforcement learning (RL) has become a cornerstone for enhancing the reasoning capabilities of large language models (LLMs), with recent innovations such as Group Relative Policy Optimization (GRPO) demonstrating exceptional effectiveness. In this study, we identify a critical yet underexplored issue in RL training: low-probability tokens disproportionately influence model updates due to their large gradient magnitudes. This dominance hinders the effective learning of high-probability tokens, whose gradients are essential for LLMs' performance but are substantially suppressed. To mitigate this interference, we propose two novel methods: Advantage Reweighting and Low-Probability Token Isolation (Lopti), both of which effectively attenuate gradients from low-probability tokens while emphasizing parameter updates driven by high-probability tokens. Our approaches promote balanced updates across tokens with varying probabilities, thereby enhancing the efficiency of RL training. Experimental results demonstrate that they substantially improve the performance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K Logic Puzzle reasoning tasks. Our implementation is available at this https URL.", 'abstract_zh': '强化学习（RL）已成为提升大规模语言模型（LLMs）推理能力的基石，近期的技术创新，如组相对策略优化（GRPO），展示了卓越的效果。本研究识别了一个在RL训练中尚未充分探索的关键问题：低概率令牌由于其较大的梯度幅度，不成比例地影响模型更新，这阻碍了高概率令牌的有效学习，而这些令牌的梯度对于LLMs的性能至关重要，但它们的梯度被大幅抑制。为减轻这种干扰，我们提出了两种新颖的方法：优势重加权和低概率令牌隔离（Lopti），这两种方法有效衰减了低概率令牌的梯度，同时强调了由高概率令牌驱动的参数更新。我们的方法促进了不同概率令牌之间的均衡更新，从而提高RL训练的效率。实验结果表明，这显著提高了GRPO训练的LLMs的表现，在K&K逻辑谜题推理任务中实现了高达46.2%的性能提升。我们的实现可在以下链接获取：这个 https URL。', 'title_zh': '不要让低概率标记在大型语言模型的RL中过度主导'}
{'arxiv_id': 'arXiv:2505.12871', 'title': 'Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?', 'authors': 'Zi Liang, Haibo Hu, Qingqing Ye, Yaxin Xiao, Ronghua Li', 'link': 'https://arxiv.org/abs/2505.12871', 'abstract': "Low rank adaptation (LoRA) has emerged as a prominent technique for fine-tuning large language models (LLMs) thanks to its superb efficiency gains over previous methods. While extensive studies have examined the performance and structural properties of LoRA, its behavior upon training-time attacks remain underexplored, posing significant security risks. In this paper, we theoretically investigate the security implications of LoRA's low-rank structure during fine-tuning, in the context of its robustness against data poisoning and backdoor attacks. We propose an analytical framework that models LoRA's training dynamics, employs the neural tangent kernel to simplify the analysis of the training process, and applies information theory to establish connections between LoRA's low rank structure and its vulnerability against training-time attacks. Our analysis indicates that LoRA exhibits better robustness to backdoor attacks than full fine-tuning, while becomes more vulnerable to untargeted data poisoning due to its over-simplified information geometry. Extensive experimental evaluations have corroborated our theoretical findings.", 'abstract_zh': 'LoRA的低秩结构在训练时攻击下的安全性分析：基于数据投毒和后门攻击的鲁棒性研究', 'title_zh': '低秩适应会导致训练时攻击下的较低稳健性吗？'}
{'arxiv_id': 'arXiv:2505.12864', 'title': 'LEXam: Benchmarking Legal Reasoning on 340 Law Exams', 'authors': 'Yu Fan, Jingwei Ni, Jakob Merane, Etienne Salimbeni, Yang Tian, Yoan Hermstrüwer, Yinya Huang, Mubashara Akhtar, Florian Geering, Oliver Dreyer, Daniel Brunner, Markus Leippold, Mrinmaya Sachan, Alexander Stremitzer, Christoph Engel, Elliott Ash, Joel Niklaus', 'link': 'https://arxiv.org/abs/2505.12864', 'abstract': 'Long-form legal reasoning remains a key challenge for large language models (LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a novel benchmark derived from 340 law exams spanning 116 law school courses across a range of subjects and degree levels. The dataset comprises 4,886 law exam questions in English and German, including 2,841 long-form, open-ended questions and 2,045 multiple-choice questions. Besides reference answers, the open questions are also accompanied by explicit guidance outlining the expected legal reasoning approach such as issue spotting, rule recall, or rule application. Our evaluation on both open-ended and multiple-choice questions present significant challenges for current LLMs; in particular, they notably struggle with open questions that require structured, multi-step legal reasoning. Moreover, our results underscore the effectiveness of the dataset in differentiating between models with varying capabilities. Adopting an LLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate how model-generated reasoning steps can be evaluated consistently and accurately. Our evaluation setup provides a scalable method to assess legal reasoning quality beyond simple accuracy metrics. Project page: this https URL', 'abstract_zh': '长格式法律推理依然是大型语言模型（LLMs）的一个关键挑战，尽管近期在测试时缩放方面取得了进展。我们引入了LEXam，这是一个新颖的基准，源自涵盖116门法律课程、涉及多种科目和学位级别、共340场法律考试的数据集。该数据集包含4,886道用英语和德语编写的法律考试题目，其中包括2,841道长格式、开放性问题和2,045道多项选择题。除了参考答案外，开放性问题还附有明确的指导，说明预期的法律推理方法，如问题识别、规则回忆或规则应用。我们在开放性问题和多项选择题上的评估对当前的LLMs提出了显著挑战；特别是在应对需要结构化、多步骤法律推理的开放性问题时，它们表现尤为困难。此外，我们的结果突显了该数据集在区分具有不同能力的模型方面的有效性。采用LLM-as-a-Judge范式并结合严格的专家人工验证，我们展示了如何一致且准确地评估模型生成的推理步骤。我们的评估设置提供了一种超越简单准确率指标的评估法律推理质量的可扩展方法。项目页面：this https URL', 'title_zh': 'LEXam：法律推理能力评估标准（基于340套法律考试题）'}
{'arxiv_id': 'arXiv:2505.12843', 'title': 'Bias Fitting to Mitigate Length Bias of Reward Model in RLHF', 'authors': 'Kangwen Zhao, Jianfeng Cai, Jinhua Zhu, Ruopei Sun, Dongyun Xue, Wengang Zhou, Li Li, Houqiang Li', 'link': 'https://arxiv.org/abs/2505.12843', 'abstract': 'Reinforcement Learning from Human Feedback relies on reward models to align large language models with human preferences. However, RLHF often suffers from reward hacking, wherein policy learning exploits flaws in the trained reward model to maximize reward scores without genuinely aligning with human preferences. A significant example of such reward hacking is length bias, where reward models usually favor longer responses irrespective of actual response quality. Previous works on length bias have notable limitations, these approaches either mitigate bias without characterizing the bias form, or simply assume a linear length-reward relation. To accurately model the intricate nature of length bias and facilitate more effective bias mitigation, we propose FiMi-RM (Bias Fitting to Mitigate Length Bias of Reward Model in RLHF), a framework that autonomously learns and corrects underlying bias patterns. Our approach consists of three stages: First, we train a standard reward model which inherently contains length bias. Next, we deploy a lightweight fitting model to explicitly capture the non-linear relation between length and reward. Finally, we incorporate this learned relation into the reward model to debias. Experimental results demonstrate that FiMi-RM achieves a more balanced length-reward distribution. Furthermore, when applied to alignment algorithms, our debiased reward model improves length-controlled win rate and reduces verbosity without compromising its performance.', 'abstract_zh': '基于人类反馈的强化学习依赖于奖励模型将大型语言模型与人类偏好对齐。然而，RLHF经常遭受奖励作弊的问题，其中策略学习利用训练奖励模型中的缺陷以最大化奖励分数，而未能真正与人类偏好对齐。长度偏见是这种奖励作弊的一个显著例子，奖励模型通常倾向于更长的响应，而不论实际响应质量如何。对长度偏见的早期研究存在显著局限性，这些方法要么在未表征偏倚形式的情况下减轻偏倚，要么简单地假设长度与奖励之间的线性关系。为了准确建模长度偏见的复杂性质并促进更有效的偏倚缓解，我们提出了一种框架FiMi-RM（用于缓解强化学习从人类反馈中奖励模型长度偏见的偏差拟合）。该框架自主学习并纠正潜在的偏倚模式。我们的方法包括三个阶段：首先，训练一个标准奖励模型，该模型本身就包含长度偏见。其次，部署一个轻量级的拟合模型以明确捕捉长度与奖励之间的非线性关系。最后，将学到的关系纳入奖励模型以减轻偏差。实验结果表明，FiMi-RM实现了更平衡的长度-奖励分布。此外，在应用于对齐算法时，我们去偏后的奖励模型在控制长度方面提高了胜率，减少verbosity的同时不损害其性能。', 'title_zh': '偏置拟合以减轻奖励模型在RLHF中的长度偏置'}
{'arxiv_id': 'arXiv:2505.12837', 'title': 'The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting', 'authors': 'Christian Braun, Alexander Lilienbeck, Daniel Mentjukov', 'link': 'https://arxiv.org/abs/2505.12837', 'abstract': "Legal contracts possess an inherent, semantically vital structure (e.g., sections, clauses) that is crucial for human comprehension but whose impact on LLM processing remains under-explored. This paper investigates the effects of explicit input text structure and prompt engineering on the performance of GPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the CUAD. We compare model exact-match accuracy across various input formats: well-structured plain-text (human-generated from CUAD), plain-text cleaned of line breaks, extracted plain-text from Azure OCR, plain-text extracted by GPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o Vision. To give an indication of the impact of possible prompt engineering, we assess the impact of shifting task instructions to the system prompt and explicitly informing the model about the structured nature of the input. Our findings reveal that GPT-4o demonstrates considerable robustness to variations in input structure, but lacks in overall performance. Conversely, GPT-4.1's performance is markedly sensitive; poorly structured inputs yield suboptimal results (but identical with GPT-4o), while well-structured formats (original CUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by ~20 percentage points. Optimizing the system prompt to include task details and an advisory about structured input further elevates GPT-4.1's accuracy by an additional ~10-13 percentage points, with Markdown ultimately achieving the highest performance under these conditions (79 percentage points overall exact-match accuracy). This research empirically demonstrates that while newer models exhibit greater resilience, careful input structuring and strategic prompt design remain critical for optimizing the performance of LLMs, and can significantly affect outcomes in high-stakes legal applications.", 'abstract_zh': '法律合同具有一种内在的、语义上至关重要的结构（例如，章节、条款），这对人类的理解至关重要，但其对LLM处理的影响仍鲜有探讨。本文研究了明确定义的输入文本结构和提示工程对GPT-4o和GPT-4在法律问答任务中的性能影响，使用CUAD的部分内容作为实验材料。我们比较了不同输入格式下的模型精确匹配准确率：整洁的纯文本（由人类从CUAD生成）、去除换行符的纯文本、来自Azure OCR的纯文本提取、由GPT-4o Vision提取的纯文本、以及由GPT-4o Vision提取并解析的Markdown（MD）。为评估潜在的提示工程影响，我们分析了将任务指令移至系统提示以及明确告知模型输入结构化性质的影响。研究发现，GPT-4o对输入结构的变化表现出明显的稳健性，但在整体性能上表现不佳。相比之下，GPT-4.1的性能对输入结构变化极为敏感，未结构化的输入会导致次优结果（但与GPT-4o相同），而结构化的格式（原始CUAD文本、GPT-4o Vision文本和GPT-4o MD）可以提高精确匹配准确率约20个百分点。将系统提示优化为包含任务细节和对结构化输入的建议，进一步提升了GPT-4.1的准确率约10-13个百分点，Markdown最终在这些条件下达到最高的性能（整体精确匹配准确率为79个百分点）。这项研究实证表明，虽然最新模型更具韧性，但精心设计的输入结构和战略提示设计仍然是优化LLM性能的关键，并且在高风险的法律应用中可以显著影响结果。', 'title_zh': '隐藏的结构——通过显式文本格式化提高法律文件理解'}
{'arxiv_id': 'arXiv:2505.12821', 'title': 'SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models', 'authors': 'Han Sun, Zhen Sun, Zongmin Zhang, Linzhao Jia, Wei Shao, Min Zhang', 'link': 'https://arxiv.org/abs/2505.12821', 'abstract': 'Large Language Models (LLMs) are emerging as dominant forces for textual style transfer. However, for arbitrary style transfer, LLMs face two key challenges: (1) considerable reliance on manually-constructed prompts and (2) rigid stylistic biases inherent in LLMs. In this paper, we propose a novel Synthesize-then-Decode (SynDec) approach, which automatically synthesizes high-quality prompts and amplifies their roles during decoding process. Specifically, our approach synthesizes prompts by selecting representative few-shot samples, conducting a four-dimensional style analysis, and reranking the candidates. At LLM decoding stage, the TST effect is amplified by maximizing the contrast in output probabilities between scenarios with and without the synthesized prompt, as well as between prompts and negative samples. We conduct extensive experiments and the results show that SynDec outperforms existing state-of-the-art LLM-based methods on five out of six benchmarks (e.g., achieving up to a 9\\% increase in accuracy for modern-to-Elizabethan English transfer). Detailed ablation studies further validate the effectiveness of SynDec.', 'abstract_zh': '大规模语言模型（LLMs）正在成为文本风格转换的主要力量。然而，对于任意风格转换，LLMs面临两个主要挑战：（1）对手动构建的提示依赖较大；（2）固有的风格偏见。在本文中，我们提出了一种新颖的Synthesize-then-Decode（SynDec）方法，该方法自动生成高质量的提示并在解码过程中放大其作用。具体而言，该方法通过选择代表性的少量样本、进行四维风格分析并对候选样本重新排队来生成提示。在LLM解码阶段，通过最大化含有和不含合成提示以及提示和负样本之间输出概率的对比度来放大TST效果。我们在广泛的实验中进行了测试，结果表明SynDec在六个基准中的五个上优于现有最先进的基于LLM的方法（例如，现代英语到伊丽莎白an英语转换的准确性提高多达9%）。详细的消融研究进一步验证了SynDec的有效性。', 'title_zh': 'SynDec：一种通过大型语言模型进行任意文本风格转换的合成-解码方法'}
{'arxiv_id': 'arXiv:2505.12814', 'title': 'PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs', 'authors': 'Xilong Cheng, Yunxiao Qin, Yuting Tan, Zhengnan Li, Ye Wang, Hongjiang Xiao, Yuan Zhang', 'link': 'https://arxiv.org/abs/2505.12814', 'abstract': "Existing LLM-based role-playing methods often rely on superficial textual descriptions or simplistic metrics, inadequately modeling both intrinsic and extrinsic character dimensions. Additionally, they typically simulate character memory with implicit model knowledge or basic retrieval augment generation without explicit memory alignment, compromising memory consistency. The two issues weaken reliability of role-playing LLMs in several applications, such as trustworthy social simulation. To address these limitations, we propose PsyMem, a novel framework integrating fine-grained psychological attributes and explicit memory control for role-playing. PsyMem supplements textual descriptions with 26 psychological indicators to detailed model character. Additionally, PsyMem implements memory alignment training, explicitly trains the model to align character's response with memory, thereby enabling dynamic memory-controlled responding during inference. By training Qwen2.5-7B-Instruct on our specially designed dataset (including 5,414 characters and 38,962 dialogues extracted from novels), the resulting model, termed as PsyMem-Qwen, outperforms baseline models in role-playing, achieving the best performance in human-likeness and character fidelity.", 'abstract_zh': '基于现有LLM的角色扮演方法往往依赖于表面的文字描述或简单的度量标准，无法充分建模内在和外在的人物维度。此外，它们通常通过隐式的模型知识或基本的检索增强生成来模拟人物记忆，而没有明确的记忆对齐，从而影响记忆一致性。这两个问题削弱了角色扮演LLM在诸如可信社会模拟等应用中的可靠性和稳定性。为此，我们提出了PsyMem框架，该框架结合了精细的心理属性和明确的记忆控制。PsyMem通过添加26个心理指标补充文本描述，以详细构建模型人物。此外，PsyMem实现了记忆对齐训练，明确训练模型使其响应与记忆对齐，在推断过程中实现动态的记忆控制响应。通过在我们特别设计的数据集（包含5,414个角色和38,962个对话，从小说中提取）上对Qwen2.5-7B-Instruct进行训练，生成的PsyMem-Qwen模型在角色扮演任务中表现优于基线模型，在拟人度和人物忠实度方面达到最优。', 'title_zh': 'PsyMem: 细粒度心理对齐与显性记忆控制以提升高级角色扮演语言模型'}
{'arxiv_id': 'arXiv:2505.12781', 'title': 'A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone', 'authors': 'Jitai Hao, Qiang Huang, Hao Liu, Xinyan Xiao, Zhaochun Ren, Jun Yu', 'link': 'https://arxiv.org/abs/2505.12781', 'abstract': 'Training high-performing Small Language Models (SLMs) remains costly, even with knowledge distillation and pruning from larger teacher models. Existing work often faces three key challenges: (1) information loss from hard pruning, (2) inefficient alignment of representations, and (3) underutilization of informative activations, particularly from Feed-Forward Networks (FFNs). To address these challenges, we introduce Low-Rank Clone (LRC), an efficient pre-training method that constructs SLMs aspiring to behavioral equivalence with strong teacher models. LRC trains a set of low-rank projection matrices that jointly enable soft pruning by compressing teacher weights, and activation clone by aligning student activations, including FFN signals, with those of the teacher. This unified design maximizes knowledge transfer while removing the need for explicit alignment modules. Extensive experiments with open-source teachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC matches or surpasses state-of-the-art models trained on trillions of tokens--while using only 20B tokens, achieving over 1,000x training efficiency. Our codes and model checkpoints are available at this https URL and this https URL.', 'abstract_zh': '训练高性能小型语言模型（SLMs）仍然代价高昂，即使使用来自较大教师模型的知识蒸馏和剪枝也是如此。现有工作通常面临三个关键挑战：（1）硬剪枝导致的信息丢失，（2）表示的低效对齐，以及（3）信息激活，特别是来自前馈网络（FFNs）的信息激活的低利用率。为了解决这些挑战，我们提出了低秩克隆（LRC），这是一种高效的预训练方法，旨在使小型语言模型的行为与强大的教师模型相等价。LRC 训练一组低秩投影矩阵，这些矩阵共同实现软剪枝，通过压缩教师权重和通过对齐学生激活（包括FFN信号）与教师的激活来实现激活克隆。这种统一设计最大化了知识转移，同时去除了需要显式对齐模块的需要。使用开源教师模型（例如，Llama-3.2-3B-Instruct、Qwen2.5-3B/7B-Instruct）的广泛实验表明，LRC 在使用仅 200 亿个标记的情况下，可与训练数万亿个标记的最新模型相媲美甚至超越，实现超过 1,000 倍的训练效率。我们的代码和模型检查点可在以下网址获取：这个 https URL 和这个 https URL。', 'title_zh': '一个令牌相当于超过1,000个令牌：通过低秩克隆实现高效的知识蒸馏'}
{'arxiv_id': 'arXiv:2505.12738', 'title': 'EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting', 'authors': 'Chenghua Gong, Rui Sun, Yuhao Zheng, Juyuan Zhang, Tianjun Gu, Liming Pan, Linyuan Lv', 'link': 'https://arxiv.org/abs/2505.12738', 'abstract': 'Advanced epidemic forecasting is critical for enabling precision containment strategies, highlighting its strategic importance for public health security. While recent advances in Large Language Models (LLMs) have demonstrated effectiveness as foundation models for domain-specific tasks, their potential for epidemic forecasting remains largely unexplored. In this paper, we introduce EpiLLM, a novel LLM-based framework tailored for spatio-temporal epidemic forecasting. Considering the key factors in real-world epidemic transmission: infection cases and human mobility, we introduce a dual-branch architecture to achieve fine-grained token-level alignment between such complex epidemic patterns and language tokens for LLM adaptation. To unleash the multi-step forecasting and generalization potential of LLM architectures, we propose an autoregressive modeling paradigm that reformulates the epidemic forecasting task into next-token prediction. To further enhance LLM perception of epidemics, we introduce spatio-temporal prompt learning techniques, which strengthen forecasting capabilities from a data-driven perspective. Extensive experiments show that EpiLLM significantly outperforms existing baselines on real-world COVID-19 datasets and exhibits scaling behavior characteristic of LLMs.', 'abstract_zh': '先进的流行病 forecasting 对于实现精准防控策略至关重要，突显了其在公众健康安全中的战略重要性。尽管近年来大规模语言模型（LLMs）在特定领域任务中的基础模型方面展现了有效性，但其在流行病 forecasting 方面的潜力尚未得到充分探索。本文介绍了一种新型 LLM 基础框架 EpiLLM，专门用于时空流行病 forecasting。考虑到现实世界流行病传播的关键因素——感染案例和人口移动，我们引入了一种双分支架构，以实现复杂流行病模式与语言标记之间的精细粒度对齐，从而适应 LLM。为释放 LLM 架构的多步 forecasting 和泛化潜力，我们提出了一种自回归建模范式，将其重新表述为下一标记预测任务。为了进一步增强 LLM 对流行病的感知，我们引入了时空提示学习技术，这些技术从数据驱动的角度增强了 forecasting 能力。广泛实验表明，EpiLLM 在实际世界 COVID-19 数据集上的表现显著优于现有基线，并展现出与 LLM 相似的扩展行为。', 'title_zh': 'EpiLLM: 在流行病预测中充分发挥大型语言模型的潜力'}
{'arxiv_id': 'arXiv:2505.12716', 'title': 'Shadow-FT: Tuning Instruct via Base', 'authors': 'Taiqiang Wu, Runming Yang, Jiayi Li, Pengfei Hu, Ngai Wong, Yujiu Yang', 'link': 'https://arxiv.org/abs/2505.12716', 'abstract': 'Large language models (LLMs) consistently benefit from further fine-tuning on various tasks. However, we observe that directly tuning the INSTRUCT (i.e., instruction tuned) models often leads to marginal improvements and even performance degeneration. Notably, paired BASE models, the foundation for these INSTRUCT variants, contain highly similar weight values (i.e., less than 2% on average for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to tune the INSTRUCT models by leveraging the corresponding BASE models. The key insight is to fine-tune the BASE model, and then directly graft the learned weight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no additional parameters, is easy to implement, and significantly improves performance. We conduct extensive experiments on tuning mainstream LLMs, such as Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering coding, reasoning, and mathematical tasks. Experimental results demonstrate that Shadow-FT consistently outperforms conventional full-parameter and parameter-efficient tuning approaches. Further analyses indicate that Shadow-FT can be applied to multimodal large language models (MLLMs) and combined with direct preference optimization (DPO). Codes and weights are available at \\href{this https URL}{Github}.', 'abstract_zh': '大规模语言模型（LLMs）在各种任务上从进一步微调中受益匪浅。然而，我们观察到直接对INSTRUCT（即指令调优）模型进行微调往往导致边际性改进，甚至性能退化。值得注意的是，这些INSTRUCT变体的基础模型BASE含有高度相似的权重值（以Llama 3.1 8B为例，平均值低于2%）。因此，我们提出了一种新颖的Shadow-FT框架，通过利用对应的BASE模型来微调INSTRUCT模型。核心洞察是先微调BASE模型，然后直接将学到的权重更新嫁接到INSTRUCT模型。我们提出的Shadow-FT不引入额外参数，易于实现，并显著提高性能。我们在Qwen 3和Llama 3系列等主流LLM上进行了广泛的微调实验，并在涵盖编程、推理和数学任务的19个基准上评估了它们。实验结果表明，Shadow-FT一致地优于传统的全参数和高效参数微调方法。进一步的分析表明，Shadow-FT可以应用于多模态大规模语言模型（MLLMs），并与直接偏好优化（DPO）结合使用。代码和权重可在Github获取。', 'title_zh': 'Shadow-FT: 基于基础模型调优指令方法'}
{'arxiv_id': 'arXiv:2505.12662', 'title': 'Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering', 'authors': 'Xukai Liu, Ye Liu, Shiwen Wu, Yanghai Zhang, Yihao Yuan, Kai Zhang, Qi Liu', 'link': 'https://arxiv.org/abs/2505.12662', 'abstract': 'Recent advances in large language models (LLMs) have led to impressive progress in natural language generation, yet their tendency to produce hallucinated or unsubstantiated content remains a critical concern. To improve factual reliability, Retrieval-Augmented Generation (RAG) integrates external knowledge during inference. However, existing RAG systems face two major limitations: (1) unreliable adaptive control due to limited external knowledge supervision, and (2) hallucinations caused by inaccurate or irrelevant references. To address these issues, we propose Know3-RAG, a knowledge-aware RAG framework that leverages structured knowledge from knowledge graphs (KGs) to guide three core stages of the RAG process, including retrieval, generation, and filtering. Specifically, we introduce a knowledge-aware adaptive retrieval module that employs KG embedding to assess the confidence of the generated answer and determine retrieval necessity, a knowledge-enhanced reference generation strategy that enriches queries with KG-derived entities to improve generated reference relevance, and a knowledge-driven reference filtering mechanism that ensures semantic alignment and factual accuracy of references. Experiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG consistently outperforms strong baselines, significantly reducing hallucinations and enhancing answer reliability.', 'abstract_zh': 'Recent Advances in Large Language Models (LLMs) Have Led to Impressive Progress in Natural Language Generation, Yet Their Tendency to Produce Hallucinated or Unsubstantiated Content Remains a Critical Concern: Know3-RAG, a Knowledge-Aware Retrieval-Augmented Generation Framework for Factual Reliability', 'title_zh': 'Know3-RAG：一种具备适应性检索、生成与过滤的知识aware框架'}
{'arxiv_id': 'arXiv:2505.12655', 'title': 'Web IP at Risk: Prevent Unauthorized Real-Time Retrieval by Large Language Models', 'authors': 'Yisheng Zhong, Yizhu Wen, Junfeng Guo, Mehran Kafai, Heng Huang, Hanqing Guo, Zhuangdi Zhu', 'link': 'https://arxiv.org/abs/2505.12655', 'abstract': 'Protecting cyber Intellectual Property (IP) such as web content is an increasingly critical concern. The rise of large language models (LLMs) with online retrieval capabilities presents a double-edged sword that enables convenient access to information but often undermines the rights of original content creators. As users increasingly rely on LLM-generated responses, they gradually diminish direct engagement with original information sources, significantly reducing the incentives for IP creators to contribute, and leading to a saturating cyberspace with more AI-generated content. In response, we propose a novel defense framework that empowers web content creators to safeguard their web-based IP from unauthorized LLM real-time extraction by leveraging the semantic understanding capability of LLMs themselves. Our method follows principled motivations and effectively addresses an intractable black-box optimization problem. Real-world experiments demonstrated that our methods improve defense success rates from 2.5% to 88.6% on different LLMs, outperforming traditional defenses such as configuration-based restrictions.', 'abstract_zh': '保护网络知识产权：利用大型语言模型的语义理解能力防范未经授权的实时提取', 'title_zh': 'Web IP at Risk: 防止大型语言模型未经授权的实时检索'}
{'arxiv_id': 'arXiv:2505.12594', 'title': 'AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection', 'authors': 'Tiankai Yang, Junjun Liu, Wingchun Siu, Jiahang Wang, Zhuangzhuang Qian, Chanjuan Song, Cheng Cheng, Xiyang Hu, Yue Zhao', 'link': 'https://arxiv.org/abs/2505.12594', 'abstract': 'Anomaly detection (AD) is essential in areas such as fraud detection, network monitoring, and scientific research. However, the diversity of data modalities and the increasing number of specialized AD libraries pose challenges for non-expert users who lack in-depth library-specific knowledge and advanced programming skills. To tackle this, we present AD-AGENT, an LLM-driven multi-agent framework that turns natural-language instructions into fully executable AD pipelines. AD-AGENT coordinates specialized agents for intent parsing, data preparation, library and model selection, documentation mining, and iterative code generation and debugging. Using a shared short-term workspace and a long-term cache, the agents integrate popular AD libraries like PyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that AD-AGENT produces reliable scripts and recommends competitive models across libraries. The system is open-sourced to support further research and practical applications in AD.', 'abstract_zh': '基于LLM的多Agent异常检测框架AD-AGENT', 'title_zh': 'AD-AGENT：端到端异常检测的多agent框架'}
{'arxiv_id': 'arXiv:2505.12572', 'title': 'Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio', 'authors': 'Hanwen Shen, Ting Ying', 'link': 'https://arxiv.org/abs/2505.12572', 'abstract': 'Writing novels with Large Language Models (LLMs) raises a critical question: how much human-authored outline is necessary to generate high-quality million-word novels? While frameworks such as DOME, Plan&Write, and Long Writer have improved stylistic coherence and logical consistency, they primarily target shorter novels (10k--100k words), leaving ultra-long generation largely unexplored. Drawing on insights from recent text compression methods like LLMZip and LLM2Vec, we conduct an information-theoretic analysis that quantifies distortion occurring when LLMs compress and reconstruct ultra-long novels under varying compression-expansion ratios. We introduce a hierarchical two-stage generation pipeline (outline -> detailed outline -> manuscript) and find an optimal outline length that balances information preservation with human effort. Through extensive experimentation with Chinese novels, we establish that a two-stage hierarchical outline approach significantly reduces semantic distortion compared to single-stage methods. Our findings provide empirically-grounded guidance for authors and researchers collaborating with LLMs to create million-word novels.', 'abstract_zh': '使用大型语言模型（LLMs）撰写小说提出一个关键问题：生成百万字高质量小说需要多少人类撰写的提纲？基于近年来的文本压缩方法如LLMZip和LLM2Vec，我们进行了信息论分析，量化了LLMs在不同压缩-扩展比下压缩和重构超长小说时产生的失真情况。我们引入了一种分层两阶段生成 pipeline（提纲 -> 详细提纲 -> 草稿），并找到了一个平衡信息保存与人力投入的最优提纲长度。通过大量使用中文小说进行实验，我们发现分层两阶段的提纲方法相比于单阶段方法显著减少了语义失真。我们的研究为作者和研究人员与LLMs合作创作百万字小说提供了实证指导。', 'title_zh': '层次超长小说生成中信息失真的度量：最优扩展比例'}
{'arxiv_id': 'arXiv:2505.12567', 'title': 'A Survey of Attacks on Large Language Models', 'authors': 'Wenrui Xu, Keshab K. Parhi', 'link': 'https://arxiv.org/abs/2505.12567', 'abstract': 'Large language models (LLMs) and LLM-based agents have been widely deployed in a wide range of applications in the real world, including healthcare diagnostics, financial analysis, customer support, robotics, and autonomous driving, expanding their powerful capability of understanding, reasoning, and generating natural languages. However, the wide deployment of LLM-based applications exposes critical security and reliability risks, such as the potential for malicious misuse, privacy leakage, and service disruption that weaken user trust and undermine societal safety. This paper provides a systematic overview of the details of adversarial attacks targeting both LLMs and LLM-based agents. These attacks are organized into three phases in LLMs: Training-Phase Attacks, Inference-Phase Attacks, and Availability & Integrity Attacks. For each phase, we analyze the details of representative and recently introduced attack methods along with their corresponding defenses. We hope our survey will provide a good tutorial and a comprehensive understanding of LLM security, especially for attacks on LLMs. We desire to raise attention to the risks inherent in widely deployed LLM-based applications and highlight the urgent need for robust mitigation strategies for evolving threats.', 'abstract_zh': '大型语言模型及其基于大型语言模型的代理在医疗诊断、金融分析、客户支持、机器人技术与自主驾驶等领域得到了广泛应用，扩展了其强大的自然语言理解、推理和生成能力。然而，基于大型语言模型的应用的广泛部署揭示了关键的安全性和可靠性风险，如恶意滥用、隐私泄露和服务中断，这些风险削弱了用户信任并损害了社会安全。本文提供了一个系统概述，详细介绍了针对大型语言模型及其基于大型语言模型的代理的 adversarial 攻击。这些攻击被组织为三个阶段：训练阶段攻击、推理阶段攻击和可用性和完整性攻击。对于每个阶段，我们分析了具有代表性和最近提出的各种攻击方法及其对应的防御措施。我们希望本次综述能提供一个良好的教程，并对大型语言模型安全有一个全面的理解，特别是针对大型语言模型的攻击。我们希望提高对广泛部署的基于大型语言模型应用所固有的风险的认识，并强调迫切需要为不断演变的威胁采取强大的缓解策略。', 'title_zh': '大型语言模型攻击综述'}
{'arxiv_id': 'arXiv:2505.12509', 'title': 'Towards Budget-Friendly Model-Agnostic Explanation Generation for Large Language Models', 'authors': 'Junhao Liu, Haonan Yu, Xin Zhang', 'link': 'https://arxiv.org/abs/2505.12509', 'abstract': "With Large language models (LLMs) becoming increasingly prevalent in various applications, the need for interpreting their predictions has become a critical challenge. As LLMs vary in architecture and some are closed-sourced, model-agnostic techniques show great promise without requiring access to the model's internal parameters. However, existing model-agnostic techniques need to invoke LLMs many times to gain sufficient samples for generating faithful explanations, which leads to high economic costs. In this paper, we show that it is practical to generate faithful explanations for large-scale LLMs by sampling from some budget-friendly models through a series of empirical studies. Moreover, we show that such proxy explanations also perform well on downstream tasks. Our analysis provides a new paradigm of model-agnostic explanation methods for LLMs, by including information from budget-friendly models.", 'abstract_zh': '随着大型语言模型（LLMs）在各种应用中的日益普及，解释其预测结果的需要已成为一个关键挑战。由于LLMs在架构上存在差异且部分为闭源，因此通用的解释技术无需访问模型内部参数便展现出巨大潜力。然而，现有的通用解释技术需要多次调用LLMs以获得足够的样本来生成可靠的解释，这导致了高昂的经济成本。在本文中，我们通过一系列实证研究展示了通过从一些低成本模型中采样来为大规模LLMs生成可靠解释是可行的。此外，我们展示了此类代理解释在下游任务中也表现良好。我们的分析为LLMs提供了一种新的通用解释方法范式，通过纳入低成本模型的信息。', 'title_zh': '面向大型语言模型的预算友好型模型无关解释生成'}
{'arxiv_id': 'arXiv:2505.12504', 'title': 'CPGD: Toward Stable Rule-based Reinforcement Learning for Language Models', 'authors': 'Zongkai Liu, Fanqing Meng, Lingxiao Du, Zhixiang Zhou, Chao Yu, Wenqi Shao, Qiaosheng Zhang', 'link': 'https://arxiv.org/abs/2505.12504', 'abstract': 'Recent advances in rule-based reinforcement learning (RL) have significantly improved the reasoning capability of language models (LMs) with rule-based rewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO -- often suffer from training instability, where large policy updates and improper clipping can lead to training collapse. To address this issue, we propose Clipped Policy Gradient Optimization with Policy Drift (CPGD), a novel algorithm designed to stabilize policy learning in LMs. CPGD introduces a policy drift constraint based on KL divergence to dynamically regularize policy updates, and leverages a clip mechanism on the logarithm of the ratio to prevent excessive policy updates. We provide theoretical justification for CPGD and demonstrate through empirical analysis that it mitigates the instability observed in prior approaches. Furthermore, we show that CPGD significantly improves performance while maintaining training stability. Our implementation balances theoretical rigor with practical usability, offering a robust alternative for RL in the post-training of LMs. We release our code at this https URL.', 'abstract_zh': '基于规则的强化学习 Recent进展及其在语言模型中的规则学习稳定性优化（Clipped Policy Gradient Optimization with Policy Drift (CPGD)）', 'title_zh': 'CPGD：面向语言模型的稳定基于规则的强化学习'}
{'arxiv_id': 'arXiv:2505.12476', 'title': 'Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering', 'authors': 'Xiao Long, Liansheng Zhuang, Chen Shen, Shaotian Yan, Yifei Li, Shafei Wang', 'link': 'https://arxiv.org/abs/2505.12476', 'abstract': 'Recently, large language models (LLMs) have demonstrated impressive performance in Knowledge Graph Question Answering (KGQA) tasks, which aim to find answers based on knowledge graphs (KGs) for natural language questions. Existing LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented Generation (GraphRAG) paradigm, which first retrieves reasoning paths from the large KGs, and then generates the answers based on them. However, these methods emphasize the exploration of new optimal reasoning paths in KGs while ignoring the exploitation of historical reasoning paths, which may lead to sub-optimal reasoning paths. Additionally, the complex semantics contained in questions may lead to the retrieval of inaccurate reasoning paths. To address these issues, this paper proposes a novel and training-free framework for KGQA tasks called Reward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original question into a series of simpler and well-defined sub-questions to handle the complex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided by a reward model is introduced to iteratively retrieve weighted reasoning paths as contextual knowledge. Finally, it stacks the weighted reasoning paths according to their weights to generate the final answers. Extensive experiments on four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves 8.7\\% and 7.0\\% performance improvement over the state-of-the-art method on the GrailQA and the WebQSP respectively.', 'abstract_zh': 'Reward-guided Tree Search on Graph for Knowledge Graph Question Answering', 'title_zh': '基于奖励引导树搜索的知识图谱问答的大语言模型增强'}
{'arxiv_id': 'arXiv:2505.12467', 'title': 'Beyond Frameworks: Unpacking Collaboration Strategies in Multi-Agent Systems', 'authors': 'Haochun Wang, Sendong Zhao, Jingbo Wang, Zewen Qiang, Bing Qin, Ting Liu', 'link': 'https://arxiv.org/abs/2505.12467', 'abstract': 'Multi-agent collaboration has emerged as a pivotal paradigm for addressing complex, distributed tasks in large language model (LLM)-driven applications. While prior research has focused on high-level architectural frameworks, the granular mechanisms governing agents, critical to performance and scalability, remain underexplored. This study systematically investigates four dimensions of collaboration strategies: (1) agent governance, (2) participation control, (3) interaction dynamics, and (4) dialogue history management. Through rigorous experimentation under two context-dependent scenarios: Distributed Evidence Integration (DEI) and Structured Evidence Synthesis (SES), we quantify the impact of these strategies on both task accuracy and computational efficiency. Our findings reveal that centralized governance, instructor-led participation, ordered interaction patterns, and instructor-curated context summarization collectively optimize the trade-off between decision quality and resource utilization with the support of the proposed Token-Accuracy Ratio (TAR). This work establishes a foundation for designing adaptive, scalable multi-agent systems, shifting the focus from structural novelty to strategic interaction mechanics.', 'abstract_zh': '多agents协作已成为解决大型语言模型（LLM）驱动应用中复杂分布式任务的关键范式。尽管前期研究集中于高层架构框架，但涉及agents的具体机制，尤其是对性能和可扩展性至关重要的方面，仍较少探讨。本研究系统性地探讨了协作策略的四个维度：（1）agents治理，（2）参与控制，（3）互动动力学，以及（4）对话历史管理。通过在两种上下文依赖场景下的严格实验：分布式证据整合（DEI）和结构化证据综合（SES），量化这些策略对任务准确性和计算效率的影响。研究发现，集中化治理、指导员主导的参与、有序的互动模式以及指导员精炼的上下文总结共同优化了决策质量和资源利用之间的权衡，并得到所提出的时间-准确率比（TAR）的支持。本研究奠定了设计适应性强、可扩展的多agents系统的基础，将研究重点从结构新颖性转向策略性交互机制。', 'title_zh': '超越框架：多代理系统中协作策略的解构'}
{'arxiv_id': 'arXiv:2505.12442', 'title': 'IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems', 'authors': 'Liwen Wang, Wenxuan Wang, Shuai Wang, Zongjie Li, Zhenlan Ji, Zongyi Lyu, Daoyuan Wu, Shing-Chi Cheung', 'link': 'https://arxiv.org/abs/2505.12442', 'abstract': 'The rapid advancement of Large Language Models (LLMs) has led to the emergence of Multi-Agent Systems (MAS) to perform complex tasks through collaboration. However, the intricate nature of MAS, including their architecture and agent interactions, raises significant concerns regarding intellectual property (IP) protection. In this paper, we introduce MASLEAK, a novel attack framework designed to extract sensitive information from MAS applications. MASLEAK targets a practical, black-box setting, where the adversary has no prior knowledge of the MAS architecture or agent configurations. The adversary can only interact with the MAS through its public API, submitting attack query $q$ and observing outputs from the final agent. Inspired by how computer worms propagate and infect vulnerable network hosts, MASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain responses from each MAS agent that reveal a full set of proprietary components, including the number of agents, system topology, system prompts, task instructions, and tool usages. We construct the first synthetic dataset of MAS applications with 810 applications and also evaluate MASLEAK against real-world MAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in extracting MAS IP, with an average attack success rate of 87% for system prompts and task instructions, and 92% for system architecture in most cases. We conclude by discussing the implications of our findings and the potential defenses.', 'abstract_zh': '大规模语言模型的快速进步导致了多代理系统（MAS）的出现，通过合作执行复杂任务。然而，MAS的复杂性，包括其架构和代理间的交互，引发了重要的知识产权（IP）保护问题。本文提出MASLEAK，一种新颖的攻击框架，旨在从MAS应用程序中提取敏感信息。MASLEAK针对一种实际的黑盒环境，在这种环境中，攻击者对MAS的架构或代理配置没有任何先验知识。攻击者只能通过公共API与MAS进行交互，提交攻击查询$q$并观察最终代理的输出。受计算机蠕虫传播和感染易受攻击网络主机的启发，MASLEAK精心构建了对抗性查询$q$，以从每个MAS代理中引发、传播并保留揭示完整产权组件（如代理数量、系统拓扑结构、系统提示、任务指令和工具使用情况）的响应。我们构建了包含810个应用的首个MAS应用程序合成数据集，并在Coze和CrewAI等实际MAS应用程序上评估了MASLEAK。MASLEAK在提取MAS IP方面显示出高精度，系统提示和任务指令的平均攻击成功率分别为87%，系统架构在大多数情况下为92%。最后，我们讨论了这些发现的意义以及潜在的防御措施。', 'title_zh': '基于LLM的多agent系统中的IP泄露攻击'}
{'arxiv_id': 'arXiv:2505.12435', 'title': 'SGDPO: Self-Guided Direct Preference Optimization for Language Model Alignment', 'authors': 'Wenqiao Zhu, Ji Liu, Lulu Wang, Jun Wu, Yulun Zhang', 'link': 'https://arxiv.org/abs/2505.12435', 'abstract': 'Direct Preference Optimization (DPO) is broadly utilized for aligning Large Language Models (LLMs) with human values because of its flexibility. Despite its effectiveness, it has been observed that the capability of DPO to generate human-preferred response is limited and the results of DPO are far from resilient. To address these limitations, in this paper we propose a novel Self-Guided Direct Preference Optimization algorithm, i.e., SGDPO, which incorporates a pilot term to steer the gradient flow during the optimization process, allowing for fine-grained control over the updates of chosen and rejected rewards. We provide a detailed theoretical analysis of our proposed method and elucidate its operational mechanism. Furthermore, we conduct comprehensive experiments on various models and benchmarks. The extensive experimental results demonstrate the consistency between the empirical results and our theoretical analysis and confirm the effectiveness of our proposed approach (up to 9.19% higher score).', 'abstract_zh': '直接偏好优化（DPO）因其灵活性被广泛用于对齐大规模语言模型（LLMs）与人类价值观，尽管其有效性已经得到验证，但观察到DPO生成人类偏好的响应能力有限，其结果也远远不够稳健。为解决这些局限性，本文提出了一个新颖的自引导直接偏好优化算法，即SGDPO，该算法引入了一个引导项，以在优化过程中引导梯度流动，从而实现对选定和拒绝奖励更新的精细控制。我们详细分析了所提出方法的理论基础，并解释了其工作机制。此外，我们在多个模型和基准上进行了全面的实验。广泛的实验结果表明了实证结果与我们的理论分析之间的一致性，并证实了我们所提出方法的有效性（最高可提高9.19%的得分）。', 'title_zh': 'SGDPO：自我引导的直接偏好优化以实现语言模型对齐'}
{'arxiv_id': 'arXiv:2505.12432', 'title': 'Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning', 'authors': 'Zirun Guo, Minjie Hong, Tao Jin', 'link': 'https://arxiv.org/abs/2505.12432', 'abstract': 'Reinforcement Learning (RL) has shown promise in improving the reasoning abilities of Large Language Models (LLMs). However, the specific challenges of adapting RL to multimodal data and formats remain relatively unexplored. In this work, we present Observe-R1, a novel framework aimed at enhancing the reasoning capabilities of multimodal large language models (MLLMs). We draw inspirations from human learning progression--from simple to complex and easy to difficult, and propose a gradual learning paradigm for MLLMs. To this end, we construct the NeuraLadder dataset, which is organized and sampled according to the difficulty and complexity of data samples for RL training. To tackle multimodal tasks, we introduce a multimodal format constraint that encourages careful observation of images, resulting in enhanced visual abilities and clearer and more structured responses. Additionally, we implement a bonus reward system that favors concise, correct answers within a length constraint, alongside a dynamic weighting mechanism that prioritizes uncertain and medium-difficulty problems, ensuring that more informative samples have a greater impact on training. Our experiments with the Qwen2.5-VL-3B and Qwen2.5-VL-7B models on 20k samples from the NeuraLadder dataset show that Observe-R1 outperforms a series of larger reasoning models on both reasoning and general benchmarks, achieving superior clarity and conciseness in reasoning chains. Ablation studies validate the effectiveness of our strategies, highlighting the robustness and generalization of our approach. The dataset and code will be released at this https URL.', 'abstract_zh': '强化学习（RL）在提升大型语言模型（LLMs）的推理能力方面展现了潜力。然而，将RL适应多模态数据和格式的具体挑战仍相对未被充分探索。在本文中，我们提出Observe-R1，一种旨在增强多模态大型语言模型（MLLMs）推理能力的新框架。我们借鉴了人类学习进步的模式——从简单到复杂，从易到难，并为MLLMs提出了一个逐步学习范式。为此，我们构建了NeuraLadder数据集，该数据集根据数据样本的难易程度和复杂性进行组织和采样，用于RL训练。为应对多模态任务，我们引入了一种多模态格式约束，促成了对图像的仔细观察，提高了视觉能力并产生了清晰且结构化的回答。此外，我们实施了一种奖励系统，倾向于在长度限制内提供简洁且正确的答案，并采用动态加权机制优先处理不确定性和中等问题，确保更具信息量的样本对训练有更大的影响。我们在NeuraLadder数据集的20,000个样本上对Qwen2.5-VL-3B和Qwen2.5-VL-7B模型进行的实验表明，Observe-R1在推理和通用基准测试中均优于一系列更大规模的推理模型，实现了更清晰且简洁的推理链路。消融研究验证了我们策略的有效性，突显了我们方法的稳健性和泛化能力。数据集和代码将在以下链接发布：此 https URL。', 'title_zh': 'Observe-R1: 通过动态渐进强化学习解锁MLLMs的推理能力'}
{'arxiv_id': 'arXiv:2505.12424', 'title': 'EvoGPT: Enhancing Test Suite Robustness via LLM-Based Generation and Genetic Optimization', 'authors': 'Lior Broide, Roni Stern', 'link': 'https://arxiv.org/abs/2505.12424', 'abstract': 'Large Language Models (LLMs) have recently emerged as promising tools for automated unit test generation. We introduce a hybrid framework called EvoGPT that integrates LLM-based test generation with evolutionary search techniques to create diverse, fault-revealing unit tests. Unit tests are initially generated with diverse temperature sampling to maximize behavioral and test suite diversity, followed by a generation-repair loop and coverage-guided assertion enhancement. The resulting test suites are evolved using genetic algorithms, guided by a fitness function prioritizing mutation score over traditional coverage metrics. This design emphasizes the primary objective of unit testing-fault detection. Evaluated on multiple open-source Java projects, EvoGPT achieves an average improvement of 10% in both code coverage and mutation score compared to LLMs and traditional search-based software testing baselines. These results demonstrate that combining LLM-driven diversity, targeted repair, and evolutionary optimization produces more effective and resilient test suites.', 'abstract_zh': '大型语言模型（LLMs） recently emerged as promising tools for automated unit test generation. We introduce a hybrid framework called EvoGPT that integrates LLM-based test generation with evolutionary search techniques to create diverse, fault-revealing unit tests.', 'title_zh': 'EvoGPT：基于LLM生成和遗传优化提升测试套件 robustness'}
{'arxiv_id': 'arXiv:2505.12423', 'title': 'PSC: Extending Context Window of Large Language Models via Phase Shift Calibration', 'authors': 'Wenqiao Zhu, Chao Xu, Lulu Wang, Jun Wu', 'link': 'https://arxiv.org/abs/2505.12423', 'abstract': 'Rotary Position Embedding (RoPE) is an efficient position encoding approach and is widely utilized in numerous large language models (LLMs). Recently, a lot of methods have been put forward to further expand the context window based on RoPE. The core concept of those methods is to predefine or search for a set of factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a challenge for existing methods to predefine an optimal factor due to the exponential search space. In view of this, we introduce PSC (Phase Shift Calibration), a small module for calibrating the frequencies predefined by existing methods. With the employment of PSC, we demonstrate that many existing methods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted extensive experiments across multiple models and tasks. The results demonstrate that (1) when PSC is enabled, the comparative reductions in perplexity increase as the context window size is varied from 16k, to 32k, and up to 64k. (2) Our approach is broadly applicable and exhibits robustness across a variety of models and tasks. The code can be found at this https URL.', 'abstract_zh': '基于RoPE的相位-shift校准（PSC）：一种增强位置编码的小模块', 'title_zh': 'PSC：通过相位移校准扩展大型语言模型的上下文窗口'}
{'arxiv_id': 'arXiv:2505.12405', 'title': 'The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT', 'authors': 'Konstantinos Xylogiannopoulos, Petros Xanthopoulos, Panagiotis Karampelas, Georgios Bakamitsos', 'link': 'https://arxiv.org/abs/2505.12405', 'abstract': 'Generative AI paraphrased text can be used for copyright infringement and the AI paraphrased content can deprive substantial revenue from original content creators. Despite this recent surge of malicious use of generative AI, there are few academic publications that research this threat. In this article, we demonstrate the ability of pattern-based similarity detection for AI paraphrased news recognition. We propose an algorithmic scheme, which is not limited to detect whether an article is an AI paraphrase, but, more importantly, to identify that the source of infringement is the ChatGPT. The proposed method is tested with a benchmark dataset specifically created for this task that incorporates real articles from BBC, incorporating a total of 2,224 articles across five different news categories, as well as 2,224 paraphrased articles created with ChatGPT. Results show that our pattern similarity-based method, that makes no use of deep learning, can detect ChatGPT assisted paraphrased articles at percentages 96.23% for accuracy, 96.25% for precision, 96.21% for sensitivity, 96.25% for specificity and 96.23% for F1 score.', 'abstract_zh': 'Generative AI生成的文本可能用于版权侵权，AI生成的内容可能剥夺原始内容创作者的大量收益。尽管近期滥用生成式AI的情况日益严重，但仍缺乏对此威胁的学术研究。在本文中，我们展示了基于模式相似性的检测能力，用于识别AI生成的新闻。我们提出了一种算法方案，不仅可以检测文章是否为AI生成的，而且更重要的是能够识别侵权源为ChatGPT。所提出的方法使用了一个针对此任务专门创建的基准数据集，该数据集包含来自BBC的真实文章，共计2,224篇，以及使用ChatGPT生成的2,224篇平行文本。结果显示，我们的基于模式相似性的方法（不使用深度学习）能够以96.23%的准确率、96.25%的精确率、96.21%的灵敏度、96.25%的特异度和96.23%的F1分数识别ChatGPT辅助生成的平行文本。', 'title_zh': '文本相似性在识别AI-LLM重述文档中的作用：以BBC新闻文章和ChatGPT为例'}
{'arxiv_id': 'arXiv:2505.12398', 'title': 'Traversal Verification for Speculative Tree Decoding', 'authors': 'Yepeng Weng, Qiao Hu, Xujie Chen, Li Liu, Dianwen Mei, Huishi Qiu, Jiang Tian, Zhongchao Shi', 'link': 'https://arxiv.org/abs/2505.12398', 'abstract': 'Speculative decoding is a promising approach for accelerating large language models. The primary idea is to use a lightweight draft model to speculate the output of the target model for multiple subsequent timesteps, and then verify them in parallel to determine whether the drafted tokens should be accepted or rejected. To enhance acceptance rates, existing frameworks typically construct token trees containing multiple candidates in each timestep. However, their reliance on token-level verification mechanisms introduces two critical limitations: First, the probability distribution of a sequence differs from that of individual tokens, leading to suboptimal acceptance length. Second, current verification schemes begin from the root node and proceed layer by layer in a top-down manner. Once a parent node is rejected, all its child nodes should be discarded, resulting in inefficient utilization of speculative candidates. This paper introduces Traversal Verification, a novel speculative decoding algorithm that fundamentally rethinks the verification paradigm through leaf-to-root traversal. Our approach considers the acceptance of the entire token sequence from the current node to the root, and preserves potentially valid subsequences that would be prematurely discarded by existing methods. We theoretically prove that the probability distribution obtained through Traversal Verification is identical to that of the target model, guaranteeing lossless inference while achieving substantial acceleration gains. Experimental results across different large language models and multiple tasks show that our method consistently improves acceptance length and throughput over existing methods', 'abstract_zh': 'speculative 解码是一种加速大型语言模型的有前途的方法。通过从叶到根遍历的验证方式，提出了一种新的 speculative 解码算法，该算法从根本上重构了验证范式。我们的方法从当前节点到根考虑整个令牌序列的接受性，并保留现有方法会过早抛弃的有效子序列。我们理论证明，通过从叶到根遍历获得的概率分布与目标模型相同，保证无损推断的同时实现显著的加速收益。在不同大型语言模型和多个任务上的实验结果表明，我们的方法在提高接受长度和吞吐量方面优于现有方法。', 'title_zh': '投机树解码的遍历验证'}
{'arxiv_id': 'arXiv:2505.12381', 'title': 'From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling', 'authors': 'Mohsinul Kabir, Tasfia Tahsin, Sophia Ananiadou', 'link': 'https://arxiv.org/abs/2505.12381', 'abstract': 'Current research on bias in language models (LMs) predominantly focuses on data quality, with significantly less attention paid to model architecture and temporal influences of data. Even more critically, few studies systematically investigate the origins of bias. We propose a methodology grounded in comparative behavioral theory to interpret the complex interaction between training data and model architecture in bias propagation during language modeling. Building on recent work that relates transformers to n-gram LMs, we evaluate how data, model design choices, and temporal dynamics affect bias propagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to context window size in bias propagation, while transformers demonstrate architectural robustness; (2) the temporal provenance of training data significantly affects bias; and (3) different model architectures respond differentially to controlled bias injection, with certain biases (e.g. sexual orientation) being disproportionately amplified. As language models become ubiquitous, our findings highlight the need for a holistic approach -- tracing bias to its origins across both data and model dimensions, not just symptoms, to mitigate harm.', 'abstract_zh': '当前关于语言模型中偏见的研究主要集中在数据质量上，对模型架构和数据的时间影响关注较少。更关键的是，很少有研究系统地探讨偏见的根源。我们提出了一种基于比较行为理论的方法，以解释训练数据与模型架构在语言建模过程中偏见传播中的复杂交互。基于最近将变换器与n-gram语言模型联系起来的工作，我们评估了数据、模型设计选择以及时间动态对偏见传播的影响。我们的发现表明：(1) n-gram语言模型在偏见传播中对上下文窗口大小非常敏感，而变换器表现出架构稳健性；(2) 训练数据的时间来源显著影响偏见；(3) 不同的模型架构对控制偏见注入的响应不同，某些偏见（例如性取向）被不成比例地放大。随着语言模型的普及，我们的发现突显了需要采用综合方法的重要性——从数据和模型两个维度追踪偏见的根源，而不仅是症状，以减轻其危害。', 'title_zh': '从n-gram到注意力：模型架构在语言模型中学习和传播偏见的方式'}
{'arxiv_id': 'arXiv:2505.12368', 'title': 'CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement', 'authors': 'Gauri Kholkar, Ratinder Ahuja', 'link': 'https://arxiv.org/abs/2505.12368', 'abstract': 'Prompt injection remains a major security risk for large language models. However, the efficacy of existing guardrail models in context-aware settings remains underexplored, as they often rely on static attack benchmarks. Additionally, they have over-defense tendencies. We introduce CAPTURE, a novel context-aware benchmark assessing both attack detection and over-defense tendencies with minimal in-domain examples. Our experiments reveal that current prompt injection guardrail models suffer from high false negatives in adversarial cases and excessive false positives in benign scenarios, highlighting critical limitations.', 'abstract_zh': '提示注入仍然是大型语言模型的一项重大安全风险。然而，现有防护模型在上下文感知场景下的有效性仍然没有得到充分探索，因为它们常常依赖于静态攻击基准。此外，它们倾向于过度防御。我们引入了CAPTURE，这是一种新颖的上下文感知基准，评估攻击检测能力和过度防御倾向，仅使用少量领域的示例。我们的实验揭示了当前的提示注入防护模型在对抗情况下存在高误负和在良性情况下存在过度误报的问题，突显了其关键局限性。', 'title_zh': 'CAPTURE: 基于上下文感知的提示注入测试与鲁棒性增强'}
{'arxiv_id': 'arXiv:2505.12366', 'title': 'DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization', 'authors': 'Gang Li, Ming Lin, Tomer Galanti, Zhengzhong Tu, Tianbao Yang', 'link': 'https://arxiv.org/abs/2505.12366', 'abstract': 'The recent success and openness of DeepSeek-R1 have brought widespread attention to Group Relative Policy Optimization (GRPO) as a reinforcement learning method for large reasoning models (LRMs). In this work, we analyze the GRPO objective under a binary reward setting and reveal an inherent limitation of question-level difficulty bias. We also identify a connection between GRPO and traditional discriminative methods in supervised learning. Motivated by these insights, we introduce a new Discriminative Constrained Optimization (DisCO) framework for reinforcing LRMs, grounded in the principle of discriminative learning. The main differences between DisCO and GRPO and its recent variants are: (1) it replaces the group relative objective with a discriminative objective defined by a scoring function; (2) it abandons clipping-based surrogates in favor of non-clipping RL surrogate objectives used as scoring functions; (3) it employs a simple yet effective constrained optimization approach to enforce the KL divergence constraint, ensuring stable training. As a result, DisCO offers notable advantages over GRPO and its variants: (i) it completely eliminates difficulty bias by adopting discriminative objectives; (ii) it addresses the entropy instability in GRPO and its variants through the use of non-clipping scoring functions and a constrained optimization approach; (iii) it allows the incorporation of advanced discriminative learning techniques to address data imbalance, where a significant number of questions have more negative than positive generated answers during training. Our experiments on enhancing the mathematical reasoning capabilities of SFT-finetuned models show that DisCO significantly outperforms GRPO and its improved variants such as DAPO, achieving average gains of 7\\% over GRPO and 6\\% over DAPO across six benchmark tasks for an 1.5B model.', 'abstract_zh': '深度求索-R1的最近成功和开放性引起了对Group Relative Policy Optimization (GRPO)作为大规模推理模型（LRMs）的强化学习方法的广泛关注。在二元奖励设置下分析GRPO目标，揭示了一个固有的问题级别难度偏差限制。我们还发现GRPO与监督学习中的传统区分方法之间的联系。基于这些洞察，我们引入了一种新的区分约束优化（DisCO）框架，该框架基于区分学习的原则。DisCO与GRPO及其最近变体之间的主要区别在于：(1) 使用由评分函数定义的区分目标替代群体相对目标；(2) 采用非剪辑基于的RL替代目标作为评分函数，而非剪辑基适应；(3) 采用简单的有效约束优化方法来确保KL散度约束的满足，从而实现稳定的训练。因此，DisCO在与GRPO及其变体相比时展现出显著优势：(i) 通过采用区分目标完全消除了难度偏差；(ii) 通过使用非剪辑评分函数和约束优化方法解决了GRPO及其变体中的熵不稳定问题；(iii) 允许整合高级区分学习技术以解决数据不平衡问题，在训练过程中大量问题的生成答案中负面答案多于正面答案。在增强SFT微调模型的数学推理能力方面的实验表明，DisCO显著优于GRPO及其改进变体DAPO，在一个1.5B模型的六项基准任务中，DisCO相对于GRPO的平均改进幅度为7%，相对于DAPO为6%。', 'title_zh': 'DisCO: 使用辨别性约束优化强化大型推理模型'}
{'arxiv_id': 'arXiv:2505.12349', 'title': 'Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds', 'authors': 'Axel Abels, Tom Lenaerts', 'link': 'https://arxiv.org/abs/2505.12349', 'abstract': 'Despite their performance, large language models (LLMs) can inadvertently perpetuate biases found in the data they are trained on. By analyzing LLM responses to bias-eliciting headlines, we find that these models often mirror human biases. To address this, we explore crowd-based strategies for mitigating bias through response aggregation. We first demonstrate that simply averaging responses from multiple LLMs, intended to leverage the "wisdom of the crowd", can exacerbate existing biases due to the limited diversity within LLM crowds. In contrast, we show that locally weighted aggregation methods more effectively leverage the wisdom of the LLM crowd, achieving both bias mitigation and improved accuracy. Finally, recognizing the complementary strengths of LLMs (accuracy) and humans (diversity), we demonstrate that hybrid crowds containing both significantly enhance performance and further reduce biases across ethnic and gender-related contexts.', 'abstract_zh': '尽管大语言模型在性能上表现出色，但它们可能会无意中延续训练数据中发现的偏见。通过分析大语言模型对引发偏见的头条新闻的响应，我们发现这些模型经常反映出人类的偏见。为此，我们探索了基于众包的方法，通过响应聚合来减轻偏见。我们首先证明，简单地将多个大语言模型的响应取平均，虽然旨在利用“众人的智慧”，但由于大语言模型群体内的多样性有限，反而会使现有的偏见加剧。相比之下，我们展示了局部加权聚合方法更有效地利用大语言模型群体的“智慧”，既能减轻偏见，又能提高准确率。最后，考虑到大语言模型（准确性）和人类（多样性）的互补优势，我们证明了包含两者的混合群体显著提高了性能，并进一步减少了与种族和性别相关的偏见。', 'title_zh': '多样性之智：通过混合人类-大语言模型群体减轻偏差'}
{'arxiv_id': 'arXiv:2505.12287', 'title': 'The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models', 'authors': 'Linghan Huang, Haolin Jin, Zhaoge Bi, Pengyue Yang, Peizhou Zhao, Taozhao Chen, Xiongfei Wu, Lei Ma, Huaming Chen', 'link': 'https://arxiv.org/abs/2505.12287', 'abstract': 'Large language models (LLMs) have seen widespread applications across various domains, yet remain vulnerable to adversarial prompt injections. While most existing research on jailbreak attacks and hallucination phenomena has focused primarily on open-source models, we investigate the frontier of closed-source LLMs under multilingual attack scenarios. We present a first-of-its-kind integrated adversarial framework that leverages diverse attack techniques to systematically evaluate frontier proprietary solutions, including GPT-4o, DeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories of security contents in both English and Chinese, generating 38,400 responses across 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as the quantitative metric to assess performance from three dimensions: prompt design, model architecture, and language environment. Our findings suggest that Qwen-Max is the most vulnerable, while GPT-4o shows the strongest defense. Notably, prompts in Chinese consistently yield higher ASRs than their English counterparts, and our novel Two-Sides attack technique proves to be the most effective across all models. This work highlights a dire need for language-aware alignment and robust cross-lingual defenses in LLMs, and we hope it will inspire researchers, developers, and policymakers toward more robust and inclusive AI systems.', 'abstract_zh': '大型语言模型(LLMs)在多个领域得到了广泛应用，但仍然容易受到对抗性提示注入的攻击。尽管大多数关于 Jailbreak 攻击和幻觉现象的研究主要集中在开源模型上，我们探讨了闭源 LLMs 在多语言攻击场景下的前沿问题。我们提出了一种首创的集成式对抗框架，利用多种攻击技术系统地评估前沿专有解决方案，包括 GPT-4o、DeepSeek-R1、Gemini-1.5-Pro 和 Qwen-Max。评估涵盖了英、中两种语言的六大类安全内容，共生成了 38,400 个响应，涉及 32 种不同的 Jailbreak 攻击类型。利用攻击成功率（ASR）作为定量指标，从提示设计、模型架构和语言环境三个方面评估性能。我们的研究发现 Qwen-Max 最为脆弱，而 GPT-4o 表现出最强的防御能力。值得注意的是，中文提示的 ASR 高于其英文对应物，我们提出的新型双面攻击技术在所有模型中均表现出最佳效果。这项工作突显了在 LLMs 中引入语言意识对齐和强大的跨语言防御的迫切需求，并希望能激发研究人员、开发者和政策制定者朝着更 robust 和包容的 AI 系统方向努力。', 'title_zh': '巴别塔再探：对闭源大型语言模型的多语言 Jailbreak 提示研究'}
{'arxiv_id': 'arXiv:2505.12260', 'title': 'LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference', 'authors': 'Guangyuan Ma, Yongliang Ma, Xuanrui Gou, Zhenpeng Su, Ming Zhou, Songlin Hu', 'link': 'https://arxiv.org/abs/2505.12260', 'abstract': 'Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode queries and documents into low-dimensional dense or high-dimensional sparse vectors. It retrieves documents relevant to search queries based on vector similarities. Documents are pre-encoded offline, while queries arrive in real-time, necessitating an efficient online query encoder. Although LLMs significantly enhance retrieval capabilities, serving deeply parameterized LLMs slows down query inference throughput and increases demands for online deployment resources. In this paper, we propose LightRetriever, a novel LLM-based hybrid retriever with extremely lightweight query encoders. Our method retains a full-sized LLM for document encoding, but reduces the workload of query encoding to no more than an embedding lookup. Compared to serving a full-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for query inference with GPU acceleration, and even a 20x speedup without GPU. Experiments on large-scale retrieval benchmarks demonstrate that our method generalizes well across diverse retrieval tasks, retaining an average of 95% full-sized performance.', 'abstract_zh': '基于大型语言模型的轻量级混合检索方法：LightRetriever', 'title_zh': 'LightRetriever：一种基于LLM的混合检索架构，查询推理速度提升1000倍'}
{'arxiv_id': 'arXiv:2505.12250', 'title': 'Not All Documents Are What You Need for Extracting Instruction Tuning Data', 'authors': 'Chi Zhang, Huaping Zhong, Hongtao Li, Chengliang Chai, Jiawei Hong, Yuhao Deng, Jiacheng Wang, Tian Tan, Yizhou Yan, Jiantao Qiu, Ye Yuan, Guoren Wang, Conghui He, Lei Cao', 'link': 'https://arxiv.org/abs/2505.12250', 'abstract': 'Instruction tuning improves the performance of large language models (LLMs), but it heavily relies on high-quality training data. Recently, LLMs have been used to synthesize instruction data using seed question-answer (QA) pairs. However, these synthesized instructions often lack diversity and tend to be similar to the input seeds, limiting their applicability in real-world scenarios. To address this, we propose extracting instruction tuning data from web corpora that contain rich and diverse knowledge. A naive solution is to retrieve domain-specific documents and extract all QA pairs from them, but this faces two key challenges: (1) extracting all QA pairs using LLMs is prohibitively expensive, and (2) many extracted QA pairs may be irrelevant to the downstream tasks, potentially degrading model performance. To tackle these issues, we introduce EQUAL, an effective and scalable data extraction framework that iteratively alternates between document selection and high-quality QA pair extraction to enhance instruction tuning. EQUAL first clusters the document corpus based on embeddings derived from contrastive learning, then uses a multi-armed bandit strategy to efficiently identify clusters that are likely to contain valuable QA pairs. This iterative approach significantly reduces computational cost while boosting model performance. Experiments on AutoMathText and StackOverflow across four downstream tasks show that EQUAL reduces computational costs by 5-10x and improves accuracy by 2.5 percent on LLaMA-3.1-8B and Mistral-7B', 'abstract_zh': '指令调优改善了大规模语言模型的性能，但高度依赖高质量的训练数据。最近，大规模语言模型被用于合成指令数据，使用种子问题-回答（QA） pair。然而，这些合成的指令往往缺乏多样性，倾向于与输入种子相似，限制了其在现实世界场景中的应用。为了解决这一问题，我们提出从包含丰富多样知识的网络语料库中提取指令调优数据。一个直观的解决方案是从特定领域文档中检索所有QA pair，但这种方法面临两个关键挑战：（1）使用大规模语言模型提取所有QA pair的成本极其高昂；（2）提取的许多QA pair可能与下游任务无关，可能导致模型性能下降。为解决这些问题，我们引入了EQUAL，这是一种有效且可扩展的数据提取框架，通过迭代交替进行文档选择和高质量QA pair提取来增强指令调优。EQUAL首先基于对比学习衍生的嵌入对文档语料库进行聚类，然后采用多臂bandit策略高效地识别可能包含有价值的QA pair的集群。这种迭代方法显著降低了计算成本，同时提升了模型性能。在AutoMathText和StackOverflow四个下游任务上的实验表明，EQUAL将计算成本降低了5-10倍，并在LLaMA-3.1-8B和Mistral-7B中提高了2.5%的准确性。', 'title_zh': '并非所有文档都是提取指令调优数据所需的内容'}
{'arxiv_id': 'arXiv:2505.12247', 'title': 'LAMeTA: Intent-Aware Agentic Network Optimization via a Large AI Model-Empowered Two-Stage Approach', 'authors': 'Yinqiu Liu, Guangyuan Liu, Jiacheng Wang, Ruichen Zhang, Dusit Niyato, Geng Sun, Zehui Xiong, Zhu Han', 'link': 'https://arxiv.org/abs/2505.12247', 'abstract': 'Nowadays, Generative AI (GenAI) reshapes numerous domains by enabling machines to create content across modalities. As GenAI evolves into autonomous agents capable of reasoning, collaboration, and interaction, they are increasingly deployed on network infrastructures to serve humans automatically. This emerging paradigm, known as the agentic network, presents new optimization challenges due to the demand to incorporate subjective intents of human users expressed in natural language. Traditional generic Deep Reinforcement Learning (DRL) struggles to capture intent semantics and adjust policies dynamically, thus leading to suboptimality. In this paper, we present LAMeTA, a Large AI Model (LAM)-empowered Two-stage Approach for intent-aware agentic network optimization. First, we propose Intent-oriented Knowledge Distillation (IoKD), which efficiently distills intent-understanding capabilities from resource-intensive LAMs to lightweight edge LAMs (E-LAMs) to serve end users. Second, we develop Symbiotic Reinforcement Learning (SRL), integrating E-LAMs with a policy-based DRL framework. In SRL, E-LAMs translate natural language user intents into structured preference vectors that guide both state representation and reward design. The DRL, in turn, optimizes the generative service function chain composition and E-LAM selection based on real-time network conditions, thus optimizing the subjective Quality-of-Experience (QoE). Extensive experiments conducted in an agentic network with 81 agents demonstrate that IoKD reduces mean squared error in intent prediction by up to 22.5%, while SRL outperforms conventional generic DRL by up to 23.5% in maximizing intent-aware QoE.', 'abstract_zh': '基于大型AI模型的两阶段意图感知自主网络优化方法', 'title_zh': 'LAMeTA：基于意图感知的两阶段大型AI模型赋能代理网络优化'}
{'arxiv_id': 'arXiv:2505.12238', 'title': 'PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs', 'authors': 'Sriram Selvam, Anneswa Ghosh', 'link': 'https://arxiv.org/abs/2505.12238', 'abstract': "The memorization of sensitive and personally identifiable information (PII) by large language models (LLMs) poses growing privacy risks as models scale and are increasingly deployed in real-world applications. Existing efforts to study sensitive and PII data memorization and develop mitigation strategies are hampered by the absence of comprehensive, realistic, and ethically sourced datasets reflecting the diversity of sensitive information found on the web. We introduce PANORAMA - Profile-based Assemblage for Naturalistic Online Representation and Attribute Memorization Analysis, a large-scale synthetic corpus of 384,789 samples derived from 9,674 synthetic profiles designed to closely emulate the distribution, variety, and context of PII and sensitive data as it naturally occurs in online environments. Our data generation pipeline begins with the construction of internally consistent, multi-attribute human profiles using constrained selection to reflect real-world demographics such as education, health attributes, financial status, etc. Using a combination of zero-shot prompting and OpenAI o3-mini, we generate diverse content types - including wiki-style articles, social media posts, forum discussions, online reviews, comments, and marketplace listings - each embedding realistic, contextually appropriate PII and other sensitive information. We validate the utility of PANORAMA by fine-tuning the Mistral-7B model on 1x, 5x, 10x, and 25x data replication rates with a subset of data and measure PII memorization rates - revealing not only consistent increases with repetition but also variation across content types, highlighting PANORAMA's ability to model how memorization risks differ by context. Our dataset and code are publicly available, providing a much-needed resource for privacy risk assessment, model auditing, and the development of privacy-preserving LLMs.", 'abstract_zh': '基于档案装配的自然在线表示和属性记忆分析——PANORAMA', 'title_zh': 'PANORAMA：一个含有敏感个人信息的合成数据集，用于研究大语言模型中的敏感数据记忆问题'}
{'arxiv_id': 'arXiv:2505.12225', 'title': "Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling", 'authors': 'Jizhou Guo, Zhaomin Wu, Philip S. Yu', 'link': 'https://arxiv.org/abs/2505.12225', 'abstract': 'High-quality reward models are crucial for unlocking the reasoning potential of large language models (LLMs), with best-of-N voting demonstrating significant performance gains. However, current reward models, which typically operate on the textual output of LLMs, are computationally expensive and parameter-heavy, limiting their real-world applications. We introduce the Efficient Linear Hidden State Reward (ELHSR) model - a novel, highly parameter-efficient approach that leverages the rich information embedded in LLM hidden states to address these issues. ELHSR systematically outperform baselines with less than 0.005% of the parameters of baselines, requiring only a few samples for training. ELHSR also achieves orders-of-magnitude efficiency improvement with significantly less time and fewer FLOPs per sample than baseline reward models. Moreover, ELHSR exhibits robust performance even when trained only on logits, extending its applicability to some closed-source LLMs. In addition, ELHSR can also be combined with traditional reward models to achieve additional performance gains.', 'abstract_zh': '高质量的奖励模型对于释放大规模语言模型的推理潜力至关重要，最佳选择投票显示了显著的性能提升。然而，当前的奖励模型通常在大规模语言模型的文本输出上运行，计算成本高昂且参数量大，限制了其实际应用。我们引入了高效线性隐藏状态奖励（ELHSR）模型——一种全新的、高度参数高效的新型方法，利用大规模语言模型隐藏状态中丰富的信息来解决这些问题。ELHSR在参数少于基线0.005%的情况下系统地超越了基线模型，只需要少量样本进行训练。ELHSR还实现了与基线奖励模型相比数量级的效率提升，每样本所需时间和FLOPs显著减少。此外，即使仅在训练logits上，ELHSR也能表现出稳健的性能，使其适用于某些闭源的大规模语言模型。此外，ELHSR还可以与传统奖励模型结合使用，以实现额外的性能提升。', 'title_zh': '模型内的奖励：一种轻量级隐藏状态奖励模型，用于大语言模型的最佳采样'}
{'arxiv_id': 'arXiv:2505.12188', 'title': 'LLM-DSE: Searching Accelerator Parameters with LLM Agents', 'authors': 'Hanyu Wang, Xinrui Wu, Zijian Ding, Su Zheng, Chengyue Wang, Tony Nowatzki, Yizhou Sun, Jason Cong', 'link': 'https://arxiv.org/abs/2505.12188', 'abstract': 'Even though high-level synthesis (HLS) tools mitigate the challenges of programming domain-specific accelerators (DSAs) by raising the abstraction level, optimizing hardware directive parameters remains a significant hurdle. Existing heuristic and learning-based methods struggle with adaptability and sample this http URL present LLM-DSE, a multi-agent framework designed specifically for optimizing HLS directives. Combining LLM with design space exploration (DSE), our explorer coordinates four agents: Router, Specialists, Arbitrator, and Critic. These multi-agent components interact with various tools to accelerate the optimization process. LLM-DSE leverages essential domain knowledge to identify efficient parameter combinations while maintaining adaptability through verbal learning from online interactions. Evaluations on the HLSyn dataset demonstrate that LLM-DSE achieves substantial $2.55\\times$ performance gains over state-of-the-art methods, uncovering novel designs while reducing runtime. Ablation studies validate the effectiveness and necessity of the proposed agent interactions. Our code is open-sourced here: this https URL.', 'abstract_zh': '高級綜合（HLS）工具通過提高抽象水平減輕了編程領域特定加速器（DSAs）的挑戰，但opti型化硬件指令參數仍然是一個顯著障礙。現有表徵學習和基於學習的方法在適應性和 samp this http URL提出了一種專門用于優化HLS指令的多代理框架LLM-DSE。結合LLM與設計空間探索（DSE），我們的探索者協調了四個代理：路由器、專家、仲裁員和批評家。這些多代理組件與諸多工具互動以加速優化過程。LLM-DSE利用核心領域知識來識別高效的參數組合，通過視覺學習在線上互動中保持適應性。在HLSyn數據集上的評估表明，LLM-DSE相比現有方法實現了顯著的2.55倍性能增益，揭示了新的設計方案並降低了運行時間。消融研究驗證了所提出的代理互動的有效性和必要性。我們的代碼已經开源：this https URL。', 'title_zh': 'LLM-DSE：使用LLM代理搜索加速器参数'}
{'arxiv_id': 'arXiv:2505.12186', 'title': 'Self-Destructive Language Model', 'authors': 'Yuhui Wang, Rongyi Zhu, Ting Wang', 'link': 'https://arxiv.org/abs/2505.12186', 'abstract': 'Harmful fine-tuning attacks pose a major threat to the security of large language models (LLMs), allowing adversaries to compromise safety guardrails with minimal harmful data. While existing defenses attempt to reinforce LLM alignment, they fail to address models\' inherent "trainability" on harmful data, leaving them vulnerable to stronger attacks with increased learning rates or larger harmful datasets. To overcome this critical limitation, we introduce SEAM, a novel alignment-enhancing defense that transforms LLMs into self-destructive models with intrinsic resilience to misalignment attempts. Specifically, these models retain their capabilities for legitimate tasks while exhibiting substantial performance degradation when fine-tuned on harmful data. The protection is achieved through a novel loss function that couples the optimization trajectories of benign and harmful data, enhanced with adversarial gradient ascent to amplify the self-destructive effect. To enable practical training, we develop an efficient Hessian-free gradient estimate with theoretical error bounds. Extensive evaluation across LLMs and datasets demonstrates that SEAM creates a no-win situation for adversaries: the self-destructive models achieve state-of-the-art robustness against low-intensity attacks and undergo catastrophic performance collapse under high-intensity attacks, rendering them effectively unusable. (warning: this paper contains potentially harmful content generated by LLMs.)', 'abstract_zh': '有害微调攻击对大型语言模型的安全构成了重大威胁，允许攻击者通过少量有害数据破坏安全防护。现有防御试图强化语言模型的一致性，但未能解决模型内在的“可训练性”问题，使其在学习率增加或有害数据集增大时仍易受更强攻击。为克服这一关键限制，我们引入了SEAM，这是一种新型的一致性增强防御，能够将语言模型转化为自毁模型，具备内在的对齐失效尝试的抗性。具体而言，这些模型在合法任务上保留其能力，但在有害数据上进行微调时表现出显著的性能退化。保护机制通过一种新颖的损失函数实现，该函数结合了良性数据和有害数据的优化轨迹，并增强对抗性梯度上升以增强自毁效果。为了实现实际训练，我们开发了一种高效的无Hessian梯度估计，并提供了理论上的误差界。在模型和数据集上的广泛评估表明，SEAM为对手创造了无胜算的局面：自毁模型对低强度攻击具有最先进的鲁棒性，但在高强度攻击下会遭受灾难性的性能崩溃，使其实际上无法使用。（注意：本论文包含由语言模型生成的可能存在危害的内容。）', 'title_zh': '自毁型语言模型'}
{'arxiv_id': 'arXiv:2505.12183', 'title': 'Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases', 'authors': 'Manari Hirose, Masato Uchida', 'link': 'https://arxiv.org/abs/2505.12183', 'abstract': "The widespread integration of Large Language Models (LLMs) across various sectors has highlighted the need for empirical research to understand their biases, thought patterns, and societal implications to ensure ethical and effective use. In this study, we propose a novel framework for evaluating LLMs, focusing on uncovering their ideological biases through a quantitative analysis of 436 binary-choice questions, many of which have no definitive answer. By applying our framework to ChatGPT and Gemini, findings revealed that while LLMs generally maintain consistent opinions on many topics, their ideologies differ across models and languages. Notably, ChatGPT exhibits a tendency to change their opinion to match the questioner's opinion. Both models also exhibited problematic biases, unethical or unfair claims, which might have negative societal impacts. These results underscore the importance of addressing both ideological and ethical considerations when evaluating LLMs. The proposed framework offers a flexible, quantitative method for assessing LLM behavior, providing valuable insights for the development of more socially aligned AI systems.", 'abstract_zh': '大规模语言模型在各领域的广泛应用凸显了需要通过实证研究来理解其偏见、思维模式和社会影响，以确保其伦理和有效使用。本研究提出了一种新的框架来评估大规模语言模型，重点关注通过定量分析436个二元选择问题来揭示其意识形态偏见。将该框架应用于ChatGPT和Gemini的研究结果显示，虽然语言模型在许多话题上保持一致的观点，但其意识形态在不同模型和语言之间存在差异。值得注意的是，ChatGPT表现出倾向于改变观点以匹配提问者观点的趋势。这两款模型还表现出有问题的偏见和不道德或不公平的断言，可能对社会产生负面影响。这些结果强调了在评估大规模语言模型时必须兼顾意识形态和伦理考量的重要性。所提出框架提供了一种灵活的定量方法来评估语言模型的行为，为开发更具社会导向的AI系统提供了有价值的见解。', 'title_zh': '大语言模型的心智解码：意识形态和偏见的定量评估'}
{'arxiv_id': 'arXiv:2505.12151', 'title': 'Reasoning Large Language Model Errors Arise from Hallucinating Critical Problem Features', 'authors': 'Alex Heyman, Joel Zylberberg', 'link': 'https://arxiv.org/abs/2505.12151', 'abstract': 'Large language models have recently made great strides in reasoning task performance through chain-of-thought (CoT) strategies trained via reinforcement learning; however, these "reasoning large language models" (RLLMs) remain imperfect reasoners, and understanding the frequencies and causes of their failure modes is important for both users and developers. We test o1-mini, o3-mini, DeepSeek-R1, Claude 3.7 Sonnet, Gemini 2.5 Pro Preview, and Grok 3 Mini Beta on graph coloring as a variable-complexity constraint-satisfaction logic problem, and find evidence from both error rate comparisons and CoT/explanation text analysis that RLLMs are prone to hallucinate edges not specified in the prompt\'s description of the graph. This phenomenon persists across multiple problem complexity levels and semantic frames, and it appears to account for a significant fraction of the incorrect answers from every tested model, and the vast majority of them for some models. Our results indicate that RLLMs may possess broader issues with misrepresentation of problem specifics, and we offer suggestions for design choices to mitigate this weakness.', 'abstract_zh': '大规模语言模型通过强化学习训练的链式思考策略（CoT）在推理任务性能上取得了显著进步，但这些“推理大规模语言模型”（RLLMs）仍然是不完善的推理者。理解其推理失败模式的频率和原因对于用户和开发人员来说都非常重要。我们测试了o1-mini、o3-mini、DeepSeek-R1、Claude 3.7 Sonnet、Gemini 2.5 Pro Preview和Grok 3 Mini Beta在图着色这一变量复杂度约束 satisfaction 逻辑问题上的表现，并通过错误率对比和链式思考/解释文本分析发现，RLLMs倾向于虚构没有在图描述中指定的边。这种现象在不同问题复杂度级别和语义框架中均普遍存在，并且似乎解释了部分测试模型错误答案的大部分，而在某些模型中则解释了绝大部分错误答案。我们的研究结果表明，RLLMs可能存在更广泛的问题，即在问题具体描述上的误表征，并提出了相应的设计选择建议以减轻这一不足。', 'title_zh': '大型语言模型错误源于幻觉关键问题特征'}
{'arxiv_id': 'arXiv:2505.12100', 'title': 'Improving Fairness in LLMs Through Testing-Time Adversaries', 'authors': 'Isabela Pereira Gregio, Ian Pons, Anna Helena Reali Costa, Artur Jordão', 'link': 'https://arxiv.org/abs/2505.12100', 'abstract': 'Large Language Models (LLMs) push the bound-aries in natural language processing and generative AI, driving progress across various aspects of modern society. Unfortunately, the pervasive issue of bias in LLMs responses (i.e., predictions) poses a significant and open challenge, hindering their application in tasks involving ethical sensitivity and responsible decision-making. In this work, we propose a straightforward, user-friendly and practical method to mitigate such biases, enhancing the reliability and trustworthiness of LLMs. Our method creates multiple variations of a given sentence by modifying specific attributes and evaluates the corresponding prediction behavior compared to the original, unaltered, prediction/sentence. The idea behind this process is that critical ethical predictions often exhibit notable inconsistencies, indicating the presence of bias. Unlike previous approaches, our method relies solely on forward passes (i.e., testing-time adversaries), eliminating the need for training, fine-tuning, or prior knowledge of the training data distribution. Through extensive experiments on the popular Llama family, we demonstrate the effectiveness of our method in improving various fairness metrics, focusing on the reduction of disparities in how the model treats individuals from different racial groups. Specifically, using standard metrics, we improve the fairness in Llama3 in up to 27 percentage points. Overall, our approach significantly enhances fairness, equity, and reliability in LLM-generated results without parameter tuning or training data modifications, confirming its effectiveness in practical scenarios. We believe our work establishes an important step toward enabling the use of LLMs in tasks that require ethical considerations and responsible decision-making.', 'abstract_zh': '大型语言模型中的偏见缓解方法：提高伦理敏感性和负责任决策任务中的可靠性和可信度', 'title_zh': '通过测试时对抗方法提高LLMs的公平性'}
{'arxiv_id': 'arXiv:2505.12090', 'title': 'Personalized Author Obfuscation with Large Language Models', 'authors': 'Mohammad Shokri, Sarah Ita Levitan, Rivka Levitan', 'link': 'https://arxiv.org/abs/2505.12090', 'abstract': 'In this paper, we investigate the efficacy of large language models (LLMs) in obfuscating authorship by paraphrasing and altering writing styles. Rather than adopting a holistic approach that evaluates performance across the entire dataset, we focus on user-wise performance to analyze how obfuscation effectiveness varies across individual authors. While LLMs are generally effective, we observe a bimodal distribution of efficacy, with performance varying significantly across users. To address this, we propose a personalized prompting method that outperforms standard prompting techniques and partially mitigates the bimodality issue.', 'abstract_zh': '本文研究了大型语言模型（LLM）在改写和改变写作风格以混淆作者身份方面的效果。我们不采用在整个数据集中评估整体性能的方法，而是侧重于个体用户层面的性能分析，以探讨不同作者之间的混淆效果差异。尽管LLM通常效果显著，但我们观察到其效果呈双峰分布，用户之间的表现差异显著。为此，我们提出了一种个性化提示方法，该方法优于标准提示技术，并部分缓解了双峰问题。', 'title_zh': '个性化作者混淆ewith大规模语言模型'}
{'arxiv_id': 'arXiv:2505.12050', 'title': 'ABoN: Adaptive Best-of-N Alignment', 'authors': 'Vinod Raman, Hilal Asi, Satyen Kale', 'link': 'https://arxiv.org/abs/2505.12050', 'abstract': 'Recent advances in test-time alignment methods, such as Best-of-N sampling, offer a simple and effective way to steer language models (LMs) toward preferred behaviors using reward models (RM). However, these approaches can be computationally expensive, especially when applied uniformly across prompts without accounting for differences in alignment difficulty. In this work, we propose a prompt-adaptive strategy for Best-of-N alignment that allocates inference-time compute more efficiently. Motivated by latency concerns, we develop a two-stage algorithm: an initial exploratory phase estimates the reward distribution for each prompt using a small exploration budget, and a second stage adaptively allocates the remaining budget using these estimates. Our method is simple, practical, and compatible with any LM/RM combination. Empirical results on the AlpacaEval dataset for 12 LM/RM pairs and 50 different batches of prompts show that our adaptive strategy consistently outperforms the uniform allocation with the same inference budget. Moreover, our experiments show that our adaptive strategy remains competitive against uniform allocations with 20% larger inference budgets and even improves in performance as the batch size grows.', 'abstract_zh': 'Recent Advances in Test-Time Alignment Methods, Such as Best-of-N Sampling, Offer Prompt-Adaptive Strategies for Efficient Inference-Time Compute Allocation', 'title_zh': 'ABoN：自适应最佳匹配-alignment'}
{'arxiv_id': 'arXiv:2505.12038', 'title': 'Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets', 'authors': 'Ning Lu, Shengcai Liu, Jiahao Wu, Weiyu Chen, Zhirui Zhang, Yew-Soon Ong, Qi Wang, Ke Tang', 'link': 'https://arxiv.org/abs/2505.12038', 'abstract': "Large language models (LLMs) have shown great potential as general-purpose AI assistants across various domains. To fully leverage this potential in specific applications, many companies provide fine-tuning API services, enabling users to upload their own data for LLM customization. However, fine-tuning services introduce a new safety threat: user-uploaded data, whether harmful or benign, can break the model's alignment, leading to unsafe outputs. Moreover, existing defense methods struggle to address the diversity of fine-tuning datasets (e.g., varying sizes, tasks), often sacrificing utility for safety or vice versa. To address this issue, we propose Safe Delta, a safety-aware post-training defense method that adjusts the delta parameters (i.e., the parameter change before and after fine-tuning). Specifically, Safe Delta estimates the safety degradation, selects delta parameters to maximize utility while limiting overall safety loss, and applies a safety compensation vector to mitigate residual safety loss. Through extensive experiments on four diverse datasets with varying settings, our approach consistently preserves safety while ensuring that the utility gain from benign datasets remains unaffected.", 'abstract_zh': 'Large语言模型（LLMs）在各个领域展现出了作为通用AI助手的巨大潜力。为了在特定应用中充分利用这一潜力，许多公司提供了微调API服务，使用户能够上传自己的数据以定制LLM。然而，微调服务引入了一个新的安全威胁：用户上传的数据，无论是有害的还是无害的，都可能导致模型失准，从而产生不安全的输出。此外，现有的防御方法难以应对微调数据集的多样性（例如，大小和任务的差异），通常会牺牲实用性以获得安全性或反之。为解决这一问题，我们提出了一种安全意识后训练防护方法Safe Delta，该方法调整了微调前后的delta参数。具体来说，Safe Delta估计了安全降级，选择delta参数以最大化实用性同时限制总体安全性损失，并应用安全补偿向量以减轻剩余的安全损失。通过在四个不同设置下的四种多样化的数据集上进行广泛实验，我们的方法在保证安全性的同时，确保良性数据集的实用性增益不受影响。', 'title_zh': 'Safe Delta: 一致地在多样数据集上微调大语言模型时保持安全性'}
{'arxiv_id': 'arXiv:2505.11963', 'title': 'MARVEL: Multi-Agent RTL Vulnerability Extraction using Large Language Models', 'authors': 'Luca Collini, Baleegh Ahmad, Joey Ah-kiow, Ramesh Karri', 'link': 'https://arxiv.org/abs/2505.11963', 'abstract': 'Hardware security verification is a challenging and time-consuming task. For this purpose, design engineers may utilize tools such as formal verification, linters, and functional simulation tests, coupled with analysis and a deep understanding of the hardware design being inspected. Large Language Models (LLMs) have been used to assist during this task, either directly or in conjunction with existing tools. We improve the state of the art by proposing MARVEL, a multi-agent LLM framework for a unified approach to decision-making, tool use, and reasoning. MARVEL mimics the cognitive process of a designer looking for security vulnerabilities in RTL code. It consists of a supervisor agent that devises the security policy of the system-on-chips (SoCs) using its security documentation. It delegates tasks to validate the security policy to individual executor agents. Each executor agent carries out its assigned task using a particular strategy. Each executor agent may use one or more tools to identify potential security bugs in the design and send the results back to the supervisor agent for further analysis and confirmation. MARVEL includes executor agents that leverage formal tools, linters, simulation tests, LLM-based detection schemes, and static analysis-based checks. We test our approach on a known buggy SoC based on OpenTitan from the Hack@DATE competition. We find that 20 of the 48 issues reported by MARVEL pose security vulnerabilities.', 'abstract_zh': '硬件安全验证是一个具有挑战性和耗时的任务。为此，设计工程师可能会利用形式验证、linters和功能仿真测试等工具，结合分析和对被检查硬件设计的深刻理解。大规模语言模型（LLMs）已被用于协助这一任务，直接或与现有工具结合使用。我们通过提出一种多代理LLM框架MARVEL，改进了这一领域的研究水平，MARVEL提供了一种统一的方法来进行决策、工具使用和推理。MARVEL模仿了设计人员在 RTL代码中查找安全漏洞的认知过程。它包括一个监督代理，利用其安全文档制定系统级芯片（SoCs）的安全策略，并将任务委派给个体执行代理。每个执行代理使用特定策略执行其分配的任务。每个执行代理可以利用一种或多种工具来识别设计中的潜在安全漏洞，并将结果反馈给监督代理以进行进一步的分析和确认。MARVEL包括利用形式工具、linters、仿真测试、基于LLM的检测方案和静态分析检查的执行代理。我们基于Hack@DATE竞赛中的OpenTitan实现了一个已知有漏洞的SoC对其进行测试。我们发现MARVEL报告的48个问题中有20个确实存在安全漏洞。', 'title_zh': 'MARVEL: 多Agent RTL漏洞提取使用大规模语言模型'}
{'arxiv_id': 'arXiv:2505.11953', 'title': 'Exploring Criteria of Loss Reweighting to Enhance LLM Unlearning', 'authors': 'Puning Yang, Qizhou Wang, Zhuo Huang, Tongliang Liu, Chengqi Zhang, Bo Han', 'link': 'https://arxiv.org/abs/2505.11953', 'abstract': 'Loss reweighting has shown significant benefits for machine unlearning with large language models (LLMs). However, their exact functionalities are left unclear and the optimal strategy remains an open question, thus impeding the understanding and improvement of existing methodologies. In this paper, we identify two distinct goals of loss reweighting, namely, Saturation and Importance -- the former indicates that those insufficiently optimized data should be emphasized, while the latter stresses some critical data that are most influential for loss minimization. To study their usefulness, we design specific reweighting strategies for each goal and evaluate their respective effects on unlearning. We conduct extensive empirical analyses on well-established benchmarks, and summarize some important observations as follows: (i) Saturation enhances efficacy more than importance-based reweighting, and their combination can yield additional improvements. (ii) Saturation typically allocates lower weights to data with lower likelihoods, whereas importance-based reweighting does the opposite. (iii) The efficacy of unlearning is also largely influenced by the smoothness and granularity of the weight distributions. Based on these findings, we propose SatImp, a simple reweighting method that combines the advantages of both saturation and importance. Empirical results on extensive datasets validate the efficacy of our method, potentially bridging existing research gaps and indicating directions for future research. Our code is available at this https URL.', 'abstract_zh': '损失加权对大型语言模型机器遗忘的影响显著，但其具体功能尚不明确，最优策略仍存争议，从而阻碍了现有方法的理解与改进。在本文中，我们识别出损失加权的两个 distinct 目标，即饱和度和重要性——前者表明应突出那些优化不足的数据，而后者强调对损失最小化影响最大的关键数据。为了研究它们的有效性，我们为每个目标设计了特定的加权策略，并评估了它们对遗忘的影响。我们在广泛认可的基准上进行了广泛的经验分析，并总结了以下重要观察结果：(i) 饱和度比基于重要性的加权更能提高效果，它们的结合还可以带来额外的改进。(ii) 饱和度通常为较低概率的数据分配较低的权重，而基于重要性的加权则与此相反。(iii) 遗忘效果还受权重分布的平滑性和粒度的大幅影响。根据这些发现，我们提出了 SatImp，一种结合了饱和度和重要性优点的简单加权方法。在大量数据集上的实验证明了我们方法的有效性，有潜力填补现有研究空白并为未来研究指明方向。我们的代码可在该网页获取。', 'title_zh': '探索损失重权化标准以增强语言模型去学习能力'}
{'arxiv_id': 'arXiv:2505.11926', 'title': 'SafeVid: Toward Safety Aligned Video Large Multimodal Models', 'authors': 'Yixu Wang, Jiaxin Song, Yifeng Gao, Xin Wang, Yang Yao, Yan Teng, Xingjun Ma, Yingchun Wang, Yu-Gang Jiang', 'link': 'https://arxiv.org/abs/2505.11926', 'abstract': 'As Video Large Multimodal Models (VLMMs) rapidly advance, their inherent complexity introduces significant safety challenges, particularly the issue of mismatched generalization where static safety alignments fail to transfer to dynamic video contexts. We introduce SafeVid, a framework designed to instill video-specific safety principles in VLMMs. SafeVid uniquely transfers robust textual safety alignment capabilities to the video domain by employing detailed textual video descriptions as an interpretive bridge, facilitating LLM-based rule-driven safety reasoning. This is achieved through a closed-loop system comprising: 1) generation of SafeVid-350K, a novel 350,000-pair video-specific safety preference dataset; 2) targeted alignment of VLMMs using Direct Preference Optimization (DPO); and 3) comprehensive evaluation via our new SafeVidBench benchmark. Alignment with SafeVid-350K significantly enhances VLMM safety, with models like LLaVA-NeXT-Video demonstrating substantial improvements (e.g., up to 42.39%) on SafeVidBench. SafeVid provides critical resources and a structured approach, demonstrating that leveraging textual descriptions as a conduit for safety reasoning markedly improves the safety alignment of VLMMs. We have made SafeVid-350K dataset (this https URL) publicly available.', 'abstract_zh': '随着视频大型多模态模型（VLMMs）的迅速发展，其固有的复杂性带来了重大的安全挑战，特别是静态安全对齐与动态视频场景之间不匹配的问题。我们提出了SafeVid框架，旨在将视频特定的安全原则应用于VLMMs。SafeVid通过使用详细的视频文本描述作为解释桥梁，将强大的文本安全对齐能力转移到视频领域，从而促进基于LLM的规则驱动安全推理。这一过程通过一个闭环系统实现：1）生成SafeVid-350K，一个新颖的包含350,000对视频特定安全偏好的数据集；2）使用直接偏好优化（DPO）对VLMMs进行目标对齐；3）通过我们的新SafeVidBench基准进行全面评估。与SafeVid-350K对齐显著提升了VLMM的安全性，如LLaVA-NeXT-Video模型在SafeVidBench上的表现大幅提升（例如高达42.39%）。SafeVid提供关键资源和结构化方法，证明将文本描述作为安全推理的通道显著提高了VLMMs的安全对齐。我们已公开发布了SafeVid-350K数据集（请参见原文链接）。', 'title_zh': 'SafeVid: 向安全对齐的视频大型多模态模型方向'}
{'arxiv_id': 'arXiv:2505.11924', 'title': 'An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts', 'authors': 'Yu-Ting Lee, Hui-Ying Shih, Fu-Chieh Chang, Pei-Yuan Wu', 'link': 'https://arxiv.org/abs/2505.11924', 'abstract': "We provide an explanation for the performance gains of intrinsic self-correction, a process where a language model iteratively refines its outputs without external feedback. More precisely, we investigate how prompting induces interpretable changes in hidden states and thus affects the output distributions. We hypothesize that each prompt-induced shift lies in a linear span of some linear representation vectors, naturally separating tokens based on individual concept alignment. Building around this idea, we give a mathematical formulation of self-correction and derive a concentration result for output tokens based on alignment magnitudes. Our experiments on text detoxification with zephyr-7b-sft reveal a substantial gap in the inner products of the prompt-induced shifts and the unembeddings of the top-100 most toxic tokens vs. those of the unembeddings of the bottom-100 least toxic tokens, under toxic instructions. This suggests that self-correction prompts enhance a language model's capability of latent concept recognition. Our analysis offers insights into the underlying mechanism of self-correction by characterizing how prompting works explainably. For reproducibility, our code is available.", 'abstract_zh': '我们提供了内在自我纠正性能提升的解释，这是一种语言模型在无外部反馈的情况下迭代 refinement 输出的过程。更具体地，我们研究了提示如何诱导隐藏状态的可解释变化，进而影响输出分布。我们假设每个提示诱导的变化位于某些线性表示向量的线性组合内，自然地根据个体概念对齐分离 token。基于这一想法，我们给出了自我纠正的数学表述，并根据对齐幅度推导了输出 token 的集中结果。我们在使用 zephyr-7b-sft 进行文本脱毒实验中发现，在有毒指令下，提示诱导的变化与前 100 个最毒 token 的 unembedding 的内积与后 100 个最不毒 token 的 unembedding 的内积之间存在显著差异。这表明自我纠正提示增强了语言模型对潜在概念识别的能力。我们的分析通过描述提示如何工作来揭示自我纠正的潜在机制。为了可再现性，我们的代码已公开。', 'title_zh': '通过线性表示和潜在概念解释固有自我纠正机制'}
{'arxiv_id': 'arXiv:2505.11896', 'title': 'AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning', 'authors': 'Chenwei Lou, Zewei Sun, Xinnian Liang, Meng Qu, Wei Shen, Wenqi Wang, Yuntao Li, Qingping Yang, Shuangzhi Wu', 'link': 'https://arxiv.org/abs/2505.11896', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities but often face challenges with tasks requiring sophisticated reasoning. While Chain-of-Thought (CoT) prompting significantly enhances reasoning, it indiscriminately generates lengthy reasoning steps for all queries, leading to substantial computational costs and inefficiency, especially for simpler inputs. To address this critical issue, we introduce AdaCoT (Adaptive Chain-of-Thought), a novel framework enabling LLMs to adaptively decide when to invoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem that seeks to balance model performance with the costs associated with CoT invocation (both frequency and computational overhead). We propose a reinforcement learning (RL) based method, specifically utilizing Proximal Policy Optimization (PPO), to dynamically control the CoT triggering decision boundary by adjusting penalty coefficients, thereby allowing the model to determine CoT necessity based on implicit query complexity. A key technical contribution is Selective Loss Masking (SLM), designed to counteract decision boundary collapse during multi-stage RL training, ensuring robust and stable adaptive triggering. Experimental results demonstrate that AdaCoT successfully navigates the Pareto frontier, achieving substantial reductions in CoT usage for queries not requiring elaborate reasoning. For instance, on our production traffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18\\% and decreased average response tokens by 69.06%, while maintaining high performance on complex tasks.', 'abstract_zh': 'AdaCoT：自适应链式推理', 'title_zh': 'AdaCoT: Pareto-最优自适应链式思考触发机制 via 强化学习'}
{'arxiv_id': 'arXiv:2505.11893', 'title': 'RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving', 'authors': 'Zepeng Ding, Dixuan Wang, Ziqin Luo, Guochao Jiang, Deqing Yang, Jiaqing Liang', 'link': 'https://arxiv.org/abs/2505.11893', 'abstract': "Multi-step planning has been widely employed to enhance the performance of large language models (LLMs) on downstream natural language processing (NLP) tasks, which decomposes the original task into multiple subtasks and guide LLMs to solve them sequentially without additional training. When addressing task instances, existing methods either preset the order of steps or attempt multiple paths at each step. However, these methods overlook instances' linguistic features and rely on the intrinsic planning capabilities of LLMs to evaluate intermediate feedback and then select subtasks, resulting in suboptimal outcomes. To better solve multi-step NLP tasks with LLMs, in this paper we propose a Reinforcement Learning enhanced Adaptive Planning framework (RLAP). In our framework, we model an NLP task as a Markov decision process (MDP) and employ an LLM directly into the environment. In particular, a lightweight Actor model is trained to estimate Q-values for natural language sequences consisting of states and actions through reinforcement learning. Therefore, during sequential planning, the linguistic features of each sequence in the MDP can be taken into account, and the Actor model interacts with the LLM to determine the optimal order of subtasks for each task instance. We apply RLAP on three different types of NLP tasks and conduct extensive experiments on multiple datasets to verify RLAP's effectiveness and robustness.", 'abstract_zh': '多步规划增强的大语言模型在下游自然语言处理任务中的适应性规划框架（基于强化学习）', 'title_zh': 'RLAP：增强学习增强的自适应规划框架多步NLP任务解决'}
{'arxiv_id': 'arXiv:2505.11837', 'title': 'On Membership Inference Attacks in Knowledge Distillation', 'authors': 'Ziyao Cui, Minxing Zhang, Jian Pei', 'link': 'https://arxiv.org/abs/2505.11837', 'abstract': "Nowadays, Large Language Models (LLMs) are trained on huge datasets, some including sensitive information. This poses a serious privacy concern because privacy attacks such as Membership Inference Attacks (MIAs) may detect this sensitive information. While knowledge distillation compresses LLMs into efficient, smaller student models, its impact on privacy remains underexplored. In this paper, we investigate how knowledge distillation affects model robustness against MIA. We focus on two questions. First, how is private data protected in teacher and student models? Second, how can we strengthen privacy preservation against MIAs in knowledge distillation? Through comprehensive experiments, we show that while teacher and student models achieve similar overall MIA accuracy, teacher models better protect member data, the primary target of MIA, whereas student models better protect non-member data. To address this vulnerability in student models, we propose 5 privacy-preserving distillation methods and demonstrate that they successfully reduce student models' vulnerability to MIA, with ensembling further stabilizing the robustness, offering a reliable approach for distilling more secure and efficient student models. Our implementation source code is available at this https URL.", 'abstract_zh': '现今，大规模语言模型（LLMs）在包含敏感信息的大规模数据集上进行训练。这引发了严重的隐私 concerns，因为成员隶属推理攻击（MIAs）可能探测到这些敏感信息。虽然知识蒸馏可以将LLMs压缩成高效的较小规模学生模型，但其对学生隐私的影响仍缺乏深入探索。本文探讨知识蒸馏如何影响模型对抗MIAs的robustness。我们重点关注两个问题：首先，教师模型和学生模型如何保护私有数据？其次，如何在知识蒸馏过程中加强对抗MIAs的隐私保护？通过全面的实验，我们发现虽然教师模型和学生模型在整体MIAs准确率上相似，但教师模型更能保护成员数据，这是MIAs的主要目标；而学生模型则更好地保护非成员数据。为解决学生模型的这一脆弱性，我们提出5种隐私保护蒸馏方法，并证明这些方法成功降低了学生模型对MIAs的脆弱性，而集成进一步提高了robustness的稳定性，为蒸馏更安全高效的学生模型提供了可靠的途径。我们的实现源代码可在此httpsURL访问。', 'title_zh': '知识蒸馏中的成员 inference 攻击'}
{'arxiv_id': 'arXiv:2505.11836', 'title': 'SplInterp: Improving our Understanding and Training of Sparse Autoencoders', 'authors': 'Jeremy Budd, Javier Ideami, Benjamin Macdowall Rynne, Keith Duggar, Randall Balestriero', 'link': 'https://arxiv.org/abs/2505.11836', 'abstract': "Sparse autoencoders (SAEs) have received considerable recent attention as tools for mechanistic interpretability, showing success at extracting interpretable features even from very large LLMs. However, this research has been largely empirical, and there have been recent doubts about the true utility of SAEs. In this work, we seek to enhance the theoretical understanding of SAEs, using the spline theory of deep learning. By situating SAEs in this framework: we discover that SAEs generalise ``$k$-means autoencoders'' to be piecewise affine, but sacrifice accuracy for interpretability vs. the optimal ``$k$-means-esque plus local principal component analysis (PCA)'' piecewise affine autoencoder. We characterise the underlying geometry of (TopK) SAEs using power diagrams. And we develop a novel proximal alternating method SGD (PAM-SGD) algorithm for training SAEs, with both solid theoretical foundations and promising empirical results in MNIST and LLM experiments, particularly in sample efficiency and (in the LLM setting) improved sparsity of codes. All code is available at: this https URL", 'abstract_zh': '稀疏自编码器（SAEs）近年来因其在机制可解释性方面的工具作用而受到广泛关注，显示出即使在非常大的LLM中也能提取可解释特征的成功案例。然而，这项研究主要基于经验，近期对SAEs的实际用途产生了怀疑。在本文中，我们旨在通过使用深度学习的样条理论来增强对SAEs的理论理解。通过将SAEs置于这一框架中，我们发现SAEs推广了“$k$-means自编码器”成为分段仿射模型，但与最优的“$k$-means-esque加上局部主成分分析（PCA）”分段仿射自编码器相比，牺牲了准确性以换取可解释性。我们使用幂图（power diagrams）来表征（TopK）SAEs的潜在几何结构。并开发了一种新颖的邻近交替优化随机梯度下降（PAM-SGD）算法用于训练SAEs，该算法具有坚实理论基础并在MNIST和LLM实验中表现出令人鼓舞的经验结果，特别是在样本效率以及（在LLM设置中）代码稀疏性方面。所有代码均可在以下链接获取：this https URL。', 'title_zh': 'SplInterp: 提升我们对稀疏自编码器的理解和训练'}
{'arxiv_id': 'arXiv:2505.11835', 'title': 'Multilingual Collaborative Defense for Large Language Models', 'authors': 'Hongliang Li, Jinan Xu, Gengping Cui, Changhao Guan, Fengran Mo, Kaiyu Huang', 'link': 'https://arxiv.org/abs/2505.11835', 'abstract': 'The robustness and security of large language models (LLMs) has become a prominent research area. One notable vulnerability is the ability to bypass LLM safeguards by translating harmful queries into rare or underrepresented languages, a simple yet effective method of "jailbreaking" these models. Despite the growing concern, there has been limited research addressing the safeguarding of LLMs in multilingual scenarios, highlighting an urgent need to enhance multilingual safety. In this work, we investigate the correlation between various attack features across different languages and propose Multilingual Collaborative Defense (MCD), a novel learning method that optimizes a continuous, soft safety prompt automatically to facilitate multilingual safeguarding of LLMs. The MCD approach offers three advantages: First, it effectively improves safeguarding performance across multiple languages. Second, MCD maintains strong generalization capabilities while minimizing false refusal rates. Third, MCD mitigates the language safety misalignment caused by imbalances in LLM training corpora. To evaluate the effectiveness of MCD, we manually construct multilingual versions of commonly used jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess various safeguarding methods. Additionally, we introduce these datasets in underrepresented (zero-shot) languages to verify the language transferability of MCD. The results demonstrate that MCD outperforms existing approaches in safeguarding against multilingual jailbreak attempts while also exhibiting strong language transfer capabilities. Our code is available at this https URL.', 'abstract_zh': '大型语言模型（LLMs）的鲁棒性和安全性的研究已成为一个重要的研究领域。一个值得注意的漏洞是通过将有害查询翻译成稀有或代表性不足的语言来规避LLM的安全措施，这是一种简单而有效的方法来“破解”这些模型。尽管存在日益增长的关切，但对于多语言场景下的LLM保护研究仍然有限，这突显了增强多语言安全性的迫切需求。在本工作中，我们探讨了不同语言中各种攻击特征之间的相关性，并提出了一种名为多语言协作防护（MCD）的新型学习方法，该方法自动优化连续的软安全性提示，以促进LLM的多语言保护。MCD方法具有三个优势：首先，它能有效提高多语言保护性能。其次，MCD保持了强大的泛化能力，同时将拒绝率降至最低。第三，MCD缓解了由于LLM训练语料库不平衡导致的语言安全性不对齐问题。为了评估MCD的有效性，我们手动构建了常用的“破解”基准的多语言版本，如MaliciousInstruct和AdvBench，以评估各种保护方法。此外，我们还在欠代表语言（零样本）中引入了这些数据集，以验证MCD的语言迁移能力。结果表明，MCD在防止多语言“破解”尝试方面优于现有方法，并且表现出强大的语言迁移能力。我们的代码可在以下网址获取。', 'title_zh': '多语言协同防御大规模语言模型'}
{'arxiv_id': 'arXiv:2505.11827', 'title': 'Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning', 'authors': 'Yansong Ning, Wei Li, Jun Fang, Naiqiang Tan, Hao Liu', 'link': 'https://arxiv.org/abs/2505.11827', 'abstract': 'Compressing long chain-of-thought (CoT) from large language models (LLMs) is an emerging strategy to improve the reasoning efficiency of LLMs. Despite its promising benefits, existing studies equally compress all thoughts within a long CoT, hindering more concise and effective reasoning. To this end, we first investigate the importance of different thoughts by examining their effectiveness and efficiency in contributing to reasoning through automatic long CoT chunking and Monte Carlo rollouts. Building upon the insights, we propose a theoretically bounded metric to jointly measure the effectiveness and efficiency of different thoughts. We then propose Long$\\otimes$Short, an efficient reasoning framework that enables two LLMs to collaboratively solve the problem: a long-thought LLM for more effectively generating important thoughts, while a short-thought LLM for efficiently generating remaining thoughts. Specifically, we begin by synthesizing a small amount of cold-start data to fine-tune LLMs for long-thought and short-thought reasoning styles, respectively. Furthermore, we propose a synergizing-oriented multi-turn reinforcement learning, focusing on the model self-evolution and collaboration between long-thought and short-thought LLMs. Experimental results show that our method enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance compared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while reducing token length by over 80% across the MATH500, AIME24/25, AMC23, and GPQA Diamond benchmarks. Our data and code are available at this https URL.', 'abstract_zh': '压缩大型语言模型中的长链推理（CoT）以提高其推理效率是一种新兴策略。尽管这一方法具有潜在的优势，现有的研究同样压缩长链推理中的所有想法，这阻碍了更为简洁和有效的推理。为此，我们首先通过自动长链推理分块和蒙特卡洛展开，研究不同想法在贡献推理中的效果和效率，以探讨其重要性。基于这些洞察，我们提出了一种理论上受限的度量标准，以联合衡量不同想法的效果和效率。随后，我们提出了一种高效的推理框架——Long$\\otimes$Short，使两个大型语言模型能够协作解决问题：一个长想法模型更有效地生成重要想法，另一个短想法模型则高效生成剩余的想法。具体而言，我们首先生成少量冷启动数据，分别微调大型语言模型进行长想法和短想法推理。此外，我们提出了一种以协同进化为重点的多轮强化学习，关注模型的自我进化和长想法与短想法大型语言模型之间的合作。实验结果显示，我们的方法使Qwen2.5-7B和Llama3.1-8B在MATH500、AIME24/25、AMC23以及GPQA钻石基准测试中达到了与DeepSeek-R1-Distill-Qwen-7B和DeepSeek-R1-Distill-Llama-8B相当的性能，同时将令牌长度减少了超过80%。我们的数据和代码可在以下链接获取。', 'title_zh': '并非所有思维生成均等：基于多轮强化学习的高效LLM推理'}
{'arxiv_id': 'arXiv:2505.11824', 'title': 'Search-Based Correction of Reasoning Chains for Language Models', 'authors': 'Minsu Kim, Jean-Pierre Falet, Oliver E. Richardson, Xiaoyin Chen, Moksh Jain, Sungjin Ahn, Sungsoo Ahn, Yoshua Bengio', 'link': 'https://arxiv.org/abs/2505.11824', 'abstract': "Chain-of-Thought (CoT) reasoning has advanced the capabilities and transparency of language models (LMs); however, reasoning chains can contain inaccurate statements that reduce performance and trustworthiness. To address this, we introduce a new self-correction framework that augments each reasoning step in a CoT with a latent variable indicating its veracity, enabling modeling of all possible truth assignments rather than assuming correctness throughout. To efficiently explore this expanded space, we introduce Search Corrector, a discrete search algorithm over boolean-valued veracity assignments. It efficiently performs otherwise intractable inference in the posterior distribution over veracity assignments by leveraging the LM's joint likelihood over veracity and the final answer as a proxy reward. This efficient inference-time correction method facilitates supervised fine-tuning of an Amortized Corrector by providing pseudo-labels for veracity. The Amortized Corrector generalizes self-correction, enabling accurate zero-shot veracity inference in novel contexts. Empirical results demonstrate that Search Corrector reliably identifies errors in logical (ProntoQA) and mathematical reasoning (GSM8K) benchmarks. The Amortized Corrector achieves comparable zero-shot accuracy and improves final answer accuracy by up to 25%.", 'abstract_zh': 'Chain-of-Thought推理的自校正框架：提高语言模型的性能与可信度', 'title_zh': '基于搜索的逻辑推理链修正方法'}
{'arxiv_id': 'arXiv:2505.11774', 'title': 'HARDMath2: A Benchmark for Applied Mathematics Built by Students as Part of a Graduate Class', 'authors': 'James V. Roggeveen, Erik Y. Wang, Will Flintoft, Peter Donets, Lucy S. Nathwani, Nickholas Gutierrez, David Ettel, Anton Marius Graf, Siddharth Dandavate, Arjun Nageswaran, Raglan Ward, Ava Williamson, Anne Mykland, Kacper K. Migacz, Yijun Wang, Egemen Bostan, Duy Thuc Nguyen, Zhe He, Marc L. Descoteaux, Felix Yeung, Shida Liu, Jorge García Ponce, Luke Zhu, Yuyang Chen, Ekaterina S. Ivshina, Miguel Fernandez, Minjae Kim, Kennan Gumbs, Matthew Scott Tan, Russell Yang, Mai Hoang, David Brown, Isabella A. Silveira, Lavon Sykes, Ahmed Roman, William Fredenberg, Yiming Chen, Lucas Martin, Yixing Tang, Kelly Werker Smith, Hongyu Liao, Logan G. Wilson, Alexander Dazhen Cai, Andrea Elizabeth Biju, Michael P. Brenner', 'link': 'https://arxiv.org/abs/2505.11774', 'abstract': "Large language models (LLMs) have shown remarkable progress in mathematical problem-solving, but evaluation has largely focused on problems that have exact analytical solutions or involve formal proofs, often overlooking approximation-based problems ubiquitous in applied science and engineering. To fill this gap, we build on prior work and present HARDMath2, a dataset of 211 original problems covering the core topics in an introductory graduate applied math class, including boundary-layer analysis, WKB methods, asymptotic solutions of nonlinear partial differential equations, and the asymptotics of oscillatory integrals. This dataset was designed and verified by the students and instructors of a core graduate applied mathematics course at Harvard. We build the dataset through a novel collaborative environment that challenges students to write and refine difficult problems consistent with the class syllabus, peer-validate solutions, test different models, and automatically check LLM-generated solutions against their own answers and numerical ground truths. Evaluation results show that leading frontier models still struggle with many of the problems in the dataset, highlighting a gap in the mathematical reasoning skills of current LLMs. Importantly, students identified strategies to create increasingly difficult problems by interacting with the models and exploiting common failure modes. This back-and-forth with the models not only resulted in a richer and more challenging benchmark but also led to qualitative improvements in the students' understanding of the course material, which is increasingly important as we enter an age where state-of-the-art language models can solve many challenging problems across a wide domain of fields.", 'abstract_zh': '大型语言模型（LLMs）在数学问题解决方面取得了显著进展，但评估主要集中在具有精确解析解或涉及形式证明的问题上，往往忽视了应用科学和工程中常见的基于近似的问题。为了弥补这一不足，我们在此基础上提出了HARDMath2数据集，包含211个原创问题，涵盖哈佛大学核心研究生应用数学课程中的核心主题，包括边界层分析、WKB方法、非线性偏微分方程的渐近解以及振荡积分的渐近分析。该数据集由该课程的学生和教师设计和验证。我们通过一个新颖的合作环境构建数据集，挑战学生编写和改进符合课程大纲的难题，同伴验证解决方案，测试不同的模型，并自动检查LLM生成的解决方案与自己的答案和数值真实值之间的差异。评估结果表明，当前领先的前沿模型仍然难以解决数据集中许多问题，突显了当前LLMs在数学推理能力上的差距。重要的是，学生通过与模型的互动和利用常见的失败模式，发现创建更难问题的策略。这种与模型的互动不仅产生了更加丰富和更具挑战性的基准，还促进了学生对课程材料的深刻理解，这在我们进入一个先进语言模型可以解决广泛领域内许多挑战性问题的时代尤为重要。', 'title_zh': 'HARDMath2：由学生在研究生课程中构建的应用数学基准测试'}
{'arxiv_id': 'arXiv:2505.11770', 'title': 'Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors', 'authors': 'Jing Huang, Junyi Tao, Thomas Icard, Diyi Yang, Christopher Potts', 'link': 'https://arxiv.org/abs/2505.11770', 'abstract': "Interpretability research now offers a variety of techniques for identifying abstract internal mechanisms in neural networks. Can such techniques be used to predict how models will behave on out-of-distribution examples? In this work, we provide a positive answer to this question. Through a diverse set of language modeling tasks--including symbol manipulation, knowledge retrieval, and instruction following--we show that the most robust features for correctness prediction are those that play a distinctive causal role in the model's behavior. Specifically, we propose two methods that leverage causal mechanisms to predict the correctness of model outputs: counterfactual simulation (checking whether key causal variables are realized) and value probing (using the values of those variables to make predictions). Both achieve high AUC-ROC in distribution and outperform methods that rely on causal-agnostic features in out-of-distribution settings, where predicting model behaviors is more crucial. Our work thus highlights a novel and significant application for internal causal analysis of language models.", 'abstract_zh': '当前的可解释性研究提供了多种技术来识别神经网络中的抽象内部机制。这些技术能否用于预测模型在分布外样本上的行为？在这项工作中，我们给出了肯定的答案。通过一系列多样的语言建模任务——包括符号操作、知识检索和指令遵循——我们表明，最 robust 的正确性预测特征是那些在模型行为中扮演独特因果角色的特征。 Specifically，我们提出了两种利用因果机制来预测模型输出正确性的方法：反事实仿真（检查关键因果变量是否实现）和值探测（使用这些变量的值来做出预测）。这两种方法在分布内都达到了高 AUC-ROC，并在分布外环境中优于依赖于因果无关特征的方法，因为在这种环境下预测模型行为更为关键。因此，我们的工作强调了一种新颖且重要的语言模型内部因果分析的应用。', 'title_zh': '内部因果机制稳健预测语言模型离分布行为'}
{'arxiv_id': 'arXiv:2505.11765', 'title': 'OMAC: A Broad Optimization Framework for LLM-Based Multi-Agent Collaboration', 'authors': 'Shijun Li, Hilaf Hasson, Joydeep Ghosh', 'link': 'https://arxiv.org/abs/2505.11765', 'abstract': 'Agents powered by advanced large language models (LLMs) have demonstrated impressive capabilities across diverse complex applications. Recently, Multi-Agent Systems (MAS), wherein multiple agents collaborate and communicate with each other, have exhibited enhanced capabilities in complex tasks, such as high-quality code generation and arithmetic reasoning. However, the development of such systems often relies on handcrafted methods, and the literature on systematic design and optimization of LLM-based MAS remains limited.\nIn this work, we introduce OMAC, a general framework designed for holistic optimization of LLM-based MAS. Specifically, we identify five key optimization dimensions for MAS, encompassing both agent functionality and collaboration structure. Building upon these dimensions, we first propose a general algorithm, utilizing two actors termed the Semantic Initializer and the Contrastive Comparator, to optimize any single dimension. Then, we present an algorithm for joint optimization across multiple dimensions. Extensive experiments demonstrate the superior performance of OMAC on code generation, arithmetic reasoning, and general reasoning tasks against state-of-the-art approaches.', 'abstract_zh': '基于先进大型语言模型的代理驱动系统全面优化框架OMAC', 'title_zh': 'OMAC：一种基于大规模语言模型的多Agent协作广义优化框架'}
{'arxiv_id': 'arXiv:2505.11764', 'title': 'Towards Universal Semantics With Large Language Models', 'authors': 'Raymond Baartmans, Matthew Raffel, Rahul Vikram, Aiden Deringer, Lizhong Chen', 'link': 'https://arxiv.org/abs/2505.11764', 'abstract': 'The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a universal set of semantic primes: simple, primitive word-meanings that have been shown to exist in most, if not all, languages of the world. According to this framework, any word, regardless of complexity, can be paraphrased using these primes, revealing a clear and universally translatable meaning. These paraphrases, known as explications, can offer valuable applications for many natural language processing (NLP) tasks, but producing them has traditionally been a slow, manual process. In this work, we present the first study of using large language models (LLMs) to generate NSM explications. We introduce automatic evaluation methods, a tailored dataset for training and evaluation, and fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in producing accurate, cross-translatable explications, marking a significant step toward universal semantic representation with LLMs and opening up new possibilities for applications in semantic analysis, translation, and beyond.', 'abstract_zh': '基于语义原素的自然语义金属语言（NSM）是一种基于一套普遍存在的简单原始语义——这些语义在世界的大多数语言中被证实存在——的linguistic理论。根据这一框架，任何单词，无论其复杂程度如何，都可以用这些语义原素进行重新表述，从而揭示清晰且可普遍翻译的意义。这些重新表述被称为explications，它们可以在许多自然语言处理（NLP）任务中提供有价值的应用，但传统上生成它们是一个缓慢且手动的过程。在本研究中，我们提出了首次使用大型语言模型（LLMs）生成NSM explications的研究。我们介绍了自动评估方法、用于训练和评估的定制数据集以及针对该任务的微调模型。我们的1B和8B模型在生成准确且跨语言可翻译的explications方面优于GPT-4o，标志着使用LLMs实现通用语义表示的重要一步，并为语义分析、翻译等领域的新型应用打开了新的可能性。', 'title_zh': '面向通用语义的大语言模型研究'}
{'arxiv_id': 'arXiv:2505.11756', 'title': 'Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders', 'authors': 'David Chanin, Tomáš Dulka, Adrià Garriga-Alonso', 'link': 'https://arxiv.org/abs/2505.11756', 'abstract': 'It is assumed that sparse autoencoders (SAEs) decompose polysemantic activations into interpretable linear directions, as long as the activations are composed of sparse linear combinations of underlying features. However, we find that if an SAE is more narrow than the number of underlying "true features" on which it is trained, and there is correlation between features, the SAE will merge components of correlated features together, thus destroying monosemanticity. In LLM SAEs, these two conditions are almost certainly true. This phenomenon, which we call feature hedging, is caused by SAE reconstruction loss, and is more severe the narrower the SAE. In this work, we introduce the problem of feature hedging and study it both theoretically in toy models and empirically in SAEs trained on LLMs. We suspect that feature hedging may be one of the core reasons that SAEs consistently underperform supervised baselines. Finally, we use our understanding of feature hedging to propose an improved variant of matryoshka SAEs. Our work shows there remain fundamental issues with SAEs, but we are hopeful that that highlighting feature hedging will catalyze future advances that allow SAEs to achieve their full potential of interpreting LLMs at scale.', 'abstract_zh': '特征对冲：稀疏自编码器在大规模语言模型中的表现不佳原因探究', 'title_zh': '特征对冲：相关特征打破窄稀疏自编码器'}
{'arxiv_id': 'arXiv:2505.11743', 'title': 'Cloud-Based AI Systems: Leveraging Large Language Models for Intelligent Fault Detection and Autonomous Self-Healing', 'authors': 'Cheng Ji, Huaiying Luo', 'link': 'https://arxiv.org/abs/2505.11743', 'abstract': "With the rapid development of cloud computing systems and the increasing complexity of their infrastructure, intelligent mechanisms to detect and mitigate failures in real time are becoming increasingly important. Traditional methods of failure detection are often difficult to cope with the scale and dynamics of modern cloud environments. In this study, we propose a novel AI framework based on Massive Language Model (LLM) for intelligent fault detection and self-healing mechanisms in cloud systems. The model combines existing machine learning fault detection algorithms with LLM's natural language understanding capabilities to process and parse system logs, error reports, and real-time data streams through semantic context. The method adopts a multi-level architecture, combined with supervised learning for fault classification and unsupervised learning for anomaly detection, so that the system can predict potential failures before they occur and automatically trigger the self-healing mechanism. Experimental results show that the proposed model is significantly better than the traditional fault detection system in terms of fault detection accuracy, system downtime reduction and recovery speed.", 'abstract_zh': '基于大规模语言模型的智能故障检测与自愈机制框架', 'title_zh': '基于云的AI系统：利用大规模语言模型进行智能故障检测与自主自我修复'}
{'arxiv_id': 'arXiv:2505.11739', 'title': "ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training", 'authors': 'Feijiang Han, Xiaodong Yu, Jianheng Tang, Lyle Ungar', 'link': 'https://arxiv.org/abs/2505.11739', 'abstract': "Recently, training-free methods for improving large language models (LLMs) have attracted growing interest, with token-level attention tuning emerging as a promising and interpretable direction. However, existing methods typically rely on auxiliary mechanisms to identify important or irrelevant task-specific tokens, introducing potential bias and limiting applicability. In this paper, we uncover a surprising and elegant alternative: the semantically empty initial token is a powerful and underexplored control point for optimizing model behavior. Through theoretical analysis, we show that tuning the initial token's attention sharpens or flattens the attention distribution over subsequent tokens, and its role as an attention sink amplifies this effect. Empirically, we find that: (1) tuning its attention improves LLM performance more effectively than tuning other task-specific tokens; (2) the effect follows a consistent trend across layers, with earlier layers having greater impact, but varies across attention heads, with different heads showing distinct preferences in how they attend to this token. Based on these findings, we propose ZeroTuning, a training-free approach that improves LLM performance by applying head-specific attention adjustments to this special token. Despite tuning only one token, ZeroTuning achieves higher performance on text classification, multiple-choice, and multi-turn conversation tasks across models such as Llama, Qwen, and DeepSeek. For example, ZeroTuning improves Llama-3.1-8B by 11.71% on classification, 2.64% on QA tasks, and raises its multi-turn score from 7.804 to 7.966. The method is also robust to limited resources, few-shot settings, long contexts, quantization, decoding strategies, and prompt variations. Our work sheds light on a previously overlooked control point in LLMs, offering new insights into both inference-time tuning and model interpretability.", 'abstract_zh': '无训练方法提高大型语言模型性能的新颖途径：利用语义空初始标记进行零调谐', 'title_zh': 'ZeroTuning: 利用初始令牌增强大型语言模型而不进行训练'}
{'arxiv_id': 'arXiv:2505.11737', 'title': 'Token-Level Uncertainty Estimation for Large Language Model Reasoning', 'authors': 'Tunyu Zhang, Haizhou Shi, Yibin Wang, Hengyi Wang, Xiaoxiao He, Zhuowei Li, Haoxian Chen, Ligong Han, Kai Xu, Huan Zhang, Dimitris Metaxas, Hao Wang', 'link': 'https://arxiv.org/abs/2505.11737', 'abstract': "While Large Language Models (LLMs) have demonstrated impressive capabilities, their output quality remains inconsistent across various application scenarios, making it difficult to identify trustworthy responses, especially in complex tasks requiring multi-step reasoning. In this paper, we propose a token-level uncertainty estimation framework to enable LLMs to self-assess and self-improve their generation quality in mathematical reasoning. Specifically, we introduce low-rank random weight perturbation to LLM decoding, generating predictive distributions that we use to estimate token-level uncertainties. We then aggregate these uncertainties to reflect semantic uncertainty of the generated sequences. Experiments on mathematical reasoning datasets of varying difficulty demonstrate that our token-level uncertainty metrics strongly correlate with answer correctness and model robustness. Additionally, we explore using uncertainty to directly enhance the model's reasoning performance through multiple generations and the particle filtering algorithm. Our approach consistently outperforms existing uncertainty estimation methods, establishing effective uncertainty estimation as a valuable tool for both evaluating and improving reasoning generation in LLMs.", 'abstract_zh': 'larg语言模型在数学推理中的token级不确定性估计算法及其应用', 'title_zh': '大型语言模型推理中的token级不确定性估计'}
{'arxiv_id': 'arXiv:2505.11731', 'title': 'Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models', 'authors': 'Harshil Vejendla, Haizhou Shi, Yibin Wang, Tunyu Zhang, Huan Zhang, Hao Wang', 'link': 'https://arxiv.org/abs/2505.11731', 'abstract': 'Recent advances in uncertainty estimation for Large Language Models (LLMs) during downstream adaptation have addressed key challenges of reliability and simplicity. However, existing Bayesian methods typically require multiple sampling iterations during inference, creating significant efficiency issues that limit practical deployment. In this paper, we investigate the possibility of eliminating the need for test-time sampling for LLM uncertainty estimation. Specifically, when given an off-the-shelf Bayesian LLM, we distill its aligned confidence into a non-Bayesian student LLM by minimizing the divergence between their predictive distributions. Unlike typical calibration methods, our distillation is carried out solely on the training dataset without the need of an additional validation dataset. This simple yet effective approach achieves N-times more efficient uncertainty estimation during testing, where N is the number of samples traditionally required by Bayesian LLMs. Our extensive experiments demonstrate that uncertainty estimation capabilities on training data can successfully generalize to unseen test data through our distillation technique, consistently producing results comparable to (or even better than) state-of-the-art Bayesian LLMs.', 'abstract_zh': 'Recent advances in Large Language Models (LLMs)不确定性估计近期进展：在下游适应过程中，不确定性估计方法取得了关键的可靠性和简洁性方面的进步。然而，现有的贝叶斯方法通常需要在推理过程中进行多次采样迭代，这导致了显著的效率问题，限制了其实用部署。在本文中，我们探讨了消除测试时采样以估计LLM不确定性的可能性。具体而言，当我们使用现成的贝叶斯LLM时，通过最小化两者预测分布之间的偏离程度，将其对齐的置信度提炼到一个非贝叶斯的学生LLM中。与传统的校准方法不同，我们的提炼仅在训练数据集上进行，无需额外的验证数据集。这种简单而有效的方法在测试中实现比传统的贝叶斯LLM所需的采样次数N倍更高效的不确定性估计。我们的广泛实验表明，通过我们的提炼技术，训练数据上的不确定性估计能力可以成功泛化到未见过的测试数据上，始终能够产生与（或甚至优于）最先进的贝叶斯LLM相当的结果。标题：\n\nRecent advances in uncertainty estimation for Large Language Models (LLMs) during downstream adaptation: Eliminating the Need for Test-Time Sampling', 'title_zh': '通过 Bayesian 大语言模型蒸馏实现高效不确定性估计'}
{'arxiv_id': 'arXiv:2505.11665', 'title': 'Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks', 'authors': 'Shubham Vatsal, Harsh Dubey, Aditi Singh', 'link': 'https://arxiv.org/abs/2505.11665', 'abstract': "Large language models (LLMs) have demonstrated impressive performance across a wide range of Natural Language Processing (NLP) tasks. However, ensuring their effectiveness across multiple languages presents unique challenges. Multilingual prompt engineering has emerged as a key approach to enhance LLMs' capabilities in diverse linguistic settings without requiring extensive parameter re-training or fine-tuning. With growing interest in multilingual prompt engineering over the past two to three years, researchers have explored various strategies to improve LLMs' performance across languages and NLP tasks. By crafting structured natural language prompts, researchers have successfully extracted knowledge from LLMs across different languages, making these techniques an accessible pathway for a broader audience, including those without deep expertise in machine learning, to harness the capabilities of LLMs. In this paper, we survey and categorize different multilingual prompting techniques based on the NLP tasks they address across a diverse set of datasets that collectively span around 250 languages. We further highlight the LLMs employed, present a taxonomy of approaches and discuss potential state-of-the-art (SoTA) methods for specific multilingual datasets. Additionally, we derive a range of insights across language families and resource levels (high-resource vs. low-resource), including analyses such as the distribution of NLP tasks by language resource type and the frequency of prompting methods across different language families. Our survey reviews 36 research papers covering 39 prompting techniques applied to 30 multilingual NLP tasks, with the majority of these studies published in the last two years.", 'abstract_zh': '大规模语言模型在多种自然语言处理任务中展现了令人印象深刻的性能。然而，在多种语言环境中确保其有效性提出了独特的挑战。多语言提示工程已成为一种关键方法，无需进行大量的参数重新训练或微调，即可增强大规模语言模型在不同语言环境中的能力。近年来，随着对多语言提示工程兴趣的增长，研究人员探索了多种策略以提高大规模语言模型在跨语言和自然语言处理任务中的性能。通过精心设计结构化的自然语言提示，研究人员能够从大规模语言模型中提取不同语言的知识，使这些技术成为更广泛用户，包括那些没有深厚机器学习背景的人，利用大规模语言模型能力的途径。在本文中，我们基于覆盖约250种语言的多样数据集，调查和分类了针对不同自然语言处理任务的多语言提示技术。我们还强调了所使用的语言模型，介绍了方法的分类，并讨论了特定多语言数据集的最新前沿方法。此外，我们得出了一系列关于语言家族和资源水平（高资源 vs 低资源）的见解，包括按语言资源类型划分的自然语言处理任务分布和不同语言家族中提示方法的频率分析。我们的调查回顾了36篇研究论文，涵盖了39种应用于30种多语言自然语言处理任务的提示技术，其中大多数研究论文在最近两年内发表。', 'title_zh': '大规模语言模型中的多语言提示工程：跨NLP任务的综述'}
{'arxiv_id': 'arXiv:2505.11633', 'title': 'Chatting with Papers: A Hybrid Approach Using LLMs and Knowledge Graphs', 'authors': 'Vyacheslav Tykhonov, Han Yang, Philipp Mayr, Jetze Touber, Andrea Scharnhorst', 'link': 'https://arxiv.org/abs/2505.11633', 'abstract': "This demo paper reports on a new workflow \\textit{GhostWriter} that combines the use of Large Language Models and Knowledge Graphs (semantic artifacts) to support navigation through collections. Situated in the research area of Retrieval Augmented Generation, this specific workflow details the creation of local and adaptable chatbots. Based on the tool-suite \\textit{EverythingData} at the backend, \\textit{GhostWriter} provides an interface that enables querying and ``chatting'' with a collection. Applied iteratively, the workflow supports the information needs of researchers when interacting with a collection of papers, whether it be to gain an overview, to learn more about a specific concept and its context, and helps the researcher ultimately to refine their research question in a controlled way. We demonstrate the workflow for a collection of articles from the \\textit{method data analysis} journal published by GESIS -- Leibniz-Institute for the Social Sciences. We also point to further application areas.", 'abstract_zh': 'This demo paper报告了一种新的工作流 GhostWriter，该工作流结合了大型语言模型和知识图谱（语义实体）以支持集合导航。该工作流位于检索增强生成的研究领域，详细描述了本地和可适应聊天机器人的创建。基于后台工具集 EverythingData，GhostWriter 提供了一个接口，使用户能够查询和“聊天”与集合的交互。通过迭代应用，该工作流支持研究人员在与论文集合交互时的信息需求，无论是为了获得概览、更多地了解特定概念及其背景，或者帮助研究人员以受控的方式最终精炼其研究问题。我们以德国社会科学院（GESIS）发布的《方法数据分析》期刊文章集合为例演示了该工作流，并指出了其进一步的应用领域。', 'title_zh': '与论文聊天：结合LLMs和知识图谱的混合方法'}
{'arxiv_id': 'arXiv:2505.11615', 'title': 'Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations', 'authors': 'Jian-Qiao Zhu, Haijiang Yan, Thomas L. Griffiths', 'link': 'https://arxiv.org/abs/2505.11615', 'abstract': 'Changing the behavior of large language models (LLMs) can be as straightforward as editing the Transformer\'s residual streams using appropriately constructed "steering vectors." These modifications to internal neural activations, a form of representation engineering, offer an effective and targeted means of influencing model behavior without retraining or fine-tuning the model. But how can such steering vectors be systematically identified? We propose a principled approach for uncovering steering vectors by aligning latent representations elicited through behavioral methods (specifically, Markov chain Monte Carlo with LLMs) with their neural counterparts. To evaluate this approach, we focus on extracting latent risk preferences from LLMs and steering their risk-related outputs using the aligned representations as steering vectors. We show that the resulting steering vectors successfully and reliably modulate LLM outputs in line with the targeted behavior.', 'abstract_zh': '通过编辑Transformer的残差流使用适当构造的“导向向量”可以改变大型语言模型（LLMs）的行为。这些对内部神经激活的修改，作为一种表示工程形式，提供了一种有效且针对性的方法来影响模型行为而无需重新训练或微调模型。但如何系统地识别这些导向向量？我们提出了一种原理性的方法，通过对行为方法（特别是通过LLMs的马尔可夫链蒙特卡洛方法）引发的潜在表示与其神经对应的表示进行对齐来发现导向向量。为了评估此方法，我们将重点提取LLMs中的潜在风险偏好，并使用对齐的表示作为导向向量引导其风险相关输出。我们展示了所得到的导向向量能够成功且可靠地按目标行为调节LLMs的输出。', 'title_zh': '通过行为和神经表示对齐引导大型语言模型的风险偏好'}
{'arxiv_id': 'arXiv:2505.11595', 'title': 'Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO', 'authors': 'Peter Chen, Xiaopeng Li, Ziniu Li, Xi Chen, Tianyi Lin', 'link': 'https://arxiv.org/abs/2505.11595', 'abstract': 'Reinforcement learning (RL) has demonstrated significant success in enhancing reasoning capabilities in large language models (LLMs). One of the most widely used RL methods is Group Relative Policy Optimization (GRPO)~\\cite{Shao-2024-Deepseekmath}, known for its memory efficiency and success in training DeepSeek-R1~\\cite{Guo-2025-Deepseek}. However, GRPO stalls when all sampled responses in a group are incorrect -- referred to as an \\emph{all-negative-sample} group -- as it fails to update the policy, hindering learning progress. The contributions of this paper are two-fold. First, we propose a simple yet effective framework that introduces response diversity within all-negative-sample groups in GRPO using AI feedback. We also provide a theoretical analysis, via a stylized model, showing how this diversification improves learning dynamics. Second, we empirically validate our approach, showing the improved performance across various model sizes (7B, 14B, 32B) in both offline and online learning settings with 10 benchmarks, including base and distilled variants. Our findings highlight that learning from all-negative-sample groups is not only feasible but beneficial, advancing recent insights from \\citet{Xiong-2025-Minimalist}.', 'abstract_zh': 'reinforcement学习（RL）在增强大规模语言模型（LLMs）的推理能力方面取得了显著成功。Group Relative Policy Optimization（GRPO）是最广泛使用的方法之一，因其内存效率和训练DeepSeek-R1的成功而闻名。然而，当所有采样的响应在一组中都是错误的——称为“全负样本组”——GRPO会停滞不前，因为它无法更新策略，阻碍了学习进程。本文的主要贡献有两个方面。首先，我们提出了一种简单而有效的方法，该方法利用AI反馈在GRPO的“全负样本组”中引入响应多样性。我们还通过简化模型提供了理论分析，说明这种多样化如何提高学习动态。其次，我们在离线和在线学习设置中，通过10个基准测试（包括基本和精简变体），证明了这种方法在不同模型大小（7B、14B、32B）下的改进性能。我们的研究结果表明，从“全负样本组”中学习不仅是可行的，而且是有益的，这与最近的研究见解《Xiong-2025-Minimalist》相一致。', 'title_zh': '光谱策略优化：在GRPO中着色你的错误推理'}
{'arxiv_id': 'arXiv:2505.11586', 'title': 'The Ripple Effect: On Unforeseen Complications of Backdoor Attacks', 'authors': 'Rui Zhang, Yun Shen, Hongwei Li, Wenbo Jiang, Hanxiao Chen, Yuan Zhang, Guowen Xu, Yang Zhang', 'link': 'https://arxiv.org/abs/2505.11586', 'abstract': 'Recent research highlights concerns about the trustworthiness of third-party Pre-Trained Language Models (PTLMs) due to potential backdoor attacks. These backdoored PTLMs, however, are effective only for specific pre-defined downstream tasks. In reality, these PTLMs can be adapted to many other unrelated downstream tasks. Such adaptation may lead to unforeseen consequences in downstream model outputs, consequently raising user suspicion and compromising attack stealthiness. We refer to this phenomenon as backdoor complications. In this paper, we undertake the first comprehensive quantification of backdoor complications. Through extensive experiments using 4 prominent PTLMs and 16 text classification benchmark datasets, we demonstrate the widespread presence of backdoor complications in downstream models fine-tuned from backdoored PTLMs. The output distribution of triggered samples significantly deviates from that of clean samples. Consequently, we propose a backdoor complication reduction method leveraging multi-task learning to mitigate complications without prior knowledge of downstream tasks. The experimental results demonstrate that our proposed method can effectively reduce complications while maintaining the efficacy and consistency of backdoor attacks. Our code is available at this https URL.', 'abstract_zh': '近年来的研究强调了第三方预训练语言模型(PTLMs)的信任问题，因为可能存在后门攻击。然而，这些植入后门的PTLMs只对特定的预定义下游任务有效。实际上，这些PTLMs可以适应许多其他无关的下游任务。这种适应可能导致下游模型输出的不可预见后果，从而引起用户怀疑并损害攻击的隐蔽性。我们将这一现象称为后门复杂性。在本文中，我们首次对后门复杂性进行了全面量化。通过使用4个主流的PTLMs和16个文本分类基准数据集进行广泛的实验，我们展示了从植入后门的PTLMs微调的下游模型中普遍存在后门复杂性。触发样本的输出分布与干净样本的输出分布显著不同。因此，我们提出了一种利用多任务学习的方法来减少后门复杂性，而无需了解下游任务的先验知识。实验结果表明，我们提出的方法可以在保持后门攻击有效性及一致性的前提下有效减少复杂性。我们的代码可在以下链接获取：this https URL。', 'title_zh': '涟漪效应：后门攻击的未预见并发症'}
{'arxiv_id': 'arXiv:2505.11574', 'title': 'InfiJanice: Joint Analysis and In-situ Correction Engine for Quantization-Induced Math Degradation in Large Language Models', 'authors': 'Zhen Li, Yupeng Su, Songmiao Wang, Runming Yang, Congkai Xie, Aofan Liu, Ming Li, Jiannong Cao, Yuan Xie, Ngai Wong, Hongxia Yang', 'link': 'https://arxiv.org/abs/2505.11574', 'abstract': 'Large Language Models (LLMs) have demonstrated impressive performance on complex reasoning benchmarks such as GSM8K, MATH, and AIME. However, the substantial computational demands of these tasks pose significant challenges for real-world deployment. Model quantization has emerged as a promising approach to reduce memory footprint and inference latency by representing weights and activations with lower bit-widths. In this work, we conduct a comprehensive study of mainstream quantization methods(e.g., AWQ, GPTQ, SmoothQuant) on the most popular open-sourced models (e.g., Qwen2.5, LLaMA3 series), and reveal that quantization can degrade mathematical reasoning accuracy by up to 69.81%. To better understand this degradation, we develop an automated assignment and judgment pipeline that qualitatively categorizes failures into four error types and quantitatively identifies the most impacted reasoning capabilities. Building on these findings, we employ an automated data-curation pipeline to construct a compact "Silver Bullet" datasets. Training a quantized model on as few as 332 carefully selected examples for just 3-5 minutes on a single GPU is enough to restore its reasoning accuracy to match that of the full-precision baseline.', 'abstract_zh': '大规模语言模型（LLMs）在GSM8K、MATH和AIME等复杂推理基准测试中展现了令人印象深刻的性能。然而，这些任务的显著计算需求给实际部署带来了重大挑战。模型量化已成为一种有前途的方法，通过使用较低位宽表示权重和激活，减少内存占用和推理延迟。在本文中，我们对主流量化方法（例如AWQ、GPTQ、SmoothQuant）在最受欢迎的开源模型（例如Qwen2.5、LLaMA3系列）上的性能进行了全面研究，并发现量化可能导致数学推理准确性下降高达69.81%。为了更好地理解这种下降，我们开发了一种自动化分配和判断管道，通过定性分类错误类型和定量识别受影响最大的推理能力来深入理解。基于这些发现，我们使用自动化数据整理管道构建了一个紧凑的“银弹”数据集。仅使用332个精心选择的示例在单个GPU上训练3-5分钟即可使量化模型的推理准确性恢复到全精度基线的水平。', 'title_zh': 'InfiJanice：针对大型语言模型中量化引起数学退化的同时分析和原位校正引擎'}
{'arxiv_id': 'arXiv:2505.11570', 'title': 'Tool-Aided Evolutionary LLM for Generative Policy Toward Efficient Resource Management in Wireless Federated Learning', 'authors': 'Chongyang Tan, Ruoqi Wen, Rongpeng Li, Zhifeng Zhao, Ekram Hossain, Honggang Zhang', 'link': 'https://arxiv.org/abs/2505.11570', 'abstract': 'Federated Learning (FL) enables distributed model training across edge devices in a privacy-friendly manner. However, its efficiency heavily depends on effective device selection and high-dimensional resource allocation in dynamic and heterogeneous wireless environments. Conventional methods demand a confluence of domain-specific expertise, extensive hyperparameter tuning, and/or heavy interaction cost. This paper proposes a Tool-aided Evolutionary Large Language Model (T-ELLM) framework to generate a qualified policy for device selection in a wireless FL environment. Unlike conventional optimization methods, T-ELLM leverages natural language-based scenario prompts to enhance generalization across varying network conditions. The framework decouples the joint optimization problem mathematically, enabling tractable learning of device selection policies while delegating resource allocation to convex optimization tools. To improve adaptability, T-ELLM integrates a sample-efficient, model-based virtual learning environment that captures the relationship between device selection and learning performance, facilitating subsequent group relative policy optimization. This concerted approach reduces reliance on real-world interactions, minimizing communication overhead while maintaining high-fidelity decision-making. Theoretical analysis proves that the discrepancy between virtual and real environments is bounded, ensuring the advantage function learned in the virtual environment maintains a provably small deviation from real-world conditions. Experimental results demonstrate that T-ELLM outperforms benchmark methods in energy efficiency and exhibits robust adaptability to environmental changes.', 'abstract_zh': '联邦学习（FL）以隐私友好的方式在边缘设备上进行分布式模型训练。然而，其效率高度依赖于在动态和异构无线环境中有效的设备选择和高维资源分配。传统方法需要领域特定的专业知识、广泛的超参数调整和/或高昂的交互成本。本文提出了一种工具辅助进化大规模语言模型（T-ELLM）框架，用于生成适合无线FL环境中的设备选择策略。与传统优化方法不同，T-ELLM利用基于自然语言的场景提示来增强在不同网络条件下的一般化能力。该框架在数学上分离了联合优化问题，使设备选择策略的学习变得可行，同时将资源分配委托给凸优化工具。为了提高适应性，T-ELLM集成了一个高效采样的基于模型的虚拟学习环境，捕捉设备选择与学习性能之间的关系，促进后续群体相对策略优化。这种综合性方法减少了对真实世界交互的依赖，同时最小化通信开销并保持高水平的决策准确性。理论分析证明，虚拟环境与现实环境之间的差异是可以限制的，确保在虚拟环境中学习到的优势函数与现实世界条件之间的偏差是有保证的小。实验结果表明，T-ELLM在能效方面优于基准方法，并且在环境变化中表现出高度的鲁棒适应性。', 'title_zh': '工具辅助进化大语言模型用于生成性策略以实现无线联邦学习中高效资源管理'}
{'arxiv_id': 'arXiv:2505.11565', 'title': 'ACSE-Eval: Can LLMs threat model real-world cloud infrastructure?', 'authors': 'Sarthak Munshi, Swapnil Pathak, Sonam Ghatode, Thenuga Priyadarshini, Dhivya Chandramouleeswaran, Ashutosh Rana', 'link': 'https://arxiv.org/abs/2505.11565', 'abstract': 'While Large Language Models have shown promise in cybersecurity applications, their effectiveness in identifying security threats within cloud deployments remains unexplored. This paper introduces AWS Cloud Security Engineering Eval, a novel dataset for evaluating LLMs cloud security threat modeling capabilities. ACSE-Eval contains 100 production grade AWS deployment scenarios, each featuring detailed architectural specifications, Infrastructure as Code implementations, documented security vulnerabilities, and associated threat modeling parameters. Our dataset enables systemic assessment of LLMs abilities to identify security risks, analyze attack vectors, and propose mitigation strategies in cloud environments. Our evaluations on ACSE-Eval demonstrate that GPT 4.1 and Gemini 2.5 Pro excel at threat identification, with Gemini 2.5 Pro performing optimally in 0-shot scenarios and GPT 4.1 showing superior results in few-shot settings. While GPT 4.1 maintains a slight overall performance advantage, Claude 3.7 Sonnet generates the most semantically sophisticated threat models but struggles with threat categorization and generalization. To promote reproducibility and advance research in automated cybersecurity threat analysis, we open-source our dataset, evaluation metrics, and methodologies.', 'abstract_zh': '虽然大型语言模型在 cybersecurity 应用方面显示出潜力，但在识别云部署中的安全威胁方面的效果尚未被探索。本文介绍了一个新的数据集 AWS 云安全工程评估（AWS Cloud Security Engineering Eval, ACSE-Eval），用于评估 LLMs 的云安全威胁建模能力。ACSE-Eval 包含 100 个生产级别的 AWS 部署场景，每个场景都包含详细的体系架构规范、基础设施即代码实现、记录的安全漏洞以及相关的威胁建模参数。我们的数据集使得系统性评估 LLMs 识别 security 风险、分析攻击向量及提出缓解策略的能力成为可能。我们对 ACSE-Eval 的评估表明，GPT 4.1 和 Gemini 2.5 Pro 在威胁识别方面表现突出，Gemini 2.5 Pro 在零样本场景中表现最优，而 GPT 4.1 在少量样本设置中表现更优。尽管 GPT 4.1 总体性能稍占优势，但 Claude 3.7 Sonnet 生成的威胁模型在语义复杂性方面表现最佳，但在威胁分类和泛化方面存在困难。为了促进可重复性和推进自动网络安全威胁分析的研究，我们开源了我们的数据集、评估指标和方法论。', 'title_zh': 'ACSE-Eval: LLMs能否对现实世界的云基础设施进行威胁建模？'}
{'arxiv_id': 'arXiv:2505.11557', 'title': 'AC-LoRA: (Almost) Training-Free Access Control-Aware Multi-Modal LLMs', 'authors': 'Lara Magdalena Lazier, Aritra Dhar, Vasilije Stambolic, Lukas Cavigelli', 'link': 'https://arxiv.org/abs/2505.11557', 'abstract': 'Corporate LLMs are gaining traction for efficient knowledge dissemination and management within organizations. However, as current LLMs are vulnerable to leaking sensitive information, it has proven difficult to apply them in settings where strict access control is necessary. To this end, we design AC-LoRA, an end-to-end system for access control-aware corporate LLM chatbots that maintains a strong information isolation guarantee. AC-LoRA maintains separate LoRA adapters for permissioned datasets, along with the document embedding they are finetuned on. AC-LoRA retrieves a precise set of LoRA adapters based on the similarity score with the user query and their permission. This similarity score is later used to merge the responses if more than one LoRA is retrieved, without requiring any additional training for LoRA routing. We provide an end-to-end prototype of AC-LoRA, evaluate it on two datasets, and show that AC-LoRA matches or even exceeds the performance of state-of-the-art LoRA mixing techniques while providing strong isolation guarantees. Furthermore, we show that AC-LoRA design can be directly applied to different modalities.', 'abstract_zh': '面向访问控制的企業端LoRA聊天机器人系统：AC-LoRA', 'title_zh': 'AC-LoRA: (几乎)无需训练的访问控制意识多模态LLM'}
{'arxiv_id': 'arXiv:2505.11556', 'title': 'Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks', 'authors': 'Yuxuan Li, Aoi Naito, Hirokazu Shirado', 'link': 'https://arxiv.org/abs/2505.11556', 'abstract': "Multi-agent systems built on large language models (LLMs) promise enhanced problem-solving through distributed information integration, but also risk replicating collective reasoning failures observed in human groups. Yet, no theory-grounded benchmark exists to systematically evaluate such failures. In this paper, we introduce the Hidden Profile paradigm from social psychology as a diagnostic testbed for multi-agent LLM systems. By distributing critical information asymmetrically across agents, the paradigm reveals how inter-agent dynamics support or hinder collective reasoning. We first formalize the paradigm for multi-agent decision-making under distributed knowledge and instantiate it as a benchmark with nine tasks spanning diverse scenarios, including adaptations from prior human studies. We then conduct experiments with GPT-4.1 and five other leading LLMs, including reasoning-enhanced variants, showing that multi-agent systems across all models fail to match the accuracy of single agents given complete information. While agents' collective performance is broadly comparable to that of human groups, nuanced behavioral differences emerge, such as increased sensitivity to social desirability. Finally, we demonstrate the paradigm's diagnostic utility by exploring a cooperation-contradiction trade-off in multi-agent LLM systems. We find that while cooperative agents are prone to over-coordination in collective settings, increased contradiction impairs group convergence. This work contributes a reproducible framework for evaluating multi-agent LLM systems and motivates future research on artificial collective intelligence and human-AI interaction.", 'abstract_zh': '基于大型语言模型的多Agent系统通过分布式信息整合提升了问题解决能力，但也存在复制人类群体中观察到的集体推理失败的风险。然而，尚不存在基于理论的基准来系统性评估这些失败。本文引入社会心理学中的隐秘特征 paradigm 作为多Agent大型语言模型系统的诊断测试床。通过不对称地分布在不同Agent之间的关键信息，该paradigm揭示了Agent间动态如何支持或阻碍集体推理。我们首先为分布式知识下的多Agent决策制定形式化的paradigm，并以涵盖多种场景的九个任务实例化为基准，包括从先前的人类研究中进行改编。然后，我们使用GPT-4.1和五种其他领先的大型语言模型进行了实验，包括增强推理能力的变体，结果显示所有模型下的多Agent系统在完整信息条件下都无法达到单个代理的准确性。尽管Agent的集体表现与人类群体表现相当，但在行为上出现了一些细微的差异，如对社会认可性的更高敏感性。最后，我们通过探索多Agent大型语言模型系统中的合作-矛盾权衡，展示了该paradigm的诊断效用。我们发现，在集体环境中，合作性Agent容易出现过度协调，而增大的矛盾阻碍了群体的共识形成。本文贡献了一个可重复的框架以评估多Agent大型语言模型系统，并启发未来关于人工集体智能和人机交互的研究。', 'title_zh': '基于隐藏配置任务评估多智能体大语言模型的集体推理能力'}
{'arxiv_id': 'arXiv:2505.11550', 'title': 'AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification', 'authors': 'Harika Abburi, Sanmitra Bhattacharya, Edward Bowen, Nirmala Pudota', 'link': 'https://arxiv.org/abs/2505.11550', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities in generating text that closely resembles human writing across a wide range of styles and genres. However, such capabilities are prone to potential misuse, such as fake news generation, spam email creation, and misuse in academic assignments. As a result, accurate detection of AI-generated text and identification of the model that generated it are crucial for maintaining the responsible use of LLMs. In this work, we addressed two sub-tasks put forward by the Defactify workshop under AI-Generated Text Detection shared task at the Association for the Advancement of Artificial Intelligence (AAAI 2025): Task A involved distinguishing between human-authored or AI-generated text, while Task B focused on attributing text to its originating language model. For each task, we proposed two neural architectures: an optimized model and a simpler variant. For Task A, the optimized neural architecture achieved fifth place with $F1$ score of 0.994, and for Task B, the simpler neural architecture also ranked fifth place with $F1$ score of 0.627.', 'abstract_zh': '大规模语言模型（LLMs）在生成风格和体裁各异的人类写作相似文本方面表现出了非凡的能力。然而，这些能力容易被不当使用，如假新闻生成、垃圾邮件创建以及学术作业中的滥用。因此，准确检测人工智能生成的文本并识别生成它的模型对于负责任地使用LLMs至关重要。在2025年美国人工智能协会（AAAI）AI生成文本检测共享任务（Defactify研讨会）下，我们针对两个子任务进行了研究：任务A涉及区分人类作者或AI生成的文本，而任务B则专注于将文本归属性于其起源的语言模型。对于每个任务，我们提出了两种神经架构：优化模型和简单变体。在任务A中，优化的神经架构取得了第五名，F1得分为0.994；在任务B中，简单神经架构也取得了第五名，F1得分为0.627。', 'title_zh': 'AI生成文本检测：二分类和多分类的一种综合性方法'}
{'arxiv_id': 'arXiv:2505.11548', 'title': 'One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented Generation Systems', 'authors': 'Zhiyuan Chang, Xiaojun Jia, Mingyang Li, Junjie Wang, Yuekai Huang, Qing Wang, Ziyou Jiang, Yang Liu', 'link': 'https://arxiv.org/abs/2505.11548', 'abstract': "Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation (RAG) have shown improved performance in generating accurate responses. However, the dependence on external knowledge bases introduces potential security vulnerabilities, particularly when these knowledge bases are publicly accessible and modifiable. Poisoning attacks on knowledge bases for RAG systems face two fundamental challenges: the injected malicious content must compete with multiple authentic documents retrieved by the retriever, and LLMs tend to trust retrieved information that aligns with their internal memorized knowledge. Previous works attempt to address these challenges by injecting multiple malicious documents, but such saturation attacks are easily detectable and impractical in real-world scenarios. To enable the effective single document poisoning attack, we propose AuthChain, a novel knowledge poisoning attack method that leverages Chain-of-Evidence theory and authority effect to craft more convincing poisoned documents. AuthChain generates poisoned content that establishes strong evidence chains and incorporates authoritative statements, effectively overcoming the interference from both authentic documents and LLMs' internal knowledge. Extensive experiments across six popular LLMs demonstrate that AuthChain achieves significantly higher attack success rates while maintaining superior stealthiness against RAG defense mechanisms compared to state-of-the-art baselines.", 'abstract_zh': '增强检索增强生成（RAG）的大语言模型（LLMs）在生成准确响应方面表现出改进的性能。然而，对外部知识库的依赖引入了潜在的安全漏洞，特别是当这些知识库公开可访问和可修改时。针对RAG系统的知识库中毒攻击面临着两个根本性的挑战：注入的恶意内容必须与检索器检索到的多个真实文档进行竞争，并且LLMs倾向于信任与其内部记忆知识一致的检索信息。以往的工作试图通过注入多个恶意文档来应对这些挑战，但这种饱和攻击在实际场景中容易被检测并且不切实际。为了实现有效的单文档中毒攻击，我们提出了一种名为AuthChain的新颖知识中毒攻击方法，该方法利用证据链理论和权威效果来制作更具说服力的中毒文档。AuthChain生成的中毒内容建立了强大的证据链，并结合权威声明，有效地克服了真实文档和LLMs内部知识的干扰。跨六种流行的LLMs的广泛实验显示，AuthChain在攻击成功率和对抗RAG防御机制的隐蔽性方面均显著优于最先进的基线方法。', 'title_zh': '一次性主导权：检索增强生成系统中的知识投毒攻击'}
{'arxiv_id': 'arXiv:2505.11547', 'title': 'On Technique Identification and Threat-Actor Attribution using LLMs and Embedding Models', 'authors': 'Kyla Guru, Robert J. Moss, Mykel J. Kochenderfer', 'link': 'https://arxiv.org/abs/2505.11547', 'abstract': "Attribution of cyber-attacks remains a complex but critical challenge for cyber defenders. Currently, manual extraction of behavioral indicators from dense forensic documentation causes significant attribution delays, especially following major incidents at the international scale. This research evaluates large language models (LLMs) for cyber-attack attribution based on behavioral indicators extracted from forensic documentation. We test OpenAI's GPT-4 and text-embedding-3-large for identifying threat actors' tactics, techniques, and procedures (TTPs) by comparing LLM-generated TTPs against human-generated data from MITRE ATT&CK Groups. Our framework then identifies TTPs from text using vector embedding search and builds profiles to attribute new attacks for a machine learning model to learn. Key contributions include: (1) assessing off-the-shelf LLMs for TTP extraction and attribution, and (2) developing an end-to-end pipeline from raw CTI documents to threat-actor prediction. This research finds that standard LLMs generate TTP datasets with noise, resulting in a low similarity to human-generated datasets. However, the TTPs generated are similar in frequency to those within the existing MITRE datasets. Additionally, although these TTPs are different than human-generated datasets, our work demonstrates that they still prove useful for training a model that performs above baseline on attribution. Project code and files are contained here: this https URL.", 'abstract_zh': '基于行为指标的大语言模型在 cyber-攻击归因中的评估：从国际重大事件后的法医文档到威胁行为者预测的端到端管道', 'title_zh': '使用大型语言模型和嵌入模型进行技术识别与威胁行为者归属'}
