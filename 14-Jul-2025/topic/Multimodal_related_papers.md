# Large Multi-modal Model Cartographic Map Comprehension for Textual Locality Georeferencing 

**Title (ZH)**: 大型多模态模型地图理解在文本局部地理配准中的应用 

**Authors**: Kalana Wijegunarathna, Kristin Stock, Christopher B. Jones  

**Link**: [PDF](https://arxiv.org/pdf/2507.08575)  

**Abstract**: Millions of biological sample records collected in the last few centuries archived in natural history collections are un-georeferenced. Georeferencing complex locality descriptions associated with these collection samples is a highly labour-intensive task collection agencies struggle with. None of the existing automated methods exploit maps that are an essential tool for georeferencing complex relations. We present preliminary experiments and results of a novel method that exploits multi-modal capabilities of recent Large Multi-Modal Models (LMM). This method enables the model to visually contextualize spatial relations it reads in the locality description. We use a grid-based approach to adapt these auto-regressive models for this task in a zero-shot setting. Our experiments conducted on a small manually annotated dataset show impressive results for our approach ($\sim$1 km Average distance error) compared to uni-modal georeferencing with Large Language Models and existing georeferencing tools. The paper also discusses the findings of the experiments in light of an LMM's ability to comprehend fine-grained maps. Motivated by these results, a practical framework is proposed to integrate this method into a georeferencing workflow. 

**Abstract (ZH)**: 近年来收集在自然历史收藏中的数百万份生物样本记录缺乏地理坐标。对这些收集样本关联的复杂地理位置描述进行地理参考化是一个劳动密集型的任务，收集机构面临挑战。现有的自动化方法并未利用地图这一地理参考化复杂关系的重要工具。我们介绍了利用近期多模态大型模型（LMM）多模态能力的一种新颖方法的初步实验和结果。该方法使模型能够通过视觉上下文化它在地理位置描述中读取的空间关系。我们采用基于网格的方法，在零样本设置下将这些自回归模型适应于这一任务。我们在一个小的手动标注数据集上进行的实验结果表明，与单一模态地理参考化结合大型语言模型和现有地理参考化工具相比，我们的方法取得了令人 impressive 的结果（平均距离误差约为1公里）。论文还探讨了实验结果在考虑LMM理解细粒度地图能力方面的发现。受到这些结果的启发，我们提出了一个实用框架，将此方法整合到地理参考化工作流程中。 

---
# Quantum Federated Learning for Multimodal Data: A Modality-Agnostic Approach 

**Title (ZH)**: 量子联邦学习在多模态数据中的应用：一种模态无关方法 

**Authors**: Atit Pokharel, Ratun Rahman, Thomas Morris, Dinh C. Nguyen  

**Link**: [PDF](https://arxiv.org/pdf/2507.08217)  

**Abstract**: Quantum federated learning (QFL) has been recently introduced to enable a distributed privacy-preserving quantum machine learning (QML) model training across quantum processors (clients). Despite recent research efforts, existing QFL frameworks predominantly focus on unimodal systems, limiting their applicability to real-world tasks that often naturally involve multiple modalities. To fill this significant gap, we present for the first time a novel multimodal approach specifically tailored for the QFL setting with the intermediate fusion using quantum entanglement. Furthermore, to address a major bottleneck in multimodal QFL, where the absence of certain modalities during training can degrade model performance, we introduce a Missing Modality Agnostic (MMA) mechanism that isolates untrained quantum circuits, ensuring stable training without corrupted states. Simulation results demonstrate that the proposed multimodal QFL method with MMA yields an improvement in accuracy of 6.84% in independent and identically distributed (IID) and 7.25% in non-IID data distributions compared to the state-of-the-art methods. 

**Abstract (ZH)**: 多模态量子联邦学习（QMFL）方法及其在量子处理器上的应用：基于量子纠缠的中间融合和缺失模态无关（MMA）机制 

---
# A Multi-Modal Fusion Framework for Brain Tumor Segmentation Based on 3D Spatial-Language-Vision Integration and Bidirectional Interactive Attention Mechanism 

**Title (ZH)**: 基于3D空间-语言-视觉集成和双向交互注意力机制的多模态融合框架用于脑肿瘤分割 

**Authors**: Mingda Zhang, Kaiwen Pan  

**Link**: [PDF](https://arxiv.org/pdf/2507.08574)  

**Abstract**: This study aims to develop a novel multi-modal fusion framework for brain tumor segmentation that integrates spatial-language-vision information through bidirectional interactive attention mechanisms to improve segmentation accuracy and boundary delineation. Methods: We propose two core components: Multi-modal Semantic Fusion Adapter (MSFA) integrating 3D MRI data with clinical text descriptions through hierarchical semantic decoupling, and Bidirectional Interactive Visual-semantic Attention (BIVA) enabling iterative information exchange between modalities. The framework was evaluated on BraTS 2020 dataset comprising 369 multi-institutional MRI scans. Results: The proposed method achieved average Dice coefficient of 0.8505 and 95% Hausdorff distance of 2.8256mm across enhancing tumor, tumor core, and whole tumor regions, outperforming state-of-the-art methods including SCAU-Net, CA-Net, and 3D U-Net. Ablation studies confirmed critical contributions of semantic and spatial modules to boundary precision. Conclusion: Multi-modal semantic fusion combined with bidirectional interactive attention significantly enhances brain tumor segmentation performance, establishing new paradigms for integrating clinical knowledge into medical image analysis. 

**Abstract (ZH)**: 本研究旨在通过双向交互注意力机制整合空间-语言-视觉信息，开发一种新颖的多模态融合框架以提高脑肿瘤分割精度和边界 delineation。方法：我们提出两个核心组件：多模态语义融合适配器（MSFA），实现3D MRI数据与临床文本描述的层次语义解耦融合，以及双向交互视觉-语义注意力（BIVA），以在模态间实现迭代信息交流。该框架在包含369个多机构MRI扫描的BraTS 2020数据集上进行了评估。结果：所提出的方法在增强肿瘤、肿瘤核心和整个肿瘤区域中获得了平均骰系数0.8505和95% Hausdorff距离2.8256mm，优于SCAU-Net、CA-Net和3D U-Net等最先进的方法。消融研究证实了语义和空间模块对边界精度的贡献至关重要。结论：结合多模态语义融合与双向交互注意力显著提升了脑肿瘤分割性能，为将临床知识整合到医学图像分析中建立了新的范式。 

---
# VideoConviction: A Multimodal Benchmark for Human Conviction and Stock Market Recommendations 

**Title (ZH)**: 视频确信：一种多模态基准，用于人类确信度与股市推荐 

**Authors**: Michael Galarnyk, Veer Kejriwal, Agam Shah, Yash Bhardwaj, Nicholas Meyer, Anand Krishnan, Sudheer Chava  

**Link**: [PDF](https://arxiv.org/pdf/2507.08104)  

**Abstract**: Social media has amplified the reach of financial influencers known as "finfluencers," who share stock recommendations on platforms like YouTube. Understanding their influence requires analyzing multimodal signals like tone, delivery style, and facial expressions, which extend beyond text-based financial analysis. We introduce VideoConviction, a multimodal dataset with 6,000+ expert annotations, produced through 457 hours of human effort, to benchmark multimodal large language models (MLLMs) and text-based large language models (LLMs) in financial discourse. Our results show that while multimodal inputs improve stock ticker extraction (e.g., extracting Apple's ticker AAPL), both MLLMs and LLMs struggle to distinguish investment actions and conviction--the strength of belief conveyed through confident delivery and detailed reasoning--often misclassifying general commentary as definitive recommendations. While high-conviction recommendations perform better than low-conviction ones, they still underperform the popular S\&P 500 index fund. An inverse strategy--betting against finfluencer recommendations--outperforms the S\&P 500 by 6.8\% in annual returns but carries greater risk (Sharpe ratio of 0.41 vs. 0.65). Our benchmark enables a diverse evaluation of multimodal tasks, comparing model performance on both full video and segmented video inputs. This enables deeper advancements in multimodal financial research. Our code, dataset, and evaluation leaderboard are available under the CC BY-NC 4.0 license. 

**Abstract (ZH)**: 社会媒体放大了所谓的“金fluencer”——在YouTube等平台上分享股票建议的金融影响者的影响。了解他们的影响需要分析语气、表达风格、面部表情等多模态信号，而这些信号超出了基于文本的金融分析。我们引入了VideoConviction多模态数据集，该数据集包含6000多个专家注解，通过457小时的人工努力生成，用于评估多模态大型语言模型（MLLMs）和文本大型语言模型（LLMs）在金融话语中的表现。我们的结果表明，虽然多模态输入提高了股票代码抽取（如提取苹果公司的代码AAPL）的效果，但无论是MLLMs还是LLMs，在区分投资行为和信心（通过自信的表达和详细的推理传达的信念强度）方面仍存在困难，经常错误地将一般评论分类为明确的建议。尽管高信心的建议表现优于低信心的建议，但它们在年度回报率上仍然落后于流行的标普500指数基金。采用与金fluencer建议相反的策略——做空金fluencer建议——在年度回报率上比标普500指数高出6.8%，但风险更大（夏普比率分别为0.41和0.65）。我们的基准测试使多模态任务的多样性评估成为可能，比较模型在完整视频和分段视频输入上的性能。这促进了多模态金融研究的更深发展。我们的代码、数据集和评估排行榜在CC BY-NC 4.0许可证下提供。 

---
