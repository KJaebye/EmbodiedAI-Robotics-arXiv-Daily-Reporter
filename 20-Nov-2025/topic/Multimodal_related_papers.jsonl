{'arxiv_id': 'arXiv:2511.15351', 'title': 'Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration', 'authors': 'Yifu Guo, Zishan Xu, Zhiyuan Yao, Yuquan Lu, Jiaye Lin, Sen Hu, Zhenheng Tang, Yingchao Li, Huacan Wang, Ronghao Chen', 'link': 'https://arxiv.org/abs/2511.15351', 'abstract': 'Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.', 'abstract_zh': '基于六能力 orchestrating 的自主多模态推理：Octopus 新范式', 'title_zh': '八爪鱼：六能力 orchestration 驱动的自主多模态推理'}
{'arxiv_id': 'arXiv:2511.15703', 'title': 'Think Visually, Reason Textually: Vision-Language Synergy in ARC', 'authors': 'Beichen Zhang, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, Jiaqi Wang', 'link': 'https://arxiv.org/abs/2511.15703', 'abstract': 'Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.', 'abstract_zh': '抽象推理从最小示例出发仍然是前沿基础模型如GPT-5和Grok 4的核心未解问题。这些模型仍然无法从少量示例中推断结构化转换规则，这是人类智能的关键特征之一。人工通用智能抽象推理 corpus (ARC-AGI) 为这一能力提供了严格的测试平台，要求概念规则归纳并在新任务中进行迁移。现有大多数方法将 ARC-AGI 视作纯粹的文本推理任务，忽视了人类在解决这类谜题时高度依赖视觉抽象的事实。然而，我们的初步实验揭示了一个悖论：简单地将 ARC-AGI 格网转换为图像会由于规则执行不够精确而降低性能。这促使我们提出一个中心假设：视觉和语言在整个推理阶段中具有互补的优势：视觉支持全局模式的抽象和验证，而语言专门负责符号规则的制定和精确执行。基于这一认识，我们引入了两种协同策略：（1）视觉-语言协同推理 (VLSR)，将 ARC-AGI 分解为模态对齐的子任务；（2）模态切换自校正 (MSSC)，利用视觉验证基于文本的推理以进行内在错误校正。大量实验表明，我们的方法在多个顶级模型和 ARC-AGI 任务上相比仅基于文本的基线方法可以取得高达 4.33% 的提升。我们的研究结果表明，将视觉抽象与语言推理统一是未来基础模型实现可迁移的人类级智能的关键一步。源代码将很快发布。', 'title_zh': '视觉思考，文本推理：ARC中的视觉语言协同作用'}
{'arxiv_id': 'arXiv:2511.15675', 'title': 'MF-GCN: A Multi-Frequency Graph Convolutional Network for Tri-Modal Depression Detection Using Eye-Tracking, Facial, and Acoustic Features', 'authors': 'Sejuti Rahman, Swakshar Deb, MD. Sameer Iqbal Chowdhury, MD. Jubair Ahmed Sourov, Mohammad Shamsuddin', 'link': 'https://arxiv.org/abs/2511.15675', 'abstract': 'Eye tracking data quantifies the attentional bias towards negative stimuli that is frequently observed in depressed groups. Audio and video data capture the affective flattening and psychomotor retardation characteristic of depression. Statistical validation confirmed their significant discriminative power in distinguishing depressed from non depressed groups. We address a critical limitation of existing graph-based models that focus on low-frequency information and propose a Multi-Frequency Graph Convolutional Network (MF-GCN). This framework consists of a novel Multi-Frequency Filter Bank Module (MFFBM), which can leverage both low and high frequency signals. Extensive evaluation against traditional machine learning algorithms and deep learning frameworks demonstrates that MF-GCN consistently outperforms baselines. In binary (depressed and non depressed) classification, the model achieved a sensitivity of 0.96 and F2 score of 0.94. For the 3 class (no depression, mild to moderate depression and severe depression) classification task, the proposed method achieved a sensitivity of 0.79 and specificity of 0.87 and siginificantly suprassed other models. To validate generalizability, the model was also evaluated on the Chinese Multimodal Depression Corpus (CMDC) dataset and achieved a sensitivity of 0.95 and F2 score of 0.96. These results confirm that our trimodal, multi frequency framework effectively captures cross modal interaction for accurate depression detection.', 'abstract_zh': '基于多频图卷积网络的抑郁检测多模态多频谱框架', 'title_zh': '多频图卷积网络：基于眼动、面部和声学特征的三模抑郁检测'}
{'arxiv_id': 'arXiv:2511.15661', 'title': 'VisPlay: Self-Evolving Vision-Language Models from Images', 'authors': 'Yicheng He, Chengsong Huang, Zongxia Li, Jiaxin Huang, Yonghui Yang', 'link': 'https://arxiv.org/abs/2511.15661', 'abstract': 'Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at this https URL', 'abstract_zh': '强化学习（RL）为通过复杂推理任务提高视觉语言模型（VLMs）提供了原则性的框架。然而，现有的RL方法通常依赖于人工标注的标签或特定任务的启发式方法来定义可验证的奖励，这两种方法都成本高昂且难以扩展。我们提出了一种自进化的RL框架VisPlay，该框架使VLMs能够自主利用大量未标注的图像数据提高其推理能力。VisPlay从一个基础的VLM开始，将其分配为两种交互的角色：一种是基于图像的问题提出者，提出具有挑战性但可回答的视觉问题；另一种是跨模态推理器，生成银级回答。这些角色通过联合训练和组相对策略优化（GRPO）进行训练，这种方法整合了多样性和难度奖励，以平衡生成问题的复杂性和银级回答的质量。VisPlay在两种模型家族中高效扩展。当在Qwen2.5-VL和MiMo-VL上训练时，VisPlay在八个基准测试中，包括MM-Vet和MMMU，在视觉推理、组合泛化和幻觉减少方面取得一致改进，展示了自进化的跨模态智能的可扩展路径。项目的页面可通过该链接访问。', 'title_zh': 'VisPlay: 自适应进化视觉-语言模型'}
{'arxiv_id': 'arXiv:2511.15435', 'title': 'HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation', 'authors': 'Linyin Luo, Yujuan Ding, Yunshan Ma, Wenqi Fan, Hanjiang Lai', 'link': 'https://arxiv.org/abs/2511.15435', 'abstract': "Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.", 'abstract_zh': '高级多模态检索增强生成（MRAG）技术已在增强大型多模态模型（LMMs）能力方面得到了广泛应用，但也带来了新的安全问题。现有的对抗性研究揭示了MRAG系统对知识投毒攻击的脆弱性，这种攻击能使检索模块召回注入的污染内容。然而，我们的工作考虑了一个不同的场景：仅通过在用户输入的图像中添加不可感知的扰动来对MRAG进行视觉攻击，而不操纵其他任何组件。这由于微调后的检索模块和大规模生成器的鲁棒性而更具挑战性，视觉扰动的效果可能在RAG链传播过程中进一步减弱。我们提出了一种新的分层视觉攻击，它使MRAG生成器的两个输入（多模态查询和增强知识）错位和中断，以迷惑其生成过程。我们进一步设计了分层两阶段策略来获得错位的增强知识。通过优化首先破坏跨模态对齐然后破坏多模态语义对齐的扰动，干扰检索模块使其召回原始数据库中的无关知识。我们在两个广泛使用的MRAG数据集OK-VQA和InfoSeek上进行了广泛实验。我们使用基于CLIP的检索模块和两个生成器BLIP-2和LLaVA。实验结果证明了我们视觉攻击在MRAG上的有效性，表现为检索和生成性能显著下降。', 'title_zh': 'HV-攻击：多模态检索增强生成的分层视觉攻击'}
{'arxiv_id': 'arXiv:2511.15204', 'title': 'Physics-Based Benchmarking Metrics for Multimodal Synthetic Images', 'authors': 'Kishor Datta Gupta, Marufa Kamal, Md. Mahfuzur Rahman, Fahad Rahman, Mohd Ariful Haque, Sunzida Siddique', 'link': 'https://arxiv.org/abs/2511.15204', 'abstract': 'Current state of the art measures like BLEU, CIDEr, VQA score, SigLIP-2 and CLIPScore are often unable to capture semantic or structural accuracy, especially for domain-specific or context-dependent scenarios. For this, this paper proposes a Physics-Constrained Multimodal Data Evaluation (PCMDE) metric combining large language models with reasoning, knowledge based mapping and vision-language models to overcome these limitations. The architecture is comprised of three main stages: (1) feature extraction of spatial and semantic information with multimodal features through object detection and VLMs; (2) Confidence-Weighted Component Fusion for adaptive component-level validation; and (3) physics-guided reasoning using large language models for structural and relational constraints (e.g., alignment, position, consistency) enforcement.', 'abstract_zh': '当前的先进度量标准，如BLEU、CIDEr、VQA得分、SigLIP-2和CLIPScore，往往难以捕捉到语义或结构准确性，尤其是在特定领域或依赖上下文的场景中。为此，本文提出了一种结合大型语言模型、推理、基于知识的映射和视觉语言模型的物理约束多模态数据评估（PCMDE）指标，以克服这些限制。该架构包含三个主要阶段：（1）通过对象检测和VLMs提取空间和语义信息的多模态特征；（2）自信加权组件融合以实现适应性的组件级验证；（3）使用大型语言模型进行基于物理的推理，以施加结构和关系约束（例如，对齐、位置、一致性）。', 'title_zh': '基于物理的多模态合成图像基准评估指标'}
{'arxiv_id': 'arXiv:2511.15162', 'title': 'Multimodal Wireless Foundation Models', 'authors': 'Ahmed Aboulfotouh, Hatem Abou-Zeid', 'link': 'https://arxiv.org/abs/2511.15162', 'abstract': 'Wireless foundation models (WFMs) have recently demonstrated promising capabilities, jointly performing multiple wireless functions and adapting effectively to new environments. However, while current WFMs process only one modality, depending on the task and operating conditions, the most informative modality changes and no single modality is best for all tasks. WFMs should therefore be designed to accept multiple modalities to enable a broader and more diverse range of tasks and scenarios. In this work, we propose and build the first multimodal wireless foundation model capable of processing both raw IQ streams and image-like wireless modalities (e.g., spectrograms and CSI) and performing multiple tasks across both. We introduce masked wireless modeling for the multimodal setting, a self-supervised objective and pretraining recipe that learns a joint representation from IQ streams and image-like wireless modalities. We evaluate the model on five tasks across both modality families: image-based (human activity sensing, RF signal classification, 5G NR positioning) and IQ-based (RF device fingerprinting, interference detection/classification). The multimodal WFM is competitive with single-modality WFMs, and in several cases surpasses their performance. Our results demonstrates the strong potential of developing multimodal WFMs that support diverse wireless tasks across different modalities. We believe this provides a concrete step toward both AI-native 6G and the vision of joint sensing, communication, and localization.', 'abstract_zh': '多模态无线基础模型：既能处理原始IQ流又能处理图像-like无线模态，并在两者之间执行多种任务', 'title_zh': '多模态无线基础模型'}
{'arxiv_id': 'arXiv:2511.15090', 'title': 'BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer', 'authors': 'Wenhan Yu, Wang Chen, Guanqiang Qi, Weikang Li, Yang Li, Lei Sha, Deguo Xia, Jizhou Huang', 'link': 'https://arxiv.org/abs/2511.15090', 'abstract': 'Document Visual Question Answering (DocVQA) is a fundamental task for multimodal document understanding and a key testbed for vision language reasoning. However, most existing DocVQA datasets are limited to the page level and lack fine grained spatial grounding, constraining the interpretability and reasoning capability of Vision Language Models (VLMs). To address this gap, we introduce BBox DocVQA a large scale, bounding box grounded dataset designed to enhance spatial reasoning and evidence localization in visual documents. We further present an automated construction pipeline, Segment Judge and Generate, which integrates a segment model for region segmentation, a VLM for semantic judgment, and another advanced VLM for question answer generation, followed by human verification for quality assurance. The resulting dataset contains 3.6 K diverse documents and 32 K QA pairs, encompassing single and multi region as well as single and multi page scenarios. Each QA instance is grounded on explicit bounding boxes, enabling fine grained evaluation of spatial semantic alignment. Benchmarking multiple state of the art VLMs (e.g., GPT 5, Qwen2.5 VL, and InternVL) on BBox DocVQA reveals persistent challenges in spatial grounding and reasoning accuracy. Furthermore, fine tuning on BBox DocVQA substantially improves both bounding box localization and answer generation, validating its effectiveness for enhancing the reasoning ability of VLMs. Our dataset and code will be publicly released to advance research on interpretable and spatially grounded vision language reasoning.', 'abstract_zh': '基于边界框的文档视觉问答（BBox DocVQA）：增强空间推理和视觉文档细粒度空间定位的大规模数据集', 'title_zh': 'BBox DocVQA：一个大规模边界框 grounding 数据集，用于增强文档视觉问答中的推理能力'}
{'arxiv_id': 'arXiv:2511.14969', 'title': 'Quality-Controlled Multimodal Emotion Recognition in Conversations with Identity-Based Transfer Learning and MAMBA Fusion', 'authors': 'Zanxu Wang, Homayoon Beigi', 'link': 'https://arxiv.org/abs/2511.14969', 'abstract': 'This paper addresses data quality issues in multimodal emotion recognition in conversation (MERC) through systematic quality control and multi-stage transfer learning. We implement a quality control pipeline for MELD and IEMOCAP datasets that validates speaker identity, audio-text alignment, and face detection. We leverage transfer learning from speaker and face recognition, assuming that identity-discriminative embeddings capture not only stable acoustic and Facial traits but also person-specific patterns of emotional expression. We employ RecoMadeEasy(R) engines for extracting 512-dimensional speaker and face embeddings, fine-tune MPNet-v2 for emotion-aware text representations, and adapt these features through emotion-specific MLPs trained on unimodal datasets. MAMBA-based trimodal fusion achieves 64.8% accuracy on MELD and 74.3% on IEMOCAP. These results show that combining identity-based audio and visual embeddings with emotion-tuned text representations on a quality-controlled subset of data yields consistent competitive performance for multimodal emotion recognition in conversation and provides a basis for further improvement on challenging, low-frequency emotion classes.', 'abstract_zh': '本文通过系统性质量控制和多阶段迁移学习解决多模态情绪识别对话（MERC）中的数据质量问题。我们为MELD和IEMOCAP数据集实施了一套质量控制流水线，验证说话人身份、音频-文本对齐和面部检测。我们利用说话人和面部识别的迁移学习，假设身份区分嵌入不仅捕捉稳定的声学和面部特征，还捕捉个体特定的情绪表达模式。我们使用RecoMadeEasy(R)引擎提取512维说话人和面部嵌入，微调MPNet-v2以获得情感意识的文本表示，并通过情感特定的MLP在单模态数据集上适应这些特征。基于MAMBA的三模态融合在MELD上达到64.8%的准确率，在IEMOCAP上达到74.3%。这些结果表明，结合基于身份的音频和视觉嵌入与情感调整的文本表示，可以在高质量控制的数据子集上获得一致的竞争性能，并为在具有挑战性的、低频率的情绪类别上进一步改进奠定了基础。', 'title_zh': '基于身份转移学习和MAMBA融合的质量控制多模态情感识别在对话中的应用'}
{'arxiv_id': 'arXiv:2511.14768', 'title': 'Causally-Informed Reinforcement Learning for Adaptive Emotion-Aware Social Media Recommendation', 'authors': 'Bhavika Jain, Robert Pitsko, Ananya Drishti, Mahfuza Farooque', 'link': 'https://arxiv.org/abs/2511.14768', 'abstract': "Social media recommendation systems play a central role in shaping users' emotional experiences. However, most systems are optimized solely for engagement metrics, such as click rate, viewing time, or scrolling, without accounting for users' emotional states. Repeated exposure to emotionally charged content has been shown to negatively affect users' emotional well-being over time. We propose an Emotion-aware Social Media Recommendation (ESMR) framework that personalizes content based on users' evolving emotional trajectories. ESMR integrates a Transformer-based emotion predictor with a hybrid recommendation policy: a LightGBM model for engagement during stable periods and a reinforcement learning agent with causally informed rewards when negative emotional states persist. Through behaviorally grounded evaluation over 30-day interaction traces, ESMR demonstrates improved emotional recovery, reduced volatility, and strong engagement retention. ESMR offers a path toward emotionally aware recommendations without compromising engagement performance.", 'abstract_zh': '基于情感意识的社会媒体推荐框架（ESMR）在塑造用户情感体验中的作用与其优化研究', 'title_zh': '因果驱动的强化学习在自适应情绪感知社交媒体推荐中应用'}
