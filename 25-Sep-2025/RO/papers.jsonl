{'arxiv_id': 'arXiv:2509.20333', 'title': 'BBoE: Leveraging Bundle of Edges for Kinodynamic Bidirectional Motion Planning', 'authors': 'Srikrishna Bangalore Raghu, Alessandro Roncone', 'link': 'https://arxiv.org/abs/2509.20333', 'abstract': 'In this work, we introduce BBoE, a bidirectional, kinodynamic, sampling-based motion planner that consistently and quickly finds low-cost solutions in environments with varying obstacle clutter. The algorithm combines exploration and exploitation while relying on precomputed robot state traversals, resulting in efficient convergence towards the goal. Our key contributions include: i) a strategy to navigate through obstacle-rich spaces by sorting and sequencing preprocessed forward propagations; and ii) BBoE, a robust bidirectional kinodynamic planner that utilizes this strategy to produce fast and feasible solutions. The proposed framework reduces planning time, diminishes solution cost and increases success rate in comparison to previous approaches.', 'abstract_zh': '本工作中，我们引入了BBoE，这是一种双向、动力学采样的运动规划器，能够在不同障碍物密度的环境中一致且迅速地找到低成本解。该算法结合了探索和利用，并依赖预计算的机器人状态遍历，从而高效地向目标收敛。我们的主要贡献包括：i) 一种通过排序和序列化预处理的前向传播策略来导航密集障碍物空间的方法；ii) BBoE，一种鲁棒的双向动力学规划器，利用此策略生成快速且可行的解决方案。所提出框架减少了规划时间，降低了解的成本，并提高了成功率，相较于先前的方法。', 'title_zh': 'BBoE：利用边缘束进行动力学双向运动规划'}
{'arxiv_id': 'arXiv:2509.20322', 'title': 'VisualMimic: Visual Humanoid Loco-Manipulation via Motion Tracking and Generation', 'authors': 'Shaofeng Yin, Yanjie Ze, Hong-Xing Yu, C. Karen Liu, Jiajun Wu', 'link': 'https://arxiv.org/abs/2509.20322', 'abstract': 'Humanoid loco-manipulation in unstructured environments demands tight integration of egocentric perception and whole-body control. However, existing approaches either depend on external motion capture systems or fail to generalize across diverse tasks. We introduce VisualMimic, a visual sim-to-real framework that unifies egocentric vision with hierarchical whole-body control for humanoid robots. VisualMimic combines a task-agnostic low-level keypoint tracker -- trained from human motion data via a teacher-student scheme -- with a task-specific high-level policy that generates keypoint commands from visual and proprioceptive input. To ensure stable training, we inject noise into the low-level policy and clip high-level actions using human motion statistics. VisualMimic enables zero-shot transfer of visuomotor policies trained in simulation to real humanoid robots, accomplishing a wide range of loco-manipulation tasks such as box lifting, pushing, football dribbling, and kicking. Beyond controlled laboratory settings, our policies also generalize robustly to outdoor environments. Videos are available at: this https URL .', 'abstract_zh': '人类态机器人在未结构化环境中的动操作需求緊密整合第一人称感知与全身控制。VisualMimic：一种统一第一人称视觉与分层全身控制的视觉仿真实现框架', 'title_zh': '视觉模拟：通过运动跟踪与生成实现视觉类人智能操作'}
{'arxiv_id': 'arXiv:2509.20297', 'title': 'mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies', 'authors': 'Remo Steiner, Alexander Millane, David Tingdahl, Clemens Volk, Vikram Ramasamy, Xinjie Yao, Peter Du, Soha Pouya, Shiwei Sheng', 'link': 'https://arxiv.org/abs/2509.20297', 'abstract': "End-to-end learning of robot control policies, structured as neural networks, has emerged as a promising approach to robotic manipulation. To complete many common tasks, relevant objects are required to pass in and out of a robot's field of view. In these settings, spatial memory - the ability to remember the spatial composition of the scene - is an important competency. However, building such mechanisms into robot learning systems remains an open research problem. We introduce mindmap (Spatial Memory in Deep Feature Maps for 3D Action Policies), a 3D diffusion policy that generates robot trajectories based on a semantic 3D reconstruction of the environment. We show in simulation experiments that our approach is effective at solving tasks where state-of-the-art approaches without memory mechanisms struggle. We release our reconstruction system, training code, and evaluation tasks to spur research in this direction.", 'abstract_zh': '基于深度特征图的三维空间记忆在无人机控制策略中的应用：End-to-end学习无人机控制策略的神经网络结构已 emerge 作为一种有前景的方法来实现机器人的操作。在这种设置下，空间记忆——即记住场景的空间组成的能力——是一项重要技能。然而，将此类机制集成到机器人学习系统中仍是一个开放的研究问题。我们引入了一种基于语义三维重建的三维扩散策略 mindmap，该策略根据环境的语义三维重建生成无人机轨迹。我们在模拟实验中展示了我们的方法在记忆机制缺失的先进方法难以解决的任务中展现出有效性。我们发布了我们的重建系统、训练代码和评估任务，以推动这一方向的研究。', 'title_zh': '思维导图：深度特征图中的空间记忆用于3D动作策略'}
{'arxiv_id': 'arXiv:2509.20286', 'title': 'Parse-Augment-Distill: Learning Generalizable Bimanual Visuomotor Policies from Single Human Video', 'authors': 'Georgios Tziafas, Jiayun Zhang, Hamidreza Kasaei', 'link': 'https://arxiv.org/abs/2509.20286', 'abstract': 'Learning visuomotor policies from expert demonstrations is an important frontier in modern robotics research, however, most popular methods require copious efforts for collecting teleoperation data and struggle to generalize out-ofdistribution. Scaling data collection has been explored through leveraging human videos, as well as demonstration augmentation techniques. The latter approach typically requires expensive simulation rollouts and trains policies with synthetic image data, therefore introducing a sim-to-real gap. In parallel, alternative state representations such as keypoints have shown great promise for category-level generalization. In this work, we bring these avenues together in a unified framework: PAD (Parse-AugmentDistill), for learning generalizable bimanual policies from a single human video. Our method relies on three steps: (a) parsing a human video demo into a robot-executable keypoint-action trajectory, (b) employing bimanual task-and-motion-planning to augment the demonstration at scale without simulators, and (c) distilling the augmented trajectories into a keypoint-conditioned policy. Empirically, we showcase that PAD outperforms state-ofthe-art bimanual demonstration augmentation works relying on image policies with simulation rollouts, both in terms of success rate and sample/cost efficiency. We deploy our framework in six diverse real-world bimanual tasks such as pouring drinks, cleaning trash and opening containers, producing one-shot policies that generalize in unseen spatial arrangements, object instances and background distractors. Supplementary material can be found in the project webpage this https URL.', 'abstract_zh': '从单个人工视频学习可泛化的双臂政策是现代机器人研究的一个重要前沿，然而，大多数流行的方法需要大量努力来收集遥操作数据，并且难以泛化到分布外。通过利用人类视频和演示增强技术扩展数据收集已经被探索。后者通常需要昂贵的模拟仿真并使用合成图像数据训练策略，因此引入了模拟仿真到现实应用之间的差距。与此同时，如关键点等替代状态表示显示出在类别级别泛化方面的巨大潜力。在本工作中，我们将这些途径整合到一个统一的框架中：PAD（解析-增强-精炼），用于从单个人工视频中学习可泛化的双臂政策。我们的方法依赖于三个步骤：(a) 将人工视频演示解析为机器人可执行的关键点-动作轨迹；(b) 使用双臂任务-运动规划在无模拟器的情况下大规模增强演示；(c) 将增强的轨迹精炼为关键点条件策略。在实验中，我们展示PAD在成功率和样本/成本效率方面均优于依赖图像策略和模拟仿真的双臂演示增强最新方法。我们在六种不同的现实世界双臂任务中部署了该框架，如倒饮料、清理垃圾和开容器，生成了在未见的空间布局、物体实例和背景干扰下泛化的单次演示策略。更多补充材料可以在项目网页上查看。', 'title_zh': 'Parse-Augment-Distill: 从单个人类视频学习可泛化的双臂视听运动策略'}
{'arxiv_id': 'arXiv:2509.20263', 'title': 'HL-IK: A Lightweight Implementation of Human-Like Inverse Kinematics in Humanoid Arms', 'authors': 'Bingjie Chen, Zihan Wang, Zhe Han, Guoping Pan, Yi Cheng, Houde Liu', 'link': 'https://arxiv.org/abs/2509.20263', 'abstract': 'Traditional IK methods for redundant humanoid manipulators emphasize end-effector (EE) tracking, frequently producing configurations that are valid mechanically but not human-like. We present Human-Like Inverse Kinematics (HL-IK), a lightweight IK framework that preserves EE tracking while shaping whole-arm configurations to appear human-like, without full-body sensing at runtime. The key idea is a learned elbow prior: using large-scale human motion data retargeted to the robot, we train a FiLM-modulated spatio-temporal attention network (FiSTA) to predict the next-step elbow pose from the EE target and a short history of EE-elbow this http URL prediction is incorporated as a small residual alongside EE and smoothness terms in a standard Levenberg-Marquardt optimizer, making HL-IK a drop-in addition to numerical IK stacks. Over 183k simulation steps, HL-IK reduces arm-similarity position and direction error by 30.6% and 35.4% on average, and by 42.2% and 47.4% on the most challenging trajectories. Hardware teleoperation on a robot distinct from simulation further confirms the gains in anthropomorphism. HL-IK is simple to integrate, adaptable across platforms via our pipeline, and adds minimal computation, enabling human-like motions for humanoid robots. Project page: this https URL', 'abstract_zh': '类人逆运动学（HL-IK）：一种轻量级的保留末端执行器跟踪并形成长臂类人配置的逆运动学框架', 'title_zh': 'HL-IK: 人体like逆运动学的轻量级实现于类人臂中'}
{'arxiv_id': 'arXiv:2509.20253', 'title': 'AnchDrive: Bootstrapping Diffusion Policies with Hybrid Trajectory Anchors for End-to-End Driving', 'authors': 'Jinhao Chai, Anqing Jiang, Hao Jiang, Shiyi Mu, Zichong Gu, Shugong Xu', 'link': 'https://arxiv.org/abs/2509.20253', 'abstract': 'End-to-end multi-modal planning has become a transformative paradigm in autonomous driving, effectively addressing behavioral multi-modality and the generalization challenge in long-tail scenarios. We propose AnchDrive, a framework for end-to-end driving that effectively bootstraps a diffusion policy to mitigate the high computational cost of traditional generative models. Rather than denoising from pure noise, AnchDrive initializes its planner with a rich set of hybrid trajectory anchors. These anchors are derived from two complementary sources: a static vocabulary of general driving priors and a set of dynamic, context-aware trajectories. The dynamic trajectories are decoded in real-time by a Transformer that processes dense and sparse perceptual features. The diffusion model then learns to refine these anchors by predicting a distribution of trajectory offsets, enabling fine-grained refinement. This anchor-based bootstrapping design allows for efficient generation of diverse, high-quality trajectories. Experiments on the NAVSIM benchmark confirm that AnchDrive sets a new state-of-the-art and shows strong gen?eralizability', 'abstract_zh': '端到端多模態规划已成为自主驾驶领域的 transformative 帕累托，有效解决了长尾场景中的行为多模態性和泛化挑战。我们提出 AnchDrive，一种框架，能够有效启动扩散策略以缓解传统生成模型的高计算成本。与从纯噪声去噪不同，AnchDrive 使用丰富的一系列混合轨迹锚点初始化其规划器。这些锚点来源于两种互补的来源：一套静态的一般驾驶先验词汇和一组动态的、上下文感知的轨迹。动态轨迹由 Transformer 实时解码，处理密集和稀疏的感觉特征。扩散模型随后通过预测轨迹偏移的分布学习细化这些锚点，实现细粒度的改进。基于锚点的启动设计允许高效生成多样且高质量的轨迹。在 NAVSIM 基准上的实验表明，AnchDrive 达到了新的 SOTA，并展示了强大的泛化能力。', 'title_zh': 'AnchDrive: 使用混合轨迹锚点bootstrap端到端驾驶策略'}
{'arxiv_id': 'arXiv:2509.20229', 'title': 'Techno-Economic analysis for Smart Hangar inspection operations through Sensing and Localisation at scale', 'authors': 'Angelos Plastropoulos, Nicolas P. Avdelidis, Argyrios Zolotas', 'link': 'https://arxiv.org/abs/2509.20229', 'abstract': 'The accuracy, resilience, and affordability of localisation are fundamental to autonomous robotic inspection within aircraft maintenance and overhaul (MRO) hangars. Hangars typically feature tall ceilings and are often made of materials such as metal. Due to its nature, it is considered a GPS-denied environment, with extensive multipath effects and stringent operational constraints that collectively create a uniquely challenging environment. This persistent gap highlights the need for domain-specific comparative studies, including rigorous cost, accuracy, and integration assessments, to inform a reliable and scalable deployment of a localisation system in the Smart Hangar. This paper presents the first techno-economic roadmap that benchmarks motion capture (MoCap), ultra-wideband (UWB), and a ceiling-mounted camera network across three operational scenarios: robot localisation, asset tracking, and surface defect detection within a 40x50 m hangar bay. A dual-layer optimisation for camera selection and positioning framework is introduced, which couples market-based camera-lens selection with an optimisation solver, producing camera layouts that minimise hardware while meeting accuracy targets. The roadmap equips MRO planners with an actionable method to balance accuracy, coverage, and budget, demonstrating that an optimised vision architecture has the potential to unlock robust and cost-effective sensing for next-generation Smart Hangars.', 'abstract_zh': '本地化精度、韧性和经济性对于航空维修与大修（MRO）机库中的自主机器人检查至关重要。机库通常具有高顶棚，且多由金属等材料构成。由于其特性，被认为是一个GPS受限的环境，存在广泛的多路径效应和严格的操作约束，这共同构成了独特而复杂的环境。这一持续存在的差距强调了需要进行特定领域的比较研究，包括严格的成本、精度和集成评估，以指导智能机库中本地化系统的可靠和可扩展部署。本文提出了第一个技术经济路线图，该路线图基于运动捕捉（MoCap）、超宽带（UWB）和天花板安装摄像头网络，在40x50米的机库空间内评估三种操作场景：机器人本地化、资产跟踪和表面缺陷检测。引入了一种双层优化框架来进行摄像头选择和定位，该框架结合市场驱动的摄像头-镜头选择与优化求解器，生成满足精度目标的同时最小化硬件的摄像头布局。该路线图为MRO规划者提供了实用的方法来平衡精度、覆盖范围和预算，证明了优化的视觉架构有潜力解锁智能机库中稳健且低成本的感知。', 'title_zh': '智能机库检测操作通过大规模传感与定位的 techno-economics 分析'}
{'arxiv_id': 'arXiv:2509.20219', 'title': 'A Biomimetic Vertebraic Soft Robotic Tail for High-Speed, High-Force Dynamic Maneuvering', 'authors': 'Sicong Liu, Jianhui Liu, Fang Chen, Wenjian Yang, Juan Yi, Yu Zheng, Zheng Wang, Wanchao Chi, Chaoyang Song', 'link': 'https://arxiv.org/abs/2509.20219', 'abstract': 'Robotic tails can enhance the stability and maneuverability of mobile robots, but current designs face a trade-off between the power of rigid systems and the safety of soft ones. Rigid tails generate large inertial effects but pose risks in unstructured environments, while soft tails lack sufficient speed and force. We present a Biomimetic Vertebraic Soft Robotic (BVSR) tail that resolves this challenge through a compliant pneumatic body reinforced by a passively jointed vertebral column inspired by musculoskeletal structures. This hybrid design decouples load-bearing and actuation, enabling high-pressure actuation (up to 6 bar) for superior dynamics while preserving compliance. A dedicated kinematic and dynamic model incorporating vertebral constraints is developed and validated experimentally. The BVSR tail achieves angular velocities above 670°/s and generates inertial forces and torques up to 5.58 N and 1.21 Nm, indicating over 200% improvement compared to non-vertebraic designs. Demonstrations on rapid cart stabilization, obstacle negotiation, high-speed steering, and quadruped integration confirm its versatility and practical utility for agile robotic platforms.', 'abstract_zh': 'Biomimetic Vertebraic Soft Robotic (BVSR) 尾部可以增强移动机器人的稳定性和机动性，但当前设计在刚性系统的功率和软性系统的安全性之间存在权衡。刚性尾巴会产生较大的惯性效应，但在未结构化环境中存在风险，而软性尾巴缺乏足够的速度和力量。我们提出了一种受生物结构启发的 Biomimetic Vertebraic Soft Robotic (BVSR) 尾部，通过借鉴肌肉骨骼结构的被动关节椎柱加强可变形气动主体来解决这一挑战。这种混合设计解耦了承载和驱动，能够实现高达 6 巴的高压驱动（从而提供卓越的动力学性能）同时保持可变形性。开发并实验验证了一个包含椎体约束的专用运动学和动力学模型。BVSR 尾部实现了超过 670°/s 的角速度，并产生了高达 5.58 N 的惯性力和 1.21 Nm 的惯性扭矩，显示出与无椎体设计相比超过 200% 的改进。快速手推车稳定、障碍物导航、高速转向和四足动物集成的演示证实了其在灵活动作机器人平台中的多样性和实用性。', 'title_zh': '一种用于高-speed高-force动态机动的仿生脊椎软体尾部'}
{'arxiv_id': 'arXiv:2509.20109', 'title': 'Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving', 'authors': 'Pengxiang Li, Yinan Zheng, Yue Wang, Huimin Wang, Hang Zhao, Jingjing Liu, Xianyuan Zhan, Kun Zhan, Xianpeng Lang', 'link': 'https://arxiv.org/abs/2509.20109', 'abstract': 'End-to-End (E2E) solutions have emerged as a mainstream approach for autonomous driving systems, with Vision-Language-Action (VLA) models representing a new paradigm that leverages pre-trained multimodal knowledge from Vision-Language Models (VLMs) to interpret and interact with complex real-world environments. However, these methods remain constrained by the limitations of imitation learning, which struggles to inherently encode physical rules during training. Existing approaches often rely on complex rule-based post-refinement, employ reinforcement learning that remains largely limited to simulation, or utilize diffusion guidance that requires computationally expensive gradient calculations. To address these challenges, we introduce ReflectDrive, a novel learning-based framework that integrates a reflection mechanism for safe trajectory generation via discrete diffusion. We first discretize the two-dimensional driving space to construct an action codebook, enabling the use of pre-trained Diffusion Language Models for planning tasks through fine-tuning. Central to our approach is a safety-aware reflection mechanism that performs iterative self-correction without gradient computation. Our method begins with goal-conditioned trajectory generation to model multi-modal driving behaviors. Based on this, we apply local search methods to identify unsafe tokens and determine feasible solutions, which then serve as safe anchors for inpainting-based regeneration. Evaluated on the NAVSIM benchmark, ReflectDrive demonstrates significant advantages in safety-critical trajectory generation, offering a scalable and reliable solution for autonomous driving systems.', 'abstract_zh': '端到端（E2E）解决方案已成为自主驾驶系统的主流方法，视觉语言动作（VLA）模型作为一种新的范式，利用预训练的多模态知识从视觉语言模型（VLMs）中解释和与复杂的现实环境交互。然而，这些方法仍然受到模仿学习限制的约束，模仿学习在训练过程中难以内在地编码物理规则。现有方法通常依赖于复杂的基于规则的后处理、有限的基于模拟的强化学习或要求昂贵梯度计算的扩散指导。为了解决这些挑战，我们提出了ReflectDrive，一种新颖的学习框架，通过离散扩散集成了一种反映机制以生成安全的轨迹。我们首先离散化二维驾驶空间以构建动作码本，通过微调使用预训练的扩散语言模型进行规划任务。我们方法的核心是一种安全意识反射机制，能够进行迭代的自我纠正而不进行梯度计算。方法从基于目标的轨迹生成建模多模态驾驶行为开始。在此基础上，我们应用局部搜索方法识别不安全的标记并确定可行的解，这些解随后作为基于填充的再生的安全锚点。在NAVSIM基准测试上评估，ReflectDrive在关键安全轨迹生成方面显示出显著优势，为自主驾驶系统提供了一种可扩展且可靠的解决方案。', 'title_zh': '离散扩散在自主驾驶反射型视觉-语言-行动模型中的应用'}
{'arxiv_id': 'arXiv:2509.20093', 'title': 'Hybrid Safety Verification of Multi-Agent Systems using $ψ$-Weighted CBFs and PAC Guarantees', 'authors': 'Venkat Margapuri, Garik Kazanjian, Naren Kosaraju', 'link': 'https://arxiv.org/abs/2509.20093', 'abstract': 'This study proposes a hybrid safety verification framework for closed-loop multi-agent systems under bounded stochastic disturbances. The proposed approach augments control barrier functions with a novel $\\psi$-weighted formulation that encodes directional control alignment between agents into the safety constraints. Deterministic admissibility is combined with empirical validation via Monte Carlo rollouts, and a PAC-style guarantee is derived based on margin-aware safety violations to provide a probabilistic safety certificate. The results from the experiments conducted under different bounded stochastic disturbances validate the feasibility of the proposed approach.', 'abstract_zh': '本文提出了一种混合安全验证框架，用于受有界随机干扰影响的闭环多agent系统。提出的观点通过一种新颖的$\\psi$加权公式将agent间的方向控制对齐编码到安全约束中，增强了控制障碍函数。确定性可接纳性与基于蒙特卡洛展开的实验验证相结合，并基于感知差错的安全违规推导出PAC风格的保证，以提供一种概率安全证明。在不同有界随机干扰下的实验结果验证了所提出方法的可行性。', 'title_zh': '基于ψ加权CBF和PAC保证的多智能体系统混合安全性验证'}
{'arxiv_id': 'arXiv:2509.20084', 'title': 'C-3TO: Continuous 3D Trajectory Optimization on Neural Euclidean Signed Distance Fields', 'authors': 'Guillermo Gil, Jose Antonio Cobano, Luis Merino, Fernando Caballero', 'link': 'https://arxiv.org/abs/2509.20084', 'abstract': "This paper introduces a novel framework for continuous 3D trajectory optimization in cluttered environments, leveraging online neural Euclidean Signed Distance Fields (ESDFs). Unlike prior approaches that rely on discretized ESDF grids with interpolation, our method directly optimizes smooth trajectories represented by fifth-order polynomials over a continuous neural ESDF, ensuring precise gradient information throughout the entire trajectory. The framework integrates a two-stage nonlinear optimization pipeline that balances efficiency, safety and smoothness. Experimental results demonstrate that C-3TO produces collision-aware and dynamically feasible trajectories. Moreover, its flexibility in defining local window sizes and optimization parameters enables straightforward adaptation to diverse user's needs without compromising performance. By combining continuous trajectory parameterization with a continuously updated neural ESDF, C-3TO establishes a robust and generalizable foundation for safe and efficient local replanning in aerial robotics.", 'abstract_zh': '本文提出了一种新颖的框架，用于利用在线神经欧几里得-signed距离场(ESDF)在复杂环境中进行连续三维轨迹优化。该方法直接优化由五次多项式表示的平滑轨迹，而非依赖于离散化的ESDF网格和插值，确保在整个轨迹中都具有精确的梯度信息。该框架集成了一个兼顾效率、安全性和平滑性的两阶段非线性优化管道。实验结果表明，C-3TO能够生成碰撞感知和动态可行的轨迹。此外，通过灵活定义局部窗口大小和优化参数，C-3TO能轻松适应多样化用户需求而不牺牲性能。通过结合连续轨迹参数化和不断更新的神经ESDF，C-3TO为航空机器人中的安全高效局部重规划提供了稳健且通用的基础。', 'title_zh': 'C-3TO：神经欧几里得符号距离场上的连续3D轨迹优化'}
{'arxiv_id': 'arXiv:2509.20082', 'title': 'Orbital Stabilization and Time Synchronization of Unstable Periodic Motions in Underactuated Robots', 'authors': 'Surov Maksim', 'link': 'https://arxiv.org/abs/2509.20082', 'abstract': 'This paper presents a control methodology for achieving orbital stabilization with simultaneous time synchronization of periodic trajectories in underactuated robotic systems. The proposed approach extends the classical transverse linearization framework to explicitly incorporate time-desynchronization dynamics. To stabilize the resulting extended transverse dynamics, we employ a combination of time-varying LQR and sliding-mode control. The theoretical results are validated experimentally through the implementation of both centralized and decentralized control strategies on a group of six Butterfly robots.', 'abstract_zh': '本文提出一种控制方法，用于实现欠驱动机器人系统中轨道稳定性和周期轨迹时间同步的同时控制。所提出的方案扩展了经典的横向线性化框架，明确纳入了时间脱同步动力学。为稳定扩展的横向动力学，我们采用了时间变增益LQR与滑模控制的组合方法。理论结果通过在六只蝴蝶机器人上实现集中式和分布式控制策略进行实验验证。', 'title_zh': '欠驱动机器人中不稳定周期运动的轨道稳定与时间同步'}
{'arxiv_id': 'arXiv:2509.20081', 'title': 'DB-TSDF: Directional Bitmask-based Truncated Signed Distance Fields for Efficient Volumetric Mapping', 'authors': 'Jose E. Maese, Luis Merino, Fernando Caballero', 'link': 'https://arxiv.org/abs/2509.20081', 'abstract': 'This paper presents a high-efficiency, CPU-only volumetric mapping framework based on a Truncated Signed Distance Field (TSDF). The system incrementally fuses raw LiDAR point-cloud data into a voxel grid using a directional bitmask-based integration scheme, producing dense and consistent TSDF representations suitable for real-time 3D reconstruction. A key feature of the approach is that the processing time per point-cloud remains constant, regardless of the voxel grid resolution, enabling high resolution mapping without sacrificing runtime performance. In contrast to most recent TSDF/ESDF methods that rely on GPU acceleration, our method operates entirely on CPU, achieving competitive results in speed. Experiments on real-world open datasets demonstrate that the generated maps attain accuracy on par with contemporary mapping techniques.', 'abstract_zh': '基于截断符号距离场的高效CPU_ONLY体积映射框架：实时3D重建中的高分辨率mapping', 'title_zh': 'DB-TSDF：基于方向位掩码的截断-signed距离场 volumetric 映射方法'}
{'arxiv_id': 'arXiv:2509.20077', 'title': 'Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning', 'authors': 'Xun Li, Rodrigo Santa Cruz, Mingze Xi, Hu Zhang, Madhawa Perera, Ziwei Wang, Ahalya Ravendran, Brandon J. Matthews, Feng Xu, Matt Adcock, Dadong Wang, Jiajun Liu', 'link': 'https://arxiv.org/abs/2509.20077', 'abstract': "To enable robots to comprehend high-level human instructions and perform complex tasks, a key challenge lies in achieving comprehensive scene understanding: interpreting and interacting with the 3D environment in a meaningful way. This requires a smart map that fuses accurate geometric structure with rich, human-understandable semantics. To address this, we introduce the 3D Queryable Scene Representation (3D QSR), a novel framework built on multimedia data that unifies three complementary 3D representations: (1) 3D-consistent novel view rendering and segmentation from panoptic reconstruction, (2) precise geometry from 3D point clouds, and (3) structured, scalable organization via 3D scene graphs. Built on an object-centric design, the framework integrates with large vision-language models to enable semantic queryability by linking multimodal object embeddings, and supporting object-level retrieval of geometric, visual, and semantic information. The retrieved data are then loaded into a robotic task planner for downstream execution. We evaluate our approach through simulated robotic task planning scenarios in Unity, guided by abstract language instructions and using the indoor public dataset Replica. Furthermore, we apply it in a digital duplicate of a real wet lab environment to test QSR-supported robotic task planning for emergency response. The results demonstrate the framework's ability to facilitate scene understanding and integrate spatial and semantic reasoning, effectively translating high-level human instructions into precise robotic task planning in complex 3D environments.", 'abstract_zh': '为了使机器人能够理解高层次的人类指令并执行复杂任务，关键挑战在于实现全面的场景理解：以有意义的方式解析和与三维环境互动。这需要一个智能地图，融合精确的几何结构和丰富的、人类可理解的语义。为此，我们提出了三维可查询场景表示（3D Queryable Scene Representation，3D QSR），这是一个基于多媒体数据的新型框架，统一了三个互补的三维表示：（1）来自全景重建的三维一致的新视角渲染和分割，（2）来自三维点云的精确几何结构，以及（3）通过三维场景图实现的结构化、可扩展的组织。基于对象中心的设计，该框架与大规模的多模态视觉语言模型集成，通过链接多模态对象嵌入实现代词查询性，并支持对象级别的几何、视觉和语义信息检索。检索的数据随后加载到机器人任务规划器中，以便下游执行。我们通过在Unity中模拟的机器人任务规划场景评估该方法，使用抽象语言指令并结合室内公共数据集Replica。此外，我们在一个真实的湿实验室环境的数字副本中应用该方法，测试QSR支持的机器人任务规划在紧急响应中的应用。结果表明，该框架能够促进场景理解并整合空间和语义推理，有效地将高层次的人类指令转化为复杂的三维环境中的精确机器人任务规划。', 'title_zh': '可查询的3D场景表示：一种用于语义推理和机器人任务规划的多模态框架'}
{'arxiv_id': 'arXiv:2509.20070', 'title': 'LLM Trainer: Automated Robotic Data Generating via Demonstration Augmentation using LLMs', 'authors': 'Abraham George, Amir Barati Farimani', 'link': 'https://arxiv.org/abs/2509.20070', 'abstract': 'We present LLM Trainer, a fully automated pipeline that leverages the world knowledge of Large Language Models (LLMs) to transform a small number of human demonstrations (as few as one) into a large robot dataset for imitation learning. Our approach decomposes demonstration generation into two steps: (1) offline demonstration annotation that extracts keyframes, salient objects, and pose-object relations; and (2) online keypose retargeting that adapts those keyframes to a new scene, given an initial observation. Using these modified keypoints, our system warps the original demonstration to generate a new trajectory, which is then executed, and the resulting demo, if successful, is saved. Because the annotation is reusable across scenes, we use Thompson sampling to optimize the annotation, significantly improving generation success rate. We evaluate our method on a range of tasks, and find that our data annotation method consistently outperforms expert-engineered baselines. We further show an ensemble policy that combines the optimized LLM feed-forward plan with a learned feedback imitation learning controller. Finally, we demonstrate hardware feasibility on a Franka Emika Panda robot. For additional materials and demonstration videos, please see the project website: this https URL', 'abstract_zh': 'LLM Trainer：利用大型语言模型知识自动生成机器人演示数据的流水线', 'title_zh': 'LLM训练器：通过LLM增强示范生成的自动化机器人数据生成'}
{'arxiv_id': 'arXiv:2509.20036', 'title': 'MARG: MAstering Risky Gap Terrains for Legged Robots with Elevation Mapping', 'authors': 'Yinzhao Dong, Ji Ma, Liu Zhao, Wanyue Li, Peng Lu', 'link': 'https://arxiv.org/abs/2509.20036', 'abstract': "Deep Reinforcement Learning (DRL) controllers for quadrupedal locomotion have demonstrated impressive performance on challenging terrains, allowing robots to execute complex skills such as climbing, running, and jumping. However, existing blind locomotion controllers often struggle to ensure safety and efficient traversal through risky gap terrains, which are typically highly complex, requiring robots to perceive terrain information and select appropriate footholds during locomotion accurately. Meanwhile, existing perception-based controllers still present several practical limitations, including a complex multi-sensor deployment system and expensive computing resource requirements. This paper proposes a DRL controller named MAstering Risky Gap Terrains (MARG), which integrates terrain maps and proprioception to dynamically adjust the action and enhance the robot's stability in these tasks. During the training phase, our controller accelerates policy optimization by selectively incorporating privileged information (e.g., center of mass, friction coefficients) that are available in simulation but unmeasurable directly in real-world deployments due to sensor limitations. We also designed three foot-related rewards to encourage the robot to explore safe footholds. More importantly, a terrain map generation (TMG) model is proposed to reduce the drift existing in mapping and provide accurate terrain maps using only one LiDAR, providing a foundation for zero-shot transfer of the learned policy. The experimental results indicate that MARG maintains stability in various risky terrain tasks.", 'abstract_zh': '基于深强化学习的克服危险间隙地形的四足机器人控制器（MAstering Risky Gap Terrains, MARG）', 'title_zh': 'MARG: 基于高程地图学习风险地型跨越技巧的腿足机器人技术'}
{'arxiv_id': 'arXiv:2509.20009', 'title': 'Lidar-based Tracking of Traffic Participants with Sensor Nodes in Existing Urban Infrastructure', 'authors': 'Simon Schäfer, Bassam Alrifaee, Ehsan Hashemi', 'link': 'https://arxiv.org/abs/2509.20009', 'abstract': 'This paper presents a lidar-only state estimation and tracking framework, along with a roadside sensing unit for integration with existing urban infrastructure. Urban deployments demand scalable, real-time tracking solutions, yet traditional remote sensing remains costly and computationally intensive, especially under perceptually degraded conditions. Our sensor node couples a single lidar with an edge computing unit and runs a computationally efficient, GPU-free observer that simultaneously estimates object state, class, dimensions, and existence probability. The pipeline performs: (i) state updates via an extended Kalman filter, (ii) dimension estimation using a 1D grid-map/Bayesian update, (iii) class updates via a lookup table driven by the most probable footprint, and (iv) existence estimation from track age and bounding-box consistency. Experiments in dynamic urban-like scenes with diverse traffic participants demonstrate real-time performance and high precision: The complete end-to-end pipeline finishes within \\SI{100}{\\milli\\second} for \\SI{99.88}{\\%} of messages, with an excellent detection rate. Robustness is further confirmed under simulated wind and sensor vibration. These results indicate that reliable, real-time roadside tracking is feasible on CPU-only edge hardware, enabling scalable, privacy-friendly deployments within existing city infrastructure. The framework integrates with existing poles, traffic lights, and buildings, reducing deployment costs and simplifying large-scale urban rollouts and maintenance efforts.', 'abstract_zh': '基于激光雷达的实时路侧状态估计与跟踪框架：与现有城市基础设施的集成', 'title_zh': '基于现有城市基础设施中的传感器节点的 Lidar 轨迹跟踪方法'}
{'arxiv_id': 'arXiv:2509.19972', 'title': 'An effective control of large systems of active particles: An application to evacuation problem', 'authors': 'Albina Klepach, Egor E. Nuzhin, Alexey A. Tsukanov, Nikolay V. Brilliantov', 'link': 'https://arxiv.org/abs/2509.19972', 'abstract': 'Manipulation of large systems of active particles is a serious challenge across diverse domains, including crowd management, control of robotic swarms, and coordinated material transport. The development of advanced control strategies for complex scenarios is hindered, however, by the lack of scalability and robustness of the existing methods, in particular, due to the need of an individual control for each agent. One possible solution involves controlling a system through a leader or a group of leaders, which other agents tend to follow. Using such an approach we develop an effective control strategy for a leader, combining reinforcement learning (RL) with artificial forces acting on the system. To describe the guidance of active particles by a leader we introduce the generalized Vicsek model. This novel method is then applied to the problem of the effective evacuation by a robot-rescuer (leader) of large groups of people from hazardous places. We demonstrate, that while a straightforward application of RL yields suboptimal results, even for advanced architectures, our approach provides a robust and efficient evacuation strategy. The source code supporting this study is publicly available at: this https URL.', 'abstract_zh': '大规模活性颗粒系统的操控在 crowd 管理、机器人集群控制以及协同材料运输等领域是一项严重挑战。然而，由于现有方法缺乏可扩展性和鲁棒性，特别是在需要为每个代理个体化控制的情况下，复杂场景下的高级控制策略的发展受到阻碍。一种可能的解决方案是通过领导者或一群领导者控制系统，其他代理倾向于跟随。采用这种方法，我们结合强化学习（RL）和对系统施加的人工力，开发了领导者的有效控制策略。为描述领导者对活性颗粒的引导，我们引入了广义 Vicsek 模型。随后，我们应用此新方法解决由机器人救援者（领导者）有效疏散大量人群的问题。我们证明，即使对于先进的架构，直接应用 RL 也会导致次优结果，而我们的方法则提供了鲁棒且高效的疏散策略。此研究的支持代码可在以下链接获取：this https URL。', 'title_zh': '大型活性粒子系统的有效控制：撤离问题的应用'}
{'arxiv_id': 'arXiv:2509.19958', 'title': 'Generalist Robot Manipulation beyond Action Labeled Data', 'authors': 'Alexander Spiridonov, Jan-Nico Zaech, Nikolay Nikolov, Luc Van Gool, Danda Pani Paudel', 'link': 'https://arxiv.org/abs/2509.19958', 'abstract': 'Recent advances in generalist robot manipulation leverage pre-trained Vision-Language Models (VLMs) and large-scale robot demonstrations to tackle diverse tasks in a zero-shot manner. A key challenge remains: scaling high-quality, action-labeled robot demonstration data, which existing methods rely on for robustness and generalization. To address this, we propose a method that benefits from videos without action labels - featuring humans and/or robots in action - enhancing open-vocabulary performance and enabling data-efficient learning of new tasks. Our method extracts dense, dynamic 3D point clouds at the hand or gripper location and uses a proposed 3D dynamics predictor for self-supervision. This predictor is then tuned to an action predictor using a smaller labeled dataset for action alignment. We show that our method not only learns from unlabeled human and robot demonstrations - improving downstream generalist robot policies - but also enables robots to learn new tasks without action labels (i.e., out-of-action generalization) in both real-world and simulated settings.', 'abstract_zh': 'Recent Advances in Generalist Robot Manipulation via Action-Unlabeled Video Data and 3D Dynamics Prediction', 'title_zh': '超越动作标注数据的一般机器人 manipulation'}
{'arxiv_id': 'arXiv:2509.19954', 'title': 'Robot Trajectron V2: A Probabilistic Shared Control Framework for Navigation', 'authors': 'Pinhao Song, Yurui Du, Ophelie Saussus, Sofie De Schrijver, Irene Caprara, Peter Janssen, Renaud Detry', 'link': 'https://arxiv.org/abs/2509.19954', 'abstract': "We propose a probabilistic shared-control solution for navigation, called Robot Trajectron V2 (RT-V2), that enables accurate intent prediction and safe, effective assistance in human-robot interaction. RT-V2 jointly models a user's long-term behavioral patterns and their noisy, low-dimensional control signals by combining a prior intent model with a posterior update that accounts for real-time user input and environmental context. The prior captures the multimodal and history-dependent nature of user intent using recurrent neural networks and conditional variational autoencoders, while the posterior integrates this with uncertain user commands to infer desired actions. We conduct extensive experiments to validate RT-V2 across synthetic benchmarks, human-computer interaction studies with keyboard input, and brain-machine interface experiments with non-human primates. Results show that RT-V2 outperforms the state of the art in intent estimation, provides safe and efficient navigation support, and adequately balances user autonomy with assistive intervention. By unifying probabilistic modeling, reinforcement learning, and safe optimization, RT-V2 offers a principled and generalizable approach to shared control for diverse assistive technologies.", 'abstract_zh': '一种用于导航的概率共享控制解决方案：机器人轨迹tron V2（RT-V2），该方案能够在人机交互中实现准确的意图预测和安全有效的辅助。', 'title_zh': 'Robot Trajectron V2: 一种概率共享控制导航框架'}
{'arxiv_id': 'arXiv:2509.19916', 'title': 'GUIDE: A Diffusion-Based Autonomous Robot Exploration Framework Using Global Graph Inference', 'authors': 'Zijun Che, Yinghong Zhang, Shengyi Liang, Boyu Zhou, Jun Ma, Jinni Zhou', 'link': 'https://arxiv.org/abs/2509.19916', 'abstract': 'Autonomous exploration in structured and complex indoor environments remains a challenging task, as existing methods often struggle to appropriately model unobserved space and plan globally efficient paths. To address these limitations, we propose GUIDE, a novel exploration framework that synergistically combines global graph inference with diffusion-based decision-making. We introduce a region-evaluation global graph representation that integrates both observed environmental data and predictions of unexplored areas, enhanced by a region-level evaluation mechanism to prioritize reliable structural inferences while discounting uncertain predictions. Building upon this enriched representation, a diffusion policy network generates stable, foresighted action sequences with significantly reduced denoising steps. Extensive simulations and real-world deployments demonstrate that GUIDE consistently outperforms state-of-the-art methods, achieving up to 18.3% faster coverage completion and a 34.9% reduction in redundant movements.', 'abstract_zh': '自主探索结构化和复杂室内环境仍然是一个具有挑战性的任务，现有方法往往难以适当建模未观察到的空间并规划全局高效路径。为解决这些问题，我们提出GUIDE，一种新颖的探索框架，结合了全局图推理与扩散决策机制。我们引入一种区域评估全局图表示，整合了已观察到的环境数据和未探索区域的预测，并通过区域级评估机制优先考虑可靠的结构性推理，同时忽略不确定的预测。基于这种丰富的表示，扩散策略网络生成了稳定、前瞻性的行动序列，显著减少了去噪步骤。广泛的技术模拟和实际部署表明，GUIDE 一致地优于现有最佳方法，实现了高达18.3%更快的覆盖率完成，并减少了34.9%的冗余移动。', 'title_zh': 'GUIDE：基于全局图推理的扩散自主导航探索框架'}
{'arxiv_id': 'arXiv:2509.19892', 'title': 'D3Grasp: Diverse and Deformable Dexterous Grasping for General Objects', 'authors': 'Keyu Wang, Bingcong Lu, Zhengxue Cheng, Hengdi Zhang, Li Song', 'link': 'https://arxiv.org/abs/2509.19892', 'abstract': 'Achieving diverse and stable dexterous grasping for general and deformable objects remains a fundamental challenge in robotics, due to high-dimensional action spaces and uncertainty in perception. In this paper, we present D3Grasp, a multimodal perception-guided reinforcement learning framework designed to enable Diverse and Deformable Dexterous Grasping. We firstly introduce a unified multimodal representation that integrates visual and tactile perception to robustly grasp common objects with diverse properties. Second, we propose an asymmetric reinforcement learning architecture that exploits privileged information during training while preserving deployment realism, enhancing both generalization and sample efficiency. Third, we meticulously design a training strategy to synthesize contact-rich, penetration-free, and kinematically feasible grasps with enhanced adaptability to deformable and contact-sensitive objects. Extensive evaluations confirm that D3Grasp delivers highly robust performance across large-scale and diverse object categories, and substantially advances the state of the art in dexterous grasping for deformable and compliant objects, even under perceptual uncertainty and real-world disturbances. D3Grasp achieves an average success rate of 95.1% in real-world trials,outperforming prior methods on both rigid and deformable objects benchmarks.', 'abstract_zh': '实现对通用和可变形物体的多样化稳定灵巧抓取仍然是机器人领域的基本挑战，由于高维度的动作空间和感知的不确定性。本文提出D3Grasp，一种多模态感知引导的强化学习框架，旨在实现多样化和可变形灵巧抓取。', 'title_zh': 'D3Grasp: 多样化且可变形的通用物体灵巧抓取'}
{'arxiv_id': 'arXiv:2509.19853', 'title': 'SAGE:State-Aware Guided End-to-End Policy for Multi-Stage Sequential Tasks via Hidden Markov Decision Process', 'authors': 'BinXu Wu, TengFei Zhang, Chen Yang, JiaHao Wen, HaoCheng Li, JingTian Ma, Zhen Chen, JingYuan Wang', 'link': 'https://arxiv.org/abs/2509.19853', 'abstract': 'Multi-stage sequential (MSS) robotic manipulation tasks are prevalent and crucial in robotics. They often involve state ambiguity, where visually similar observations correspond to different actions. We present SAGE, a state-aware guided imitation learning framework that models tasks as a Hidden Markov Decision Process (HMDP) to explicitly capture latent task stages and resolve ambiguity. We instantiate the HMDP with a state transition network that infers hidden states, and a state-aware action policy that conditions on both observations and hidden states to produce actions, thereby enabling disambiguation across task stages. To reduce manual annotation effort, we propose a semi-automatic labeling pipeline combining active learning and soft label interpolation. In real-world experiments across multiple complex MSS tasks with state ambiguity, SAGE achieved 100% task success under the standard evaluation protocol, markedly surpassing the baselines. Ablation studies further show that such performance can be maintained with manual labeling for only about 13% of the states, indicating its strong effectiveness.', 'abstract_zh': '多阶段顺序（MSS）机器人操作任务在机器人领域普遍存在且至关重要。它们经常涉及状态不确定性，其中视觉上相似的观测对应不同的操作。我们提出了SAGE，一种状态感知引导的模仿学习框架，将任务建模为隐马尔可夫决策过程（HMDP）以明确捕捉隐含的任务阶段并解决不确定性。我们使用状态转换网络实例化HMDP以推断隐藏状态，并使用状态感知的动作策略基于观测和隐藏状态生成动作，从而在任务阶段之间实现去混淆。为了减少手动标注努力，我们提出了一种结合主动学习和软标签插值的半自动标注管道。在使用标准评估协议的多个具有状态不确定性的复杂MSS任务的真实世界实验中，SAGE实现了100%的任务成功，明显超过了基线。进一步的消融研究显示，这种性能可以通过仅为约13%的状态进行手动标注来保持，表明其强大的有效性。', 'title_zh': 'SAGE：具有状态感知引导的多阶段序列任务端到端策略隐马尔可夫决策过程'}
{'arxiv_id': 'arXiv:2509.19851', 'title': 'Where Did I Leave My Glasses? Open-Vocabulary Semantic Exploration in Real-World Semi-Static Environments', 'authors': 'Benjamin Bogenberger, Oliver Harrison, Orrin Dahanaggamaarachchi, Lukas Brunke, Jingxing Qian, Siqi Zhou, Angela P. Schoellig', 'link': 'https://arxiv.org/abs/2509.19851', 'abstract': "Robots deployed in real-world environments, such as homes, must not only navigate safely but also understand their surroundings and adapt to environment changes. To perform tasks efficiently, they must build and maintain a semantic map that accurately reflects the current state of the environment. Existing research on semantic exploration largely focuses on static scenes without persistent object-level instance tracking. A consistent map is, however, crucial for real-world robotic applications where objects in the environment can be removed, reintroduced, or shifted over time. In this work, to close this gap, we propose an open-vocabulary, semantic exploration system for semi-static environments. Our system maintains a consistent map by building a probabilistic model of object instance stationarity, systematically tracking semi-static changes, and actively exploring areas that have not been visited for a prolonged period of time. In addition to active map maintenance, our approach leverages the map's semantic richness with LLM-based reasoning for open-vocabulary object-goal navigation. This enables the robot to search more efficiently by prioritizing contextually relevant areas. We evaluate our approach across multiple real-world semi-static environments. Our system detects 95% of map changes on average, improving efficiency by more than 29% as compared to random and patrol baselines. Overall, our approach achieves a mapping precision within 2% of a fully rebuilt map while requiring substantially less exploration and further completes object goal navigation tasks about 14% faster than the next-best tested strategy (coverage patrolling). A video of our work can be found at this http URL .", 'abstract_zh': '现实环境中（如家庭）部署的机器人不仅要安全导航，还要理解其环境并适应环境变化。为了高效执行任务，它们必须构建并维护一个准确反映当前环境状态的语义地图。现有基于语义探索的研究主要集中在静态场景，而忽略了持续的对象级实例跟踪。然而，在现实世界的机器人应用中，环境中的物体可能会被移除、重新引入或随着时间的推移而移动，因此一致的地图至关重要。在此工作中，为了弥补这一差距，我们提出了一种适用于半静态环境的开放词汇语义探索系统。该系统通过构建对象实例稳定性的概率模型、系统地跟踪半静态变化、并主动探索长时间未被访问的区域，来维护一致的地图。除了积极维护地图，我们的方法还利用基于LLM的推理来利用地图的语义丰富性进行开放词汇对象目标导航。这使得机器人可以通过优先考虑上下文相关区域来更有效地进行搜索。我们在多个真实世界的半静态环境中评估了我们的方法。我们的系统平均检测出95%的地图变化，与随机巡逻基线相比，效率提高了超过29%。总体而言，我们的方法在映射精度方面与完全重建的地图相差仅2%以内，同时探索需求大大减少，并且比测试的最佳策略（覆盖巡逻）快约14%完成对象目标导航任务。', 'title_zh': '我在哪里留下了我的眼镜？半静态现实环境中的开放词汇语义探索'}
{'arxiv_id': 'arXiv:2509.19804', 'title': 'DynaFlow: Dynamics-embedded Flow Matching for Physically Consistent Motion Generation from State-only Demonstrations', 'authors': 'Sowoo Lee, Dongyun Kang, Jaehyun Park, Hae-Won Park', 'link': 'https://arxiv.org/abs/2509.19804', 'abstract': 'This paper introduces DynaFlow, a novel framework that embeds a differentiable simulator directly into a flow matching model. By generating trajectories in the action space and mapping them to dynamically feasible state trajectories via the simulator, DynaFlow ensures all outputs are physically consistent by construction. This end-to-end differentiable architecture enables training on state-only demonstrations, allowing the model to simultaneously generate physically consistent state trajectories while inferring the underlying action sequences required to produce them. We demonstrate the effectiveness of our approach through quantitative evaluations and showcase its real-world applicability by deploying the generated actions onto a physical Go1 quadruped robot. The robot successfully reproduces diverse gait present in the dataset, executes long-horizon motions in open-loop control and translates infeasible kinematic demonstrations into dynamically executable, stylistic behaviors. These hardware experiments validate that DynaFlow produces deployable, highly effective motions on real-world hardware from state-only demonstrations, effectively bridging the gap between kinematic data and real-world execution.', 'abstract_zh': '本论文介绍了DynaFlow这一新颖框架，该框架直接将可微分模拟器嵌入到流匹配模型中。通过在动作空间中生成轨迹并借助模拟器将其映射为动态可行的状态轨迹，DynaFlow通过设计确保所有输出均具有物理一致性。这一端到端的可微分架构允许仅基于状态示例进行训练，使模型能够在生成物理一致的状态轨迹的同时推断出产生这些轨迹所需的动作序列。通过定量评估展示了该方法的有效性，并通过将生成的动作应用于物理Go1四足机器人展示了其实用性。机器人成功再现了数据集中多种步态，执行了开环控制下的长时序动作，并将不可能的运动学示例转换为动态可执行且具有风格化的行为。这些硬件实验验证了DynaFlow能够从仅状态示例中生成在实际硬件上可部署且高效的运动，从而有效弥合了运动学数据与实际执行之间的差距。', 'title_zh': 'DynaFlow：嵌入动力学的流匹配方法，用于来自状态演示的一致运动生成'}
{'arxiv_id': 'arXiv:2509.19752', 'title': 'Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training', 'authors': 'Rushuai Yang, Hangxing Wei, Ran Zhang, Zhiyuan Feng, Xiaoyu Chen, Tong Li, Chuheng Zhang, Li Zhao, Jiang Bian, Xiu Su, Yi Chen', 'link': 'https://arxiv.org/abs/2509.19752', 'abstract': 'Vision-language-action (VLA) models have shown strong generalization across tasks and embodiments; however, their reliance on large-scale human demonstrations limits their scalability owing to the cost and effort of manual data collection. Reinforcement learning (RL) offers a potential alternative to generate demonstrations autonomously, yet conventional RL algorithms often struggle on long-horizon manipulation tasks with sparse rewards. In this paper, we propose a modified diffusion policy optimization algorithm to generate high-quality and low-variance trajectories, which contributes to a diffusion RL-powered VLA training pipeline. Our algorithm benefits from not only the high expressiveness of diffusion models to explore complex and diverse behaviors but also the implicit regularization of the iterative denoising process to yield smooth and consistent demonstrations. We evaluate our approach on the LIBERO benchmark, which includes 130 long-horizon manipulation tasks, and show that the generated trajectories are smoother and more consistent than both human demonstrations and those from standard Gaussian RL policies. Further, training a VLA model exclusively on the diffusion RL-generated data achieves an average success rate of 81.9%, which outperforms the model trained on human data by +5.3% and that on Gaussian RL-generated data by +12.6%. The results highlight our diffusion RL as an effective alternative for generating abundant, high-quality, and low-variance demonstrations for VLA models.', 'abstract_zh': '视觉-语言-行动（VLA）模型在跨任务和实体的泛化能力上表现出色，但由于大规模人工演示数据收集的成本和努力限制了其可扩展性，基于强化学习（RL）的方法提供了自主生成演示的潜在替代方案，然而，传统的RL算法在具有稀疏奖励的长时效应操作任务上往往表现不佳。本文提出了一种改进的扩散策略优化算法，以生成高质量和低方差的轨迹，这促进了基于扩散RL的VLA训练管道的发展。我们的算法不仅受益于扩散模型的高度表现力，可以探索复杂多样的行为，还受益于迭代去噪过程中的隐式正则化，以产生平滑一致的演示。我们使用包含130个长时效应操作任务的LIBERO基准对我们的方法进行了评估，结果显示生成的轨迹比人工演示和标准高斯RL策略生成的轨迹更平滑、更一致。进一步地，仅使用扩散RL生成的数据训练VLA模型，其平均成功率为81.9%，分别比使用人工数据训练的模型高出5.3%和比使用高斯RL生成数据训练的模型高出12.6%。结果表明，我们的扩散RL是一种有效的方法，可以为VLA模型生成丰富的、高质量的和低方差的演示。', 'title_zh': '超越人类示范：基于扩散的强化学习生成VLA训练数据'}
{'arxiv_id': 'arXiv:2509.19734', 'title': 'Trajectory Planning Using Safe Ellipsoidal Corridors as Projections of Orthogonal Trust Regions', 'authors': 'Akshay Jaitly, Jon Arrizabalaga, Guanrui Li', 'link': 'https://arxiv.org/abs/2509.19734', 'abstract': 'Planning collision free trajectories in complex environments remains a core challenge in robotics. Existing corridor based planners which rely on decomposition of the free space into collision free subsets scale poorly with environmental complexity and require explicit allocations of time windows to trajectory segments. We introduce a new trajectory parameterization that represents trajectories in a nonconvex collision free corridor as being in a convex cartesian product of balls. This parameterization allows us to decouple problem size from geometric complexity of the solution and naturally avoids explicit time allocation by allowing trajectories to evolve continuously inside ellipsoidal corridors. Building on this representation, we formulate the Orthogonal Trust Region Problem (Orth-TRP), a specialized convex program with separable block constraints, and develop a solver that exploits this parallel structure and the unique structure of each parallel subproblem for efficient optimization. Experiments on a quadrotor trajectory planning benchmark show that our approach produces smoother trajectories and lower runtimes than state-of-the-art corridor based planners, especially in highly complicated environments.', 'abstract_zh': '在复杂环境中的碰撞自由轨迹规划仍然是机器人领域的核心挑战。现有的基于走廊的规划方法依赖于将自由空间分解为碰撞自由子集，随着环境复杂性的增加，这些方法的可扩展性较差，并且需要为轨迹段明确分配时间窗口。我们提出了一种新的轨迹参数化方法，将碰撞自由走廊表示为凸笛卡尔球体乘积。这种方法允许我们将问题规模与解决方案的几何复杂性解耦，并且自然地避免了为轨迹段明确分配时间窗口，因为轨迹可以在椭球走廊内连续演化。基于这种表示，我们提出了正交信赖域问题（Orth-TRP），这是一种具有可分块约束的特殊凸规划，并开发了一个利用这种并行结构和每个并行子问题的独特结构进行高效优化的求解器。实验研究表明，在四旋翼飞行器轨迹规划基准测试中，我们的方法比最先进的基于走廊的方法产生了更平滑的轨迹并具有更低的运行时间，尤其是在高度复杂的环境中。', 'title_zh': '使用正交信任区域投影的安全椭球走廊轨迹规划'}
{'arxiv_id': 'arXiv:2509.19732', 'title': 'Simultaneous estimation of contact position and tool shape with high-dimensional parameters using force measurements and particle filtering', 'authors': 'Kyo Kutsuzawa, Mitsuhiro Hayashibe', 'link': 'https://arxiv.org/abs/2509.19732', 'abstract': 'Estimating the contact state between a grasped tool and the environment is essential for performing contact tasks such as assembly and object manipulation. Force signals are valuable for estimating the contact state, as they can be utilized even when the contact location is obscured by the tool. Previous studies proposed methods for estimating contact positions using force/torque signals; however, most methods require the geometry of the tool surface to be known. Although several studies have proposed methods that do not require the tool shape, these methods require considerable time for estimation or are limited to tools with low-dimensional shape parameters. Here, we propose a method for simultaneously estimating the contact position and tool shape, where the tool shape is represented by a grid, which is high-dimensional (more than 1000 dimensional). The proposed method uses a particle filter in which each particle has individual tool shape parameters, thereby to avoid directly handling a high-dimensional parameter space. The proposed method is evaluated through simulations and experiments using tools with curved shapes on a plane. Consequently, the proposed method can estimate the shape of the tool simultaneously with the contact positions, making the contact-position estimation more accurate.', 'abstract_zh': '估计被握住的工具与环境之间的接触状态对于执行装配和物体操作等接触任务至关重要。力信号对于估计接触状态很有价值，即使接触位置被工具遮挡时也能够利用。前人研究提出了基于力/力矩信号估计接触位置的方法；然而，大多数方法要求知道工具表面的几何形状。虽然有一些研究提出了不需要知道工具形状的方法，但这些方法需要较长的估计时间，或者只能应用于具有低维形状参数的工具。在这里，我们提出了一种同时估计接触位置和工具形状的方法，其中工具形状用网格表示，具有高维（超过1000维）的参数。所提方法使用了粒子滤波器，每个粒子具有独立的工具形状参数，从而避免直接处理高维参数空间。所提方法通过在平面上使用曲线形状的工具进行仿真和实验进行评估。结果表明，所提方法可以同时估计工具的形状和接触位置，从而提高接触位置估计的准确性。', 'title_zh': '基于力测量和粒子滤波的高维参数同时估计刀具位置和形状'}
{'arxiv_id': 'arXiv:2509.19725', 'title': 'Towards Autonomous Robotic Electrosurgery via Thermal Imaging', 'authors': 'Naveed D. Riaziat, Joseph Chen, Axel Krieger, Jeremy D. Brown', 'link': 'https://arxiv.org/abs/2509.19725', 'abstract': 'Electrosurgery is a surgical technique that can improve tissue cutting by reducing cutting force and bleeding. However, electrosurgery adds a risk of thermal injury to surrounding tissue. Expert surgeons estimate desirable cutting velocities based on experience but have no quantifiable reference to indicate if a particular velocity is optimal. Furthermore, prior demonstrations of autonomous electrosurgery have primarily used constant tool velocity, which is not robust to changes in electrosurgical tissue characteristics, power settings, or tool type. Thermal imaging feedback provides information that can be used to reduce thermal injury while balancing cutting force by controlling tool velocity. We introduce Thermography for Electrosurgical Rate Modulation via Optimization (ThERMO) to autonomously reduce thermal injury while balancing cutting force by intelligently controlling tool velocity. We demonstrate ThERMO in tissue phantoms and compare its performance to the constant velocity approach. Overall, ThERMO improves cut success rate by a factor of three and can reduce peak cutting force by a factor of two. ThERMO responds to varying environmental disturbances, reduces damage to tissue, and completes cutting tasks that would otherwise result in catastrophic failure for the constant velocity approach.', 'abstract_zh': '基于热图像的电外科速率优化热保护方法（ThERMO）', 'title_zh': '基于热成像的自主机器人电外科研究'}
{'arxiv_id': 'arXiv:2509.19712', 'title': 'TopoCut: Learning Multi-Step Cutting with Spectral Rewards and Discrete Diffusion Policies', 'authors': 'Liquan Wang, Jiangjie Bian, Eric Heiden, Animesh Garg', 'link': 'https://arxiv.org/abs/2509.19712', 'abstract': 'Robotic manipulation tasks involving cutting deformable objects remain challenging due to complex topological behaviors, difficulties in perceiving dense object states, and the lack of efficient evaluation methods for cutting outcomes. In this paper, we introduce TopoCut, a comprehensive benchmark for multi-step robotic cutting tasks that integrates a cutting environment and generalized policy learning. TopoCut is built upon three core components: (1) We introduce a high-fidelity simulation environment based on a particle-based elastoplastic solver with compliant von Mises constitutive models, augmented by a novel damage-driven topology discovery mechanism that enables accurate tracking of multiple cutting pieces. (2) We develop a comprehensive reward design that integrates the topology discovery with a pose-invariant spectral reward model based on Laplace-Beltrami eigenanalysis, facilitating consistent and robust assessment of cutting quality. (3) We propose an integrated policy learning pipeline, where a dynamics-informed perception module predicts topological evolution and produces particle-wise, topology-aware embeddings to support PDDP (Particle-based Score-Entropy Discrete Diffusion Policy) for goal-conditioned policy learning. Extensive experiments demonstrate that TopoCut supports trajectory generation, scalable learning, precise evaluation, and strong generalization across diverse object geometries, scales, poses, and cutting goals.', 'abstract_zh': '涉及切削变形物体的机器人操作任务由于复杂的拓扑行为、密集物体状态感知的难度以及切割结果评估方法的缺乏而具有挑战性。本文介绍了TopoCut，一个集成切削环境和广义策略学习的多步骤机器人切削任务全面基准。TopoCut基于三个核心组件构建：（1）我们引入了一个基于颗粒基弹塑性求解器的高保真模拟环境，并结合了一种新颖的损伤驱动拓扑发现机制，能够准确跟踪多个切削件。（2）我们开发了全面的奖励设计，将拓扑发现与基于拉普拉斯-贝尔特拉米特征分析的姿势不变的频谱奖励模型结合起来，促进切割质量的一致性和稳健评估。（3）我们提出了一种集成策略学习流水线，其中动态信息感知模块预测拓扑演化并生成颗粒级别的、拓扑意识的嵌入，以支持基于PDDP（基于颗粒的评分-熵离散扩散策略）的目标条件策略学习。广泛实验表明，TopoCut 支持轨迹生成、可扩展学习、精确评估和在不同物体几何形状、尺度、姿态和切割目标下的强大泛化。', 'title_zh': 'TopoCut: 学习多步切割的光谱奖励与离散扩散策略'}
{'arxiv_id': 'arXiv:2509.19696', 'title': 'Diffusion-Based Impedance Learning for Contact-Rich Manipulation Tasks', 'authors': 'Noah Geiger, Tamim Asfour, Neville Hogan, Johannes Lachner', 'link': 'https://arxiv.org/abs/2509.19696', 'abstract': 'Learning methods excel at motion generation in the information domain but are not primarily designed for physical interaction in the energy domain. Impedance Control shapes physical interaction but requires task-aware tuning by selecting feasible impedance parameters. We present Diffusion-Based Impedance Learning, a framework that combines both domains. A Transformer-based Diffusion Model with cross-attention to external wrenches reconstructs a simulated Zero-Force Trajectory (sZFT). This captures both translational and rotational task-space behavior. For rotations, we introduce a novel SLERP-based quaternion noise scheduler that ensures geometric consistency. The reconstructed sZFT is then passed to an energy-based estimator that updates stiffness and damping parameters. A directional rule is applied that reduces impedance along non task axes while preserving rigidity along task directions. Training data were collected for a parkour scenario and robotic-assisted therapy tasks using teleoperation with Apple Vision Pro. With only tens of thousands of samples, the model achieved sub-millimeter positional accuracy and sub-degree rotational accuracy. Its compact model size enabled real-time torque control and autonomous stiffness adaptation on a KUKA LBR iiwa robot. The controller achieved smooth parkour traversal within force and velocity limits and 30/30 success rates for cylindrical, square, and star peg insertions without any peg-specific demonstrations in the training data set. All code for the Transformer-based Diffusion Model, the robot controller, and the Apple Vision Pro telemanipulation framework is publicly available. These results mark an important step towards Physical AI, fusing model-based control for physical interaction with learning-based methods for trajectory generation.', 'abstract_zh': '基于扩散的学习阻抗控制框架：融合信息域和能量域的学习方法', 'title_zh': '基于扩散的阻抗学习用于接触丰富操作任务'}
{'arxiv_id': 'arXiv:2509.19688', 'title': 'Formal Safety Verification and Refinement for Generative Motion Planners via Certified Local Stabilization', 'authors': 'Devesh Nath, Haoran Yin, Glen Chou', 'link': 'https://arxiv.org/abs/2509.19688', 'abstract': 'We present a method for formal safety verification of learning-based generative motion planners. Generative motion planners (GMPs) offer advantages over traditional planners, but verifying the safety and dynamic feasibility of their outputs is difficult since neural network verification (NNV) tools scale only to a few hundred neurons, while GMPs often contain millions. To preserve GMP expressiveness while enabling verification, our key insight is to imitate the GMP by stabilizing references sampled from the GMP with a small neural tracking controller and then applying NNV to the closed-loop dynamics. This yields reachable sets that rigorously certify closed-loop safety, while the controller enforces dynamic feasibility. Building on this, we construct a library of verified GMP references and deploy them online in a way that imitates the original GMP distribution whenever it is safe to do so, improving safety without retraining. We evaluate across diverse planners, including diffusion, flow matching, and vision-language models, improving safety in simulation (on ground robots and quadcopters) and on hardware (differential-drive robot).', 'abstract_zh': '基于学习的生成运动规划形式化安全验证方法', 'title_zh': '基于认证局部稳定性的生成运动规划器的形式安全验证与细化'}
{'arxiv_id': 'arXiv:2509.19672', 'title': 'Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains', 'authors': 'Dongzhe Zheng, Wenjie Mei', 'link': 'https://arxiv.org/abs/2509.19672', 'abstract': 'Stochastic optimal control methods often struggle in complex non-convex landscapes, frequently becoming trapped in local optima due to their inability to learn from historical trajectory data. This paper introduces Memory-Augmented Potential Field Theory, a unified mathematical framework that integrates historical experience into stochastic optimal control. Our approach dynamically constructs memory-based potential fields that identify and encode key topological features of the state space, enabling controllers to automatically learn from past experiences and adapt their optimization strategy. We provide a theoretical analysis showing that memory-augmented potential fields possess non-convex escape properties, asymptotic convergence characteristics, and computational efficiency. We implement this theoretical framework in a Memory-Augmented Model Predictive Path Integral (MPPI) controller that demonstrates significantly improved performance in challenging non-convex environments. The framework represents a generalizable approach to experience-based learning within control systems (especially robotic dynamics), enhancing their ability to navigate complex state spaces without requiring specialized domain knowledge or extensive offline training.', 'abstract_zh': '增强记忆的潜在场理论：一种将历史经验融入随机最优控制的统一数学框架', 'title_zh': '记忆增强潜力场理论：非凸域自适应控制的框架'}
{'arxiv_id': 'arXiv:2509.19658', 'title': 'RoboSSM: Scalable In-context Imitation Learning via State-Space Models', 'authors': 'Youngju Yoo, Jiaheng Hu, Yifeng Zhu, Bo Liu, Qiang Liu, Roberto Martín-Martín, Peter Stone', 'link': 'https://arxiv.org/abs/2509.19658', 'abstract': 'In-context imitation learning (ICIL) enables robots to learn tasks from prompts consisting of just a handful of demonstrations. By eliminating the need for parameter updates at deployment time, this paradigm supports few-shot adaptation to novel tasks. However, recent ICIL methods rely on Transformers, which have computational limitations and tend to underperform when handling longer prompts than those seen during training. In this work, we introduce RoboSSM, a scalable recipe for in-context imitation learning based on state-space models (SSM). Specifically, RoboSSM replaces Transformers with Longhorn -- a state-of-the-art SSM that provides linear-time inference and strong extrapolation capabilities, making it well-suited for long-context prompts. We evaluate our approach on the LIBERO benchmark and compare it against strong Transformer-based ICIL baselines. Experiments show that RoboSSM extrapolates effectively to varying numbers of in-context demonstrations, yields high performance on unseen tasks, and remains robust in long-horizon scenarios. These results highlight the potential of SSMs as an efficient and scalable backbone for ICIL. Our code is available at this https URL.', 'abstract_zh': '基于状态空间模型的在上下文模仿学习（RoboSSM）：一种可扩展的方法', 'title_zh': 'RoboSSM: 基于状态空间模型的可扩展上下文模仿学习'}
{'arxiv_id': 'arXiv:2509.19636', 'title': 'Minimalistic Autonomous Stack for High-Speed Time-Trial Racing', 'authors': 'Mahmoud Ali, Hassan Jardali, Youwei Yu, Durgakant Pushp, Lantao Liu', 'link': 'https://arxiv.org/abs/2509.19636', 'abstract': "Autonomous racing has seen significant advancements, driven by competitions such as the Indy Autonomous Challenge (IAC) and the Abu Dhabi Autonomous Racing League (A2RL). However, developing an autonomous racing stack for a full-scale car is often constrained by limited access to dedicated test tracks, restricting opportunities for real-world validation. While previous work typically requires extended development cycles and significant track time, this paper introduces a minimalistic autonomous racing stack for high-speed time-trial racing that emphasizes rapid deployment and efficient system integration with minimal on-track testing. The proposed stack was validated on real speedways, achieving a top speed of 206 km/h within just 11 hours' practice run on the track with 325 km in total. Additionally, we present the system performance analysis, including tracking accuracy, vehicle dynamics, and safety considerations, offering insights for teams seeking to rapidly develop and deploy an autonomous racing stack with limited track access.", 'abstract_zh': '自主赛车技术取得了显著进步，得益于如印第自主挑战赛（IAC）和阿布扎比自主赛车联盟（A2RL）等比赛的推动。然而，开发适用于全尺寸汽车的自主赛车系统通常受限于专用测试赛道的有限访问权限，限制了实地验证的机会。以往的工作通常需要较长的研发周期和大量的赛道时间，本论文介绍了一种 minimalist 自主赛车堆栈，强调快速部署和高效的系统集成，并最大限度减少赛道上的测试。所提出的堆栈已在实际赛车场上进行了验证，仅用11小时的练习运行便达到了206 km/h的最高速度，总计行驶325 km。此外，我们还呈现了系统的性能分析，包括跟踪精度、车辆动力学和安全性考虑，为希望在有限的赛道访问权限下快速开发和部署自主赛车堆栈的团队提供了见解。', 'title_zh': '高速计时赛的极简自主堆栈'}
{'arxiv_id': 'arXiv:2509.19626', 'title': 'EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data', 'authors': 'Ryan Punamiya, Dhruv Patel, Patcharapong Aphiwetsa, Pranav Kuppili, Lawrence Y. Zhu, Simar Kareer, Judy Hoffman, Danfei Xu', 'link': 'https://arxiv.org/abs/2509.19626', 'abstract': 'Egocentric human experience data presents a vast resource for scaling up end-to-end imitation learning for robotic manipulation. However, significant domain gaps in visual appearance, sensor modalities, and kinematics between human and robot impede knowledge transfer. This paper presents EgoBridge, a unified co-training framework that explicitly aligns the policy latent spaces between human and robot data using domain adaptation. Through a measure of discrepancy on the joint policy latent features and actions based on Optimal Transport (OT), we learn observation representations that not only align between the human and robot domain but also preserve the action-relevant information critical for policy learning. EgoBridge achieves a significant absolute policy success rate improvement by 44% over human-augmented cross-embodiment baselines in three real-world single-arm and bimanual manipulation tasks. EgoBridge also generalizes to new objects, scenes, and tasks seen only in human data, where baselines fail entirely. Videos and additional information can be found at this https URL', 'abstract_zh': '以自我为中心的人类体验数据为机器人操作的端到端 imitation learning 扩容提供了巨大的资源。然而，人类和机器人在视觉外观、传感器模态和运动学之间的显著领域差距阻碍了知识迁移。本文提出了 EgoBridge，一种统一的协同训练框架，通过领域适应显式对齐人类和机器人数据的策略潜在空间。基于最优传输（Optimal Transport，OT）在联合策略潜在特征和动作之间的不一致性度量中，我们学习了既能对齐人类和机器人领域又能保留对策略学习至关重要的动作相关信息的观测表示。在三个真实世界的单臂和双臂操作任务中，EgoBridge 的绝对策略成功率相较于基于人类数据增强的跨主体基线显著提高了 44%。EgoBridge 还可以泛化到仅在人类数据中出现的新物体、场景和任务，而基线则完全失败。更多视频和详细信息请参见此链接：[补充链接文字]。', 'title_zh': 'EgoBridge: 基于第一人称人类数据的泛化imitation学习域适应'}
{'arxiv_id': 'arXiv:2509.19610', 'title': 'Look as You Leap: Planning Simultaneous Motion and Perception for High-DOF Robots', 'authors': 'Qingxi Meng, Emiliano Flores, Carlos Quintero-Peña, Peizhu Qian, Zachary Kingston, Shannan K. Hamlin, Vaibhav Unhelkar, Lydia E. Kavraki', 'link': 'https://arxiv.org/abs/2509.19610', 'abstract': 'In this work, we address the problem of planning robot motions for a high-degree-of-freedom (DoF) robot that effectively achieves a given perception task while the robot and the perception target move in a dynamic environment. Achieving navigation and perception tasks simultaneously is challenging, as these objectives often impose conflicting requirements. Existing methods that compute motion under perception constraints fail to account for obstacles, are designed for low-DoF robots, or rely on simplified models of perception. Furthermore, in dynamic real-world environments, robots must replan and react quickly to changes and directly evaluating the quality of perception (e.g., object detection confidence) is often expensive or infeasible at runtime. This problem is especially important in human-centered environments such as homes and hospitals, where effective perception is essential for safe and reliable operation. To address these challenges, we propose a GPU-parallelized perception-score-guided probabilistic roadmap planner with a neural surrogate model (PS-PRM). The planner explicitly incorporates the estimated quality of a perception task into motion planning for high-DoF robots. Our method uses a learned model to approximate perception scores and leverages GPU parallelism to enable efficient online replanning in dynamic settings. We demonstrate that our planner, evaluated on high-DoF robots, outperforms baseline methods in both static and dynamic environments in both simulation and real-robot experiments.', 'abstract_zh': '基于感知评分指导的并行概率路障规划方法（用于高自由度机器人在动态环境下的运动规划）', 'title_zh': '凌空跃起时凝视：为高自由度机器人同时规划运动与感知'}
{'arxiv_id': 'arXiv:2509.19597', 'title': 'From Space to Time: Enabling Adaptive Safety with Learned Value Functions via Disturbance Recasting', 'authors': 'Sander Tonkens, Nikhil Uday Shinde, Azra Begzadić, Michael C. Yip, Jorge Cortés, Sylvia L. Herbert', 'link': 'https://arxiv.org/abs/2509.19597', 'abstract': 'The widespread deployment of autonomous systems in safety-critical environments such as urban air mobility hinges on ensuring reliable, performant, and safe operation under varying environmental conditions. One such approach, value function-based safety filters, minimally modifies a nominal controller to ensure safety. Recent advances leverage offline learned value functions to scale these safety filters to high-dimensional systems. However, these methods assume detailed priors on all possible sources of model mismatch, in the form of disturbances in the environment -- information that is rarely available in real world settings. Even in well-mapped environments like urban canyons or industrial sites, drones encounter complex, spatially-varying disturbances arising from payload-drone interaction, turbulent airflow, and other environmental factors. We introduce SPACE2TIME, which enables safe and adaptive deployment of offline-learned safety filters under unknown, spatially-varying disturbances. The key idea is to reparameterize spatial variations in disturbance as temporal variations, enabling the use of precomputed value functions during online operation. We validate SPACE2TIME on a quadcopter through extensive simulations and hardware experiments, demonstrating significant improvement over baselines.', 'abstract_zh': '基于值函数的安全滤波器在不同空间变化干扰下的安全自适应部署', 'title_zh': '从空间到时间：通过干扰重铸实现基于学习价值函数的自适应安全'}
{'arxiv_id': 'arXiv:2509.19579', 'title': 'Terra: Hierarchical Terrain-Aware 3D Scene Graph for Task-Agnostic Outdoor Mapping', 'authors': 'Chad R. Samuelson, Abigail Austin, Seth Knoop, Blake Romrell, Gabriel R. Slade, Timothy W. McLain, Joshua G. Mangelson', 'link': 'https://arxiv.org/abs/2509.19579', 'abstract': 'Outdoor intelligent autonomous robotic operation relies on a sufficiently expressive map of the environment. Classical geometric mapping methods retain essential structural environment information, but lack a semantic understanding and organization to allow high-level robotic reasoning. 3D scene graphs (3DSGs) address this limitation by integrating geometric, topological, and semantic relationships into a multi-level graph-based map. Outdoor autonomous operations commonly rely on terrain information either due to task-dependence or the traversability of the robotic platform. We propose a novel approach that combines indoor 3DSG techniques with standard outdoor geometric mapping and terrain-aware reasoning, producing terrain-aware place nodes and hierarchically organized regions for outdoor environments. Our method generates a task-agnostic metric-semantic sparse map and constructs a 3DSG from this map for downstream planning tasks, all while remaining lightweight for autonomous robotic operation. Our thorough evaluation demonstrates our 3DSG method performs on par with state-of-the-art camera-based 3DSG methods in object retrieval and surpasses them in region classification while remaining memory efficient. We demonstrate its effectiveness in diverse robotic tasks of object retrieval and region monitoring in both simulation and real-world environments.', 'abstract_zh': '室外智能自主机器人操作依赖于环境的充分表达地图。古典几何制图方法保留了环境的基本结构信息，但缺乏语义理解与组织，无法支持高层次的机器人推理。三维场景图（3Dscene graphs，3DSGs）通过将几何学、拓扑学和语义关系整合到多级图基地图中来解决这一局限。室外自主操作通常依赖于地形信息，要么出于任务相关性，要么由于机器人平台的通达性。我们提出了一种新的方法，将室内3DSG技术与标准室外几何制图和地形感知推理相结合，生成terrain-aware的地点节点和层次组织的区域，以支持室外环境。我们的方法生成了一种任务无关的度量语义稀疏地图，并从该地图构建三维场景图以支持下游规划任务，同时保持轻量级以适应自主机器人操作。我们的详尽评估表明，我们的3DSG方法在物体检索方面的性能与最新的基于相机的3DSG方法相当，在区域分类方面更为出色，并且保持了内存效率。我们展示了其在模拟和真实环境中的物体检索和区域监控等各种机器人任务中的有效性。', 'title_zh': 'Terra：层次地形感知3D场景图实现任务无关室外mapping'}
{'arxiv_id': 'arXiv:2509.19573', 'title': 'Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning', 'authors': 'Zachary Olkin, Kejun Li, William D. Compton, Aaron D. Ames', 'link': 'https://arxiv.org/abs/2509.19573', 'abstract': "Achieving highly dynamic behaviors on humanoid robots, such as running, requires controllers that are both robust and precise, and hence difficult to design. Classical control methods offer valuable insight into how such systems can stabilize themselves, but synthesizing real-time controllers for nonlinear and hybrid dynamics remains challenging. Recently, reinforcement learning (RL) has gained popularity for locomotion control due to its ability to handle these complex dynamics. In this work, we embed ideas from nonlinear control theory, specifically control Lyapunov functions (CLFs), along with optimized dynamic reference trajectories into the reinforcement learning training process to shape the reward. This approach, CLF-RL, eliminates the need to handcraft and tune heuristic reward terms, while simultaneously encouraging certifiable stability and providing meaningful intermediate rewards to guide learning. By grounding policy learning in dynamically feasible trajectories, we expand the robot's dynamic capabilities and enable running that includes both flight and single support phases. The resulting policy operates reliably on a treadmill and in outdoor environments, demonstrating robustness to disturbances applied to the torso and feet. Moreover, it achieves accurate global reference tracking utilizing only on-board sensors, making a critical step toward integrating these dynamic motions into a full autonomy stack.", 'abstract_zh': '实现类人机器人等动态行为（如跑步）需要既 robust 又精确的控制器，因此设计起来很困难。经典控制方法为理解此类系统如何实现自身稳定提供了宝贵的见解，但合成用于非线性和混合动力学的实时控制器仍具有挑战性。近年来，强化学习（RL）因其能够处理这些复杂动力学而受到了越来越多的关注。在本工作中，我们将非线性控制理论中的想法，特别是控制李雅普诺夫函数（CLFs），以及优化的动态参考轨迹嵌入到强化学习训练过程中，以塑造奖励。这种CLF-RL方法消除了手动设计和调整启发式奖励项的需求，同时促进了可验证的稳定性，并为引导学习提供了有意义的中间奖励。通过将策略学习基于动态可行的轨迹，我们扩展了机器人的动态能力，并使机器人能够在包括飞行和单支撑相在内的跑步中表现出色。所获得的策略在跑步机和户外环境中可靠运行，表现出对作用于躯干和脚的干扰的鲁棒性。此外，它仅使用机载传感器实现了精确的整体参考轨迹跟踪，为将这些动态动作整合到完整的自主性堆栈中迈出了关键一步。', 'title_zh': '追逐稳定性：基于控制李雅普诺夫函数引导的强化学习 humanoid 运动控制'}
{'arxiv_id': 'arXiv:2509.19571', 'title': 'Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action', 'authors': 'Sacha Morin, Kumaraditya Gupta, Mahtab Sandhu, Charlie Gauthier, Francesco Argenziano, Kirsty Ellis, Liam Paull', 'link': 'https://arxiv.org/abs/2509.19571', 'abstract': 'Executing open-ended natural language queries is a core problem in robotics. While recent advances in imitation learning and vision-language-actions models (VLAs) have enabled promising end-to-end policies, these models struggle when faced with complex instructions and new scenes. An alternative is to design an explicit scene representation as a queryable interface between the robot and the world, using query results to guide downstream motion planning. In this work, we present Agentic Scene Policies (ASP), an agentic framework that leverages the advanced semantic, spatial, and affordance-based querying capabilities of modern scene representations to implement a capable language-conditioned robot policy. ASP can execute open-vocabulary queries in a zero-shot manner by explicitly reasoning about object affordances in the case of more complex skills. Through extensive experiments, we compare ASP with VLAs on tabletop manipulation problems and showcase how ASP can tackle room-level queries through affordance-guided navigation, and a scaled-up scene representation. (Project page: this https URL)', 'abstract_zh': '执行开放式的自然语言查询是机器人技术中的核心问题。虽然近期在模仿学习和视觉-语言-动作模型（VLAs）方面的进展已经使端到端策略变得颇具前景，但这些模型在面对复杂的指示和新场景时表现不佳。一种替代方案是设计一个明确的场景表示作为机器人与世界之间的查询接口，利用查询结果指导后续的动作规划。在这项工作中，我们提出了有能行动态场景策略（Agentic Scene Policies, ASP），这是一种利用现代场景表示的高级语义、空间和利用基于查询能力的有能行动态场景策略框架，以实现一个基于语言的机器人策略。ASP 可以在零样本情况下执行开放式词汇查询，通过明确推理对象的利用能力来应对更复杂的技能。通过大量实验，我们将 ASP 与 VLAs 在桌面上的操作问题上进行了对比，并展示了ASP如何通过利用基于利用导航和扩展的场景表示来应对房间级别的查询。', 'title_zh': '代理场景策略：统一空间、语义和作用方式的机器人动作框架'}
{'arxiv_id': 'arXiv:2509.19555', 'title': 'AnySafe: Adapting Latent Safety Filters at Runtime via Safety Constraint Parameterization in the Latent Space', 'authors': 'Sankalp Agrawal, Junwon Seo, Kensuke Nakamura, Ran Tian, Andrea Bajcsy', 'link': 'https://arxiv.org/abs/2509.19555', 'abstract': "Recent works have shown that foundational safe control methods, such as Hamilton-Jacobi (HJ) reachability analysis, can be applied in the latent space of world models. While this enables the synthesis of latent safety filters for hard-to-model vision-based tasks, they assume that the safety constraint is known a priori and remains fixed during deployment, limiting the safety filter's adaptability across scenarios. To address this, we propose constraint-parameterized latent safety filters that can adapt to user-specified safety constraints at runtime. Our key idea is to define safety constraints by conditioning on an encoding of an image that represents a constraint, using a latent-space similarity measure. The notion of similarity to failure is aligned in a principled way through conformal calibration, which controls how closely the system may approach the constraint representation. The parameterized safety filter is trained entirely within the world model's imagination, treating any image seen by the model as a potential test-time constraint, thereby enabling runtime adaptation to arbitrary safety constraints. In simulation and hardware experiments on vision-based control tasks with a Franka manipulator, we show that our method adapts at runtime by conditioning on the encoding of user-specified constraint images, without sacrificing performance. Video results can be found on this https URL", 'abstract_zh': '近期的研究表明，基础的安全控制方法，如哈密尔顿-雅可比（HJ）可达性分析，可以应用于世界模型的潜在空间中。虽然这种方法允许为基于视觉且难以建模的任务生成潜在空间的安全滤波器，但它假设安全约束在部署前已知且在部署过程中保持不变，从而限制了安全滤波器在不同场景中的适应性。为解决这一问题，我们提出了参数化的潜在空间安全滤波器，可以在运行时根据用户指定的安全约束进行适应。我们的核心思想是通过基于潜在空间的相似度度量来条件编码表示约束的图像，定义安全约束。通过对称校准将失败相似性的观念以一种原则性的方式进行对齐，从而控制系统接近约束表示的程度。参数化的安全滤波器完全在世界模型的想象中进行训练，将模型所见的任何图像视为潜在的测试时约束，从而允许在运行时对任意安全约束进行适应。在使用弗兰卡操作器进行的基于视觉的控制任务的仿真和硬件实验中，我们展示了该方法在运行时通过条件编码用户指定的约束图像进行适应，同时不牺牲性能。更多视频结果，请访问这个网址：这个 https URL。', 'title_zh': 'AnySafe: 通过潜在空间安全性约束参数化在运行时适应潜在安全性过滤器'}
{'arxiv_id': 'arXiv:2509.19545', 'title': 'RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots', 'authors': 'Min Dai, Aaron D. Ames', 'link': 'https://arxiv.org/abs/2509.19545', 'abstract': "We present RoMoCo, an open-source C++ toolbox for the synthesis and evaluation of reduced-order model-based planners and whole-body controllers for bipedal and humanoid robots. RoMoCo's modular architecture unifies state-of-the-art planners and whole-body locomotion controllers under a consistent API, enabling rapid prototyping and reproducible benchmarking. By leveraging reduced-order models for platform-agnostic gait generation, RoMoCo enables flexible controller design across diverse robots. We demonstrate its versatility and performance through extensive simulations on the Cassie, Unitree H1, and G1 robots, and validate its real-world efficacy with hardware experiments on the Cassie and G1 humanoids.", 'abstract_zh': 'RoMoCo：一种开源C++工具箱，用于 bipedal 和 humanoid 机器人基于降阶模型的规划器和全身控制器的设计与评估', 'title_zh': 'RoMoCo: 用于 bipedal 和 humanoid 机器人基于降阶模型的运动控制工具箱'}
{'arxiv_id': 'arXiv:2509.19541', 'title': 'Autonomous Elemental Characterization Enabled by a Low Cost Robotic Platform Built Upon a Generalized Software Architecture', 'authors': 'Xuan Cao, Yuxin Wu, Michael L. Whittaker', 'link': 'https://arxiv.org/abs/2509.19541', 'abstract': 'Despite the rapidly growing applications of robots in industry, the use of robots to automate tasks in scientific laboratories is less prolific due to lack of generalized methodologies and high cost of hardware. This paper focuses on the automation of characterization tasks necessary for reducing cost while maintaining generalization, and proposes a software architecture for building robotic systems in scientific laboratory environment. A dual-layer (this http URL and ROS) action server design is the basic building block, which facilitates the implementation of a web-based front end for user-friendly operations and the use of ROS Behavior Tree for convenient task planning and execution. A robotic platform for automating mineral and material sample characterization is built upon the architecture, with an open source, low-cost three-axis computer numerical control gantry system serving as the main robot. A handheld laser induced breakdown spectroscopy (LIBS) analyzer is integrated with a 3D printed adapter, enabling automated 2D chemical mapping. We demonstrate the utility of automated chemical mapping by scanning of the surface of a spodumene-bearing pegmatite core sample with a 1071-point dense hyperspectral map acquired at a rate of 1520 bits per second. Automated LIBS scanning enables controlled chemical quantification in the laboratory that complements field-based measurements acquired with the same handheld device, linking resource exploration and processing steps in the supply chain for lithium-based battery materials.', 'abstract_zh': '尽管工业领域中机器人的应用快速增长，但由于缺乏通用方法和高昂的硬件成本，机器人在科学实验室中的自动化应用相对较少。本文专注于降低自动化成本同时保持通用性的表征任务自动化，并提出了一种适用于科学实验室环境的机器人系统软件架构。该架构采用两层（此链接和ROS）动作服务器设计，便于实现基于Web的前端操作界面以及使用ROS行为树进行便捷的任务规划与执行。在此架构基础上构建了一个用于矿物和材料样品表征的机器人平台，其中开源低成本的三轴计算机数控龙门架系统作为主要机器人。手持式激光诱导击穿光谱（LIBS）分析仪通过3D打印适配器集成，实现自动化的二维化学映射。通过以每秒1520位特率获取的包含1071个点的高光谱图对含锂辉石脉岩芯样品的表面进行扫描，展示了自动化化学映射的实用性。自动化LIBS扫描能够在实验室中实现可控的化学定量测量，补充手持设备在野外获得的测量结果，从而连接锂基电池材料资源勘探和处理步骤的供应链。', 'title_zh': '基于通用软件架构的低成本机器人平台实现自主元素表征'}
{'arxiv_id': 'arXiv:2509.19525', 'title': 'Real-Time Reinforcement Learning for Dynamic Tasks with a Parallel Soft Robot', 'authors': 'James Avtges, Jake Ketchum, Millicent Schlafly, Helena Young, Taekyoung Kim, Allison Pinosky, Ryan L. Truby, Todd D. Murphey', 'link': 'https://arxiv.org/abs/2509.19525', 'abstract': 'Closed-loop control remains an open challenge in soft robotics. The nonlinear responses of soft actuators under dynamic loading conditions limit the use of analytic models for soft robot control. Traditional methods of controlling soft robots underutilize their configuration spaces to avoid nonlinearity, hysteresis, large deformations, and the risk of actuator damage. Furthermore, episodic data-driven control approaches such as reinforcement learning (RL) are traditionally limited by sample efficiency and inconsistency across initializations. In this work, we demonstrate RL for reliably learning control policies for dynamic balancing tasks in real-time single-shot hardware deployments. We use a deformable Stewart platform constructed using parallel, 3D-printed soft actuators based on motorized handed shearing auxetic (HSA) structures. By introducing a curriculum learning approach based on expanding neighborhoods of a known equilibrium, we achieve reliable single-deployment balancing at arbitrary coordinates. In addition to benchmarking the performance of model-based and model-free methods, we demonstrate that in a single deployment, Maximum Diffusion RL is capable of learning dynamic balancing after half of the actuators are effectively disabled, by inducing buckling and by breaking actuators with bolt cutters. Training occurs with no prior data, in as fast as 15 minutes, with performance nearly identical to the fully-intact platform. Single-shot learning on hardware facilitates soft robotic systems reliably learning in the real world and will enable more diverse and capable soft robots.', 'abstract_zh': '软体机器人中的闭环控制仍然是一个开放挑战。软执行器在动态加载条件下的非线性响应限制了对软体机器人控制的分析模型的应用。传统方法在控制软体机器人时未充分利用其配置空间，以避免非线性、滞回现象、大变形以及执行器损坏的风险。此外，基于片断数据驱动的方法，如强化学习（RL），传统上由于样本效率低下和初始化一致性差而受到限制。在本文中，我们演示了在实时单片硬件部署中使用RL可靠地学习动力平衡任务的控制策略。我们使用基于电机驱动铰链膨胀（HSA）结构的并行3D打印软执行器构建的可变形Stewart平台。通过引入基于扩展已知平衡区邻域的 curriculum 学习方法，我们实现了在任意坐标处的可靠单次部署平衡。除了基准测试基于模型和非模型方法的性能之外，我们还展示了在单次部署中，最大扩散RL能够在半数执行器有效失效的情况下学习动力平衡，通过引发屈曲和使用扳手剪断执行器。培训无需任何先验数据，最快可在15分钟内完成，并且性能几乎与完整的平台相同。在硬件上的单次学习使得软体机器人系统能够在现实世界中可靠地学习，并将使软体机器人更加多样化和具备更强的能力。', 'title_zh': '并行软机器人中的实时强化学习动态任务'}
{'arxiv_id': 'arXiv:2509.19522', 'title': 'Bioinspired SLAM Approach for Unmanned Surface Vehicle', 'authors': 'Fabio Coelho, Joao Victor T. Borges, Paulo Padrao, Jose Fuentes, Ramon R. Costa, Liu Hsu, Leonardo Bobadilla', 'link': 'https://arxiv.org/abs/2509.19522', 'abstract': 'This paper presents OpenRatSLAM2, a new version of OpenRatSLAM - a bioinspired SLAM framework based on computational models of the rodent hippocampus. OpenRatSLAM2 delivers low-computation-cost visual-inertial based SLAM, suitable for GPS-denied environments. Our contributions include a ROS2-based architecture, experimental results on new waterway datasets, and insights into system parameter tuning. This work represents the first known application of RatSLAM on USVs. The estimated trajectory was compared with ground truth data using the Hausdorff distance. The results show that the algorithm can generate a semimetric map with an error margin acceptable for most robotic applications.', 'abstract_zh': 'OpenRatSLAM2：一种基于啮齿动物海马体计算模型的生物启发式SLAM框架的新版本', 'title_zh': '生物启发的自主水面车辆SLAM方法'}
{'arxiv_id': 'arXiv:2509.19521', 'title': 'A Bimanual Gesture Interface for ROS-Based Mobile Manipulators Using TinyML and Sensor Fusion', 'authors': 'Najeeb Ahmed Bhuiyan, M. Nasimul Huq, Sakib H. Chowdhury, Rahul Mangharam', 'link': 'https://arxiv.org/abs/2509.19521', 'abstract': 'Gesture-based control for mobile manipulators faces persistent challenges in reliability, efficiency, and intuitiveness. This paper presents a dual-hand gesture interface that integrates TinyML, spectral analysis, and sensor fusion within a ROS framework to address these limitations. The system uses left-hand tilt and finger flexion, captured using accelerometer and flex sensors, for mobile base navigation, while right-hand IMU signals are processed through spectral analysis and classified by a lightweight neural network. This pipeline enables TinyML-based gesture recognition to control a 7-DOF Kinova Gen3 manipulator. By supporting simultaneous navigation and manipulation, the framework improves efficiency and coordination compared to sequential methods. Key contributions include a bimanual control architecture, real-time low-power gesture recognition, robust multimodal sensor fusion, and a scalable ROS-based implementation. The proposed approach advances Human-Robot Interaction (HRI) for industrial automation, assistive robotics, and hazardous environments, offering a cost-effective, open-source solution with strong potential for real-world deployment and further optimization.', 'abstract_zh': '基于手势的移动 manipulator 控制面临可靠性和直观性等方面的持续挑战。本文提出了一种集成 TinyML、谱分析和传感器融合的双臂手势界面，以解决这些限制。该系统利用左臂倾斜和手指弯曲（通过加速度计和弯曲传感器捕获）进行移动基座导航，而右臂惯性测量单元（IMU）信号通过谱分析处理并由轻量级神经网络分类，从而实现基于 TinyML 的手势识别控制 7 自由度 Kinova Gen3 手臂。该框架通过同时支持导航和操作，提高了效率和协调性，相比于顺序方法。主要贡献包括双臂控制架构、实时低功耗手势识别、稳健的多模态传感器融合以及基于 ROS 的可扩展实现。所提出的方法推动了工业自动化、辅助机器人和危险环境中的人机交互（HRI），提供了一种成本效益高、开源的解决方案，在实际部署和进一步优化方面具有很强的潜力。', 'title_zh': '基于TinyML和传感器融合的ROSquila双手手势界面-Mobile操纵器'}
{'arxiv_id': 'arXiv:2509.19486', 'title': 'Supercomputing for High-speed Avoidance and Reactive Planning in Robots', 'authors': 'Kieran S. Lachmansingh, José R. González-Estrada, Ryan E. Grant, Matthew K. X. J. Pan', 'link': 'https://arxiv.org/abs/2509.19486', 'abstract': 'This paper presents SHARP (Supercomputing for High-speed Avoidance and Reactive Planning), a proof-of-concept study demonstrating how high-performance computing (HPC) can enable millisecond-scale responsiveness in robotic control. While modern robots face increasing demands for reactivity in human--robot shared workspaces, onboard processors are constrained by size, power, and cost. Offloading to HPC offers massive parallelism for trajectory planning, but its feasibility for real-time robotics remains uncertain due to network latency and jitter. We evaluate SHARP in a stress-test scenario where a 7-DOF manipulator must dodge high-speed foam projectiles. Using a parallelized multi-goal A* search implemented with MPI on both local and remote HPC clusters, the system achieves mean planning latencies of 22.9 ms (local) and 30.0 ms (remote, ~300 km away), with avoidance success rates of 84% and 88%, respectively. These results show that when round-trip latency remains within the tens-of-milliseconds regime, HPC-side computation is no longer the bottleneck, enabling avoidance well below human reaction times. The SHARP results motivate hybrid control architectures: low-level reflexes remain onboard for safety, while bursty, high-throughput planning tasks are offloaded to HPC for scalability. By reporting per-stage timing and success rates, this study provides a reproducible template for assessing real-time feasibility of HPC-driven robotics. Collectively, SHARP reframes HPC offloading as a viable pathway toward dependable, reactive robots in dynamic environments.', 'abstract_zh': '基于超级计算的高速避障与反应规划（SHARP）原理研究', 'title_zh': '超算在机器人高速避障与反应规划中的应用'}
{'arxiv_id': 'arXiv:2509.19480', 'title': 'OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation', 'authors': 'Noriaki Hirose, Catherine Glossop, Dhruv Shah, Sergey Levine', 'link': 'https://arxiv.org/abs/2509.19480', 'abstract': 'Humans can flexibly interpret and compose different goal specifications, such as language instructions, spatial coordinates, or visual references, when navigating to a destination. In contrast, most existing robotic navigation policies are trained on a single modality, limiting their adaptability to real-world scenarios where different forms of goal specification are natural and complementary. In this work, we present a training framework for robotic foundation models that enables omni-modal goal conditioning for vision-based navigation. Our approach leverages a high-capacity vision-language-action (VLA) backbone and trains with three primary goal modalities: 2D poses, egocentric images, and natural language, as well as their combinations, through a randomized modality fusion strategy. This design not only expands the pool of usable datasets but also encourages the policy to develop richer geometric, semantic, and visual representations. The resulting model, OmniVLA, achieves strong generalization to unseen environments, robustness to scarce modalities, and the ability to follow novel natural language instructions. We demonstrate that OmniVLA outperforms specialist baselines across modalities and offers a flexible foundation for fine-tuning to new modalities and tasks. We believe OmniVLA provides a step toward broadly generalizable and flexible navigation policies, and a scalable path for building omni-modal robotic foundation models. We present videos showcasing OmniVLA performance and will release its checkpoints and training code on our project page.', 'abstract_zh': '人类在导航到目的地时能够灵活地解释和组合不同类型的目標 specifications，如语言指令、空间坐标或视觉参考。相比之下，现有的大多数机器人导航策略仅针对单一模态进行训练，限制了它们在真实世界场景中的适应性，而不同的目标 specifications 自然且互补。在此项工作中，我们提出了一种训练框架，使基于视觉的导航能够适应多模态目标 conditioning。我们的方法利用高容量的视觉-语言-动作（VLA）骨干网络，并通过随机模态融合策略训练三种主要的目标模态：2D 姿态、第一人称图像和自然语言，以及它们的组合。该设计不仅扩展了可用数据集的范围，还鼓励策略发展更丰富的几何、语义和视觉表示。结果模型 OmniVLA 在未见过的环境中有较强的泛化能力、对稀少模态的鲁棒性，并且能够遵循新的自然语言指令。我们展示了 OmniVLA 在不同模态下优于专门基准模型，并提供了向新模态和任务微调的灵活性基础。我们相信 OmniVLA 为广泛泛化和灵活的导航策略提供了一步进展，并为构建多模态机器人基础模型指明了可扩展的道路。我们展示了 OmniVLA 的性能视频，并将在项目页面上发布其检查点和训练代码。', 'title_zh': '全景VLA：一种用于机器人导航的跨模态视觉-语言-动作模型'}
{'arxiv_id': 'arXiv:2509.19473', 'title': 'Crater Observing Bio-inspired Rolling Articulator (COBRA)', 'authors': 'Adarsh Salagame, Henry Noyes, Alireza Ramezani, Eric Sihite, Arash Kalantari', 'link': 'https://arxiv.org/abs/2509.19473', 'abstract': "NASA aims to establish a sustainable human basecamp on the Moon as a stepping stone for future missions to Mars and beyond. The discovery of water ice on the Moon's craters located in permanently shadowed regions, which can provide drinking water, oxygen, and rocket fuel, is therefore of critical importance. However, current methods to access lunar ice deposits are limited. While rovers have been used to explore the lunar surface for decades, they face significant challenges in navigating harsh terrains, such as permanently shadowed craters, due to the high risk of immobilization. This report introduces COBRA (Crater Observing Bio-inspired Rolling Articulator), a multi-modal snake-style robot designed to overcome mobility challenges in Shackleton Crater's rugged environment. COBRA combines slithering and tumbling locomotion to adapt to various crater terrains. In snake mode, it uses sidewinding to traverse flat or low inclined surfaces, while in tumbling mode, it forms a circular barrel by linking its head and tail, enabling rapid movement with minimal energy on steep slopes. Equipped with an onboard computer, stereo camera, inertial measurement unit, and joint encoders, COBRA facilitates real-time data collection and autonomous operation. This paper highlights COBRAs robustness and efficiency in navigating extreme terrains through both simulations and experimental validation.", 'abstract_zh': 'NASA旨在建立一个可持续的人类基地，作为未来火星及其他更远深空任务的跳板。月球永久阴影区域坑洞中发现的水冰对于提供饮用水、氧气和火箭燃料至关重要，因此具有关键意义。然而，当前获取月球冰资源的方法有限。尽管探测车已用于数十年的月表探索，但它们在恶劣地形，如永久阴影坑洞中面临显著的移动挑战，因其被卡住的风险很高。本报告介绍了COBRA（Crater Observing Bio-inspired Rolling Articulator），一种多模式蛇形机器人，旨在克服谢克尔顿坑崎岖环境中的移动难题。COBRA结合了滑行和滚动运动，以适应各种坑洞地形。在蛇形模式下，它使用侧向迂回穿越平坦或低倾斜表面；在滚动模式下，它通过连接其头部和尾部形成一个圆筒状结构，从而在陡峭坡面实现快速、低能耗移动。配备机载计算机、立体摄像头、惯性测量单元和关节编码器，COBRA促进了实时数据收集和自主操作。本文通过模拟和实验验证突显了COBRA在极端地形中高效、稳健的导航能力。', 'title_zh': '基于生物启发滚动关节的撞击观测装置（COBRA）'}
{'arxiv_id': 'arXiv:2509.19463', 'title': 'CU-Multi: A Dataset for Multi-Robot Collaborative Perception', 'authors': 'Doncey Albin, Daniel McGann, Miles Mena, Annika Thomas, Harel Biggie, Xuefei Sun, Steve McGuire, Jonathan P. How, Christoffer Heckman', 'link': 'https://arxiv.org/abs/2509.19463', 'abstract': 'A central challenge for multi-robot systems is fusing independently gathered perception data into a unified representation. Despite progress in Collaborative SLAM (C-SLAM), benchmarking remains hindered by the scarcity of dedicated multi-robot datasets. Many evaluations instead partition single-robot trajectories, a practice that may only partially reflect true multi-robot operations and, more critically, lacks standardization, leading to results that are difficult to interpret or compare across studies. While several multi-robot datasets have recently been introduced, they mostly contain short trajectories with limited inter-robot overlap and sparse intra-robot loop closures. To overcome these limitations, we introduce CU-Multi, a dataset collected over multiple days at two large outdoor sites on the University of Colorado Boulder campus. CU-Multi comprises four synchronized runs with aligned start times and controlled trajectory overlap, replicating the distinct perspectives of a robot team. It includes RGB-D sensing, RTK GPS, semantic LiDAR, and refined ground-truth odometry. By combining overlap variation with dense semantic annotations, CU-Multi provides a strong foundation for reproducible evaluation in multi-robot collaborative perception tasks.', 'abstract_zh': '多机器人系统的一个关键挑战是将独立收集的感知数据融合为统一表示。尽管在协作SLAM（C-SLAM）方面取得了进展，但由于专用多机器人数据集的稀缺性，基准测试仍然受到阻碍。许多评估将单机器人轨迹分割，这种做法可能仅部分反映真正的多机器人操作，并且更关键的是缺乏标准化，导致结果难以在研究之间进行解释或比较。虽然最近引入了多个多机器人数据集，但它们主要包含有限的跨机器人重叠和稀疏的内部机器人回环闭合轨迹。为克服这些限制，我们引入了CU-Multi数据集，该数据集在科罗拉多大学 Boulder 校园的两个大型户外地点上收集自多日的数据。CU-Multi 包含四次同步运行，具有对齐的起始时间并控制轨迹重叠，复制了机器人团队的不同视角。它包括RGB-D传感、RTK GPS、语义LiDAR以及校准的地面真相里程计。通过结合重叠变化和密集的语义注释，CU-Multi 为多机器人协作感知任务的再现性评估提供了坚实的基础。', 'title_zh': 'CU-Multi: 一种多机器人协作感知数据集'}
{'arxiv_id': 'arXiv:2509.19460', 'title': 'Self-evolved Imitation Learning in Simulated World', 'authors': 'Yifan Ye, Jun Cen, Jing Chen, Zhihe Lu', 'link': 'https://arxiv.org/abs/2509.19460', 'abstract': 'Imitation learning has been a trend recently, yet training a generalist agent across multiple tasks still requires large-scale expert demonstrations, which are costly and labor-intensive to collect. To address the challenge of limited supervision, we propose Self-Evolved Imitation Learning (SEIL), a framework that progressively improves a few-shot model through simulator interactions. The model first attempts tasksin the simulator, from which successful trajectories are collected as new demonstrations for iterative refinement. To enhance the diversity of these demonstrations, SEIL employs dual-level augmentation: (i) Model-level, using an Exponential Moving Average (EMA) model to collaborate with the primary model, and (ii) Environment-level, introducing slight variations in initial object positions. We further introduce a lightweight selector that filters complementary and informative trajectories from the generated pool to ensure demonstration quality. These curated samples enable the model to achieve competitive performance with far fewer training examples. Extensive experiments on the LIBERO benchmark show that SEIL achieves a new state-of-the-art performance in few-shot imitation learning scenarios. Code is available at this https URL.', 'abstract_zh': '自我进化 imitation 学习（SEIL）：通过模拟器交互逐步提高 few-shot 模型性能', 'title_zh': '自进化模仿学习在仿真世界中'}
{'arxiv_id': 'arXiv:2509.19454', 'title': 'ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation', 'authors': 'Jason Chen, I-Chun Arthur Liu, Gaurav Sukhatme, Daniel Seita', 'link': 'https://arxiv.org/abs/2509.19454', 'abstract': 'Training robust bimanual manipulation policies via imitation learning requires demonstration data with broad coverage over robot poses, contacts, and scene contexts. However, collecting diverse and precise real-world demonstrations is costly and time-consuming, which hinders scalability. Prior works have addressed this with data augmentation, typically for either eye-in-hand (wrist camera) setups with RGB inputs or for generating novel images without paired actions, leaving augmentation for eye-to-hand (third-person) RGB-D training with new action labels less explored. In this paper, we propose Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation (ROPA), an offline imitation learning data augmentation method that fine-tunes Stable Diffusion to synthesize third-person RGB and RGB-D observations of novel robot poses. Our approach simultaneously generates corresponding joint-space action labels while employing constrained optimization to enforce physical consistency through appropriate gripper-to-object contact constraints in bimanual scenarios. We evaluate our method on 5 simulated and 3 real-world tasks. Our results across 2625 simulation trials and 300 real-world trials demonstrate that ROPA outperforms baselines and ablations, showing its potential for scalable RGB and RGB-D data augmentation in eye-to-hand bimanual manipulation. Our project website is available at: this https URL.', 'abstract_zh': '通过模仿学习训练 robust 双手操作策略需要广泛覆盖机器人姿态、接触和场景上下文的示范数据。然而，收集多样且精确的实时示范代价高昂且耗时，这限制了其可扩展性。先前的研究通过数据增强来解决这一问题，通常集中在带有 RGB 输入的眼在手（腕部相机）设置上，或生成新的图像而没有配对的操作，而对于眼到手（第三人称）RGB-D 训练的数据增强，带有新操作标签的情况则探索较少。本文提出 Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation (ROPA)，这是一种离线模仿学习数据增强方法，通过调整 Stable Diffusion 来合成第三人称的 RGB 和 RGB-D 观测值的新机器人姿态。我们的方法同时生成相应的关节空间操作标签，并通过合适的双手操作接触约束应用约束优化来确保物理一致性。我们在 5 个模拟和 3 个真实世界任务上评估了该方法。我们的结果表明，在 2625 个模拟试验和 300 个真实世界试验中，ROPA 比基线和消融实验更优，展示了其在眼到手双手操作中实现可扩展的 RGB 和 RGB-D 数据增强的潜力。我们的项目网站可通过以下链接访问：this https URL。', 'title_zh': 'ROPA：RGB-D 双手数据增强中的合成机器人姿态生成'}
{'arxiv_id': 'arXiv:2509.19452', 'title': 'HUNT: High-Speed UAV Navigation and Tracking in Unstructured Environments via Instantaneous Relative Frames', 'authors': 'Alessandro Saviolo, Jeffrey Mao, Giuseppe Loianno', 'link': 'https://arxiv.org/abs/2509.19452', 'abstract': 'Search and rescue operations require unmanned aerial vehicles to both traverse unknown unstructured environments at high speed and track targets once detected. Achieving both capabilities under degraded sensing and without global localization remains an open challenge. Recent works on relative navigation have shown robust tracking by anchoring planning and control to a visible detected object, but cannot address navigation when no target is in the field of view. We present HUNT (High-speed UAV Navigation and Tracking), a real-time framework that unifies traversal, acquisition, and tracking within a single relative formulation. HUNT defines navigation objectives directly from onboard instantaneous observables such as attitude, altitude, and velocity, enabling reactive high-speed flight during search. Once a target is detected, the same perception-control pipeline transitions seamlessly to tracking. Outdoor experiments in dense forests, container compounds, and search-and-rescue operations with vehicles and mannequins demonstrate robust autonomy where global methods fail.', 'abstract_zh': '高速无人 aerial 航空器导航与跟踪', 'title_zh': 'HUNT: 高速无人机在无结构环境中基于瞬时相对坐标帧的导航与跟踪'}
{'arxiv_id': 'arXiv:2509.20328', 'title': 'Video models are zero-shot learners and reasoners', 'authors': 'Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, Robert Geirhos', 'link': 'https://arxiv.org/abs/2509.20328', 'abstract': "The remarkable zero-shot capabilities of Large Language Models (LLMs) have propelled natural language processing from task-specific models to unified, generalist foundation models. This transformation emerged from simple primitives: large, generative models trained on web-scale data. Curiously, the same primitives apply to today's generative video models. Could video models be on a trajectory towards general-purpose vision understanding, much like LLMs developed general-purpose language understanding? We demonstrate that Veo 3 can solve a broad variety of tasks it wasn't explicitly trained for: segmenting objects, detecting edges, editing images, understanding physical properties, recognizing object affordances, simulating tool use, and more. These abilities to perceive, model, and manipulate the visual world enable early forms of visual reasoning like maze and symmetry solving. Veo's emergent zero-shot capabilities indicate that video models are on a path to becoming unified, generalist vision foundation models.", 'abstract_zh': '大型语言模型的卓越零样本能力已将自然语言处理从任务特定模型转变为统一的通用基础模型。这一转变源自简单的基础：在Web规模数据上训练的大规模生成模型。有趣的是，这些基础同样适用于今天生成视频的模型。视频模型是否正在朝向通用视觉理解发展，就像大型语言模型发展出通用语言理解能力一样？我们证明，Veo 3能够解决它未明确训练的任务：物体分割、边缘检测、图像编辑、理解物理属性、识别物体功能、模拟工具使用等。这些感知、建模和操控视觉世界的能力使视频模型能够进行早期形式的视觉推理，如迷宫和对称性解决。Veo的涌现零样本能力表明，视频模型正在通向统一的通用视觉基础模型的道路。', 'title_zh': '视频模型是零样本学习者和推理器'}
{'arxiv_id': 'arXiv:2509.20314', 'title': 'On Robustness of Consensus over Pseudo-Undirected Path Graphs', 'authors': 'Abhinav Sinha, Dwaipayan Mukherjee, Shashi Ranjan Kumar', 'link': 'https://arxiv.org/abs/2509.20314', 'abstract': 'Consensus over networked agents is typically studied using undirected or directed communication graphs. Undirected graphs enforce symmetry in information exchange, leading to convergence to the average of initial states, while directed graphs permit asymmetry but make consensus dependent on root nodes and their influence. Both paradigms impose inherent restrictions on achievable consensus values and network robustness. This paper introduces a theoretical framework for achieving consensus over a class of network topologies, termed pseudo-undirected graphs, which retains bidirectional connectivity between node pairs but allows the corresponding edge weights to differ, including the possibility of negative values under bounded conditions. The resulting Laplacian is generally non-symmetric, yet it guarantees consensus under connectivity assumptions, to expand the solution space, which enables the system to achieve a stable consensus value that can lie outside the convex hull of the initial state set. We derive admissibility bounds for negative weights for a pseudo-undirected path graph, and show an application in the simultaneous interception of a moving target.', 'abstract_zh': '基于伪无向图的网络代理共识研究', 'title_zh': '伪无向路径图上一致性算法的健壮性研究'}
{'arxiv_id': 'arXiv:2509.20107', 'title': 'Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models', 'authors': 'JuanaJuana Valeria Hurtado, Rohit Mohan, Abhinav Valada', 'link': 'https://arxiv.org/abs/2509.20107', 'abstract': 'Hyperspectral imaging (HSI) captures spatial information along with dense spectral measurements across numerous narrow wavelength bands. This rich spectral content has the potential to facilitate robust robotic perception, particularly in environments with complex material compositions, varying illumination, or other visually challenging conditions. However, current HSI semantic segmentation methods underperform due to their reliance on architectures and learning frameworks optimized for RGB inputs. In this work, we propose a novel hyperspectral adapter that leverages pretrained vision foundation models to effectively learn from hyperspectral data. Our architecture incorporates a spectral transformer and a spectrum-aware spatial prior module to extract rich spatial-spectral features. Additionally, we introduce a modality-aware interaction block that facilitates effective integration of hyperspectral representations and frozen vision Transformer features through dedicated extraction and injection mechanisms. Extensive evaluations on three benchmark autonomous driving datasets demonstrate that our architecture achieves state-of-the-art semantic segmentation performance while directly using HSI inputs, outperforming both vision-based and hyperspectral segmentation methods. We make the code available at this https URL.', 'abstract_zh': '高光谱成像（HSI）捕获了大量窄波段密集光谱测量的空间信息。丰富的光谱内容有望在复杂材料组成、变化光照或其它视觉挑战条件下，增强机器人的感知能力。然而，当前的HSI语义分割方法因依赖于优化的RGB输入架构和学习框架而表现欠佳。本文提出了一种新颖的高光谱适配器，该适配器利用预训练的视觉基础模型有效学习高光谱数据。我们的架构结合了光谱变换器和光谱感知的空间先验模块，以提取丰富的空间-光谱特征。此外，我们引入了一种模态感知交互块，通过专用的提取和注入机制，促进高光谱表示与冻结视觉Transformer特征的有效整合。在三个基准自动驾驶数据集上的广泛评估表明，我们的架构在直接使用HSI输入的情况下实现了最先进的语义分割性能，优于基于视觉和高光谱分割方法。代码已发布在该网址。', 'title_zh': '基于视觉基础模型的超谱适配器用于语义分割'}
{'arxiv_id': 'arXiv:2509.20021', 'title': 'Embodied AI: From LLMs to World Models', 'authors': 'Tongtong Feng, Xin Wang, Yu-Gang Jiang, Wenwu Zhu', 'link': 'https://arxiv.org/abs/2509.20021', 'abstract': 'Embodied Artificial Intelligence (AI) is an intelligent system paradigm for achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications and driving the evolution from cyberspace to physical systems. Recent breakthroughs in Large Language Models (LLMs) and World Models (WMs) have drawn significant attention for embodied AI. On the one hand, LLMs empower embodied AI via semantic reasoning and task decomposition, bringing high-level natural language instructions and low-level natural language actions into embodied cognition. On the other hand, WMs empower embodied AI by building internal representations and future predictions of the external world, facilitating physical law-compliant embodied interactions. As such, this paper comprehensively explores the literature in embodied AI from basics to advances, covering both LLM driven and WM driven works. In particular, we first present the history, key technologies, key components, and hardware systems of embodied AI, as well as discuss its development via looking from unimodal to multimodal angle. We then scrutinize the two burgeoning fields of embodied AI, i.e., embodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs, meticulously delineating their indispensable roles in end-to-end embodied cognition and physical laws-driven embodied interactions. Building upon the above advances, we further share our insights on the necessity of the joint MLLM-WM driven embodied AI architecture, shedding light on its profound significance in enabling complex tasks within physical worlds. In addition, we examine representative applications of embodied AI, demonstrating its wide applicability in real-world scenarios. Last but not least, we point out future research directions of embodied AI that deserve further investigation.', 'abstract_zh': '具身人工智能：从语义推理到物理世界的智能系统 paradigms 与发展路径', 'title_zh': '具身人工智能：从大规模语言模型到世界模型'}
{'arxiv_id': 'arXiv:2509.19843', 'title': 'PersONAL: Towards a Comprehensive Benchmark for Personalized Embodied Agents', 'authors': 'Filippo Ziliotto, Jelin Raphael Akkara, Alessandro Daniele, Lamberto Ballan, Luciano Serafini, Tommaso Campari', 'link': 'https://arxiv.org/abs/2509.19843', 'abstract': 'Recent advances in Embodied AI have enabled agents to perform increasingly complex tasks and adapt to diverse environments. However, deploying such agents in realistic human-centered scenarios, such as domestic households, remains challenging, particularly due to the difficulty of modeling individual human preferences and behaviors. In this work, we introduce PersONAL (PERSonalized Object Navigation And Localization, a comprehensive benchmark designed to study personalization in Embodied AI. Agents must identify, retrieve, and navigate to objects associated with specific users, responding to natural-language queries such as "find Lily\'s backpack". PersONAL comprises over 2,000 high-quality episodes across 30+ photorealistic homes from the HM3D dataset. Each episode includes a natural-language scene description with explicit associations between objects and their owners, requiring agents to reason over user-specific semantics. The benchmark supports two evaluation modes: (1) active navigation in unseen environments, and (2) object grounding in previously mapped scenes. Experiments with state-of-the-art baselines reveal a substantial gap to human performance, highlighting the need for embodied agents capable of perceiving, reasoning, and memorizing over personalized information; paving the way towards real-world assistive robot.', 'abstract_zh': '近期，体现式AI的发展使代理能够执行越来越复杂的任务并适应多种环境。然而，在诸如家庭这样的现实的人本中心场景中部署这些代理仍然颇具挑战性，特别是由于难以建模个体的人类偏好和行为。在此项工作中，我们介绍了PersONAL（PERSonalized Object Navigation And Localization），一个全面的基准测试，旨在研究体现式AI中的个性化问题。代理必须识别、检索并导航至与特定用户相关联的对象，响应诸如“找到莉莉的背包”之类的自然语言查询。PersONAL包含来自HM3D数据集超过2,000个高质量的场景集，囊括30多个照片级真实感的家庭。每个场景集包括自然语言的场景描述，明确对象与其所有者之间的关联，要求代理进行用户特定语义的推理。基准测试支持两种评估模式：（1）在未见过的环境中进行主动导航，（2）在先前映射的场景中进行物体语义关联。使用当前最先进的基线进行的实验揭示了与人类表现之间存在显著差距，突显了能够感知、推理和记忆个性化信息的体现式代理的必要性；为走向实际辅助机器人铺平了道路。', 'title_zh': 'PersONAL: 向全面个性化体态智能体基准迈进'}
{'arxiv_id': 'arXiv:2509.19789', 'title': 'RDAR: Reward-Driven Agent Relevance Estimation for Autonomous Driving', 'authors': 'Carlo Bosio, Greg Woelki, Noureldin Hendy, Nicholas Roy, Byungsoo Kim', 'link': 'https://arxiv.org/abs/2509.19789', 'abstract': 'Human drivers focus only on a handful of agents at any one time. On the other hand, autonomous driving systems process complex scenes with numerous agents, regardless of whether they are pedestrians on a crosswalk or vehicles parked on the side of the road. While attention mechanisms offer an implicit way to reduce the input to the elements that affect decisions, existing attention mechanisms for capturing agent interactions are quadratic, and generally computationally expensive. We propose RDAR, a strategy to learn per-agent relevance -- how much each agent influences the behavior of the controlled vehicle -- by identifying which agents can be excluded from the input to a pre-trained behavior model. We formulate the masking procedure as a Markov Decision Process where the action consists of a binary mask indicating agent selection. We evaluate RDAR on a large-scale driving dataset, and demonstrate its ability to learn an accurate numerical measure of relevance by achieving comparable driving performance, in terms of overall progress, safety and performance, while processing significantly fewer agents compared to a state of the art behavior model.', 'abstract_zh': '基于代理相关性的自主驾驶注意力机制：RDAR策略', 'title_zh': 'RDAR：基于奖励驱动的代理相关性估计在自主驾驶中的应用'}
{'arxiv_id': 'arXiv:2509.19713', 'title': 'VIMD: Monocular Visual-Inertial Motion and Depth Estimation', 'authors': 'Saimouli Katragadda, Guoquan Huang', 'link': 'https://arxiv.org/abs/2509.19713', 'abstract': 'Accurate and efficient dense metric depth estimation is crucial for 3D visual perception in robotics and XR. In this paper, we develop a monocular visual-inertial motion and depth (VIMD) learning framework to estimate dense metric depth by leveraging accurate and efficient MSCKF-based monocular visual-inertial motion tracking. At the core the proposed VIMD is to exploit multi-view information to iteratively refine per-pixel scale, instead of globally fitting an invariant affine model as in the prior work. The VIMD framework is highly modular, making it compatible with a variety of existing depth estimation backbones. We conduct extensive evaluations on the TartanAir and VOID datasets and demonstrate its zero-shot generalization capabilities on the AR Table dataset. Our results show that VIMD achieves exceptional accuracy and robustness, even with extremely sparse points as few as 10-20 metric depth points per image. This makes the proposed VIMD a practical solution for deployment in resource constrained settings, while its robust performance and strong generalization capabilities offer significant potential across a wide range of scenarios.', 'abstract_zh': '单目视觉惯性运动与深度（VIMD）学习框架：基于准确高效的MSCKF单目视觉惯性运动跟踪的密集度规深度估计', 'title_zh': '单目视觉惯性运动及深度估计（VIMD）'}
{'arxiv_id': 'arXiv:2509.19644', 'title': 'The Impact of 2D Segmentation Backbones on Point Cloud Predictions Using 4D Radar', 'authors': 'William L. Muckelroy III, Mohammed Alsakabi, John M. Dolan, Ozan K. Tonguz', 'link': 'https://arxiv.org/abs/2509.19644', 'abstract': "LiDAR's dense, sharp point cloud (PC) representations of the surrounding environment enable accurate perception and significantly improve road safety by offering greater scene awareness and understanding. However, LiDAR's high cost continues to restrict the broad adoption of high-level Autonomous Driving (AD) systems in commercially available vehicles. Prior research has shown progress towards circumventing the need for LiDAR by training a neural network, using LiDAR point clouds as ground truth (GT), to produce LiDAR-like 3D point clouds using only 4D Radars. One of the best examples is a neural network created to train a more efficient radar target detector with a modular 2D convolutional neural network (CNN) backbone and a temporal coherence network at its core that uses the RaDelft dataset for training (see arXiv:2406.04723). In this work, we investigate the impact of higher-capacity segmentation backbones on the quality of the produced point clouds. Our results show that while very high-capacity models may actually hurt performance, an optimal segmentation backbone can provide a 23.7% improvement over the state-of-the-art (SOTA).", 'abstract_zh': 'LiDAR的密集精确点云表示使周围环境的准确感知成为可能，显著提高了道路安全，通过提供更大的场景意识和理解。然而，LiDAR的高成本继续限制了高级自动驾驶（AD）系统在商用车辆中的广泛应用。先前的研究表明，通过训练神经网络并使用LiDAR点云作为ground truth（GT），可以使用仅4D雷达生成类似于LiDAR的3D点云，从而规避对LiDAR的需求。一个典型的例子是使用模块化的2D卷积神经网络（CNN）骨干和核心采用时间一致网络的方法训练更高效的雷达目标检测器（见arXiv:2406.04723）。在本项研究中，我们探讨了更高容量的分割骨干网络对生成点云质量的影响。我们的结果显示，尽管非常高容量的模型可能会损害性能，但最优的分割骨干网络仍能比现有的最先进（SOTA）方法提高23.7%。', 'title_zh': '二维分割骨干对使用四维雷达预测点云的影响'}
{'arxiv_id': 'arXiv:2509.19524', 'title': 'Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for Robotic Manipulation', 'authors': 'Ramy ElMallah, Krish Chhajer, Chi-Guhn Lee', 'link': 'https://arxiv.org/abs/2509.19524', 'abstract': 'Robot learning papers typically report a single binary success rate (SR), which obscures where a policy succeeds or fails along a multi-step manipulation task. We argue that subgoal-level reporting should become routine: for each trajectory, a vector of per-subgoal SRs that makes partial competence visible (e.g., grasp vs. pour). We propose a blueprint for StepEval, a cost-aware plug-in evaluation framework that utilizes vision-language models (VLMs) as automated judges of subgoal outcomes from recorded images or videos. Rather than proposing new benchmarks or APIs, our contribution is to outline design principles for a scalable, community-driven open-source project. In StepEval, the primary artifact for policy evaluation is the per-subgoal SR vector; however, other quantities (e.g., latency or cost estimates) are also considered for framework-optimization diagnostics to help the community tune evaluation efficiency and accuracy when ground-truth subgoal success labels are available. We discuss how such a framework can remain model-agnostic, support single- or multi-view inputs, and be lightweight enough to adopt across labs. The intended contribution is a shared direction: a minimal, extensible seed that invites open-source contributions, so that scoring the steps, not just the final goal, becomes a standard and reproducible practice.', 'abstract_zh': '机器人学习论文通常只报告单二分类成功率（SR），这会掩盖单一多步骤操作任务中策略成功或失败的具体位置。我们主张子目标级别报告应成为常规做法：为每条轨迹提供一个子目标级别的SR向量，以使部分能力可见（例如，抓取 vs 倾倒）。我们提出了一种StepEval的成本意识插件评估框架的设计蓝图，该框架利用视觉语言模型（VLMs）自动评估从记录图像或视频中得出的子目标结果。我们的贡献不在于提出新的基准或API，而在于概述一个可扩展、由社区驱动的开源项目的构建原理。在StepEval中，策略评估的主要成果是子目标级别的SR向量；此外，还可以考虑其他数量（例如，延迟或成本估计）以进行框架优化诊断，以帮助社区调优评估效率和准确性，当有 ground-truth 子目标成功标签时。我们讨论了该框架如何保持模型无感知、支持单视图或多视图输入，并且足够轻量以便在各个实验室中采用。我们的目标是提供一个共享的方向：一个最小的、可扩展的种子，邀请开源贡献，使评估步骤而不仅仅是最终目标成为标准且可重复的做法。', 'title_zh': '按步骤打分，而不仅是目标打分：基于VLM的机器人操作子目标评估'}
{'arxiv_id': 'arXiv:2509.19477', 'title': 'Robust Near-Optimal Nonlinear Target Enclosing Guidance', 'authors': 'Abhinav Sinha, Rohit V. Nanavati', 'link': 'https://arxiv.org/abs/2509.19477', 'abstract': "This paper proposes a nonlinear optimal guidance law that enables a pursuer to enclose a target within arbitrary geometric patterns, which extends beyond conventional circular encirclement. The design operates using only relative state measurements and formulates a target enclosing guidance law in which the vehicle's lateral acceleration serves as the steering control, making it well-suited for aerial vehicles with turning constraints. Our approach generalizes and extends existing guidance strategies that are limited to target encirclement and provides a degree of optimality. At the same time, the exact information of the target's maneuver is unnecessary during the design. The guidance law is developed within the framework of a state-dependent Riccati equation (SDRE), thereby providing a systematic way to handle nonlinear dynamics through a pseudo-linear representation to design locally optimal feedback guidance commands through state-dependent weighting matrices. While SDRE ensures near-optimal performance in the absence of strong disturbances, we further augment the design to incorporate an integral sliding mode manifold to compensate when disturbances push the system away from the nominal trajectory, and demonstrate that the design provides flexibility in the sense that the (possibly time-varying) stand-off curvature could also be treated as unknown. Simulations demonstrate the efficacy of the proposed approach.", 'abstract_zh': '这篇论文提出了一种非线性最优引导律，使追踪器能够将目标包围在任意几何图案内，超越了传统的圆形包围。该设计仅使用相对状态测量值，并将目标包围引导律公式化，其中车辆的横向加速度用作转向控制，使其适用于具有转弯约束的航空器。我们的方法既泛化了现有的仅限于目标包围的引导策略，又提供了一定程度的最优性。同时，在设计过程中无需知道目标机动的精确信息。引导律在状态依赖型Riccati方程（SDRE）框架内开发，通过状态依赖型加权矩阵提供了一种通过伪线性表示处理非线性动力学的系统方法，以设计局部最优反馈引导指令。虽然SDRE在无强干扰的情况下保证了接近最优的性能，我们进一步通过引入积分滑模流形来补充设计，以补偿当干扰将系统推向非理想轨迹时的情况，并证明设计在某种程度上具有灵活性，即可能随时间变化的临界曲率也可以被视为未知数。仿真展示了所提方法的有效性。', 'title_zh': '鲁棒近最优非线性目标包络制导'}
{'arxiv_id': 'arXiv:2509.19379', 'title': 'Learning from Observation: A Survey of Recent Advances', 'authors': 'Returaj Burnwal, Hriday Mehta, Nirav Pravinbhai Bhatt, Balaraman Ravindran', 'link': 'https://arxiv.org/abs/2509.19379', 'abstract': "Imitation Learning (IL) algorithms offer an efficient way to train an agent by mimicking an expert's behavior without requiring a reward function. IL algorithms often necessitate access to state and action information from expert demonstrations. Although expert actions can provide detailed guidance, requiring such action information may prove impractical for real-world applications where expert actions are difficult to obtain. To address this limitation, the concept of learning from observation (LfO) or state-only imitation learning (SOIL) has recently gained attention, wherein the imitator only has access to expert state visitation information. In this paper, we present a framework for LfO and use it to survey and classify existing LfO methods in terms of their trajectory construction, assumptions and algorithm's design choices. This survey also draws connections between several related fields like offline RL, model-based RL and hierarchical RL. Finally, we use our framework to identify open problems and suggest future research directions.", 'abstract_zh': '模仿学习（IL）算法提供了一种通过模仿专家行为来训练代理的方法，无需使用奖励函数。尽管专家行为可以提供详细的指导，但要求获取这种行为信息在实际应用中可能不切实际，特别是当专家行为难以获得时。为此，基于观测的学习（LfO）或仅基于状态的模仿学习（SOIL）的概念最近引起了关注，其中模仿学习者仅能访问专家状态访问信息。本文提出了一种LfO框架，并使用该框架对现有LfO方法进行综述和分类，按轨迹构建、假设和算法设计选择对这些方法进行分类。此外，本文还探讨了与离线强化学习、模型导向的强化学习和层次化强化学习等相关领域的联系。最后，本文利用这一框架识别出了开放性问题，并建议了未来的研究方向。', 'title_zh': '基于观察学习：近期进展综述'}
{'arxiv_id': 'arXiv:2509.19318', 'title': 'Scensory: Automated Real-Time Fungal Identification and Spatial Mapping', 'authors': 'Yanbaihui Liu, Erica Babusci, Claudia K. Gunsch, Boyuan Chen', 'link': 'https://arxiv.org/abs/2509.19318', 'abstract': 'Indoor fungal contamination poses significant risks to public health, yet existing detection methods are slow, costly, and lack spatial resolution. Conventional approaches rely on laboratory analysis or high-concentration sampling, making them unsuitable for real-time monitoring and scalable deployment. We introduce \\textbf{\\textit{Scensory}}, a robot-enabled olfactory system that simultaneously identifies fungal species and localizes their spatial origin using affordable volatile organic compound (VOC) sensor arrays and deep learning. Our key idea is that temporal VOC dynamics encode both chemical and spatial signatures, which we decode through neural architectures trained on robot-automated data collection. We demonstrate two operational modes: a passive multi-array configuration for environmental monitoring, and a mobile single-array configuration for active source tracking. Across five fungal species, our system achieves up to 89.85\\% accuracy in species detection and 87.31\\% accuracy in localization under ambient conditions, where each prediction only takes 3--7\\,s sensor inputs. Additionally, by computationally analyzing model behavior, we can uncover key biochemical signatures without additional laboratory experiments. Our approach enables real-time, spatially aware fungal monitoring and establishes a scalable and affordable framework for autonomous environmental sensing.', 'abstract_zh': '室内真菌污染对公共健康构成显著风险，现有检测方法速度缓慢、成本高昂且缺乏空间分辨率。传统的做法依赖于实验室分析或高浓度采样，使得实时监测和规模化部署不具备可行性。我们介绍了\\textbf{\\textit{Scensory}}——一种基于机器人嗅觉系统的解决方案，该系统利用经济实惠的挥发性有机化合物(VOC)传感器阵列和深度学习同时识别真菌种类并定位其空间来源。我们提出的核心思想是时间上的VOC动态编码了化学和空间特征，通过基于机器人自动化数据收集训练的神经架构进行解码。我们展示了两种操作模式：一种是用于环境监测的被动多阵列配置，另一种是用于主动源头追踪的便携式单阵列配置。在五种真菌物种中，我们的系统在环境条件下实现高达89.85%的种类检测准确率和87.31%的定位准确率，每项预测仅需3-7秒的传感器输入。此外，通过计算分析模型行为，我们可以在不进行额外实验室实验的情况下揭示关键的生物化学特征。我们的方法实现了实时、空间感知的真菌监测，并建立了一个可扩展且经济实惠的自主环境感知框架。', 'title_zh': 'Scensory: 自动实时真菌识别与空间映射'}
