# A Survey on Vision-Language-Action Models: An Action Tokenization Perspective 

**Title (ZH)**: 视觉-语言-行动模型综述：一种行动分词视角 

**Authors**: Yifan Zhong, Fengshuo Bai, Shaofei Cai, Xuchuan Huang, Zhang Chen, Xiaowei Zhang, Yuanfei Wang, Shaoyang Guo, Tianrui Guan, Ka Nam Lui, Zhiquan Qi, Yitao Liang, Yuanpei Chen, Yaodong Yang  

**Link**: [PDF](https://arxiv.org/pdf/2507.01925)  

**Abstract**: The remarkable advancements of vision and language foundation models in multimodal understanding, reasoning, and generation has sparked growing efforts to extend such intelligence to the physical world, fueling the flourishing of vision-language-action (VLA) models. Despite seemingly diverse approaches, we observe that current VLA models can be unified under a single framework: vision and language inputs are processed by a series of VLA modules, producing a chain of \textit{action tokens} that progressively encode more grounded and actionable information, ultimately generating executable actions. We further determine that the primary design choice distinguishing VLA models lies in how action tokens are formulated, which can be categorized into language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning. However, there remains a lack of comprehensive understanding regarding action tokens, significantly impeding effective VLA development and obscuring future directions. Therefore, this survey aims to categorize and interpret existing VLA research through the lens of action tokenization, distill the strengths and limitations of each token type, and identify areas for improvement. Through this systematic review and analysis, we offer a synthesized outlook on the broader evolution of VLA models, highlight underexplored yet promising directions, and contribute guidance for future research, hoping to bring the field closer to general-purpose intelligence. 

**Abstract (ZH)**: 视觉语言行动模型在多模态理解、推理和生成方面的显著进展激发了将此类智能扩展到物理世界的努力，推动了视觉语言行动（VLA）模型的蓬勃发展。尽管当前方法看似多样，我们观察到现有的VLA模型可以统一到一个框架下：视觉和语言输入通过一系列VLA模块处理，生成一系列逐步包含更多具体和可执行信息的“行动令牌”，最终生成可执行的动作。进一步研究发现，区分VLA模型的主要设计选择在于如何定义行动令牌，这些可以被分类为语言描述、代码、功能、轨迹、目标状态、潜在表示、原始动作和推理。然而，关于行动令牌的全面理解仍然缺失，严重阻碍了有效VLA的发展和未来方向的明确。因此，本文旨在通过行动令牌化视角分类和解释现有的VLA研究，提炼每种令牌类型的优势和限制，并识别改进领域。通过系统的回顾和分析，我们提供了一个对VLA模型更广阔的演进的综合展望，突出潜在但未充分探索的方向，并为未来的研究提供指导，希望推动该领域向通用智能迈进。 

---
# How Do Vision-Language Models Process Conflicting Information Across Modalities? 

**Title (ZH)**: 视觉-语言模型如何处理跨模态的矛盾信息？ 

**Authors**: Tianze Hua, Tian Yun, Ellie Pavlick  

**Link**: [PDF](https://arxiv.org/pdf/2507.01790)  

**Abstract**: AI models are increasingly required to be multimodal, integrating disparate input streams into a coherent state representation on which subsequent behaviors and actions can be based. This paper seeks to understand how such models behave when input streams present conflicting information. Focusing specifically on vision-language models, we provide inconsistent inputs (e.g., an image of a dog paired with the caption "A photo of a cat") and ask the model to report the information present in one of the specific modalities (e.g., "What does the caption say / What is in the image?"). We find that models often favor one modality over the other, e.g., reporting the image regardless of what the caption says, but that different models differ in which modality they favor. We find evidence that the behaviorally preferred modality is evident in the internal representational structure of the model, and that specific attention heads can restructure the representations to favor one modality over the other. Moreover, we find modality-agnostic "router heads" which appear to promote answers about the modality requested in the instruction, and which can be manipulated or transferred in order to improve performance across datasets and modalities. Together, the work provides essential steps towards identifying and controlling if and how models detect and resolve conflicting signals within complex multimodal environments. 

**Abstract (ZH)**: AI模型在处理冲突输入信息时的多模态行为研究 

---
# ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving 

**Title (ZH)**: ECCV 2024 W-CODA: 第一届多模态corner cases感知与理解工作坊 

**Authors**: Kai Chen, Ruiyuan Gao, Lanqing Hong, Hang Xu, Xu Jia, Holger Caesar, Dengxin Dai, Bingbing Liu, Dzmitry Tsishkou, Songcen Xu, Chunjing Xu, Qiang Xu, Huchuan Lu, Dit-Yan Yeung  

**Link**: [PDF](https://arxiv.org/pdf/2507.01735)  

**Abstract**: In this paper, we present details of the 1st W-CODA workshop, held in conjunction with the ECCV 2024. W-CODA aims to explore next-generation solutions for autonomous driving corner cases, empowered by state-of-the-art multimodal perception and comprehension techniques. 5 Speakers from both academia and industry are invited to share their latest progress and opinions. We collect research papers and hold a dual-track challenge, including both corner case scene understanding and generation. As the pioneering effort, we will continuously bridge the gap between frontier autonomous driving techniques and fully intelligent, reliable self-driving agents robust towards corner cases. 

**Abstract (ZH)**: 本文展示了与ECCV 2024联合举办的第1届W-CODA研讨会的详细情况。W-CODA旨在通过利用最先进的多模态感知和理解技术探索自主驾驶corner case的下一代解决方案。邀请了5位来自学术界和工业界的讲者分享他们的最新进展和观点。我们收集研究论文并举行双轨挑战赛，包括corner case场景的理解和生成。作为这一领域的首创努力，我们将持续弥合前沿自主驾驶技术与面向corner case的智能、可靠的自主驱动代理之间的差距。 

---
# Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring 

**Title (ZH)**: 自主人工智能 surveillance：多模态深度学习在认知和行为监测中的应用 

**Authors**: Ameer Hamza, Zuhaib Hussain But, Umar Arif, Samiya, M. Abdullah Asad, Muhammad Naeem  

**Link**: [PDF](https://arxiv.org/pdf/2507.01590)  

**Abstract**: This study presents a novel classroom surveillance system that integrates multiple modalities, including drowsiness, tracking of mobile phone usage, and face recognition,to assess student attentiveness with enhanced this http URL system leverages the YOLOv8 model to detect both mobile phone and sleep usage,(Ghatge et al., 2024) while facial recognition is achieved through LResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These models work in synergy to provide comprehensive, real-time monitoring, offering insights into student engagement and behavior.(S et al., 2023) The framework is trained on specialized datasets, such as the RMFD dataset for face recognition and a Roboflow dataset for mobile phone detection. The extensive evaluation of the system shows promising results. Sleep detection achieves 97. 42% mAP@50, face recognition achieves 86. 45% validation accuracy and mobile phone detection reach 85. 89% mAP@50. The system is implemented within a core PHP web application and utilizes ESP32-CAM hardware for seamless data capture.(Neto et al., 2024) This integrated approach not only enhances classroom monitoring, but also ensures automatic attendance recording via face recognition as students remain seated in the classroom, offering scalability for diverse educational environments.(Banada,2025) 

**Abstract (ZH)**: 一种综合多模态技术（包括困倦检测、移动设备使用跟踪和面部识别）以评估学生专注度的新型课堂监控系统：基于YOLOv8和LResNet Occ FC的综合方法 

---
# Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence 

**Title (ZH)**: 顺着线索：跨模态智能在行人重识别中的实验研究 

**Authors**: Robert Aufschläger, Youssef Shoeb, Azarm Nowzad, Michael Heigl, Fabian Bally, Martin Schramm  

**Link**: [PDF](https://arxiv.org/pdf/2507.01504)  

**Abstract**: The collection and release of street-level recordings as Open Data play a vital role in advancing autonomous driving systems and AI research. However, these datasets pose significant privacy risks, particularly for pedestrians, due to the presence of Personally Identifiable Information (PII) that extends beyond biometric traits such as faces. In this paper, we present cRID, a novel cross-modal framework combining Large Vision-Language Models, Graph Attention Networks, and representation learning to detect textual describable clues of PII and enhance person re-identification (Re-ID). Our approach focuses on identifying and leveraging interpretable features, enabling the detection of semantically meaningful PII beyond low-level appearance cues. We conduct a systematic evaluation of PII presence in person image datasets. Our experiments show improved performance in practical cross-dataset Re-ID scenarios, notably from Market-1501 to CUHK03-np (detected), highlighting the framework's practical utility. Code is available at this https URL. 

**Abstract (ZH)**: 基于跨模态框架的个人可识别信息检测与行人再识别-enhancing privacy protection in street-level recordings as open data 

---
# PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning 

**Title (ZH)**: PathCoT: 以思维链方式进行零样本病理视觉推理的提示方法 

**Authors**: Junjie Zhou, Yingli Zuo, Shichang Feng, Peng Wan, Qi Zhu, Daoqiang Zhang, Wei Shao  

**Link**: [PDF](https://arxiv.org/pdf/2507.01029)  

**Abstract**: With the development of generative artificial intelligence and instruction tuning techniques, multimodal large language models (MLLMs) have made impressive progress on general reasoning tasks. Benefiting from the chain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning problem step-by-step. However, existing MLLMs still face significant challenges when applied to pathology visual reasoning tasks: (1) LLMs often underperforms because they lack domain-specific information, which can lead to model hallucinations. (2) The additional reasoning steps in CoT may introduce errors, leading to the divergence of answers. To address these limitations, we propose PathCoT, a novel zero-shot CoT prompting method which integrates the pathology expert-knowledge into the reasoning process of MLLMs and incorporates self-evaluation to mitigate divergence of answers. Specifically, PathCoT guides the MLLM with prior knowledge to perform as pathology experts, and provides comprehensive analysis of the image with their domain-specific knowledge. By incorporating the experts' knowledge, PathCoT can obtain the answers with CoT reasoning. Furthermore, PathCoT incorporates a self-evaluation step that assesses both the results generated directly by MLLMs and those derived through CoT, finally determining the reliable answer. The experimental results on the PathMMU dataset demonstrate the effectiveness of our method on pathology visual understanding and reasoning. 

**Abstract (ZH)**: 基于病理专家知识的零样本链式思考路径推理方法 

---
