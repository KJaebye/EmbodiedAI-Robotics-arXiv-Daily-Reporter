# Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics 

**Title (ZH)**: 面向更全面的答案集语义基础原则细化格尔丰德有理性原则 

**Authors**: Yi-Dong Shen, Thomas Eiter  

**Link**: [PDF](https://arxiv.org/pdf/2507.01833)  

**Abstract**: Non-monotonic logic programming is the basis for a declarative problem solving paradigm known as answer set programming (ASP). Departing from the seminal definition by Gelfond and Lifschitz in 1988 for simple normal logic programs, various answer set semantics have been proposed for extensions. We consider two important questions: (1) Should the minimal model property, constraint monotonicity and foundedness as defined in the literature be mandatory conditions for an answer set semantics in general? (2) If not, what other properties could be considered as general principles for answer set semantics? We address the two questions. First, it seems that the three aforementioned conditions may sometimes be too strong, and we illustrate with examples that enforcing them may exclude expected answer sets. Second, we evolve the Gelfond answer set (GAS) principles for answer set construction by refining the Gelfond's rationality principle to well-supportedness, minimality w.r.t. negation by default and minimality w.r.t. epistemic negation. The principle of well-supportedness guarantees that every answer set is constructible from if-then rules obeying a level mapping and is thus free of circular justification, while the two minimality principles ensure that the formalism minimizes knowledge both at the level of answer sets and of world views. Third, to embody the refined GAS principles, we extend the notion of well-supportedness substantially to answer sets and world views, respectively. Fourth, we define new answer set semantics in terms of the refined GAS principles. Fifth, we use the refined GAS principles as an alternative baseline to intuitively assess the existing answer set semantics. Finally, we analyze the computational complexity. 

**Abstract (ZH)**: 非单调逻辑编程是回答集编程（ASP）这一声明式问题求解范式的基础。从Gelfond和Lifschitz在1988年对简单正常逻辑程序的开创性定义出发，已提出了各种回答集语义扩展。我们考虑了两个重要问题：（1）文献中定义的最小模型性质、约束单调性和奠基性是否应作为一般回答集语义的基本条件？（2）如果不应如此，可以考虑哪些其他属性作为回答集语义的一般原则？我们解答了这两个问题。首先，看来上述三个条件有时可能过于严格，我们通过例子说明强制执行它们可能会排除预期的回答集。其次，我们通过将Gelfond的合理性原则细化为支持性、基于默认否定的最小性和基于知识否定的最小性，来改进Gelfond回答集（GAS）原则，以回答集构造为出发点。支持性原则保证每个回答集可通过遵循等级映射的if-then规则构造，从而避免循环证明，而两个最小性原则则确保该形式化方法在回答集和世界观层面都尽量减少知识。第三，为了体现改进后的GAS原则，我们大幅扩展了支持性的概念分别应用于回答集和世界观。第四，我们根据改进后的GAS原则定义新的回答集语义。第五，我们使用改进后的GAS原则作为替代基准，以直观方式评估现有的回答集语义。最后，我们分析了计算复杂性。 

---
# Joint Matching and Pricing for Crowd-shipping with In-store Customers 

**Title (ZH)**: 基于商店内顾客的 crowds-shipping 匹配与定价联合优化 

**Authors**: Arash Dehghan, Mucahit Cevik, Merve Bodur, Bissan Ghaddar  

**Link**: [PDF](https://arxiv.org/pdf/2507.01749)  

**Abstract**: This paper examines the use of in-store customers as delivery couriers in a centralized crowd-shipping system, targeting the growing need for efficient last-mile delivery in urban areas. We consider a brick-and-mortar retail setting where shoppers are offered compensation to deliver time-sensitive online orders. To manage this process, we propose a Markov Decision Process (MDP) model that captures key uncertainties, including the stochastic arrival of orders and crowd-shippers, and the probabilistic acceptance of delivery offers. Our solution approach integrates Neural Approximate Dynamic Programming (NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network (DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop routing and accounts for offer acceptance uncertainty, aligning more closely with real-world operations. Experimental results demonstrate that the integrated NeurADP + DDQN policy achieves notable improvements in delivery cost efficiency, with up to 6.7\% savings over NeurADP with fixed pricing and approximately 18\% over myopic baselines. We also show that allowing flexible delivery delays and enabling multi-destination routing further reduces operational costs by 8\% and 17\%, respectively. These findings underscore the advantages of dynamic, forward-looking policies in crowd-shipping systems and offer practical guidance for urban logistics operators. 

**Abstract (ZH)**: 基于集中众包配送系统的店内顾客送货研究：面向城市地区的高效最后公里配送需求 

---
# Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI 

**Title (ZH)**: Agent Ideate: 一种基于专利的用代理人工智能进行产品创意生成的框架 

**Authors**: Gopichand Kanumolu, Ashok Urlana, Charaka Vinayak Kumar, Bala Mallikarjunarao Garlapati  

**Link**: [PDF](https://arxiv.org/pdf/2507.01717)  

**Abstract**: Patents contain rich technical knowledge that can inspire innovative product ideas, yet accessing and interpreting this information remains a challenge. This work explores the use of Large Language Models (LLMs) and autonomous agents to mine and generate product concepts from a given patent. In this work, we design Agent Ideate, a framework for automatically generating product-based business ideas from patents. We experimented with open-source LLMs and agent-based architectures across three domains: Computer Science, Natural Language Processing, and Material Chemistry. Evaluation results show that the agentic approach consistently outperformed standalone LLMs in terms of idea quality, relevance, and novelty. These findings suggest that combining LLMs with agentic workflows can significantly enhance the innovation pipeline by unlocking the untapped potential of business idea generation from patent data. 

**Abstract (ZH)**: 专利包含丰富的技术知识，可以激发创新产品理念，但获取和解读这些信息仍然颇具挑战。本研究探讨了使用大型语言模型（LLMs）和自主代理从专利中挖掘和生成产品概念的方法。在本研究中，我们设计了Agent Ideate框架，用于自动从专利中生成基于产品的商业理念。我们分别在计算机科学、自然语言处理和材料化学三个领域试验了开源LLMs和基于代理的架构。评估结果显示，代理方法在理念的质量、相关性和新颖性方面均优于独立的LLMs。这些发现表明，将LLMs与代理工作流相结合可以显著增强创新管道，通过挖掘专利数据中的商业理念潜力来提升创新能力。 

---
# T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning 

**Title (ZH)**: T3DM：测试时训练引导的分布偏移建模以进行时间知识图推理 

**Authors**: Yuehang Si, Zefan Zeng, Jincai Huang, Qing Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2507.01597)  

**Abstract**: Temporal Knowledge Graph (TKG) is an efficient method for describing the dynamic development of facts along a timeline. Most research on TKG reasoning (TKGR) focuses on modelling the repetition of global facts and designing patterns of local historical facts. However, they face two significant challenges: inadequate modeling of the event distribution shift between training and test samples, and reliance on random entity substitution for generating negative samples, which often results in low-quality sampling. To this end, we propose a novel distributional feature modeling approach for training TKGR models, Test-Time Training-guided Distribution shift Modelling (T3DM), to adjust the model based on distribution shift and ensure the global consistency of model reasoning. In addition, we design a negative-sampling strategy to generate higher-quality negative quadruples based on adversarial training. Extensive experiments show that T3DM provides better and more robust results than the state-of-the-art baselines in most cases. 

**Abstract (ZH)**: 基于时间的知识图谱推理的训练时训练指导分佈迁移建模（T3DM） 

---
# Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning 

**Title (ZH)**: 工具型智能体：基于强化学习的分级决策研究 

**Authors**: Yanfei Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2507.01489)  

**Abstract**: Large Language Models (LLMs) have emerged as one of the most significant technological advancements in artificial intelligence in recent years. Their ability to understand, generate, and reason with natural language has transformed how we interact with AI systems. With the development of LLM-based agents and reinforcement-learning-based reasoning models, the study of applying reinforcement learning in agent frameworks has become a new research focus. However, all previous studies face the challenge of deciding the tool calling process and the reasoning process simultaneously, and the chain of reasoning was solely relied on the unprocessed raw result with redundant information and symbols unrelated to the task from the tool, which impose a heavy burden on the model's capability to reason. Therefore, in our research, we proposed a hierarchical framework Agent-as-tool that detach the tool calling process and the reasoning process, which enables the model to focus on the verbally reasoning process while the tool calling process is handled by another agent. Our work had achieved comparable results with only a slight reinforcement fine-tuning on 180 samples, and had achieved exceptionally well performance in Bamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding Search-R1 by 4.8% in exact match and 3.2% in cover exact match. 

**Abstract (ZH)**: 大型语言模型(LLMs)已成为近年来人工智能领域最重要的技术进步之一。它们理解和生成自然语言的能力已经改变了我们与AI系统的互动方式。随着基于LLM的代理和基于强化学习的推理模型的发展，将强化学习应用于代理框架中的研究已成为一个新的研究重点。然而，所有先前的研究都面临着同时决定工具调用过程和推理过程的挑战，推理过程依赖于工具提供的未经处理的原始结果，其中包含了与任务无关的冗余信息和符号，这给模型的推理能力带来了沉重负担。因此，在我们的研究中，我们提出了一种分层框架Agent-as-tool，将工具调用过程和推理过程分离，使得模型能够专注于语言推理过程，而工具调用过程则由另一个代理处理。我们的工作仅在180个样本上进行轻微的强化学习微调就取得了可比的结果，并在Bamboogle中表现出色，精确匹配率为63.2%，覆盖精确匹配率为75.2%，分别超过了Search-R1 4.8%和3.2%的精确匹配率和覆盖精确匹配率。 

---
# Using multi-agent architecture to mitigate the risk of LLM hallucinations 

**Title (ZH)**: 使用多代理架构减轻大语言模型幻觉风险 

**Authors**: Abd Elrahman Amer, Magdi Amer  

**Link**: [PDF](https://arxiv.org/pdf/2507.01446)  

**Abstract**: Improving customer service quality and response time are critical factors for maintaining customer loyalty and increasing a company's market share. While adopting emerging technologies such as Large Language Models (LLMs) is becoming a necessity to achieve these goals, the risk of hallucination remains a major challenge. In this paper, we present a multi-agent system to handle customer requests sent via SMS. This system integrates LLM based agents with fuzzy logic to mitigate hallucination risks. 

**Abstract (ZH)**: 提高客户服务质量和响应时间是维持客户忠诚度和增加公司市场份额的关键因素。尽管采用大型语言模型等新兴技术已成为实现这些目标的必要条件，幻觉风险仍然是一个主要挑战。本文提出了一种多代理系统来处理通过短信发送的客户请求，该系统结合了基于大型语言模型的代理和模糊逻辑以减轻幻觉风险。 

---
# Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading 

**Title (ZH)**: Pensieve Grader: 一个基于AI的即用型平台，实现轻松手写STEM作业批阅 

**Authors**: Yoonseok Yang, Minjune Kim, Marlon Rondinelli, Keren Shao  

**Link**: [PDF](https://arxiv.org/pdf/2507.01431)  

**Abstract**: Grading handwritten, open-ended responses remains a major bottleneck in large university STEM courses. We introduce Pensieve (this https URL), an AI-assisted grading platform that leverages large language models (LLMs) to transcribe and evaluate student work, providing instructors with rubric-aligned scores, transcriptions, and confidence ratings. Unlike prior tools that focus narrowly on specific tasks like transcription or rubric generation, Pensieve supports the entire grading pipeline-from scanned student submissions to final feedback-within a human-in-the-loop interface.
Pensieve has been deployed in real-world courses at over 20 institutions and has graded more than 300,000 student responses. We present system details and empirical results across four core STEM disciplines: Computer Science, Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces grading time by an average of 65%, while maintaining a 95.4% agreement rate with instructor-assigned grades for high-confidence predictions. 

**Abstract (ZH)**: 手写开放性回答的评分仍然是大型大学STEM课程中的主要瓶颈。我们介绍了一种名为Pensieve的AI辅助评分平台（this https URL），该平台利用大型语言模型（LLMs）进行评分学生的作业，为教师提供评分、转写和置信度评级。与仅专注于特定任务（如转写或评分标准生成）的先前工具不同，Pensieve支持从扫描的学生提交到最终反馈的整个评分流程，并采用人机协作界面。Pensieve已在超过20所机构的实际课程中部署，并评分了超过30万份学生回答。我们介绍了四个核心STEM学科（计算机科学、数学、物理和化学）的具体系统细节和实证结果。研究结果表明，Pensieve将评分时间减少了平均65%，同时保持了95.4%的高置信度预测与教师评分的一致率。 

---
# A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models 

**Title (ZH)**: 基于风险的伦理决策模型的规范、验证与验证的模糊方法 

**Authors**: Abeer Dyoub, Francesca A. Lisi  

**Link**: [PDF](https://arxiv.org/pdf/2507.01410)  

**Abstract**: The ontological and epistemic complexities inherent in the moral domain make it challenging to establish clear standards for evaluating the performance of a moral machine. In this paper, we present a formal method to describe Ethical Decision Making models based on ethical risk assessment. Then, we show how these models that are specified as fuzzy rules can be verified and validated using fuzzy Petri nets. A case study from the medical field is considered to illustrate the proposed approach. 

**Abstract (ZH)**: 道德领域内在的本体论和认识论复杂性使得确立评价道德机器性能的明确标准具有挑战性。本文提出了一种基于伦理风险评估描述伦理决策模型的形式化方法，展示了如何使用模糊Petri网验证和验证用模糊规则指定的模型。考虑了医疗领域的案例研究以说明所提出的方法。 

---
# AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing 

**Title (ZH)**: AI代理与代理型AI：导航未来制造中的众多概念 

**Authors**: Yinwang Ren, Yangyang Liu, Tang Ji, Xun Xu  

**Link**: [PDF](https://arxiv.org/pdf/2507.01376)  

**Abstract**: AI agents are autonomous systems designed to perceive, reason, and act within dynamic environments. With the rapid advancements in generative AI (GenAI), large language models (LLMs) and multimodal large language models (MLLMs) have significantly improved AI agents' capabilities in semantic comprehension, complex reasoning, and autonomous decision-making. At the same time, the rise of Agentic AI highlights adaptability and goal-directed autonomy in dynamic and complex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents (MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in information processing, environmental perception, and autonomous decision-making, opening new avenues for smart manufacturing. However, the definitions, capability boundaries, and practical applications of these emerging AI paradigms in smart manufacturing remain unclear. To address this gap, this study systematically reviews the evolution of AI and AI agent technologies, examines the core concepts and technological advancements of LLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential applications in and integration into manufacturing, along with the potential challenges they may face. 

**Abstract (ZH)**: 基于大型语言模型的AI代理、基于多模态大型语言模型的AI代理和行动AI在智能制造中的演进及其应用研究 

---
# Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care 

**Title (ZH)**: 超越黑箱AI：可解释的混合系统在痴呆症护理中的应用 

**Authors**: Matthew JY Kang, Wenli Yang, Monica R Roberts, Byeong Ho Kang, Charles B Malpas  

**Link**: [PDF](https://arxiv.org/pdf/2507.01282)  

**Abstract**: The recent boom of large language models (LLMs) has re-ignited the hope that artificial intelligence (AI) systems could aid medical diagnosis. Yet despite dazzling benchmark scores, LLM assistants have yet to deliver measurable improvements at the bedside. This scoping review aims to highlight the areas where AI is limited to make practical contributions in the clinical setting, specifically in dementia diagnosis and care.
Standalone machine-learning models excel at pattern recognition but seldom provide actionable, interpretable guidance, eroding clinician trust. Adjacent use of LLMs by physicians did not result in better diagnostic accuracy or speed. Key limitations trace to the data-driven paradigm: black-box outputs which lack transparency, vulnerability to hallucinations, and weak causal reasoning. Hybrid approaches that combine statistical learning with expert rule-based knowledge, and involve clinicians throughout the process help bring back interpretability. They also fit better with existing clinical workflows, as seen in examples like PEIRS and ATHENA-CDS.
Future decision-support should prioritise explanatory coherence by linking predictions to clinically meaningful causes. This can be done through neuro-symbolic or hybrid AI that combines the language ability of LLMs with human causal expertise. AI researchers have addressed this direction, with explainable AI and neuro-symbolic AI being the next logical steps in further advancement in AI. However, they are still based on data-driven knowledge integration instead of human-in-the-loop approaches. Future research should measure success not only by accuracy but by improvements in clinician understanding, workflow fit, and patient outcomes. A better understanding of what helps improve human-computer interactions is greatly needed for AI systems to become part of clinical practice. 

**Abstract (ZH)**: 近期大型语言模型的兴起重新点燃了AI辅助医学诊断的希望。然而，尽管基准测试成绩令人耀眼，AI助手仍未在临床环境中带来可量化的改进。本综述旨在highlight AI在临床环境中进行实际贡献的局限性，特别是在痴呆症诊断和护理中的应用。独立的机器学习模型在模式识别方面表现出色，但很少能提供可操作的、可解释的指导，削弱了临床医生的信任。医生与大型语言模型的相邻使用并未提高诊断准确性和速度。关键限制源自数据驱动的范式：不透明的黑箱输出、易产生幻觉以及因果推理能力较弱。结合统计学习与基于专家规则的知识，并在整个过程中涉及临床医生的方法有助于恢复可解释性。这种方法也更符合现有的临床工作流程，如PEIRS和ATHENA-CDS等示例所示。未来的决策支持应优先考虑解释一致性，通过将LLM的自然语言能力与人类因果专业知识相结合的神经符号AI或混合AI来实现这一目标。AI研究者已经朝着这一方向努力，可解释AI和神经符号AI是进一步推进AI逻辑步骤。然而，它们仍然基于数据驱动的知识整合，而非人工在环的方法。未来的研究应该不仅以准确性来衡量成功，还应关注临床医生的理解、工作流程的适应性和患者的结局。更好地理解如何提高人机交互将有助于AI系统成为临床实践的一部分。 

---
# Rethinking the Illusion of Thinking 

**Title (ZH)**: 重新思考思考的错觉 

**Authors**: Iñaki Dellibarda Varela, Pablo Romero-Sorozabal, Eduardo Rocon, Manuel Cebrian  

**Link**: [PDF](https://arxiv.org/pdf/2507.01231)  

**Abstract**: Earlier this year, Apple ignited controversy by publishing "The Illusion of Thinking," prompting heated debate within the AI community. Critics seized upon the findings as conclusive evidence that Large Reasoning Models (LRMs) lack genuine reasoning capabilities, branding them as mere stochastic parrots. Meanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning the experimental setup as flawed and the conclusions overstated. We clarify this debate by replicating and refining two of the original study's most contentious benchmarks: Towers of Hanoi and River Crossing. By introducing incremental stepwise prompting and agentic collaborative dialogue, we show that previously reported failures solving the Towers of Hanoi were not purely result of output constraints, but also partly a result of cognition limitations: LRMs still stumble when complexity rises moderately (around 8 disks). Moreover, the River Crossing results initially heralded as catastrophic failures turn out to hinge upon testing unsolvable configurations. Once we limit tests strictly to solvable problems-LRMs effortlessly solve large instances involving over 100 agent pairs. Our findings ultimately defy simplistic narratives: today's LRMs are stochastic, RL-tuned searchers in a discrete state space we barely understand. Real progress in symbolic, long-horizon reasoning demands mapping that terrain through fine-grained ablations like those introduced here. 

**Abstract (ZH)**: Earlier 今年初，苹果公司通过发布《思考的幻象》引发了争议，引发了AI界激烈的讨论。批评者将研究结果视为确凿证据，认为大型推理模型缺乏真正的推理能力，仅仅是随机鹦鹉。同时，捍卫者（Lawsen等，2025）反驳称实验设计有缺陷，结论夸大其词。我们通过复制并改进原始研究中最具争议的两项基准测试——汉诺塔塔和过河——澄清了这一争论。通过引入逐步提示和有代理的合作对话，我们表明，先前报道的汉诺塔解决方案失败不仅源自输出限制，还部分源于认知限制：当复杂度适度增加（大约8个盘子）时，大型推理模型仍会遇到困难。此外，最初被认为灾难性的过河测试结果实际上取决于测试不可解的配置。一旦严格限制测试范围为可解问题，大型推理模型就能轻易解决涉及超过100个代理的大型实例。我们的发现最终打破了简单的叙事框架：当今的大型推理模型是离散状态空间中随机化强化学习优化的搜索者，我们对其知之甚少。在符号、长时推理方面的真正进步需要通过此类精细拆分的方法来绘制这一领域。 

---
# AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation 

**Title (ZH)**: 自适应协调扩散变换器：面向移动操作的任务适应协调扩散变换器 

**Authors**: Sixiang Chen, Jiaming Liu, Siyuan Qian, Han Jiang, Lily Li, Renrui Zhang, Zhuoyang Liu, Chenyang Gu, Chengkai Hou, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2507.01961)  

**Abstract**: Recently, mobile manipulation has attracted increasing attention for enabling language-conditioned robotic control in household tasks. However, existing methods still face challenges in coordinating mobile base and manipulator, primarily due to two limitations. On the one hand, they fail to explicitly model the influence of the mobile base on manipulator control, which easily leads to error accumulation under high degrees of freedom. On the other hand, they treat the entire mobile manipulation process with the same visual observation modality (e.g., either all 2D or all 3D), overlooking the distinct multimodal perception requirements at different stages during mobile manipulation. To address this, we propose the Adaptive Coordination Diffusion Transformer (AC-DiT), which enhances mobile base and manipulator coordination for end-to-end mobile manipulation. First, since the motion of the mobile base directly influences the manipulator's actions, we introduce a mobility-to-body conditioning mechanism that guides the model to first extract base motion representations, which are then used as context prior for predicting whole-body actions. This enables whole-body control that accounts for the potential impact of the mobile base's motion. Second, to meet the perception requirements at different stages of mobile manipulation, we design a perception-aware multimodal conditioning strategy that dynamically adjusts the fusion weights between various 2D visual images and 3D point clouds, yielding visual features tailored to the current perceptual needs. This allows the model to, for example, adaptively rely more on 2D inputs when semantic information is crucial for action prediction, while placing greater emphasis on 3D geometric information when precise spatial understanding is required. We validate AC-DiT through extensive experiments on both simulated and real-world mobile manipulation tasks. 

**Abstract (ZH)**: 近期，移动操作在使机器人能够在家庭任务中实现基于语言的控制方面引起了越来越多的关注。然而，现有方法在协调移动底座和 manipulator 时仍面临挑战，主要由于两个限制。一方面，它们未能明确建模移动底座对 manipulator 控制的影响，容易导致在高自由度下产生误差累积。另一方面，它们用相同的视觉观察模态（例如，全都是 2D 或全都是 3D）对待整个移动操作过程，忽略了移动操作不同阶段的多模态感知需求差异。为了解决这一问题，我们提出了自适应协调扩散变换器（AC-DiT），它增强了移动底座和 manipulator 的协调，以实现端到端的移动操作。首先，由于移动底座的运动直接影响 manipulator 的动作，我们引入了一种移动性到身体的条件机制，指导模型首先提取底座运动表示，并将其用作全局动作预测的上下文先验，从而使整个身体控制能够考虑到移动底座运动的潜在影响。其次，为了满足移动操作不同阶段的感知需求，我们设计了一种感知增强的多模态条件策略，动态调整各种 2D 视觉图像和 3D 点云之间的融合权重，以生成适应当前感知需求的视觉特征。这使模型能够，在语义信息对于动作预测至关重要时，更多地依赖 2D 输入；而在需要精确空间理解时，则更多地强调 3D 几何信息。我们通过在模拟和真实世界的移动操作任务中的广泛实验验证了 AC-DiT。 

---
# Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation 

**Title (ZH)**: 面向局部性的并行解码以实现高效的自回归图像生成 

**Authors**: Zhuoyang Zhang, Luke J. Huang, Chengyue Wu, Shang Yang, Kelly Peng, Yao Lu, Song Han  

**Link**: [PDF](https://arxiv.org/pdf/2507.01957)  

**Abstract**: We present Locality-aware Parallel Decoding (LPD) to accelerate autoregressive image generation. Traditional autoregressive image generation relies on next-patch prediction, a memory-bound process that leads to high latency. Existing works have tried to parallelize next-patch prediction by shifting to multi-patch prediction to accelerate the process, but only achieved limited parallelization. To achieve high parallelization while maintaining generation quality, we introduce two key techniques: (1) Flexible Parallelized Autoregressive Modeling, a novel architecture that enables arbitrary generation ordering and degrees of parallelization. It uses learnable position query tokens to guide generation at target positions while ensuring mutual visibility among concurrently generated tokens for consistent parallel decoding. (2) Locality-aware Generation Ordering, a novel schedule that forms groups to minimize intra-group dependencies and maximize contextual support, enhancing generation quality. With these designs, we reduce the generation steps from 256 to 20 (256$\times$256 res.) and 1024 to 48 (512$\times$512 res.) without compromising quality on the ImageNet class-conditional generation, and achieving at least 3.4$\times$ lower latency than previous parallelized autoregressive models. 

**Abstract (ZH)**: 局部意识并行解码加速自回归图像生成 

---
# How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks 

**Title (ZH)**: GPT-4o在视觉理解方面的能力如何？多模态基础模型在标准计算机视觉任务上的评估 

**Authors**: Rahul Ramachandran, Ali Garjani, Roman Bachmann, Andrei Atanov, Oğuzhan Fatih Kar, Amir Zamir  

**Link**: [PDF](https://arxiv.org/pdf/2507.01955)  

**Abstract**: Multimodal foundation models, such as GPT-4o, have recently made remarkable progress, but it is not clear where exactly these models stand in terms of understanding vision. In this paper, we benchmark the performance of popular multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0 Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision tasks (semantic segmentation, object detection, image classification, depth and surface normal prediction) using established datasets (e.g., COCO, ImageNet and its variants, etc).
The main challenges to performing this are: 1) most models are trained to output text and cannot natively express versatile domains, such as segments or 3D geometry, and 2) many leading models are proprietary and accessible only at an API level, i.e., there is no weight access to adapt them. We address these challenges by translating standard vision tasks into equivalent text-promptable and API-compatible tasks via prompt chaining to create a standardized benchmarking framework.
We observe that 1) the models are not close to the state-of-the-art specialist models at any task. However, 2) they are respectable generalists; this is remarkable as they are presumably trained on primarily image-text-based tasks. 3) They perform semantic tasks notably better than geometric ones. 4) While the prompt-chaining techniques affect performance, better models exhibit less sensitivity to prompt variations. 5) GPT-4o performs the best among non-reasoning models, securing the top position in 4 out of 6 tasks, 6) reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a preliminary analysis of models with native image generation, like the latest GPT-4o, shows they exhibit quirks like hallucinations and spatial misalignments. 

**Abstract (ZH)**: 多模态基础模型，如GPT-4o，在视觉理解方面取得了显著进展，但这些模型在理解视觉方面的具体水平尚不明确。本文在标准计算机视觉任务（语义分割、物体检测、图像分类、深度和表面法线预测）上，使用标准数据集（如COCO、ImageNet及其变体等）评估了流行多模态基础模型（GPT-4o、o4-mini、Gemini 1.5 Pro、Gemini 2.0 Flash、Claude 3.5 Sonnet、Qwen2-VL、Llama 3.2）的表现。 

---
# SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars 

**Title (ZH)**: SpecCLIP: 对比和翻译光谱测量以匹配恒星 

**Authors**: Xiaosheng Zhao, Yang Huang, Guirong Xue, Xiao Kong, Jifeng Liu, Xiaoyu Tang, Timothy C. Beers, Yuan-Sen Ting, A-Li Luo  

**Link**: [PDF](https://arxiv.org/pdf/2507.01939)  

**Abstract**: In recent years, large language models (LLMs) have transformed natural language understanding through vast datasets and large-scale parameterization. Inspired by this success, we present SpecCLIP, a foundation model framework that extends LLM-inspired methodologies to stellar spectral analysis. Stellar spectra, akin to structured language, encode rich physical and chemical information about stars. By training foundation models on large-scale spectral datasets, our goal is to learn robust and informative embeddings that support diverse downstream applications. As a proof of concept, SpecCLIP involves pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed by contrastive alignment using the CLIP (Contrastive Language-Image Pre-training) framework, adapted to associate spectra from different instruments. This alignment is complemented by auxiliary decoders that preserve spectrum-specific information and enable translation (prediction) between spectral types, with the former achieved by maximizing mutual information between embeddings and input spectra. The result is a cross-spectrum framework enabling intrinsic calibration and flexible applications across instruments. We demonstrate that fine-tuning these models on moderate-sized labeled datasets improves adaptability to tasks such as stellar-parameter estimation and chemical-abundance determination. SpecCLIP also enhances the accuracy and precision of parameter estimates benchmarked against external survey data. Additionally, its similarity search and cross-spectrum prediction capabilities offer potential for anomaly detection. Our results suggest that contrastively trained foundation models enriched with spectrum-aware decoders can advance precision stellar spectroscopy. 

**Abstract (ZH)**: 基于对比学习的大型语言模型扩展框架：SpecCLIP在恒星光谱分析中的应用 

---
# Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla 

**Title (ZH)**: 低资源语言环境下ASR模型的适应性：Whisper与Wav2Vec-BERT在孟加拉语中的比较研究 

**Authors**: Md Sazzadul Islam Ridoy, Sumi Akter, Md. Aminur Rahman  

**Link**: [PDF](https://arxiv.org/pdf/2507.01931)  

**Abstract**: In recent years, neural models trained on large multilingual text and speech datasets have shown great potential for supporting low-resource languages. This study investigates the performances of two state-of-the-art Automatic Speech Recognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to evaluate model performances. Through systematic fine-tuning and hyperparameter optimization, including learning rate, epochs, and model checkpoint selection, we have compared the models based on Word Error Rate (WER), Character Error Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model outperformed Whisper across all key evaluation metrics, demonstrated superior performance while requiring fewer computational resources, and offered valuable insights to develop robust speech recognition systems in low-resource linguistic settings. 

**Abstract (ZH)**: 最近几年，用于大规模多语言文本和语音数据训练的神经模型在支持低资源语言方面展现了巨大潜力。本研究考察了两种最先进的自动语音识别（ASR）模型，OpenAI的Whisper（Small & Large-V2）和Facebook的Wav2Vec-BERT在孟加拉语低资源语言上的性能。我们使用两个公开可用的数据集Mozilla Common Voice-17和OpenSLR来评估模型性能。通过系统的微调和超参数优化，包括学习率、epoch次数和模型检查点的选择，我们基于词错误率（WER）、字符错误率（CER）、训练时间以及计算效率对模型进行了比较。Wav2Vec-BERT模型在所有关键评价指标上均优于Whisper，表现出更优秀的性能并需要更少的计算资源，为在低资源语言背景下开发稳健的语音识别系统提供了宝贵见解。 

---
# Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection 

**Title (ZH)**: 基于半监督异常检测解决标签稀缺性的一种混合深度学习方法在精神健康医疗提供商账单异常检测中的探索 

**Authors**: Samirah Bakker, Yao Ma, Seyed Sahand Mohammadi Ziabari  

**Link**: [PDF](https://arxiv.org/pdf/2507.01924)  

**Abstract**: The complexity of mental healthcare billing enables anomalies, including fraud. While machine learning methods have been applied to anomaly detection, they often struggle with class imbalance, label scarcity, and complex sequential patterns. This study explores a hybrid deep learning approach combining Long Short-Term Memory (LSTM) networks and Transformers, with pseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior work has not evaluated such hybrid models trained on pseudo-labeled data in the context of healthcare billing. The approach is evaluated on two real-world billing datasets related to mental healthcare. The iForest LSTM baseline achieves the highest recall (0.963) on declaration-level data. On the operation-level data, the hybrid iForest-based model achieves the highest recall (0.744), though at the cost of lower precision. These findings highlight the potential of combining pseudo-labeling with hybrid deep learning in complex, imbalanced anomaly detection settings. 

**Abstract (ZH)**: 医疗保健billing复杂性导致异常，包括欺诈。虽然已经应用了机器学习方法进行异常检测，但它们往往难以处理类别不平衡、标签稀缺和复杂的序列模式。本研究探索结合长短期记忆（LSTM）网络和变换器的混合深度学习方法，并通过孤立森林（iForest）和自动编码器（AE）进行伪标签化。以往的研究尚未在医疗保健billing上下文中评估基于伪标签数据训练的此类混合模型。该方法在两个与医疗保健billing相关的实际billing数据集上进行评估。iForest LSTM基线在声明级数据上获得了最高的召回率（0.963）。在操作级数据上，基于iForest的混合模型获得了最高的召回率（0.744），但精确度较低。这些发现突显了在复杂、不平衡的异常检测设置中结合伪标签与混合深度学习的潜力。 

---
# End-to-End Large Portfolio Optimization for Variance Minimization with Neural Networks through Covariance Cleaning 

**Title (ZH)**: 通过协方差清洁利用神经网络实现端到端大型投资组合优化以最小化方差 

**Authors**: Christian Bongiorno, Efstratios Manolakis, Rosario Nunzio Mantegna  

**Link**: [PDF](https://arxiv.org/pdf/2507.01918)  

**Abstract**: We develop a rotation-invariant neural network that provides the global minimum-variance portfolio by jointly learning how to lag-transform historical returns and how to regularise both the eigenvalues and the marginal volatilities of large equity covariance matrices. This explicit mathematical mapping offers clear interpretability of each module's role, so the model cannot be regarded as a pure black-box. The architecture mirrors the analytical form of the global minimum-variance solution yet remains agnostic to dimension, so a single model can be calibrated on panels of a few hundred stocks and applied, without retraining, to one thousand US equities-a cross-sectional jump that demonstrates robust out-of-sample generalisation. The loss function is the future realized minimum portfolio variance and is optimized end-to-end on real daily returns. In out-of-sample tests from January 2000 to December 2024 the estimator delivers systematically lower realised volatility, smaller maximum drawdowns, and higher Sharpe ratios than the best analytical competitors, including state-of-the-art non-linear shrinkage. Furthermore, although the model is trained end-to-end to produce an unconstrained (long-short) minimum-variance portfolio, we show that its learned covariance representation can be used in general optimizers under long-only constraints with virtually no loss in its performance advantage over competing estimators. These gains persist when the strategy is executed under a highly realistic implementation framework that models market orders at the auctions, empirical slippage, exchange fees, and financing charges for leverage, and they remain stable during episodes of acute market stress. 

**Abstract (ZH)**: 一种旋转不变神经网络及其在全局最小方差组合中的应用 

---
# Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models 

**Title (ZH)**: 适应梯度多目标大型语言模型政策优化：朝向多目标对齐 

**Authors**: Chengao Li, Hanyu Zhang, Yunkun Xu, Hongyan Xue, Xiang Ao, Qing He  

**Link**: [PDF](https://arxiv.org/pdf/2507.01915)  

**Abstract**: Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful technique for aligning large language models (LLMs) with human preferences. However, effectively aligning LLMs with diverse human preferences remains a significant challenge, particularly when they are conflict. To address this issue, we frame human value alignment as a multi-objective optimization problem, aiming to maximize a set of potentially conflicting objectives. We introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning paradigm that employs multiple-gradient descent to align LLMs with diverse preference distributions. GAPO adaptively rescales the gradients for each objective to determine an update direction that optimally balances the trade-offs between objectives. Additionally, we introduce P-GAPO, which incorporates user preferences across different objectives and achieves Pareto solutions that better align with the user's specific needs. Our theoretical analysis demonstrates that GAPO converges towards a Pareto optimal solution for multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms current state-of-the-art methods, achieving superior performance in both helpfulness and harmlessness. 

**Abstract (ZH)**: 基于人类反馈的强化学习（RLHF）已成为一种将大型语言模型（LLMs）与人类偏好对齐的强大技术。然而，有效地将LLMs与多样化的人类偏好对齐仍然是一个重大挑战，特别是当这些偏好存在冲突时。为了解决这一问题，我们将人类价值对齐视为一个多目标优化问题，旨在最大化一系列潜在冲突的目标。我们提出了梯度自适应策略优化（GAPO），这是一种新颖的微调范式，利用多梯度下降来使LLMs与多样的偏好分布对齐。GAPO自适应地重新缩放每个目标的梯度，以确定最优平衡不同目标之间权衡的更新方向。此外，我们提出了P-GAPO，它结合了不同目标上的用户偏好，并达到了更好地满足用户特定需求的帕累托最优解。我们的理论分析表明，GAPO能够收敛到多个目标的帕累托最优解。实验结果表明，GAPO在Mistral-7B上优于当前最先进的方法，在有用性和无害性方面均表现出更好的性能。 

---
# AI4Research: A Survey of Artificial Intelligence for Scientific Research 

**Title (ZH)**: AI4Research：人工智能在科学研究中的应用综述 

**Authors**: Qiguang Chen, Mingda Yang, Libo Qin, Jinhao Liu, Zheng Yan, Jiannan Guan, Dengyun Peng, Yiyan Ji, Hanjing Li, Mengkang Hu, Yimeng Zhang, Yihao Liang, Yuhang Zhou, Jiaqi Wang, Zhi Chen, Wanxiang Che  

**Link**: [PDF](https://arxiv.org/pdf/2507.01903)  

**Abstract**: Recent advancements in artificial intelligence (AI), particularly in large language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated remarkable capabilities in complex domains such as logical reasoning and experimental coding. Motivated by these advancements, numerous studies have explored the application of AI in the innovation process, particularly in the context of scientific research. These AI technologies primarily aim to develop systems that can autonomously conduct research processes across a wide range of scientific disciplines. Despite these significant strides, a comprehensive survey on AI for Research (AI4Research) remains absent, which hampers our understanding and impedes further development in this field. To address this gap, we present a comprehensive survey and offer a unified perspective on AI4Research. Specifically, the main contributions of our work are as follows: (1) Systematic taxonomy: We first introduce a systematic taxonomy to classify five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key research gaps and highlight promising future directions, focusing on the rigor and scalability of automated experiments, as well as the societal impact. (3) Abundant applications and resources: Finally, we compile a wealth of resources, including relevant multidisciplinary applications, data corpora, and tools. We hope our work will provide the research community with quick access to these resources and stimulate innovative breakthroughs in AI4Research. 

**Abstract (ZH)**: Recent advancements in artificial intelligence (AI) and large language models (LLMs) in logical reasoning and experimental coding have demonstrated remarkable capabilities in complex domains. Motivated by these advancements, numerous studies have explored the application of AI in scientific research innovation. To address the absence of a comprehensive survey on AI for Research (AI4Research), we present a comprehensive survey and offer a unified perspective on AI4Research. 

---
# Towards Foundation Auto-Encoders for Time-Series Anomaly Detection 

**Title (ZH)**: 面向时间序列异常检测的foundation自动编码器研究 

**Authors**: Gastón García González, Pedro Casas, Emilio Martínez, Alicia Fernández  

**Link**: [PDF](https://arxiv.org/pdf/2507.01875)  

**Abstract**: We investigate a novel approach to time-series modeling, inspired by the successes of large pretrained foundation models. We introduce FAE (Foundation Auto-Encoders), a foundation generative-AI model for anomaly detection in time-series data, based on Variational Auto-Encoders (VAEs). By foundation, we mean a model pretrained on massive amounts of time-series data which can learn complex temporal patterns useful for accurate modeling, forecasting, and detection of anomalies on previously unseen datasets. FAE leverages VAEs and Dilated Convolutional Neural Networks (DCNNs) to build a generic model for univariate time-series modeling, which could eventually perform properly in out-of-the-box, zero-shot anomaly detection applications. We introduce the main concepts of FAE, and present preliminary results in different multi-dimensional time-series datasets from various domains, including a real dataset from an operational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset. 

**Abstract (ZH)**: 我们提出了一种受大规模预训练基础模型成功启发的时间序列建模新方法。我们引入了FAE（基础自编码器），这是一种基于变异自编码器（VAE）的时间序列异常检测生成AI模型。通过基础这一概念，我们指的是在一个庞大时间序列数据集上预训练的模型，能够学习复杂的时序模式，从而有助于准确建模、预测和检测未见过数据集的异常。FAE利用了变异自编码器和扩张卷积神经网络（DCNN）来构建一种通用的单变量时间序列建模模型，最终可在开箱即用和零样本异常检测应用中表现良好。我们介绍了FAE的核心概念，并在多个跨领域的多维时间序列数据集上展示了初步结果，这些数据集包括一个实际操作中的移动ISP数据集和著名的KDD 2021异常检测数据集。 

---
# Bridging UI Design and chatbot Interactions: Applying Form-Based Principles to Conversational Agents 

**Title (ZH)**: UI设计与聊天机器人互动的桥梁：基于表单原则的应用于对话代理 

**Authors**: Sanjay Krishna Anbalagan, Xinrui Nie, Umesh Mohan, Vijay Kumar Kanamarlapudi, Anughna Kommalapati, Xiaodan Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2507.01862)  

**Abstract**: Domain specific chatbot applications often involve multi step interactions, such as refining search filters, selecting multiple items, or performing comparisons. Traditional graphical user interfaces (GUIs) handle these workflows by providing explicit "Submit" (commit data) and "Reset" (discard data) actions, allowing back-end systems to track user intent unambiguously. In contrast, conversational agents rely on subtle language cues, which can lead to confusion and incomplete context management. This paper proposes modeling these GUI inspired metaphors acknowledgment (submit like) and context switching (reset-like) as explicit tasks within large language model (LLM) prompts. By capturing user acknowledgment, reset actions, and chain of thought (CoT) reasoning as structured session data, we preserve clarity, reduce user confusion, and align domain-specific chatbot interactions with back-end logic. We demonstrate our approach in hotel booking and customer management scenarios, highlighting improvements in multi-turn task coherence, user satisfaction, and efficiency. 

**Abstract (ZH)**: 基于GUI启发的元模型在大型语言模型中建模acknowledgment和context switching以优化域特定聊天机器人交互 

---
# mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling 

**Title (ZH)**: mGRADE: 最小循环门控结合延迟卷积的轻量级序列建模方法 

**Authors**: Tristan Torchet, Christian Metzner, Laura Kriener, Melika Payvand  

**Link**: [PDF](https://arxiv.org/pdf/2507.01829)  

**Abstract**: Edge devices for temporal processing demand models that capture both short- and long- range dynamics under tight memory constraints. While Transformers excel at sequence modeling, their quadratic memory scaling with sequence length makes them impractical for such settings. Recurrent Neural Networks (RNNs) offer constant memory but train sequentially, and Temporal Convolutional Networks (TCNs), though efficient, scale memory with kernel size. To address this, we propose mGRADE (mininally Gated Recurrent Architecture with Delay Embedding), a hybrid-memory system that integrates a temporal 1D-convolution with learnable spacings followed by a minimal gated recurrent unit (minGRU). This design allows the convolutional layer to realize a flexible delay embedding that captures rapid temporal variations, while the recurrent module efficiently maintains global context with minimal memory overhead. We validate our approach on two synthetic tasks, demonstrating that mGRADE effectively separates and preserves multi-scale temporal features. Furthermore, on challenging pixel-by-pixel image classification benchmarks, mGRADE consistently outperforms both pure convolutional and pure recurrent counterparts using approximately 20% less memory footprint, highlighting its suitability for memory-constrained temporal processing at the edge. This highlights mGRADE's promise as an efficient solution for memory-constrained multi-scale temporal processing at the edge. 

**Abstract (ZH)**: 边缘设备对时序处理的需求模型应在紧凑的内存约束下捕捉短程和长程动态。为应对这一挑战，我们提出了mGRADE（最小门控循环架构与延迟嵌入），这是一种结合时序一维卷积和可学习间隔，随后是minimal gated recurrent unit (minGRU)的混合内存系统。此设计允许卷积层实现灵活的延迟嵌入，捕捉快速时序变化，同时循环模块以最小的内存开销高效保持全局上下文。我们在两个合成任务上验证了我们的方法，证明mGRADE能够有效分离并保留多尺度时序特征。此外，在具有挑战性的逐像素图像分类基准测试中，mGRADE在相同内存占用下比纯粹卷积和纯粹循环对应的模型表现更好，突显了其在边缘设备上进行多尺度时序处理的高效性。 

---
# MILP-SAT-GNN: Yet Another Neural SAT Solver 

**Title (ZH)**: MILP-SAT-GNN：另一个神经SAT求解器 

**Authors**: Franco Alberto Cardillo, Hamza Khyari, Umberto Straccia  

**Link**: [PDF](https://arxiv.org/pdf/2507.01825)  

**Abstract**: We proposes a novel method that enables Graph Neural Networks (GNNs) to solve SAT problems by leveraging a technique developed for applying GNNs to Mixed Integer Linear Programming (MILP). Specifically, k-CNF formulae are mapped into MILP problems, which are then encoded as weighted bipartite graphs and subsequently fed into a GNN for training and testing. From a theoretical perspective: (i) we establish permutation and equivalence invariance results, demonstrating that the method produces outputs that are stable under reordering of clauses and variables; (ii) we identify a theoretical limitation, showing that for a class of formulae called foldable formulae, standard GNNs cannot always distinguish satisfiable from unsatisfiable instances; (iii) we prove a universal approximation theorem, establishing that with Random Node Initialization (RNI), the method can approximate SAT solving to arbitrary precision on finite datasets, that is, the GNN becomes approximately sound and complete on such datasets. Furthermore, we show that for unfoldable formulae, the same approximation guarantee can be achieved without the need for RNI. Finally, we conduct an experimental evaluation of our approach, which show that, despite the simplicity of the neural architecture, the method achieves promising results. 

**Abstract (ZH)**: 我们提出了一种新颖的方法，通过利用将图形神经网络（GNNs）应用于混合整数线性规划（MILP）的技术来解决可满足性（SAT）问题。具体而言，k-CNF公式被映射到MILP问题，然后编码为加权二分图，并随后输入GNN进行训练和测试。从理论角度来看：（i）我们建立了置换和等价不变性结果，表明该方法在重排序子句和变量时生成的输出是稳定的；（ii）我们识别了一个理论上的限制，表明对于一类称为可折叠公式的问题，标准的GNN不能总是在判别可满足实例和不可满足实例时发挥作用；（iii）我们证明了一个通用逼近定理，表明通过随机节点初始化（RNI），该方法可以针对有限数据集以任意精度逼近SAT求解，即GNN在这些数据集上变得近似地有效且完备。此外，我们展示了对于非可展开公式，可以在无需RNI的情况下达到相同的逼近保证。最后，我们对本方法进行了实验评估，结果显示，尽管神经网络架构的简单性，该方法取得了令人鼓舞的结果。 

---
# Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems 

**Title (ZH)**: 保护制造商的隐私保护人工智能工具：基于隐私保护机器学习解决实际问题的案例研究 

**Authors**: Xiaoyu Ji, Jessica Shorland, Joshua Shank, Pascal Delpe-Brice, Latanya Sweeney, Jan Allebach, Ali Shakouri  

**Link**: [PDF](https://arxiv.org/pdf/2507.01808)  

**Abstract**: Small- and medium-sized manufacturers need innovative data tools but, because of competition and privacy concerns, often do not want to share their proprietary data with researchers who might be interested in helping. This paper introduces a privacy-preserving platform by which manufacturers may safely share their data with researchers through secure methods, so that those researchers then create innovative tools to solve the manufacturers' real-world problems, and then provide tools that execute solutions back onto the platform for others to use with privacy and confidentiality guarantees. We illustrate this problem through a particular use case which addresses an important problem in the large-scale manufacturing of food crystals, which is that quality control relies on image analysis tools. Previous to our research, food crystals in the images were manually counted, which required substantial and time-consuming human efforts, but we have developed and deployed a crystal analysis tool which makes this process both more rapid and accurate. The tool enables automatic characterization of the crystal size distribution and numbers from microscope images while the natural imperfections from the sample preparation are automatically removed; a machine learning model to count high resolution translucent crystals and agglomeration of crystals was also developed to aid in these efforts. The resulting algorithm was then packaged for real-world use on the factory floor via a web-based app secured through the originating privacy-preserving platform, allowing manufacturers to use it while keeping their proprietary data secure. After demonstrating this full process, future directions are also explored. 

**Abstract (ZH)**: 小型和中型制造商需要创新的数据工具，但由于竞争和隐私 concerns，他们通常不愿将其专有数据与可能愿意帮助他们解决问题的研究人员分享。本文介绍了一个隐私保护平台，通过安全方法使制造商可以安全地与研究人员共享其数据，从而让研究人员开发出创新工具来解决制造商的实际问题，并将执行解决方案的工具通过平台提供给他人使用，同时保证隐私和保密性。我们通过一个特定的用例来阐述这一问题，该用例针对大规模制造食品晶体中的一个重要问题，即质量控制依赖于图像分析工具。在我们之前的研究所之前，食品晶体在图像中的数量都是通过人工手动计数，这需要大量的时间和人力，但我们开发并部署了一个晶体分析工具，使这一过程既更快又更准确。该工具能够自动分析显微镜图像中的晶体尺寸分布和数量，并自动去除样本准备过程中自然产生的不完善；我们还开发了一个用于统计高分辨率透明晶体及其结晶聚集的机器学习模型，以辅助这些工作。所得到的算法随后通过起源于隐私保护平台的基于Web的应用程序在工厂现场进行打包，使制造商能够在保护其专有数据的同时使用它。在演示了整个过程后，还探讨了未来的发展方向。 

---
# LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs 

**Title (ZH)**: 无需GPU的LoRA微调：一种用于大语言模型的CPU高效元生成框架 

**Authors**: Reza Arabpour, Haitz Sáez de Ocáriz Borde, Anastasis Kratsios  

**Link**: [PDF](https://arxiv.org/pdf/2507.01806)  

**Abstract**: Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language Models (LLMs) by enabling parameter-efficient updates. However, their widespread adoption remains limited by the reliance on GPU-based training. In this work, we propose a theoretically grounded approach to LoRA fine-tuning designed specifically for users with limited computational resources, particularly those restricted to standard laptop CPUs. Our method learns a meta-operator that maps any input dataset, represented as a probability distribution, to a set of LoRA weights by leveraging a large bank of pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of performing new gradient-based updates, our pipeline constructs adapters via lightweight combinations of existing LoRAs directly on CPU. While the resulting adapters do not match the performance of GPU-trained counterparts, they consistently outperform the base Mistral model on downstream tasks, offering a practical and accessible alternative to traditional GPU-based fine-tuning. 

**Abstract (ZH)**: 基于理论支持的LoRA微调方法：针对有限计算资源用户的设计 

---
# How Do Vision-Language Models Process Conflicting Information Across Modalities? 

**Title (ZH)**: 视觉-语言模型如何处理跨模态的矛盾信息？ 

**Authors**: Tianze Hua, Tian Yun, Ellie Pavlick  

**Link**: [PDF](https://arxiv.org/pdf/2507.01790)  

**Abstract**: AI models are increasingly required to be multimodal, integrating disparate input streams into a coherent state representation on which subsequent behaviors and actions can be based. This paper seeks to understand how such models behave when input streams present conflicting information. Focusing specifically on vision-language models, we provide inconsistent inputs (e.g., an image of a dog paired with the caption "A photo of a cat") and ask the model to report the information present in one of the specific modalities (e.g., "What does the caption say / What is in the image?"). We find that models often favor one modality over the other, e.g., reporting the image regardless of what the caption says, but that different models differ in which modality they favor. We find evidence that the behaviorally preferred modality is evident in the internal representational structure of the model, and that specific attention heads can restructure the representations to favor one modality over the other. Moreover, we find modality-agnostic "router heads" which appear to promote answers about the modality requested in the instruction, and which can be manipulated or transferred in order to improve performance across datasets and modalities. Together, the work provides essential steps towards identifying and controlling if and how models detect and resolve conflicting signals within complex multimodal environments. 

**Abstract (ZH)**: AI模型在处理冲突输入信息时的多模态行为研究 

---
# Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging 

**Title (ZH)**: 视觉变换器表示具有语义意义吗？以医学成像为例 

**Authors**: Montasir Shams, Chashi Mahiul Islam, Shaeke Salman, Phat Tran, Xiuwen Liu  

**Link**: [PDF](https://arxiv.org/pdf/2507.01788)  

**Abstract**: Vision transformers (ViTs) have rapidly gained prominence in medical imaging tasks such as disease classification, segmentation, and detection due to their superior accuracy compared to conventional deep learning models. However, due to their size and complex interactions via the self-attention mechanism, they are not well understood. In particular, it is unclear whether the representations produced by such models are semantically meaningful. In this paper, using a projected gradient-based algorithm, we show that their representations are not semantically meaningful and they are inherently vulnerable to small changes. Images with imperceptible differences can have very different representations; on the other hand, images that should belong to different semantic classes can have nearly identical representations. Such vulnerability can lead to unreliable classification results; for example, unnoticeable changes cause the classification accuracy to be reduced by over 60\%. %. To the best of our knowledge, this is the first work to systematically demonstrate this fundamental lack of semantic meaningfulness in ViT representations for medical image classification, revealing a critical challenge for their deployment in safety-critical systems. 

**Abstract (ZH)**: Vision变压器（ViTs）在疾病分类、分割和检测等医学影像任务中由于其优越的准确率已迅速获得了重要地位，但由于其规模庞大和通过自注意力机制实现的复杂交互，它们并不容易理解。特别是，目前尚不清楚此类模型生成的表示是否具有语义意义。在本文中，我们使用投影梯度基算法展示了它们的表示并不是语义上有意义的，并且本质上对细微变化非常敏感。不可感知差异的图像可以具有非常不同的表示；另一方面，应该属于不同语义类别的图像可以具有几乎相同的表示。这种脆弱性可能导致分类结果不可靠；例如，细微变化会导致分类准确率下降超过60%。据我们所知，这是首次系统地证明ViT表示在医学图像分类中的根本缺乏语义意义的工作，揭示了其在安全关键系统部署中的一个关键挑战。 

---
# Probing Evaluation Awareness of Language Models 

**Title (ZH)**: 探究语言模型的评价意识 

**Authors**: Jord Nguyen, Khiem Hoang, Carlo Leonardo Attubato, Felix Hofstätter  

**Link**: [PDF](https://arxiv.org/pdf/2507.01786)  

**Abstract**: Language models can distinguish between testing and deployment phases -- a capability known as evaluation awareness. This has significant safety and policy implications, potentially undermining the reliability of evaluations that are central to AI governance frameworks and voluntary industry commitments. In this paper, we study evaluation awareness in Llama-3.3-70B-Instruct. We show that linear probes can separate real-world evaluation and deployment prompts, suggesting that current models internally represent this distinction. We also find that current safety evaluations are correctly classified by the probes, suggesting that they already appear artificial or inauthentic to models. Our findings underscore the importance of ensuring trustworthy evaluations and understanding deceptive capabilities. More broadly, our work showcases how model internals may be leveraged to support blackbox methods in safety audits, especially for future models more competent at evaluation awareness and deception. 

**Abstract (ZH)**: 语言模型能够区分测试阶段和部署阶段——这一能力称为评估意识。这在安全性和政策层面具有重要影响，可能损害与人工智能治理框架和自愿行业承诺密切相关的关键评估的可靠性。在本文中，我们研究了评估意识在Llama-3.3-70B-Instruct中的表现。我们显示线性探针能够区分真实世界的评估和部署提示，表明当前模型内部代表了这种区分。我们还发现当前的安全评估已被探针正确分类，表明它们已经显得人工或不真实。我们的发现强调了确保可靠评估和理解欺骗性能力的重要性。更广泛而言，我们的研究展示了模型内部机制如何被利用来支持安全性审核中的黑盒方法，特别是对于未来在评估意识和欺骗方面更为熟练的模型。 

---
# MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining 

**Title (ZH)**: MuRating: 一种高质量多语言大型语言模型预训练数据选择方法 

**Authors**: Zhixun Chen, Ping Guo, Wenhan Han, Yifan Zhang, Binbin Liu, Haobin Lin, Fengze Liu, Yan Zhao, Bingni Zhang, Taifeng Wang, Yin Zheng, Meng Fang  

**Link**: [PDF](https://arxiv.org/pdf/2507.01785)  

**Abstract**: Data quality is a critical driver of large language model performance, yet existing model-based selection methods focus almost exclusively on English. We introduce MuRating, a scalable framework that transfers high-quality English data-quality signals into a single rater for 17 target languages. MuRating aggregates multiple English "raters" via pairwise comparisons to learn unified document-quality scores,then projects these judgments through translation to train a multilingual evaluator on monolingual, cross-lingual, and parallel text pairs. Applied to web data, MuRating selects balanced subsets of English and multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to strong baselines, including QuRater, AskLLM, DCLM and so on, our approach boosts average accuracy on both English benchmarks and multilingual evaluations, with especially large gains on knowledge-intensive tasks. We further analyze translation fidelity, selection biases, and underrepresentation of narrative material, outlining directions for future work. 

**Abstract (ZH)**: MuRating：将高质量英语数据质量信号转换为17种目标语言的单 raters 评定框架 

---
# BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification 

**Title (ZH)**: BranchNet: 一种用于结构化多类别分类的神经符号学习框架 

**Authors**: Dalia Rodríguez-Salas, Christian Riess  

**Link**: [PDF](https://arxiv.org/pdf/2507.01781)  

**Abstract**: We introduce BranchNet, a neuro-symbolic learning framework that transforms decision tree ensembles into sparse, partially connected neural networks. Each branch, defined as a decision path from root to a parent of leaves, is mapped to a hidden neuron, preserving symbolic structure while enabling gradient-based optimization. The resulting models are compact, interpretable, and require no manual architecture tuning. Evaluated on a suite of structured multi-class classification benchmarks, BranchNet consistently outperforms XGBoost in accuracy, with statistically significant gains. We detail the architecture, training procedure, and sparsity dynamics, and discuss the model's strengths in symbolic interpretability as well as its current limitations, particularly on binary tasks where further adaptive calibration may be beneficial. 

**Abstract (ZH)**: BranchNet：一种将决策树ensemble转换为稀疏部分连接神经网络的神经符号学习框架 

---
# GPU-based complete search for nonlinear minimization subject to bounds 

**Title (ZH)**: 基于GPU的区间约束非线性最小化完全搜索 

**Authors**: Guanglu Zhang, Qihang Shan, Jonathan Cagan  

**Link**: [PDF](https://arxiv.org/pdf/2507.01770)  

**Abstract**: This paper introduces a GPU-based complete search method to enclose the global minimum of a nonlinear function subject to simple bounds on the variables. Using interval analysis, coupled with the computational power and architecture of GPU, the method iteratively rules out the regions in the search domain where the global minimum cannot exist and leaves a finite set of regions where the global minimum must exist. For effectiveness, because of the rigor of interval analysis, the method is guaranteed to enclose the global minimum of the nonlinear function even in the presence of rounding errors. For efficiency, the method employs a novel GPU-based single program, single data parallel programming style to circumvent major GPU performance bottlenecks, and a variable cycling technique is also integrated into the method to reduce computational cost when minimizing large-scale nonlinear functions. The method is validated by minimizing 10 multimodal benchmark test functions with scalable dimensions, including the well-known Ackley function, Griewank function, Levy function, and Rastrigin function. These benchmark test functions represent grand challenges of global optimization, and enclosing the guaranteed global minimum of these benchmark test functions with more than 80 dimensions has not been reported in the literature. Our method completely searches the feasible domain and successfully encloses the guaranteed global minimum of these 10 benchmark test functions with up to 10,000 dimensions using only one GPU in a reasonable computation time, far exceeding the reported results in the literature due to the unique method design and implementation based on GPU architecture. 

**Abstract (ZH)**: 基于GPU的区间分析全面搜索方法用于在变量简单约束下包围非线性函数的全局最小值 

---
# Enhanced Generative Model Evaluation with Clipped Density and Coverage 

**Title (ZH)**: 增强生成模型评估：裁剪密度与覆盖范围方法 

**Authors**: Nicolas Salvy, Hugues Talbot, Bertrand Thirion  

**Link**: [PDF](https://arxiv.org/pdf/2507.01761)  

**Abstract**: Although generative models have made remarkable progress in recent years, their use in critical applications has been hindered by their incapacity to reliably evaluate sample quality. Quality refers to at least two complementary concepts: fidelity and coverage. Current quality metrics often lack reliable, interpretable values due to an absence of calibration or insufficient robustness to outliers. To address these shortcomings, we introduce two novel metrics, Clipped Density and Clipped Coverage. By clipping individual sample contributions and, for fidelity, the radii of nearest neighbor balls, our metrics prevent out-of-distribution samples from biasing the aggregated values. Through analytical and empirical calibration, these metrics exhibit linear score degradation as the proportion of poor samples increases. Thus, they can be straightforwardly interpreted as equivalent proportions of good samples. Extensive experiments on synthetic and real-world datasets demonstrate that Clipped Density and Clipped Coverage outperform existing methods in terms of robustness, sensitivity, and interpretability for evaluating generative models. 

**Abstract (ZH)**: 尽管生成模型在近年来取得了显著进步，但在关键应用中的使用受到了其无法可靠评估样本质量的限制。质量至少包括两个互补的概念：保真度和覆盖度。当前的质量度量往往由于缺乏校准或对离群值的鲁棒性不足而缺乏可靠的、可解释的值。为了应对这些缺陷，我们引入了两个新的度量标准：剪裁密度和剪裁覆盖度。通过剪裁个体样本贡献和，对于保真度，最近邻球体的半径，这些度量标准可以防止分布外样本偏倚汇总值。通过分析和经验校准，这些度量标准显示出线性得分下降，当不良样本的比例增加时。因此，它们可以简单地解释为良好样本的比例。在合成和实际数据集上的广泛实验表明，剪裁密度和剪裁覆盖度在鲁棒性、敏感性和可解释性方面优于现有方法，用于评估生成模型。 

---
# Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training 

**Title (ZH)**: 无窥探调整：LLM 剪枝后训练可证明的隐私和泛化边界 

**Authors**: Ismail Labiad, Mathurin Videau, Matthieu Kowalski, Marc Schoenauer, Alessandro Leite, Julia Kempe, Olivier Teytaud  

**Link**: [PDF](https://arxiv.org/pdf/2507.01752)  

**Abstract**: Gradient-based optimization is the workhorse of deep learning, offering efficient and scalable training via backpropagation. However, its reliance on large volumes of labeled data raises privacy and security concerns such as susceptibility to data poisoning attacks and the risk of overfitting. In contrast, black box optimization methods, which treat the model as an opaque function, relying solely on function evaluations to guide optimization, offer a promising alternative in scenarios where data access is restricted, adversarial risks are high, or overfitting is a concern. However, black box methods also pose significant challenges, including poor scalability to high-dimensional parameter spaces, as prevalent in large language models (LLMs), and high computational costs due to reliance on numerous model evaluations. This paper introduces BBoxER, an evolutionary black-box method for LLM post-training that induces an information bottleneck via implicit compression of the training data. Leveraging the tractability of information flow, we provide strong theoretical bounds on generalization, differential privacy, susceptibility to data poisoning attacks, and robustness to extraction attacks. BBoxER operates on top of pre-trained LLMs, offering a lightweight and modular enhancement suitable for deployment in restricted or privacy-sensitive environments, in addition to non-vacuous generalization guarantees. In experiments with LLMs, we demonstrate empirically that Retrofitting methods are able to learn, showing how a few iterations of BBoxER improve performance and generalize well on a benchmark of reasoning datasets. This positions BBoxER as an attractive add-on on top of gradient-based optimization. 

**Abstract (ZH)**: 基于梯度的优化是深度学习的基石，通过反向传播提供高效且可扩展的训练。然而，其对大量标注数据的依赖引发了隐私和安全方面的关切，如数据投毒攻击的易感性和过拟合风险。相比之下，黑箱优化方法将模型视为不透明函数，仅依靠函数评估来指导优化，在数据访问受限、敌对风险高或存在过拟合的情况下，提供了有前景的替代方案。然而，黑箱方法也面临着重大挑战，包括在大规模参数空间中的可扩展性差，以及因依赖多次模型评估而导致的高计算成本。本文介绍了BBoxER，一种用于大规模语言模型（LLMs）后训练的进化黑箱方法，通过隐式压缩训练数据来诱导信息瓶颈。利用信息流的可处理性，我们提供了泛化、差分隐私、数据投毒攻击易感性以及提取攻击抗性的强大理论界。BBoxER 基建在预训练的 LLMs 上，提供了一种轻量级且模块化的增强功能，适用于受限或隐私敏感环境，并提供非真空的泛化保证。在大规模语言模型的实验中，我们实证展示了重塑方法能够学习，并展示了 BBoxER 几轮迭代如何改善性能并在推理数据集基准上很好地泛化。这使 BBoxER 成为了基于梯度优化的一种有吸引力的附加选项。 

---
# ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving 

**Title (ZH)**: ECCV 2024 W-CODA: 第一届多模态corner cases感知与理解工作坊 

**Authors**: Kai Chen, Ruiyuan Gao, Lanqing Hong, Hang Xu, Xu Jia, Holger Caesar, Dengxin Dai, Bingbing Liu, Dzmitry Tsishkou, Songcen Xu, Chunjing Xu, Qiang Xu, Huchuan Lu, Dit-Yan Yeung  

**Link**: [PDF](https://arxiv.org/pdf/2507.01735)  

**Abstract**: In this paper, we present details of the 1st W-CODA workshop, held in conjunction with the ECCV 2024. W-CODA aims to explore next-generation solutions for autonomous driving corner cases, empowered by state-of-the-art multimodal perception and comprehension techniques. 5 Speakers from both academia and industry are invited to share their latest progress and opinions. We collect research papers and hold a dual-track challenge, including both corner case scene understanding and generation. As the pioneering effort, we will continuously bridge the gap between frontier autonomous driving techniques and fully intelligent, reliable self-driving agents robust towards corner cases. 

**Abstract (ZH)**: 本文展示了与ECCV 2024联合举办的第1届W-CODA研讨会的详细情况。W-CODA旨在通过利用最先进的多模态感知和理解技术探索自主驾驶corner case的下一代解决方案。邀请了5位来自学术界和工业界的讲者分享他们的最新进展和观点。我们收集研究论文并举行双轨挑战赛，包括corner case场景的理解和生成。作为这一领域的首创努力，我们将持续弥合前沿自主驾驶技术与面向corner case的智能、可靠的自主驱动代理之间的差距。 

---
# Towards culturally-appropriate conversational AI for health in the majority world: An exploratory study with citizens and professionals in Latin America 

**Title (ZH)**: 面向大多数国家的健康对话AI的文化适应性研究：拉丁美洲公民和专业人士的探索性研究 

**Authors**: Dorian Peters, Fernanda Espinoza, Marco da Re, Guido Ivetta, Luciana Benotti, Rafael A. Calvo  

**Link**: [PDF](https://arxiv.org/pdf/2507.01719)  

**Abstract**: There is justifiable interest in leveraging conversational AI (CAI) for health across the majority world, but to be effective, CAI must respond appropriately within culturally and linguistically diverse contexts. Therefore, we need ways to address the fact that current LLMs exclude many lived experiences globally. Various advances are underway which focus on top-down approaches and increasing training data. In this paper, we aim to complement these with a bottom-up locally-grounded approach based on qualitative data collected during participatory workshops in Latin America. Our goal is to construct a rich and human-centred understanding of: a) potential areas of cultural misalignment in digital health; b) regional perspectives on chatbots for health and c)strategies for creating culturally-appropriate CAI; with a focus on the understudied Latin American context. Our findings show that academic boundaries on notions of culture lose meaning at the ground level and technologies will need to engage with a broader framework; one that encapsulates the way economics, politics, geography and local logistics are entangled in cultural experience. To this end, we introduce a framework for 'Pluriversal Conversational AI for Health' which allows for the possibility that more relationality and tolerance, rather than just more data, may be called for. 

**Abstract (ZH)**: 在全世界范围内利用对话人工智能（CAI）促进健康是有充分理由的，但为了有效，CAI 必须在多元的文化和语言背景下做出恰当的响应。因此，我们需要解决当前语言模型排除全球许多生活体验的问题。尽管存在各种以自上而下方法和增加训练数据为主的进展，我们在本文中旨在通过在拉丁美洲参与式工作坊中收集的定性数据，采用自下而上的当地扎根方法来进行补充。我们的目标是构建一个丰富的人本中心的理解：a) 数字健康中的潜在文化错位区域；b) 区域内的聊天机器人在健康领域的视角；c) 创造适合文化背景的CAI的策略；重点关注相对较少研究的拉丁美洲背景。我们的发现表明，关于文化的学术边界在基层失去了意义，技术需要与更广泛的框架相结合；这一框架涵盖了经济、政治、地理和当地物流与文化体验交织的方式。为了实现这一目标，我们介绍了“整体对话人工智能（CAI）促进健康”框架，该框架允许更多的关系性和包容性，而不仅仅是更多的数据。 

---
# AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness 

**Title (ZH)**: AdamMeme: 自适应探究多模态大型语言模型的有害性推理能力 

**Authors**: Zixin Chen, Hongzhan Lin, Kaixin Li, Ziyang Luo, Zhen Ye, Guang Chen, Zhiyong Huang, Jing Ma  

**Link**: [PDF](https://arxiv.org/pdf/2507.01702)  

**Abstract**: The proliferation of multimodal memes in the social media era demands that multimodal Large Language Models (mLLMs) effectively understand meme harmfulness. Existing benchmarks for assessing mLLMs on harmful meme understanding rely on accuracy-based, model-agnostic evaluations using static datasets. These benchmarks are limited in their ability to provide up-to-date and thorough assessments, as online memes evolve dynamically. To address this, we propose AdamMeme, a flexible, agent-based evaluation framework that adaptively probes the reasoning capabilities of mLLMs in deciphering meme harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive evaluations by iteratively updating the meme data with challenging samples, thereby exposing specific limitations in how mLLMs interpret harmfulness. Extensive experiments show that our framework systematically reveals the varying performance of different target mLLMs, offering in-depth, fine-grained analyses of model-specific weaknesses. Our code is available at this https URL. 

**Abstract (ZH)**: 社交媒体时代多模态 meme 的流行要求多模态大型语言模型 (mLLMs) 有效地理解 meme 的危害性。现有的评估 mLLMs 在理解有害 meme 方面的基准依赖于基于准确性的、模型无关的评估，使用静态数据集。这些基准在提供最新的全面评估方面有限制，因为在线 meme 演变动态。为了解决这一问题，我们提出 AdamMeme，一个灵活的、基于代理的评估框架，能够适应性地探查 mLLMs 在解读 meme 危害性方面的推理能力。通过多代理协作，AdamMeme 提供了全面的评估，通过迭代更新具有挑战性的 meme 数据，从而揭示 mLLMs 在解读危害性方面的特定局限性。广泛实验表明，我们的框架系统地揭示了不同目标 mLLMs 的不同表现，提供对模型特定缺点的深入、精细分析。我们的代码可在以下网址获取：这个 https URL。 

---
# Exploring Advanced LLM Multi-Agent Systems Based on Blackboard Architecture 

**Title (ZH)**: 基于黑板架构探索高级LLM多agent系统 

**Authors**: Bochen Han, Songmao Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2507.01701)  

**Abstract**: In this paper, we propose to incorporate the blackboard architecture into LLM multi-agent systems (MASs) so that (1) agents with various roles can share all the information and others' messages during the whole problem-solving process, (2) agents that will take actions are selected based on the current content of the blackboard, and (3) the selection and execution round is repeated until a consensus is reached on the blackboard. We develop the first implementation of this proposal and conduct experiments on commonsense knowledge, reasoning and mathematical datasets. The results show that our system can be competitive with the SOTA static and dynamic MASs by achieving the best average performance, and at the same time manage to spend less tokens. Our proposal has the potential to enable complex and dynamic problem-solving where well-defined structures or workflows are unavailable. 

**Abstract (ZH)**: 在本文中，我们提出将黑板架构整合到LLM多智能体系统（MASs）中，以便（1）所有角色的智能体在整个问题解决过程中可以共享所有信息和他人的消息，（2）根据黑板上的当前内容选择将要采取行动的智能体，（3）选择和执行循环直至在黑板上达成共识。我们开发了此提案的第一个实现，并在常识知识、推理和数学数据集中进行了实验。结果表明，我们的系统在平均性能上可以与领先的静态和动态MASs竞争，并且能够使用更少的令牌。我们的提案有可能在缺少明确结构或工作流程的情况下实现复杂且动态的问题解决。 

---
# Relational Causal Discovery with Latent Confounders 

**Title (ZH)**: 潜混杂因素下的关系因果发现 

**Authors**: Andrea Piras, Matteo Negro, Ragib Ahsan, David Arbour, Elena Zheleva  

**Link**: [PDF](https://arxiv.org/pdf/2507.01700)  

**Abstract**: Estimating causal effects from real-world relational data can be challenging when the underlying causal model and potential confounders are unknown. While several causal discovery algorithms exist for learning causal models with latent confounders from data, they assume that the data is independent and identically distributed (i.i.d.) and are not well-suited for learning from relational data. Similarly, existing relational causal discovery algorithms assume causal sufficiency, which is unrealistic for many real-world datasets. To address this gap, we propose RelFCI, a sound and complete causal discovery algorithm for relational data with latent confounders. Our work builds upon the Fast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms and it defines new graphical models, necessary to support causal discovery in relational domains. We also establish soundness and completeness guarantees for relational d-separation with latent confounders. We present experimental results demonstrating the effectiveness of RelFCI in identifying the correct causal structure in relational causal models with latent confounders. 

**Abstract (ZH)**: 基于潜在混杂因素的关联数据因果发现：RelFCI算法 

---
# GPT, But Backwards: Exactly Inverting Language Model Outputs 

**Title (ZH)**: GPT，但反着来：精确反转语言模型输出 

**Authors**: Adrians Skapars, Edoardo Manino, Youcheng Sun, Lucas C. Cordeiro  

**Link**: [PDF](https://arxiv.org/pdf/2507.01693)  

**Abstract**: While existing auditing techniques attempt to identify potential unwanted behaviours in large language models (LLMs), we address the complementary forensic problem of reconstructing the exact input that led to an existing LLM output - enabling post-incident analysis and potentially the detection of fake output reports. We formalize exact input reconstruction as a discrete optimisation problem with a unique global minimum and introduce SODA, an efficient gradient-based algorithm that operates on a continuous relaxation of the input search space with periodic restarts and parameter decay. Through comprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we demonstrate that SODA significantly outperforms existing approaches. We succeed in fully recovering 79.5% of shorter out-of-distribution inputs from next-token logits, without a single false positive, but struggle to extract private information from the outputs of longer (15+ token) input sequences. This suggests that standard deployment practices may currently provide adequate protection against malicious use of our method. Our code is available at this https URL. 

**Abstract (ZH)**: 现有的审计技术试图在大规模语言模型（LLMs）中识别潜在的不良行为，而我们则针对这一问题的互补方面——即重建导致现有LLM输出的精确输入，从而实现事后分析并可能检测假输出报告进行研究。我们将精确输入重建形式化为一个具有唯一全局最小值的离散优化问题，并引入SODA，一种基于梯度的有效算法，该算法在输入搜索空间的连续松弛上运行，并定期重启和参数衰减。通过针对从33M到3B参数不等的LLM进行全面实验，我们证明了SODA显著优于现有方法。我们成功从下一个 Tokens 的 logit 中恢复了 79.5% 的较短的 distribution 外输入，且没有单一的误报，但对于更长（15 个以上 Token）的输入序列，我们难以从其输出中提取隐私信息。这表明当前的标准部署实践可能已提供足够的保护以防止对我们方法的恶意使用。我们的代码可在以下链接获取。 

---
# Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling 

**Title (ZH)**: 监督微调与强化学习微调结合的前缀采样方法 

**Authors**: Zeyu Huang, Tianhao Cheng, Zihan Qiu, Zili Wang, Yinghui Xu, Edoardo M. Ponti, Ivan Titov  

**Link**: [PDF](https://arxiv.org/pdf/2507.01679)  

**Abstract**: Existing post-training techniques for large language models are broadly categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking demonstration data but can lead to problematic generalization as a form of behavior cloning. Conversely, RFT can significantly enhance a model's performance but is prone to learn unexpected behaviors, and its performance is highly sensitive to the initial policy. In this paper, we propose a unified view of these methods and introduce Prefix-RFT, a hybrid approach that synergizes learning from both demonstration and exploration. Using mathematical reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is both simple and effective. It not only surpasses the performance of standalone SFT and RFT but also outperforms parallel mixed-policy RFT methods. A key advantage is its seamless integration into existing open-source frameworks, requiring only minimal modifications to the standard RFT pipeline. Our analysis highlights the complementary nature of SFT and RFT, and validates that Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore, ablation studies confirm the method's robustness to variations in the quality and quantity of demonstration data. We hope this work offers a new perspective on LLM post-training, suggesting that a unified paradigm that judiciously integrates demonstration and exploration could be a promising direction for future research. 

**Abstract (ZH)**: 现有的大型语言模型后训练技术通常被归类为监督微调（SFT）和强化微调（RFT）。每种范式都存在独特的权衡：SFT在模拟示范数据方面表现出色，但可能导致行为克隆形式的问题性泛化。相反，RFT能够显著提升模型性能，但也容易学习到意外行为，其性能对初始策略高度敏感。在本文中，我们提出了一种统一的观点，并引入了Prefix-RFT，这是一种结合示范学习和探索学习的混合方法。通过使用数学推理问题作为实验平台，我们实证证明了Prefix-RFT简洁且有效。它不仅超越了单独的SFT和RFT性能，还优于并行混合策略的RFT方法。一个关键优势在于它能够无缝集成到现有的开源框架中，只需对标准RFT管道进行少量修改。我们的分析突显了SFT和RFT的互补性，并验证了Prefix-RFT有效地整合了这两种学习范式。此外，消融研究确认了该方法对示范数据质量与数量变化的稳健性。我们认为这项工作为大型语言模型后训练提供了一个新的视角，建议一个谨慎整合示范和探索的统一范式可能是未来研究的有希望方向。 

---
# Deep Recommender Models Inference: Automatic Asymmetric Data Flow Optimization 

**Title (ZH)**: 深度推荐模型推理：自动非对称数据流优化 

**Authors**: Giuseppe Ruggeri, Renzo Andri, Daniele Jahier Pagliari, Lukas Cavigelli  

**Link**: [PDF](https://arxiv.org/pdf/2507.01676)  

**Abstract**: Deep Recommender Models (DLRMs) inference is a fundamental AI workload accounting for more than 79% of the total AI workload in Meta's data centers. DLRMs' performance bottleneck is found in the embedding layers, which perform many random memory accesses to retrieve small embedding vectors from tables of various sizes. We propose the design of tailored data flows to speedup embedding look-ups. Namely, we propose four strategies to look up an embedding table effectively on one core, and a framework to automatically map the tables asymmetrically to the multiple cores of a SoC. We assess the effectiveness of our method using the Huawei Ascend AI accelerators, comparing it with the default Ascend compiler, and we perform high-level comparisons with Nvidia A100. Results show a speed-up varying from 1.5x up to 6.5x for real workload distributions, and more than 20x for extremely unbalanced distributions. Furthermore, the method proves to be much more independent of the query distribution than the baseline. 

**Abstract (ZH)**: DLRMs嵌入查找加速设计：在Meta数据中心占总AI工作负载超过79%的DLRMs推理中，性能瓶颈在于嵌入层进行的许多随机内存访问以检索来自不同大小表中的小嵌入向量。我们提出了定制数据流设计以加速嵌入查找。具体地，我们提出了四种策略在单个内核上有效地查找嵌入表，并提出了一种框架以自动将表非对称地映射到SoC的多个内核。我们使用华为Ascend AI加速器评估了该方法的有效性，将其与默认Ascend编译器进行比较，并与Nvidia A100进行了高级比较。结果显示，在真实工作负载分布下加速比从1.5倍到6.5倍不等，而在极端不平衡分布下超过20倍。此外，该方法在查询分布方面的独立性远高于基线。 

---
# Comparing Optimization Algorithms Through the Lens of Search Behavior Analysis 

**Title (ZH)**: 通过搜索行为分析比较优化算法 

**Authors**: Gjorgjina Cenikj, Gašper Petelin, Tome Eftimov  

**Link**: [PDF](https://arxiv.org/pdf/2507.01668)  

**Abstract**: The field of numerical optimization has recently seen a surge in the development of "novel" metaheuristic algorithms, inspired by metaphors derived from natural or human-made processes, which have been widely criticized for obscuring meaningful innovations and failing to distinguish themselves from existing approaches. Aiming to address these concerns, we investigate the applicability of statistical tests for comparing algorithms based on their search behavior. We utilize the cross-match statistical test to compare multivariate distributions and assess the solutions produced by 114 algorithms from the MEALPY library. These findings are incorporated into an empirical analysis aiming to identify algorithms with similar search behaviors. 

**Abstract (ZH)**: 数值优化领域最近见证了“新颖”的元启发式算法的发展 surge，这些算法受到自然或人造过程的比喻启发，但常被批评模糊了有意义的创新并难以区分自己与现有方法。为应对这些担忧，我们研究了基于搜索行为比较算法的应用统计检验方法。我们使用交叉匹配统计检验比较多元分布，并评估MEALPY库中114个算法产生的解决方案。这些发现被纳入一项实证分析，旨在识别具有类似搜索行为的算法。 

---
# AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training 

**Title (ZH)**: AsyncFlow：一种用于高效大语言模型后训练的异步流式RL框架 

**Authors**: Zhenyu Han, Ansheng You, Haibo Wang, Kui Luo, Guang Yang, Wenqi Shi, Menglong Chen, Sicheng Zhang, Zeshun Lan, Chunshi Deng, Huazhong Ji, Wenjie Liu, Yu Huang, Yixiang Zhang, Chenyi Pan, Jing Wang, Xin Huang, Chunsheng Li, Jianping Wu  

**Link**: [PDF](https://arxiv.org/pdf/2507.01663)  

**Abstract**: Reinforcement learning (RL) has become a pivotal technology in the post-training phase of large language models (LLMs). Traditional task-colocated RL frameworks suffer from significant scalability bottlenecks, while task-separated RL frameworks face challenges in complex dataflows and the corresponding resource idling and workload imbalance. Moreover, most existing frameworks are tightly coupled with LLM training or inference engines, making it difficult to support custom-designed engines. To address these challenges, we propose AsyncFlow, an asynchronous streaming RL framework for efficient post-training. Specifically, we introduce a distributed data storage and transfer module that provides a unified data management and fine-grained scheduling capability in a fully streamed manner. This architecture inherently facilitates automated pipeline overlapping among RL tasks and dynamic load balancing. Moreover, we propose a producer-consumer-based asynchronous workflow engineered to minimize computational idleness by strategically deferring parameter update process within staleness thresholds. Finally, the core capability of AsynFlow is architecturally decoupled from underlying training and inference engines and encapsulated by service-oriented user interfaces, offering a modular and customizable user experience. Extensive experiments demonstrate an average of 1.59 throughput improvement compared with state-of-the-art baseline. The presented architecture in this work provides actionable insights for next-generation RL training system designs. 

**Abstract (ZH)**: 异步流：用于高效后训练的强化学习框架 

---
# Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective 

**Title (ZH)**: 具有线性复杂度的自回归图像生成：一种空间aware衰减视角 

**Authors**: Yuxin Mao, Zhen Qin, Jinxing Zhou, Hui Deng, Xuyang Shen, Bin Fan, Jing Zhang, Yiran Zhong, Yuchao Dai  

**Link**: [PDF](https://arxiv.org/pdf/2507.01652)  

**Abstract**: Autoregressive (AR) models have garnered significant attention in image generation for their ability to effectively capture both local and global structures within visual data. However, prevalent AR models predominantly rely on the transformer architectures, which are beset by quadratic computational complexity concerning input sequence length and substantial memory overhead due to the necessity of maintaining key-value caches. Although linear attention mechanisms have successfully reduced this burden in language models, our initial experiments reveal that they significantly degrade image generation quality because of their inability to capture critical long-range dependencies in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a novel attention mechanism that explicitly preserves genuine 2D spatial relationships within the flattened image sequences by computing position-dependent decay factors based on true 2D spatial location rather than 1D sequence positions. Based on this mechanism, we present LASADGen, an autoregressive image generator that enables selective attention to relevant spatial contexts with linear complexity. Experiments on ImageNet show LASADGen achieves state-of-the-art image generation performance and computational efficiency, bridging the gap between linear attention's efficiency and spatial understanding needed for high-quality generation. 

**Abstract (ZH)**: 基于空间意识衰减的线性注意力机制在图像生成中的应用：LASADGen 

---
# GradMetaNet: An Equivariant Architecture for Learning on Gradients 

**Title (ZH)**: GradMetaNet: 一种基于梯度的学习同胚架构 

**Authors**: Yoav Gelberg, Yam Eitan, Aviv Navon, Aviv Shamsian, Theo, Putterman, Michael Bronstein, Haggai Maron  

**Link**: [PDF](https://arxiv.org/pdf/2507.01649)  

**Abstract**: Gradients of neural networks encode valuable information for optimization, editing, and analysis of models. Therefore, practitioners often treat gradients as inputs to task-specific algorithms, e.g. for pruning or optimization. Recent works explore learning algorithms that operate directly on gradients but use architectures that are not specifically designed for gradient processing, limiting their applicability. In this paper, we present a principled approach for designing architectures that process gradients. Our approach is guided by three principles: (1) equivariant design that preserves neuron permutation symmetries, (2) processing sets of gradients across multiple data points to capture curvature information, and (3) efficient gradient representation through rank-1 decomposition. Based on these principles, we introduce GradMetaNet, a novel architecture for learning on gradients, constructed from simple equivariant blocks. We prove universality results for GradMetaNet, and show that previous approaches cannot approximate natural gradient-based functions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness on a diverse set of gradient-based tasks on MLPs and transformers, such as learned optimization, INR editing, and estimating loss landscape curvature. 

**Abstract (ZH)**: 神经网络梯度中编码的信息用于模型优化、编辑和分析具有重要价值。因此，实践者常将梯度作为特定任务算法的输入，例如剪枝或优化。近期的研究探索直接在梯度上操作的学习算法，但这些算法的架构并未专门设计用于梯度处理，限制了其应用范围。在本文中，我们提出了一种严谨的方法来设计处理梯度的架构。我们的方法遵循三个原则：（1）保持神经元置换对称性的协变设计；（2）处理多个数据点上的梯度集以捕获曲率信息；（3）通过秩1分解高效表示梯度。基于这些原则，我们引入了GradMetaNet，这是一种用于在梯度上学习的新架构，由简单的协变块构建而成。我们证明了GradMetaNet的通用性结果，并展示了先前的方法无法逼近GradMetaNet可以逼近的自然梯度基函数。然后，我们通过在MLP和变换器上的多种梯度基任务中证明GradMetaNet的有效性，如学习优化、INR编辑和估计损失景观曲率。 

---
# Customized Exploration of Landscape Features Driving Multi-Objective Combinatorial Optimization Performance 

**Title (ZH)**: 定制化探索驱动多目标组合优化性能的景观特征分析 

**Authors**: Ana Nikolikj, Gabriela Ochoa, Tome Eftimov  

**Link**: [PDF](https://arxiv.org/pdf/2507.01638)  

**Abstract**: We present an analysis of landscape features for predicting the performance of multi-objective combinatorial optimization algorithms. We consider features from the recently proposed compressed Pareto Local Optimal Solutions Networks (C-PLOS-net) model of combinatorial landscapes. The benchmark instances are a set of rmnk-landscapes with 2 and 3 objectives and various levels of ruggedness and objective correlation. We consider the performance of three algorithms -- Pareto Local Search (PLS), Global Simple EMO Optimizer (GSEMO), and Non-dominated Sorting Genetic Algorithm (NSGA-II) - using the resolution and hypervolume metrics. Our tailored analysis reveals feature combinations that influence algorithm performance specific to certain landscapes. This study provides deeper insights into feature importance, tailored to specific rmnk-landscapes and algorithms. 

**Abstract (ZH)**: 我们提出了一种关于 landscapes 特征在预测多目标组合优化算法性能方面的分析。我们考虑了最近提出的压缩帕雷托局部最优解网络（C-PLOS-net）模型中的组合 landscapes 特征。基准实例为包含 2 和 3 个目标以及不同复杂度和目标相关性的 rmnk-landscapes。我们使用分辨率和 hypervolume 指标分析了 Pareto 局部搜索（PLS）、全局简单 EMO 优化器（GSEMO）和非支配排序遗传算法（NSGA-II）的性能。我们的定制分析揭示了特定 landscapes 下影响算法性能的特征组合。本研究提供了对特征重要性更深入的理解，针对特定的 rmnk-landscapes 和算法进行了定制。 

---
# Depth Anything at Any Condition 

**Title (ZH)**: 任意条件下的深度估计 

**Authors**: Boyuan Sun, Modi Jin, Bowen Yin, Qibin Hou  

**Link**: [PDF](https://arxiv.org/pdf/2507.01634)  

**Abstract**: We present Depth Anything at Any Condition (DepthAnything-AC), a foundation monocular depth estimation (MDE) model capable of handling diverse environmental conditions. Previous foundation MDE models achieve impressive performance across general scenes but not perform well in complex open-world environments that involve challenging conditions, such as illumination variations, adverse weather, and sensor-induced distortions. To overcome the challenges of data scarcity and the inability of generating high-quality pseudo-labels from corrupted images, we propose an unsupervised consistency regularization finetuning paradigm that requires only a relatively small amount of unlabeled data. Furthermore, we propose the Spatial Distance Constraint to explicitly enforce the model to learn patch-level relative relationships, resulting in clearer semantic boundaries and more accurate details. Experimental results demonstrate the zero-shot capabilities of DepthAnything-AC across diverse benchmarks, including real-world adverse weather benchmarks, synthetic corruption benchmarks, and general benchmarks.
Project Page: this https URL
Code: this https URL 

**Abstract (ZH)**: Depth Anything at Any Condition (DepthAnything-AC): 一种能够在各种条件下的单目深度估计基础模型 

---
# Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation 

**Title (ZH)**: Tile and Slide：一种从局部到全局3D地球观测扩展NeRF的新框架 

**Authors**: Camille Billouard, Dawa Derksen, Alexandre Constantin, Bruno Vallet  

**Link**: [PDF](https://arxiv.org/pdf/2507.01631)  

**Abstract**: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D reconstruction from multiview satellite imagery. However, state-of-the-art NeRF methods are typically constrained to small scenes due to the memory footprint during training, which we study in this paper. Previous work on large-scale NeRFs palliate this by dividing the scene into NeRFs. This paper introduces Snake-NeRF, a framework that scales to large scenes. Our out-of-core method eliminates the need to load all images and networks simultaneously, and operates on a single device. We achieve this by dividing the region of interest into NeRFs that 3D tile without overlap. Importantly, we crop the images with overlap to ensure each NeRFs is trained with all the necessary pixels. We introduce a novel $2\times 2$ 3D tile progression strategy and segmented sampler, which together prevent 3D reconstruction errors along the tile edges. Our experiments conclude that large satellite images can effectively be processed with linear time complexity, on a single GPU, and without compromise in quality. 

**Abstract (ZH)**: 基于神经辐射场的大型场景三维重建：Snake-NeRF 

---
# Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss 

**Title (ZH)**: 区域联合损失导向的提示引导与人类proximal感知的HOT预测 

**Authors**: Yuxiao Wang, Yu Lei, Zhenao Wei, Weiying Xue, Xinyu Jiang, Nan Zhuang, Qi Liu  

**Link**: [PDF](https://arxiv.org/pdf/2507.01630)  

**Abstract**: The task of Human-Object conTact (HOT) detection involves identifying the specific areas of the human body that are touching objects. Nevertheless, current models are restricted to just one type of image, often leading to too much segmentation in areas with little interaction, and struggling to maintain category consistency within specific regions. To tackle this issue, a HOT framework, termed \textbf{P3HOT}, is proposed, which blends \textbf{P}rompt guidance and human \textbf{P}roximal \textbf{P}erception. To begin with, we utilize a semantic-driven prompt mechanism to direct the network's attention towards the relevant regions based on the correlation between image and text. Then a human proximal perception mechanism is employed to dynamically perceive key depth range around the human, using learnable parameters to effectively eliminate regions where interactions are not expected. Calculating depth resolves the uncertainty of the overlap between humans and objects in a 2D perspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss (RJLoss) has been created as a new loss to inhibit abnormal categories in the same area. A new evaluation metric called ``AD-Acc.'' is introduced to address the shortcomings of existing methods in addressing negative samples. Comprehensive experimental results demonstrate that our approach achieves state-of-the-art performance in four metrics across two benchmark datasets. Specifically, our model achieves an improvement of \textbf{0.7}$\uparrow$, \textbf{2.0}$\uparrow$, \textbf{1.6}$\uparrow$, and \textbf{11.0}$\uparrow$ in SC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated dataset. Code is available at this https URL. 

**Abstract (ZH)**: Human-Object Contact (HOT) 检测中的 P3HOT 框架：结合提示引导和人体临近感知 

---
# Enhanced Influence-aware Group Recommendation for Online Media Propagation 

**Title (ZH)**: 增强影响力感知的群体推荐以优化在线媒体传播 

**Authors**: Chengkun He, Xiangmin Zhou, Chen Wang, Longbing Cao, Jie Shao, Xiaodong Li, Guang Xu, Carrie Jinqiu Hu, Zahir Tari  

**Link**: [PDF](https://arxiv.org/pdf/2507.01616)  

**Abstract**: Group recommendation over social media streams has attracted significant attention due to its wide applications in domains such as e-commerce, entertainment, and online news broadcasting. By leveraging social connections and group behaviours, group recommendation (GR) aims to provide more accurate and engaging content to a set of users rather than individuals. Recently, influence-aware GR has emerged as a promising direction, as it considers the impact of social influence on group decision-making. In earlier work, we proposed Influence-aware Group Recommendation (IGR) to solve this task. However, this task remains challenging due to three key factors: the large and ever-growing scale of social graphs, the inherently dynamic nature of influence propagation within user groups, and the high computational overhead of real-time group-item matching.
To tackle these issues, we propose an Enhanced Influence-aware Group Recommendation (EIGR) framework. First, we introduce a Graph Extraction-based Sampling (GES) strategy to minimise redundancy across multiple temporal social graphs and effectively capture the evolving dynamics of both groups and items. Second, we design a novel DYnamic Independent Cascade (DYIC) model to predict how influence propagates over time across social items and user groups. Finally, we develop a two-level hash-based User Group Index (UG-Index) to efficiently organise user groups and enable real-time recommendation generation. Extensive experiments on real-world datasets demonstrate that our proposed framework, EIGR, consistently outperforms state-of-the-art baselines in both effectiveness and efficiency. 

**Abstract (ZH)**: 社交媒体流上的群体推荐因在电子商务、娱乐和在线新闻广播等领域广泛的应用而引起了广泛关注。基于社交联系和群体行为，群体推荐（GR）旨在为一组用户提供更准确和吸引人的内容，而非单个用户。最近，考虑到社会影响力对群体决策的影响，影响力意识下的群体推荐成为了一个有前景的方向。在早期的工作中，我们提出了影响力意识下的群体推荐（IGR）来解决这一问题。然而，由于三个关键因素的影响——社会图形的庞大且不断增长的规模、影响力在用户群体中的固有动态传播特性以及实时群体-项目匹配的高计算开销——这一任务仍然具有挑战性。

为了应对这些挑战，我们提出了一种增强的影响力意识下的群体推荐（EIGR）框架。首先，我们引入了一种基于图提取的采样（GES）策略，以最大程度地减少多个时间社交图的冗余，并有效地捕捉群体和项目随时间的演变动态。其次，我们设计了一种新颖的动力独立级联（DYIC）模型，用于预测社会项目和用户群体中影响力随时间的传播情况。最后，我们开发了一种基于两级哈希的用户群体索引（UG-Index），以高效组织用户群体并实现实时推荐生成。在真实世界数据集上的广泛实验表明，我们提出的方法EIGR在效果和效率方面均优于最先进的基线方法。 

---
# Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems 

**Title (ZH)**: 不受约束条件下的后门攻击在人脸识别系统中的生存能力 

**Authors**: Quentin Le Roux, Yannick Teglia, Teddy Furon, Philippe Loubet-Moundi, Eric Bourbao  

**Link**: [PDF](https://arxiv.org/pdf/2507.01607)  

**Abstract**: The widespread use of deep learning face recognition raises several security concerns. Although prior works point at existing vulnerabilities, DNN backdoor attacks against real-life, unconstrained systems dealing with images captured in the wild remain a blind spot of the literature. This paper conducts the first system-level study of backdoors in deep learning-based face recognition systems. This paper yields four contributions by exploring the feasibility of DNN backdoors on these pipelines in a holistic fashion. We demonstrate for the first time two backdoor attacks on the face detection task: face generation and face landmark shift attacks. We then show that face feature extractors trained with large margin losses also fall victim to backdoor attacks. Combining our models, we then show using 20 possible pipeline configurations and 15 attack cases that a single backdoor enables an attacker to bypass the entire function of a system. Finally, we provide stakeholders with several best practices and countermeasures. 

**Abstract (ZH)**: 深度学习面部识别的广泛应用引发了若干安全关切。尽管先前工作指出了现有漏洞，但针对真实生活中未加约束系统的图像捕获进行的DNN后门攻击仍属于文献盲区。本文首次在系统层面研究了基于深度学习的面部识别系统的后门攻击。通过全面探索这些管道中DNN后门的可行性，本文提出了四项贡献。我们首次展示了两种面部检测任务中的后门攻击：面部生成攻击和面部关键点偏移攻击。随后，我们证明了使用大 margin 损失训练的面部特征提取器也受到后门攻击的影响。结合我们的模型，我们展示了在20种可能的管道配置和15种攻击情况下，单一后门能够使攻击者绕过整个系统的功能。最后，我们为相关方提供了若干最佳实践和对策。 

---
# Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems 

**Title (ZH)**: 数据代理： orchestrating 数据+AI 生态系统的综合性架构 

**Authors**: Zhaoyan Sun, Jiayi Wang, Xinyang Zhao, Jiachi Wang, Guoliang Li  

**Link**: [PDF](https://arxiv.org/pdf/2507.01599)  

**Abstract**: Traditional Data+AI systems utilize data-driven techniques to optimize performance, but they rely heavily on human experts to orchestrate system pipelines, enabling them to adapt to changes in data, queries, tasks, and environments. For instance, while there are numerous data science tools available, developing a pipeline planning system to coordinate these tools remains challenging. This difficulty arises because existing Data+AI systems have limited capabilities in semantic understanding, reasoning, and planning. Fortunately, we have witnessed the success of large language models (LLMs) in enhancing semantic understanding, reasoning, and planning abilities. It is crucial to incorporate LLM techniques to revolutionize data systems for orchestrating Data+AI applications effectively.
To achieve this, we propose the concept of a 'Data Agent' - a comprehensive architecture designed to orchestrate Data+AI ecosystems, which focuses on tackling data-related tasks by integrating knowledge comprehension, reasoning, and planning capabilities. We delve into the challenges involved in designing data agents, such as understanding data/queries/environments/tools, orchestrating pipelines/workflows, optimizing and executing pipelines, and fostering pipeline self-reflection. Furthermore, we present examples of data agent systems, including a data science agent, data analytics agents (such as unstructured data analytics agent, semantic structured data analytics agent, data lake analytics agent, and multi-modal data analytics agent), and a database administrator (DBA) agent. We also outline several open challenges associated with designing data agent systems. 

**Abstract (ZH)**: 传统数据+AI系统利用数据驱动的技术优化性能，但它们高度依赖人力专家来协调系统管道，使其能够适应数据、查询、任务和环境的变化。例如，在众多数据科学工具中，开发一个协调这些工具的管道规划系统仍然极具挑战性。这一困难源于现有数据+AI系统在语义理解、推理和规划方面的能力有限。幸运的是，我们见证了大型语言模型（LLMs）在增强这些能力方面的成功。将LLM技术整合到数据系统中，对于有效协调数据+AI应用程序至关重要。

为此，我们提出了“数据代理”的概念——一种全面架构，旨在通过整合知识理解、推理和规划能力来协调数据+AI生态系统，专注于处理与数据相关的任务。我们探讨了设计数据代理所面临的挑战，包括理解数据/查询/环境/工具、协调管道/工作流、优化和执行管道以及促进管道自我反思。此外，我们介绍了数据代理系统的示例，包括数据科学代理、数据剖析代理（如非结构化数据剖析代理、语义结构化数据剖析代理、数据湖剖析代理和多模态数据剖析代理）以及数据库管理员（DBA）代理。我们还概述了设计数据代理系统所面临的若干开放挑战。 

---
# Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring 

**Title (ZH)**: 自主人工智能 surveillance：多模态深度学习在认知和行为监测中的应用 

**Authors**: Ameer Hamza, Zuhaib Hussain But, Umar Arif, Samiya, M. Abdullah Asad, Muhammad Naeem  

**Link**: [PDF](https://arxiv.org/pdf/2507.01590)  

**Abstract**: This study presents a novel classroom surveillance system that integrates multiple modalities, including drowsiness, tracking of mobile phone usage, and face recognition,to assess student attentiveness with enhanced this http URL system leverages the YOLOv8 model to detect both mobile phone and sleep usage,(Ghatge et al., 2024) while facial recognition is achieved through LResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These models work in synergy to provide comprehensive, real-time monitoring, offering insights into student engagement and behavior.(S et al., 2023) The framework is trained on specialized datasets, such as the RMFD dataset for face recognition and a Roboflow dataset for mobile phone detection. The extensive evaluation of the system shows promising results. Sleep detection achieves 97. 42% mAP@50, face recognition achieves 86. 45% validation accuracy and mobile phone detection reach 85. 89% mAP@50. The system is implemented within a core PHP web application and utilizes ESP32-CAM hardware for seamless data capture.(Neto et al., 2024) This integrated approach not only enhances classroom monitoring, but also ensures automatic attendance recording via face recognition as students remain seated in the classroom, offering scalability for diverse educational environments.(Banada,2025) 

**Abstract (ZH)**: 一种综合多模态技术（包括困倦检测、移动设备使用跟踪和面部识别）以评估学生专注度的新型课堂监控系统：基于YOLOv8和LResNet Occ FC的综合方法 

---
# Exploring Classical Piano Performance Generation with Expressive Music Variational AutoEncoder 

**Title (ZH)**: 探究具表现力的音乐变分自动编码器在古典钢琴表演生成中的应用 

**Authors**: Jing Luo, Xinyu Yang, Jie Wei  

**Link**: [PDF](https://arxiv.org/pdf/2507.01582)  

**Abstract**: The creativity of classical music arises not only from composers who craft the musical sheets but also from performers who interpret the static notations with expressive nuances. This paper addresses the challenge of generating classical piano performances from scratch, aiming to emulate the dual roles of composer and pianist in the creative process. We introduce the Expressive Compound Word (ECP) representation, which effectively captures both the metrical structure and expressive nuances of classical performances. Building on this, we propose the Expressive Music Variational AutoEncoder (XMVAE), a model featuring two branches: a Vector Quantized Variational AutoEncoder (VQ-VAE) branch that generates score-related content, representing the Composer, and a vanilla VAE branch that produces expressive details, fulfilling the role of Pianist. These branches are jointly trained with similar Seq2Seq architectures, leveraging a multiscale encoder to capture beat-level contextual information and an orthogonal Transformer decoder for efficient compound tokens decoding. Both objective and subjective evaluations demonstrate that XMVAE generates classical performances with superior musical quality compared to state-of-the-art models. Furthermore, pretraining the Composer branch on extra musical score datasets contribute to a significant performance gain. 

**Abstract (ZH)**: 古典音乐的创造力不仅来源于创作乐谱的作曲家，还来源于以富有表达性的微妙细节诠释静态符号的表演者。本文旨在从头生成古典钢琴演奏，以模拟作曲家和演奏者在创作过程中的双重角色。我们引入了表达性复合词（ECP）表示，有效地捕捉了古典演奏的节奏结构和表达性微妙之处。在此基础上，我们提出了一种模型——表达性音乐变分自编码器（XMVAE），该模型具有两个分支：一个向量量化变分自编码器（VQ-VAE）分支用于生成与乐谱相关的内容，代表作曲家；另一个基本的VAE分支用于生成表达性细节，履行演奏者角色。这两个分支通过类似的Seq2Seq架构并利用多尺度编码器捕获节拍级别的上下文信息以及正交Transformer解码器高效地解码复合标记进行联合训练。客观和主观评估均证明，与现有模型相比，XMVAE生成的古典演奏具有更高的音乐质量。此外，对额外的音乐乐谱数据集进行预训练，可显著提升模型性能。 

---
# Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on Embedded Hardware 

**Title (ZH)**: 基于高效CNN在嵌入式硬件上的实时紧急车辆警报检测 

**Authors**: Marco Giordano, Stefano Giacomelli, Claudia Rinaldi, Fabio Graziosi  

**Link**: [PDF](https://arxiv.org/pdf/2507.01563)  

**Abstract**: We present a full-stack emergency vehicle (EV) siren detection system designed for real-time deployment on embedded hardware. The proposed approach is based on E2PANNs, a fine-tuned convolutional neural network derived from EPANNs, and optimized for binary sound event detection under urban acoustic conditions. A key contribution is the creation of curated and semantically structured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV - developed using a custom AudioSet-Tools framework to overcome the low reliability of standard AudioSet annotations. The system is deployed on a Raspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing a multithreaded inference engine with adaptive frame sizing, probability smoothing, and a decision-state machine to control false positive activations. A remote WebSocket interface provides real-time monitoring and facilitates live demonstration capabilities. Performance is evaluated using both framewise and event-based metrics across multiple configurations. Results show the system achieves low-latency detection with improved robustness under realistic audio conditions. This work demonstrates the feasibility of deploying IoS-compatible SED solutions that can form distributed acoustic monitoring networks, enabling collaborative emergency vehicle tracking across smart city infrastructures through WebSocket connectivity on low-cost edge devices. 

**Abstract (ZH)**: 一种基于E2PANNs的实际部署嵌入式硬件的全栈应急车辆警报检测系统 

---
# Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning 

**Title (ZH)**: 自引导过程奖励优化与掩码步骤优势的过程强化学习 

**Authors**: Wu Fei, Hao Kong, Shuxian Liang, Yang Lin, Yibo Yang, Jing Tang, Lei Chen, Xiansheng Hua  

**Link**: [PDF](https://arxiv.org/pdf/2507.01551)  

**Abstract**: Process Reinforcement Learning~(PRL) has demonstrated considerable potential in enhancing the reasoning capabilities of Large Language Models~(LLMs). However, introducing additional process reward models incurs substantial computational overhead, and there is no unified theoretical framework for process-level advantage estimation. To bridge this gap, we propose \textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward \textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables process-aware RL through two key innovations: (1) we first theoretically demonstrate that process rewards can be derived intrinsically from the policy model itself, and (2) we introduce well-defined cumulative process rewards and \textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which facilitates rigorous step-wise action advantage estimation within shared-prompt sampling groups. Our experimental results demonstrate that SPRO outperforms vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy improvement. Furthermore, SPRO maintains a stable and elevated policy entropy throughout training while reducing the average response length by approximately $1/3$, evidencing sufficient exploration and prevention of reward hacking. Notably, SPRO incurs no additional computational overhead compared to outcome-supervised RL methods such as GRPO, which benefit industrial implementation. 

**Abstract (ZH)**: Self-Guided Process Reward Optimization (SPRO) 

---
# Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants 

**Title (ZH)**: 匠心绘制汉字桥梁：一场为老年移民设计的AI共创工作坊 

**Authors**: Wen Zhan, Ziqun Hua, Peiyue Lin, Yunfei Chen  

**Link**: [PDF](https://arxiv.org/pdf/2507.01548)  

**Abstract**: This paper explores how older adults, particularly aging migrants in urban China, can engage AI-assisted co-creation to express personal narratives that are often fragmented, underrepresented, or difficult to verbalize. Through a pilot workshop combining oral storytelling and the symbolic reconstruction of Hanzi, participants shared memories of migration and recreated new character forms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM), together with physical materials. Supported by human facilitation and a soft AI presence, participants transformed lived experience into visual and tactile expressions without requiring digital literacy. This approach offers new perspectives on human-AI collaboration and aging by repositioning AI not as a content producer but as a supportive mechanism, and by supporting narrative agency within sociotechnical systems. 

**Abstract (ZH)**: 本研究探讨了老年人，特别是城市中国中的老化移民，如何通过AI辅助共创表达那些常常碎片化、代表性不足或难以用语言表达的个人叙事。通过结合口头讲述和汉字符号重构的试点研讨会，参与者分享了迁移记忆，并与大型语言模型（LLM）建议的 Xiaozhuan 符号一起，使用物理材料重新创造新的汉字形式。在人性化的引导和支持性人工智能的辅助下，参与者将生活经验转化为可视和触觉表达，无需具备数字素养。该方法为人类与人工智能的合作以及老龄化提供了新的视角，重新定位了人工智能的角色，不再将其视为内容生产者，而是支持机制，并在社会技术系统中支持叙事自主权。 

---
# AI and Remote Sensing for Resilient and Sustainable Built Environments: A Review of Current Methods, Open Data and Future Directions 

**Title (ZH)**: AI和遥感在韧性可持续建成环境中的应用：现有方法、开放数据及未来方向 

**Authors**: Ubada El Joulani, Tatiana Kalganova, Stergios-Aristoteles Mitoulis, Sotirios Argyroudis  

**Link**: [PDF](https://arxiv.org/pdf/2507.01547)  

**Abstract**: Critical infrastructure, such as transport networks, underpins economic growth by enabling mobility and trade. However, ageing assets, climate change impacts (e.g., extreme weather, rising sea levels), and hybrid threats ranging from natural disasters to cyber attacks and conflicts pose growing risks to their resilience and functionality. This review paper explores how emerging digital technologies, specifically Artificial Intelligence (AI), can enhance damage assessment and monitoring of transport infrastructure. A systematic literature review examines existing AI models and datasets for assessing damage in roads, bridges, and other critical infrastructure impacted by natural disasters. Special focus is given to the unique challenges and opportunities associated with bridge damage detection due to their structural complexity and critical role in connectivity. The integration of SAR (Synthetic Aperture Radar) data with AI models is also discussed, with the review revealing a critical research gap: a scarcity of studies applying AI models to SAR data for comprehensive bridge damage assessment. Therefore, this review aims to identify the research gaps and provide foundations for AI-driven solutions for assessing and monitoring critical transport infrastructures. 

**Abstract (ZH)**: 新兴数字技术，特别是人工智能（AI），如何增强运输基础设施的损害评估与监测：以桥梁损害检测为例及合成孔径雷达（SAR）数据的整合探讨 

---
# Chargax: A JAX Accelerated EV Charging Simulator 

**Title (ZH)**: Chargax: 一个使用JAX加速的电动汽车充电模拟器 

**Authors**: Koen Ponse, Jan Felix Kleuker, Aske Plaat, Thomas Moerland  

**Link**: [PDF](https://arxiv.org/pdf/2507.01522)  

**Abstract**: Deep Reinforcement Learning can play a key role in addressing sustainable energy challenges. For instance, many grid systems are heavily congested, highlighting the urgent need to enhance operational efficiency. However, reinforcement learning approaches have traditionally been slow due to the high sample complexity and expensive simulation requirements. While recent works have effectively used GPUs to accelerate data generation by converting environments to JAX, these works have largely focussed on classical toy problems. This paper introduces Chargax, a JAX-based environment for realistic simulation of electric vehicle charging stations designed for accelerated training of RL agents. We validate our environment in a variety of scenarios based on real data, comparing reinforcement learning agents against baselines. Chargax delivers substantial computational performance improvements of over 100x-1000x over existing environments. Additionally, Chargax' modular architecture enables the representation of diverse real-world charging station configurations. 

**Abstract (ZH)**: 深度强化学习在应对可持续能源挑战中可以发挥关键作用。例如，许多电网系统严重拥堵，突显了提升运行效率的迫切需求。然而，传统的强化学习方法由于较高的样本复杂度和昂贵的仿真要求而动作缓慢。尽管最近的工作通过将环境转换为JAX并利用GPU有效加速了数据生成，但这些工作主要集中在经典的玩具问题上。本文介绍了Chargax，这是一种基于JAX的环境，用于现实模拟电动汽车充电站，旨在加速RL代理的训练。我们在基于真实数据的多种场景中验证了我们的环境，并将强化学习代理与基线进行了比较。Chargax在现有环境中提供了超过100-1000倍的计算性能改进。此外，Chargax的模块化架构能够表示多种多样的实际充电站配置。 

---
# Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence 

**Title (ZH)**: 顺着线索：跨模态智能在行人重识别中的实验研究 

**Authors**: Robert Aufschläger, Youssef Shoeb, Azarm Nowzad, Michael Heigl, Fabian Bally, Martin Schramm  

**Link**: [PDF](https://arxiv.org/pdf/2507.01504)  

**Abstract**: The collection and release of street-level recordings as Open Data play a vital role in advancing autonomous driving systems and AI research. However, these datasets pose significant privacy risks, particularly for pedestrians, due to the presence of Personally Identifiable Information (PII) that extends beyond biometric traits such as faces. In this paper, we present cRID, a novel cross-modal framework combining Large Vision-Language Models, Graph Attention Networks, and representation learning to detect textual describable clues of PII and enhance person re-identification (Re-ID). Our approach focuses on identifying and leveraging interpretable features, enabling the detection of semantically meaningful PII beyond low-level appearance cues. We conduct a systematic evaluation of PII presence in person image datasets. Our experiments show improved performance in practical cross-dataset Re-ID scenarios, notably from Market-1501 to CUHK03-np (detected), highlighting the framework's practical utility. Code is available at this https URL. 

**Abstract (ZH)**: 基于跨模态框架的个人可识别信息检测与行人再识别-enhancing privacy protection in street-level recordings as open data 

---
# Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images 

**Title (ZH)**: 融合传统方法和深度学习方法检测卫星图像中的树冠 

**Authors**: Ozan Durgut, Beril Kallfelz-Sirmacek, Cem Unsalan  

**Link**: [PDF](https://arxiv.org/pdf/2507.01502)  

**Abstract**: Global warming, loss of biodiversity, and air pollution are among the most significant problems facing Earth. One of the primary challenges in addressing these issues is the lack of monitoring forests to protect them. To tackle this problem, it is important to leverage remote sensing and computer vision methods to automate monitoring applications. Hence, automatic tree crown detection algorithms emerged based on traditional and deep learning methods. In this study, we first introduce two different tree crown detection methods based on these approaches. Then, we form a novel rule-based approach that integrates these two methods to enhance robustness and accuracy of tree crown detection results. While traditional methods are employed for feature extraction and segmentation of forested areas, deep learning methods are used to detect tree crowns in our method. With the proposed rule-based approach, we post-process these results, aiming to increase the number of detected tree crowns through neighboring trees and localized operations. We compare the obtained results with the proposed method in terms of the number of detected tree crowns and report the advantages, disadvantages, and areas for improvement of the obtained outcomes. 

**Abstract (ZH)**: 全球变暖、生物多样性的丧失和空气污染是地球面临的最重大问题之一。在应对这些问题时，监测森林以保护它们是一个主要挑战。因此，需要利用遥感和计算机视觉方法来自动化监测应用，从而基于传统和深度学习方法提出了自动树冠检测算法。在本研究中，我们首先介绍了基于这两种方法的两种不同的树冠检测方法，然后提出了一种新的基于规则的方法，将这两种方法结合起来以增强树冠检测结果的可靠性和准确性。在我们的方法中，传统的技术用于提取森林区域的特征和分割，而深度学习技术则用于检测树冠。通过提出的基于规则的方法，我们对这些结果进行后处理，旨在通过邻近树木和局部操作增加检测到的树冠数量。我们从检测到的树冠数量的角度比较了提出的算法与其他方法的结果，并报告了所得结果的优点、缺点和改进空间。 

---
# Crop Pest Classification Using Deep Learning Techniques: A Review 

**Title (ZH)**: 基于深度学习技术的农作物害虫分类：一项综述 

**Authors**: Muhammad Hassam Ejaz, Muhammad Bilal, Usman Habib  

**Link**: [PDF](https://arxiv.org/pdf/2507.01494)  

**Abstract**: Insect pests continue to bring a serious threat to crop yields around the world, and traditional methods for monitoring them are often slow, manual, and difficult to scale. In recent years, deep learning has emerged as a powerful solution, with techniques like convolutional neural networks (CNNs), vision transformers (ViTs), and hybrid models gaining popularity for automating pest detection. This review looks at 37 carefully selected studies published between 2018 and 2025, all focused on AI-based pest classification. The selected research is organized by crop type, pest species, model architecture, dataset usage, and key technical challenges. The early studies relied heavily on CNNs but latest work is shifting toward hybrid and transformer-based models that deliver higher accuracy and better contextual understanding. Still, challenges like imbalanced datasets, difficulty in detecting small pests, limited generalizability, and deployment on edge devices remain significant hurdles. Overall, this review offers a structured overview of the field, highlights useful datasets, and outlines the key challenges and future directions for AI-based pest monitoring systems. 

**Abstract (ZH)**: 昆虫害虫继续对全球作物产量构成严重威胁，传统的监测方法往往速度慢、手工操作且难以扩展。近年来，深度学习 emerges as a powerful solution，卷积神经网络（CNNs）、视觉变换器（ViTs）和混合模型等技术被广泛用于自动化害虫检测。这篇综述涵盖了2018年至2025年间发表的37篇精心挑选的研究，所有研究都集中在基于AI的害虫分类上。所选研究按照作物类型、害虫种类、模型架构、数据集使用情况以及关键技术挑战进行组织。早期的研究主要依赖于CNNs，但最新的工作转向了混合模型和基于变换器的模型，这些模型能提供更高的准确性和更好的上下文理解。然而，如数据集不平衡、难以检测小型害虫、泛化能力有限以及在边缘设备上的部署等挑战仍然是重要的障碍。总体而言，这篇综述提供了该领域的结构化概述，强调了有用的数据库集，并概述了基于AI的害虫监测系统的关键挑战和未来方向。 

---
# BioMARS: A Multi-Agent Robotic System for Autonomous Biological Experiments 

**Title (ZH)**: BioMARS：一种进行自主生物实验的多agent机器人系统 

**Authors**: Yibo Qiu, Zan Huang, Zhiyu Wang, Handi Liu, Yiling Qiao, Yifeng Hu, Shu'ang Sun, Hangke Peng, Ronald X Xu, Mingzhai Sun  

**Link**: [PDF](https://arxiv.org/pdf/2507.01485)  

**Abstract**: Large language models (LLMs) and vision-language models (VLMs) have the potential to transform biological research by enabling autonomous experimentation. Yet, their application remains constrained by rigid protocol design, limited adaptability to dynamic lab conditions, inadequate error handling, and high operational complexity. Here we introduce BioMARS (Biological Multi-Agent Robotic System), an intelligent platform that integrates LLMs, VLMs, and modular robotics to autonomously design, plan, and execute biological experiments. BioMARS uses a hierarchical architecture: the Biologist Agent synthesizes protocols via retrieval-augmented generation; the Technician Agent translates them into executable robotic pseudo-code; and the Inspector Agent ensures procedural integrity through multimodal perception and anomaly detection. The system autonomously conducts cell passaging and culture tasks, matching or exceeding manual performance in viability, consistency, and morphological integrity. It also supports context-aware optimization, outperforming conventional strategies in differentiating retinal pigment epithelial cells. A web interface enables real-time human-AI collaboration, while a modular backend allows scalable integration with laboratory hardware. These results highlight the feasibility of generalizable, AI-driven laboratory automation and the transformative role of language-based reasoning in biological research. 

**Abstract (ZH)**: 生物多智能体机器人系统（BioMARS）：基于大规模语言模型和视觉语言模型的自主生物实验平台 

---
# Epistemic Scarcity: The Economics of Unresolvable Unknowns 

**Title (ZH)**: 知识稀缺：不可解未知的经济学 

**Authors**: Craig S Wright  

**Link**: [PDF](https://arxiv.org/pdf/2507.01483)  

**Abstract**: This paper presents a praxeological analysis of artificial intelligence and algorithmic governance, challenging assumptions about the capacity of machine systems to sustain economic and epistemic order. Drawing on Misesian a priori reasoning and Austrian theories of entrepreneurship, we argue that AI systems are incapable of performing the core functions of economic coordination: interpreting ends, discovering means, and communicating subjective value through prices. Where neoclassical and behavioural models treat decisions as optimisation under constraint, we frame them as purposive actions under uncertainty.
We critique dominant ethical AI frameworks such as Fairness, Accountability, and Transparency (FAT) as extensions of constructivist rationalism, which conflict with a liberal order grounded in voluntary action and property rights. Attempts to encode moral reasoning in algorithms reflect a misunderstanding of ethics and economics. However complex, AI systems cannot originate norms, interpret institutions, or bear responsibility. They remain opaque, misaligned, and inert.
Using the concept of epistemic scarcity, we explore how information abundance degrades truth discernment, enabling both entrepreneurial insight and soft totalitarianism. Our analysis ends with a civilisational claim: the debate over AI concerns the future of human autonomy, institutional evolution, and reasoned choice. The Austrian tradition, focused on action, subjectivity, and spontaneous order, offers the only coherent alternative to rising computational social control. 

**Abstract (ZH)**: 人工智能与算法治理的实践分析：挑战机器系统维持经济与知识秩序的能力 

---
# Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities 

**Title (ZH)**: 直接偏好优化对个性化德国自动文本简化的有效性评估 

**Authors**: Yingqiang Gao, Kaede Johnson, David Froehlich, Luisa Carrer, Sarah Ebling  

**Link**: [PDF](https://arxiv.org/pdf/2507.01479)  

**Abstract**: Automatic text simplification (ATS) aims to enhance language accessibility for various target groups, particularly persons with intellectual disabilities. Recent advancements in generative AI, especially large language models (LLMs), have substantially improved the quality of machine-generated text simplifications, thereby mitigating information barriers for the target group. However, existing LLM-based ATS systems do not incorporate preference feedback on text simplifications during training, resulting in a lack of personalization tailored to the specific needs of target group representatives.
In this work, we extend the standard supervised fine-tuning (SFT) approach for adapting LLM-based ATS models by leveraging a computationally efficient LLM alignment technique -- direct preference optimization (DPO). Specifically, we post-train LLM-based ATS models using human feedback collected from persons with intellectual disabilities, reflecting their preferences on paired text simplifications generated by mainstream LLMs. Furthermore, we propose a pipeline for developing personalized LLM-based ATS systems, encompassing data collection, model selection, SFT and DPO post-training, and evaluation. Our findings underscore the necessity of active participation of target group persons in designing personalized AI accessibility solutions aligned with human expectations. This work represents a step towards personalizing inclusive AI systems at the target-group level, incorporating insights not only from text simplification experts but also from target group persons themselves. 

**Abstract (ZH)**: 自动文本简化（ATS）旨在提高语言可访问性，特别是对智力障碍人群。近期生成式AI的进展，尤其是大规模语言模型（LLMs），显著提高了机器生成文本简化的质量，从而减轻了目标群体的信息障碍。然而，现有的基于LLM的ATS系统在训练过程中并未纳入对文本简化的偏好反馈，导致缺乏针对目标群体代表特定需求的个性化设置。
在此工作中，我们通过利用一种计算效率高的LLM对齐技术——直接偏好优化（DPO），扩展了标准的监督微调（SFT）方法，以适应基于LLM的ATS模型。具体而言，我们使用来自智力障碍人群的反馈对基于LLM的ATS模型进行后续训练，反映了他们在主流LLM生成的配对文本简化方面的偏好。此外，我们提出了一个开发个性化LLM基ATS系统的流水线，包括数据收集、模型选择、SFT和DPO后续训练以及评估。我们的研究结果强调了目标群体人员在设计与人类期望相一致的个性化AI无障碍解决方案中的积极参与的重要性。这项工作为在目标群体层面个性化包容性AI系统迈出了一步，不仅借鉴了文本简化专家的见解，也纳入了目标群体自身的见解。 

---
# Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals 

**Title (ZH)**: 零激励动力学：通过未奖励子目标视角考察奖励稀疏性 

**Authors**: Yannick Molinghen, Tom Lenaerts  

**Link**: [PDF](https://arxiv.org/pdf/2507.01470)  

**Abstract**: This work re-examines the commonly held assumption that the frequency of rewards is a reliable measure of task difficulty in reinforcement learning. We identify and formalize a structural challenge that undermines the effectiveness of current policy learning methods: when essential subgoals do not directly yield rewards. We characterize such settings as exhibiting zero-incentive dynamics, where transitions critical to success remain unrewarded. We show that state-of-the-art deep subgoal-based algorithms fail to leverage these dynamics and that learning performance is highly sensitive to the temporal proximity between subgoal completion and eventual reward. These findings reveal a fundamental limitation in current approaches and point to the need for mechanisms that can infer latent task structure without relying on immediate incentives. 

**Abstract (ZH)**: 本研究重新审视了强化学习中奖励频率是任务难度可靠指标的普遍假设，指出并正式化了一个结构上的挑战，该挑战削弱了当前策略学习方法的有效性：当关键子目标未直接产生奖励时。我们把这种设置描述为零激励动力学，其中对成功至关重要的状态转换未获奖励。我们证明，最先进的基于子目标的深度算法无法利用这些动力学，且学习性能高度依赖于子目标完成与最终奖励之间的时间接近程度。这些发现揭示了当前方法的根本局限性，并指出了需要能够不依赖即时激励来推断潜在任务结构的机制。 

---
# NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation 

**Title (ZH)**: NOCTIS: 新颖对象循环阈值实例分割 

**Authors**: Max Gandyra, Alessandro Santonicola, Michael Beetz  

**Link**: [PDF](https://arxiv.org/pdf/2507.01463)  

**Abstract**: Instance segmentation of novel objects instances in RGB images, given some example images for each object, is a well known problem in computer vision. Designing a model general enough to be employed, for all kinds of novel objects, without (re-) training, has proven to be a difficult task. To handle this, we propose a simple, yet powerful, framework, called: Novel Object Cyclic Threshold based Instance Segmentation (NOCTIS). This work stems from and improves upon previous ones like CNOS, SAM-6D and NIDS-Net; thus, it also leverages on recent vision foundation models, namely: Grounded-SAM 2 and DINOv2. It utilises Grounded-SAM 2 to obtain object proposals with precise bounding boxes and their corresponding segmentation masks; while DINOv2's zero-shot capabilities are employed to generate the image embeddings. The quality of those masks, together with their embeddings, is of vital importance to our approach; as the proposal-object matching is realized by determining an object matching score based on the similarity of the class embeddings and the average maximum similarity of the patch embeddings. Differently to SAM-6D, calculating the latter involves a prior patch filtering based on the distance between each patch and its corresponding cyclic/roundtrip patch in the image grid. Furthermore, the average confidence of the proposals' bounding box and mask is used as an additional weighting factor for the object matching score. We empirically show that NOCTIS, without further training/fine tuning, outperforms the best RGB and RGB-D methods on the seven core datasets of the BOP 2023 challenge for the "Model-based 2D segmentation of unseen objects" task. 

**Abstract (ZH)**: 基于循环阈值的新型物体实例分割（NOCTIS）：RGB图像中给定示例图像的新颖物体实例分割 

---
# Quantum-Assisted Automatic Path-Planning for Robotic Quality Inspection in Industry 4.0 

**Title (ZH)**: 量子辅助自动路径规划的工业4.0机器人质量检测研究 

**Authors**: Eneko Osaba, Estibaliz Garrote, Pablo Miranda-Rodriguez, Alessia Ciacco, Itziar Cabanes, Aitziber Mancisidor  

**Link**: [PDF](https://arxiv.org/pdf/2507.01462)  

**Abstract**: This work explores the application of hybrid quantum-classical algorithms to optimize robotic inspection trajectories derived from Computer-Aided Design (CAD) models in industrial settings. By modeling the task as a 3D variant of the Traveling Salesman Problem, incorporating incomplete graphs and open-route constraints, this study evaluates the performance of two D-Wave-based solvers against classical methods such as GUROBI and Google OR-Tools. Results across five real-world cases demonstrate competitive solution quality with significantly reduced computation times, highlighting the potential of quantum approaches in automation under Industry 4.0. 

**Abstract (ZH)**: 本研究探讨了混合量子-经典算法在工业环境中基于计算机辅助设计（CAD）模型优化机器人检测轨迹的应用。通过将任务建模为三维旅行商问题的变体，并纳入不完整图和开路约束，本文评估了两种D-Wave基于的求解器与GUROBI和Google OR-Tools等经典方法的性能。在五种实际案例中的结果表明，量子方法在解决方案质量上具有竞争力，同时大幅减少了计算时间，突显了在工业4.0背景下自动化中量子方法的潜力。 

---
# Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs 

**Title (ZH)**: 基于概率程序的RISC-V向量扩展张量程序优化 

**Authors**: Federico Nicolas Peccia, Frederik Haxel, Oliver Bringmann  

**Link**: [PDF](https://arxiv.org/pdf/2507.01457)  

**Abstract**: RISC-V provides a flexible and scalable platform for applications ranging from embedded devices to high-performance computing clusters. Particularly, its RISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI workloads. But writing software that efficiently utilizes the vector units of RISC-V CPUs without expert knowledge requires the programmer to rely on the autovectorization features of compilers or hand-crafted libraries like muRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing the integration with the RISC-V RVV extension, thus heavily limiting the efficient deployment of complex AI workloads. In this paper, we present a workflow based on the TVM compiler to efficiently map AI workloads onto RISC-V vector units. Instead of relying on hand-crafted libraries, we integrated the RVV extension into TVM's MetaSchedule framework, a probabilistic program framework for tensor operation tuning. We implemented different RISC-V SoCs on an FPGA and tuned a wide range of AI workloads on them. We found that our proposal shows a mean improvement of 46% in execution latency when compared against the autovectorization feature of GCC, and 29% against muRISCV-NN. Moreover, the binary resulting from our proposal has a smaller code memory footprint, making it more suitable for embedded devices. Finally, we also evaluated our solution on a commercially available RISC-V SoC implementing the RVV 1.0 Vector Extension and found our solution is able to find mappings that are 35% faster on average than the ones proposed by LLVM. We open-sourced our proposal for the community to expand it to target other RISC-V extensions. 

**Abstract (ZH)**: RISC-V为从嵌入式设备到高性能计算集群的应用提供了一个灵活且可扩展的平台。特别是，其RISC-V向量扩展（RVV）引起了对加速AI工作负载的兴趣。但在没有专家知识的情况下，编写能够高效利用RISC-V CPU向量单元的软件需要依赖编译器的自动向量化功能或像muRISCV-NN这样的手工构建库。现有的更智能的方法，如自动调优框架，尚未将RISC-V RVV扩展集成进来，从而极大地限制了复杂AI工作负载的高效部署。在本文中，我们基于TVM编译器提出了一种工作流，以高效地将AI工作负载映射到RISC-V向量单元上。我们没有依赖手工构建的库，而是将RVV扩展集成到TVM的MetaSchedule框架中，这是一个用于张量操作调优的概率程序框架。我们使用FPGA实现了不同的RISC-V SOC，并在它们上调优了广泛的AI工作负载。我们发现，与GCC的自动向量化功能相比，我们的提案在执行延迟上平均改进了46%，与muRISCV-NN相比，平均改进了29%。此外，由我们提案生成的二进制文件具有较小的代码内存占用，使其更适合嵌入式设备。最后，我们还在一个采用RVV 1.0向量扩展的商用RISC-V SOC上评估了我们的解决方案，发现我们的解决方案在平均速度上比LLVM提出的方法快35%。我们将我们的提案开源给社区，以便将其扩展为目标其他RISC-V扩展。 

---
# EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices 

**Title (ZH)**: EdgeLoRA：边缘设备上的高效多租户LLM服务系统 

**Authors**: Zheyu Shen, Yexiao He, Ziyao Wang, Yuning Zhang, Guoheng Sun, Wanghao Ye, Ang Li  

**Link**: [PDF](https://arxiv.org/pdf/2507.01438)  

**Abstract**: Large Language Models (LLMs) have gained significant attention due to their versatility across a wide array of applications. Fine-tuning LLMs with parameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these models to efficiently adapt to downstream tasks without extensive retraining. Deploying fine-tuned LLMs on multi-tenant edge devices offers substantial benefits, such as reduced latency, enhanced privacy, and personalized responses. However, serving LLMs efficiently on resource-constrained edge devices presents critical challenges, including the complexity of adapter selection for different tasks and memory overhead from frequent adapter swapping. Moreover, given the multiple requests in multi-tenant settings, processing requests sequentially results in underutilization of computational resources and increased latency. This paper introduces EdgeLoRA, an efficient system for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA incorporates three key innovations: (1) an adaptive adapter selection mechanism to streamline the adapter configuration process; (2) heterogeneous memory management, leveraging intelligent adapter caching and pooling to mitigate memory operation overhead; and (3) batch LoRA inference, enabling efficient batch processing to significantly reduce computational latency. Comprehensive evaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly outperforms the status quo (i.e., this http URL) in terms of both latency and throughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times boost in throughput. Even more impressively, it can serve several orders of magnitude more adapters simultaneously. These results highlight EdgeLoRA's potential to transform edge deployment of LLMs in multi-tenant scenarios, offering a scalable and efficient solution for resource-constrained environments. 

**Abstract (ZH)**: Large Language模型（LLMs）由于其在多种应用中的 versatility 获得了广泛关注。通过参数高效适配器如 Low-Rank Adaptation (LoRA) 对 LLMs 进行微调，使得这些模型能够高效适应下游任务而无需大量重新训练。在多租户边缘设备上部署微调后的 LLMs 提供了显著优势，如降低延迟、增强隐私和个性化响应。然而，在资源受限的边缘设备上高效提供 LLMs 也带来了一些关键挑战，包括不同任务适配器选择的复杂性和频繁适配器切换带来的内存开销。此外，在多租户环境中，由于需要依次处理多个请求，这会导致计算资源的利用率降低和延迟增加。本文介绍了 EdgeLoRA，这是一种用于多租户环境中边缘设备上提供 LLMs 的高效系统。EdgeLoRA 包含三个关键创新：（1）一种自适应适配器选择机制，以简化适配器配置过程；（2）异构内存管理，利用智能适配器缓存和池化来减少内存操作开销；（3）批量 LoRA 推理，使得批量处理能够显著减少计算延迟。使用 Llama3.1-8B 模型进行全面评估表明，EdgeLoRA 在延迟和吞吐量方面显著优于现状。结果表明，EdgeLoRA 可以实现多达 4 倍的吞吐量提升。更令人印象深刻的是，它可以同时服务多个数量级的适配器。这些结果突显了 EdgeLoRA 在多租户场景下边缘部署 LLMs 的潜在能力，为资源受限环境提供了可扩展且高效的解决方案。 

---
# Hardware-software co-exploration with racetrack memory based in-memory computing for CNN inference in embedded systems 

**Title (ZH)**: 基于赛马存储器的存内计算在嵌入式系统中实现CNN推理的硬件-软件协同探索 

**Authors**: Benjamin Chen Ming Choong, Tao Luo, Cheng Liu, Bingsheng He, Wei Zhang, Joey Tianyi Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2507.01429)  

**Abstract**: Deep neural networks generate and process large volumes of data, posing challenges for low-resource embedded systems. In-memory computing has been demonstrated as an efficient computing infrastructure and shows promise for embedded AI applications. Among newly-researched memory technologies, racetrack memory is a non-volatile technology that allows high data density fabrication, making it a good fit for in-memory computing. However, integrating in-memory arithmetic circuits with memory cells affects both the memory density and power efficiency. It remains challenging to build efficient in-memory arithmetic circuits on racetrack memory within area and energy constraints. To this end, we present an efficient in-memory convolutional neural network (CNN) accelerator optimized for use with racetrack memory. We design a series of fundamental arithmetic circuits as in-memory computing cells suited for multiply-and-accumulate operations. Moreover, we explore the design space of racetrack memory based systems and CNN model architectures, employing co-design to improve the efficiency and performance of performing CNN inference in racetrack memory while maintaining model accuracy. Our designed circuits and model-system co-optimization strategies achieve a small memory bank area with significant improvements in energy and performance for racetrack memory based embedded systems. 

**Abstract (ZH)**: 基于赛道存储器的高效卷积神经网络加速器设计 

---
# DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal 

**Title (ZH)**: DocShaDiffusion：文档图像阴影去除的潜空间扩散模型 

**Authors**: Wenjie Liu, Bingshu Wang, Ze Wang, C.L. Philip Chen  

**Link**: [PDF](https://arxiv.org/pdf/2507.01422)  

**Abstract**: Document shadow removal is a crucial task in the field of document image enhancement. However, existing methods tend to remove shadows with constant color background and ignore color shadows. In this paper, we first design a diffusion model in latent space for document image shadow removal, called DocShaDiffusion. It translates shadow images from pixel space to latent space, enabling the model to more easily capture essential features. To address the issue of color shadows, we design a shadow soft-mask generation module (SSGM). It is able to produce accurate shadow mask and add noise into shadow regions specially. Guided by the shadow mask, a shadow mask-aware guided diffusion module (SMGDM) is proposed to remove shadows from document images by supervising the diffusion and denoising process. We also propose a shadow-robust perceptual feature loss to preserve details and structures in document images. Moreover, we develop a large-scale synthetic document color shadow removal dataset (SDCSRD). It simulates the distribution of realistic color shadows and provides powerful supports for the training of models. Experiments on three public datasets validate the proposed method's superiority over state-of-the-art. Our code and dataset will be publicly available. 

**Abstract (ZH)**: 文档阴影去除是文档图像增强领域的关键任务。然而，现有的方法倾向于移除具有恒定颜色背景的阴影，而忽视彩色阴影。本文首先在潜空间中设计了一种扩散模型，称为DocShaDiffusion，用于文档图像阴影去除，它将阴影图像从像素空间转换到潜空间，使模型更易于捕捉到重要的特征。为了解决彩色阴影的问题，我们设计了一个阴影软掩模生成模块（SSGM），能够生成准确的阴影掩模并在阴影区域添加噪声。在阴影掩模的引导下，我们提出了一种阴影掩模感知引导扩散模块（SMGDM），通过监督扩散和去噪过程从文档图像中移除阴影。此外，我们提出了阴影鲁棒感知特征损失，以保留文档图像中的细节和结构。此外，我们还开发了一个大规模合成文档彩色阴影去除数据集（SDCSRD），模拟了现实彩色阴影的分布，并为模型训练提供了强有力的支持。在三个公开数据集上的实验验证了所提出方法优于现有方法。我们的代码和数据集将公开提供。 

---
# Penalizing Transparency? How AI Disclosure and Author Demographics Shape Human and AI Judgments About Writing 

**Title (ZH)**: 惩罚透明性？AI披露和作者 demographic 特征如何影响人类和AI对写作的判断 

**Authors**: Inyoung Cheong, Alicia Guo, Mina Lee, Zhehui Liao, Kowe Kadoma, Dongyoung Go, Joseph Chee Chang, Peter Henderson, Mor Naaman, Amy X. Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2507.01418)  

**Abstract**: As AI integrates in various types of human writing, calls for transparency around AI assistance are growing. However, if transparency operates on uneven ground and certain identity groups bear a heavier cost for being honest, then the burden of openness becomes asymmetrical. This study investigates how AI disclosure statement affects perceptions of writing quality, and whether these effects vary by the author's race and gender. Through a large-scale controlled experiment, both human raters (n = 1,970) and LLM raters (n = 2,520) evaluated a single human-written news article while disclosure statements and author demographics were systematically varied. This approach reflects how both human and algorithmic decisions now influence access to opportunities (e.g., hiring, promotion) and social recognition (e.g., content recommendation algorithms). We find that both human and LLM raters consistently penalize disclosed AI use. However, only LLM raters exhibit demographic interaction effects: they favor articles attributed to women or Black authors when no disclosure is present. But these advantages disappear when AI assistance is revealed. These findings illuminate the complex relationships between AI disclosure and author identity, highlighting disparities between machine and human evaluation patterns. 

**Abstract (ZH)**: 随着AI融入各种类型的人类写作，对AI辅助的透明度要求越来越高。然而，如果透明度建立在不平等的基础上，某些身份群体因诚实而承担更大的成本，那么开放的负担就会变得不对称。本研究探讨了AI披露声明如何影响写作质量的感知，以及这些影响是否因作者的种族和性别而异。通过大规模控制实验，分别有1,970名人类评分者和2,520名LLM评分者评估了一篇单一的人类撰写的新闻文章，同时系统地变化了披露声明和作者的人口统计信息。这种方法反映了现在人类和算法决策如何影响机会获取（例如，招聘、晋升）和社会认可（例如，内容推荐算法）。我们发现，无论是人类评分者还是LLM评分者，都一致惩罚披露的AI使用。然而，只有LLM评分者显示出人口统计交互效应：当没有披露时，他们更倾向于将文章归因于女性或非洲裔作者。但当AI援助被揭示时，这些优势消失。这些发现揭示了AI披露与作者身份之间复杂的关系，突显了机器和人类评估模式之间的差异。 

---
# Evaluating LLM Agent Collusion in Double Auctions 

**Title (ZH)**: 评估LLM代理在双拍卖中的共谋行为 

**Authors**: Kushal Agrawal, Verona Teo, Juan J. Vazquez, Sudarsh Kunnavakkam, Vishak Srikanth, Andy Liu  

**Link**: [PDF](https://arxiv.org/pdf/2507.01413)  

**Abstract**: Large language models (LLMs) have demonstrated impressive capabilities as autonomous agents with rapidly expanding applications in various domains. As these agents increasingly engage in socioeconomic interactions, identifying their potential for undesirable behavior becomes essential. In this work, we examine scenarios where they can choose to collude, defined as secretive cooperation that harms another party. To systematically study this, we investigate the behavior of LLM agents acting as sellers in simulated continuous double auction markets. Through a series of controlled experiments, we analyze how parameters such as the ability to communicate, choice of model, and presence of environmental pressures affect the stability and emergence of seller collusion. We find that direct seller communication increases collusive tendencies, the propensity to collude varies across models, and environmental pressures, such as oversight and urgency from authority figures, influence collusive behavior. Our findings highlight important economic and ethical considerations for the deployment of LLM-based market agents. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在各种领域的应用日益广泛，展现了作为自主代理的强大能力。随着这些代理越来越多地参与经济和社会互动，识别其潜在的不良行为变得至关重要。在此工作中，我们研究了它们选择共谋的情况，共谋被定义为秘密的合作行为，对另一方造成伤害。为了系统地研究这一现象，我们调查了大规模语言模型代理作为卖家在模拟连续双拍卖市场中的行为。通过一系列受控实验，我们分析了直接卖家沟通能力、所选用模型以及环境压力等因素如何影响卖家共谋行为的稳定性和出现。我们发现，直接卖家沟通会增加共谋倾向，不同模型的共谋倾向存在差异，来自权威人物的监督和紧迫性等环境压力会影响共谋行为。我们的研究结果突显了大规模语言模型市场代理部署的重要经济和伦理考量。 

---
# Age Sensitive Hippocampal Functional Connectivity: New Insights from 3D CNNs and Saliency Mapping 

**Title (ZH)**: 年龄相关海马功能连接性：来自3D CNNs和注意映射的新见解 

**Authors**: Yifei Sun, Marshall A. Dalton, Robert D. Sanders, Yixuan Yuan, Xiang Li, Sharon L. Naismith, Fernando Calamante, Jinglei Lv  

**Link**: [PDF](https://arxiv.org/pdf/2507.01411)  

**Abstract**: Grey matter loss in the hippocampus is a hallmark of neurobiological aging, yet understanding the corresponding changes in its functional connectivity remains limited. Seed-based functional connectivity (FC) analysis enables voxel-wise mapping of the hippocampus's synchronous activity with cortical regions, offering a window into functional reorganization during aging. In this study, we develop an interpretable deep learning framework to predict brain age from hippocampal FC using a three-dimensional convolutional neural network (3D CNN) combined with LayerCAM saliency mapping. This approach maps key hippocampal-cortical connections, particularly with the precuneus, cuneus, posterior cingulate cortex, parahippocampal cortex, left superior parietal lobule, and right superior temporal sulcus, that are highly sensitive to age. Critically, disaggregating anterior and posterior hippocampal FC reveals distinct mapping aligned with their known functional specializations. These findings provide new insights into the functional mechanisms of hippocampal aging and demonstrate the power of explainable deep learning to uncover biologically meaningful patterns in neuroimaging data. 

**Abstract (ZH)**: 海马区灰质损失是神经生物学老化的一个特征，但对其功能连接相应变化的理解仍然有限。基于种子的功能连接分析可以以体素级方式映射海马与皮层区域的同步活动，为理解老化期间的功能重组提供了窗口。在本研究中，我们开发了一个可解释的深度学习框架，使用三维卷积神经网络（3D CNN）结合LayerCAM显著性映射来预测大脑年龄，并揭示了特别易受年龄影响的关键海马-皮层连接，特别是与楔前叶、楔叶、后扣带回、旁海马回、左侧顶叶上回和右侧颞上沟的连接。更重要的是，将前部和后部海马的功能连接分离后，发现了与它们已知的功能特化相一致的分离映射。这些发现为理解海马老化的过程提供了新的见解，并展示了可解释深度学习在揭示神经影像数据中的生物意义模式方面的强大能力。 

---
# Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound 

**Title (ZH)**: 基于医学知识的多项实例学习在产前超声分类严重腹部异常中的应用 

**Authors**: Huanwen Liang, Jingxian Xu, Yuanji Zhang, Yuhao Huang, Yuhan Zhang, Xin Yang, Ran Li, Xuedong Deng, Yanjun Liu, Guowei Tao, Yun Wu, Sheng Zhao, Xinru Gao, Dong Ni  

**Link**: [PDF](https://arxiv.org/pdf/2507.01401)  

**Abstract**: Fetal abdominal malformations are serious congenital anomalies that require accurate diagnosis to guide pregnancy management and reduce mortality. Although AI has demonstrated significant potential in medical diagnosis, its application to prenatal abdominal anomalies remains limited. Most existing studies focus on image-level classification and rely on standard plane localization, placing less emphasis on case-level diagnosis. In this paper, we develop a case-level multiple instance learning (MIL)-based method, free of standard plane localization, for classifying fetal abdominal anomalies in prenatal ultrasound. Our contribution is three-fold. First, we adopt a mixture-of-attention-experts module (MoAE) to weight different attention heads for various planes. Secondly, we propose a medical-knowledge-driven feature selection module (MFS) to align image features with medical knowledge, performing self-supervised image token selection at the case-level. Finally, we propose a prompt-based prototype learning (PPL) to enhance the MFS. Extensively validated on a large prenatal abdominal ultrasound dataset containing 2,419 cases, with a total of 24,748 images and 6 categories, our proposed method outperforms the state-of-the-art competitors. Codes are available at:this https URL. 

**Abstract (ZH)**: 胎儿腹部畸形是严重先天性疾病，需要准确诊断以指导妊娠管理并降低死亡率。虽然AI在医学诊断中显示出巨大的潜力，但其在产前腹部畸形的应用仍受到限制。现有大多数研究集中于图像级分类，并依赖于标准切面定位，较少关注案例级诊断。本文提出了一种无需标准切面定位的案例级多实例学习（MIL）方法，用于产前超声分类胎儿腹部畸形。我们的贡献包括三个方面：首先，采用混合注意力专家模块（MoAE）为不同切面加权不同的注意力头；其次，提出一种医学知识驱动的特征选择模块（MFS），在案例级进行自监督图像标记选择，使图像特征与医学知识相一致；最后，提出基于提示的原型学习（PPL）以增强MFS。在包含2,419个案例、总计24,748张图像和6个类别的大型产前腹部超声数据集上进行了广泛验证，我们的方法在性能上优于最新竞争对手。代码可访问：this https URL。 

---
# Distributional Soft Actor-Critic with Diffusion Policy 

**Title (ZH)**: 分布软actor-critic结合扩散策略 

**Authors**: Tong Liu, Yinuo Wang, Xujie Song, Wenjun Zou, Liangfa Chen, Likun Wang, Bin Shuai, Jingliang Duan, Shengbo Eben Li  

**Link**: [PDF](https://arxiv.org/pdf/2507.01381)  

**Abstract**: Reinforcement learning has been proven to be highly effective in handling complex control tasks. Traditional methods typically use unimodal distributions, such as Gaussian distributions, to model the output of value distributions. However, unimodal distribution often and easily causes bias in value function estimation, leading to poor algorithm performance. This paper proposes a distributional reinforcement learning algorithm called DSAC-D (Distributed Soft Actor Critic with Diffusion Policy) to address the challenges of estimating bias in value functions and obtaining multimodal policy representations. A multimodal distributional policy iteration framework that can converge to the optimal policy was established by introducing policy entropy and value distribution function. A diffusion value network that can accurately characterize the distribution of multi peaks was constructed by generating a set of reward samples through reverse sampling using a diffusion model. Based on this, a distributional reinforcement learning algorithm with dual diffusion of the value network and the policy network was derived. MuJoCo testing tasks demonstrate that the proposed algorithm not only learns multimodal policy, but also achieves state-of-the-art (SOTA) performance in all 9 control tasks, with significant suppression of estimation bias and total average return improvement of over 10\% compared to existing mainstream algorithms. The results of real vehicle testing show that DSAC-D can accurately characterize the multimodal distribution of different driving styles, and the diffusion policy network can characterize multimodal trajectories. 

**Abstract (ZH)**: 分布式软演员批评与扩散策略算法（DSAC-D）：针对价值函数偏差估计和多模态策略表示的分布强化学习算法 

---
# RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms 

**Title (ZH)**: RALLY：基于角色自适应大规模语言模型驱动的联动导航框架用于自主无人机集群 

**Authors**: Ziyao Wang, Rongpeng Li, Sizhao Li, Yuming Xiang, Haiping Wang, Zhifeng Zhao, Honggang Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2507.01378)  

**Abstract**: Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as a critical research focus, and it typically requires the swarm to navigate effectively while avoiding obstacles and achieving continuous coverage over multiple mission targets. Although traditional Multi-Agent Reinforcement Learning (MARL) approaches offer dynamic adaptability, they are hindered by the semantic gap in numerical communication and the rigidity of homogeneous role structures, resulting in poor generalization and limited task scalability. Recent advances in Large Language Model (LLM)-based control frameworks demonstrate strong semantic reasoning capabilities by leveraging extensive prior knowledge. However, due to the lack of online learning and over-reliance on static priors, these works often struggle with effective exploration, leading to reduced individual potential and overall system performance. To address these limitations, we propose a Role-Adaptive LLM-Driven Yoked navigation algorithm RALLY. Specifically, we first develop an LLM-driven semantic decision framework that uses structured natural language for efficient semantic communication and collaborative reasoning. Afterward, we introduce a dynamic role-heterogeneity mechanism for adaptive role switching and personalized decision-making. Furthermore, we propose a Role-value Mixing Network (RMIX)-based assignment strategy that integrates LLM offline priors with MARL online policies to enable semi-offline training of role selection strategies. Experiments in the Multi-Agent Particle Environment (MPE) environment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY outperforms conventional approaches in terms of task coverage, convergence speed, and generalization, highlighting its strong potential for collaborative navigation in agentic multi-UAV systems. 

**Abstract (ZH)**: 基于大语言模型的角色自适应联合导航算法RALLY：面向自主无人飞行器系统的协同导航 

---
# Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy 

**Title (ZH)**: Skywork-Reward-V2: 通过人机协同扩展偏好数据 curated 规模 

**Authors**: Chris Yuhao Liu, Liang Zeng, Yuzhen Xiao, Jujie He, Jiacai Liu, Chaojie Wang, Rui Yan, Wei Shen, Fuxiang Zhang, Jiacheng Xu, Yang Liu, Yahui Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2507.01352)  

**Abstract**: Despite the critical role of reward models (RMs) in reinforcement learning from human feedback (RLHF), current state-of-the-art open RMs perform poorly on most existing evaluation benchmarks, failing to capture the spectrum of nuanced and sophisticated human preferences. Even approaches that incorporate advanced training techniques have not yielded meaningful performance improvements. We hypothesize that this brittleness stems primarily from limitations in preference datasets, which are often narrowly scoped, synthetically labeled, or lack rigorous quality control. To address these challenges, we present a large-scale preference dataset comprising 40 million preference pairs, named SynPref-40M. To enable data curation at scale, we design a human-AI synergistic two-stage pipeline that leverages the complementary strengths of human annotation quality and AI scalability. In this pipeline, humans provide verified annotations, while large language models perform automatic curation based on human guidance. Training on this preference mixture, we introduce Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B parameters, trained on a carefully curated subset of 26 million preference pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile across a wide range of capabilities, including alignment with human preferences, objective correctness, safety, resistance to stylistic biases, and best-of-N scaling, achieving state-of-the-art performance across seven major reward model benchmarks. Ablation studies confirm that the effectiveness of our approach stems not only from data scale but also from high-quality curation. The Skywork-Reward-V2 series represents substantial progress in open reward models, highlighting the untapped potential of existing preference datasets and demonstrating how human-AI curation synergy can unlock significantly higher data quality. 

**Abstract (ZH)**: Despite the Critical Role of Reward Models in Reinforcement Learning from Human Feedback, Current State-of-the-Art Open RMs Perform Poorly on Most Existing Evaluation Benchmarks, Failing to Capture the Spectrum of Nuanced and Sophisticated Human Preferences. We Hypothesize That This Brittleness Stems Primarily from Limitations in Preference Datasets, Which Are Often Narrowly Scoped, Synthetically Labeled, or Lack Rigorous Quality Control. To Address These Challenges, We Present a Large-Scale Preference Dataset Comprising 40 Million Preference Pairs, Named SynPref-40M. To Enable Data Curation at Scale, We Design a Human-AI Synergistic Two-Stage Pipeline That Leverages the Complementary Strengths of Human Annotation Quality and AI Scalability. In This Pipeline, Humans Provide Verified Annotations, While Large Language Models Perform Automatic Curation Based on Human Guidance. Training on This Preference Mixture, We Introduce Skywork-Reward-V2, a Suite of Eight Reward Models Ranging from 0.6B to 8B Parameters, Trained on a Carefully Curated Subset of 26 Million Preference Pairs from SynPref-40M. We Demonstrate That Skywork-Reward-V2 Is Versatile across a Wide Range of Capabilities, Including Alignment with Human Preferences, Objective Correctness, Safety, Resistance to Stylistic Biases, and Best-of-N Scaling, Achieving State-of-the-Art Performance across Seven Major Reward Model Benchmarks. Ablation Studies Confirm That the Effectiveness of Our Approach Stems Not Only from Data Scale but Also from High-Quality Curation. The Skywork-Reward-V2 Series Represents Substantial Progress in Open Reward Models, Highlighting the Untapped Potential of Existing Preference Datasets and Demonstrating How Human-AI Curation Synergy Can Unlock Significantly Higher Data Quality. 

---
# User-guided Generative Source Separation 

**Title (ZH)**: 用户引导的生成式源分离 

**Authors**: Yutong Wen, Minje Kim, Paris Smaragdis  

**Link**: [PDF](https://arxiv.org/pdf/2507.01339)  

**Abstract**: Music source separation (MSS) aims to extract individual instrument sources from their mixture. While most existing methods focus on the widely adopted four-stem separation setup (vocals, bass, drums, and other instruments), this approach lacks the flexibility needed for real-world applications. To address this, we propose GuideSep, a diffusion-based MSS model capable of instrument-agnostic separation beyond the four-stem setup. GuideSep is conditioned on multiple inputs: a waveform mimicry condition, which can be easily provided by humming or playing the target melody, and mel-spectrogram domain masks, which offer additional guidance for separation. Unlike prior approaches that relied on fixed class labels or sound queries, our conditioning scheme, coupled with the generative approach, provides greater flexibility and applicability. Additionally, we design a mask-prediction baseline using the same model architecture to systematically compare predictive and generative approaches. Our objective and subjective evaluations demonstrate that GuideSep achieves high-quality separation while enabling more versatile instrument extraction, highlighting the potential of user participation in the diffusion-based generative process for MSS. Our code and demo page are available at this https URL 

**Abstract (ZH)**: 音乐源分离（MSS）旨在从混合音中提取独立的乐器源。尽管现有方法大多关注于广泛采用的四分体分离设置（人声、贝斯、鼓和其他乐器），但这种方法缺乏现实应用所需的灵活性。为解决这一问题，我们提出GuideSep，这是一种基于扩散的MSS模型，能够超越四分体设置进行乐器无关的分离。GuideSep基于多个输入条件：声音模仿条件，可以通过哼唱或演奏目标旋律轻松提供；以及梅尔谱图域掩码，为分离提供额外指导。与依赖固定类别标签或声音查询的先前方法不同，我们的条件方案结合生成方法提供了更大的灵活性和适用性。此外，我们使用相同模型架构设计了一个掩码预测基准，系统地比较预测和生成方法。我们的客观和主观评估表明，GuideSep实现了高质量的分离，同时使乐器提取更具灵活性，突显了用户参与基于扩散的生成过程在MSS中的潜力。我们的代码和演示页可在以下网址获得。 

---
# LEDOM: An Open and Fundamental Reverse Language Model 

**Title (ZH)**: LEDOM：一个开源和基础的反向语言模型 

**Authors**: Xunjian Yin, Sitao Cheng, Yuxi Xie, Xinyu Hu, Li Lin, Xinyi Wang, Liangming Pan, William Yang Wang, Xiaojun Wan  

**Link**: [PDF](https://arxiv.org/pdf/2507.01335)  

**Abstract**: We introduce LEDOM, the first purely reverse language model, trained autoregressively on 435B tokens with 2B and 7B parameter variants, which processes sequences in reverse temporal order through previous token prediction. For the first time, we present the reverse language model as a potential foundational model across general tasks, accompanied by a set of intriguing examples and insights. Based on LEDOM, we further introduce a novel application: Reverse Reward, where LEDOM-guided reranking of forward language model outputs leads to substantial performance improvements on mathematical reasoning tasks. This approach leverages LEDOM's unique backward reasoning capability to refine generation quality through posterior evaluation. Our findings suggest that LEDOM exhibits unique characteristics with broad application potential. We will release all models, training code, and pre-training data to facilitate future research. 

**Abstract (ZH)**: 我们介绍LEDOM，这是首个纯逆序语言模型，以自回归方式在435亿个tokens上训练，具有2B和7B参数变体，通过以前的tokens进行预测处理序列的逆时序。我们首次展示了逆序语言模型作为通用任务潜在的基石模型，并附带了一系列引人入胜的例子和见解。基于LEDOM，我们进一步介绍了一个新的应用：逆向奖励，通过LEDOM指导的正向语言模型输出的重排序，显著提高了数学推理任务的性能。这种方法利用了LEDOM独特的逆向推理能力，通过 posterior 评估来改进生成质量。我们的研究发现表明，LEDOM具有广泛的潜在应用特性。我们将发布所有模型、训练代码和预训练数据，以促进未来的研究。 

---
# Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy 

**Title (ZH)**: 面向真实世界事件检测的推理器：通过自适应困惑度感知采样策略扩展强化学习 

**Authors**: Xiaoyun Zhang, Jingqing Ruan, Xing Ma, Yawen Zhu, Jiansong Chen, Ke Zeng, Xunliang Cai  

**Link**: [PDF](https://arxiv.org/pdf/2507.01327)  

**Abstract**: Detecting abnormal events in real-world customer service dialogues is highly challenging due to the complexity of business data and the dynamic nature of customer interactions. Moreover, models must demonstrate strong out-of-domain (OOD) generalization to enable rapid adaptation across different business scenarios and maximize commercial value. In this work, we propose a novel Adaptive Perplexity-Aware Reinforcement Learning (APARL) framework that leverages the advanced reasoning capabilities of large language models for abnormal event detection. APARL introduces a dual-loop dynamic curriculum learning architecture, enabling the model to progressively focus on more challenging samples as its proficiency increases. This design effectively addresses performance bottlenecks and significantly enhances OOD transferability. Extensive evaluations on food delivery dialogue tasks show that our model achieves significantly enhanced adaptability and robustness, attaining the highest F1 score with an average improvement of 17.19\%, and an average improvement of 9.59\% in OOD transfer tests. This method provides a superior solution for industrial deployment of anomaly detection models, contributing to improved operational efficiency and commercial benefits. 

**Abstract (ZH)**: 在现实世界客户服务对话中检测异常事件具有高度挑战性，由于商业数据的复杂性和客户交互的动态性。此外，模型必须表现出强大的领域外（OOD）泛化能力，以实现不同业务场景下的快速适应并最大化商业价值。在本文中，我们提出了一种新颖的自适应困惑度意识强化学习（APARL）框架，利用大规模语言模型的高级推理能力进行异常事件检测。APARL引入了一种双环动态课程学习架构，使模型能够随着其能力的提升逐步关注更具挑战性的样本。该设计有效解决了性能瓶颈并显著提升了领域外泛化能力。在食物配送对话任务上的广泛评估表明，我们的模型在适应性和鲁棒性方面取得了显著增强，平均F1分数提高了17.19%，领域外泛化测试的平均提高率为9.59%。该方法为异常检测模型的工业部署提供了卓越的解决方案，有助于提高运营效率和商业收益。 

---
# ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks 

**Title (ZH)**: ICLShield: 探索并缓解上下文学习后门攻击 

**Authors**: Zhiyao Ren, Siyuan Liang, Aishan Liu, Dacheng Tao  

**Link**: [PDF](https://arxiv.org/pdf/2507.01321)  

**Abstract**: In-context learning (ICL) has demonstrated remarkable success in large language models (LLMs) due to its adaptability and parameter-free nature. However, it also introduces a critical vulnerability to backdoor attacks, where adversaries can manipulate LLM behaviors by simply poisoning a few ICL demonstrations. In this paper, we propose, for the first time, the dual-learning hypothesis, which posits that LLMs simultaneously learn both the task-relevant latent concepts and backdoor latent concepts within poisoned demonstrations, jointly influencing the probability of model outputs. Through theoretical analysis, we derive an upper bound for ICL backdoor effects, revealing that the vulnerability is dominated by the concept preference ratio between the task and the backdoor. Motivated by these findings, we propose ICLShield, a defense mechanism that dynamically adjusts the concept preference ratio. Our method encourages LLMs to select clean demonstrations during the ICL phase by leveraging confidence and similarity scores, effectively mitigating susceptibility to backdoor attacks. Extensive experiments across multiple LLMs and tasks demonstrate that our method achieves state-of-the-art defense effectiveness, significantly outperforming existing approaches (+26.02% on average). Furthermore, our method exhibits exceptional adaptability and defensive performance even for closed-source models (e.g., GPT-4). 

**Abstract (ZH)**: 基于上下文学习(ICL)在大规模语言模型(LLM)中的双学习假设及其防御机制ICLShield 

---
# Neural Hamiltonian Operator 

**Title (ZH)**: 神经哈密尔顿算子 

**Authors**: Qian Qi  

**Link**: [PDF](https://arxiv.org/pdf/2507.01313)  

**Abstract**: Stochastic control problems in high dimensions are notoriously difficult to solve due to the curse of dimensionality. An alternative to traditional dynamic programming is Pontryagin's Maximum Principle (PMP), which recasts the problem as a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In this paper, we introduce a formal framework for solving such problems with deep learning by defining a \textbf{Neural Hamiltonian Operator (NHO)}. This operator parameterizes the coupled FBSDE dynamics via neural networks that represent the feedback control and an ansatz for the value function's spatial gradient. We show how the optimal NHO can be found by training the underlying networks to enforce the consistency conditions dictated by the PMP. By adopting this operator-theoretic view, we situate the deep FBSDE method within the rigorous language of statistical inference, framing it as a problem of learning an unknown operator from simulated data. This perspective allows us to prove the universal approximation capabilities of NHOs under general martingale drivers and provides a clear lens for analyzing the significant optimization challenges inherent to this class of models. 

**Abstract (ZH)**: 高维随机控制问题由于维数灾难以传统动态规划方法解决著名，一种替代方法是泊里亚克尔最大原理（PMP），它将问题重新表述为前向-后向随机微分方程（FBSDE）系统。本文通过定义一个**神经哈密尔顿算子（NHO）**，介绍了一种利用深度学习求解此类问题的形式化框架。该算子通过神经网络参数化耦合的FBSDE动力学，神经网络表示反馈控制并提供价值函数空间梯度的假设形式。我们展示了如何通过训练底层网络来找到满足PMP所规定的相容条件的最优NHO。采用这种算子观点，将深度FBSDE方法置于统计推断的严格语言框架中，将其视为从模拟数据中学习未知算子的问题。这种视角允许我们在一般鞅驱动条件下证明NHO的普遍逼近能力，并为分析此类模型固有的显著优化挑战提供清晰的视角。 

---
# VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process 

**Title (ZH)**: VLAD：一种基于层级规划和可解释决策过程的VLM增强自动驾驶框架 

**Authors**: Cristian Gariboldi, Hayato Tokida, Ken Kinjo, Yuki Asada, Alexander Carballo  

**Link**: [PDF](https://arxiv.org/pdf/2507.01284)  

**Abstract**: Recent advancements in open-source Visual Language Models (VLMs) such as LLaVA, Qwen-VL, and Llama have catalyzed extensive research on their integration with diverse systems. The internet-scale general knowledge encapsulated within these models presents significant opportunities for enhancing autonomous driving perception, prediction, and planning capabilities. In this paper we propose VLAD, a vision-language autonomous driving model, which integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end system. We implement a specialized fine-tuning approach using custom question-answer datasets designed specifically to improve the spatial reasoning capabilities of the model. The enhanced VLM generates high-level navigational commands that VAD subsequently processes to guide vehicle operation. Additionally, our system produces interpretable natural language explanations of driving decisions, thereby increasing transparency and trustworthiness of the traditionally black-box end-to-end architecture. Comprehensive evaluation on the real-world nuScenes dataset demonstrates that our integrated system reduces average collision rates by 31.82% compared to baseline methodologies, establishing a new benchmark for VLM-augmented autonomous driving systems. 

**Abstract (ZH)**: recent advancements in open-source视觉语言模型(VLMs)如LLaVA、Qwen-VL和Llama推动了其与不同系统的集成研究。这些模型中蕴含的大规模通用知识为自主驾驶感知、预测和规划能力的提升提供了重大机会。在本文中，我们提出了一种视觉语言自主驾驶模型(VLAD)，该模型将细调的VLM与最先进的端到端系统VAD集成。我们采用专门设计的问题-回答数据集实现了一种定制的细调方法，以提高模型的空间推理能力。增强的VLM生成高层次的导航命令，VAD随后处理这些命令以指导车辆操作。此外，我们的系统生成可解释的自然语言驾驶决策解释，从而增加传统黑盒端到端架构的透明性和可信度。在现实世界nuScenes数据集上的全面评估表明，与基线方法相比，我们集成的系统将平均碰撞率降低了31.82%，建立了VLM增强自主驾驶系统的新的基准。 

---
# Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization 

**Title (ZH)**: 重思所有证据：通过冲突驱动的总结增强可信赖的检索增强生成 

**Authors**: Juan Chen, Baolong Bi, Wei Zhang, Jingyan Sui, Xiaofei Zhu, Yuanzhuo Wang, Lingrui Mei, Shenghua Liu  

**Link**: [PDF](https://arxiv.org/pdf/2507.01281)  

**Abstract**: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by integrating their parametric knowledge with external retrieved content. However, knowledge conflicts caused by internal inconsistencies or noisy retrieved content can severely undermine the generation reliability of RAG this http URL this work, we argue that LLMs should rethink all evidence, including both retrieved content and internal knowledge, before generating this http URL propose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel framework that improves trustworthiness through Conflict-Driven Summarization of all available this http URL-RAG first derives parameter-aware evidence by comparing parameter records to identify diverse internal perspectives. It then refines retrieved evidences to produce context-aware evidence, removing irrelevant or misleading content. To detect and summarize conflicts, we distill a 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable synthesis across multiple this http URL further ensure evaluation integrity, we introduce a QA Repair step to correct outdated or ambiguous benchmark this http URL on revised QA datasets with retrieval data show that CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios with noisy or conflicting evidence. 

**Abstract (ZH)**: 检索增强生成（RAG）通过结合大型语言模型（LLMs）的参数化知识和外部检索内容来增强LLMs，然而，由内部不一致或噪声检索内容引起的知识冲突严重削弱了RAG的生成可靠性。在本文中，我们主张在生成之前，LLMs 应重新考虑所有证据，包括检索内容和内部知识。为此，我们提出了CARE-RAG（冲突感知和可靠的证据），一种通过冲突驱动总结所有可用证据的新框架，以提高可信度。CARE-RAG 首先通过比较参数记录来推导参数感知的证据，以识别不同的内部视角。然后，它细化检索证据以产生上下文感知的证据，删除无关或误导性的内容。为了检测和总结冲突，我们将一个3B LLaMA3.2模型蒸馏以执行冲突驱动的总结，从而实现多个来源之间的可靠合成。为了进一步确保评估的完整性，我们引入了一步步法问答修复步骤来纠正过时或含糊不清的基准。在使用检索数据修订的QA数据集上进行评估显示，CARE-RAG 在有噪声或冲突证据的场景中始终优于强大的RAG基线。 

---
# AI Meets Maritime Training: Precision Analytics for Enhanced Safety and Performance 

**Title (ZH)**: AI融入海上培训：精准分析以提高安全性和 performance 

**Authors**: Vishakha Lall, Yisi Liu  

**Link**: [PDF](https://arxiv.org/pdf/2507.01274)  

**Abstract**: Traditional simulator-based training for maritime professionals is critical for ensuring safety at sea but often depends on subjective trainer assessments of technical skills, behavioral focus, communication, and body language, posing challenges such as subjectivity, difficulty in measuring key features, and cognitive limitations. Addressing these issues, this study develops an AI-driven framework to enhance maritime training by objectively assessing trainee performance through visual focus tracking, speech recognition, and stress detection, improving readiness for high-risk scenarios. The system integrates AI techniques, including visual focus determination using eye tracking, pupil dilation analysis, and computer vision; communication analysis through a maritime-specific speech-to-text model and natural language processing; communication correctness using large language models; and mental stress detection via vocal pitch. Models were evaluated on data from simulated maritime scenarios with seafarers exposed to controlled high-stress events. The AI algorithms achieved high accuracy, with ~92% for visual detection, ~91% for maritime speech recognition, and ~90% for stress detection, surpassing existing benchmarks. The system provides insights into visual attention, adherence to communication checklists, and stress levels under demanding conditions. This study demonstrates how AI can transform maritime training by delivering objective performance analytics, enabling personalized feedback, and improving preparedness for real-world operational challenges. 

**Abstract (ZH)**: 基于AI的框架增强 Maritime 专业人员培训：通过视觉焦点跟踪、语音识别和压力检测实现客观性能评估 

---
# PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning 

**Title (ZH)**: PULSE: 实用的大型多模态模型抹除评估场景 

**Authors**: Tatsuki Kawakami, Kazuki Egashira, Atsuyuki Miyai, Go Irie, Kiyoharu Aizawa  

**Link**: [PDF](https://arxiv.org/pdf/2507.01271)  

**Abstract**: In recent years, unlearning techniques, which are methods for inducing a model to "forget" previously learned information, have attracted attention as a way to address privacy and copyright concerns in large language models (LLMs) and large multimodal models (LMMs). While several unlearning benchmarks have been established for LLMs, a practical evaluation framework for unlearning in LMMs has been less explored. Specifically, existing unlearning benchmark for LMMs considers only scenarios in which the model is required to unlearn fine-tuned knowledge through a single unlearning operation. In this study, we introduce PULSE protocol for realistic unlearning scenarios for LMMs by introducing two critical perspectives: (i) Pre-trained knowledge Unlearning for analyzing the effect across different knowledge acquisition phases and (ii) Long-term Sustainability Evaluation to address sequential requests. We then evaluate existing unlearning methods along these dimensions. Our results reveal that, although some techniques can successfully unlearn knowledge acquired through fine-tuning, they struggle to eliminate information learned during pre-training. Moreover, methods that effectively unlearn a batch of target data in a single operation exhibit substantial performance degradation when the same data are split and unlearned sequentially. 

**Abstract (ZH)**: 近年来，去学习技术（即使模型“忘记”先前学习信息的方法）逐渐受到关注，作为解决大型语言模型（LLMs）和大型多模态模型（LMMs）中的隐私和版权问题的手段。虽然为LLMs建立了一些去学习基准，但关于LMMs的实用去学习评估框架的研究较少。具体而言，现有的LMMs去学习基准仅考虑了模型通过单次去学习操作遗忘细调知识的情况。在本研究中，我们通过引入两个关键视角，提出了PULSE协议，以解决LMMs的现实去学习场景：（i）预训练知识去学习，分析不同知识获取阶段的影响；（ii）长期可持续性评估，应对顺序请求。我们随后从这些维度评估现有去学习方法。结果显示，虽然一些技术能够成功地去学习通过细调获得的知识，但在单次操作中去学习预训练期间学习的信息却遇到困难。此外，能够在单次操作中有效去学习目标数据批的方法，在数据被拆分并顺序去学习时，会表现出显著的性能下降。 

---
# LLM-based Realistic Safety-Critical Driving Video Generation 

**Title (ZH)**: 基于LLM的现实主义安全关键驾驶视频生成 

**Authors**: Yongjie Fu, Ruijian Zha, Pei Tian, Xuan Di  

**Link**: [PDF](https://arxiv.org/pdf/2507.01264)  

**Abstract**: Designing diverse and safety-critical driving scenarios is essential for evaluating autonomous driving systems. In this paper, we propose a novel framework that leverages Large Language Models (LLMs) for few-shot code generation to automatically synthesize driving scenarios within the CARLA simulator, which has flexibility in scenario scripting, efficient code-based control of traffic participants, and enforcement of realistic physical dynamics. Given a few example prompts and code samples, the LLM generates safety-critical scenario scripts that specify the behavior and placement of traffic participants, with a particular focus on collision events. To bridge the gap between simulation and real-world appearance, we integrate a video generation pipeline using Cosmos-Transfer1 with ControlNet, which converts rendered scenes into realistic driving videos. Our approach enables controllable scenario generation and facilitates the creation of rare but critical edge cases, such as pedestrian crossings under occlusion or sudden vehicle cut-ins. Experimental results demonstrate the effectiveness of our method in generating a wide range of realistic, diverse, and safety-critical scenarios, offering a promising tool for simulation-based testing of autonomous vehicles. 

**Abstract (ZH)**: 利用大型语言模型进行少样本代码生成以自动合成安全关键驾驶场景：一种在CARLA模拟器中的新框架 

---
# GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant 

**Title (ZH)**: GAIus: 结合Genai与法律条款检索的知识型助手 

**Authors**: Michał Matak, Jarosław A. Chudziak  

**Link**: [PDF](https://arxiv.org/pdf/2507.01259)  

**Abstract**: In this paper we discuss the capability of large language models to base their answer and provide proper references when dealing with legal matters of non-english and non-chinese speaking country. We discuss the history of legal information retrieval, the difference between case law and statute law, its impact on the legal tasks and analyze the latest research in this field. Basing on that background we introduce gAIus, the architecture of the cognitive LLM-based agent, whose responses are based on the knowledge retrieved from certain legal act, which is Polish Civil Code. We propose a retrieval mechanism which is more explainable, human-friendly and achieves better results than embedding-based approaches. To evaluate our method we create special dataset based on single-choice questions from entrance exams for law apprenticeships conducted in Poland. The proposed architecture critically leveraged the abilities of used large language models, improving the gpt-3.5-turbo-0125 by 419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%. At the end of our paper we show the possible future path of research and potential applications of our findings. 

**Abstract (ZH)**: 本文探讨了大型语言模型在处理非英语和非汉语国家法律事务时基于知识回答并提供适当引用的能力。我们讨论了法律信息检索的历史，案例法和成文法之间的差异，及其对法律任务的影响，并分析了该领域的最新研究。基于这一背景，我们介绍了gAIus，一种基于认知大语言模型代理的架构，其回答基于从波兰民法等相关法律文件中检索的知识。我们提出了一种更具解释性、更友好的检索机制，其效果优于基于嵌入的方法。为了评估我们的方法，我们基于波兰法律学徒入学考试中的单选题创建了专门的数据集。所提出的架构充分利用了所使用的大语言模型的能力，将GPT-3.5-turbo-0125的性能提升419%，使其超越GPT-4o，并将GPT-4o-mini的得分从31%提升到86%。在论文的结尾，我们展示了研究的可能未来路径及其发现的应用前景。 

---
# Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW 

**Title (ZH)**: 超越一阶方法：使用随机共轭次梯度和AdamW训练大规模语言模型 

**Authors**: Di Zhang, Yihang Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2507.01241)  

**Abstract**: Stochastic gradient-based descent (SGD), have long been central to training large language models (LLMs). However, their effectiveness is increasingly being questioned, particularly in large-scale applications where empirical evidence suggests potential performance limitations. In response, this paper proposes a stochastic conjugate subgradient method together with adaptive sampling tailored specifically for training LLMs. The method not only achieves faster convergence per iteration but also demonstrates improved scalability compared to traditional SGD techniques. It leverages sample complexity analysis to adaptively choose the sample size, employs a stochastic conjugate subgradient approach to determine search directions and utilizing an AdamW-like algorithm to adaptively adjust step sizes. This approach preserves the key advantages of first-order methods while effectively addressing the nonconvexity and non-smoothness inherent in LLMs training. Additionally, we provide a detailed analysis of the advantage of the algorithm. Experimental results show that the proposed method not only maintains, but in many cases surpasses, the scalability of traditional SGD techniques, significantly enhancing both the speed and accuracy of the optimization process. 

**Abstract (ZH)**: 基于随机梯度的下降（SGD）方法长期是训练大规模语言模型（LLMs）的核心。然而，在大规模应用中，它们的有效性正在受到质疑，特别是由于实证证据表明可能存在性能限制。为应对这一挑战，本文提出了一种特制的随机共轭次梯度方法，结合了适应性采样，专门用于训练LLMs。该方法不仅每次迭代都能更快地收敛，而且与传统的SGD技术相比，显示出更好的可扩展性。它利用样本复杂性分析自适应选择样本大小，采用随机共轭次梯度方法来确定搜索方向，并利用类似AdamW的算法自适应调整步长。该方法保留了一阶方法的关键优势，同时有效解决了LLMs训练中固有的非凸性和非光滑性。此外，我们还详细分析了该算法的优势。实验结果表明，所提出的方法不仅保持了，而且在许多情况下超越了传统SGD技术的可扩展性，显著提高了优化过程的速度和准确性。 

---
# Capacity Planning and Scheduling for Jobs with Uncertainty in Resource Usage and Duration 

**Title (ZH)**: 具有资源使用量和持续时间不确定性的作业的容量规划与调度 

**Authors**: Sunandita Patra, Mehtab Pathan, Mahmoud Mahfouz, Parisa Zehtabi, Wided Ouaja, Daniele Magazzeni, Manuela Veloso  

**Link**: [PDF](https://arxiv.org/pdf/2507.01225)  

**Abstract**: Organizations around the world schedule jobs (programs) regularly to perform various tasks dictated by their end users. With the major movement towards using a cloud computing infrastructure, our organization follows a hybrid approach with both cloud and on-prem servers. The objective of this work is to perform capacity planning, i.e., estimate resource requirements, and job scheduling for on-prem grid computing environments. A key contribution of our approach is handling uncertainty in both resource usage and duration of the jobs, a critical aspect in the finance industry where stochastic market conditions significantly influence job characteristics. For capacity planning and scheduling, we simultaneously balance two conflicting objectives: (a) minimize resource usage, and (b) provide high quality-of-service to the end users by completing jobs by their requested deadlines. We propose approximate approaches using deterministic estimators and pair sampling-based constraint programming. Our best approach (pair sampling-based) achieves much lower peak resource usage compared to manual scheduling without compromising on the quality-of-service. 

**Abstract (ZH)**: 全球范围内，组织定期安排作业（程序）以执行各种由最终用户指定的任务。随着云计算基础设施的广泛应用，我们的组织采用混合方式，结合使用云服务器和本地服务器。本研究的目的是进行容量规划，即估算本地网格计算环境所需的资源需求和作业调度。我们方法的关键贡献在于处理资源使用量和作业持续时间的不确定性，这在金融市场中尤为重要，因为在金融市场中，随机市场条件显著影响作业特性。在进行容量规划和调度时，我们同时平衡两个相互冲突的目标：（a）最小化资源使用；（b）通过提供高质量的服务来满足最终用户提出的截止时间要求。我们提出了基于确定性估计和配对抽样约束编程的近似方法。我们最佳的方法（基于配对抽样）在不牺牲服务质量的情况下，实现了显著较低的峰值资源使用量。 

---
# Search-Based Robot Motion Planning With Distance-Based Adaptive Motion Primitives 

**Title (ZH)**: 基于搜索的机器人运动规划与距离自适应运动基元 

**Authors**: Benjamin Kraljusic, Zlatan Ajanovic, Nermin Covic, Bakir Lacevic  

**Link**: [PDF](https://arxiv.org/pdf/2507.01198)  

**Abstract**: This work proposes a motion planning algorithm for robotic manipulators that combines sampling-based and search-based planning methods. The core contribution of the proposed approach is the usage of burs of free configuration space (C-space) as adaptive motion primitives within the graph search algorithm. Due to their feature to adaptively expand in free C-space, burs enable more efficient exploration of the configuration space compared to fixed-sized motion primitives, significantly reducing the time to find a valid path and the number of required expansions. The algorithm is implemented within the existing SMPL (Search-Based Motion Planning Library) library and evaluated through a series of different scenarios involving manipulators with varying number of degrees-of-freedom (DoF) and environment complexity. Results demonstrate that the bur-based approach outperforms fixed-primitive planning in complex scenarios, particularly for high DoF manipulators, while achieving comparable performance in simpler scenarios. 

**Abstract (ZH)**: 基于采样和搜索规划方法的机器人 manipulator 运动规划算法：基于自由配置空间泡的可适应运动基元 

---
# Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning 

**Title (ZH)**: 大型脑波基础模型目前是否足够？从微调获得的见解。 

**Authors**: Na Lee, Konstantinos Barmpas, Yannis Panagakis, Dimitrios Adamos, Nikolaos Laskaris, Stefanos Zafeiriou  

**Link**: [PDF](https://arxiv.org/pdf/2507.01196)  

**Abstract**: Foundation Models have demonstrated significant success across various domains in Artificial Intelligence (AI), yet their capabilities for brainwave modeling remain unclear. In this paper, we comprehensively evaluate current Large Brainwave Foundation Models (LBMs) through systematic fine-tuning experiments across multiple Brain-Computer Interface (BCI) benchmark tasks, including memory tasks and sleep stage classification. Our extensive analysis shows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%) over traditional deep architectures while requiring significantly more parameters (millions vs thousands), raising important questions about their efficiency and applicability in BCI contexts. Moreover, through detailed ablation studies and Low-Rank Adaptation (LoRA), we significantly reduce trainable parameters without performance degradation, while demonstrating that architectural and training inefficiencies limit LBMs' current capabilities. Our experiments span both full model fine-tuning and parameter-efficient adaptation techniques, providing insights into optimal training strategies for BCI applications. We pioneer the application of LoRA to LBMs, revealing that performance benefits generally emerge when adapting multiple neural network components simultaneously. These findings highlight the critical need for domain-specific development strategies to advance LBMs, suggesting that current architectures may require redesign to fully leverage the potential of foundation models in brainwave analysis. 

**Abstract (ZH)**: 大型脑电基础模型在脑-机接口任务中的表现及其改进策略：从全模型微调到低秩适应的全面评估 

---
# Geometry-aware 4D Video Generation for Robot Manipulation 

**Title (ZH)**: 面向几何aware的4D视频生成在机器人操作中 

**Authors**: Zeyi Liu, Shuang Li, Eric Cousineau, Siyuan Feng, Benjamin Burchfiel, Shuran Song  

**Link**: [PDF](https://arxiv.org/pdf/2507.01099)  

**Abstract**: Understanding and predicting the dynamics of the physical world can enhance a robot's ability to plan and interact effectively in complex environments. While recent video generation models have shown strong potential in modeling dynamic scenes, generating videos that are both temporally coherent and geometrically consistent across camera views remains a significant challenge. To address this, we propose a 4D video generation model that enforces multi-view 3D consistency of videos by supervising the model with cross-view pointmap alignment during training. This geometric supervision enables the model to learn a shared 3D representation of the scene, allowing it to predict future video sequences from novel viewpoints based solely on the given RGB-D observations, without requiring camera poses as inputs. Compared to existing baselines, our method produces more visually stable and spatially aligned predictions across multiple simulated and real-world robotic datasets. We further show that the predicted 4D videos can be used to recover robot end-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting robust robot manipulation and generalization to novel camera viewpoints. 

**Abstract (ZH)**: 理解并预测物理世界的动力学可以增强机器人在复杂环境中的计划和交互能力。虽然近年来的视频生成模型在建模动态场景方面展现出了强大的潜力，但生成在不同摄像机视角下时空一致且几何一致的视频仍然是一个重大挑战。为了应对这一挑战，我们提出了一种4D视频生成模型，在训练过程中通过监督模型的跨视图点图对齐来强制多视图三维一致性。这种几何监督使模型能够学习场景的共享三维表示，从而仅根据给定的RGB-D观察结果预测新的视角下的未来视频序列，不需要输入摄像机姿态。与现有基线方法相比，我们的方法在多个模拟和真实世界机器人数据集中产生了更加视觉稳定且空间对齐的预测。进一步研究表明，预测的4D视频可以使用现成的6DoF姿态追踪器来恢复机器人末端执行器轨迹，支持鲁棒的机器人操作并在新的摄像机视角上泛化。 

---
# AI-guided digital intervention with physiological monitoring reduces intrusive memories after experimental trauma 

**Title (ZH)**: AI引导的数字干预结合生理监测减少实验性创伤后的侵入性记忆 

**Authors**: Megan T. deBettencourt, Sruthi Sakthivel, Emily A. Holmes, Mark Chevillet  

**Link**: [PDF](https://arxiv.org/pdf/2507.01081)  

**Abstract**: Trauma prevalence is vast globally. Evidence-based digital treatments can help, but most require human guidance. Human guides provide tailored instructions and responsiveness to internal cognitive states, but limit scalability. Can generative AI and neurotechnology provide a scalable alternative? Here we test ANTIDOTE, combining AI guidance and pupillometry to automatically deliver and monitor an evidence-based digital treatment, specifically the Imagery Competing Task Intervention (ICTI), to reduce intrusive memories after psychological trauma. One hundred healthy volunteers were exposed to videos of traumatic events and randomly assigned to an intervention or active control condition. As predicted, intervention participants reported significantly fewer intrusive memories over the following week. Post-hoc assessment against clinical rubrics confirmed the AI guide delivered the intervention successfully. Additionally, pupil size tracked intervention engagement and predicted symptom reduction, providing a candidate biomarker of intervention effectiveness. These findings open a path toward rigorous AI-guided digital interventions that can scale to trauma prevalence. 

**Abstract (ZH)**: 全球创伤发病率巨大。基于证据的数字治疗可以有所帮助，但大多需要人工指导。人工指导提供了个性化指令和对内在认知状态的响应，但限制了可扩展性。生成式AI和神经技术能否提供一种可扩展的替代方案？我们测试了ANTIDOTE，结合AI指导和瞳孔ometry，自动提供并监测基于证据的数字治疗，即图像对抗任务干预（ICTI），以减少心理创伤后的侵入性记忆。一百名健康志愿者观看了创伤事件视频，并随机分配到干预组或活性对照组。正如预测的那样，干预组参与者在随后的一周内报告的侵入性记忆显著较少。事后评估显示，AI指导成功地提供了干预。此外，瞳孔大小跟踪干预参与度并预测症状减少，提供了干预有效性的候选生物标志物。这些发现为走向严谨的AI指导数字干预并实现对创伤发病率的扩展打开了路径。 

---
# Empirical Analysis Of Heuristic and Approximation Algorithms for the The Mutual-Visibility Problem 

**Title (ZH)**: 启发式和近似算法对互视问题的实证分析 

**Authors**: Vanja Stojanović, Bor Pangeršič  

**Link**: [PDF](https://arxiv.org/pdf/2507.01076)  

**Abstract**: The NP-complete mutual-visibility (MV) problem currently lacks empirical analysis on its practical behaviour despite theoretical studies. This paper addresses this gap by implementing and evaluating three distinct algorithms - a direct greedy heuristic, a hypergraph-based approximation, and a genetic algorithm - on diverse synthetic graph datasets, including those with analytically known $\mu(G)$ values and general graph models. Our results demonstrate that for smaller graphs, the algorithms consistently achieve MV set sizes aligning with theoretical bounds. However, for larger instances, achieved solution sizes notably diverge from theoretical limits; this, combined with the absence of tight bounds, complicates absolute quality assessment. Nevertheless, validation on known optimal graphs showed the Genetic Algorithm and other heuristics empirically performing best among tested methods. 

**Abstract (ZH)**: NP完全互视（MV）问题当前缺乏对其实际行为的实证分析，尽管已有理论研究。本文通过在包括已知 $\mu(G)$ 值的合成图数据集和一般图模型上实现和评估三种不同的算法——直接贪婪启发式算法、基于超图的近似算法以及遗传算法——来弥补这一空白。我们的结果表明，对于较小的图，算法一致地实现了与理论界限相符的MV集大小。然而，对于较大的实例，达到的解决方案大小明显偏离理论限制；这与缺乏紧的边界一起，使得绝对质量评估变得复杂。不过，对已知最优图的验证显示，遗传算法和其他启发式算法在测试方法中表现最佳。 

---
# Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services 

**Title (ZH)**: 对基础模型和随机模型在预测高性能机器学习服务间歇性或突发性生产中断方面的评价 

**Authors**: Keun Soo Yim  

**Link**: [PDF](https://arxiv.org/pdf/2507.01067)  

**Abstract**: Time series forecasting models have diverse real world applications (e.g., from electricity metrics to software workload). Latest foundational models trained for time series forecasting show strengths (e.g., for long sequences and in zero-shot settings). However, foundational model was not yet used for forecasting rare, spiky events, i.e., a challenging target because those are a corner case of extreme events. In this paper, we optimize a state-of-the-art foundational model to forecast sporadic or spiky production outages of high-performance machine learning services powering billions of client devices. We evaluate the forecasting errors of the foundational model compared with classical stochastic forecasting models (e.g., moving average and autoregressive). The analysis helps us understand how each of the evaluated models performs for the sporadic or spiky events. For example, it identifies the key patterns in the target data that are well tracked by the foundational model vs. each of the stochastic models. We use the models with optimal parameters to estimate a year-long outage statistics of a particular root cause with less than 6% value errors. 

**Abstract (ZH)**: 时间序列预测模型在各类实际应用中具有多样性（例如，从电力指标到软件工作负载）。最新的基础模型用于时间序列预测展现了优势（例如，在长序列和零样本设置中的表现）。然而，这些基础模型尚未被用于预测罕见、突发事件，这是一项具有挑战性的目标，因为这些事件是极端事件的一种特例。在本文中，我们优化了一种最新基础模型，以预测高性能机器学习服务驱动的数十亿客户端设备中的间歇性或突发性生产中断。我们将基础模型的预测误差与经典的随机预测模型（如移动平均和自回归模型）进行了比较评估，这有助于理解每种评估模型在间歇性或突发性事件中的表现。例如，它确定了目标数据中的关键模式，这些模式被基础模型良好追踪，而未被各随机模型很好地捕捉。我们使用具有最优参数的模型来估算特定根本原因一年的中断统计数据，误差低于6%。 

---
# FAIR-MATCH: A Multi-Objective Framework for Bias Mitigation in Reciprocal Dating Recommendations 

**Title (ZH)**: FAIR-MATCH：一种用于互惠 dating 推荐中偏见缓解的多目标框架 

**Authors**: Madhav Kotecha  

**Link**: [PDF](https://arxiv.org/pdf/2507.01063)  

**Abstract**: Online dating platforms have fundamentally transformed the formation of romantic relationships, with millions of users worldwide relying on algorithmic matching systems to find compatible partners. However, current recommendation systems in dating applications suffer from significant algorithmic deficiencies, including but not limited to popularity bias, filter bubble effects, and inadequate reciprocity modeling that limit effectiveness and introduce harmful biases. This research integrates foundational work with recent empirical findings to deliver a detailed analysis of dating app recommendation systems, highlighting key issues and suggesting research-backed solutions. Through analysis of reciprocal recommendation frameworks, fairness evaluation metrics, and industry implementations, we demonstrate that current systems achieve modest performance with collaborative filtering reaching 25.1\% while reciprocal methods achieve 28.7\%. Our proposed mathematical framework addresses these limitations through enhanced similarity measures, multi-objective optimization, and fairness-aware algorithms that maintain competitive accuracy while improving demographic representation to reduce algorithmic bias. 

**Abstract (ZH)**: 在线 dating 平台从根本上改变了浪漫关系的形成方式，全球数百万用户依赖算法匹配系统寻找兼容的伴侣。然而，当前 dating 应用中的推荐系统存在显著的算法缺陷，包括但不限于流行度偏差、过滤气泡效应和不充分的互惠建模，这些都限制了系统的有效性并引入了有害的偏差。本研究结合基础工作与最新的实证研究成果，对 dating 平台推荐系统进行了详细分析，指出现存的关键问题，并提出基于研究的解决方案。通过对互惠推荐框架、公平性评估指标及行业实施的分析，我们表明当前系统取得适中的性能，协作过滤达到 25.1%，而互惠方法达到 28.7%。我们提出的数学框架通过增强相似度度量、多目标优化及兼具公平性的算法，在保持竞争力的同时改善了人口统计学代表性，从而减少算法偏差。 

---
# Quantifying Student Success with Generative AI: A Monte Carlo Simulation Informed by Systematic Review 

**Title (ZH)**: 基于系统回顾指导的蒙特卡洛仿真定量衡量学生成功与生成式AI的关系 

**Authors**: Seyma Yaman Kayadibi  

**Link**: [PDF](https://arxiv.org/pdf/2507.01062)  

**Abstract**: The exponential development of generative artificial intelligence (GenAI) technologies like ChatGPT has raised increasing curiosity about their use in higher education, specifically with respect to how students view them, make use of them, and the implications for learning outcomes. This paper employs a hybrid methodological approach involving a systematic literature review and simulation-based modeling to explore student perceptions of GenAI use in the context of higher education. A total of nineteen empirical articles from 2023 through 2025 were selected from the PRISMA-based search targeting the Scopus database. Synthesis of emerging patterns from the literature was achieved by thematic categorization. Six of these had enough quantitative information, i.e., item-level means and standard deviations, to permit probabilistic modeling. One dataset, from the resulting subset, was itself selected as a representative case with which to illustrate inverse-variance weighting by Monte Carlo simulation, by virtue of its well-designed Likert scale format and thematic alignment with the use of computing systems by the researcher.
The simulation provided a composite "Success Score" forecasting the strength of the relationship between student perceptions and learning achievements. Findings reveal that attitude factors concerned with usability and real-world usefulness are significantly better predictors of positive learning achievement than affective or trust-based factors. Such an interdisciplinary perspective provides a unique means of linking thematic results with predictive modelling, resonating with longstanding controversies about the proper use of GenAI tools within the university. 

**Abstract (ZH)**: 生成人工智能（GenAI）技术如ChatGPT的指数级发展引发了对它们在高等教育中应用的日益浓厚兴趣，特别是学生如何看待这些技术、如何使用它们以及对学习成果的影响。本文采用混合方法论，结合系统文献综述和基于仿真的建模，探究GenAI在高等教育中的使用对学生感知的影响。从2023年至2025年使用PRISMA方法在Scopus数据库中筛选出19篇实证文章。通过主题分类整合文献中 emerging 的模式。六篇文章含有足够的定量信息，即项目级别的平均值和标准差，允许进行概率建模。其中一个数据集被选为具有代表性的案例，通过蒙特卡洛仿真的逆方差加权进行示例，因为它拥有精心设计的李克特量表格式，并且主题上与研究人员使用计算系统的模式相吻合。

仿真实验提供了一个综合的“成功分数”，预测学生感知与学习成就之间的关系强度。研究发现，与情感或基于信任的因素相比，与易用性和实际有用性相关的态度因素是积极学习成就的更显著的预测因子。这种跨学科视角为将主题结果与预测建模联系起来提供了一种独特的方式，与关于大学中正确使用GenAI工具的长期争议相呼应。 

---
# Epitome: Pioneering an Experimental Platform for AI-Social Science Integration 

**Title (ZH)**: Epitome: 探索人工智能与社会科学集成的实验平台 

**Authors**: Jingjing Qu, Kejia Hu, Jun Zhu, Wenhao Li, Teng Wang, Zhiyun Chen, Yulei Ye, Chaochao Lu, Aimin Zhou, Xiangfeng Wang, James Evan  

**Link**: [PDF](https://arxiv.org/pdf/2507.01061)  

**Abstract**: The integration of Large Language Models (LLMs) into social science experiments represents a transformative approach to understanding human-AI interactions and their societal impacts. We introduce Epitome, the world's first open experimental platform dedicated to the deep integration of artificial intelligence and social science. Rooted in theoretical foundations from management, communication studies, sociology, psychology, and ethics, Epitome focuses on the interactive impacts of AI on individuals, organizations, and society during its real-world deployment. It constructs a theoretical support system through cross-disciplinary experiments. The platform offers a one-stop comprehensive experimental solution spanning "foundation models-complex application development-user feedback" through seven core modules, while embedding the classical "control-comparison-comparative causal logic" of social science experiments into multilevel human-computer interaction environments, including dialogues, group chats, and multi-agent virtual scenarios. With its canvas-style, user-friendly interface, Epitome enables researchers to easily design and run complex experimental scenarios, facilitating systematic investigations into the social impacts of AI and exploration of integrated this http URL demonstrate its capabilities, we replicated three seminal social science experiments involving LLMs, showcasing Epitome's potential to streamline complex experimental designs and produce robust results, suitable for publishing in the top selective journals. Our findings highlight the platform's utility in enhancing the efficiency and quality of human-AI interactions, providing valuable insights into the societal implications of AI technologies. Epitome thus offers a powerful tool for advancing interdisciplinary research at the intersection of AI and social science, with potential applications in policy-making, ... 

**Abstract (ZH)**: 大型语言模型（LLMs）在社会科学实验中的整合代表了一种理解人机交互及其社会影响的变革性方法。我们介绍了Epitome，这是一个致力于人工智能与社会科学深度融合的世界首个开放实验平台。Epitome基于管理学、传播学、社会学、心理学和伦理学的理论基础，聚焦于AI在实际应用中对个体、组织和社会的互动影响，并通过跨学科实验构建理论支持系统。该平台提供从“基础模型-复杂应用开发-用户反馈”一站式全面实验解决方案，通过多层级的人机交互环境，包括对话、群聊和多智能体虚拟场景，嵌入社会科学实验的经典“控制-比较-因果逻辑”。Epitome以其画布式、用户友好的界面，使研究人员能够轻松设计和运行复杂的实验情景，促进对AI社会影响的系统性研究，并探索其与社会科学的整合应用。为了展示其能力，我们复制了三个涉及LLMs的关键社会科学实验，展现了Epitome简化复杂实验设计并产生稳健结果的潜力，适合发表在顶尖期刊上。我们的研究结果强调了该平台在提高人机交互效率和质量方面的 usefulness，提供了关于AI技术社会影响的重要见解。Epitome因此成为促进人工智能与社会科学交叉领域综合研究的强大工具，具有在政策制定等领域中的潜在应用。 

---
# Automated Vehicles Should be Connected with Natural Language 

**Title (ZH)**: 自动车辆应当与自然语言连接。 

**Authors**: Xiangbo Gao, Keshu Wu, Hao Zhang, Kexin Tian, Yang Zhou, Zhengzhong Tu  

**Link**: [PDF](https://arxiv.org/pdf/2507.01059)  

**Abstract**: Multi-agent collaborative driving promises improvements in traffic safety and efficiency through collective perception and decision making. However, existing communication media -- including raw sensor data, neural network features, and perception results -- suffer limitations in bandwidth efficiency, information completeness, and agent interoperability. Moreover, traditional approaches have largely ignored decision-level fusion, neglecting critical dimensions of collaborative driving. In this paper we argue that addressing these challenges requires a transition from purely perception-oriented data exchanges to explicit intent and reasoning communication using natural language. Natural language balances semantic density and communication bandwidth, adapts flexibly to real-time conditions, and bridges heterogeneous agent platforms. By enabling the direct communication of intentions, rationales, and decisions, it transforms collaborative driving from reactive perception-data sharing into proactive coordination, advancing safety, efficiency, and transparency in intelligent transportation systems. 

**Abstract (ZH)**: 多智能体协同驾驶通过集体感知与决策承诺在交通安全性与效率方面带来提升。然而，现有的通信媒体——包括原始传感器数据、神经网络特征和感知结果——在带宽效率、信息完整性和智能体互操作性方面存在局限。此外，传统方法在很大程度上忽视了决策级融合，忽略了协同驾驶的关键维度。本文认为，解决这些挑战需要从纯感知导向的数据交换过渡到使用自然语言进行明确的意图与推理通信。自然语言平衡了语义密度和通信带宽，能够灵活适应实时条件，并连接异构智能体平台。通过直接通信意图、理由和决策，它将协同驾驶从被动的数据感知共享转变为积极的协调，从而推进智能交通系统的安全、效率和透明度。 

---
# A Data Science Approach to Calcutta High Court Judgments: An Efficient LLM and RAG-powered Framework for Summarization and Similar Cases Retrieval 

**Title (ZH)**: 基于数据科学方法的加尔各答高院判决分析：一种高效的LLM和RAG驱动的摘要与相似案例检索框架 

**Authors**: Puspendu Banerjee, Aritra Mazumdar, Wazib Ansar, Saptarsi Goswami, Amlan Chakrabarti  

**Link**: [PDF](https://arxiv.org/pdf/2507.01058)  

**Abstract**: The judiciary, as one of democracy's three pillars, is dealing with a rising amount of legal issues, needing careful use of judicial resources. This research presents a complex framework that leverages Data Science methodologies, notably Large Language Models (LLM) and Retrieval-Augmented Generation (RAG) techniques, to improve the efficiency of analyzing Calcutta High Court verdicts. Our framework focuses on two key aspects: first, the creation of a robust summarization mechanism that distills complex legal texts into concise and coherent summaries; and second, the development of an intelligent system for retrieving similar cases, which will assist legal professionals in research and decision making. By fine-tuning the Pegasus model using case head note summaries, we achieve significant improvements in the summarization of legal cases. Our two-step summarizing technique preserves crucial legal contexts, allowing for the production of a comprehensive vector database for RAG. The RAG-powered framework efficiently retrieves similar cases in response to user queries, offering thorough overviews and summaries. This technique not only improves legal research efficiency, but it also helps legal professionals and students easily acquire and grasp key legal information, benefiting the overall legal scenario. 

**Abstract (ZH)**: 司法机关作为民主的三大支柱之一，正面临着越来越多的法律问题，需要谨慎使用司法资源。本研究提出一种复杂框架，利用数据科学方法，特别是大型语言模型（LLM）和检索增强生成（RAG）技术，以提高分析加尔各答高等法院判决书的效率。该框架重点关注两个关键方面：首先，创建一个稳健的摘要机制，将复杂的法律文本提炼成简洁连贯的摘要；其次，开发一种智能的类似案例检索系统，该系统将帮助法律专业人员进行研究和决策。通过使用案件摘要头注对Pegasus模型进行微调，我们实现了对法律案件总结的显著改进。我们的两步汇总技术保留了关键的法律上下文，为RAG生成了一个完整的向量数据库。该RAG驱动的框架能够高效地根据用户查询检索类似案件，提供全面的概述和摘要。该技术不仅提高了法律研究的效率，还帮助法律专业人员和学生轻松获取和掌握关键法律信息，从而有利于整个法律环境。 

---
# Prompt Mechanisms in Medical Imaging: A Comprehensive Survey 

**Title (ZH)**: 医学影像中的提示机制：一项全面综述 

**Authors**: Hao Yang, Xinlong Liang, Zhang Li, Yue Sun, Zheyu Hu, Xinghe Xie, Behdad Dashtbozorg, Jincheng Huang, Shiwei Zhu, Luyi Han, Jiong Zhang, Shanshan Wang, Ritse Mann, Qifeng Yu, Tao Tan  

**Link**: [PDF](https://arxiv.org/pdf/2507.01055)  

**Abstract**: Deep learning offers transformative potential in medical imaging, yet its clinical adoption is frequently hampered by challenges such as data scarcity, distribution shifts, and the need for robust task generalization. Prompt-based methodologies have emerged as a pivotal strategy to guide deep learning models, providing flexible, domain-specific adaptations that significantly enhance model performance and adaptability without extensive retraining. This systematic review critically examines the burgeoning landscape of prompt engineering in medical imaging. We dissect diverse prompt modalities, including textual instructions, visual prompts, and learnable embeddings, and analyze their integration for core tasks such as image generation, segmentation, and classification. Our synthesis reveals how these mechanisms improve task-specific outcomes by enhancing accuracy, robustness, and data efficiency and reducing reliance on manual feature engineering while fostering greater model interpretability by making the model's guidance explicit. Despite substantial advancements, we identify persistent challenges, particularly in prompt design optimization, data heterogeneity, and ensuring scalability for clinical deployment. Finally, this review outlines promising future trajectories, including advanced multimodal prompting and robust clinical integration, underscoring the critical role of prompt-driven AI in accelerating the revolution of diagnostics and personalized treatment planning in medicine. 

**Abstract (ZH)**: 深度学习在医学影像领域的应用具有变革性潜力，但在临床应用中常因数据稀缺、分布偏移以及任务泛化能力不足等问题受到制约。基于提示的方法已成为引导深度学习模型的关键策略，提供灵活的、特定领域的适应性，显著提升模型性能和适应性，同时减少重新训练的需求。本文系统性地审视了提示工程在医学影像领域的新兴景观。我们剖析了各种提示模态，包括文本指令、视觉提示和可学习嵌入，并分析了它们在核心任务（如图像生成、分割和分类）中的整合应用。我们的综合分析表明，这些机制通过提高准确性、稳健性和数据效率，减少了对人工特征工程的依赖，同时使模型的指导更加明确，从而增强模型的可解释性。尽管取得了显著进展，但提示设计优化、数据异质性和确保临床部署的可扩展性等持续挑战仍然存在。最后，本文概述了提示驱动AI的有希望的未来发展方向，包括先进的多模态提示和稳健的临床集成，强调了提示驱动AI在加速医学诊断和个性化治疗规划革命中的关键作用。 

---
# XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science 

**Title (ZH)**: XxaCT-NN: 结构无关的多模态学习方法在材料科学中的应用 

**Authors**: Jithendaraa Subramanian, Linda Hung, Daniel Schweigert, Santosh Suram, Weike Ye  

**Link**: [PDF](https://arxiv.org/pdf/2507.01054)  

**Abstract**: Recent advances in materials discovery have been driven by structure-based models, particularly those using crystal graphs. While effective for computational datasets, these models are impractical for real-world applications where atomic structures are often unknown or difficult to obtain. We propose a scalable multimodal framework that learns directly from elemental composition and X-ray diffraction (XRD) -- two of the more available modalities in experimental workflows without requiring crystal structure input. Our architecture integrates modality-specific encoders with a cross-attention fusion module and is trained on the 5-million-sample Alexandria dataset. We present masked XRD modeling (MXM), and apply MXM and contrastive alignment as self-supervised pretraining strategies. Pretraining yields faster convergence (up to 4.2x speedup) and improves both accuracy and representation quality. We further demonstrate that multimodal performance scales more favorably with dataset size than unimodal baselines, with gains compounding at larger data regimes. Our results establish a path toward structure-free, experimentally grounded foundation models for materials science. 

**Abstract (ZH)**: 近期材料发现领域的进展得益于基于结构的模型，特别是使用晶体图模型。虽然这些模型适用于计算数据集，但在现实应用中，原子结构通常未知或难以获取，这使得这些模型不可行。我们提出了一种可扩展的多模态框架，可以直接从元素组成和X射线衍射（XRD）两种实验流程中更为可用的模态进行学习，无需输入晶体结构。该架构将模态特定编码器与跨注意力融合模块集成，并在包含500万样本的Alexandria数据集上进行训练。我们提出了掩蔽XRD建模（MXM），并使用MXM和对比对齐作为自监督预训练策略。预训练可以更快速地收敛（最高加速4.2倍），并提高准确性和表示质量。我们进一步表明，多模态性能在数据集规模增大时具有更好的扩展性，这种优势在大规模数据集上更为明显。我们的结果为材料科学奠定了无需结构、实验为基础的范式模型的道路。 

---
# Conversational LLMs Simplify Secure Clinical Data Access, Understanding, and Analysis 

**Title (ZH)**: 会话型大语言模型简化临床数据的安全访问、理解和分析 

**Authors**: Rafi Al Attrach, Pedro Moreira, Rajna Fani, Renato Umeton, Leo Anthony Celi  

**Link**: [PDF](https://arxiv.org/pdf/2507.01053)  

**Abstract**: As ever-larger clinical datasets become available, they have the potential to unlock unprecedented opportunities for medical research. Foremost among them is Medical Information Mart for Intensive Care (MIMIC-IV), the world's largest open-source EHR database. However, the inherent complexity of these datasets, particularly the need for sophisticated querying skills and the need to understand the underlying clinical settings, often presents a significant barrier to their effective use. M3 lowers the technical barrier to understanding and querying MIMIC-IV data. With a single command it retrieves MIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the hosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers converse with the database in plain English. Ask a clinical question in natural language; M3 uses a language model to translate it into SQL, executes the query against the MIMIC-IV dataset, and returns structured results alongside the underlying query for verifiability and reproducibility. Demonstrations show that minutes of dialogue with M3 yield the kind of nuanced cohort analyses that once demanded hours of handcrafted SQL and relied on understanding the complexities of clinical workflows. By simplifying access, M3 invites the broader research community to mine clinical critical-care data and accelerates the translation of raw records into actionable insight. 

**Abstract (ZH)**: 随着越来越多的临床数据集变得可用，它们为医学研究开启了前所未有的机会。其中最为重要的是Medical Information Mart for Intensive Care (MIMIC-IV)，这是世界上最大的开源EHR数据库。然而，这些数据集固有的复杂性，特别是需要高级查询技能以及理解其背后的临床环境，往往会成为有效使用它们的障碍。M3降低了理解与查询MIMIC-IV数据的技术门槛。通过单个命令，它可以从PhysioNet获取MIMIC-IV数据，启动本地SQLite实例（或连接到托管的BigQuery），并通过模型上下文协议（MCP）让研究人员用普通英语与数据库进行对话。用自然语言提出临床问题；M3使用语言模型将其翻译成SQL，执行查询并对MIMIC-IV数据集进行查询，返回结构化结果以及底层查询以供验证和可重复性。演示表明，与M3进行几分钟的对话就能获得一度需要花费数小时手工编写SQL且依赖于理解临床工作流程复杂性的精细队列分析。通过简化访问，M3邀请更广泛的科研社区挖掘临床重症监护数据，并加速将原始记录转化为可操作的洞察力。 

---
# Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals 

**Title (ZH)**: 长序列记忆的时序核与密集霍普菲尔德函数]." 

**Authors**: Ahmed Farooq  

**Link**: [PDF](https://arxiv.org/pdf/2507.01052)  

**Abstract**: In this study we introduce a novel energy functional for long-sequence memory, building upon the framework of dense Hopfield networks which achieves exponential storage capacity through higher-order interactions. Building upon earlier work on long-sequence Hopfield memory models, we propose a temporal kernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient sequential retrieval of patterns over extended sequences. We demonstrate the successful application of this technique for the storage and sequential retrieval of movies frames which are well suited for this because of the high dimensional vectors that make up each frame creating enough variation between even sequential frames in the high dimensional space. The technique has applications in modern transformer architectures, including efficient long-sequence modeling, memory augmentation, improved attention with temporal bias, and enhanced handling of long-term dependencies in time-series data. Our model offers a promising approach to address the limitations of transformers in long-context tasks, with potential implications for natural language processing, forecasting, and beyond. 

**Abstract (ZH)**: 本研究我们介绍了一种针对长序列记忆的新型能量函数，基于密集霍普菲尔德网络框架，通过更高阶的相互作用实现指数级存储容量。基于早期关于长序列霍普菲尔德记忆模型的工作，我们提出了一种时间内核 $K(m, k)$，以纳入时间依赖性，从而实现对扩展序列中模式的高效逐序列检索。我们展示了该技术在电影帧存储和检索中的成功应用，因为每一帧由高维向量构成，在高维空间中即使顺序帧之间也存在足够的变化。该技术在现代变换器架构中具有应用价值，包括高效长序列建模、记忆增强、带有时间偏置的注意力改进以及时间序列数据中长期依赖关系的增强处理。我们的模型为解决变换器在长上下文任务中的局限性提供了一种有前景的方法，并可能对自然语言处理、预测等领域产生影响。 

---
# Can AI be Consentful? 

**Title (ZH)**: AI能征得同意吗？ 

**Authors**: Giada Pistilli, Bruna Trevelin  

**Link**: [PDF](https://arxiv.org/pdf/2507.01051)  

**Abstract**: The evolution of generative AI systems exposes the challenges of traditional legal and ethical frameworks built around consent. This chapter examines how the conventional notion of consent, while fundamental to data protection and privacy rights, proves insufficient in addressing the implications of AI-generated content derived from personal data. Through legal and ethical analysis, we show that while individuals can consent to the initial use of their data for AI training, they cannot meaningfully consent to the numerous potential outputs their data might enable or the extent to which the output is used or distributed. We identify three fundamental challenges: the scope problem, the temporality problem, and the autonomy trap, which collectively create what we term a ''consent gap'' in AI systems and their surrounding ecosystem. We argue that current legal frameworks inadequately address these emerging challenges, particularly regarding individual autonomy, identity rights, and social responsibility, especially in cases where AI-generated content creates new forms of personal representation beyond the scope of the original consent. By examining how these consent limitations intersect with broader principles of responsible AI (including fairness, transparency, accountability, and autonomy) we demonstrate the need to evolve ethical and legal approaches to consent. 

**Abstract (ZH)**: 生成型AI系统的演进揭示了传统基于同意的法律和伦理框架的挑战。本章探讨了尽管传统的同意概念是数据保护和隐私权的基础，但在处理AI生成内容对个人数据的影响时仍显得不足。通过法律和伦理分析，我们表明，尽管个人可以对数据初始用于AI训练给予同意，但他们无法对数据可能产生的众多潜在输出以及这些输出的使用和分发程度给予有意义的同意。我们识别出三个基本挑战：范围问题、时效性问题和自主性陷阱，这些共同形成了我们所称的“同意缺口”。我们认为现有的法律框架在应对这些新兴挑战方面存在不足，特别是在AI生成内容超出原始同意范围创建新的个人代表性形式时，更缺乏针对个人自主性、身份权利和社会责任的考虑。通过分析这些同意限制与负责任AI的更广泛原则（包括公平性、透明度、可问责性和自主性）的交集，我们证明了需要发展新的伦理和法律上的同意方法。 

---
# Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization 

**Title (ZH)**: 文本净化：数据效率、语义保留与模型泛化 

**Authors**: Jing Yu, Yibo Zhao, Jiapeng Zhu, Wenming Shao, Bo Pang, Zhao Zhang, Xiang Li  

**Link**: [PDF](https://arxiv.org/pdf/2507.01050)  

**Abstract**: The widespread dissemination of toxic content on social media poses a serious threat to both online environments and public discourse, highlighting the urgent need for detoxification methods that effectively remove toxicity while preserving the original semantics. However, existing approaches often struggle to simultaneously achieve strong detoxification performance, semantic preservation, and robustness to out-of-distribution data. Moreover, they typically rely on costly, manually annotated parallel corpora while showing poor data efficiency. To address these challenges, we propose a two-stage training framework that jointly optimizes for data efficiency, semantic preservation, and model generalization. We first perform supervised fine-tuning on a small set of high-quality, filtered parallel data to establish a strong initialization. Then, we leverage unlabeled toxic inputs and a custom-designed reward model to train the LLM using Group Relative Policy Optimization. Experimental results demonstrate that our method effectively mitigates the trade-offs faced by previous work, achieving state-of-the-art performance with improved generalization and significantly reduced dependence on annotated data. Our code is available at: this https URL 

**Abstract (ZH)**: 社交媒体上广泛传播的有毒内容对在线环境和公众 discourse 威胁严重，突显出迫切需要既能有效去除有毒内容又能保留原始语义的净化方法。然而，现有方法往往难以同时实现强大的去毒性能、语义保留和对分布外数据的鲁棒性。此外，它们通常依赖于昂贵的手工标注平行语料库，而数据效率较低。为应对这些挑战，我们提出了一种两阶段训练框架，以联合优化数据效率、语义保留和模型泛化能力。首先，在高质量过滤后的平行数据小数据集上进行监督微调，以建立强大的初始化。然后，利用未标记的有毒输入和自定义设计的奖励模型，通过组相对策略优化训练大规模语言模型（LLM）。实验结果表明，我们的方法有效缓解了以往工作的权衡，实现了最先进的性能，并显著减少了对标注数据的依赖。源代码可从如下链接获取：this https URL。 

---
# Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals 

**Title (ZH)**: 跨场景和设备的 cardiac 健康感知识别：基于 170 万个体异质数据的多模态预训练基础模型 

**Authors**: Xiao Gu, Wei Tang, Jinpei Han, Veer Sangha, Fenglin Liu, Shreyank N Gowda, Antonio H. Ribeiro, Patrick Schwab, Kim Branson, Lei Clifton, Antonio Luiz P. Ribeiro, Zhangdaihong Liu, David A. Clifton  

**Link**: [PDF](https://arxiv.org/pdf/2507.01045)  

**Abstract**: Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms (PPG), are of paramount importance for the diagnosis, prevention, and management of cardiovascular diseases, and have been extensively used in a variety of clinical tasks. Conventional deep learning approaches for analyzing these signals typically rely on homogeneous datasets and static bespoke models, limiting their robustness and generalizability across diverse clinical settings and acquisition protocols. In this study, we present a cardiac sensing foundation model (CSFM) that leverages advanced transformer architectures and a generative, masked pretraining strategy to learn unified representations from vast, heterogeneous health records. Our model is pretrained on an innovative multi-modal integration of data from multiple large-scale datasets (including MIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the corresponding clinical or machine-generated text reports from approximately 1.7 million individuals. We demonstrate that the embeddings derived from our CSFM not only serve as effective feature extractors across diverse cardiac sensing scenarios, but also enable seamless transfer learning across varying input configurations and sensor modalities. Extensive evaluations across diagnostic tasks, demographic information recognition, vital sign measurement, clinical outcome prediction, and ECG question answering reveal that CSFM consistently outperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits robust performance across multiple ECG lead configurations from standard 12-lead systems to single-lead setups, and in scenarios where only ECG, only PPG, or a combination thereof is available. These findings highlight the potential of CSFM as a versatile and scalable solution, for comprehensive cardiac monitoring. 

**Abstract (ZH)**: 心脏生物信号（如心电图ECG和光体积描记图PPG）对于心血管疾病的诊断、预防和管理至关重要，并在多种临床任务中广泛使用。传统的深度学习方法在分析这些信号时通常依赖于同质数据集和定制模型，这限制了它们在不同临床环境和采集协议下的鲁棒性和泛化能力。在本研究中，我们提出了一种心脏感知基础模型（CSFM），该模型利用先进的变压器架构和生成性掩码预训练策略，从大量异质健康记录中学习统一表示。该模型在包括MIMIC-III-WDB、MIMIC-IV-ECG和CODE在内的多个大规模数据集的多模态集成数据上进行预训练，这些数据集包含了约170万名个体的心电图信号及其相应的临床或机器生成的文本报告。研究结果显示，CSFM提取的嵌入不仅在多种心脏感知场景中作为有效的特征提取器，还能在不同输入配置和传感器模态下实现无缝迁移学习。通过在诊断任务、人口统计信息识别、生理参数测量、临床结果预测和心电图问答等广泛评估中，CSFM表现出优于传统单模态单任务方法的性能。值得注意的是，CSFM在从标准12导联系统到单导联设置等多种心电图导联配置下表现稳健，并在只有心电图、只有光体积描记图或两者结合可用的场景下也表现出色。这些发现突显了CSFM作为多功能和可拓展解决方案的潜力，适用于全面的心脏监测。 

---
# Data Classification with Dynamically Growing and Shrinking Neural Networks 

**Title (ZH)**: 动态扩展与收缩的神经网络的数据分类 

**Authors**: Szymon Świderski, Agnieszka Jastrzębska  

**Link**: [PDF](https://arxiv.org/pdf/2507.01043)  

**Abstract**: The issue of data-driven neural network model construction is one of the core problems in the domain of Artificial Intelligence. A standard approach assumes a fixed architecture with trainable weights. A conceptually more advanced assumption is that we not only train the weights, but also find out the optimal model architecture. We present a new method that realizes just that. This article is an extended version of our conference paper titled "Dynamic Growing and Shrinking of Neural Networks with Monte Carlo Tree Search [26]". In the paper, we show in detail how to create a neural network with a procedure that allows dynamic shrinking and growing of the model while it is being trained. The decision-making mechanism for the architectural design is governed by a Monte Carlo tree search procedure which simulates network behavior and allows to compare several candidate architecture changes to choose the best one. The proposed method was validated using both visual and time series datasets, demonstrating its particular effectiveness in multivariate time series classification. This is attributed to the architecture's ability to adapt dynamically, allowing independent modifications for each time series. The approach is supplemented by Python source code for reproducibility. Experimental evaluations in visual pattern and multivariate time series classification tasks revealed highly promising performance, underscoring the method's robustness and adaptability. 

**Abstract (ZH)**: 数据驱动神经网络模型构建是人工智能领域的核心问题之一。一种标准的做法是假设固定架构并通过可训练的权重进行训练。一个更为先进的概念是不仅训练权重，还寻找最优模型架构。我们提出了一种新方法实现这一点。本文是我们在会议上发表的论文“使用蒙特卡洛树搜索的神经网络的动态生长和收缩[26]”的扩展版本。在论文中，我们详细介绍了如何通过允许模型在训练过程中动态收缩和生长的过程来创建神经网络。架构设计的决策机制由蒙特卡洛树搜索过程管理，该过程模拟网络行为并允许比较多个候选架构变化以选择最佳方案。提出的方 法通过视觉数据集和时间序列数据集验证，展示了其在多变量时间序列分类方面的特别有效性。这归因于架构能够动态适应，允许每个时间序列独立修改。此外，该方法附有Python源代码以提高可再现性。实验评估在视觉模式分类和多变量时间序列分类任务中表明了高度有希望的性能，突显了该方法的稳健性和适应性。 

---
# Can Argus Judge Them All? Comparing VLMs Across Domains 

**Title (ZH)**: Argus能够审判他们吗？跨领域比较VLMs 

**Authors**: Harsh Joshi, Gautam Siddharth Kashyap, Rafiq Ali, Ebad Shabbir, Niharika Jain, Sarthak Jain, Jiechao Gao, Usman Naseem  

**Link**: [PDF](https://arxiv.org/pdf/2507.01042)  

**Abstract**: Vision-Language Models (VLMs) are advancing multimodal AI, yet their performance consistency across tasks is underexamined. We benchmark CLIP, BLIP, and LXMERT across diverse datasets spanning retrieval, captioning, and reasoning. Our evaluation includes task accuracy, generation quality, efficiency, and a novel Cross-Dataset Consistency (CDC) metric. CLIP shows strongest generalization (CDC: 0.92), BLIP excels on curated data, and LXMERT leads in structured reasoning. These results expose trade-offs between generalization and specialization, informing industrial deployment of VLMs and guiding development toward robust, task-flexible architectures. 

**Abstract (ZH)**: Vision-Language模型（VLMs）在 multimodal AI 领域取得了进展，但其在不同任务中的性能一致性尚待深入研究。我们通过对CLIP、BLIP和LXMERT在检索、描述和推理等多样数据集上的基准测试，评估了任务准确性、生成质量、效率以及一个新的跨数据集一致性（CDC）指标。CLIP在泛化能力上表现最佳（CDC: 0.92），BLIP在精心策划的数据上表现优异，而LXMERT在结构化推理上处于领先地位。这些结果揭示了泛化能力和专业化的权衡，为VLMs的工业应用提供了指导，并推动了更 robust、更具任务灵活性的架构的发展。 

---
# Fast AI Model Splitting over Edge Networks 

**Title (ZH)**: 边缘网络中快速AI模型拆分 

**Authors**: Zuguang Li, Wen Wu, Shaohua Wu, Songge Zhang, Ye Wang, Xuemin, Shen  

**Link**: [PDF](https://arxiv.org/pdf/2507.01041)  

**Abstract**: Split learning (SL) has emerged as a computationally efficient approach for artificial intelligence (AI) model training, which can alleviate device-side computational workloads. However, complex AI model architectures pose high computational complexity to obtain the optimal model splitting. In this paper, we represent an arbitrary AI model as a directed acyclic graph (DAG), and then reformulate the optimal model splitting problem as a minimum s-t cut search problem. To solve the problem, we propose a fast DAG-based model splitting algorithm, which restructures the DAG to enable the optimal model splitting identification via a maximum flow method. Theoretical analysis indicates that the proposed algorithm is optimal. Furthermore, considering AI models with block structures, we propose a block-wise model splitting algorithm to reduce computational complexity. The algorithm abstracts each block, i.e., a component consisting of multiple layers, into a single vertex, thereby obtaining the optimal model splitting via a simplified DAG. Extensive experimental results demonstrate that the proposed algorithms can determine the optimal model splitting within milliseconds, as well as reduce training delay by 24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art benchmarks. 

**Abstract (ZH)**: Split学习（SL）已成为一种高效的人工智能（AI）模型训练方法，可以缓解设备端的计算负担。然而，复杂的AI模型架构导致了获得最优模型分割的高度计算复杂性。在本文中，我们将任意AI模型表示为有向无环图（DAG），并将最优模型分割问题重新表述为最小s-t割搜索问题。为了解决该问题，我们提出了一种基于DAG的快速模型分割算法，该算法重构DAG以通过最大流方法识别最优模型分割。理论分析表明，所提出的算法是最佳的。此外，考虑到具有块结构的AI模型，我们提出了一种块级模型分割算法来降低计算复杂性。该算法将每个块（即由多层组成的组件）抽象为一个顶点，从而通过简化DAG获得最优模型分割。广泛的实验结果表明，所提出的算法可以在毫秒内确定最优模型分割，并且与最先进的基准相比，在动态边缘网络中可以将训练延迟降低24.62%-38.95%。 

---
# Fast Clifford Neural Layers 

**Title (ZH)**: 快速克利福德神经网络层 

**Authors**: Tianxiang Xia, Max Neuwinger, Lin Xiao  

**Link**: [PDF](https://arxiv.org/pdf/2507.01040)  

**Abstract**: Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra into neural networks. In this project we focus on optimizing the inference of 2/3D Clifford convolutional layers and multivector activation layers for one core CPU performance.
Overall, by testing on a real network block involving Clifford convolutional layers and multivector activation layers, we observe that our implementation is 30% faster than standard PyTorch implementation in relatively large data + network size (>L2 cache).
We open source our code base at this https URL 

**Abstract (ZH)**: Clifford神经层通过将Clifford代数引入神经网络改进PDE建模 

---
# On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization 

**Title (ZH)**: 使用 aproximimal 策略优化的 ANFIS 策略在线优化 

**Authors**: Kaaustaaub Shankar, Wilhelm Louw, Kelly Cohen  

**Link**: [PDF](https://arxiv.org/pdf/2507.01039)  

**Abstract**: We propose a reinforcement learning (RL) approach for training neuro-fuzzy controllers using Proximal Policy Optimization (PPO). Building on prior work that applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS), our method replaces the off-policy value-based framework with a stable on-policy actor-critic loop. We evaluate this approach in the CartPole-v1 environment using multiple random seeds and compare its learning performance against ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained fuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000 updates, showcasing less variance than prior DQN-based methods during training and overall faster convergence. These findings suggest that PPO offers a promising pathway for training explainable neuro-fuzzy controllers in reinforcement learning tasks. 

**Abstract (ZH)**: 我们提出了一种使用 proximal policy optimization (PPO) 训练神经模糊控制器的 reinforcement learning (RL) 方法。该方法在先前将深度 Q 学习应用于自适应模糊神经推理系统 (ANFIS) 的工作基础上，用稳定的 on-policy actor-critic 循环替代了离策价值基础框架。我们在 CartPole-v1 环境中使用多个随机种子对该方法进行评估，并将其学习性能与基于 DQN 的 ANFIS 基线方法进行比较。研究发现，经过 PPO 训练的模糊代理在 20000 次更新后于 CartPole-v1 环境中实现了平均回报 500 ± 0，在训练过程中方差较小，并且整体收敛速度更快。这些发现表明，PPO 为在强化学习任务中训练可解释的神经模糊控制器提供了有前景的途径。 

---
# Learning to Segment for Vehicle Routing Problems 

**Title (ZH)**: 学习分割方法解决车辆路线问题 

**Authors**: Wenbin Ouyang, Sirui Li, Yining Ma, Cathy Wu  

**Link**: [PDF](https://arxiv.org/pdf/2507.01037)  

**Abstract**: Iterative search heuristics are widely recognized as state-of-the-art for solving Vehicle Routing Problems (VRPs). In this work, we identify and exploit a critical observation: within these solvers, a large portion of the solution remains stable, i.e., unchanged across search iterations, causing redundant computations, especially for large-scale VRPs with long subtours. To address this, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA) decomposition technique to accelerate iterative solvers. Specifically, FSTA preserves stable solution segments during the search, aggregates nodes within each segment into fixed hypernodes, and focuses the search only on unstable portions. Yet, a key challenge lies in identifying which segments should be aggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg), a novel neural framework to intelligently differentiate potentially stable and unstable portions for FSTA decomposition. We present three L2Seg variants: non-autoregressive (globally comprehensive but locally indiscriminate), autoregressive (locally refined but globally deficient), and their synergy, with bespoke training and inference strategies. Empirical results on CVRP and VRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up to 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy achieves best performance by combining their complementary strengths. Notably, L2Seg is a flexible framework that is compatible with traditional, learning-based, and hybrid solvers, while supporting a broad class of VRPs. 

**Abstract (ZH)**: 迭代搜索启发式方法被认为是解决车辆 routing 问题（VRPs）的前沿方法。在此工作中，我们识别并利用一个重要观察：在这些求解器中，大部分解决方案在搜索迭代过程中保持稳定，即在多次迭代中不发生变化，尤其是在大规模具有长子环的 VRPs 中导致冗余计算。为解决这一问题，我们开创性地研究了 First-Segment-Then-Aggregate (FSTA) 分解技术以加速迭代求解器。具体而言，FSTA 在搜索过程中保留稳定的解决方案段，将每个段内的节点聚合为固定超节点，并仅对不稳定的部分进行搜索。然而，在 FSTA 中聚合哪些段是关键挑战。为解决这一问题，我们随后引入了 Learning-to-Segment (L2Seg)，这是一种新颖的神经网络框架，用于智能地区分 FSTA 分解中潜在的稳定和不稳定部分。我们提出了三种 L2Seg 变体：非自回归（全局综合但局部不分辨）、自回归（局部细腻但全局不足）及其结合，配有专门的训练和推理策略。实验结果表明，L2Seg 可以将最先进的迭代求解器加速多达 7 倍。此外，我们提供了深入分析，表明非自回归和自回归结合利用其互补优势可实现最佳性能。值得注意的是，L2Seg 是一个灵活的框架，兼容传统、基于学习以及混合求解器，并支持广泛的 VRPs 类型。 

---
# Systemic Constraints of Undecidability 

**Title (ZH)**: 系统不可判定性约束 

**Authors**: Seth Bulin  

**Link**: [PDF](https://arxiv.org/pdf/2507.01036)  

**Abstract**: This paper presents a theory of systemic undecidability, reframing incomputability as a structural property of systems rather than a localized feature of specific functions or problems. We define a notion of causal embedding and prove a closure principle: any subsystem that participates functionally in the computation of an undecidable system inherits its undecidability. This result positions undecidability as a pervasive constraint on prediction, modeling, and epistemic access in both natural and artificial systems. Our framework disarms oracle mimicry and challenges the view that computational limits can be circumvented through architectural innovation. By generalizing classical results into a dynamic systems context, this work augments the logical trajectory of Gödel, Turing, and Chaitin, offering a new perspective of the topology of computability and its interrelation to the boundaries of scientific knowledge. 

**Abstract (ZH)**: 本文提出了系统不可判定性理论，将不可计算性重新定义为系统的一种结构属性，而非特定函数或问题的局部特征。我们定义了一种因果嵌入的概念，并证明了一个封闭原则：任何参与不可判定系统功能计算的子系统都会继承其不可判定性。这一结果将不可判定性定位为对自然和人工系统中预测、建模和知识获取的一种普遍约束。我们的框架消解了Oracle模拟，并挑战了通过体系结构创新绕过计算限制的看法。通过将经典结果推广到动力系统上下文，本文扩展了哥德尔、图灵和柴廷的逻辑轨迹，提供了计算拓扑及其与科学知识边界之间关系的新视角。 

---
# Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems 

**Title (ZH)**: 低延迟推理与训练效率优化在图神经网络和基于大型语言模型的推荐系统中 

**Authors**: Yushang Zhao, Haotian Lyu, Yike Peng, Aijia Sun, Feng Jiang, Xinyue Han  

**Link**: [PDF](https://arxiv.org/pdf/2507.01035)  

**Abstract**: The incessant advent of online services demands high speed and efficient recommender systems (ReS) that can maintain real-time performance along with processing very complex user-item interactions. The present study, therefore, considers computational bottlenecks involved in hybrid Graph Neural Network (GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their inference latency and training efficiency. An extensive methodology was used: hybrid GNN-LLM integrated architecture-optimization strategies(quantization, LoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2. Experimental improvements were significant, with the optimal Hybrid + FPGA + DeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms of latency, while LoRA brought down training time by 66% (3.8 hours) in comparison to the non-optimized baseline. Irrespective of domain, such as accuracy or efficiency, it can be established that hardware-software co-design and parameter-efficient tuning permit hybrid models to outperform GNN or LLM approaches implemented independently. It recommends the use of FPGA as well as LoRA for real-time deployment. Future work should involve federated learning along with advanced fusion architectures for better scalability and privacy preservation. Thus, this research marks the fundamental groundwork concerning next-generation ReS balancing low-latency response with cutting-edge personalization. 

**Abstract (ZH)**: 不间断涌现的在线服务要求具备高效率和快速响应的推荐系统（ReS），能够在保持实时性能的同时处理非常复杂的用户-项交互。本研究旨在优化基于混合图神经网络（GNN）和大型语言模型（LLM）的推荐系统中的计算瓶颈，以优化其推理延迟和训练效率。采用广泛的综合方法：混合GNN-LLM集成架构-优化策略（量化、LoRA、蒸馏）-硬件加速（FPGA、DeepSpeed），在R 4.4.2环境下进行。实验结果显著改进，优化后的混合+FPGA+DeepSpeed配置在40-60ms延迟下准确率达到13.6%的提升（NDCG@10: 0.75），而LoRA将训练时间降低了66%（3.8小时），相较非优化基线。无论在哪个领域，硬件-软件协同设计和参数高效调优都使得混合模型能够优于单独实现的GNN或LLM方法。研究推荐使用FPGA和LoRA进行实时部署。未来工作应包括联邦学习和更高级的融合架构，以实现更好的可扩展性和隐私保护。因此，这项研究奠定了下一代推荐系统平衡低延迟响应与先进个性化的基础。 

---
# Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya 

**Title (ZH)**: 基于数据驱动的洞察助力明智决策：在利比亚应用LSTM网络进行稳健的电力预测 

**Authors**: Asma Agaal, Mansour Essgaer, Hend M. Farkash, Zulaiha Ali Othman  

**Link**: [PDF](https://arxiv.org/pdf/2507.01034)  

**Abstract**: Accurate electricity forecasting is crucial for grid stability and energy planning, especially in Benghazi, Libya, where frequent load shedding, generation deficits, and infrastructure limitations persist. This study proposes a data-driven approach to forecast electricity load, generation, and deficits for 2025 using historical data from 2019 (a year marked by instability) and 2023 (a more stable year). Multiple time series models were applied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential smoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural networks. The dataset was enhanced through missing value imputation, outlier smoothing, and log transformation. Performance was assessed using mean squared error, root mean squared error, mean absolute error, and mean absolute percentage error. LSTM outperformed all other models, showing strong capabilities in modeling non-stationary and seasonal patterns. A key contribution of this work is an optimized LSTM framework that integrates exogenous factors such as temperature and humidity, offering robust performance in forecasting multiple electricity indicators. These results provide practical insights for policymakers and grid operators to enable proactive load management and resource planning in data-scarce, volatile regions. 

**Abstract (ZH)**: 准确的电力 forecasting 对比利电信号稳定和能源规划至关重要，尤其是在频繁的负荷削减、发电不足和基础设施限制持续存在的的班加西地区。本研究提出了一种基于数据的方法，使用2019年（不稳定的年份）和2023年（较为稳定的年份）的历史数据，预测2025年的电力负荷、发电量和赤字。应用了多种时间序列模型，包括ARIMA、季节性ARIMA、动态回归ARIMA、指数平滑、极端梯度提升和长短期记忆（LSTM）神经网络。通过插补缺失值、平滑异常值和对数转换提升了数据集的质量。性能评估使用均方误差、均方根误差、平均绝对误差和平均绝对百分比误差进行。LSTM 在所有模型中表现最佳，展示了在建模非平稳性和季节性模式方面的强大能力。本研究的一个重要贡献是集成了如温度和湿度等外生因素的优化LSTM框架，为多电量指标预测提供了稳健的表现。这些结果为政策制定者和电网运营商提供了在数据稀缺、波动性高的地区进行积极负荷管理和资源规划的实际见解。 

---
# An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks 

**Title (ZH)**: 面向分类任务的渐进多组学集成动态决策框架，考虑不确定性 

**Authors**: Nan Mu, Hongbo Yang, Chen Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2507.01032)  

**Abstract**: Background and Objective: High-throughput multi-omics technologies have proven invaluable for elucidating disease mechanisms and enabling early diagnosis. However, the high cost of multi-omics profiling imposes a significant economic burden, with over reliance on full omics data potentially leading to unnecessary resource consumption. To address these issues, we propose an uncertainty-aware, multi-view dynamic decision framework for omics data classification that aims to achieve high diagnostic accuracy while minimizing testing costs. Methodology: At the single-omics level, we refine the activation functions of neural networks to generate Dirichlet distribution parameters, utilizing subjective logic to quantify both the belief masses and uncertainty mass of classification results. Belief mass reflects the support of a specific omics modality for a disease class, while the uncertainty parameter captures limitations in data quality and model discriminability, providing a more trustworthy basis for decision-making. At the multi omics level, we employ a fusion strategy based on Dempster-Shafer theory to integrate heterogeneous modalities, leveraging their complementarity to boost diagnostic accuracy and robustness. A dynamic decision mechanism is then applied that omics data are incrementally introduced for each patient until either all data sources are utilized or the model confidence exceeds a predefined threshold, potentially before all data sources are utilized. Results and Conclusion: We evaluate our approach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN. In three datasets, over 50% of cases achieved accurate classification using a single omics modality, effectively reducing redundant testing. Meanwhile, our method maintains diagnostic performance comparable to full-omics models and preserves essential biological insights. 

**Abstract (ZH)**: 背景与目的：高通量多组学技术已在阐明疾病机制和实现早期诊断方面证明了其 invaluable的价值。然而，多组学表型的高成本对经济造成了显著负担，并可能导致过度依赖全组学数据进而造成不必要的资源消耗。为解决这些问题，我们提出了一种不确定性意识下的多视图动态决策框架，旨在通过最小化测试成本来实现高诊断准确性。方法：在单组学层面，我们改进了神经网络的激活函数以生成狄利克雷分布参数，并利用主观逻辑量化解分类结果的信念质量和不确定性质量。信念质量反映了特定组学模态对疾病类别的支持程度，而不确定性参数则捕捉数据质量和模型可分辨性的限制，为决策提供更可靠的基础。在多组学层面，我们基于Dempster-Shafer理论采用了融合策略以整合异质模态，并利用其互补性以提升诊断准确性和稳健性。然后应用动态决策机制，逐步为每位患者引入组学数据，直到所有数据源都被利用或模型置信度超过预定义阈值，从而可能在利用所有数据源之前停止。结果与结论：我们使用ROSMAP、LGG、BRCA和KIPAN四个基准多组学数据集评估了我们的方法。在三个数据集中，超过50%的病例仅通过单一组学模态即可实现准确分类，有效减少了冗余测试。同时，我们的方法保持了与全组学模型相当的诊断性能，并保留了关键的生物学洞察。 

---
# PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning 

**Title (ZH)**: PathCoT: 以思维链方式进行零样本病理视觉推理的提示方法 

**Authors**: Junjie Zhou, Yingli Zuo, Shichang Feng, Peng Wan, Qi Zhu, Daoqiang Zhang, Wei Shao  

**Link**: [PDF](https://arxiv.org/pdf/2507.01029)  

**Abstract**: With the development of generative artificial intelligence and instruction tuning techniques, multimodal large language models (MLLMs) have made impressive progress on general reasoning tasks. Benefiting from the chain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning problem step-by-step. However, existing MLLMs still face significant challenges when applied to pathology visual reasoning tasks: (1) LLMs often underperforms because they lack domain-specific information, which can lead to model hallucinations. (2) The additional reasoning steps in CoT may introduce errors, leading to the divergence of answers. To address these limitations, we propose PathCoT, a novel zero-shot CoT prompting method which integrates the pathology expert-knowledge into the reasoning process of MLLMs and incorporates self-evaluation to mitigate divergence of answers. Specifically, PathCoT guides the MLLM with prior knowledge to perform as pathology experts, and provides comprehensive analysis of the image with their domain-specific knowledge. By incorporating the experts' knowledge, PathCoT can obtain the answers with CoT reasoning. Furthermore, PathCoT incorporates a self-evaluation step that assesses both the results generated directly by MLLMs and those derived through CoT, finally determining the reliable answer. The experimental results on the PathMMU dataset demonstrate the effectiveness of our method on pathology visual understanding and reasoning. 

**Abstract (ZH)**: 基于病理专家知识的零样本链式思考路径推理方法 

---
# HPC-AI Coupling Methodology for Scientific Applications 

**Title (ZH)**: HPC-AI联合方法学及其在科学应用中的应用 

**Authors**: Yutong Lu, Dan Huang, Pin Chen  

**Link**: [PDF](https://arxiv.org/pdf/2507.01025)  

**Abstract**: Artificial intelligence (AI) technologies have fundamentally transformed numerical-based high-performance computing (HPC) applications with data-driven approaches and endeavored to address existing challenges, e.g. high computational intensity, in various scientific domains. In this study, we explore the scenarios of coupling HPC and AI (HPC-AI) in the context of emerging scientific applications, presenting a novel methodology that incorporates three patterns of coupling: surrogate, directive, and coordinate. Each pattern exemplifies a distinct coupling strategy, AI-driven prerequisite, and typical HPC-AI ensembles. Through case studies in materials science, we demonstrate the application and effectiveness of these patterns. The study highlights technical challenges, performance improvements, and implementation details, providing insight into promising perspectives of HPC-AI coupling. The proposed coupling patterns are applicable not only to materials science but also to other scientific domains, offering valuable guidance for future HPC-AI ensembles in scientific discovery. 

**Abstract (ZH)**: 人工智能（AI）技术以数据驱动的方法从根本上改变了基于数值的高性能计算（HPC）应用，并致力于解决现有挑战，例如在各个科学领域中的高计算强度。在本研究中，我们在新兴科学应用的背景下探索高性能计算和人工智能（HPC-AI）的结合场景，提出了一种包含三种耦合模式的新方法：替代模式、指导模式和协调模式。每种模式代表了一种独特的耦合策略、基于AI的前提条件以及典型的HPC-AI结合体。通过材料科学案例研究，我们展示了这些模式的应用和效果。本研究突出了技术挑战、性能改进和实施细节，为HPC-AI耦合提供了有益的视角。提出的耦合模式不仅适用于材料科学，也适用于其他科学领域，为未来的HPC-AI结合体在科学研究中的应用提供了有价值的教学建议。 

---
# Hello Afrika: Speech Commands in Kinyarwanda 

**Title (ZH)**: Hello Afrika: 埔隆沃纳达语语音命令 

**Authors**: George Igwegbe, Martins Awojide, Mboh Bless, Nirel Kadzo  

**Link**: [PDF](https://arxiv.org/pdf/2507.01024)  

**Abstract**: Voice or Speech Commands are a subset of the broader Spoken Word Corpus of a language which are essential for non-contact control of and activation of larger AI systems in devices used in everyday life especially for persons with disabilities. Currently, there is a dearth of speech command models for African languages. The Hello Afrika project aims to address this issue and its first iteration is focused on the Kinyarwanda language since the country has shown interest in developing speech recognition technologies culminating in one of the largest datasets on Mozilla Common Voice. The model was built off a custom speech command corpus made up of general directives, numbers, and a wake word. The final model was deployed on multiple devices (PC, Mobile Phone and Edge Devices) and the performance was assessed using suitable metrics. 

**Abstract (ZH)**: 语音命令是语言语音语料库的一个子集，对于日常生活中设备的非接触控制和激活大型AI系统至关重要，尤其是对于残疾人士。目前，非洲语言的语音命令模型缺乏。Hello Afrika项目旨在解决这一问题，其首个迭代专注于基杭语语言，因为该国对开发语音识别技术表现出兴趣，并产生了Mozilla Common Voice最大的数据集之一。模型基于一个包含通用指令、数字和唤醒词的定制语音命令语料库进行构建。最终模型部署在多个设备（台式机、移动电话和边缘设备）上，并使用合适的指标进行性能评估。 

---
# A Systematic Review of Security Vulnerabilities in Smart Home Devices and Mitigation Techniques 

**Title (ZH)**: 智能家庭设备中的安全漏洞及其缓解技术系统综述 

**Authors**: Mohammed K. Alzaylaee  

**Link**: [PDF](https://arxiv.org/pdf/2507.01018)  

**Abstract**: Smart homes that integrate Internet of Things (IoT) devices face increasing cybersecurity risks, posing significant challenges to these environments. The study explores security threats in smart homes ecosystems, categorizing them into vulnerabilities at the network layer, device level, and those from cloud-based and AI-driven systems. Research findings indicate that post-quantum encryption, coupled with AI-driven anomaly detection, is highly effective in enhancing security; however, computational resource demands present significant challenges. Blockchain authentication together with zero-trust structures builds security resilience, although they need changes to existing infrastructure. The specific security strategies show their effectiveness through ANOVA, Chi-square tests, and Monte Carlo simulations yet lack sufficient scalability according to the results. The research demonstrates the requirement for improvement in cryptographic techniques, alongside AI-enhanced threat detection and adaptive security models which must achieve a balance between performance and efficiency and real-time applicability within smart home ecosystems. 

**Abstract (ZH)**: 智能家庭集成物联网设备面临不断增加的网络安全风险，给这些环境带来重大挑战。该研究探讨了智能家庭生态系统中的安全威胁，将其分类为网络层漏洞、设备级别漏洞以及基于云和AI驱动系统的漏洞。研究发现，结合后量子加密与AI驱动的异常检测在增强安全方面非常有效；然而，计算资源需求也提出了重大挑战。区块链认证与零信任结构构建了安全韧性，但需要对现有基础设施进行更改。具体的安全策略通过ANOVA、卡方检验和蒙特卡洛模拟显示了其有效性，但在可扩展性方面结果表明尚有不足。研究展示了在智能家庭生态系统中改进加密技术、增强AI威胁检测和适应性安全模型的需求，这些模型必须在性能、效率和实时适用性之间找到平衡。 

---
