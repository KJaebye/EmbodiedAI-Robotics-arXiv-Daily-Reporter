{'arxiv_id': 'arXiv:2503.02881', 'title': 'Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation', 'authors': 'Han Xue, Jieji Ren, Wendi Chen, Gu Zhang, Yuan Fang, Guoying Gu, Huazhe Xu, Cewu Lu', 'link': 'https://arxiv.org/abs/2503.02881', 'abstract': 'Humans can accomplish complex contact-rich tasks using vision and touch, with highly reactive capabilities such as quick adjustments to environmental changes and adaptive control of contact forces; however, this remains challenging for robots. Existing visual imitation learning (IL) approaches rely on action chunking to model complex behaviors, which lacks the ability to respond instantly to real-time tactile feedback during the chunk execution. Furthermore, most teleoperation systems struggle to provide fine-grained tactile / force feedback, which limits the range of tasks that can be performed. To address these challenges, we introduce TactAR, a low-cost teleoperation system that provides real-time tactile feedback through Augmented Reality (AR), along with Reactive Diffusion Policy (RDP), a novel slow-fast visual-tactile imitation learning algorithm for learning contact-rich manipulation skills. RDP employs a two-level hierarchy: (1) a slow latent diffusion policy for predicting high-level action chunks in latent space at low frequency, (2) a fast asymmetric tokenizer for closed-loop tactile feedback control at high frequency. This design enables both complex trajectory modeling and quick reactive behavior within a unified framework. Through extensive evaluation across three challenging contact-rich tasks, RDP significantly improves performance compared to state-of-the-art visual IL baselines through rapid response to tactile / force feedback. Furthermore, experiments show that RDP is applicable across different tactile / force sensors. Code and videos are available on this https URL.', 'abstract_zh': '基于增强现实的低成本触觉反馈远程操作系统及其反应扩散策略算法：接触丰富操作技能的视触觉模拟学习', 'title_zh': '反应扩散策略：接触丰富操作中的慢-快视觉-触觉政策学习'}
{'arxiv_id': 'arXiv:2503.02834', 'title': 'MuBlE: MuJoCo and Blender simulation Environment and Benchmark for Task Planning in Robot Manipulation', 'authors': 'Michal Nazarczuk, Karla Stepanova, Jan Kristof Behrens, Matej Hoffmann, Krystian Mikolajczyk', 'link': 'https://arxiv.org/abs/2503.02834', 'abstract': "Current embodied reasoning agents struggle to plan for long-horizon tasks that require to physically interact with the world to obtain the necessary information (e.g. 'sort the objects from lightest to heaviest'). The improvement of the capabilities of such an agent is highly dependent on the availability of relevant training environments. In order to facilitate the development of such systems, we introduce a novel simulation environment (built on top of robosuite) that makes use of the MuJoCo physics engine and high-quality renderer Blender to provide realistic visual observations that are also accurate to the physical state of the scene. It is the first simulator focusing on long-horizon robot manipulation tasks preserving accurate physics modeling. MuBlE can generate mutlimodal data for training and enable design of closed-loop methods through environment interaction on two levels: visual - action loop, and control - physics loop. Together with the simulator, we propose SHOP-VRB2, a new benchmark composed of 10 classes of multi-step reasoning scenarios that require simultaneous visual and physical measurements.", 'abstract_zh': '当前的具身推理代理在计划需要物理互动以获得必要信息的长期任务时存在局限（例如，“按轻重排序物体”）。这类代理能力的提升高度依赖于相关训练环境的可用性。为了促进这类系统的开发，我们引入了一个新的仿真环境（基于robosuite构建），该环境利用MuJoCo物理引擎和高质量渲染器Blender提供逼真的视觉观察，准确反映场景的物理状态。MuBlE是第一个专注于保留精确物理建模的长期机器人操作任务的仿真器。MuBlE可以生成多模态数据用于训练，并通过视觉-行动环路和控制-物理环路的环境互动，支持闭环方法的设计。结合该仿真器，我们提出了一个新的基准SHOP-VRB2，包含10类需要同时进行视觉和物理测量的多步推理场景。', 'title_zh': 'MuBlE: MuJoCo 和 Blender 组合的仿真环境及机器人 manipulation 任务规划基准'}
{'arxiv_id': 'arXiv:2503.02774', 'title': 'Digital Model-Driven Genetic Algorithm for Optimizing Layout and Task Allocation in Human-Robot Collaborative Assemblies', 'authors': 'Christian Cella, Matteo Bruce Robin, Marco Faroni, Andrea Maria Zanchettin, Paolo Rocco', 'link': 'https://arxiv.org/abs/2503.02774', 'abstract': 'This paper addresses the optimization of human-robot collaborative work-cells before their physical deployment. Most of the times, such environments are designed based on the experience of the system integrators, often leading to sub-optimal solutions. Accurate simulators of the robotic cell, accounting for the presence of the human as well, are available today and can be used in the pre-deployment. We propose an iterative optimization scheme where a digital model of the work-cell is updated based on a genetic algorithm. The methodology focuses on the layout optimization and task allocation, encoding both the problems simultaneously in the design variables handled by the genetic algorithm, while the task scheduling problem depends on the result of the upper-level one. The final solution balances conflicting objectives in the fitness function and is validated to show the impact of the objectives with respect to a baseline, which represents possible initial choices selected based on the human judgment.', 'abstract_zh': '本文在物理部署之前优化了人机协作工作单元。现有的精确仿真器可以考虑人类的存在，用于事先部署。我们提出了一种迭代优化方案，其中基于遗传算法不断更新工作单元的数字模型。该方法侧重于布局优化和任务分配，同时将两者编码为遗传算法处理的设计变量，而任务调度问题依赖于更高层次的优化结果。最终解决方案在适应性函数中平衡了冲突的目标，并通过与基于人工判断的初始选择进行基线比较，验证了目标的影响。', 'title_zh': '基于数字模型驱动的遗传算法在人机协作装配中优化布局与任务分配'}
{'arxiv_id': 'arXiv:2503.02752', 'title': 'Deep Learning-Enhanced Visual Monitoring in Hazardous Underwater Environments with a Swarm of Micro-Robots', 'authors': 'Shuang Chen, Yifeng He, Barry Lennox, Farshad Arvin, Amir Atapour-Abarghouei', 'link': 'https://arxiv.org/abs/2503.02752', 'abstract': "Long-term monitoring and exploration of extreme environments, such as underwater storage facilities, is costly, labor-intensive, and hazardous. Automating this process with low-cost, collaborative robots can greatly improve efficiency. These robots capture images from different positions, which must be processed simultaneously to create a spatio-temporal model of the facility. In this paper, we propose a novel approach that integrates data simulation, a multi-modal deep learning network for coordinate prediction, and image reassembly to address the challenges posed by environmental disturbances causing drift and rotation in the robots' positions and orientations. Our approach enhances the precision of alignment in noisy environments by integrating visual information from snapshots, global positional context from masks, and noisy coordinates. We validate our method through extensive experiments using synthetic data that simulate real-world robotic operations in underwater settings. The results demonstrate very high coordinate prediction accuracy and plausible image assembly, indicating the real-world applicability of our approach. The assembled images provide clear and coherent views of the underwater environment for effective monitoring and inspection, showcasing the potential for broader use in extreme settings, further contributing to improved safety, efficiency, and cost reduction in hazardous field monitoring. Code is available on this https URL.", 'abstract_zh': '长期监测和探索极端环境（如水下存储设施）的成本高、劳动密集且危险。通过使用低成本协作机器人自动化这一过程可以大大提高效率。这些机器人从不同位置捕捉图像，必须同时处理以构建设施的时空模型。在本文中，我们提出了一种新颖的方法，该方法整合了数据模拟、多模态深度学习网络进行坐标预测以及图像重组，以应对环境扰动导致的机器人位置和方向漂移和旋转带来的挑战。我们的方法通过整合快照中的视觉信息、来自掩码的全局位置上下文以及嘈杂的坐标，提高了在嘈杂环境中的对齐精度。我们通过使用模拟水下环境中真实机器人操作的合成数据进行广泛实验，验证了我们的方法。实验结果表明，坐标预测精度非常高，并且图像重组合理，表明我们的方法在实际应用中的可行性。重组后的图像提供了清晰连贯的水下环境视图，有助于有效的监测和检查，展示了在极端环境中有更广泛使用潜力，进一步提高了危险领域监测的安全性、效率和成本效益。代码可在以下链接获取。', 'title_zh': '基于微机器人群体的深学习增强海底危化环境视觉监测'}
{'arxiv_id': 'arXiv:2503.02723', 'title': 'ImpedanceGPT: VLM-driven Impedance Control of Swarm of Mini-drones for Intelligent Navigation in Dynamic Environment', 'authors': 'Faryal Batool, Malaika Zafar, Yasheerah Yaqoot, Roohan Ahmed Khan, Muhammad Haris Khan, Aleksey Fedoseev, Dzmitry Tsetserukou', 'link': 'https://arxiv.org/abs/2503.02723', 'abstract': 'Swarm robotics plays a crucial role in enabling autonomous operations in dynamic and unpredictable environments. However, a major challenge remains ensuring safe and efficient navigation in environments filled with both dynamic alive (e.g., humans) and dynamic inanimate (e.g., non-living objects) obstacles. In this paper, we propose ImpedanceGPT, a novel system that combines a Vision-Language Model (VLM) with retrieval-augmented generation (RAG) to enable real-time reasoning for adaptive navigation of mini-drone swarms in complex environments.\nThe key innovation of ImpedanceGPT lies in the integration of VLM and RAG, which provides the drones with enhanced semantic understanding of their surroundings. This enables the system to dynamically adjust impedance control parameters in response to obstacle types and environmental conditions. Our approach not only ensures safe and precise navigation but also improves coordination between drones in the swarm.\nExperimental evaluations demonstrate the effectiveness of the system. The VLM-RAG framework achieved an obstacle detection and retrieval accuracy of 80 % under optimal lighting. In static environments, drones navigated dynamic inanimate obstacles at 1.4 m/s but slowed to 0.7 m/s with increased separation around humans. In dynamic environments, speed adjusted to 1.0 m/s near hard obstacles, while reducing to 0.6 m/s with higher deflection to safely avoid moving humans.', 'abstract_zh': '集群机器人在动态和不可预测环境下实现自主操作中发挥着关键作用。然而，确保在充满动态有生命（例如，人类）和动态无生命（例如，非生活的物体）障碍物的环境中进行安全和高效的导航仍是一项重大挑战。本文提出了一种新颖的系统ImpedanceGPT，该系统将视觉-语言模型（VLM）与检索增强生成（RAG）结合起来，以实现对复杂环境中微型无人机集群适应性导航的实时推理。ImpedanceGPT的关键创新在于VLM和RAG的集成，这为无人机提供了对其周围环境增强的语义理解，使系统能够根据障碍物类型和环境条件动态调整阻抗控制参数。本方法不仅确保了安全和精确的导航，还改善了集群中无人机之间的协调。实验评估证明了该系统的有效性。在最佳光照条件下，VLM-RAG框架实现了80%的障碍物检测和检索准确性。在静态环境中，无人机以1.4 m/s的速度导航动态无生命障碍物，而在人类周围增加间距时减速至0.7 m/s。在动态环境中，当接近坚硬障碍物时速度调整为1.0 m/s，而在高偏转以安全避开移动人类时减速至0.6 m/s。', 'title_zh': '阻抗GPT：基于VLM的微型无人机群阻抗控制以实现智能动态环境下的导航'}
{'arxiv_id': 'arXiv:2503.02700', 'title': 'Multi-Strategy Enhanced COA for Path Planning in Autonomous Navigation', 'authors': 'Yifei Wang, Jacky Keung, Haohan Xu, Yuchen Cao, Zhenyu Mao', 'link': 'https://arxiv.org/abs/2503.02700', 'abstract': "Autonomous navigation is reshaping various domains in people's life by enabling efficient and safe movement in complex environments. Reliable navigation requires algorithmic approaches that compute optimal or near-optimal trajectories while satisfying task-specific constraints and ensuring obstacle avoidance. However, existing methods struggle with slow convergence and suboptimal solutions, particularly in complex environments, limiting their real-world applicability. To address these limitations, this paper presents the Multi-Strategy Enhanced Crayfish Optimization Algorithm (MCOA), a novel approach integrating three key strategies: 1) Refractive Opposition Learning, enhancing population diversity and global exploration, 2) Stochastic Centroid-Guided Exploration, balancing global and local search to prevent premature convergence, and 3) Adaptive Competition-Based Selection, dynamically adjusting selection pressure for faster convergence and improved solution quality. Empirical evaluations underscore the remarkable planning speed and the amazing solution quality of MCOA in both 3D Unmanned Aerial Vehicle (UAV) and 2D mobile robot path planning. Against 11 baseline algorithms, MCOA achieved a 69.2% reduction in computational time and a 16.7% improvement in minimizing overall path cost in 3D UAV scenarios. Furthermore, in 2D path planning, MCOA outperformed baseline approaches by 44% on average, with an impressive 75.6% advantage in the largest 60*60 grid setting. These findings validate MCOA as a powerful tool for optimizing autonomous navigation in complex environments. The source code is available at: this https URL.", 'abstract_zh': '自主导航正重新塑造人们生活的各个领域，通过在复杂环境中实现高效和安全的移动。可靠的导航需要算法方法来计算最优或近优轨迹，同时满足特定任务约束并确保避障。然而，现有方法在复杂环境中难以实现快速收敛和最优解，限制了其在实际中的应用。为此，本文提出了一种新型的多策略增强蟹虾优化算法（MCOA），该算法整合了三种关键策略：1）折射反对学习，增强种群多样性和全局探索；2）随机质心引导探索，平衡全局和局部搜索以防止过早收敛；3）自适应竞争选择，动态调整选择压力以实现更快的收敛和更好的解质量。实证评估表明，MCOA 在三维无人驾驶航空器（UAV）和二维移动机器人路径规划中的规划速度和解质量都表现出色。与11种基准算法相比，MCOA 在三维UAV场景中的计算时间减少了69.2%，在整体路径成本最小化方面提高了16.7%。此外，在二维路径规划中，MCOA 平均优于基准方法44%，在60×60的最大网格设置中则表现出75.6%的优势。这些发现验证了MCOA 是优化复杂环境中自主导航的有效工具。源代码可在以下链接获取：this https URL。', 'title_zh': '多策略增强COA自主导航路径规划'}
{'arxiv_id': 'arXiv:2503.02698', 'title': 'FlowPlan: Zero-Shot Task Planning with LLM Flow Engineering for Robotic Instruction Following', 'authors': 'Zijun Lin, Chao Tang, Hanjing Ye, Hong Zhang', 'link': 'https://arxiv.org/abs/2503.02698', 'abstract': 'Robotic instruction following tasks require seamless integration of visual perception, task planning, target localization, and motion execution. However, existing task planning methods for instruction following are either data-driven or underperform in zero-shot scenarios due to difficulties in grounding lengthy instructions into actionable plans under operational constraints. To address this, we propose FlowPlan, a structured multi-stage LLM workflow that elevates zero-shot pipeline and bridges the performance gap between zero-shot and data-driven in-context learning methods. By decomposing the planning process into modular stages--task information retrieval, language-level reasoning, symbolic-level planning, and logical evaluation--FlowPlan generates logically coherent action sequences while adhering to operational constraints and further extracts contextual guidance for precise instance-level target localization. Benchmarked on the ALFRED and validated in real-world applications, our method achieves competitive performance relative to data-driven in-context learning methods and demonstrates adaptability across diverse environments. This work advances zero-shot task planning in robotic systems without reliance on labeled data. Project website: this https URL.', 'abstract_zh': '机器人指令跟随任务需要无缝集成视觉感知、任务规划、目标定位和动作执行。然而，现有的指令跟随任务规划方法要么是数据驱动的，要么在零样本场景下表现不佳，因为在操作约束条件下将 lengthy 的指令转化为可行的计划存在困难。为解决这一问题，我们提出了 FlowPlan，这是一种结构化的多阶段LLM工作流，提升了零样本管道并弥合了零样本和数据驱动的在上下文学习方法之间的性能差距。通过将规划过程分解为模块化阶段——任务信息检索、语言层面推理、符号层面规划和逻辑评估——FlowPlan 生成合乎逻辑的动作序列并遵循操作约束，进一步提取上下文指导以实现精确的实例级目标定位。在 ALFRED 上进行基准测试并通过实际应用验证，我们的方法在与数据驱动的在上下文学习方法相当的同时展示了跨不同环境的适应性。本工作无需标记数据便推进了机器人系统中的零样本任务规划。项目网站: https://this-url.', 'title_zh': 'FlowPlan: 基于LLM流程工程的零样本任务规划与机器人指令跟随'}
{'arxiv_id': 'arXiv:2503.02624', 'title': 'Human-aligned Safe Reinforcement Learning for Highway On-Ramp Merging in Dense Traffic', 'authors': 'Yang Li, Shijie Yuan, Yuan Chang, Xiaolong Chen, Qisong Yang, Zhiyuan Yang, Hongmao Qin', 'link': 'https://arxiv.org/abs/2503.02624', 'abstract': "Most reinforcement learning (RL) approaches for the decision-making of autonomous driving consider safety as a reward instead of a cost, which makes it hard to balance the tradeoff between safety and other objectives. Human risk preference has also rarely been incorporated, and the trained policy might be either conservative or aggressive for users. To this end, this study proposes a human-aligned safe RL approach for autonomous merging, in which the high-level decision problem is formulated as a constrained Markov decision process (CMDP) that incorporates users' risk preference into the safety constraints, followed by a model predictive control (MPC)-based low-level control. The safety level of RL policy can be adjusted by computing cost limits of CMDP's constraints based on risk preferences and traffic density using a fuzzy control method. To filter out unsafe or invalid actions, we design an action shielding mechanism that pre-executes RL actions using an MPC method and performs collision checks with surrounding agents. We also provide theoretical proof to validate the effectiveness of the shielding mechanism in enhancing RL's safety and sample efficiency. Simulation experiments in multiple levels of traffic densities show that our method can significantly reduce safety violations without sacrificing traffic efficiency. Furthermore, due to the use of risk preference-aware constraints in CMDP and action shielding, we can not only adjust the safety level of the final policy but also reduce safety violations during the training stage, proving a promising solution for online learning in real-world environments.", 'abstract_zh': '一种考虑用户风险偏好的自主变道安全强化学习方法', 'title_zh': '面向人类安全的高速公路入口汇入的安全强化学习'}
{'arxiv_id': 'arXiv:2503.02587', 'title': 'Learning Dexterous In-Hand Manipulation with Multifingered Hands via Visuomotor Diffusion', 'authors': 'Piotr Koczy, Michael C. Welle, Danica Kragic', 'link': 'https://arxiv.org/abs/2503.02587', 'abstract': 'We present a framework for learning dexterous in-hand manipulation with multifingered hands using visuomotor diffusion policies. Our system enables complex in-hand manipulation tasks, such as unscrewing a bottle lid with one hand, by leveraging a fast and responsive teleoperation setup for the four-fingered Allegro Hand. We collect high-quality expert demonstrations using an augmented reality (AR) interface that tracks hand movements and applies inverse kinematics and motion retargeting for precise control. The AR headset provides real-time visualization, while gesture controls streamline teleoperation. To enhance policy learning, we introduce a novel demonstration outlier removal approach based on HDBSCAN clustering and the Global-Local Outlier Score from Hierarchies (GLOSH) algorithm, effectively filtering out low-quality demonstrations that could degrade performance. We evaluate our approach extensively in real-world settings and provide all experimental videos on the project website: this https URL', 'abstract_zh': '我们提出了一种使用多指手进行基于视觉运动扩散策略的灵巧手内操作学习框架。该系统通过利用四指Allegro手的快速响应远程操作设置，实现了复杂的手内操作任务，例如单手旋开瓶盖。我们使用增强现实（AR）接口收集高质量的专家演示，该接口可以追踪手部运动并应用逆动力学和运动移植以实现精确控制。AR头显提供实时可视化，而手势控制简化了远程操作。为了提高策略学习，我们引入了一种基于HDBSCAN聚类和层次全局-局部异常评分（GLOSH）算法的新颖的演示异常值移除方法，有效地过滤出可能影响性能的低质量演示。我们在实际应用场景中进行了广泛评估，并在项目网站上提供了所有实验视频：this https URL。', 'title_zh': '基于多指手的视觉运动扩散学习手内灵巧操作'}
{'arxiv_id': 'arXiv:2503.02572', 'title': 'RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour', 'authors': 'Valerii Serpiva, Artem Lykov, Artyom Myshlyaev, Muhammad Haris Khan, Ali Alridha Abdulkarim, Oleg Sautenkov, Dzmitry Tsetserukou', 'link': 'https://arxiv.org/abs/2503.02572', 'abstract': "RaceVLA presents an innovative approach for autonomous racing drone navigation by leveraging Visual-Language-Action (VLA) to emulate human-like behavior. This research explores the integration of advanced algorithms that enable drones to adapt their navigation strategies based on real-time environmental feedback, mimicking the decision-making processes of human pilots. The model, fine-tuned on a collected racing drone dataset, demonstrates strong generalization despite the complexity of drone racing environments. RaceVLA outperforms OpenVLA in motion (75.0 vs 60.0) and semantic generalization (45.5 vs 36.3), benefiting from the dynamic camera and simplified motion tasks. However, visual (79.6 vs 87.0) and physical (50.0 vs 76.7) generalization were slightly reduced due to the challenges of maneuvering in dynamic environments with varying object sizes. RaceVLA also outperforms RT-2 across all axes - visual (79.6 vs 52.0), motion (75.0 vs 55.0), physical (50.0 vs 26.7), and semantic (45.5 vs 38.8), demonstrating its robustness for real-time adjustments in complex environments. Experiments revealed an average velocity of 1.04 m/s, with a maximum speed of 2.02 m/s, and consistent maneuverability, demonstrating RaceVLA's ability to handle high-speed scenarios effectively. These findings highlight the potential of RaceVLA for high-performance navigation in competitive racing contexts. The RaceVLA codebase, pretrained weights, and dataset are available at this http URL: this https URL", 'abstract_zh': 'RaceVLA：一种利用视觉-语言-行动（VLA）实现类人类行为的自主赛车无人机导航创新方法', 'title_zh': '基于VLA的人类行为类比赛无人机导航'}
{'arxiv_id': 'arXiv:2503.02552', 'title': 'World Models for Anomaly Detection during Model-Based Reinforcement Learning Inference', 'authors': 'Fabian Domberg, Georg Schildbach', 'link': 'https://arxiv.org/abs/2503.02552', 'abstract': "Learning-based controllers are often purposefully kept out of real-world applications due to concerns about their safety and reliability. We explore how state-of-the-art world models in Model-Based Reinforcement Learning can be utilized beyond the training phase to ensure a deployed policy only operates within regions of the state-space it is sufficiently familiar with. This is achieved by continuously monitoring discrepancies between a world model's predictions and observed system behavior during inference. It allows for triggering appropriate measures, such as an emergency stop, once an error threshold is surpassed. This does not require any task-specific knowledge and is thus universally applicable. Simulated experiments on established robot control tasks show the effectiveness of this method, recognizing changes in local robot geometry and global gravitational magnitude. Real-world experiments using an agile quadcopter further demonstrate the benefits of this approach by detecting unexpected forces acting on the vehicle. These results indicate how even in new and adverse conditions, safe and reliable operation of otherwise unpredictable learning-based controllers can be achieved.", 'abstract_zh': '基于模型的强化学习中先进世界模型的应用：通过持续监控确保部署策略仅在充分熟悉的状态空间区域内运行', 'title_zh': '基于模型的强化学习推理过程中异常检测的World Models研究'}
{'arxiv_id': 'arXiv:2503.02525', 'title': 'Magic in Human-Robot Interaction (HRI)', 'authors': 'Martin Cooney, Alexey Vinel', 'link': 'https://arxiv.org/abs/2503.02525', 'abstract': '"Magic" is referred to here and there in the robotics literature, from "magical moments" afforded by a mobile bubble machine, to "spells" intended to entertain and motivate children--but what exactly could this concept mean for designers? Here, we present (1) some theoretical discussion on how magic could inform interaction designs based on reviewing the literature, followed by (2) a practical description of using such ideas to develop a simplified prototype, which received an award in an international robot magic competition. Although this topic can be considered unusual and some negative connotations exist (e.g., unrealistic thinking can be referred to as magical), our results seem to suggest that magic, in the experiential, supernatural, and illusory senses of the term, could be useful to consider in various robot design contexts, also for artifacts like home assistants and autonomous vehicles--thus, inviting further discussion and exploration.', 'abstract_zh': '在这里，机器人文献中提到的“魔力”是指从移动气泡机提供的“神奇时刻”到意图娱乐和激励儿童的“法术”——但这个概念对设计师意味着什么？本文首先基于文献综述提出（1）关于“魔力”如何指导交互设计的一些理论讨论，随后介绍（2）如何利用这些想法开发一个简化的原型，该原型在国际机器人魔术竞赛中获奖。尽管这一主题可以被认为是不寻常的，且存在一些负面含义（例如，不切实际的思考可以被称为魔幻思维），但我们的结果似乎表明，在体验、超自然和幻觉的意义上考虑“魔力”，可以在各种机器人设计情境中以及类似于家庭助理和自动驾驶车辆的物件中是有用的，因此，激励进一步的讨论和探索。', 'title_zh': '人类机器人交互中的魔力'}
{'arxiv_id': 'arXiv:2503.02465', 'title': 'UAV-VLRR: Vision-Language Informed NMPC for Rapid Response in UAV Search and Rescue', 'authors': 'Yasheerah Yaqoot, Muhammad Ahsan Mustafa, Oleg Sautenkov, Dzmitry Tsetserukou', 'link': 'https://arxiv.org/abs/2503.02465', 'abstract': 'Emergency search and rescue (SAR) operations often require rapid and precise target identification in complex environments where traditional manual drone control is inefficient. In order to address these scenarios, a rapid SAR system, UAV-VLRR (Vision-Language-Rapid-Response), is developed in this research. This system consists of two aspects: 1) A multimodal system which harnesses the power of Visual Language Model (VLM) and the natural language processing capabilities of ChatGPT-4o (LLM) for scene interpretation. 2) A non-linearmodel predictive control (NMPC) with built-in obstacle avoidance for rapid response by a drone to fly according to the output of the multimodal system. This work aims at improving response times in emergency SAR operations by providing a more intuitive and natural approach to the operator to plan the SAR mission while allowing the drone to carry out that mission in a rapid and safe manner. When tested, our approach was faster on an average by 33.75% when compared with an off-the-shelf autopilot and 54.6% when compared with a human pilot. Video of UAV-VLRR: this https URL', 'abstract_zh': '紧急搜救(SAR)操作往往需要在传统手动无人机控制效率低下的复杂环境中进行快速而精确的目标识别。为了应对这些场景，本研究开发了一种快速SAR系统——UAV-VLRR（Vision-Language-Rapid-Response）。该系统包括两个方面：1) 一个集成了视觉语言模型(VLM)和ChatGPT-4o大规模语言模型(LLL)自然语言处理能力的模态系统，用于场景解释。2) 集成了障碍物避免的非线性模型预测控制(NMPC)，使无人机能够根据模态系统的输出进行快速响应飞行。本工作旨在通过提供一种更直观、更自然的方式来操作员规划SAR任务，同时使无人机能够安全、快速地执行任务，从而改善紧急SAR操作的响应时间。测试结果显示，与现成的自动驾驶系统相比，我们的方法平均快33.75%，与人类飞行员相比快54.6%。UAV-VLRR视频：this https URL', 'title_zh': 'UAV-VLRR：视觉-语言引导的快速响应无人机搜索与救援非线性模型预测控制'}
{'arxiv_id': 'arXiv:2503.02454', 'title': 'UAV-VLPA*: A Vision-Language-Path-Action System for Optimal Route Generation on a Large Scales', 'authors': 'Oleg Sautenkov, Aibek Akhmetkazy, Yasheerah Yaqoot, Muhammad Ahsan Mustafa, Grik Tadevosyan, Artem Lykov, Dzmitry Tsetserukou', 'link': 'https://arxiv.org/abs/2503.02454', 'abstract': "The UAV-VLPA* (Visual-Language-Planning-and-Action) system represents a cutting-edge advancement in aerial robotics, designed to enhance communication and operational efficiency for unmanned aerial vehicles (UAVs). By integrating advanced planning capabilities, the system addresses the Traveling Salesman Problem (TSP) to optimize flight paths, reducing the total trajectory length by 18.5\\% compared to traditional methods. Additionally, the incorporation of the A* algorithm enables robust obstacle avoidance, ensuring safe and efficient navigation in complex environments. The system leverages satellite imagery processing combined with the Visual Language Model (VLM) and GPT's natural language processing capabilities, allowing users to generate detailed flight plans through simple text commands. This seamless fusion of visual and linguistic analysis empowers precise decision-making and mission planning, making UAV-VLPA* a transformative tool for modern aerial operations. With its unmatched operational efficiency, navigational safety, and user-friendly functionality, UAV-VLPA* sets a new standard in autonomous aerial robotics, paving the way for future innovations in the field.", 'abstract_zh': 'UAV-VLPA*（视觉-语言-规划与行动）系统代表了飞行机器人领域的前沿进展，旨在提升无人机的通信和操作效率。通过整合先进的规划能力，该系统解决了旅行商问题（TSP），将飞行路径的总轨迹长度减少了18.5%，相较于传统方法更为高效。此外，A*算法的引入使得稳健的障碍物规避成为可能，确保在复杂环境中安全高效的导航。该系统结合了卫星影像处理、视觉语言模型（VLM）和GPT的自然语言处理能力，使用户能够通过简单的文本命令生成详细的飞行计划。这种视觉与语言分析的无缝融合赋予了精确的决策和任务规划能力，使UAV-VLPA*成为现代飞行操作中的一项变革性工具。凭借其无与伦比的操作效率、导航安全性以及用户友好的功能，UAV-VLPA*为自主飞行机器人领域设定了新的标准，为该领域的未来创新铺平了道路。', 'title_zh': 'UAV-VLPA*：一种大规模路径规划的视觉-语言-路径-行动系统'}
{'arxiv_id': 'arXiv:2503.02405', 'title': 'A comparison of visual representations for real-world reinforcement learning in the context of vacuum gripping', 'authors': 'Nico Sutter, Valentin N. Hartmann, Stelian Coros', 'link': 'https://arxiv.org/abs/2503.02405', 'abstract': 'When manipulating objects in the real world, we need reactive feedback policies that take into account sensor information to inform decisions. This study aims to determine how different encoders can be used in a reinforcement learning (RL) framework to interpret the spatial environment in the local surroundings of a robot arm. Our investigation focuses on comparing real-world vision with 3D scene inputs, exploring new architectures in the process. We built on the SERL framework, providing us with a sample efficient and stable RL foundation we could build upon, while keeping training times minimal. The results of this study indicate that spatial information helps to significantly outperform the visual counterpart, tested on a box picking task with a vacuum gripper. The code and videos of the evaluations are available at this https URL.', 'abstract_zh': '在真实世界中操作物体时，我们需要反应性反馈策略来利用传感器信息来指导决策。本研究旨在探讨在机器臂周边环境中，不同编码器在强化学习框架中如何用于解读空间环境。我们的研究重点在于比较实际视觉输入和3D场景输入的表现，并在此过程中探索新的网络架构。我们基于SERL框架，提供了高效且稳定的学习基础，并保持了较低的训练时间。研究结果表明，空间信息在盒子拾取任务中显著优于视觉信息，使用真空吸盘进行测试。评估的代码和视频可在以下链接获取。', 'title_zh': '现实世界真空吸放操作中视觉表示方法的比较'}
{'arxiv_id': 'arXiv:2503.02310', 'title': 'Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding', 'authors': 'Wenxuan Song, Jiayi Chen, Pengxiang Ding, Han Zhao, Wei Zhao, Zhide Zhong, Zongyuan Ge, Jun Ma, Haoang Li', 'link': 'https://arxiv.org/abs/2503.02310', 'abstract': 'Vision-Language-Action (VLA) models demonstrate remarkable potential for generalizable robotic manipulation. The performance of VLA models can be improved by integrating with action chunking, a critical technique for effective control. However, action chunking linearly scales up action dimensions in VLA models with increased chunking sizes. This reduces the inference efficiency. To tackle this problem, we propose PD-VLA, the first parallel decoding framework for VLA models integrated with action chunking. Our framework reformulates autoregressive decoding as a nonlinear system solved by parallel fixed-point iterations. This approach preserves model performance with mathematical guarantees while significantly improving decoding speed. In addition, it enables training-free acceleration without architectural changes, as well as seamless synergy with existing acceleration techniques. Extensive simulations validate that our PD-VLA maintains competitive success rates while achieving 2.52 times execution frequency on manipulators (with 7 degrees of freedom) compared with the fundamental VLA model. Furthermore, we experimentally identify the most effective settings for acceleration. Finally, real-world experiments validate its high applicability across different tasks.', 'abstract_zh': '基于视觉-语言-动作的并行解码框架（PD-VLA）：结合动作分块的高效机器人操作 modeling', 'title_zh': '基于动作切片集成的并行解码加速视觉-语言-动作模型'}
{'arxiv_id': 'arXiv:2503.02277', 'title': 'Active Robot Curriculum Learning from Online Human Demonstrations', 'authors': 'Muhan Hou, Koen Hindriks, A.E. Eiben, Kim Baraka', 'link': 'https://arxiv.org/abs/2503.02277', 'abstract': 'Learning from Demonstrations (LfD) allows robots to learn skills from human users, but its effectiveness can suffer due to sub-optimal teaching, especially from untrained demonstrators. Active LfD aims to improve this by letting robots actively request demonstrations to enhance learning. However, this may lead to frequent context switches between various task situations, increasing the human cognitive load and introducing errors to demonstrations. Moreover, few prior studies in active LfD have examined how these active query strategies may impact human teaching in aspects beyond user experience, which can be crucial for developing algorithms that benefit both robot learning and human teaching. To tackle these challenges, we propose an active LfD method that optimizes the query sequence of online human demonstrations via Curriculum Learning (CL), where demonstrators are guided to provide demonstrations in situations of gradually increasing difficulty. We evaluate our method across four simulated robotic tasks with sparse rewards and conduct a user study (N=26) to investigate the influence of active LfD methods on human teaching regarding teaching performance, post-guidance teaching adaptivity, and teaching transferability. Our results show that our method significantly improves learning performance compared to three other LfD baselines in terms of the final success rate of the converged policy and sample efficiency. Additionally, results from our user study indicate that our method significantly reduces the time required from human demonstrators and decreases failed demonstration attempts. It also enhances post-guidance human teaching in both seen and unseen scenarios compared to another active LfD baseline, indicating enhanced teaching performance, greater post-guidance teaching adaptivity, and better teaching transferability achieved by our method.', 'abstract_zh': '基于示例的主动学习（Active LfD）通过课程学习优化人类演示的查询序列，提高机器人技能学习性能及人类教学效果', 'title_zh': '基于在线人类示范的活性机器人课程学习'}
{'arxiv_id': 'arXiv:2503.02256', 'title': 'Continual Multi-Robot Learning from Black-Box Visual Place Recognition Models', 'authors': 'Kenta Tsukahara, Kanji Tanaka, Daiki Iwata, Jonathan Tay Yu Liang', 'link': 'https://arxiv.org/abs/2503.02256', 'abstract': 'In the context of visual place recognition (VPR), continual learning (CL) techniques offer significant potential for avoiding catastrophic forgetting when learning new places. However, existing CL methods often focus on knowledge transfer from a known model to a new one, overlooking the existence of unknown black-box models. We explore a novel multi-robot CL approach that enables knowledge transfer from black-box VPR models (teachers), such as those of local robots encountered by traveler robots (students) in unknown environments. Specifically, we introduce Membership Inference Attack, or MIA, the only major privacy attack applicable to black-box models, and leverage it to reconstruct pseudo training sets, which serve as the key knowledge to be exchanged between robots, from black-box VPR models. Furthermore, we aim to overcome the inherently low sampling efficiency of MIA by leveraging insights on place class prediction distribution and un-learned class detection imported from the VPR literature as a prior distribution. We also analyze both the individual effects of these methods and their combined impact. Experimental results demonstrate that our black-box MIA (BB-MIA) approach is remarkably powerful despite its simplicity, significantly enhancing the VPR capability of lower-performing robots through brief communication with other robots. This study contributes to optimizing knowledge sharing between robots in VPR and enhancing autonomy in open-world environments with multi-robot systems that are fault-tolerant and scalable.', 'abstract_zh': '基于视觉地方识别的黑盒连续学习方法：克服遗忘并增强多机器人系统的自主能力', 'title_zh': '黑箱视觉定位模型的持续多机器人学习'}
{'arxiv_id': 'arXiv:2503.02249', 'title': 'Large Language Models as Natural Selector for Embodied Soft Robot Design', 'authors': 'Changhe Chen, Xiaohao Xu, Xiangdong Wang, Xiaonan Huang', 'link': 'https://arxiv.org/abs/2503.02249', 'abstract': 'Designing soft robots is a complex and iterative process that demands cross-disciplinary expertise in materials science, mechanics, and control, often relying on intuition and extensive experimentation. While Large Language Models (LLMs) have demonstrated impressive reasoning abilities, their capacity to learn and apply embodied design principles--crucial for creating functional robotic systems--remains largely unexplored. This paper introduces RoboCrafter-QA, a novel benchmark to evaluate whether LLMs can learn representations of soft robot designs that effectively bridge the gap between high-level task descriptions and low-level morphological and material choices. RoboCrafter-QA leverages the EvoGym simulator to generate a diverse set of soft robot design challenges, spanning robotic locomotion, manipulation, and balancing tasks. Our experiments with state-of-the-art multi-modal LLMs reveal that while these models exhibit promising capabilities in learning design representations, they struggle with fine-grained distinctions between designs with subtle performance differences. We further demonstrate the practical utility of LLMs for robot design initialization. Our code and benchmark will be available to encourage the community to foster this exciting research direction.', 'abstract_zh': '设计软机器人是一个复杂且迭代的过程，要求多学科专业知识，包括材料科学、机械学和控制学，通常依赖直觉和大量实验。尽管大型语言模型（LLMs）展现出了强大的推理能力，但它们在学习和应用体现于设计的基本原理方面的能力——这对于创建功能性的机器人系统至关重要——仍然 largely unexplored 省略为“缺乏探索”。本文介绍 RoboCrafter-QA，这是一个新的基准，用于评估 LLMs 是否能够学习代表软机器人设计的表示，这些表示能够有效地弥合高层次任务描述与低层次形态和材料选择之间的差距。RoboCrafter-QA 利用 EvoGym 模拟器生成一系列软机器人设计挑战，涵盖机器人运动、操作和平衡任务。我们的实验证明，虽然这些模型在学习设计表示方面表现出令人鼓舞的能力，但在区分具有微妙性能差异的设计方面存在困难。我们进一步展示了 LLMs 在机器人设计初始化方面的实际应用价值。我们的代码和基准将可供社区使用，以促进这一令人兴奋的研究方向。', 'title_zh': '大型语言模型作为自然选择器用于体现式软机器人设计'}
{'arxiv_id': 'arXiv:2503.02208', 'title': 'ADMM-MCBF-LCA: A Layered Control Architecture for Safe Real-Time Navigation', 'authors': 'Anusha Srikanthan, Yifan Xue, Vijay Kumar, Nikolai Matni, Nadia Figueroa', 'link': 'https://arxiv.org/abs/2503.02208', 'abstract': 'We consider the problem of safe real-time navigation of a robot in a dynamic environment with moving obstacles of arbitrary smooth geometries and input saturation constraints. We assume that the robot detects and models nearby obstacle boundaries with a short-range sensor and that this detection is error-free. This problem presents three main challenges: i) input constraints, ii) safety, and iii) real-time computation. To tackle all three challenges, we present a layered control architecture (LCA) consisting of an offline path library generation layer, and an online path selection and safety layer. To overcome the limitations of reactive methods, our offline path library consists of feasible controllers, feedback gains, and reference trajectories. To handle computational burden and safety, we solve online path selection and generate safe inputs that run at 100 Hz. Through simulations on Gazebo and Fetch hardware in an indoor environment, we evaluate our approach against baselines that are layered, end-to-end, or reactive. Our experiments demonstrate that among all algorithms, only our proposed LCA is able to complete tasks such as reaching a goal, safely. When comparing metrics such as safety, input error, and success rate, we show that our approach generates safe and feasible inputs throughout the robot execution.', 'abstract_zh': '我们考虑在具有任意光滑几何形状的移动障碍物和输入饱和约束的动态环境中，确保机器人实时安全导航的问题。我们假设机器人能够通过短距离传感器准确检测并建模附近障碍物边界。该问题主要包含三个挑战：i）输入约束，ii）安全性，iii）实时计算。为应对这三个挑战，我们提出了一种分层控制架构（LCA），包括离线路径库生成层和在线路径选择与安全层。为克服反应式方法的局限性，我们的离线路径库包含可行控制器、反馈增益和参考轨迹。为处理计算负担和确保安全性，我们在在线路径选择中生成安全输入，并以100 Hz的频率运行。通过在室内环境中的Gazebo和Fetch硬件上的仿真实验，我们将我们的方法与层次化、端到端或反应式的基线方法进行了对比。实验结果表明，只有我们提出的LCA能够安全地完成任务，如到达目标。在比较安全性、输入误差和成功率等指标时，我们证明了我们的方法在整个机器人执行过程中生成了安全且可行的输入。', 'title_zh': 'ADMM-MCBF-LCA：一种安全实时导航的分层控制架构'}
{'arxiv_id': 'arXiv:2503.02107', 'title': 'Balancing Act: Trading Off Doppler Odometry and Map Registration for Efficient Lidar Localization', 'authors': 'Katya M. Papais, Daniil Lisus, David J. Yoon, Andrew Lambert, Keith Y.K. Leung, Timothy D. Barfoot', 'link': 'https://arxiv.org/abs/2503.02107', 'abstract': 'Most autonomous vehicles rely on accurate and efficient localization, which is achieved by comparing live sensor data to a preexisting map, to navigate their environment. Balancing the accuracy of localization with computational efficiency remains a significant challenge, as high-accuracy methods often come with higher computational costs. In this paper, we present two ways of improving lidar localization efficiency and study their impact on performance. First, we integrate a lightweight Doppler-based odometry method into a topometric localization pipeline and compare its performance against an iterative closest point (ICP)-based method. We highlight the trade-offs between these approaches: the Doppler estimator offers faster, lightweight updates, while ICP provides higher accuracy at the cost of increased computational load. Second, by controlling the frequency of localization updates and leveraging odometry estimates between them, we demonstrate that accurate localization can be maintained while optimizing for computational efficiency using either odometry method. Our experimental results show that localizing every 10 lidar frames strikes a favourable balance, achieving a localization accuracy below 0.05 meters in translation and below 0.1 degrees in orientation while reducing computational effort by over 30% in an ICP-based pipeline. We quantify the trade-off of accuracy to computational effort using over 100 kilometers of real-world driving data in different on-road environments.', 'abstract_zh': '大多数自动驾驶车辆依靠精确高效的定位来导航环境，这一过程是通过将实时传感器数据与预先存在的地图进行比对实现的。在保持定位精度与计算效率之间的平衡仍然是一个重要挑战，因为高精度的方法往往伴随着更高的计算成本。在本文中，我们提出了两种改进lidar定位效率的方法，并研究了它们对性能的影响。首先，我们将一种轻量级的多普勒基于的里程计方法集成到顶点定位管道中，并将其性能与基于迭代最近点（ICP）的方法进行比较。我们强调了这两种方法之间的权衡：多普勒估计器提供了更快、更轻量级的更新，而ICP则在计算负担增加的前提下提供更高的精度。其次，通过控制定位更新的频率并在它们之间利用里程计估计，我们证明了可以在优化计算效率的同时使用任何一种里程计方法保持精确的定位。我们的实验结果显示，每隔10个lidar帧进行一次定位，可以在保持低于0.05米的平移精度和低于0.1度的方向精度的同时，使基于ICP的管道计算成本减少超过30%。我们使用超过100公里的实物驾驶数据在不同的道路环境中量化了精度和计算成本之间的权衡。', 'title_zh': '平衡之道：雷达测距机载测距与地图注册之间的权衡以实现高效激光雷达定位'}
{'arxiv_id': 'arXiv:2503.02106', 'title': 'OVAMOS: A Framework for Open-Vocabulary Multi-Object Search in Unknown Environments', 'authors': 'Qianwei Wang, Yifan Xu, Vineet Kamat, Carol Menassa', 'link': 'https://arxiv.org/abs/2503.02106', 'abstract': 'Object search is a fundamental task for robots deployed in indoor building environments, yet challenges arise due to observation instability, especially for open-vocabulary models. While foundation models (LLMs/VLMs) enable reasoning about object locations even without direct visibility, the ability to recover from failures and replan remains crucial. The Multi-Object Search (MOS) problem further increases complexity, requiring the tracking multiple objects and thorough exploration in novel environments, making observation uncertainty a significant obstacle. To address these challenges, we propose a framework integrating VLM-based reasoning, frontier-based exploration, and a Partially Observable Markov Decision Process (POMDP) framework to solve the MOS problem in novel environments. VLM enhances search efficiency by inferring object-environment relationships, frontier-based exploration guides navigation in unknown spaces, and POMDP models observation uncertainty, allowing recovery from failures in occlusion and cluttered environments. We evaluate our framework on 120 simulated scenarios across several Habitat-Matterport3D (HM3D) scenes and a real-world robot experiment in a 50-square-meter office, demonstrating significant improvements in both efficiency and success rate over baseline methods.', 'abstract_zh': '基于视觉语言模型的多对象搜索框架：结合前沿探索与部分可观测马尔可夫决策过程', 'title_zh': 'OVAMOS：一种面向未知环境的多对象开放词汇搜索框架'}
{'arxiv_id': 'arXiv:2503.02076', 'title': 'CorrA: Leveraging Large Language Models for Dynamic Obstacle Avoidance of Autonomous Vehicles', 'authors': 'Shanting Wang, Panagiotis Typaldos, Andreas A. Malikopoulos', 'link': 'https://arxiv.org/abs/2503.02076', 'abstract': 'In this paper, we present Corridor-Agent (CorrA), a framework that integrates large language models (LLMs) with model predictive control (MPC) to address the challenges of dynamic obstacle avoidance in autonomous vehicles. Our approach leverages LLM reasoning ability to generate appropriate parameters for sigmoid-based boundary functions that define safe corridors around obstacles, effectively reducing the state-space of the controlled vehicle. The proposed framework adjusts these boundaries dynamically based on real-time vehicle data that guarantees collision-free trajectories while also ensuring both computational efficiency and trajectory optimality. The problem is formulated as an optimal control problem and solved with differential dynamic programming (DDP) for constrained optimization, and the proposed approach is embedded within an MPC framework. Extensive simulation and real-world experiments demonstrate that the proposed framework achieves superior performance in maintaining safety and efficiency in complex, dynamic environments compared to a baseline MPC approach.', 'abstract_zh': '本文提出了Corridor-Agent (CorrA)框架，该框架将大型语言模型（LLMs）与模型预测控制（MPC）相结合，以解决自主车辆在动态避障中的挑战。该方法利用LLM的推理能力生成基于Sigmoid边界函数的适当参数，定义障碍物周围的安全走廊，有效减少了受控车辆的状态空间。所提出的框架根据实时车辆数据动态调整这些边界，保证无碰撞轨迹的同时，也确保计算效率和轨迹优化。该问题被形式化为最优控制问题，并使用差分动态规划（DDP）进行约束优化求解，所提出的方法嵌入到MPC框架中。大量仿真实验和实地实验表明，与基线的MPC方法相比，所提出的框架在复杂动态环境中的安全性和效率方面表现更为优异。', 'title_zh': 'CorrA：利用大型语言模型实现自主车辆动态障碍物避免'}
{'arxiv_id': 'arXiv:2503.02048', 'title': 'FRMD: Fast Robot Motion Diffusion with Consistency-Distilled Movement Primitives for Smooth Action Generation', 'authors': 'Xirui Shi, Jun Jin', 'link': 'https://arxiv.org/abs/2503.02048', 'abstract': 'We consider the problem of using diffusion models to generate fast, smooth, and temporally consistent robot motions. Although diffusion models have demonstrated superior performance in robot learning due to their task scalability and multi-modal flexibility, they suffer from two fundamental limitations: (1) they often produce non-smooth, jerky motions due to their inability to capture temporally consistent movement dynamics, and (2) their iterative sampling process incurs prohibitive latency for many robotic tasks. Inspired by classic robot motion generation methods such as DMPs and ProMPs, which capture temporally and spatially consistent dynamic of trajectories using low-dimensional vectors -- and by recent advances in diffusion-based image generation that use consistency models with probability flow ODEs to accelerate the denoising process, we propose Fast Robot Motion Diffusion (FRMD). FRMD uniquely integrates Movement Primitives (MPs) with Consistency Models to enable efficient, single-step trajectory generation. By leveraging probabilistic flow ODEs and consistency distillation, our method models trajectory distributions while learning a compact, time-continuous motion representation within an encoder-decoder architecture. This unified approach eliminates the slow, multi-step denoising process of conventional diffusion models, enabling efficient one-step inference and smooth robot motion generation. We extensively evaluated our FRMD on the well-recognized Meta-World and ManiSkills Benchmarks, ranging from simple to more complex manipulation tasks, comparing its performance against state-of-the-art baselines. Our results show that FRMD generates significantly faster, smoother trajectories while achieving higher success rates.', 'abstract_zh': '使用快速机器人运动扩散模型生成快速、平滑且时间一致的机器人运动', 'title_zh': 'FRMD：快速机器人运动扩散与一致性提炼运动基元的平滑动作生成'}
{'arxiv_id': 'arXiv:2503.02311', 'title': 'Target Return Optimizer for Multi-Game Decision Transformer', 'authors': 'Kensuke Tatematsu, Akifumi Wachi', 'link': 'https://arxiv.org/abs/2503.02311', 'abstract': 'Achieving autonomous agents with robust generalization capabilities across diverse games and tasks remains one of the ultimate goals in AI research. Recent advancements in transformer-based offline reinforcement learning, exemplified by the MultiGame Decision Transformer [Lee et al., 2022], have shown remarkable performance across various games or tasks. However, these approaches depend heavily on human expertise, presenting substantial challenges for practical deployment, particularly in scenarios with limited prior game-specific knowledge. In this paper, we propose an algorithm called Multi-Game Target Return Optimizer (MTRO) to autonomously determine game-specific target returns within the Multi-Game Decision Transformer framework using solely offline datasets. MTRO addresses the existing limitations by automating the target return configuration process, leveraging environmental reward information extracted from offline datasets. Notably, MTRO does not require additional training, enabling seamless integration into existing Multi-Game Decision Transformer architectures. Our experimental evaluations on Atari games demonstrate that MTRO enhances the performance of RL policies across a wide array of games, underscoring its potential to advance the field of autonomous agent development.', 'abstract_zh': '实现跨多种游戏和任务具有 robust 通用化能力的自主智能体仍然是 AI 研究中的最终目标之一。基于.transformer 的 Offline 强化学习的 Recent 进展，例如 MultiGame Decision Transformer [Lee et al., 2022]，展示了在各种游戏或任务中的出色表现。然而，这些方法高度依赖于人工专业知识，特别是在缺乏特定游戏先验知识的场景中带来了巨大的实际部署挑战。本文提出了一种名为 Multi-Game Target Return Optimizer (MTRO) 的算法，该算法在 Multi-Game Decision Transformer 框架中仅使用离线数据集自主确定游戏特定的目标回报。MTRO 通过利用从离线数据集中提取的环境奖励信息自动配置目标回报，解决了现有方法的局限性。值得注意的是，MTRO 不需要额外的训练，可以无缝集成到现有的 Multi-Game Decision Transformer 架构中。我们在 Arcade Learning Environment 游戏上的实验评估表明，MTRO 能够提高强化学习策略在多种游戏中的性能，突显了其在自主智能体开发领域的潜在价值。', 'title_zh': '多游戏决策变换器的目标回报优化器'}
{'arxiv_id': 'arXiv:2503.02247', 'title': 'WMNav: Integrating Vision-Language Models into World Models for Object Goal Navigation', 'authors': 'Dujun Nie, Xianda Guo, Yiqun Duan, Ruijun Zhang, Long Chen', 'link': 'https://arxiv.org/abs/2503.02247', 'abstract': 'Object Goal Navigation-requiring an agent to locate a specific object in an unseen environment-remains a core challenge in embodied AI. Although recent progress in Vision-Language Model (VLM)-based agents has demonstrated promising perception and decision-making abilities through prompting, none has yet established a fully modular world model design that reduces risky and costly interactions with the environment by predicting the future state of the world. We introduce WMNav, a novel World Model-based Navigation framework powered by Vision-Language Models (VLMs). It predicts possible outcomes of decisions and builds memories to provide feedback to the policy module. To retain the predicted state of the environment, WMNav proposes the online maintained Curiosity Value Map as part of the world model memory to provide dynamic configuration for navigation policy. By decomposing according to a human-like thinking process, WMNav effectively alleviates the impact of model hallucination by making decisions based on the feedback difference between the world model plan and observation. To further boost efficiency, we implement a two-stage action proposer strategy: broad exploration followed by precise localization. Extensive evaluation on HM3D and MP3D validates WMNav surpasses existing zero-shot benchmarks in both success rate and exploration efficiency (absolute improvement: +3.2% SR and +3.2% SPL on HM3D, +13.5% SR and +1.1% SPL on MP3D). Project page: this https URL.', 'abstract_zh': '基于世界模型的物体目标导航：一种利用视觉语言模型的动力学导航框架', 'title_zh': 'WMNav: 将视觉语言模型集成到世界模型中进行物体目标导航'}
{'arxiv_id': 'arXiv:2503.02143', 'title': 'Four Principles for Physically Interpretable World Models', 'authors': 'Jordan Peper, Zhenjiang Mao, Yuang Geng, Siyuan Pan, Ivan Ruchkin', 'link': 'https://arxiv.org/abs/2503.02143', 'abstract': 'As autonomous systems are increasingly deployed in open and uncertain settings, there is a growing need for trustworthy world models that can reliably predict future high-dimensional observations. The learned latent representations in world models lack direct mapping to meaningful physical quantities and dynamics, limiting their utility and interpretability in downstream planning, control, and safety verification. In this paper, we argue for a fundamental shift from physically informed to physically interpretable world models - and crystallize four principles that leverage symbolic knowledge to achieve these ends: (1) structuring latent spaces according to the physical intent of variables, (2) learning aligned invariant and equivariant representations of the physical world, (3) adapting training to the varied granularity of supervision signals, and (4) partitioning generative outputs to support scalability and verifiability. We experimentally demonstrate the value of each principle on two benchmarks. This paper opens several intriguing research directions to achieve and capitalize on full physical interpretability in world models.', 'abstract_zh': '随着自主系统在开放和不确定环境中越来越多地被部署，可靠预测高维未来观测的可信赖世界模型的需求不断增加。现有世界模型中的学习潜在表示缺乏与有意义的物理量和动力学的直接映射，限制了其在下游规划、控制和安全性验证中的实用性和可解释性。本文主张从基于物理原理转变为基于物理解释的世界模型——并提炼出四条原则，利用符号知识实现这些目标：（1）根据变量的物理意图结构化潜在空间，（2）学习与物理世界对齐的不变和协变表示，（3）调整训练以适应监督信号的多样化粒度，（4）拆分生成输出以支持可扩展性和可验证性。我们在两个基准测试上实验证明了每条原则的价值。本文开启了几个有趣的研究方向，以实现和利用世界模型中的全面物理解释。', 'title_zh': '物理可解释的世界模型的四个原则'}
{'arxiv_id': 'arXiv:2503.02505', 'title': 'ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment', 'authors': 'Shaofei Cai, Zhancun Mu, Anji Liu, Yitao Liang', 'link': 'https://arxiv.org/abs/2503.02505', 'abstract': "We aim to develop a goal specification method that is semantically clear, spatially sensitive, and intuitive for human users to guide agent interactions in embodied environments. Specifically, we propose a novel cross-view goal alignment framework that allows users to specify target objects using segmentation masks from their own camera views rather than the agent's observations. We highlight that behavior cloning alone fails to align the agent's behavior with human intent when the human and agent camera views differ significantly. To address this, we introduce two auxiliary objectives: cross-view consistency loss and target visibility loss, which explicitly enhance the agent's spatial reasoning ability. According to this, we develop ROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an improvement in the efficiency of inference 3x to 6x. We show ROCKET-2 can directly interpret goals from human camera views for the first time, paving the way for better human-agent interaction.", 'abstract_zh': '我们旨在开发一种语义明确、空间敏感且易于人类用户直观使用的任务规范方法，以指导代理在具身环境中的交互。具体而言，我们提出了一种新颖的跨视角任务对齐框架，允许用户使用自身相机视角下的分割掩码来指定目标对象，而不仅仅是依靠代理的观察结果。我们强调，仅通过行为克隆无法在人类与代理相机视角差异显著时使代理行为与人类意图对齐。为此，我们引入了两个辅助目标：跨视角一致性损失和目标可见性损失，以明确增强代理的空间推理能力。基于此，我们开发了ROCKET-2，这是一个在Minecraft中训练的先进代理，实现了推理效率提高3至6倍的效果。我们首次展示了ROCKET-2可以直接从人类相机视角解析任务需求，为改善人机交互奠定了基础。', 'title_zh': 'ROCKET-2: 通过跨视图目标对齐引导视觉运动策略'}
{'arxiv_id': 'arXiv:2503.02012', 'title': 'Pretrained Embeddings as a Behavior Specification Mechanism', 'authors': 'Parv Kapoor, Abigail Hammer, Ashish Kapoor, Karen Leung, Eunsuk Kang', 'link': 'https://arxiv.org/abs/2503.02012', 'abstract': 'We propose an approach to formally specifying the behavioral properties of systems that rely on a perception model for interactions with the physical world. The key idea is to introduce embeddings -- mathematical representations of a real-world concept -- as a first-class construct in a specification language, where properties are expressed in terms of distances between a pair of ideal and observed embeddings. To realize this approach, we propose a new type of temporal logic called Embedding Temporal Logic (ETL), and describe how it can be used to express a wider range of properties about AI-enabled systems than previously possible. We demonstrate the applicability of ETL through a preliminary evaluation involving planning tasks in robots that are driven by foundation models; the results are promising, showing that embedding-based specifications can be used to steer a system towards desirable behaviors.', 'abstract_zh': '我们提出了一种通过感知模型正式规定系统行为属性的方法。关键思想是将嵌入——现实世界概念的数学表示——作为规范语言中的一级构造，其中属性用理想嵌入和观测嵌入之间的距离来表达。为了实现这一方法，我们提出了一种新的时序逻辑类型，称为嵌入时序逻辑(ETL)，并描述了它是如何能够表达比以往更多的关于AI使能系统属性的方法。通过在由基础模型驱动的机器人规划任务中的初步评估，展示了ETL的应用前景，结果表明基于嵌入的规范可以引导系统朝向期望的行为。', 'title_zh': '预训练嵌入作为行为规范机制'}
{'arxiv_id': 'arXiv:2503.02595', 'title': 'StageDesigner: Artistic Stage Generation for Scenography via Theater Scripts', 'authors': 'Zhaoxing Gan, Mengtian Li, Ruhua Chen, Zhongxia Ji, Sichen Guo, Huanling Hu, Guangnan Ye, Zuo Hu', 'link': 'https://arxiv.org/abs/2503.02595', 'abstract': 'In this work, we introduce StageDesigner, the first comprehensive framework for artistic stage generation using large language models combined with layout-controlled diffusion models. Given the professional requirements of stage scenography, StageDesigner simulates the workflows of seasoned artists to generate immersive 3D stage scenes. Specifically, our approach is divided into three primary modules: Script Analysis, which extracts thematic and spatial cues from input scripts; Foreground Generation, which constructs and arranges essential 3D objects; and Background Generation, which produces a harmonious background aligned with the narrative atmosphere and maintains spatial coherence by managing occlusions between foreground and background elements. Furthermore, we introduce the StagePro-V1 dataset, a dedicated dataset with 276 unique stage scenes spanning different historical styles and annotated with scripts, images, and detailed 3D layouts, specifically tailored for this task. Finally, evaluations using both standard and newly proposed metrics, along with extensive user studies, demonstrate the effectiveness of StageDesigner. Project can be found at: this https URL', 'abstract_zh': '本研究引入了StageDesigner，这是首个结合大规模语言模型与布局控制扩散模型的艺术舞台生成综合性框架。面对舞台布景的专业要求，StageDesigner模拟了资深艺术家的工作流程，生成沉浸式的3D舞台场景。具体而言，我们的方法分为三个主要模块：脚本分析，提取输入脚本中的主题和空间线索；前景生成，构建和排列关键3D对象；背景生成，产生与叙事氛围和谐一致的背景，并通过管理前景与背景元素间的遮挡保持空间连贯性。此外，我们还引入了StagePro-V1数据集，这是一个专门针对此项任务的数据集，包含276个不同历史风格的独特舞台场景，并标注有脚本、图像和详细的3D布局。最后，使用标准和新提出的评价指标以及广泛用户研究展示了StageDesigner的有效性。项目可在以下链接找到：this https URL。', 'title_zh': '舞台设计师：基于戏剧剧本的舞台艺术生成'}
{'arxiv_id': 'arXiv:2503.02351', 'title': 'MindSimulator: Exploring Brain Concept Localization via Synthetic FMRI', 'authors': 'Guangyin Bao, Qi Zhang, Zixuan Gong, Zhuojia Wu, Duoqian Miao', 'link': 'https://arxiv.org/abs/2503.02351', 'abstract': 'Concept-selective regions within the human cerebral cortex exhibit significant activation in response to specific visual stimuli associated with particular concepts. Precisely localizing these regions stands as a crucial long-term goal in neuroscience to grasp essential brain functions and mechanisms. Conventional experiment-driven approaches hinge on manually constructed visual stimulus collections and corresponding brain activity recordings, constraining the support and coverage of concept localization. Additionally, these stimuli often consist of concept objects in unnatural contexts and are potentially biased by subjective preferences, thus prompting concerns about the validity and generalizability of the identified regions. To address these limitations, we propose a data-driven exploration approach. By synthesizing extensive brain activity recordings, we statistically localize various concept-selective regions. Our proposed MindSimulator leverages advanced generative technologies to learn the probability distribution of brain activity conditioned on concept-oriented visual stimuli. This enables the creation of simulated brain recordings that reflect real neural response patterns. Using the synthetic recordings, we successfully localize several well-studied concept-selective regions and validate them against empirical findings, achieving promising prediction accuracy. The feasibility opens avenues for exploring novel concept-selective regions and provides prior hypotheses for future neuroscience research.', 'abstract_zh': '人类大脑皮层中的概念选择性区域对特定概念相关的视觉刺激表现出显著激活响应。精确定位这些区域是神经科学中长期的关键目标，以理解基本的脑功能和机制。传统的实验驱动方法依赖于手动构建的视觉刺激集合及其相应的脑活动记录，这限制了概念定位的支持和覆盖范围。此外，这些刺激通常包含在不自然情境下的概念对象，并可能受主观偏好的影响，从而引发了关于所识别区域的有效性和普适性的担忧。为了解决这些限制，我们提出了一个数据驱动的探索方法。通过合成大量脑活动记录，我们统计性地定位了各种概念选择性区域。我们提出的MindSimulator利用先进的生成技术学习概念导向的视觉刺激条件下的脑活动概率分布，从而创建反映真实神经反应模式的模拟脑记录。利用合成记录，我们成功定位了多个已研究的概念选择性区域，并通过与实证发现进行验证，实现了有前景的预测准确性。该可行性为探索新的概念选择性区域并为未来的神经科学研究提供先验假设提供了途径。', 'title_zh': 'MindSimulator: 探索脑概念定位的合成功能性磁共振成像方法'}
