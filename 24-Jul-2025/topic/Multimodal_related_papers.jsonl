{'arxiv_id': 'arXiv:2507.17664', 'title': 'Talk2Event: Grounded Understanding of Dynamic Scenes from Event Cameras', 'authors': 'Lingdong Kong, Dongyue Lu, Ao Liang, Rong Li, Yuhao Dong, Tianshuai Hu, Lai Xing Ng, Wei Tsang Ooi, Benoit R. Cottereau', 'link': 'https://arxiv.org/abs/2507.17664', 'abstract': 'Event cameras offer microsecond-level latency and robustness to motion blur, making them ideal for understanding dynamic environments. Yet, connecting these asynchronous streams to human language remains an open challenge. We introduce Talk2Event, the first large-scale benchmark for language-driven object grounding in event-based perception. Built from real-world driving data, we provide over 30,000 validated referring expressions, each enriched with four grounding attributes -- appearance, status, relation to viewer, and relation to other objects -- bridging spatial, temporal, and relational reasoning. To fully exploit these cues, we propose EventRefer, an attribute-aware grounding framework that dynamically fuses multi-attribute representations through a Mixture of Event-Attribute Experts (MoEE). Our method adapts to different modalities and scene dynamics, achieving consistent gains over state-of-the-art baselines in event-only, frame-only, and event-frame fusion settings. We hope our dataset and approach will establish a foundation for advancing multimodal, temporally-aware, and language-driven perception in real-world robotics and autonomy.', 'abstract_zh': '事件相机提供微秒级延迟和对运动模糊的 robustness，使其成为理解动态环境的理想选择。然而，将这些异步流与人类语言连接起来仍然是一个开放的挑战。我们引入了 Talk2Event，这是一个用于基于事件感知的语言驱动对象定位的大规模基准。该基准源自真实世界的驾驶数据，提供了超过 30,000 个验证的指示短语，并且每个短语都附有四种定位属性——外观、状态、相对于观察者的关系以及与其他物体的关系，这些属性跨越了空间、时间和关系推理。为了充分利用这些线索，我们提出了 EventRefer，这是一个属性感知的定位框架，通过对多属性表示进行动态融合，利用事件-属性专家混合体（MoEE）来实现这一目的。该方法能够适应不同的模态和场景动态，实现了在基于事件、基于帧和事件-帧融合设置中对现有最佳基线的一致改进。我们希望我们的数据集和方法能够为推进多模态、时间感知和语言驱动的感知奠定基础，在实际机器人技术和自主驾驶领域发挥作用。', 'title_zh': 'Talk2Event：事件相机中动态场景的 grounded 理解'}
{'arxiv_id': 'arXiv:2507.17012', 'title': 'Towards Autonomous Sustainability Assessment via Multimodal AI Agents', 'authors': 'Zhihan Zhang, Alexander Metzger, Yuxuan Mei, Felix Hähnlein, Zachary Englhardt, Tingyu Cheng, Gregory D. Abowd, Shwetak Patel, Adriana Schulz, Vikram Iyer', 'link': 'https://arxiv.org/abs/2507.17012', 'abstract': 'Interest in sustainability information has surged in recent years. However, the data required for a life cycle assessment (LCA) that maps the materials and processes from product manufacturing to disposal into environmental impacts (EI) are often unavailable. Here we reimagine conventional LCA by introducing multimodal AI agents that emulate interactions between LCA experts and stakeholders like product managers and engineers to calculate the cradle-to-gate (production) carbon emissions of electronic devices. The AI agents iteratively generate a detailed life-cycle inventory leveraging a custom data abstraction and software tools that extract information from online text and images from repair communities and government certifications. This approach reduces weeks or months of expert time to under one minute and closes data availability gaps while yielding carbon footprint estimates within 19% of expert LCAs with zero proprietary data. Additionally, we develop a method to directly estimate EI by comparing an input to a cluster of products with similar descriptions and known carbon footprints. This runs in 3 ms on a laptop with a MAPE of 12.28% on electronic products. Further, we develop a data-driven method to generate emission factors. We use the properties of an unknown material to represent it as a weighted sum of emission factors for similar materials. Compared to human experts picking the closest LCA database entry, this improves MAPE by 120.26%. We analyze the data and compute scaling of this approach and discuss its implications for future LCA workflows.', 'abstract_zh': '近年来，对可持续性信息的兴趣激增。然而，从产品制造到处置的生命周期评估（LCA）中所需的用于映射材料和过程到环境影响（EI）的数据往往不可用。我们通过引入模拟LCA专家与产品经理、工程师等利益相关者之间交互的多模态AI代理，重新构想了传统的LCA方法，以计算电子设备的摇篮到大门（生产）碳排放。这些AI代理利用自定义数据抽象和软件工具，从维修社区和政府认证的在线文本和图像中提取信息，逐步生成详细的生命周期清单。这种方法将专家所需的时间从几周或几个月缩短到不到一分钟，并弥补了数据可用性的缺口，同时提供的碳足迹估算值与专家进行的LCA相差不超过19%，且不依赖于专有数据。此外，我们开发了一种直接估算环境影响的方法，通过将输入与具有相似描述和已知碳足迹的产品集群进行比较实现。在笔记本电脑上，这种方法可在3毫秒内运行，并在电子产品的预测中具有12.28%的平均绝对百分比误差（MAPE）。进一步地，我们开发了一种数据驱动的方法来生成排放因子。我们使用未知材料的属性将其表示为类似材料排放因子的加权和。与人类专家选择最接近的LCA数据库条目相比，这种方法的MAPE降低了120.26%。我们分析数据并计算此方法的扩展性，讨论其对未来LCA工作流程的影响。', 'title_zh': '面向自主可持续性评估的多模态AI代理系统'}
{'arxiv_id': 'arXiv:2507.17467', 'title': 'Probing Vision-Language Understanding through the Visual Entailment Task: promises and pitfalls', 'authors': 'Elena Pitta, Tom Kouwenhoven, Tessa Verhoef', 'link': 'https://arxiv.org/abs/2507.17467', 'abstract': "This study investigates the extent to which the Visual Entailment (VE) task serves as a reliable probe of vision-language understanding in multimodal language models, using the LLaMA 3.2 11B Vision model as a test case. Beyond reporting performance metrics, we aim to interpret what these results reveal about the underlying possibilities and limitations of the VE task. We conduct a series of experiments across zero-shot, few-shot, and fine-tuning settings, exploring how factors such as prompt design, the number and order of in-context examples and access to visual information might affect VE performance. To further probe the reasoning processes of the model, we used explanation-based evaluations. Results indicate that three-shot inference outperforms the zero-shot baselines. However, additional examples introduce more noise than they provide benefits. Additionally, the order of the labels in the prompt is a critical factor that influences the predictions. In the absence of visual information, the model has a strong tendency to hallucinate and imagine content, raising questions about the model's over-reliance on linguistic priors. Fine-tuning yields strong results, achieving an accuracy of 83.3% on the e-SNLI-VE dataset and outperforming the state-of-the-art OFA-X model. Additionally, the explanation evaluation demonstrates that the fine-tuned model provides semantically meaningful explanations similar to those of humans, with a BERTScore F1-score of 89.2%. We do, however, find comparable BERTScore results in experiments with limited vision, questioning the visual grounding of this task. Overall, our results highlight both the utility and limitations of VE as a diagnostic task for vision-language understanding and point to directions for refining multimodal evaluation methods.", 'abstract_zh': '本研究探讨视觉蕴含任务（VE）在多模态语言模型中作为视觉-语言理解可靠探针的程度，以LLaMA 3.2 11B Vision模型为测试案例。除了报告性能指标，我们还旨在解释这些结果揭示的视觉蕴含任务潜在能力和局限性。我们在零样本、少样本和微调设置下进行了一系列实验，探索提示设计、上下文示例的数量和顺序以及对视觉信息的访问如何影响视觉蕴含性能。为了进一步探查模型的推理过程，我们使用了解释基评估方法。结果显示，三样本推理优于零样本基准。然而，额外的示例引入的噪声多于提供的益处。此外，提示中标签的顺序是影响预测的关键因素。在缺乏视觉信息的情况下，模型具有强烈的想象内容的趋势，这引发了人们对模型过度依赖语言先验的质疑。微调取得了显著结果，在e-SNLI-VE数据集上准确率达到83.3%，超越了最先进的OFA-X模型。此外，解释评估表明，经过微调的模型提供了与人类相似的语义上意义明确的解释，BERTScore F1得分为89.2%。然而，在视觉受限实验中也获得了可比的BERTScore结果，这质疑了该任务的视觉定位。总体而言，我们的结果突显了视觉蕴含任务作为视觉-语言理解诊断任务的实用性和局限性，并指出了改进多模态评估方法的方向。', 'title_zh': '通过视觉蕴含任务探究视觉-语言理解： promise 和 pitfalls'}
{'arxiv_id': 'arXiv:2507.17080', 'title': 'VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings', 'authors': 'Ramin Giahi, Kehui Yao, Sriram Kollipara, Kai Zhao, Vahid Mirjalili, Jianpeng Xu, Topojoy Biswas, Evren Korpeoglu, Kannan Achan', 'link': 'https://arxiv.org/abs/2507.17080', 'abstract': 'Multimodal learning plays a critical role in e-commerce recommendation platforms today, enabling accurate recommendations and product understanding. However, existing vision-language models, such as CLIP, face key challenges in e-commerce recommendation systems: 1) Weak object-level alignment, where global image embeddings fail to capture fine-grained product attributes, leading to suboptimal retrieval performance; 2) Ambiguous textual representations, where product descriptions often lack contextual clarity, affecting cross-modal matching; and 3) Domain mismatch, as generic vision-language models may not generalize well to e-commerce-specific data. To address these limitations, we propose a framework, VL-CLIP, that enhances CLIP embeddings by integrating Visual Grounding for fine-grained visual understanding and an LLM-based agent for generating enriched text embeddings. Visual Grounding refines image representations by localizing key products, while the LLM agent enhances textual features by disambiguating product descriptions. Our approach significantly improves retrieval accuracy, multimodal retrieval effectiveness, and recommendation quality across tens of millions of items on one of the largest e-commerce platforms in the U.S., increasing CTR by 18.6%, ATC by 15.5%, and GMV by 4.0%. Additional experimental results show that our framework outperforms vision-language models, including CLIP, FashionCLIP, and GCL, in both precision and semantic alignment, demonstrating the potential of combining object-aware visual grounding and LLM-enhanced text representation for robust multimodal recommendations.', 'abstract_zh': '多模态学习在电子商务推荐平台中发挥着关键作用，通过提高准确的推荐和产品理解。然而，现有的视觉-语言模型，如CLIP，在电子商务推荐系统中面临关键挑战：1）物体级对齐较弱，全局图像嵌入未能捕捉到细粒度的产品属性，导致检索性能不佳；2）文本表示模糊，产品描述经常缺乏上下文清晰度，影响跨模态匹配；3）领域不匹配，通用视觉-语言模型在电子商务特定数据上可能无法很好地泛化。为了解决这些限制，我们提出了一种框架VL-CLIP，通过集成视觉定位以增强细粒度的视觉理解，并结合基于LLM的代理生成丰富的文本嵌入来增强CLIP嵌入。视觉定位通过定位关键产品细化图像表示，而LLM代理通过澄清产品描述来增强文本特征。我们的方法显著提高了检索准确性、多模态检索效果和推荐质量，提升了一个美国最大的电子商务平台上的点击率（CTR）18.6%、添加到购物车率（ATC）15.5%和商品交易总额（GMV）4.0%。额外的实验结果表明，我们的框架在精度和语义对齐方面优于包括CLIP、FashionCLIP和GCL在内的视觉-语言模型，展示了对象感知的视觉定位和LLM增强的文本表示相结合在稳健的多模态推荐中的潜力。', 'title_zh': 'VL-CLIP: 通过视觉定位和LLM增强的CLIP嵌入提高多模态推荐'}
{'arxiv_id': 'arXiv:2507.16854', 'title': 'CLAMP: Contrastive Learning with Adaptive Multi-loss and Progressive Fusion for Multimodal Aspect-Based Sentiment Analysis', 'authors': 'Xiaoqiang He', 'link': 'https://arxiv.org/abs/2507.16854', 'abstract': "Multimodal aspect-based sentiment analysis(MABSA) seeks to identify aspect terms within paired image-text data and determine their fine grained sentiment polarities, representing a fundamental task for improving the effectiveness of applications such as product review systems and public opinion monitoring. Existing methods face challenges such as cross modal alignment noise and insufficient consistency in fine-grained representations. While global modality alignment methods often overlook the connection between aspect terms and their corresponding local visual regions, bridging the representation gap between text and images remains a challenge. To address these limitations, this paper introduces an end to end Contrastive Learning framework with Adaptive Multi-loss and Progressive Attention Fusion(CLAMP). The framework is composed of three novel modules: Progressive Attention Fusion network, Multi-task Contrastive Learning, and Adaptive Multi-loss Aggregation. The Progressive Attention Fusion network enhances fine-grained alignment between textual features and image regions via hierarchical, multi-stage cross modal interactions, effectively suppressing irrelevant visual noise. Secondly, multi-task contrastive learning combines global modal contrast and local granularity alignment to enhance cross modal representation consistency. Adaptive Multi-loss Aggregation employs a dynamic uncertainty based weighting mechanism to calibrate loss contributions according to each task's uncertainty, thereby mitigating gradient interference. Evaluation on standard public benchmarks demonstrates that CLAMP consistently outperforms the vast majority of existing state of the art methods.", 'abstract_zh': '多模态方面级情感分析（MABSA）旨在识别配对的图像-文本数据中的方面术语并确定其细粒度情感极性，这对提高产品评论系统和公共意见监控等应用的效果至关重要。现有方法面临跨模态对齐噪声和细粒度表示不足的挑战。虽然全局模态对齐方法往往忽略了方面术语与其相应的局部视觉区域之间的联系，但在文本和图像之间架桥表示差距仍是一个挑战。为解决这些局限性，本文提出了一种端到端的自适应多损失渐进注意力融合对比学习框架（CLAMP）。该框架由三个新颖模块组成：渐进注意力融合网络、多任务对比学习和自适应多损失聚合。渐进注意力融合网络通过多层次、多阶段跨模态交互来增强文本特征与图像区域之间的细粒度对齐，有效地抑制了无关视觉噪声。其次，多任务对比学习结合了全局模态对比和局部细节对齐，以增强跨模态表示一致性。自适应多损失聚合采用基于动态不确定性权重机制来根据每个任务的不确定性校准损失贡献，从而减轻梯度干扰。在标准公开基准上的评估表明，CLAMP在大多数现有最先进的方法中表现出优越性能。', 'title_zh': 'CLAMP：自适应多损失对比学习与 progressive 融合在多模态方面级情感分析中的应用'}
