{'arxiv_id': 'arXiv:2507.17727', 'title': 'CA-Cut: Crop-Aligned Cutout for Data Augmentation to Learn More Robust Under-Canopy Navigation', 'authors': 'Robel Mamo, Taeyeong Choi', 'link': 'https://arxiv.org/abs/2507.17727', 'abstract': 'State-of-the-art visual under-canopy navigation methods are designed with deep learning-based perception models to distinguish traversable space from crop rows. While these models have demonstrated successful performance, they require large amounts of training data to ensure reliability in real-world field deployment. However, data collection is costly, demanding significant human resources for in-field sampling and annotation. To address this challenge, various data augmentation techniques are commonly employed during model training, such as color jittering, Gaussian blur, and horizontal flip, to diversify training data and enhance model robustness. In this paper, we hypothesize that utilizing only these augmentation techniques may lead to suboptimal performance, particularly in complex under-canopy environments with frequent occlusions, debris, and non-uniform spacing of crops. Instead, we propose a novel augmentation method, so-called Crop-Aligned Cutout (CA-Cut) which masks random regions out in input images that are spatially distributed around crop rows on the sides to encourage trained models to capture high-level contextual features even when fine-grained information is obstructed. Our extensive experiments with a public cornfield dataset demonstrate that masking-based augmentations are effective for simulating occlusions and significantly improving robustness in semantic keypoint predictions for visual navigation. In particular, we show that biasing the mask distribution toward crop rows in CA-Cut is critical for enhancing both prediction accuracy and generalizability across diverse environments achieving up to a 36.9% reduction in prediction error. In addition, we conduct ablation studies to determine the number of masks, the size of each mask, and the spatial distribution of masks to maximize overall performance.', 'abstract_zh': '利用作物行对齐剪切增强提高森林下视觉导航性能', 'title_zh': '基于作物对齐剪裁的数据增强方法以学习更鲁棒的林下导航'}
{'arxiv_id': 'arXiv:2507.17445', 'title': "IndoorBEV: Joint Detection and Footprint Completion of Objects via Mask-based Prediction in Indoor Scenarios for Bird's-Eye View Perception", 'authors': 'Haichuan Li, Changda Tian, Panos Trahanias, Tomi Westerlund', 'link': 'https://arxiv.org/abs/2507.17445', 'abstract': "Detecting diverse objects within complex indoor 3D point clouds presents significant challenges for robotic perception, particularly with varied object shapes, clutter, and the co-existence of static and dynamic elements where traditional bounding box methods falter. To address these limitations, we propose IndoorBEV, a novel mask-based Bird's-Eye View (BEV) method for indoor mobile robots.\nIn a BEV method, a 3D scene is projected into a 2D BEV grid which handles naturally occlusions and provides a consistent top-down view aiding to distinguish static obstacles from dynamic agents. The obtained 2D BEV results is directly usable to downstream robotic tasks like navigation, motion prediction, and planning. Our architecture utilizes an axis compact encoder and a window-based backbone to extract rich spatial features from this BEV map. A query-based decoder head then employs learned object queries to concurrently predict object classes and instance masks in the BEV space. This mask-centric formulation effectively captures the footprint of both static and dynamic objects regardless of their shape, offering a robust alternative to bounding box regression. We demonstrate the effectiveness of IndoorBEV on a custom indoor dataset featuring diverse object classes including static objects\nand dynamic elements like robots and miscellaneous items, showcasing its potential for robust indoor scene understanding.", 'abstract_zh': '室内复杂三维点云中多样物体的检测对机器人感知构成了重大挑战，特别是在物体形状各异、存在杂乱以及静动态元素共存的情况下，传统的边界框方法往往力有未逮。为了解决这些问题，我们提出了一种新的基于掩码的鸟瞰图（BEV）方法——IndoorBEV，适用于室内移动机器人。', 'title_zh': 'IndoorBEV：基于掩码预测的室内场景中物体联合检测与足迹完成的鸟瞰视图感知'}
{'arxiv_id': 'arXiv:2507.17383', 'title': 'Confidence Calibration in Vision-Language-Action Models', 'authors': 'Thomas P Zollo, Richard Zemel', 'link': 'https://arxiv.org/abs/2507.17383', 'abstract': 'Trustworthy robot behavior requires not only high levels of task success but also that the robot can reliably quantify how likely it is to succeed. To this end, we present the first systematic study of confidence calibration in vision-language-action (VLA) foundation models, which map visual observations and natural-language instructions to low-level robot motor commands. We begin with extensive benchmarking to understand the critical relationship between task success and calibration error across multiple datasets and VLA variants, finding that task performance and calibration are not in tension. Next, we introduce prompt ensembles for VLAs, a lightweight, Bayesian-inspired algorithm that averages confidence across paraphrased instructions and consistently improves calibration. We further analyze calibration over the task time horizon, showing that confidence is often most reliable after making some progress, suggesting natural points for risk-aware intervention. Finally, we reveal differential miscalibration across action dimensions and propose action-wise Platt scaling, a method to recalibrate each action dimension independently to produce better confidence estimates. Our aim in this study is to begin to develop the tools and conceptual understanding necessary to render VLAs both highly performant and highly trustworthy via reliable uncertainty quantification.', 'abstract_zh': '可信的机器人行为不仅需要高任务成功率，还需要机器人能够可靠地量化其成功概率的可信度。为此，我们首次系统研究了视觉-语言-行动（VLA）基础模型中的信心校准问题，这些模型将视觉观测和自然语言指令映射到低级机器人动作命令。我们首先通过广泛的基准测试来理解任务成功与校准误差之间的关键关系，跨越多个数据集和VLA变体，发现任务性能和校准之间不存在冲突。接着，我们引入了视觉-语言-行动提示集合，这是一种轻量级、借鉴贝叶斯思想的算法，通过平均措辞相似指令的信心值来持续改进校准。我们进一步分析了任务时间范围内的校准情况，发现通常是在取得一定进展后信心最为可靠，这表明了风险感知干预的自然时机。最后，我们揭示了不同行动维度上的差异性校准偏差，并提出了按行动维度再校准的方法，该方法可以独立校准每个行动维度，以生成更好的信心估计。我们在此研究中的目标是开始开发必要的工具和概念理解，通过可靠的不确定性量化使VLA在性能和可信度方面达到高度。', 'title_zh': '视觉-语言-行动模型的置信度校准'}
{'arxiv_id': 'arXiv:2507.17253', 'title': 'Optimizing Delivery Logistics: Enhancing Speed and Safety with Drone Technology', 'authors': 'Maharshi Shastri, Ujjval Shrivastav', 'link': 'https://arxiv.org/abs/2507.17253', 'abstract': 'The increasing demand for fast and cost effective last mile delivery solutions has catalyzed significant advancements in drone based logistics. This research describes the development of an AI integrated drone delivery system, focusing on route optimization, object detection, secure package handling, and real time tracking. The proposed system leverages YOLOv4 Tiny for object detection, the NEO 6M GPS module for navigation, and the A7670 SIM module for real time communication. A comparative analysis of lightweight AI models and hardware components is conducted to determine the optimal configuration for real time UAV based delivery. Key challenges including battery efficiency, regulatory compliance, and security considerations are addressed through the integration of machine learning techniques, IoT devices, and encryption protocols. Preliminary studies demonstrate improvement in delivery time compared to conventional ground based logistics, along with high accuracy recipient authentication through facial recognition. The study also discusses ethical implications and societal acceptance of drone deliveries, ensuring compliance with FAA, EASA and DGCA regulatory standards. Note: This paper presents the architecture, design, and preliminary simulation results of the proposed system. Experimental results, simulation benchmarks, and deployment statistics are currently being acquired. A comprehensive analysis will be included in the extended version of this work.', 'abstract_zh': '基于AI的无人机配送系统开发：路径优化、物体检测、安全包裹处理及实时追踪的研究', 'title_zh': '优化配送物流：无人机技术提升速度与安全'}
{'arxiv_id': 'arXiv:2507.17665', 'title': 'Perspective-Invariant 3D Object Detection', 'authors': 'Ao Liang, Lingdong Kong, Dongyue Lu, Youquan Liu, Jian Fang, Huaici Zhao, Wei Tsang Ooi', 'link': 'https://arxiv.org/abs/2507.17665', 'abstract': 'With the rise of robotics, LiDAR-based 3D object detection has garnered significant attention in both academia and industry. However, existing datasets and methods predominantly focus on vehicle-mounted platforms, leaving other autonomous platforms underexplored. To bridge this gap, we introduce Pi3DET, the first benchmark featuring LiDAR data and 3D bounding box annotations collected from multiple platforms: vehicle, quadruped, and drone, thereby facilitating research in 3D object detection for non-vehicle platforms as well as cross-platform 3D detection. Based on Pi3DET, we propose a novel cross-platform adaptation framework that transfers knowledge from the well-studied vehicle platform to other platforms. This framework achieves perspective-invariant 3D detection through robust alignment at both geometric and feature levels. Additionally, we establish a benchmark to evaluate the resilience and robustness of current 3D detectors in cross-platform scenarios, providing valuable insights for developing adaptive 3D perception systems. Extensive experiments validate the effectiveness of our approach on challenging cross-platform tasks, demonstrating substantial gains over existing adaptation methods. We hope this work paves the way for generalizable and unified 3D perception systems across diverse and complex environments. Our Pi3DET dataset, cross-platform benchmark suite, and annotation toolkit have been made publicly available.', 'abstract_zh': '随着机器人技术的发展，基于LiDAR的3D物体检测在学术界和工业界引起了广泛关注。然而，现有的数据集和方法主要集中在车载平台，而其他自主平台则研究较少。为弥补这一不足，我们引入了Pi3DET，这是一个首个包含来自多种平台（车辆、四足机器人和无人机）的LiDAR数据及其3D边界框标注的数据集，从而促进了非车辆平台的3D物体检测研究以及跨平台3D检测研究。基于Pi3DET，我们提出了一种新的跨平台适应框架，该框架将来自研究成熟的车载平台的知识转移至其他平台。该框架通过几何和特征层面的鲁棒对齐实现视角不变的3D检测。此外，我们建立了一个基准来评估当前3D检测器在跨平台场景下的鲁棒性和鲁棒性，为开发适应性强的3D感知系统提供了宝贵的见解。广泛的实验验证了我们方法在挑战性的跨平台任务中的有效性，显示出相对于现有适应方法的显著改进。我们希望这项工作能为各种复杂环境中的可泛化和统一的3D感知系统铺平道路。我们的Pi3DET数据集、跨平台基准套件和标注工具箱已公开发布。', 'title_zh': '视角不变的3D物体检测'}
{'arxiv_id': 'arXiv:2507.17661', 'title': 'Monocular Semantic Scene Completion via Masked Recurrent Networks', 'authors': 'Xuzhi Wang, Xinran Wu, Song Wang, Lingdong Kong, Ziping Zhao', 'link': 'https://arxiv.org/abs/2507.17661', 'abstract': "Monocular Semantic Scene Completion (MSSC) aims to predict the voxel-wise occupancy and semantic category from a single-view RGB image. Existing methods adopt a single-stage framework that aims to simultaneously achieve visible region segmentation and occluded region hallucination, while also being affected by inaccurate depth estimation. Such methods often achieve suboptimal performance, especially in complex scenes. We propose a novel two-stage framework that decomposes MSSC into coarse MSSC followed by the Masked Recurrent Network. Specifically, we propose the Masked Sparse Gated Recurrent Unit (MS-GRU) which concentrates on the occupied regions by the proposed mask updating mechanism, and a sparse GRU design is proposed to reduce the computation cost. Additionally, we propose the distance attention projection to reduce projection errors by assigning different attention scores according to the distance to the observed surface. Experimental results demonstrate that our proposed unified framework, MonoMRN, effectively supports both indoor and outdoor scenes and achieves state-of-the-art performance on the NYUv2 and SemanticKITTI datasets. Furthermore, we conduct robustness analysis under various disturbances, highlighting the role of the Masked Recurrent Network in enhancing the model's resilience to such challenges. The source code is publicly available.", 'abstract_zh': '单目语义场景补全 (MSSC) 的目标是从单视角 RGB 图像中预测体素级的占有状态和语义类别。现有方法采用单一阶段框架，旨在同时实现可见区域分割和被遮挡区域的想象，但受不准确深度估计的影响。这些方法在复杂场景中往往无法获得最佳性能。我们提出了一种新颖的两阶段框架，将 MSSC 分解为粗略 MSSC 阶段，随后是 Masked Recurrent Network。具体地，我们提出了 Masked Sparse Gated Recurrent Unit (MS-GRU)，通过提出的掩码更新机制关注被占用区域，并提出稀疏 GRU 设计以降低计算成本。此外，我们提出了距离注意力投影，通过根据与观测表面的距离分配不同的注意力分数来减少投影误差。实验结果表明，我们提出的统一框架 MonoMRN 有效地支持室内和室外场景，并在 NYUv2 和 SemanticKITTI 数据集上实现了最新的性能。此外，我们在各种干扰下进行了鲁棒性分析，突显了 Masked Recurrent Network 在增强模型对这些挑战的抵抗力方面的作用。源代码已公开。', 'title_zh': '单目语义场景补全 via 遮蔽循环网络'}
{'arxiv_id': 'arXiv:2507.17089', 'title': 'IONext: Unlocking the Next Era of Inertial Odometry', 'authors': 'Shanshan Zhang, Siyue Wang, Tianshui Wen, Qi Zhang, Ziheng Zhou, Lingxiang Zheng, Yu Yang', 'link': 'https://arxiv.org/abs/2507.17089', 'abstract': 'Researchers have increasingly adopted Transformer-based models for inertial odometry. While Transformers excel at modeling long-range dependencies, their limited sensitivity to local, fine-grained motion variations and lack of inherent inductive biases often hinder localization accuracy and generalization. Recent studies have shown that incorporating large-kernel convolutions and Transformer-inspired architectural designs into CNN can effectively expand the receptive field, thereby improving global motion perception. Motivated by these insights, we propose a novel CNN-based module called the Dual-wing Adaptive Dynamic Mixer (DADM), which adaptively captures both global motion patterns and local, fine-grained motion features from dynamic inputs. This module dynamically generates selective weights based on the input, enabling efficient multi-scale feature aggregation. To further improve temporal modeling, we introduce the Spatio-Temporal Gating Unit (STGU), which selectively extracts representative and task-relevant motion features in the temporal domain. This unit addresses the limitations of temporal modeling observed in existing CNN approaches. Built upon DADM and STGU, we present a new CNN-based inertial odometry backbone, named Next Era of Inertial Odometry (IONext). Extensive experiments on six public datasets demonstrate that IONext consistently outperforms state-of-the-art (SOTA) Transformer- and CNN-based methods. For instance, on the RNIN dataset, IONext reduces the average ATE by 10% and the average RTE by 12% compared to the representative model iMOT.', 'abstract_zh': '基于双翼自适应动态混合模块和时空门控单元的新型惯性里程计', 'title_zh': 'IONext: 解锁惯性里程计的下一个时代'}
{'arxiv_id': 'arXiv:2507.17418', 'title': 'Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning', 'authors': 'Joobin Jin, Seokjun Hong, Gyeongseon Baek, Yeeun Kim, Byeongjoon Noh', 'link': 'https://arxiv.org/abs/2507.17418', 'abstract': 'Precise modeling of microscopic vehicle trajectories is critical for traffic behavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a context-aware trajectory generation framework that synthesizes realistic urban driving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses nonlinear interdependencies and training instability inherent in microscopic settings. By explicitly conditioning on surrounding vehicles and road geometry, Ctx2TrajGen generates interaction-aware trajectories aligned with real-world context. Experiments on the drone-captured DRIFT dataset demonstrate superior performance over existing methods in terms of realism, behavioral diversity, and contextual fidelity, offering a robust solution to data scarcity and domain shift without simulation.', 'abstract_zh': '基于GAIL的上下文感知轨迹生成框架Ctx2TrajGen：面向交通行为分析和自主驾驶系统的小尺度车辆轨迹精准建模', 'title_zh': 'Ctx2TrajGen：基于交通情境的生成对抗 imitation 学习微尺度车辆轨迹生成'}
{'arxiv_id': 'arXiv:2507.17745', 'title': 'Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention', 'authors': 'Yiwen Chen, Zhihao Li, Yikai Wang, Hu Zhang, Qin Li, Chi Zhang, Guosheng Lin', 'link': 'https://arxiv.org/abs/2507.17745', 'abstract': 'Recent advances in sparse voxel representations have significantly improved the quality of 3D content generation, enabling high-resolution modeling with fine-grained geometry. However, existing frameworks suffer from severe computational inefficiencies due to the quadratic complexity of attention mechanisms in their two-stage diffusion pipelines. In this work, we propose Ultra3D, an efficient 3D generation framework that significantly accelerates sparse voxel modeling without compromising quality. Our method leverages the compact VecSet representation to efficiently generate a coarse object layout in the first stage, reducing token count and accelerating voxel coordinate prediction. To refine per-voxel latent features in the second stage, we introduce Part Attention, a geometry-aware localized attention mechanism that restricts attention computation within semantically consistent part regions. This design preserves structural continuity while avoiding unnecessary global attention, achieving up to 6.7x speed-up in latent generation. To support this mechanism, we construct a scalable part annotation pipeline that converts raw meshes into part-labeled sparse voxels. Extensive experiments demonstrate that Ultra3D supports high-resolution 3D generation at 1024 resolution and achieves state-of-the-art performance in both visual fidelity and user preference.', 'abstract_zh': '近期稀疏体素表示的进展显著提高了3D内容生成的质量，能够实现高分辨率的精细几何建模。然而，现有框架由于两阶段扩散管道中注意机制的二次复杂性而遭受严重的计算效率低下问题。在此工作中，我们提出Ultra3D，一种高效且加速稀疏体素建模的3D生成框架，同时保持高质量。我们的方法利用紧凑的VecSet表示，在第一阶段高效生成粗略的物体布局，减少标记数量并加速体素坐标预测。为了在第二阶段细化每个体素的潜在特征，我们引入了Part Attention，一种几何感知的局部注意机制，仅在语义一致的部分区域范围内进行注意计算。这种设计保持了结构连续性，同时避免不必要的全局注意，潜在特征生成速度最高可提升6.7倍。为了支持这一机制，我们构建了一个可扩展的部分注释管道，将原始网格转换为部分标记的稀疏体素。广泛实验表明，Ultra3D支持1024分辨率的高分辨率3D生成，并在视觉保真度和用户体验方面达到最佳性能。', 'title_zh': 'Ultra3D：具有部分注意力机制的高效高保真3D生成'}
{'arxiv_id': 'arXiv:2507.17616', 'title': 'Vision Transformer attention alignment with human visual perception in aesthetic object evaluation', 'authors': 'Miguel Carrasco, César González-Martín, José Aranda, Luis Oliveros', 'link': 'https://arxiv.org/abs/2507.17616', 'abstract': 'Visual attention mechanisms play a crucial role in human perception and aesthetic evaluation. Recent advances in Vision Transformers (ViTs) have demonstrated remarkable capabilities in computer vision tasks, yet their alignment with human visual attention patterns remains underexplored, particularly in aesthetic contexts. This study investigates the correlation between human visual attention and ViT attention mechanisms when evaluating handcrafted objects. We conducted an eye-tracking experiment with 30 participants (9 female, 21 male, mean age 24.6 years) who viewed 20 artisanal objects comprising basketry bags and ginger jars. Using a Pupil Labs eye-tracker, we recorded gaze patterns and generated heat maps representing human visual attention. Simultaneously, we analyzed the same objects using a pre-trained ViT model with DINO (Self-DIstillation with NO Labels), extracting attention maps from each of the 12 attention heads. We compared human and ViT attention distributions using Kullback-Leibler divergence across varying Gaussian parameters (sigma=0.1 to 3.0). Statistical analysis revealed optimal correlation at sigma=2.4 +-0.03, with attention head #12 showing the strongest alignment with human visual patterns. Significant differences were found between attention heads, with heads #7 and #9 demonstrating the greatest divergence from human attention (p< 0.05, Tukey HSD test). Results indicate that while ViTs exhibit more global attention patterns compared to human focal attention, certain attention heads can approximate human visual behavior, particularly for specific object features like buckles in basketry items. These findings suggest potential applications of ViT attention mechanisms in product design and aesthetic evaluation, while highlighting fundamental differences in attention strategies between human perception and current AI models.', 'abstract_zh': '视觉注意力机制在人类感知和审美评价中起着关键作用。最近Vision Transformers (ViTs)在计算机视觉任务中表现出色，但它们与人类视觉注意力模式的对齐，在审美情境下仍需进一步探索。本研究探讨了在评估手工艺品时，人类视觉注意力与ViT注意力机制之间的关联。我们进行了一项眼动跟踪实验，共有30名参与者（9名女性，21名男性，平均年龄24.6岁），他们观看了20件手工制品（包括编篮包和姜罐）。使用Pupil Labs眼动仪记录了注视模式，并生成了表示人类视觉注意力的热图。同时，我们使用预训练的ViT模型（DINO，带有自我蒸馏且无标签）分析了这些对象，从每个多头注意力机制中提取注意力图。我们使用高斯参数（sigma=0.1到3.0）的Kullback-Leibler散度比较了人类和ViT的注意力分布。统计分析表明，在sigma=2.4 ± 0.03时相关性最佳，其中第12个多头注意力机制与人类视觉模式的对齐最为紧密。头号7和头号9展示出与人类注意力最显著的差异（p < 0.05，Tukey HSD检验）。研究结果表明，尽管ViTs表现出比人类聚焦注意力更广泛的注意力模式，但某些注意力机制可以近似人类视觉行为，特别是对于手工制品中的特定特征如编篮包的搭扣。这些发现表明ViT注意力机制在产品设计和审美评价中的潜在应用，并强调了人类感知与当前AI模型之间注意力策略的基本差异。', 'title_zh': '视觉变换器注意力机制与人类视觉感知在美学对象评价中的对齐'}
{'arxiv_id': 'arXiv:2507.17412', 'title': 'Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for Tumor Flagging and Staging', 'authors': 'Farnaz Khun Jush, Steffen Vogler, Matthias Lenga', 'link': 'https://arxiv.org/abs/2507.17412', 'abstract': "The increasing volume of medical images poses challenges for radiologists in retrieving relevant cases. Content-based image retrieval (CBIR) systems offer potential for efficient access to similar cases, yet lack standardized evaluation and comprehensive studies. Building on prior studies for tumor characterization via CBIR, this study advances CBIR research for volumetric medical images through three key contributions: (1) a framework eliminating reliance on pre-segmented data and organ-specific datasets, aligning with large and unstructured image archiving systems, i.e. PACS in clinical practice; (2) introduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT's contextualized late interaction mechanism for 3D medical imaging; (3) comprehensive evaluation across four tumor sites using three feature extractors and three database configurations. Our evaluations highlight the significant advantages of C-MIR. We demonstrate the successful adaptation of the late interaction principle to volumetric medical images, enabling effective context-aware re-ranking. A key finding is C-MIR's ability to effectively localize the region of interest, eliminating the need for pre-segmentation of datasets and offering a computationally efficient alternative to systems relying on expensive data enrichment steps. C-MIR demonstrates promising improvements in tumor flagging, achieving improved performance, particularly for colon and lung tumors (p<0.05). C-MIR also shows potential for improving tumor staging, warranting further exploration of its capabilities. Ultimately, our work seeks to bridge the gap between advanced retrieval techniques and their practical applications in healthcare, paving the way for improved diagnostic processes.", 'abstract_zh': '医学图像数量的增长为放射学家检索相关病例带来了挑战。基于内容的图像检索（CBIR）系统提供了高效访问相似病例的潜力，但缺乏标准化评估和全面研究。在此基础上，本研究通过三项关键贡献促进了CBIR在体积医学图像中的研究：（1）构建了一种框架，无需依赖预先分割的数据和器官特异性数据集，与临床实践中大规模和非结构化的图像归档系统（如PACS）相契合；（2）引入了C-MIR，这是一种新的体积重排方法，通过适应ColBERT的上下文化晚期交互机制，用于3D医学成像；（3）在四个肿瘤部位上使用三种特征提取器和三种数据库配置进行了全面评估。我们的评估突显了C-MIR的重要优势。我们展示了晚期交互原则在体积医学图像中的成功适配，从而使有效的上下文感知重排成为可能。一个关键发现是C-MIR能够有效地定位感兴趣区域，消除对数据集预分割的需求，并为依赖昂贵数据丰富步骤的系统提供了计算高效的替代方案。C-MIR在肿瘤标记方面表现出有前景的改进，特别是在结肠和肺部肿瘤方面（p<0.05）。C-MIR还显示了在肿瘤分期方面改进的潜力，值得进一步探索其能力。最终，我们的工作旨在填补高级检索技术在医疗保健实际应用中的差距，为改进诊断过程铺平道路。', 'title_zh': '基于内容的3D图像检索及受ColBERT启发的再排序在肿瘤标记和分期中的应用'}
{'arxiv_id': 'arXiv:2507.17347', 'title': 'Swin-TUNA : A Novel PEFT Approach for Accurate Food Image Segmentation', 'authors': 'Haotian Chen, Zhiyong Xiao', 'link': 'https://arxiv.org/abs/2507.17347', 'abstract': 'In the field of food image processing, efficient semantic segmentation techniques are crucial for industrial applications. However, existing large-scale Transformer-based models (such as FoodSAM) face challenges in meeting practical deploymentrequirements due to their massive parameter counts and high computational resource demands. This paper introduces TUNable Adapter module (Swin-TUNA), a Parameter Efficient Fine-Tuning (PEFT) method that integrates multiscale trainable adapters into the Swin Transformer architecture, achieving high-performance food image segmentation by updating only 4% of the parameters. The core innovation of Swin-TUNA lies in its hierarchical feature adaptation mechanism: it designs separable convolutions in depth and dimensional mappings of varying scales to address the differences in features between shallow and deep networks, combined with a dynamic balancing strategy for tasks-agnostic and task-specific features. Experiments demonstrate that this method achieves mIoU of 50.56% and 74.94% on the FoodSeg103 and UECFoodPix Complete datasets, respectively, surpassing the fully parameterized FoodSAM model while reducing the parameter count by 98.7% (to only 8.13M). Furthermore, Swin-TUNA exhibits faster convergence and stronger generalization capabilities in low-data scenarios, providing an efficient solution for assembling lightweight food image.', 'abstract_zh': '可调适配器模块在Swin Transformer中的高效细调方法及其在食品图像分割中的应用', 'title_zh': 'Swin-TUNA：一种用于准确食物图像分割的新型PEFT方法'}
{'arxiv_id': 'arXiv:2507.17297', 'title': 'On Temporal Guidance and Iterative Refinement in Audio Source Separation', 'authors': 'Tobias Morocutti, Jonathan Greif, Paul Primus, Florian Schmid, Gerhard Widmer', 'link': 'https://arxiv.org/abs/2507.17297', 'abstract': "Spatial semantic segmentation of sound scenes (S5) involves the accurate identification of active sound classes and the precise separation of their sources from complex acoustic mixtures. Conventional systems rely on a two-stage pipeline - audio tagging followed by label-conditioned source separation - but are often constrained by the absence of fine-grained temporal information critical for effective separation. In this work, we address this limitation by introducing a novel approach for S5 that enhances the synergy between the event detection and source separation stages. Our key contributions are threefold. First, we fine-tune a pre-trained Transformer to detect active sound classes. Second, we utilize a separate instance of this fine-tuned Transformer to perform sound event detection (SED), providing the separation module with detailed, time-varying guidance. Third, we implement an iterative refinement mechanism that progressively enhances separation quality by recursively reusing the separator's output from previous iterations. These advancements lead to significant improvements in both audio tagging and source separation performance, as demonstrated by our system's second-place finish in Task 4 of the DCASE Challenge 2025. Our implementation and model checkpoints are available in our GitHub repository: this https URL .", 'abstract_zh': '声 scene 空间语义分割 (S5) 涉及准确识别活动声类并从复杂声学混合中精确分离其来源。传统系统依赖于两阶段流水线——音频标记后继以基于标签的声源分离，但常受限于缺乏有效分离所必需的细粒度时间信息。在本文中，我们通过引入一种增强事件检测与声源分离阶段协同的新方法来解决这一限制。我们的主要贡献包括三个方面。首先，我们微调了一个预训练的 Transformer 来检测活动声类。其次，我们利用一个单独的微调 Transformer 实例来进行声事件检测 (SED)，为分离模块提供详细的、时间变化的指导。第三，我们实施了一种迭代精炼机制，通过递归重复使用上一次迭代的分离器输出来逐步提升分离质量。这些进步在音频标记和声源分离性能上取得了显著提高，如我们在 2025 年 DCASE 挑战赛 Task 4 中获得第二名所展示的那样。我们的实现和模型检查点可在我们的 GitHub 仓库中获取：this https URL 。', 'title_zh': '基于时间指导与迭代细化的音频源分离'}
{'arxiv_id': 'arXiv:2507.17185', 'title': 'Asymmetric Lesion Detection with Geometric Patterns and CNN-SVM Classification', 'authors': 'M. A. Rasel, Sameem Abdul Kareem, Zhenli Kwan, Nik Aimee Azizah Faheem, Winn Hui Han, Rebecca Kai Jan Choong, Shin Shen Yong, Unaizah Obaidellah', 'link': 'https://arxiv.org/abs/2507.17185', 'abstract': 'In dermoscopic images, which allow visualization of surface skin structures not visible to the naked eye, lesion shape offers vital insights into skin diseases. In clinically practiced methods, asymmetric lesion shape is one of the criteria for diagnosing melanoma. Initially, we labeled data for a non-annotated dataset with symmetrical information based on clinical assessments. Subsequently, we propose a supporting technique, a supervised learning image processing algorithm, to analyze the geometrical pattern of lesion shape, aiding non-experts in understanding the criteria of an asymmetric lesion. We then utilize a pre-trained convolutional neural network (CNN) to extract shape, color, and texture features from dermoscopic images for training a multiclass support vector machine (SVM) classifier, outperforming state-of-the-art methods from the literature. In the geometry-based experiment, we achieved a 99.00% detection rate for dermatological asymmetric lesions. In the CNN-based experiment, the best performance is found with 94% Kappa Score, 95% Macro F1-score, and 97% Weighted F1-score for classifying lesion shapes (Asymmetric, Half-Symmetric, and Symmetric).', 'abstract_zh': '在皮肤镜图像中，这些图像允许可视化肉眼不可见的皮肤表面结构，病灶形状为皮肤疾病提供了关键见解。在临床实践中，病灶不对称性是诊断黑色素瘤的标准之一。最初，我们基于临床评估为一个未标注的数据集标注对称信息。随后，我们提出了一个支持技术，即监督学习图像处理算法，用于分析病灶形状的几何模式，帮助非专家理解不对称病灶的标准。我们然后使用预训练的卷积神经网络（CNN）从皮肤镜图像中提取形状、颜色和纹理特征，训练一个多类支持向量机（SVM）分类器，性能超越了文献中的现有方法。在基于几何的实验中，病理性不对称病灶的检测率达到99.00%。在基于CNN的实验中，最佳性能为94%的Kappa分数、95%的宏F1分数和97%加权F1分数，用于分类病灶形状（不对称、半对称和对称）。', 'title_zh': '不对称病灶检测：基于几何模式和CNN-SVM分类'}
{'arxiv_id': 'arXiv:2507.17083', 'title': "SDGOCC: Semantic and Depth-Guided Bird's-Eye View Transformation for 3D Multimodal Occupancy Prediction", 'authors': 'Zaipeng Duan, Chenxu Dang, Xuzhong Hu, Pei An, Junfeng Ding, Jie Zhan, Yunbiao Xu, Jie Ma', 'link': 'https://arxiv.org/abs/2507.17083', 'abstract': 'Multimodal 3D occupancy prediction has garnered significant attention for its potential in autonomous driving. However, most existing approaches are single-modality: camera-based methods lack depth information, while LiDAR-based methods struggle with occlusions. Current lightweight methods primarily rely on the Lift-Splat-Shoot (LSS) pipeline, which suffers from inaccurate depth estimation and fails to fully exploit the geometric and semantic information of 3D LiDAR points. Therefore, we propose a novel multimodal occupancy prediction network called SDG-OCC, which incorporates a joint semantic and depth-guided view transformation coupled with a fusion-to-occupancy-driven active distillation. The enhanced view transformation constructs accurate depth distributions by integrating pixel semantics and co-point depth through diffusion and bilinear discretization. The fusion-to-occupancy-driven active distillation extracts rich semantic information from multimodal data and selectively transfers knowledge to image features based on LiDAR-identified regions. Finally, for optimal performance, we introduce SDG-Fusion, which uses fusion alone, and SDG-KL, which integrates both fusion and distillation for faster inference. Our method achieves state-of-the-art (SOTA) performance with real-time processing on the Occ3D-nuScenes dataset and shows comparable performance on the more challenging SurroundOcc-nuScenes dataset, demonstrating its effectiveness and robustness. The code will be released at this https URL.', 'abstract_zh': '多模态3D占用率预测在自主驾驶领域引起了广泛关注。然而，现有的大多数方法是单模态的：基于摄像头的方法缺乏深度信息，而基于LiDAR的方法在遮挡问题上表现不佳。当前的轻量化方法主要依赖于Lift-Splat-Shoot (LSS) 管道，但该方法存在深度估计不准确的问题，未能充分利用3D LiDAR点的几何和语义信息。因此，我们提出了一种名为SDG-OCC的新颖多模态占用率预测网络，该网络结合了联合语义和深度导向的视图变换以及融合驱动的主动蒸馏。增强的视图变换通过扩散和双线性离散化整合像素语义和共点深度，构建准确的深度分布。融合驱动的主动蒸馏从多模态数据中提取丰富的语义信息，并基于LiDAR标识的区域选择性地向图像特征转移知识。最后，为了实现最佳性能，我们引入了SDG-Fusion（仅使用融合）和SDG-KL（将融合和蒸馏结合起来，以实现更快的推理速度）。我们的方法在Occ3D-nuScenes数据集上实现了实时处理，并在更具挑战性的SurroundOcc-nuScenes数据集上表现出可比拟的性能，展示了其有效性和鲁棒性。代码将在以下几个网址之一发布：https://github.com/username/repo。', 'title_zh': 'SDGOCC: 语义和深度引导的鸟瞰视角变换用于3D多元模式占用预测'}
{'arxiv_id': 'arXiv:2507.17047', 'title': 'Controllable Hybrid Captioner for Improved Long-form Video Understanding', 'authors': 'Kuleen Sasse, Efsun Sarioglu Kayi, Arun Reddy', 'link': 'https://arxiv.org/abs/2507.17047', 'abstract': 'Video data, especially long-form video, is extremely dense and high-dimensional. Text-based summaries of video content offer a way to represent query-relevant content in a much more compact manner than raw video. In addition, textual representations are easily ingested by state-of-the-art large language models (LLMs), which enable reasoning over video content to answer complex natural language queries. To solve this issue, we rely on the progressive construction of a text-based memory by a video captioner operating on shorter chunks of the video, where spatio-temporal modeling is computationally feasible. We explore ways to improve the quality of the activity log comprised solely of short video captions. Because the video captions tend to be focused on human actions, and questions may pertain to other information in the scene, we seek to enrich the memory with static scene descriptions using Vision Language Models (VLMs). Our video understanding system relies on the LaViLa video captioner in combination with a LLM to answer questions about videos. We first explored different ways of partitioning the video into meaningful segments such that the textual descriptions more accurately reflect the structure of the video content. Furthermore, we incorporated static scene descriptions into the captioning pipeline using LLaVA VLM, resulting in a more detailed and complete caption log and expanding the space of questions that are answerable from the textual memory. Finally, we have successfully fine-tuned the LaViLa video captioner to produce both action and scene captions, significantly improving the efficiency of the captioning pipeline compared to using separate captioning models for the two tasks. Our model, controllable hybrid captioner, can alternate between different types of captions according to special input tokens that signals scene changes detected in the video.', 'abstract_zh': '基于视频的高密度和高维度特性，特别是长视频，文本摘要为表示查询相关的内容提供了一种更加紧凑的方式，相比原始视频而言。此外，文本表示易于被最先进的大型语言模型（LLMs）处理，从而可以在视频内容上进行推理以回答复杂的自然语言查询。为了解决这个问题，我们依靠视频标注器逐步构建文本记忆，该标注器在视频的较短片段上操作，从而实现时序建模的可行性。我们探索了提高仅由短视频标注组成的活动日志质量的方法。由于视频标注通常集中在人类动作上，而问题可能涉及场景中的其他信息，我们寻求使用视觉语言模型（VLMs）丰富静态场景描述以增强记忆。我们的视频理解系统依赖LaViLa视频标注器与LLM结合，以回答关于视频的问题。我们首先探索了不同的方法，将视频分割成有意义的片段，从而使文本描述更准确地反映出视频内容的结构。进一步地，我们通过将LLaVA VLM纳入标注管道中，将静态场景描述融入其中，从而生成了更加详细和完整的标注日志，并扩大了可以从文本记忆中解答的问题范围。最后，我们成功微调了LaViLa视频标注器，使其既能生成动作也能生成场景标注，显著提高了标注管道的效率，比使用两个独立模型执行这两种任务的效率更高。我们的模型，可控混合标注器，可根据视频中检测到的场景变化特殊输入标记交替生成不同类型的标注。', 'title_zh': '可控混合 captioner 以提升长视频理解'}
{'arxiv_id': 'arXiv:2507.17029', 'title': 'StreamME: Simplify 3D Gaussian Avatar within Live Stream', 'authors': 'Luchuan Song, Yang Zhou, Zhan Xu, Yi Zhou, Deepali Aneja, Chenliang Xu', 'link': 'https://arxiv.org/abs/2507.17029', 'abstract': 'We propose StreamME, a method focuses on fast 3D avatar reconstruction. The StreamME synchronously records and reconstructs a head avatar from live video streams without any pre-cached data, enabling seamless integration of the reconstructed appearance into downstream applications. This exceptionally fast training strategy, which we refer to as on-the-fly training, is central to our approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating the reliance on MLPs in deformable 3DGS and relying solely on geometry, which significantly improves the adaptation speed to facial expression. To further ensure high efficiency in on-the-fly training, we introduced a simplification strategy based on primary points, which distributes the point clouds more sparsely across the facial surface, optimizing points number while maintaining rendering quality. Leveraging the on-the-fly training capabilities, our method protects the facial privacy and reduces communication bandwidth in VR system or online conference. Additionally, it can be directly applied to downstream application such as animation, toonify, and relighting. Please refer to our project page for more details: this https URL.', 'abstract_zh': '我们提出StreamME，一种快速3D角色重建的方法。StreamME无需预缓存数据，即可同步记录并从实时视频流中重建头部角色，从而无缝地将重建的外观集成到下游应用中。我们称之为“即用即训”的这项极其快速的训练策略在我们的方法中起着核心作用。我们的方法基于3D高斯点积（3DGS），消除了可变形3DGS对MLPs的依赖，仅依赖于几何结构，这显著提高了对表情变化的适应速度。为了进一步确保“即用即训”的高效性，我们引入了一种基于主点的简化策略，通过更稀疏地分布在面部表面来优化点的数量，同时保持渲染效果。利用“即用即训”的能力，我们的方法可以保护面部隐私并减少VR系统或在线会议中的通信带宽。此外，它还可以直接应用于下游应用，如动画、卡通化和重新光照。更多信息请参阅我们的项目页面：[this URL]。', 'title_zh': 'StreamME: 简化实时流中的3D高斯Avatar'}
{'arxiv_id': 'arXiv:2507.16884', 'title': 'SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative Modeling', 'authors': 'Yi Guo, Wei Wang, Zhihang Yuan, Rong Cao, Kuan Chen, Zhengyang Chen, Yuanyuan Huo, Yang Zhang, Yuping Wang, Shouda Liu, Yuxuan Wang', 'link': 'https://arxiv.org/abs/2507.16884', 'abstract': 'Generative models like Flow Matching have achieved state-of-the-art performance but are often hindered by a computationally expensive iterative sampling process. To address this, recent work has focused on few-step or one-step generation by learning the average velocity field, which directly maps noise to data. MeanFlow, a leading method in this area, learns this field by enforcing a differential identity that connects the average and instantaneous velocities. In this work, we argue that this differential formulation is a limiting special case of a more fundamental principle. We return to the first principles of average velocity and leverage the additivity property of definite integrals. This leads us to derive a novel, purely algebraic identity we term Interval Splitting Consistency. This identity establishes a self-referential relationship for the average velocity field across different time intervals without resorting to any differential operators. Based on this principle, we introduce SplitMeanFlow, a new training framework that enforces this algebraic consistency directly as a learning objective. We formally prove that the differential identity at the core of MeanFlow is recovered by taking the limit of our algebraic consistency as the interval split becomes infinitesimal. This establishes SplitMeanFlow as a direct and more general foundation for learning average velocity fields. From a practical standpoint, our algebraic approach is significantly more efficient, as it eliminates the need for JVP computations, resulting in simpler implementation, more stable training, and broader hardware compatibility. One-step and two-step SplitMeanFlow models have been successfully deployed in large-scale speech synthesis products (such as Doubao), achieving speedups of 20x.', 'abstract_zh': '基于区间拆分一致性的平均流生成模型', 'title_zh': 'SplitMeanFlow: 几乎步生成建模中的区间划分一致性'}
{'arxiv_id': 'arXiv:2507.16880', 'title': 'Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less Local Than Assumed', 'authors': 'Antoni Kowalczuk, Dominik Hintersdorf, Lukas Struppek, Kristian Kersting, Adam Dziedzic, Franziska Boenisch', 'link': 'https://arxiv.org/abs/2507.16880', 'abstract': 'Text-to-image diffusion models (DMs) have achieved remarkable success in image generation. However, concerns about data privacy and intellectual property remain due to their potential to inadvertently memorize and replicate training data. Recent mitigation efforts have focused on identifying and pruning weights responsible for triggering replication, based on the assumption that memorization can be localized. Our research assesses the robustness of these pruning-based approaches. We demonstrate that even after pruning, minor adjustments to text embeddings of input prompts are sufficient to re-trigger data replication, highlighting the fragility of these defenses. Furthermore, we challenge the fundamental assumption of memorization locality, by showing that replication can be triggered from diverse locations within the text embedding space, and follows different paths in the model. Our findings indicate that existing mitigation strategies are insufficient and underscore the need for methods that truly remove memorized content, rather than attempting to suppress its retrieval. As a first step in this direction, we introduce a novel adversarial fine-tuning method that iteratively searches for replication triggers and updates the model to increase robustness. Through our research, we provide fresh insights into the nature of memorization in text-to-image DMs and a foundation for building more trustworthy and compliant generative AI.', 'abstract_zh': '基于文本到图像的扩散模型（DMs）在图像生成方面取得了显著成功，但数据隐私和知识产权方面的担忧依然存在，因为这些模型有可能无意中记忆并复现训练数据。近期的缓解努力集中在识别并修剪负责触发复现的权重上，前提是假设记忆是局部化的。我们的研究评估了这些修剪方法的鲁棒性。我们证明，即使进行了修剪，对输入提示的文本嵌入进行细微调整也足以重新触发数据复现，强调了这些防御措施的脆弱性。此外，我们通过展示复现可以从文本嵌入空间中不同的位置触发，并且在模型中沿不同的路径进行，挑战了记忆局部化的基本假设。我们的研究结果表明，现有缓解策略是不足的，并强调了需要能够真正移除记忆内容的方法的重要性，而不仅仅是试图抑制其检索。作为这一方向上的第一步，我们提出了一个新颖的对抗微调方法，该方法迭代地搜索复现触发因素并对模型进行更新以提高其鲁棒性。通过我们的研究，我们提供了关于文本到图像DMs中记忆本质的新见解，并为构建更加可靠和合规的生成AI奠定了基础。', 'title_zh': '寻找多里：文本到图像扩散模型中的记忆作用不如假设的那么局部。'}
{'arxiv_id': 'arXiv:2507.16864', 'title': 'Reinforcement Learning in hyperbolic space for multi-step reasoning', 'authors': 'Tao Xu, Dung-Yang Lee, Momiao Xiong', 'link': 'https://arxiv.org/abs/2507.16864', 'abstract': 'Multi-step reasoning is a fundamental challenge in artificial intelligence, with applications ranging from mathematical problem-solving to decision-making in dynamic environments. Reinforcement Learning (RL) has shown promise in enabling agents to perform multi-step reasoning by optimizing long-term rewards. However, conventional RL methods struggle with complex reasoning tasks due to issues such as credit assignment, high-dimensional state representations, and stability concerns. Recent advancements in Transformer architectures and hyperbolic geometry have provided novel solutions to these challenges. This paper introduces a new framework that integrates hyperbolic Transformers into RL for multi-step reasoning. The proposed approach leverages hyperbolic embeddings to model hierarchical structures effectively. We present theoretical insights, algorithmic details, and experimental results that include Frontier Math and nonlinear optimal control problems. Compared to RL with vanilla transformer, the hyperbolic RL largely improves accuracy by (32%~44%) on FrontierMath benchmark, (43%~45%) on nonlinear optimal control benchmark, while achieving impressive reduction in computational time by (16%~32%) on FrontierMath benchmark, (16%~17%) on nonlinear optimal control benchmark. Our work demonstrates the potential of hyperbolic Transformers in reinforcement learning, particularly for multi-step reasoning tasks that involve hierarchical structures.', 'abstract_zh': '基于超曲面变换器的强化学习多步推理框架', 'title_zh': '在双曲空间中进行多步推理的强化学习'}
{'arxiv_id': 'arXiv:2507.16861', 'title': 'Look Before You Fuse: 2D-Guided Cross-Modal Alignment for Robust 3D Detection', 'authors': 'Xiang Li', 'link': 'https://arxiv.org/abs/2507.16861', 'abstract': "Integrating LiDAR and camera inputs into a unified Bird's-Eye-View (BEV) representation is crucial for enhancing 3D perception capabilities of autonomous vehicles. However, current methods are often affected by misalignment between camera and LiDAR features. This misalignment leads to inaccurate depth supervision in camera branch and erroneous fusion during cross-modal feature aggregation. The root cause of this misalignment lies in projection errors, stemming from minor extrinsic calibration inaccuracies and rolling shutter effect of LiDAR during vehicle motion. In this work, our key insight is that these projection errors are predominantly concentrated at object-background boundaries, which are readily identified by 2D detectors. Based on this, our main motivation is to utilize 2D object priors to pre-align cross-modal features before fusion. To address local misalignment, we propose Prior Guided Depth Calibration (PGDC), which leverages 2D priors to correct local misalignment and preserve correct cross-modal feature pairs. To resolve global misalignment, we introduce Discontinuity Aware Geometric Fusion (DAGF) to process calibrated results from PGDC, suppressing noise and explicitly enhancing sharp transitions at object-background boundaries. To effectively utilize these transition-aware depth representations, we incorporate Structural Guidance Depth Modulator (SGDM), using a gated attention mechanism to efficiently fuse aligned depth and image features. Our proposed method achieves state-of-the-art performance on nuScenes validation dataset, with its mAP and NDS reaching 71.5% and 73.6% respectively.", 'abstract_zh': '将LiDAR和相机输入整合到统一的鸟瞰图（BEV）表示中对于增强自动驾驶车辆的三维感知能力至关重要。然而，目前的方法往往受到相机和LiDAR特征之间的对齐偏差的影响。这种对齐偏差导致了相机分支中的不准确深度监督和跨模态特征聚合时的错误融合。这种对齐偏差的根本原因在于投影误差，这些误差源自于车辆运动过程中外参校准的小幅不准确和LiDAR的卷帘快门效应。在本工作中，我们的关键洞察是这些投影误差主要集中在物体与背景的边界上，这些边界可以被二维检测器轻易识别。基于此，我们的主要动机是利用二维物体先验在融合前对齐跨模态特征。为解决局部对齐偏差，我们提出了先验引导深度校准（PGDC），该方法利用二维先验纠正局部对齐偏差并保留正确的跨模态特征对。为解决全局对齐偏差，我们引入了不连续性感知几何融合（DAGF），对PGDC校准的结果进行处理，抑制噪声并明确增强物体与背景边界处的锐变过渡。为了有效地利用这些过渡感知深度表示，我们引入了结构引导深度调制器（SGDM），使用门控注意力机制高效融合对齐的深度和图像特征。我们提出的方法在nuScenes验证数据集上达到了最先进的性能，其mAP和NDS分别达到了71.5%和73.6%。', 'title_zh': '先看后融合：2D导向的跨模态对齐以实现稳健的3D检测'}
{'arxiv_id': 'arXiv:2507.16850', 'title': 'Toward a Real-Time Framework for Accurate Monocular 3D Human Pose Estimation with Geometric Priors', 'authors': 'Mohamed Adjel', 'link': 'https://arxiv.org/abs/2507.16850', 'abstract': 'Monocular 3D human pose estimation remains a challenging and ill-posed problem, particularly in real-time settings and unconstrained environments. While direct imageto-3D approaches require large annotated datasets and heavy models, 2D-to-3D lifting offers a more lightweight and flexible alternative-especially when enhanced with prior knowledge. In this work, we propose a framework that combines real-time 2D keypoint detection with geometry-aware 2D-to-3D lifting, explicitly leveraging known camera intrinsics and subject-specific anatomical priors. Our approach builds on recent advances in self-calibration and biomechanically-constrained inverse kinematics to generate large-scale, plausible 2D-3D training pairs from MoCap and synthetic datasets. We discuss how these ingredients can enable fast, personalized, and accurate 3D pose estimation from monocular images without requiring specialized hardware. This proposal aims to foster discussion on bridging data-driven learning and model-based priors to improve accuracy, interpretability, and deployability of 3D human motion capture on edge devices in the wild.', 'abstract_zh': '单目3D人体姿态估计仍然是一个具有挑战性和病态的问题，特别是在实时设置和不受约束的环境中。虽然直接从图像到3D的方法需要大量标注的数据集和复杂的模型，但从2D到3D的提升提供了更轻量级和灵活的选择—尤其是在结合先验知识时。在本文中，我们提出了一种结合实时2D关键点检测与几何感知的2D到3D提升的框架，明确利用已知相机内参和个体特定的解剖先验知识。我们的方法建立在自我校准和生物力学约束逆运动学的最新进展之上，从动捕和合成数据集中生成大规模、合理的2D到3D训练对。我们讨论了这些成分如何能够实现快速、个性化且准确的单目图像3D姿态估计，无需专用硬件。本提案旨在促进关于结合数据驱动学习与模型驱动先验，以提高自然环境中基于边缘设备的人体运动捕捉准确度、可解释性和部署性的讨论。', 'title_zh': '面向实时的基于几何先验的准确单目3D人体姿态估计框架'}
{'arxiv_id': 'arXiv:2507.16849', 'title': 'Post-Disaster Affected Area Segmentation with a Vision Transformer (ViT)-based EVAP Model using Sentinel-2 and Formosat-5 Imagery', 'authors': 'Yi-Shan Chu, Hsuan-Cheng Wei', 'link': 'https://arxiv.org/abs/2507.16849', 'abstract': 'We propose a vision transformer (ViT)-based deep learning framework to refine disaster-affected area segmentation from remote sensing imagery, aiming to support and enhance the Emergent Value Added Product (EVAP) developed by the Taiwan Space Agency (TASA). The process starts with a small set of manually annotated regions. We then apply principal component analysis (PCA)-based feature space analysis and construct a confidence index (CI) to expand these labels, producing a weakly supervised training set. These expanded labels are then used to train ViT-based encoder-decoder models with multi-band inputs from Sentinel-2 and Formosat-5 imagery. Our architecture supports multiple decoder variants and multi-stage loss strategies to improve performance under limited supervision. During the evaluation, model predictions are compared with higher-resolution EVAP output to assess spatial coherence and segmentation consistency. Case studies on the 2022 Poyang Lake drought and the 2023 Rhodes wildfire demonstrate that our framework improves the smoothness and reliability of segmentation results, offering a scalable approach for disaster mapping when accurate ground truth is unavailable.', 'abstract_zh': '基于视景变换器的灾害影响区域遥感影像分割深度学习框架：支持台湾太空署Emergent Value Added Product（EVAP）的发展', 'title_zh': '基于Vision Transformer (ViT) 的EVAP模型：利用Sentinel-2和Formosat-5影像的灾害影响区域分割'}
