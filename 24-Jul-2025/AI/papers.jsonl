{'arxiv_id': 'arXiv:2507.17730', 'title': 'Online Submission and Evaluation System Design for Competition Operations', 'authors': 'Zhe Chen, Daniel Harabor, Ryan Hechnenberger, Nathan R. Sturtevant', 'link': 'https://arxiv.org/abs/2507.17730', 'abstract': 'Research communities have developed benchmark datasets across domains to compare the performance of algorithms and techniques However, tracking the progress in these research areas is not easy, as publications appear in different venues at the same time, and many of them claim to represent the state-of-the-art. To address this, research communities often organise periodic competitions to evaluate the performance of various algorithms and techniques, thereby tracking advancements in the field. However, these competitions pose a significant operational burden. The organisers must manage and evaluate a large volume of submissions. Furthermore, participants typically develop their solutions in diverse environments, leading to compatibility issues during the evaluation of their submissions. This paper presents an online competition system that automates the submission and evaluation process for a competition. The competition system allows organisers to manage large numbers of submissions efficiently, utilising isolated environments to evaluate submissions. This system has already been used successfully for several competitions, including the Grid-Based Pathfinding Competition and the League of Robot Runners competition.', 'abstract_zh': '研究社区在跨领域开发基准数据集以比较算法和技术性能，但跟踪这些研究领域的进展并不容易，因为研究成果同时出现在不同的出版平台上，且许多研究声称代表了当前最先进的水平。为了应对这一挑战，研究社区通常组织定期竞赛来评估各种算法和技术的表现，从而追踪领域的发展。然而，这些竞赛伴随着显著的操作负担。组织者必须管理并评估大量的提交。此外，参赛者通常在其解决方案开发的多种环境中进行，导致提交评估过程中出现兼容性问题。本文介绍了一个在线竞赛系统，该系统自动化了竞赛的提交和评估过程。竞赛系统使组织者能够高效管理大量的提交，并使用隔离环境来评估提交。该系统已经在多个比赛中成功应用，包括基于网格路径寻找竞赛和机器人跑步联盟竞赛。', 'title_zh': '竞赛运营的在线提交与评价系统设计'}
{'arxiv_id': 'arXiv:2507.17699', 'title': "Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning Models via Tool Augmentations", 'authors': 'Zhao Song, Song Yue, Jiahao Zhang', 'link': 'https://arxiv.org/abs/2507.17699', 'abstract': "Large Reasoning Models (LRMs) have become a central focus in today's large language model (LLM) research, where models are designed to output a step-by-step thinking process before arriving at a final answer to handle complex reasoning tasks. Despite their promise, recent empirical studies (e.g., [Shojaee et al., 2025] from Apple) suggest that this thinking process may not actually enhance reasoning ability, where LLMs without explicit reasoning actually outperform LRMs on tasks with low or high complexity. In this work, we revisit these findings and investigate whether the limitations of LRMs persist when tool augmentations are introduced. We incorporate two types of tools, Python interpreters and scratchpads, and evaluate three representative LLMs and their LRM counterparts on Apple's benchmark reasoning puzzles. Our results show that, with proper tool use, LRMs consistently outperform their non-reasoning counterparts across all levels of task complexity. These findings challenge the recent narrative that reasoning is an illusion and highlight the potential of tool-augmented LRMs for solving complex problems.", 'abstract_zh': '大型推理模型 (LRMs) 已成为当今大型语言模型 (LLMs) 研究的中心焦点，模型旨在在得出最终答案之前输出逐步推理过程以处理复杂的推理任务。尽管LRMs充满潜力，但近期的经验研究表明，这种推理过程可能实际上并未增强推理能力，未显式进行推理的LLMs在低复杂度或高复杂度任务中反而表现更佳。在本文中，我们重新审视这些发现，并调查引入工具辅助后LRMs的限制是否仍然存在。我们引入了Python解释器和草稿纸两种类型的工具，并在苹果公司的基准推理谜题上评估了三种代表性的LLMs及其对应的LRMs。结果表明，通过适当使用工具，LRMs在所有复杂度级别上均能超越其非推理版本。这些发现挑战了最近关于推理是一个幻觉的说法，并突显了工具增强的LRMs解决复杂问题的潜力。', 'title_zh': '思考并非幻觉：通过工具增强克服推理模型的局限性'}
{'arxiv_id': 'arXiv:2507.17695', 'title': 'Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks', 'authors': 'Ilias Chatzistefanidis, Navid Nikaein', 'link': 'https://arxiv.org/abs/2507.17695', 'abstract': "Large Language Model (LLM)-based autonomous agents are expected to play a vital role in the evolution of 6G networks, by empowering real-time decision-making related to management and service provisioning to end-users. This shift facilitates the transition from a specialized intelligence approach, where artificial intelligence (AI) algorithms handle isolated tasks, to artificial general intelligence (AGI)-driven networks, where agents possess broader reasoning capabilities and can manage diverse network functions. In this paper, we introduce a novel agentic paradigm that combines LLMs with real-time optimization algorithms towards Trustworthy AI, defined as symbiotic agents. Optimizers at the LLM's input-level provide bounded uncertainty steering for numerically precise tasks, whereas output-level optimizers supervised by the LLM enable adaptive real-time control. We design and implement two novel agent types including: (i) Radio Access Network optimizers, and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We further propose an end-to-end architecture for AGI networks and evaluate it on a 5G testbed capturing channel fluctuations from moving vehicles. Results show that symbiotic agents reduce decision errors fivefold compared to standalone LLM-based agents, while smaller language models (SLM) achieve similar accuracy with a 99.9% reduction in GPU resource overhead and in near-real-time loops of 82 ms. A multi-agent demonstration for collaborative RAN on the real-world testbed highlights significant flexibility in service-level agreement and resource allocation, reducing RAN over-utilization by approximately 44%. Drawing on our findings and open-source implementations, we introduce the symbiotic paradigm as the foundation for next-generation, AGI-driven networks-systems designed to remain adaptable, efficient, and trustworthy even as LLMs advance.", 'abstract_zh': '基于大型语言模型的自主代理在6G网络演进中的作用：可信赖人工智能的共生代理 Paradigm', 'title_zh': '共生代理：一种 trustworthy AGI 驱动网络的新范式'}
{'arxiv_id': 'arXiv:2507.17680', 'title': 'Simulating multiple human perspectives in socio-ecological systems using large language models', 'authors': 'Yongchao Zeng, Calum Brown, Ioannis Kyriakou, Ronja Hotz, Mark Rounsevell', 'link': 'https://arxiv.org/abs/2507.17680', 'abstract': 'Understanding socio-ecological systems requires insights from diverse stakeholder perspectives, which are often hard to access. To enable alternative, simulation-based exploration of different stakeholder perspectives, we develop the HoPeS (Human-Oriented Perspective Shifting) modelling framework. HoPeS employs agents powered by large language models (LLMs) to represent various stakeholders; users can step into the agent roles to experience perspectival differences. A simulation protocol serves as a "scaffold" to streamline multiple perspective-taking simulations, supporting users in reflecting on, transitioning between, and integrating across perspectives. A prototype system is developed to demonstrate HoPeS in the context of institutional dynamics and land use change, enabling both narrative-driven and numerical experiments. In an illustrative experiment, a user successively adopts the perspectives of a system observer and a researcher - a role that analyses data from the embedded land use model to inform evidence-based decision-making for other LLM agents representing various institutions. Despite the user\'s effort to recommend technically sound policies, discrepancies persist between the policy recommendation and implementation due to stakeholders\' competing advocacies, mirroring real-world misalignment between researcher and policymaker perspectives. The user\'s reflection highlights the subjective feelings of frustration and disappointment as a researcher, especially due to the challenge of maintaining political neutrality while attempting to gain political influence. Despite this, the user exhibits high motivation to experiment with alternative narrative framing strategies, suggesting the system\'s potential in exploring different perspectives. Further system and protocol refinement are likely to enable new forms of interdisciplinary collaboration in socio-ecological simulations.', 'abstract_zh': '理解社会生态系统需要从多利益相关方视角获取见解，但这些视角往往难以获取。为使不同利益相关方视角的替代性、基于模拟的探索成为可能，我们开发了HoPeS（Human-Oriented Perspective Shifting）建模框架。HoPeS 利用大型语言模型（LLMs）驱动的代理来代表各种利益相关方；用户可以扮演这些代理角色以体验不同的视角差异。一种模拟协议作为“支架”，简化了多视角换位模拟的流程，支持用户反思、转换和整合不同视角。一个原型系统被开发出来，以展示HoPeS在机构动态和土地利用变化中的应用，允许进行基于叙事和数值的实验。在一个示例性实验中，用户依次扮演系统观察者和研究人员的角色——后者基于嵌入的土地利用模型的数据进行分析，以指导其他代表各种机构的LLM代理做出基于证据的决策。尽管用户努力推荐技术上合理的政策，但由于利益相关方的竞争性诉求，政策建议与实施之间仍存在差异，这反映了研究人员和政策制定者视角间的真实世界偏差。用户的反思突显了作为研究人员的主观挫败感和失望感，尤其是在试图保持政治中立的同时争取政治影响力方面遇到的挑战。然而，尽管如此，用户显示出很高的实验不同叙事框架策略的动力，这表明系统在探索不同视角方面具有潜力。进一步的系统和协议改进很可能使跨学科协作在社会生态模拟中成为可能。', 'title_zh': '使用大型语言模型在社会-生态系统中模拟多重人类视角'}
{'arxiv_id': 'arXiv:2507.17539', 'title': 'Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning', 'authors': 'Xinyao Liu, Diping Song', 'link': 'https://arxiv.org/abs/2507.17539', 'abstract': "Multimodal large language models (MLLMs) demonstrate significant potential in the field of medical diagnosis. However, they face critical challenges in specialized domains such as ophthalmology, particularly the fragmentation of annotation granularity and inconsistencies in clinical reasoning logic, which hinder precise cross-modal understanding. This paper introduces FundusExpert, an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning capabilities, along with FundusGen, a dataset constructed through the intelligent Fundus-Engine system. Fundus-Engine automates localization and leverages MLLM-based semantic expansion to integrate global disease classification, local object detection, and fine-grained feature analysis within a single fundus image. Additionally, by constructing a clinically aligned cognitive chain, it guides the model to generate interpretable reasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen, achieves the best performance in ophthalmic question-answering tasks, surpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in zero-shot report generation tasks, achieving a clinical consistency of 77.0%, significantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling law between data quality and model capability ($L \\propto N^{0.068}$), demonstrating that the cognitive alignment annotations in FundusGen enhance data utilization efficiency. By integrating region-level localization with diagnostic reasoning chains, our work develops a scalable, clinically-aligned MLLM and explores a pathway toward bridging the visual-language gap in specific MLLMs. Our project can be found at this https URL.", 'abstract_zh': '多模态大规模语言模型（MLLMs）在医疗诊断领域展现出显著潜力，特别是在眼科这一专业领域。然而，它们在眼科领域面临着标注细粒度碎片化和临床推理逻辑不一致等关键挑战，这阻碍了跨模态的精确理解。本文介绍了眼科专用的MLLM——FundusExpert，以及通过智能Fundus-Engine系统构建的FundusGen数据集。Fundus-Engine实现了区域级别的定位，并借助基于MLLM的语义扩展，在单张眼底图像中整合了全球疾病分类、局部对象检测和精细特征分析。此外，通过构建临床对齐的认知链条，它引导模型生成可解释的推理路径。经过FundusGen指令数据微调后，FundusExpert在眼科问答任务中取得了最佳性能，超越了平均准确率为40B MedRegA的26.6%。同时，在零样本报告生成任务中，其临床一致性为77.0%，远远超过GPT-4o的47.6%。此外，我们揭示了数据质量和模型能力之间的标度律（$L \\propto N^{0.068}$），表明FundusGen中的认知对齐标注提高了数据利用效率。通过结合区域级别的定位和诊断推理链条，我们的工作开发了一种可扩展、临床对齐的MLLM，并探索了特定MLLM中视觉-语言差距的桥梁构建路径。更多信息请访问：this https URL。', 'title_zh': '通过临床认知链推理构建眼科MLLM以实现定位-诊断协作'}
{'arxiv_id': 'arXiv:2507.17514', 'title': 'TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy AI Self-Assessment', 'authors': 'Athanasios Davvetas, Xenia Ziouvelou, Ypatia Dami, Alexis Kaponis, Konstantina Giouvanopoulou, Michael Papademas', 'link': 'https://arxiv.org/abs/2507.17514', 'abstract': "This paper introduces the TAI Scan Tool, a RAG-based TAI self-assessment tool with minimalistic input. The current version of the tool supports the legal TAI assessment, with a particular emphasis on facilitating compliance with the AI Act. It involves a two-step approach with a pre-screening and an assessment phase. The assessment output of the system includes insight regarding the risk-level of the AI system according to the AI Act, while at the same time retrieving relevant articles to aid with compliance and notify on their obligations. Our qualitative evaluation using use-case scenarios yields promising results, correctly predicting risk levels while retrieving relevant articles across three distinct semantic groups. Furthermore, interpretation of results shows that the tool's reasoning relies on comparison with the setting of high-risk systems, a behaviour attributed to their deployment requiring careful consideration, and therefore frequently presented within the AI Act.", 'abstract_zh': '基于RAG的TAI自评估工具TAI Scan Tool及其minimalistic输入方法：促进AI法案合规的双重评估框架', 'title_zh': 'TAI扫描工具：一种基于RAG的 minimalist 输入以实现可信AI自我评估的工具'}
{'arxiv_id': 'arXiv:2507.17512', 'title': 'Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning', 'authors': 'Yu Li, Zhuoshi Pan, Honglin Lin, Mengyuan Sun, Conghui He, Lijun Wu', 'link': 'https://arxiv.org/abs/2507.17512', 'abstract': "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing research has predominantly concentrated on isolated reasoning domains such as mathematical problem-solving, coding tasks, or logical reasoning. However, real world reasoning scenarios inherently demand an integrated application of multiple cognitive skills. Despite this, the interplay among these reasoning skills under reinforcement learning remains poorly understood. To bridge this gap, we present a systematic investigation of multi-domain reasoning within the RLVR framework, explicitly focusing on three primary domains: mathematical reasoning, code generation, and logical puzzle solving. We conduct a comprehensive study comprising four key components: (1) Leveraging the GRPO algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the models' in-domain improvements and cross-domain generalization capabilities when trained on single-domain datasets. (2) Additionally, we examine the intricate interactions including mutual enhancements and conflicts that emerge during combined cross-domain training. (3) To further understand the influence of SFT on RL, we also analyze and compare performance differences between base and instruct models under identical RL configurations. (4) Furthermore, we delve into critical RL training details, systematically exploring the impacts of curriculum learning strategies, variations in reward design, and language-specific factors. Through extensive experiments, our results offer significant insights into the dynamics governing domain interactions, revealing key factors influencing both specialized and generalizable reasoning performance. These findings provide valuable guidance for optimizing RL methodologies to foster comprehensive, multi-domain reasoning capabilities in LLMs.", 'abstract_zh': '可验证奖励的强化学习（RLVR）已经 emerged 作为增强大型语言模型推理能力的强大范式。现有研究主要集中在数学问题解决、编码任务或逻辑推理等孤立的推理领域。然而，现实世界的推理场景要求多种认知技能的综合应用。尽管如此，这些推理技能在强化学习中的相互作用依然知之甚少。为弥合这一差距，我们系统地探讨了 RLVR 框架下的多域推理，明确专注于三个主要领域：数学推理、代码生成和逻辑谜题解决。我们的研究包括四个关键组成部分：（1）利用 GRPO 算法和 Qwen-2.5-7B 模型家族，我们全面评估了单域数据集训练下模型的领域内改进和跨域泛化能力；（2）此外，我们还研究了在联合跨域训练过程中出现的复杂交互，包括相互增强和冲突；（3）为进一步理解 SFT 对 RL 的影响，我们还分析并比较了在相同 RL 配置下基础模型和指令模型的性能差异；（4）此外，我们深入探讨了关键的 RL 训练细节，系统地探索了逐级学习策略、奖励设计的变化以及语言特定因素的影响。通过大量实验，我们的结果提供了关于领域交互动力学的重要见解，揭示了影响专业和泛化推理性能的关键因素。这些发现为优化 RL 方法以促进大型语言模型的全面、多域推理能力提供了宝贵的指导。', 'title_zh': '一域之畔，众域受益：基于数据的多域推理研究——通过强化学习实现'}
{'arxiv_id': 'arXiv:2507.17493', 'title': 'Automated Hybrid Grounding Using Structural and Data-Driven Heuristics', 'authors': 'Alexander Beiser, Markus Hecher, Stefan Woltran', 'link': 'https://arxiv.org/abs/2507.17493', 'abstract': 'The grounding bottleneck poses one of the key challenges that hinders the widespread adoption of Answer Set Programming in industry. Hybrid Grounding is a step in alleviating the bottleneck by combining the strength of standard bottom-up grounding with recently proposed techniques where rule bodies are decoupled during grounding. However, it has remained unclear when hybrid grounding shall use body-decoupled grounding and when to use standard bottom-up grounding. In this paper, we address this issue by developing automated hybrid grounding: we introduce a splitting algorithm based on data-structural heuristics that detects when to use body-decoupled grounding and when standard grounding is beneficial. We base our heuristics on the structure of rules and an estimation procedure that incorporates the data of the instance. The experiments conducted on our prototypical implementation demonstrate promising results, which show an improvement on hard-to-ground scenarios, whereas on hard-to-solve instances we approach state-of-the-art performance.', 'abstract_zh': '基于数据结构启发式的自动混合接地方法', 'title_zh': '基于结构与数据驱动启发式的自动化混合接地方法'}
{'arxiv_id': 'arXiv:2507.17487', 'title': 'CQE under Epistemic Dependencies: Algorithms and Experiments (extended version)', 'authors': 'Lorenzo Marconi, Flavia Ricci, Riccardo Rosati', 'link': 'https://arxiv.org/abs/2507.17487', 'abstract': 'We investigate Controlled Query Evaluation (CQE) over ontologies, where information disclosure is regulated by epistemic dependencies (EDs), a family of logical rules recently proposed for the CQE framework. In particular, we combine EDs with the notion of optimal GA censors, i.e. maximal sets of ground atoms that are entailed by the ontology and can be safely revealed. We focus on answering Boolean unions of conjunctive queries (BUCQs) with respect to the intersection of all optimal GA censors - an approach that has been shown in other contexts to ensure strong security guarantees with favorable computational behavior. First, we characterize the security of this intersection-based approach and identify a class of EDs (namely, full EDs) for which it remains safe. Then, for a subclass of EDs and for DL-Lite_R ontologies, we show that answering BUCQs in the above CQE semantics is in AC^0 in data complexity by presenting a suitable, detailed first-order rewriting algorithm. Finally, we report on experiments conducted in two different evaluation scenarios, showing the practical feasibility of our rewriting function.', 'abstract_zh': '我们研究基于命题依赖关系（EDs）调控信息披露的知识本体上的受控查询评估（CQE）。特别是，我们将命题依赖关系与最优GA滤器的概念结合，即满足本体且可以安全揭示的最大底原子集。我们关注在所有最优GA滤器的交集中回答布尔合取查询（BUCQs）。这种方法在其他上下文中已被证明能够提供强大的安全保证并具有有利的计算行为。首先，我们刻画了这种基于交集的方法的安全性，并识别出一类对于其仍然安全的命题依赖关系（即全命题依赖关系）。然后，对于命题依赖关系的一个子类以及DL-Lite_R本体，我们通过展示合适的、详细的一阶重写算法，证明在数据复杂性上以AC^0回答上述CQE语义下的BUCQs。最后，我们在两个不同的评估场景中进行了实验，展示了我们重写函数的实用可行性。', 'title_zh': '知识依赖下的azine质量检测：算法与实验（扩展版）'}
{'arxiv_id': 'arXiv:2507.17482', 'title': 'LTLZinc: a Benchmarking Framework for Continual Learning and Neuro-Symbolic Temporal Reasoning', 'authors': 'Luca Salvatore Lorello, Nikolaos Manginas, Marco Lippi, Stefano Melacci', 'link': 'https://arxiv.org/abs/2507.17482', 'abstract': 'Neuro-symbolic artificial intelligence aims to combine neural architectures with symbolic approaches that can represent knowledge in a human-interpretable formalism. Continual learning concerns with agents that expand their knowledge over time, improving their skills while avoiding to forget previously learned concepts. Most of the existing approaches for neuro-symbolic artificial intelligence are applied to static scenarios only, and the challenging setting where reasoning along the temporal dimension is necessary has been seldom explored. In this work we introduce LTLZinc, a benchmarking framework that can be used to generate datasets covering a variety of different problems, against which neuro-symbolic and continual learning methods can be evaluated along the temporal and constraint-driven dimensions. Our framework generates expressive temporal reasoning and continual learning tasks from a linear temporal logic specification over MiniZinc constraints, and arbitrary image classification datasets. Fine-grained annotations allow multiple neural and neuro-symbolic training settings on the same generated datasets. Experiments on six neuro-symbolic sequence classification and four class-continual learning tasks generated by LTLZinc, demonstrate the challenging nature of temporal learning and reasoning, and highlight limitations of current state-of-the-art methods. We release the LTLZinc generator and ten ready-to-use tasks to the neuro-symbolic and continual learning communities, in the hope of fostering research towards unified temporal learning and reasoning frameworks.', 'abstract_zh': '神经符号人工智能旨在结合神经架构与可由人类解释的形式化知识表示的符号方法。连续学习关注能够随时间扩展知识的代理，改进技能同时避免遗忘之前学习的概念。大多数现有的神经符号人工智能方法仅应用于静态场景，而需要在时间维度上进行推理的挑战性设置鲜有探索。在本文中，我们介绍了LTLZinc基准框架，可用于生成涵盖多种不同问题的数据集，以供神经符号和连续学习方法在时间维度和驱动约束的维度上进行评估。我们的框架从MiniZinc约束和线性时序逻辑规范生成具有表现力的时间推理和连续学习任务，并可适配任意图像分类数据集。精细标注允许在相同的生成数据集上进行多种神经和神经符号训练设置。LTLZinc生成的六个神经符号序列分类任务和四个类连续学习任务的实验展示了时间学习与推理的挑战性，并突显了当前最先进的方法的局限性。我们发布了LTLZinc生成器和十个即用型任务，希望促进统一的时间学习与推理框架的研究。', 'title_zh': 'LTLZinc：连续学习和神经符号时间推理的基准框架'}
{'arxiv_id': 'arXiv:2507.17477', 'title': 'An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models', 'authors': 'Haoran Sun, Zekun Zhang, Shaoning Zeng', 'link': 'https://arxiv.org/abs/2507.17477', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable progress in instruction following and general-purpose reasoning. However, achieving high-quality alignment with human intent and safety norms without human annotations remains a fundamental challenge. In this work, we propose an Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to improve LLM alignment in a fully automated manner. UDASA first generates multiple responses for each input and quantifies output uncertainty across three dimensions: semantics, factuality, and value alignment. Based on these uncertainty scores, the framework constructs preference pairs and categorizes training samples into three stages, conservative, moderate, and exploratory, according to their uncertainty difference. The model is then optimized progressively across these stages. In addition, we conduct a series of preliminary studies to validate the core design assumptions and provide strong empirical motivation for the proposed framework. Experimental results show that UDASA outperforms existing alignment methods across multiple tasks, including harmlessness, helpfulness, truthfulness, and controlled sentiment generation, significantly improving model performance.', 'abstract_zh': '大型语言模型（LLMs）在指令遵循和通用推理方面取得了显著进展。然而，在没有人类标注的情况下实现与人类意图和安全规范的高质量化身仍是一个基本挑战。本文提出了一种不确定性驱动的自适应自我对齐（UDASA）框架，旨在以完全自动的方式改进LLM对齐。UDASA首先为每个输入生成多个响应，并从语义、事实性和价值对齐三个维度量化输出不确定性。基于这些不确定性评分，框架构建偏好对并根据不确定性差异将训练样本分为保守、适度和探索性三个阶段。然后，模型在这三个阶段中逐步优化。此外，我们进行了一系列初步研究以验证核心设计假设，并为提出的框架提供了强大的实证支持。实验结果表明，UDASA在多个任务（包括无害性、帮助性、真实性以及可控情感生成）上优于现有对齐方法，显著提高了模型性能。', 'title_zh': '基于不确定性驱动的自适应自我对齐框架：应用于大型语言模型'}
{'arxiv_id': 'arXiv:2507.17418', 'title': 'Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning', 'authors': 'Joobin Jin, Seokjun Hong, Gyeongseon Baek, Yeeun Kim, Byeongjoon Noh', 'link': 'https://arxiv.org/abs/2507.17418', 'abstract': 'Precise modeling of microscopic vehicle trajectories is critical for traffic behavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a context-aware trajectory generation framework that synthesizes realistic urban driving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses nonlinear interdependencies and training instability inherent in microscopic settings. By explicitly conditioning on surrounding vehicles and road geometry, Ctx2TrajGen generates interaction-aware trajectories aligned with real-world context. Experiments on the drone-captured DRIFT dataset demonstrate superior performance over existing methods in terms of realism, behavioral diversity, and contextual fidelity, offering a robust solution to data scarcity and domain shift without simulation.', 'abstract_zh': '基于GAIL的上下文感知轨迹生成框架Ctx2TrajGen：面向交通行为分析和自主驾驶系统的小尺度车辆轨迹精准建模', 'title_zh': 'Ctx2TrajGen：基于交通情境的生成对抗 imitation 学习微尺度车辆轨迹生成'}
{'arxiv_id': 'arXiv:2507.17289', 'title': 'Compliance Brain Assistant: Conversational Agentic AI for Assisting Compliance Tasks in Enterprise Environments', 'authors': 'Shitong Zhu, Chenhao Fang, Derek Larson, Neel Reddy Pochareddy, Rajeev Rao, Sophie Zeng, Yanqing Peng, Wendy Summer, Alex Goncalves, Arya Pudota, Herve Robert', 'link': 'https://arxiv.org/abs/2507.17289', 'abstract': "This paper presents Compliance Brain Assistant (CBA), a conversational, agentic AI assistant designed to boost the efficiency of daily compliance tasks for personnel in enterprise environments. To strike a good balance between response quality and latency, we design a user query router that can intelligently choose between (i) FastTrack mode: to handle simple requests that only need additional relevant context retrieved from knowledge corpora; and (ii) FullAgentic mode: to handle complicated requests that need composite actions and tool invocations to proactively discover context across various compliance artifacts, and/or involving other APIs/models for accommodating requests. A typical example would be to start with a user query, use its description to find a specific entity and then use the entity's information to query other APIs for curating and enriching the final AI response.\nOur experimental evaluations compared CBA against an out-of-the-box LLM on various real-world privacy/compliance-related queries targeting various personas. We found that CBA substantially improved upon the vanilla LLM's performance on metrics such as average keyword match rate (83.7% vs. 41.7%) and LLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full routing-based design against the `fast-track only` and `full-agentic` modes and found that it had a better average match-rate and pass-rate while keeping the run-time approximately the same. This finding validated our hypothesis that the routing mechanism leads to a good trade-off between the two worlds.", 'abstract_zh': '基于对话的企业环境合规助手：Compliance Brain Assistant', 'title_zh': '合规大脑助手：企业环境中辅助合规任务的对话型代理AI'}
{'arxiv_id': 'arXiv:2507.17258', 'title': "Students' Feedback Requests and Interactions with the SCRIPT Chatbot: Do They Get What They Ask For?", 'authors': 'Andreas Scholl, Natalie Kiesler', 'link': 'https://arxiv.org/abs/2507.17258', 'abstract': "Building on prior research on Generative AI (GenAI) and related tools for programming education, we developed SCRIPT, a chatbot based on ChatGPT-4o-mini, to support novice learners. SCRIPT allows for open-ended interactions and structured guidance through predefined prompts. We evaluated the tool via an experiment with 136 students from an introductory programming course at a large German university and analyzed how students interacted with SCRIPT while solving programming tasks with a focus on their feedback preferences. The results reveal that students' feedback requests seem to follow a specific sequence. Moreover, the chatbot responses aligned well with students' requested feedback types (in 75%), and it adhered to the system prompt constraints. These insights inform the design of GenAI-based learning support systems and highlight challenges in balancing guidance and flexibility in AI-assisted tools.", 'abstract_zh': '基于前期关于生成人工智能（GenAI）及其相关编程教育工具的研究，我们开发了SCRIPT，这是一种基于ChatGPT-4o-mini的聊天机器人，旨在支持初学者学习。SCRIPT通过预定义提示实现开放式互动和结构化指导。我们通过一项在大型德国大学入门级编程课程中进行的实验评估了该工具，并分析了学生在解决编程任务时与SCRIPT互动的情况，重点关注他们的反馈偏好。研究结果表明，学生的反馈请求似乎遵循一定的顺序。此外，聊天机器人的响应与学生请求的反馈类型高度一致（75%），并遵循系统提示约束。这些洞见为基于生成人工智能的学习支持系统的设计提供了指导，并突显了在人工智能辅助工具中平衡指导与灵活性的挑战。', 'title_zh': '学生对SCRIPT聊天机器人的反馈请求及其互动：他们是否得到了所需的内容？'}
{'arxiv_id': 'arXiv:2507.17257', 'title': 'Agent Identity Evals: Measuring Agentic Identity', 'authors': 'Elija Perrier, Michael Timothy Bennett', 'link': 'https://arxiv.org/abs/2507.17257', 'abstract': 'Central to agentic capability and trustworthiness of language model agents (LMAs) is the extent they maintain stable, reliable, identity over time. However, LMAs inherit pathologies from large language models (LLMs) (statelessness, stochasticity, sensitivity to prompts and linguistically-intermediation) which can undermine their identifiability, continuity, persistence and consistency. This attrition of identity can erode their reliability, trustworthiness and utility by interfering with their agentic capabilities such as reasoning, planning and action. To address these challenges, we introduce \\textit{agent identity evals} (AIE), a rigorous, statistically-driven, empirical framework for measuring the degree to which an LMA system exhibit and maintain their agentic identity over time, including their capabilities, properties and ability to recover from state perturbations. AIE comprises a set of novel metrics which can integrate with other measures of performance, capability and agentic robustness to assist in the design of optimal LMA infrastructure and scaffolding such as memory and tools. We set out formal definitions and methods that can be applied at each stage of the LMA life-cycle, and worked examples of how to apply them.', 'abstract_zh': '语言模型代理（LMAs）的代理能力和可信度的核心在于它们在时间上维持稳定可靠身份的程度。然而，LMAs 从大型语言模型（LLMs）继承了病态特征（无状态性、随机性、对提示的敏感性和语义中介性），这些特征可能削弱它们的身份可识别性、连续性、持久性和一致性。这种身份的流失可能通过干扰它们的代理能力（如推理、规划和行动）来削弱它们的可靠性、可信度和实用性。为应对这些挑战，我们引入了“代理身份评估”（AIE），这是一种严谨的、以统计为驱动的实证框架，用于衡量LMA系统在其运行过程中展现和维持其代理身份的程度，包括其能力、属性及其从状态干扰中恢复的能力。AIE 包含一系列新型度量标准，可以与其他性能、能力和代理鲁棒性的度量标准结合使用，以协助设计最优的LMA基础设施和支撑工具（如记忆和工具）。我们提出了形式化定义和方法，可以在LMA生命周期的每个阶段应用，并提供了如何应用这些方法的实例。', 'title_zh': '代理身份评估：测量代理身份'}
{'arxiv_id': 'arXiv:2507.17214', 'title': 'Our Cars Can Talk: How IoT Brings AI to Vehicles', 'authors': 'Amod Kant Agrawal', 'link': 'https://arxiv.org/abs/2507.17214', 'abstract': 'Bringing AI to vehicles and enabling them as sensing platforms is key to transforming maintenance from reactive to proactive. Now is the time to integrate AI copilots that speak both languages: machine and driver. This article offers a conceptual and technical perspective intended to spark interdisciplinary dialogue and guide future research and development in intelligent vehicle systems, predictive maintenance, and AI-powered user interaction.', 'abstract_zh': '将AI引入车辆并使其成为感官平台，是将维护从被动转变为主动的关键。现在是时候整合既懂机器又懂驾驶者的AI偕驾者了。本文提供了一个概念性和技术性的视角，旨在激发跨学科对话，并引导未来在智能车辆系统、预测性维护和AI驱动的用户交互方面的研究与开发。', 'title_zh': '我们的汽车能交谈：物联网如何为车辆带来人工智能'}
{'arxiv_id': 'arXiv:2507.17168', 'title': "Improving LLMs' Generalized Reasoning Abilities by Graph Problems", 'authors': 'Qifan Zhang, Nuo Chen, Zehua Li, Miao Peng, Jing Tang, Jia Li', 'link': 'https://arxiv.org/abs/2507.17168', 'abstract': 'Large Language Models (LLMs) have made remarkable strides in reasoning tasks, yet their performance often falters on novel and complex problems. Domain-specific continued pretraining (CPT) methods, such as those tailored for mathematical reasoning, have shown promise but lack transferability to broader reasoning tasks. In this work, we pioneer the use of Graph Problem Reasoning (GPR) to enhance the general reasoning capabilities of LLMs. GPR tasks, spanning pathfinding, network analysis, numerical computation, and topological reasoning, require sophisticated logical and relational reasoning, making them ideal for teaching diverse reasoning patterns. To achieve this, we introduce GraphPile, the first large-scale corpus specifically designed for CPT using GPR data. Spanning 10.9 billion tokens across 23 graph tasks, the dataset includes chain-of-thought, program-of-thought, trace of execution, and real-world graph data. Using GraphPile, we train GraphMind on popular base models Llama 3 and 3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in mathematical reasoning and up to 21.2 percent improvement in non-mathematical reasoning tasks such as logical and commonsense reasoning. By being the first to harness GPR for enhancing reasoning patterns and introducing the first dataset of its kind, our work bridges the gap between domain-specific pretraining and universal reasoning capabilities, advancing the adaptability and robustness of LLMs.', 'abstract_zh': '大型语言模型（LLMs）在推理任务中取得了显著进展，但在处理新颖和复杂问题时表现往往不佳。针对特定领域的持续预训练（CPT）方法，如数学推理领域，显示出了潜力，但缺乏向更广泛推理任务的迁移能力。本工作中，我们首次采用图问题推理（GPR）来增强LLMs的一般推理能力。GPR任务涵盖路径查找、网络分析、数值计算和拓扑推理等内容，需要复杂的逻辑和关系推理，使其成为传授多样化推理模式的理想选择。为此，我们引入了GraphPile，这是首个专门用于GPR数据的大型语料库，用于CPT。GraphPile包含来自23个图任务的109亿个 Tokens，其中包括推理链、思维程序、执行轨迹以及实际图数据。利用GraphPile，我们在流行的基模型Llama 3和3.1以及Gemma 2上训练了GraphMind，数学推理准确率提高了4.9%，非数学推理任务，如逻辑推理和常识推理，提高了21.2%。通过首次利用GPR来增强推理模式，并引入首个此类数据集，我们的工作填补了领域特定预训练与通用推理能力之间的空白，推动了LLMs的适应性和 robustness。', 'title_zh': '通过图问题提高LLMs的泛化推理能力'}
{'arxiv_id': 'arXiv:2507.17118', 'title': 'HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study', 'authors': 'Mandar Pitale, Jelena Frtunikj, Abhinaw Priyadershi, Vasu Singh, Maria Spence', 'link': 'https://arxiv.org/abs/2507.17118', 'abstract': 'AI has become integral to safety-critical areas like autonomous driving systems (ADS) and robotics. The architecture of recent autonomous systems are trending toward end-to-end (E2E) monolithic architectures such as large language models (LLMs) and vision language models (VLMs). In this paper, we review different architectural solutions and then evaluate the efficacy of common safety analyses such as failure modes and effect analysis (FMEA) and fault tree analysis (FTA). We show how these techniques can be improved for the intricate nature of the foundational models, particularly in how they form and utilize latent representations. We introduce HySAFE-AI, Hybrid Safety Architectural Analysis Framework for AI Systems, a hybrid framework that adapts traditional methods to evaluate the safety of AI systems. Lastly, we offer hints of future work and suggestions to guide the evolution of future AI safety standards.', 'abstract_zh': 'AI已在自主驾驶系统（ADS）和机器人等安全关键领域中发挥着核心作用。 recent autonomous systems的架构趋势是采用端到端（E2E）的单块架构，如大型语言模型（LLMs）和视觉语言模型（VLMs）。本文回顾了不同的架构解决方案，并评估了失效模式和影响分析（FMEA）以及故障树分析（FTA）等常见安全分析技术的有效性。我们展示了如何改进这些技术以适应基础模型的复杂性，特别是它们如何形成和利用潜在表示。我们提出了一种新的混合框架HySAFE-AI，即AI系统混合安全架构分析框架，该框架将传统方法适应用于评估AI系统的安全性。最后，我们提供了未来工作的提示，并提出了指导未来AI安全标准演进的建议。', 'title_zh': 'HySafe-AI：人工智能系统的混合安全架构分析框架：一个案例研究'}
{'arxiv_id': 'arXiv:2507.17075', 'title': 'LoRA is All You Need for Safety Alignment of Reasoning LLMs', 'authors': 'Yihao Xue, Baharan Mirzasoleiman', 'link': 'https://arxiv.org/abs/2507.17075', 'abstract': 'Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex problems that were previously out of reach. To ensure LLMs do not assist with harmful requests, safety alignment fine-tuning is necessary in the post-training phase. However, safety alignment fine-tuning has recently been shown to significantly degrade reasoning abilities, a phenomenon known as the "Safety Tax". In this work, we show that using LoRA for SFT on refusal datasets effectively aligns the model for safety without harming its reasoning capabilities. This is because restricting the safety weight updates to a low-rank space minimizes the interference with the reasoning weights. Our extensive experiments across four benchmarks covering math, science, and coding show that this approach produces highly safe LLMs -- with safety levels comparable to full-model fine-tuning -- without compromising their reasoning abilities. Additionally, we observe that LoRA induces weight updates with smaller overlap with the initial weights compared to full-model fine-tuning. We also explore methods that further reduce such overlap -- via regularization or during weight merging -- and observe some improvement on certain tasks. We hope this result motivates designing approaches that yield more consistent improvements in the reasoning-safety trade-off.', 'abstract_zh': '利用LoRA进行拒绝数据集上的SFT有效提升了模型的安全性而不损害其推理能力', 'title_zh': 'LoRA即为实现推理大语言模型安全对齐所需的一切'}
{'arxiv_id': 'arXiv:2507.17054', 'title': 'New Mechanisms in Flex Distribution for Bounded Suboptimal Multi-Agent Path Finding', 'authors': 'Shao-Hung Chan, Thomy Phan, Jiaoyang Li, Sven Koenig', 'link': 'https://arxiv.org/abs/2507.17054', 'abstract': 'Multi-Agent Path Finding (MAPF) is the problem of finding a set of collision-free paths, one for each agent in a shared environment. Its objective is to minimize the sum of path costs (SOC), where the path cost of each agent is defined as the travel time from its start location to its target location. Explicit Estimation Conflict-Based Search (EECBS) is the leading algorithm for bounded-suboptimal MAPF, with the SOC of the solution being at most a user-specified factor $w$ away from optimal. EECBS maintains sets of paths and a lower bound $LB$ on the optimal SOC. Then, it iteratively selects a set of paths whose SOC is at most $w \\cdot LB$ and introduces constraints to resolve collisions. For each path in a set, EECBS maintains a lower bound on its optimal path that satisfies constraints. By finding an individually bounded-suboptimal path with cost at most a threshold of $w$ times its lower bound, EECBS guarantees to find a bounded-suboptimal solution. To speed up EECBS, previous work uses flex distribution to increase the threshold. Though EECBS with flex distribution guarantees to find a bounded-suboptimal solution, increasing the thresholds may push the SOC beyond $w \\cdot LB$, forcing EECBS to switch among different sets of paths instead of resolving collisions on a particular set of paths, and thus reducing efficiency. To address this issue, we propose Conflict-Based Flex Distribution that distributes flex in proportion to the number of collisions. We also estimate the delays needed to satisfy constraints and propose Delay-Based Flex Distribution. On top of that, we propose Mixed-Strategy Flex Distribution, combining both in a hierarchical framework. We prove that EECBS with our new flex distribution mechanisms is complete and bounded-suboptimal. Our experiments show that our approaches outperform the original (greedy) flex distribution.', 'abstract_zh': '多智能体路径寻找（MAPF）是寻找一个共享环境中每个智能体的碰撞自由路径的问题。其目标是最小化路径成本总和（SOC），其中每个智能体的路径成本定义为其起始位置到目标位置的旅行时间。显式估计冲突基搜索（EECBS）是用于近似最优MAPF的领先算法，解决方案的SOC最多偏离最优值一个用户指定的因素$w$。EECBS维护路径集和最优路径成本下界（LB）。然后，它迭代地选择一个SOC最多为$w \\cdot LB$的路径集，并引入约束以解决冲突。对于路径集中的每条路径，EECBS维护一个满足约束的最优路径下界。通过找到一个单个路径成本最多为下界阈值$w$倍的近似最优路径，EECBS保证找到一个近似最优解。为了加速EECBS，以前的工作使用灵活分布来增加阈值。尽管带有灵活分布的EECBS保证能找到一个近似最优解，但增加阈值可能会使SOC超过$w \\cdot LB$，迫使EECBS在不同路径集之间切换而不是在一个特定路径集上解决冲突，从而降低效率。为了解决这一问题，我们提出了按冲突比例分配灵活度的冲突基灵活分布，并估计满足约束所需的延迟并提出了基于延迟的灵活分布。在此基础上，我们提出了混合策略灵活分布，结合了这两种方法。我们证明了带有我们新灵活分布机制的EECBS是完整的和近似最优的。我们的实验表明，我们的方法优于原始（贪心）灵活分布。', 'title_zh': '在约束次优化多代理路径寻找中的柔性分布新机制'}
{'arxiv_id': 'arXiv:2507.17012', 'title': 'Towards Autonomous Sustainability Assessment via Multimodal AI Agents', 'authors': 'Zhihan Zhang, Alexander Metzger, Yuxuan Mei, Felix Hähnlein, Zachary Englhardt, Tingyu Cheng, Gregory D. Abowd, Shwetak Patel, Adriana Schulz, Vikram Iyer', 'link': 'https://arxiv.org/abs/2507.17012', 'abstract': 'Interest in sustainability information has surged in recent years. However, the data required for a life cycle assessment (LCA) that maps the materials and processes from product manufacturing to disposal into environmental impacts (EI) are often unavailable. Here we reimagine conventional LCA by introducing multimodal AI agents that emulate interactions between LCA experts and stakeholders like product managers and engineers to calculate the cradle-to-gate (production) carbon emissions of electronic devices. The AI agents iteratively generate a detailed life-cycle inventory leveraging a custom data abstraction and software tools that extract information from online text and images from repair communities and government certifications. This approach reduces weeks or months of expert time to under one minute and closes data availability gaps while yielding carbon footprint estimates within 19% of expert LCAs with zero proprietary data. Additionally, we develop a method to directly estimate EI by comparing an input to a cluster of products with similar descriptions and known carbon footprints. This runs in 3 ms on a laptop with a MAPE of 12.28% on electronic products. Further, we develop a data-driven method to generate emission factors. We use the properties of an unknown material to represent it as a weighted sum of emission factors for similar materials. Compared to human experts picking the closest LCA database entry, this improves MAPE by 120.26%. We analyze the data and compute scaling of this approach and discuss its implications for future LCA workflows.', 'abstract_zh': '近年来，对可持续性信息的兴趣激增。然而，从产品制造到处置的生命周期评估（LCA）中所需的用于映射材料和过程到环境影响（EI）的数据往往不可用。我们通过引入模拟LCA专家与产品经理、工程师等利益相关者之间交互的多模态AI代理，重新构想了传统的LCA方法，以计算电子设备的摇篮到大门（生产）碳排放。这些AI代理利用自定义数据抽象和软件工具，从维修社区和政府认证的在线文本和图像中提取信息，逐步生成详细的生命周期清单。这种方法将专家所需的时间从几周或几个月缩短到不到一分钟，并弥补了数据可用性的缺口，同时提供的碳足迹估算值与专家进行的LCA相差不超过19%，且不依赖于专有数据。此外，我们开发了一种直接估算环境影响的方法，通过将输入与具有相似描述和已知碳足迹的产品集群进行比较实现。在笔记本电脑上，这种方法可在3毫秒内运行，并在电子产品的预测中具有12.28%的平均绝对百分比误差（MAPE）。进一步地，我们开发了一种数据驱动的方法来生成排放因子。我们使用未知材料的属性将其表示为类似材料排放因子的加权和。与人类专家选择最接近的LCA数据库条目相比，这种方法的MAPE降低了120.26%。我们分析数据并计算此方法的扩展性，讨论其对未来LCA工作流程的影响。', 'title_zh': '面向自主可持续性评估的多模态AI代理系统'}
{'arxiv_id': 'arXiv:2507.17748', 'title': 'Large Learning Rates Simultaneously Achieve Robustness to Spurious Correlations and Compressibility', 'authors': 'Melih Barsbey, Lucas Prieto, Stefanos Zafeiriou, Tolga Birdal', 'link': 'https://arxiv.org/abs/2507.17748', 'abstract': 'Robustness and resource-efficiency are two highly desirable properties for modern machine learning models. However, achieving them jointly remains a challenge. In this paper, we position high learning rates as a facilitator for simultaneously achieving robustness to spurious correlations and network compressibility. We demonstrate that large learning rates also produce desirable representation properties such as invariant feature utilization, class separation, and activation sparsity. Importantly, our findings indicate that large learning rates compare favorably to other hyperparameters and regularization methods, in consistently satisfying these properties in tandem. In addition to demonstrating the positive effect of large learning rates across diverse spurious correlation datasets, models, and optimizers, we also present strong evidence that the previously documented success of large learning rates in standard classification tasks is likely due to its effect on addressing hidden/rare spurious correlations in the training dataset.', 'abstract_zh': '现代机器学习模型的健壮性和资源效率是两个高度 desirable 的属性。然而，同时实现这两个属性仍是一项挑战。本文将高学习率定位为同时实现对虚假相关性的鲁棒性和网络压缩性的促进因素。我们证明了高学习率还会产生诸如不变特征利用、类间分离和激活稀疏性等 desirable 的表示属性。重要的是，我们的研究发现表明，高学习率在一致地满足这些属性方面优于其他超参数和正则化方法。除了在多种虚假相关数据集、模型和优化器上展示高学习率的积极影响外，我们还提供了强有力的证据，表明高学习率在标准分类任务中取得成功的原因很可能是其对解决训练数据集中隐藏的/稀有的虚假相关性的效果。', 'title_zh': '大的学习率同时实现对虚假相关性的鲁棒性和压缩性'}
{'arxiv_id': 'arXiv:2507.17747', 'title': 'Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks', 'authors': 'Linbo Cao, Jinman Zhao', 'link': 'https://arxiv.org/abs/2507.17747', 'abstract': 'As frontier language models increasingly saturate standard QA benchmarks, concerns about data contamination, memorization, and escalating dataset creation costs persist. We propose a debate-driven evaluation paradigm that transforms any existing QA dataset into structured adversarial debates--where one model is given the official answer to defend, and another constructs and defends an alternative answer--adjudicated by a judge model blind to the correct solution. By forcing multi-round argumentation, this approach substantially increases difficulty while penalizing shallow memorization, yet reuses QA items to reduce curation overhead. We make two main contributions: (1) an evaluation pipeline to systematically convert QA tasks into debate-based assessments, and (2) a public benchmark that demonstrates our paradigm\'s effectiveness on a subset of MMLU-Pro questions, complete with standardized protocols and reference models. Empirical results validate the robustness of the method and its effectiveness against data contamination--a Llama 3.1 model fine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%) but performed worse in debates. Results also show that even weaker judges can reliably differentiate stronger debaters, highlighting how debate-based evaluation can scale to future, more capable systems while maintaining a fraction of the cost of creating new benchmarks. Overall, our framework underscores that "pretraining on the test set is no longer all you need," offering a sustainable path for measuring the genuine reasoning ability of advanced language models.', 'abstract_zh': '随着前沿语言模型在标准QA基准测试中的持续进步，数据污染、记忆化以及数据集创建成本上升的问题依然存在。我们提出了一种辩论驱动的评估框架，将任何现有的QA数据集转化为结构化的 adversarial 辩论——其中一方模型被给予官方答案进行辩护，另一方构建并辩护替代答案，由对正确答案不知情的裁判模型进行裁决。通过强制进行多轮论证，这种方法显著增加了评估难度并惩罚浅层记忆化，同时重用QA项以减少策展成本。我们做出了两项主要贡献：（1）一个评估流水线，系统地将QA任务转化为基于辩论的评估；（2）一个公开基准，在MMLU-Pro部分问题上展示了我们框架的有效性，包括标准化协议和参考模型。实证结果验证了该方法的健壮性和对数据污染的有效防御——一种在测试问题上进行微调的Llama 3.1模型在辩论中的表现不如其在准确率上的大幅提升（50% -> 82%）。结果还表明，即使是较弱的裁判也能可靠地区分较强的辩手，突显了基于辩论的评估如何随着未来更强大系统的到来而扩展，并以较低的成本维持基准创造。总体而言，我们的框架强调了“在测试集上进行预训练不再是唯一需求”，为衡量先进语言模型的真实推理能力提供了可持续的路径。', 'title_zh': '基于测试集预训练已不再足够：一种由辩论驱动的QA基准方法'}
{'arxiv_id': 'arXiv:2507.17746', 'title': 'Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains', 'authors': 'Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, Sean Hendryx', 'link': 'https://arxiv.org/abs/2507.17746', 'abstract': 'Extending Reinforcement Learning with Verifiable Rewards (RLVR) to real-world tasks often requires balancing objective and subjective evaluation criteria. However, many such tasks lack a single, unambiguous ground truth-making it difficult to define reliable reward signals for post-training language models. While traditional preference-based methods offer a workaround, they rely on opaque reward functions that are difficult to interpret and prone to spurious correlations. We introduce $\\textbf{Rubrics as Rewards}$ (RaR), a framework that uses structured, checklist-style rubrics as interpretable reward signals for on-policy training with GRPO. Our best RaR method yields up to a $28\\%$ relative improvement on HealthBench-1k compared to simple Likert-based approaches, while matching or surpassing the performance of reward signals derived from expert-written references. By treating rubrics as structured reward signals, we show that RaR enables smaller-scale judge models to better align with human preferences and sustain robust performance across model scales.', 'abstract_zh': '用结构化评分表作为奖励的强化学习验证奖励（RLVR）拓展到实际任务往往需要平衡客观和主观评估标准。然而，许多实际任务缺乏单一明确的ground truth，这使得为后训练语言模型定义可靠的奖励信号变得困难。虽然传统基于偏好方法提供了一种解决办法，但它们依赖于难以解释的奖励函数，容易产生虚假相关性。我们提出了评分表作为奖励（RaR）框架，该框架使用结构化清单风格的评分表作为可解释的奖励信号，用于基于GRPO的政策在线训练。我们表现最好的RaR方法在HealthBench-1k上相对于基于李克特量表的简单方法获得了最多28%的相对改进，同时匹配或超过了从专家编写参考中派生的奖励信号的性能。通过将评分表视为结构化奖励信号，我们展示了RaR如何使规模较小的评分者模型更好地与人类偏好对齐，并在不同模型规模下保持稳健性能。', 'title_zh': '评分标准作为奖励：强化学习超越可验证领域'}
{'arxiv_id': 'arXiv:2507.17745', 'title': 'Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention', 'authors': 'Yiwen Chen, Zhihao Li, Yikai Wang, Hu Zhang, Qin Li, Chi Zhang, Guosheng Lin', 'link': 'https://arxiv.org/abs/2507.17745', 'abstract': 'Recent advances in sparse voxel representations have significantly improved the quality of 3D content generation, enabling high-resolution modeling with fine-grained geometry. However, existing frameworks suffer from severe computational inefficiencies due to the quadratic complexity of attention mechanisms in their two-stage diffusion pipelines. In this work, we propose Ultra3D, an efficient 3D generation framework that significantly accelerates sparse voxel modeling without compromising quality. Our method leverages the compact VecSet representation to efficiently generate a coarse object layout in the first stage, reducing token count and accelerating voxel coordinate prediction. To refine per-voxel latent features in the second stage, we introduce Part Attention, a geometry-aware localized attention mechanism that restricts attention computation within semantically consistent part regions. This design preserves structural continuity while avoiding unnecessary global attention, achieving up to 6.7x speed-up in latent generation. To support this mechanism, we construct a scalable part annotation pipeline that converts raw meshes into part-labeled sparse voxels. Extensive experiments demonstrate that Ultra3D supports high-resolution 3D generation at 1024 resolution and achieves state-of-the-art performance in both visual fidelity and user preference.', 'abstract_zh': '近期稀疏体素表示的进展显著提高了3D内容生成的质量，能够实现高分辨率的精细几何建模。然而，现有框架由于两阶段扩散管道中注意机制的二次复杂性而遭受严重的计算效率低下问题。在此工作中，我们提出Ultra3D，一种高效且加速稀疏体素建模的3D生成框架，同时保持高质量。我们的方法利用紧凑的VecSet表示，在第一阶段高效生成粗略的物体布局，减少标记数量并加速体素坐标预测。为了在第二阶段细化每个体素的潜在特征，我们引入了Part Attention，一种几何感知的局部注意机制，仅在语义一致的部分区域范围内进行注意计算。这种设计保持了结构连续性，同时避免不必要的全局注意，潜在特征生成速度最高可提升6.7倍。为了支持这一机制，我们构建了一个可扩展的部分注释管道，将原始网格转换为部分标记的稀疏体素。广泛实验表明，Ultra3D支持1024分辨率的高分辨率3D生成，并在视觉保真度和用户体验方面达到最佳性能。', 'title_zh': 'Ultra3D：具有部分注意力机制的高效高保真3D生成'}
{'arxiv_id': 'arXiv:2507.17744', 'title': 'Yume: An Interactive World Generation Model', 'authors': 'Xiaofeng Mao, Shaoheng Lin, Zhen Li, Chuanhao Li, Wenshuo Peng, Tong He, Jiangmiao Pang, Mingmin Chi, Yu Qiao, Kaipeng Zhang', 'link': 'https://arxiv.org/abs/2507.17744', 'abstract': 'Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present a preview version of \\method, which creates a dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce a well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer~(MVDT) with a memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset \\sekai to train \\method, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on this https URL. Yume will update monthly to achieve its original goal. Project page: this https URL.', 'abstract_zh': 'Yume旨在使用图像、文本或视频创建一个交互式、逼真且动态的世界，允许用户通过外围设备或神经信号进行探索和控制。在本报告中，我们展示了\\method的预览版本，该版本可以从输入图像中生成动态世界，并允许用户通过键盘操作进行世界探索。为了实现这一高保真度和交互式的视频生成，我们提出了一种精心设计的框架，包括相机运动量化、视频生成架构、高级采样器和模型加速四个主要组成部分。首先，我们对相机运动进行量化，以实现稳定的训练和用户友好的键盘输入交互。然后，我们引入了具有记忆模块的Masked Video Diffusion Transformer (MVDT)，以进行自回归方式的无限视频生成。接着，我们引入了训练免费的抗伪影机制（AAM）和基于随机微分方程的时间旅行采样（TTS-SDE），以提高视觉质量并实现更精确的控制。此外，我们通过对抗蒸馏和缓存机制的协同优化来研究模型加速。我们使用高质量的世界探索数据集\\sekai对\\method进行了训练，并在多种场景和应用中取得了显著成果。所有数据、代码库和模型权重均可通过该链接下载。Yume将每月更新以实现其原始目标。项目页面：该链接。', 'title_zh': '梦境：一个互动世界生成模型'}
{'arxiv_id': 'arXiv:2507.17731', 'title': 'Flow Matching Meets Biology and Life Science: A Survey', 'authors': 'Zihao Li, Zhichen Zeng, Xiao Lin, Feihao Fang, Yanru Qu, Zhe Xu, Zhining Liu, Xuying Ning, Tianxin Wei, Ge Liu, Hanghang Tong, Jingrui He', 'link': 'https://arxiv.org/abs/2507.17731', 'abstract': 'Over the past decade, advances in generative modeling, such as generative adversarial networks, masked autoencoders, and diffusion models, have significantly transformed biological research and discovery, enabling breakthroughs in molecule design, protein generation, drug discovery, and beyond. At the same time, biological applications have served as valuable testbeds for evaluating the capabilities of generative models. Recently, flow matching has emerged as a powerful and efficient alternative to diffusion-based generative modeling, with growing interest in its application to problems in biology and life sciences. This paper presents the first comprehensive survey of recent developments in flow matching and its applications in biological domains. We begin by systematically reviewing the foundations and variants of flow matching, and then categorize its applications into three major areas: biological sequence modeling, molecule generation and design, and peptide and protein generation. For each, we provide an in-depth review of recent progress. We also summarize commonly used datasets and software tools, and conclude with a discussion of potential future directions. The corresponding curated resources are available at this https URL.', 'abstract_zh': '过去十年，生成模型的发展，如生成对抗网络、掩码自编码器和扩散模型，显著地改变了生物学研究和发现，促进了分子设计、蛋白质生成、药物发现等领域的重要突破。同时，生物学应用已成为评估生成模型能力的重要试验场。近年来，流匹配逐渐成为扩散模型生成方法的强有力且高效的替代方案，其在生物学和生命科学领域的问题中显示出越来越大的应用兴趣。本文首次全面综述了流匹配的 recent 发展及其在生物学领域中的应用。我们首先系统地回顾了流匹配的基础及其变体，然后将其应用分类为三个主要领域：生物序列建模、分子生成与设计、以及肽和蛋白质生成。对于每一方面，我们都提供了详细的综述，总结了常用的数据集和软件工具，并讨论了未来的研究方向。相关精选资源可访问此链接：[此 https URL]。', 'title_zh': '流匹配遇见生物学和生命科学：综述'}
{'arxiv_id': 'arXiv:2507.17725', 'title': 'On the Interaction of Compressibility and Adversarial Robustness', 'authors': 'Melih Barsbey, Antônio H. Ribeiro, Umut Şimşekli, Tolga Birdal', 'link': 'https://arxiv.org/abs/2507.17725', 'abstract': 'Modern neural networks are expected to simultaneously satisfy a host of desirable properties: accurate fitting to training data, generalization to unseen inputs, parameter and computational efficiency, and robustness to adversarial perturbations. While compressibility and robustness have each been studied extensively, a unified understanding of their interaction still remains elusive. In this work, we develop a principled framework to analyze how different forms of compressibility - such as neuron-level sparsity and spectral compressibility - affect adversarial robustness. We show that these forms of compression can induce a small number of highly sensitive directions in the representation space, which adversaries can exploit to construct effective perturbations. Our analysis yields a simple yet instructive robustness bound, revealing how neuron and spectral compressibility impact $L_\\infty$ and $L_2$ robustness via their effects on the learned representations. Crucially, the vulnerabilities we identify arise irrespective of how compression is achieved - whether via regularization, architectural bias, or implicit learning dynamics. Through empirical evaluations across synthetic and realistic tasks, we confirm our theoretical predictions, and further demonstrate that these vulnerabilities persist under adversarial training and transfer learning, and contribute to the emergence of universal adversarial perturbations. Our findings show a fundamental tension between structured compressibility and robustness, and suggest new pathways for designing models that are both efficient and secure.', 'abstract_zh': '现代神经网络预期能够同时满足多种 desirable 属性：对训练数据的准确拟合、对未见输入的泛化能力、参数和计算效率以及对抗扰动的鲁棒性。虽然可压缩性和鲁棒性各自已经得到了广泛的研究，但它们之间的相互作用仍然不清楚。在本文中，我们开发了一个原则性的框架来分析不同形式的可压缩性（如神经元级稀疏性和谱压缩性）如何影响对抗鲁棒性。我们表明，这些压缩形式可以在表示空间中诱导出少量高度敏感的方向，对手可以利用这些方向构造有效的扰动。我们的分析给出了一个简单而富有启发性的鲁棒性界，揭示了神经元和谱压缩性如何通过它们对学习表示的影响来影响 $L_\\infty$ 和 $L_2$ 鲁棒性。至关重要的是，我们发现的脆弱性无论压缩是如何实现的（如正则化、架构性偏差或隐含的学习动力学）都会存在。通过在合成和实际任务上的实验评估，我们证实了我们的理论预测，并进一步证明了这些脆弱性在对抗训练和迁移学习下仍然存在，并促进了普遍对抗扰动的出现。我们的发现表明了结构化可压缩性和鲁棒性之间的基本冲突，并提出了设计同时高效和安全模型的新途径。', 'title_zh': '压缩性和对抗鲁棒性的交互研究'}
{'arxiv_id': 'arXiv:2507.17718', 'title': 'AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer', 'authors': 'Danny D. Leybzon, Shreyas Tirumala, Nishant Jain, Summer Gillen, Michael Jackson, Cameron McPhee, Jennifer Schmidt', 'link': 'https://arxiv.org/abs/2507.17718', 'abstract': "With the rise of voice-enabled artificial intelligence (AI) systems, quantitative survey researchers have access to a new data-collection mode: AI telephone surveying. By using AI to conduct phone interviews, researchers can scale quantitative studies while balancing the dual goals of human-like interactivity and methodological rigor. Unlike earlier efforts that used interactive voice response (IVR) technology to automate these surveys, voice AI enables a more natural and adaptive respondent experience as it is more robust to interruptions, corrections, and other idiosyncrasies of human speech.\nWe built and tested an AI system to conduct quantitative surveys based on large language models (LLM), automatic speech recognition (ASR), and speech synthesis technologies. The system was specifically designed for quantitative research, and strictly adhered to research best practices like question order randomization, answer order randomization, and exact wording.\nTo validate the system's effectiveness, we deployed it to conduct two pilot surveys with the SSRS Opinion Panel and followed-up with a separate human-administered survey to assess respondent experiences. We measured three key metrics: the survey completion rates, break-off rates, and respondent satisfaction scores. Our results suggest that shorter instruments and more responsive AI interviewers may contribute to improvements across all three metrics studied.", 'abstract_zh': '随着语音激活人工智能（AI）系统的兴起，定量调研研究人员获得了新的数据收集模式：AI电话调研。通过使用AI进行电话访谈，研究人员可以扩大定量研究规模，同时平衡人性化互动和方法论严谨性的双重目标。与早期使用交互式语音应答（IVR）技术自动化这些调查的努力不同，语音AI能提供更加自然和适应性强的受访者体验，因为它更能应对打断、修正和其他人类语言的特异性。\n\n我们基于大规模语言模型（LLM）、自动语音识别（ASR）和语音合成技术构建并测试了一个用于定量调研的AI系统。该系统特别设计用于定量研究，并严格遵循最佳研究实践，如问题顺序随机化、答案顺序随机化和精确 wording。\n\n为了验证系统的有效性，我们在SSRS意见面板上部署了该系统进行两个试点调查，并进行了单独的人工管理调查以评估受访者体验。我们测量了三个关键指标：调查完成率、中途放弃率和受访者满意度评分。我们的结果显示，更短的调查工具和更具响应性的AI访谈员可能有助于所有三个指标的改进。', 'title_zh': 'AI电话调查：使用AI访谈员自动化定量数据收集'}
{'arxiv_id': 'arXiv:2507.17717', 'title': 'From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes', 'authors': 'Karen Zhou, John Giorgi, Pranav Mani, Peng Xu, Davis Liang, Chenhao Tan', 'link': 'https://arxiv.org/abs/2507.17717', 'abstract': "AI-generated clinical notes are increasingly used in healthcare, but evaluating their quality remains a challenge due to high subjectivity and limited scalability of expert review. Existing automated metrics often fail to align with real-world physician preferences. To address this, we propose a pipeline that systematically distills real user feedback into structured checklists for note evaluation. These checklists are designed to be interpretable, grounded in human feedback, and enforceable by LLM-based evaluators. Using deidentified data from over 21,000 clinical encounters, prepared in accordance with the HIPAA safe harbor standard, from a deployed AI medical scribe system, we show that our feedback-derived checklist outperforms baseline approaches in our offline evaluations in coverage, diversity, and predictive power for human ratings. Extensive experiments confirm the checklist's robustness to quality-degrading perturbations, significant alignment with clinician preferences, and practical value as an evaluation methodology. In offline research settings, the checklist can help identify notes likely to fall below our chosen quality thresholds.", 'abstract_zh': '基于用户反馈的结构化检查列表在AI生成的临床笔记评估中的应用：克服主观性和可扩展性挑战，提高与临床医师偏好的一致性', 'title_zh': '从反馈到检查列表：基于实际应用的AI生成临床笔记评估'}
{'arxiv_id': 'arXiv:2507.17691', 'title': 'CASCADE: LLM-Powered JavaScript Deobfuscator at Google', 'authors': 'Shan Jiang, Pranoy Kovuri, David Tao, Zhixun Tan', 'link': 'https://arxiv.org/abs/2507.17691', 'abstract': "Software obfuscation, particularly prevalent in JavaScript, hinders code comprehension and analysis, posing significant challenges to software testing, static analysis, and malware detection. This paper introduces CASCADE, a novel hybrid approach that integrates the advanced coding capabilities of Gemini with the deterministic transformation capabilities of a compiler Intermediate Representation (IR), specifically JavaScript IR (JSIR). By employing Gemini to identify critical prelude functions, the foundational components underlying the most prevalent obfuscation techniques, and leveraging JSIR for subsequent code transformations, CASCADE effectively recovers semantic elements like original strings and API names, and reveals original program behaviors. This method overcomes limitations of existing static and dynamic deobfuscation techniques, eliminating hundreds to thousands of hardcoded rules while achieving reliability and flexibility. CASCADE is already deployed in Google's production environment, demonstrating substantial improvements in JavaScript deobfuscation efficiency and reducing reverse engineering efforts.", 'abstract_zh': '软件混淆，特别是在JavaScript中普遍存在，阻碍了代码理解和分析，对软件测试、静态分析和恶意软件检测构成了重大挑战。本文介绍了CASCADE，这是一种新颖的混合方法，结合了Gemini高级编码能力和编译器中间表示（IR）的确定性转换能力，特别是JavaScript IR（JSIR）。通过使用Gemini来识别关键的前导函数，即最常见混淆技术的基础组件，并结合JSIR进行后续代码转换，CASCADE有效地恢复了如原始字符串和API名称等语义元素，并揭示了原始程序行为。该方法克服了现有静态和动态去混淆技术的局限性，消除了成千上万的手动硬编码规则，同时保持了可靠性和灵活性。CASCADE已经在Google的生产环境中部署，显示出JavaScript去混淆效率的显著提升，并减少了逆向工程的工作量。', 'title_zh': 'CASCADE：由大规模语言模型驱动的JavaScript去混淆工具（Google）'}
{'arxiv_id': 'arXiv:2507.17668', 'title': 'How Should We Meta-Learn Reinforcement Learning Algorithms?', 'authors': 'Alexander David Goldie, Zilin Wang, Jakob Nicolaus Foerster, Shimon Whiteson', 'link': 'https://arxiv.org/abs/2507.17668', 'abstract': 'The process of meta-learning algorithms from data, instead of relying on manual design, is growing in popularity as a paradigm for improving the performance of machine learning systems. Meta-learning shows particular promise for reinforcement learning (RL), where algorithms are often adapted from supervised or unsupervised learning despite their suboptimality for RL. However, until now there has been a severe lack of comparison between different meta-learning algorithms, such as using evolution to optimise over black-box functions or LLMs to propose code. In this paper, we carry out this empirical comparison of the different approaches when applied to a range of meta-learned algorithms which target different parts of the RL pipeline. In addition to meta-train and meta-test performance, we also investigate factors including the interpretability, sample cost and train time for each meta-learning algorithm. Based on these findings, we propose several guidelines for meta-learning new RL algorithms which will help ensure that future learned algorithms are as performant as possible.', 'abstract_zh': '从数据中元学习算法的过程正在日益成为提高机器学习系统性能的一种 paradigma，尤其是在强化学习（RL）领域，元学习显示出特别的前景，尽管从监督或无监督学习适应算法往往不尽完善。然而，迄今为止，不同元学习算法之间的比较严重缺乏，例如使用进化优化黑盒函数或使用大规模语言模型（LLM）提出代码。在本文中，我们对不同方法在多种目标RL管道不同部分的元学习算法中的应用进行了实证比较。此外，我们还调查了每种元学习算法的可解释性、样本成本和训练时间等因素。基于这些发现，我们提出了几条用于开发新RL算法的元学习准则，以确保未来学习的算法尽可能高效。', 'title_zh': '如何元学习强化学习算法？'}
{'arxiv_id': 'arXiv:2507.17616', 'title': 'Vision Transformer attention alignment with human visual perception in aesthetic object evaluation', 'authors': 'Miguel Carrasco, César González-Martín, José Aranda, Luis Oliveros', 'link': 'https://arxiv.org/abs/2507.17616', 'abstract': 'Visual attention mechanisms play a crucial role in human perception and aesthetic evaluation. Recent advances in Vision Transformers (ViTs) have demonstrated remarkable capabilities in computer vision tasks, yet their alignment with human visual attention patterns remains underexplored, particularly in aesthetic contexts. This study investigates the correlation between human visual attention and ViT attention mechanisms when evaluating handcrafted objects. We conducted an eye-tracking experiment with 30 participants (9 female, 21 male, mean age 24.6 years) who viewed 20 artisanal objects comprising basketry bags and ginger jars. Using a Pupil Labs eye-tracker, we recorded gaze patterns and generated heat maps representing human visual attention. Simultaneously, we analyzed the same objects using a pre-trained ViT model with DINO (Self-DIstillation with NO Labels), extracting attention maps from each of the 12 attention heads. We compared human and ViT attention distributions using Kullback-Leibler divergence across varying Gaussian parameters (sigma=0.1 to 3.0). Statistical analysis revealed optimal correlation at sigma=2.4 +-0.03, with attention head #12 showing the strongest alignment with human visual patterns. Significant differences were found between attention heads, with heads #7 and #9 demonstrating the greatest divergence from human attention (p< 0.05, Tukey HSD test). Results indicate that while ViTs exhibit more global attention patterns compared to human focal attention, certain attention heads can approximate human visual behavior, particularly for specific object features like buckles in basketry items. These findings suggest potential applications of ViT attention mechanisms in product design and aesthetic evaluation, while highlighting fundamental differences in attention strategies between human perception and current AI models.', 'abstract_zh': '视觉注意力机制在人类感知和审美评价中起着关键作用。最近Vision Transformers (ViTs)在计算机视觉任务中表现出色，但它们与人类视觉注意力模式的对齐，在审美情境下仍需进一步探索。本研究探讨了在评估手工艺品时，人类视觉注意力与ViT注意力机制之间的关联。我们进行了一项眼动跟踪实验，共有30名参与者（9名女性，21名男性，平均年龄24.6岁），他们观看了20件手工制品（包括编篮包和姜罐）。使用Pupil Labs眼动仪记录了注视模式，并生成了表示人类视觉注意力的热图。同时，我们使用预训练的ViT模型（DINO，带有自我蒸馏且无标签）分析了这些对象，从每个多头注意力机制中提取注意力图。我们使用高斯参数（sigma=0.1到3.0）的Kullback-Leibler散度比较了人类和ViT的注意力分布。统计分析表明，在sigma=2.4 ± 0.03时相关性最佳，其中第12个多头注意力机制与人类视觉模式的对齐最为紧密。头号7和头号9展示出与人类注意力最显著的差异（p < 0.05，Tukey HSD检验）。研究结果表明，尽管ViTs表现出比人类聚焦注意力更广泛的注意力模式，但某些注意力机制可以近似人类视觉行为，特别是对于手工制品中的特定特征如编篮包的搭扣。这些发现表明ViT注意力机制在产品设计和审美评价中的潜在应用，并强调了人类感知与当前AI模型之间注意力策略的基本差异。', 'title_zh': '视觉变换器注意力机制与人类视觉感知在美学对象评价中的对齐'}
{'arxiv_id': 'arXiv:2507.17596', 'title': 'PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving', 'authors': 'Maciej K. Wozniak, Lianhang Liu, Yixi Cai, Patric Jensfelt', 'link': 'https://arxiv.org/abs/2507.17596', 'abstract': 'While end-to-end autonomous driving models show promising results, their practical deployment is often hindered by large model sizes, a reliance on expensive LiDAR sensors and computationally intensive BEV feature representations. This limits their scalability, especially for mass-market vehicles equipped only with cameras. To address these challenges, we propose PRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving architecture operates using only camera data, without explicit BEV representation and forgoing the need for LiDAR. PRIX leverages a visual feature extractor coupled with a generative planning head to predict safe trajectories from raw pixel inputs directly. A core component of our architecture is the Context-aware Recalibration Transformer (CaRT), a novel module designed to effectively enhance multi-level visual features for more robust planning. We demonstrate through comprehensive experiments that PRIX achieves state-of-the-art performance on the NavSim and nuScenes benchmarks, matching the capabilities of larger, multimodal diffusion planners while being significantly more efficient in terms of inference speed and model size, making it a practical solution for real-world deployment. Our work is open-source and the code will be at this https URL.', 'abstract_zh': '基于原始像素的计划：端到端驾驶架构 PRIX', 'title_zh': 'PRIX: 从原始像素中学习规划以实现端到端自主驾驶'}
{'arxiv_id': 'arXiv:2507.17580', 'title': 'Enhancing Quantum Federated Learning with Fisher Information-Based Optimization', 'authors': 'Amandeep Singh Bhatia, Sabre Kais', 'link': 'https://arxiv.org/abs/2507.17580', 'abstract': "Federated Learning (FL) has become increasingly popular across different sectors, offering a way for clients to work together to train a global model without sharing sensitive data. It involves multiple rounds of communication between the global model and participating clients, which introduces several challenges like high communication costs, heterogeneous client data, prolonged processing times, and increased vulnerability to privacy threats. In recent years, the convergence of federated learning and parameterized quantum circuits has sparked significant research interest, with promising implications for fields such as healthcare and finance. By enabling decentralized training of quantum models, it allows clients or institutions to collaboratively enhance model performance and outcomes while preserving data privacy. Recognizing that Fisher information can quantify the amount of information that a quantum state carries under parameter changes, thereby providing insight into its geometric and statistical properties. We intend to leverage this property to address the aforementioned challenges. In this work, we propose a Quantum Federated Learning (QFL) algorithm that makes use of the Fisher information computed on local client models, with data distributed across heterogeneous partitions. This approach identifies the critical parameters that significantly influence the quantum model's performance, ensuring they are preserved during the aggregation process. Our research assessed the effectiveness and feasibility of QFL by comparing its performance against other variants, and exploring the benefits of incorporating Fisher information in QFL settings. Experimental results on ADNI and MNIST datasets demonstrate the effectiveness of our approach in achieving better performance and robustness against the quantum federated averaging method.", 'abstract_zh': 'federated learning (联邦学习)已在不同领域愈发流行，提供了一种客户端不共享敏感数据即可共同训练全局模型的方式。这涉及全局模型与参与客户端之间的多轮通信，引入了高通信成本、客户端数据异构性、处理时间延长以及隐私威胁增加等挑战。近年来，联邦学习与参数化量子电路的结合引起了广泛关注，并在医疗保健和金融等领域具有潜在影响。通过允许客户端或机构协作训练量子模型，同时保留数据隐私，它使客户端能够共同提升模型性能和成果。认识到费舍尔信息可以量化量子状态下在参数变化下所携带的信息量，从而提供其几何和统计性质的见解。我们计划利用这一特性来应对上述挑战。在这项工作中，我们提出了一种量子联邦学习（QFL）算法，该算法利用分布在异构分区上的本地客户端模型计算的费舍尔信息。该方法识别出对量子模型性能影响显著的关键参数，并确保在聚合过程中这些参数得以保留。我们的研究通过将其性能与其他变体进行比较，并探讨在QFL设置中整合费舍尔信息的好处，评估了QFL的有效性和可行性。ADNI和MNIST数据集上的实验结果表明，我们的方法在量子联邦平均方法中实现了更好的性能和更强的鲁棒性。', 'title_zh': '基于 Fisher 信息的优化增强量子联邦学习'}
{'arxiv_id': 'arXiv:2507.17534', 'title': 'Federated Majorize-Minimization: Beyond Parameter Aggregation', 'authors': 'Aymeric Dieuleveut, Gersende Fort, Mahmoud Hegazy, Hoi-To Wai', 'link': 'https://arxiv.org/abs/2507.17534', 'abstract': 'This paper proposes a unified approach for designing stochastic optimization algorithms that robustly scale to the federated learning setting. Our work studies a class of Majorize-Minimization (MM) problems, which possesses a linearly parameterized family of majorizing surrogate functions. This framework encompasses (proximal) gradient-based algorithms for (regularized) smooth objectives, the Expectation Maximization algorithm, and many problems seen as variational surrogate MM. We show that our framework motivates a unifying algorithm called Stochastic Approximation Stochastic Surrogate MM (\\SSMM), which includes previous stochastic MM procedures as special instances. We then extend \\SSMM\\ to the federated setting, while taking into consideration common bottlenecks such as data heterogeneity, partial participation, and communication constraints; this yields \\QSMM. The originality of \\QSMM\\ is to learn locally and then aggregate information characterizing the \\textit{surrogate majorizing function}, contrary to classical algorithms which learn and aggregate the \\textit{original parameter}. Finally, to showcase the flexibility of this methodology beyond our theoretical setting, we use it to design an algorithm for computing optimal transport maps in the federated setting.', 'abstract_zh': '本文提出了一种统一的设计鲁棒扩展到联邦学习设置的随机优化算法的方法。我们研究了一类极大化-最小化（MM）问题，该问题具有线性参数化的极大化代理函数族。该框架包括基于梯度（正则化光滑目标的）算法、期望最大化算法以及许多被视为变分代理MM的问题。我们展示了该框架启发了一种统一算法，称为随机逼近代理极大化函数（\\SSMM）算法，其中包括了先前的随机MM过程作为特殊情况。随后，我们将\\SSMM扩展到联邦设置，同时考虑了数据异质性、部分参与和通信约束等问题；这产生了\\QSMM。\\QSMM的独特之处在于它在本地学习然后聚合描述代理极大化函数的信息，而传统算法则学习并聚合原始参数。最后，为了展示该方法在超越我们理论设置时的灵活性，我们使用它为联邦设置下的最优传输映射设计了一个算法。', 'title_zh': '联邦次梯度最大化:minimization超越参数聚合'}
{'arxiv_id': 'arXiv:2507.17526', 'title': 'Integrating Physics-Based and Data-Driven Approaches for Probabilistic Building Energy Modeling', 'authors': 'Leandro Von Krannichfeldt, Kristina Orehounig, Olga Fink', 'link': 'https://arxiv.org/abs/2507.17526', 'abstract': 'Building energy modeling is a key tool for optimizing the performance of building energy systems. Historically, a wide spectrum of methods has been explored -- ranging from conventional physics-based models to purely data-driven techniques. Recently, hybrid approaches that combine the strengths of both paradigms have gained attention. These include strategies such as learning surrogates for physics-based models, modeling residuals between simulated and observed data, fine-tuning surrogates with real-world measurements, using physics-based outputs as additional inputs for data-driven models, and integrating the physics-based output into the loss function the data-driven model. Despite this progress, two significant research gaps remain. First, most hybrid methods focus on deterministic modeling, often neglecting the inherent uncertainties caused by factors like weather fluctuations and occupant behavior. Second, there has been little systematic comparison within a probabilistic modeling framework. This study addresses these gaps by evaluating five representative hybrid approaches for probabilistic building energy modeling, focusing on quantile predictions of building thermodynamics in a real-world case study. Our results highlight two main findings. First, the performance of hybrid approaches varies across different building room types, but residual learning with a Feedforward Neural Network performs best on average. Notably, the residual approach is the only model that produces physically intuitive predictions when applied to out-of-distribution test data. Second, Quantile Conformal Prediction is an effective procedure for calibrating quantile predictions in case of indoor temperature modeling.', 'abstract_zh': '构建能源模型是优化建筑能源系统性能的关键工具。历史上，广泛探索了从基于常规物理的方法到纯粹的数据驱动技术的多种方法。最近，结合这两种范式优势的混合方法引起了关注。这些方法包括学习基于物理模型的代理模型、模拟数据和观察数据之间的残差建模、使用实际测量数据 fine-tune 代理模型、将基于物理的输出作为数据驱动模型的附加输入，以及将基于物理的输出集成到数据驱动模型的损失函数中。尽管取得了这些进展，但仍存在两个主要的研究空白。首先，大多数混合方法专注于确定性建模，经常忽略天气波动和 occupants 行为等因素引起的固有不确定性。其次，在概率建模框架内缺乏系统的比较。本研究通过评估五种代表性的混合方法来弥补这些空白，重点关注实际案例研究中的建筑热力学的分位数预测。我们的结果强调了两个主要发现。首先，不同建筑房间类型的混合方法性能各异，但采用前向神经网络学习残差的方法在平均表现上最佳。值得注意的是，残差方法是唯一一种在处理离分布测试数据时产生物理直观预测的模型。其次，室内温度建模中分位数一致性预测是一种有效的方法来校准分位数预测。', 'title_zh': '基于物理原理和数据驱动的建筑能耗概率模型集成方法'}
{'arxiv_id': 'arXiv:2507.17518', 'title': 'Enabling Cyber Security Education through Digital Twins and Generative AI', 'authors': 'Vita Santa Barletta, Vito Bavaro, Miriana Calvano, Antonio Curci, Antonio Piccinno, Davide Pio Posa', 'link': 'https://arxiv.org/abs/2507.17518', 'abstract': 'Digital Twins (DTs) are gaining prominence in cybersecurity for their ability to replicate complex IT (Information Technology), OT (Operational Technology), and IoT (Internet of Things) infrastructures, allowing for real time monitoring, threat analysis, and system simulation. This study investigates how integrating DTs with penetration testing tools and Large Language Models (LLMs) can enhance cybersecurity education and operational readiness. By simulating realistic cyber environments, this approach offers a practical, interactive framework for exploring vulnerabilities and defensive strategies. At the core of this research is the Red Team Knife (RTK), a custom penetration testing toolkit aligned with the Cyber Kill Chain model. RTK is designed to guide learners through key phases of cyberattacks, including reconnaissance, exploitation, and response within a DT powered ecosystem. The incorporation of Large Language Models (LLMs) further enriches the experience by providing intelligent, real-time feedback, natural language threat explanations, and adaptive learning support during training exercises. This combined DT LLM framework is currently being piloted in academic settings to develop hands on skills in vulnerability assessment, threat detection, and security operations. Initial findings suggest that the integration significantly improves the effectiveness and relevance of cybersecurity training, bridging the gap between theoretical knowledge and real-world application. Ultimately, the research demonstrates how DTs and LLMs together can transform cybersecurity education to meet evolving industry demands.', 'abstract_zh': '数字孪生(DTs)在网络安全中的应用及其与渗透测试工具和大型语言模型(LLMs)的整合：增强网络安全教育与操作准备', 'title_zh': '通过数字孪生和生成式AI赋能网络安全教育'}
{'arxiv_id': 'arXiv:2507.17513', 'title': 'HOTA: Hamiltonian framework for Optimal Transport Advection', 'authors': 'Nazar Buzun, Daniil Shlenskii, Maxim Bobrin, Dmitry V. Dylov', 'link': 'https://arxiv.org/abs/2507.17513', 'abstract': 'Optimal transport (OT) has become a natural framework for guiding the probability flows. Yet, the majority of recent generative models assume trivial geometry (e.g., Euclidean) and rely on strong density-estimation assumptions, yielding trajectories that do not respect the true principles of optimality in the underlying manifold. We present Hamiltonian Optimal Transport Advection (HOTA), a Hamilton-Jacobi-Bellman based method that tackles the dual dynamical OT problem explicitly through Kantorovich potentials, enabling efficient and scalable trajectory optimization. Our approach effectively evades the need for explicit density modeling, performing even when the cost functionals are non-smooth. Empirically, HOTA outperforms all baselines in standard benchmarks, as well as in custom datasets with non-differentiable costs, both in terms of feasibility and optimality.', 'abstract_zh': 'Hamiltonian Optimal Transport Advection (HOTA)基于哈密尔顿-雅可比-贝尔曼方法通过 Kantorovich 能量显式解决对偶动态OT问题，实现高效的轨迹优化', 'title_zh': '霍塔：基于哈密顿框架的最优输运推进'}
{'arxiv_id': 'arXiv:2507.17494', 'title': 'To Trust or Not to Trust: On Calibration in ML-based Resource Allocation for Wireless Networks', 'authors': 'Rashika Raina, Nidhi Simmons, David E. Simmons, Michel Daoud Yacoub, Trung Q. Duong', 'link': 'https://arxiv.org/abs/2507.17494', 'abstract': "In next-generation communications and networks, machine learning (ML) models are expected to deliver not only accurate predictions but also well-calibrated confidence scores that reflect the true likelihood of correct decisions. This paper studies the calibration performance of an ML-based outage predictor within a single-user, multi-resource allocation framework. We first establish key theoretical properties of this system's outage probability (OP) under perfect calibration. Importantly, we show that as the number of resources grows, the OP of a perfectly calibrated predictor approaches the expected output conditioned on it being below the classification threshold. In contrast, when only one resource is available, the system's OP equals the model's overall expected output. We then derive the OP conditions for a perfectly calibrated predictor. These findings guide the choice of the classification threshold to achieve a desired OP, helping system designers meet specific reliability requirements. We also demonstrate that post-processing calibration cannot improve the system's minimum achievable OP, as it does not introduce new information about future channel states. Additionally, we show that well-calibrated models are part of a broader class of predictors that necessarily improve OP. In particular, we establish a monotonicity condition that the accuracy-confidence function must satisfy for such improvement to occur. To demonstrate these theoretical properties, we conduct a rigorous simulation-based analysis using post-processing calibration techniques: Platt scaling and isotonic regression. As part of this framework, the predictor is trained using an outage loss function specifically designed for this system. Furthermore, this analysis is performed on Rayleigh fading channels with temporal correlation captured by Clarke's 2D model, which accounts for receiver mobility.", 'abstract_zh': '下一代通信与网络中，基于机器学习的 outage 预测器的校准性能研究', 'title_zh': '是信还是不信：关于基于ML的无线网络资源分配中的校准问题'}
{'arxiv_id': 'arXiv:2507.17486', 'title': "Unsupervised anomaly detection using Bayesian flow networks: application to brain FDG PET in the context of Alzheimer's disease", 'authors': 'Hugues Roy, Reuben Dorent, Ninon Burgos', 'link': 'https://arxiv.org/abs/2507.17486', 'abstract': "Unsupervised anomaly detection (UAD) plays a crucial role in neuroimaging for identifying deviations from healthy subject data and thus facilitating the diagnosis of neurological disorders. In this work, we focus on Bayesian flow networks (BFNs), a novel class of generative models, which have not yet been applied to medical imaging or anomaly detection. BFNs combine the strength of diffusion frameworks and Bayesian inference. We introduce AnoBFN, an extension of BFNs for UAD, designed to: i) perform conditional image generation under high levels of spatially correlated noise, and ii) preserve subject specificity by incorporating a recursive feedback from the input image throughout the generative process. We evaluate AnoBFN on the challenging task of Alzheimer's disease-related anomaly detection in FDG PET images. Our approach outperforms other state-of-the-art methods based on VAEs (beta-VAE), GANs (f-AnoGAN), and diffusion models (AnoDDPM), demonstrating its effectiveness at detecting anomalies while reducing false positive rates.", 'abstract_zh': '无监督异常检测（UAD）在神经影像学中起着重要作用，用于识别与健康受试者数据的偏差，从而辅助神经精神障碍的诊断。在本文中，我们关注贝叶斯流网络（BFNs），这是一种新型的生成模型，尚未被应用于医学影像或异常检测。BFNs结合了扩散框架和贝叶斯推断的优势。我们引入了AnoBFN，这是一种BFNs的扩展，旨在：i) 在高空间相关噪声下进行条件图像生成；ii) 通过在整个生成过程中从输入图像引入递归反馈来保留受试者特异性。我们在FDG PET图像相关的阿尔茨海默病异常检测这一具有挑战性的任务上评估了AnoBFN。我们的方法在基于VAE（β-VAE）、GAN（f-AnoGAN）和扩散模型（AnoDDPM）的其他最先进的方法中表现出色，证明了其在检测异常同时降低假阳性率的有效性。', 'title_zh': '基于贝叶斯流网络的无监督异常检测：阿尔茨海默病背景下脑FDG PET的应用'}
{'arxiv_id': 'arXiv:2507.17476', 'title': 'MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs', 'authors': 'Alexander R. Fabbri, Diego Mares, Jorge Flores, Meher Mankikar, Ernesto Hernandez, Dean Lee, Bing Liu, Chen Xing', 'link': 'https://arxiv.org/abs/2507.17476', 'abstract': "Although recent Large Language Models (LLMs) have shown rapid improvement on reasoning benchmarks in English, the evaluation of such LLMs' multilingual reasoning capability across diverse languages and cultural contexts remains limited. Existing multilingual reasoning benchmarks are typically constructed by translating existing English reasoning benchmarks, biasing these benchmarks towards reasoning problems with context in English language/cultures. In this work, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a benchmark designed to assess LLMs on more than 1,000 native, linguistic and culturally grounded reasoning questions written by native speakers in French, Spanish, and Chinese. MultiNRC covers four core reasoning categories: language-specific linguistic reasoning, wordplay & riddles, cultural/tradition reasoning, and math reasoning with cultural relevance. For cultural/tradition reasoning and math reasoning with cultural relevance, we also provide English equivalent translations of the multilingual questions by manual translation from native speakers fluent in English. This set of English equivalents can provide a direct comparison of LLM reasoning capacity in other languages vs. English on the same reasoning questions. We systematically evaluate current 14 leading LLMs covering most LLM families on MultiNRC and its English equivalent set. The results show that (1) current LLMs are still not good at native multilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs exhibit distinct strengths and weaknesses in handling linguistic, cultural, and logical reasoning tasks; (3) Most models perform substantially better in math reasoning in English compared to in original languages (+10%), indicating persistent challenges with culturally grounded knowledge.", 'abstract_zh': '多语言本土推理挑战（MultiNRC）：评估大型语言模型在法语、西班牙语和汉语背景下的推理能力', 'title_zh': 'MultNRC：一个具有挑战性的本源多语言推理评估基准模型'}
{'arxiv_id': 'arXiv:2507.17472', 'title': 'BGM-HAN: A Hierarchical Attention Network for Accurate and Fair Decision Assessment on Semi-Structured Profiles', 'authors': 'Junhua Liu, Roy Ka-Wei Lee, Kwan Hui Lim', 'link': 'https://arxiv.org/abs/2507.17472', 'abstract': 'Human decision-making in high-stakes domains often relies on expertise and heuristics, but is vulnerable to hard-to-detect cognitive biases that threaten fairness and long-term outcomes. This work presents a novel approach to enhancing complex decision-making workflows through the integration of hierarchical learning alongside various enhancements. Focusing on university admissions as a representative high-stakes domain, we propose BGM-HAN, an enhanced Byte-Pair Encoded, Gated Multi-head Hierarchical Attention Network, designed to effectively model semi-structured applicant data. BGM-HAN captures multi-level representations that are crucial for nuanced assessment, improving both interpretability and predictive performance. Experimental results on real admissions data demonstrate that our proposed model significantly outperforms both state-of-the-art baselines from traditional machine learning to large language models, offering a promising framework for augmenting decision-making in domains where structure, context, and fairness matter. Source code is available at: this https URL.', 'abstract_zh': '高风险领域的决策制定往往依赖于专业知识和启发式方法，但容易受到难以察觉的认知偏见的影响，这些偏见威胁着公平性和长期结果。本研究提出了一种新的方法，通过将层次学习与各种增强技术集成来改进复杂的决策工作流程。以大学招生这一典型的高风险领域为例，我们提出了BGM-HAN模型，这是一种增强的字对编码、门控多头层次注意力网络，旨在有效建模半结构化的申请者数据。BGM-HAN捕捉到多级表示，这对于精细评估至关重要，提高了模型的可解释性和预测性能。实验证明，我们的模型在实际招生数据上显著优于传统机器学习和大型语言模型的最新基线，为在注重结构、上下文和公平性的领域中增强决策提供了一个有前景的框架。相关源代码可在：this https URL 获取。', 'title_zh': 'BGM-HAN：一种用于半结构化个人资料准确且公平决策评估的分层注意力网络'}
{'arxiv_id': 'arXiv:2507.17470', 'title': 'Demonstration of Efficient Predictive Surrogates for Large-scale Quantum Processors', 'authors': 'Wei-You Liao, Yuxuan Du, Xinbiao Wang, Tian-Ci Tian, Yong Luo, Bo Du, Dacheng Tao, He-Liang Huang', 'link': 'https://arxiv.org/abs/2507.17470', 'abstract': 'The ongoing development of quantum processors is driving breakthroughs in scientific discovery. Despite this progress, the formidable cost of fabricating large-scale quantum processors means they will remain rare for the foreseeable future, limiting their widespread application. To address this bottleneck, we introduce the concept of predictive surrogates, which are classical learning models designed to emulate the mean-value behavior of a given quantum processor with provably computational efficiency. In particular, we propose two predictive surrogates that can substantially reduce the need for quantum processor access in diverse practical scenarios. To demonstrate their potential in advancing digital quantum simulation, we use these surrogates to emulate a quantum processor with up to 20 programmable superconducting qubits, enabling efficient pre-training of variational quantum eigensolvers for families of transverse-field Ising models and identification of non-equilibrium Floquet symmetry-protected topological phases. Experimental results reveal that the predictive surrogates not only reduce measurement overhead by orders of magnitude, but can also surpass the performance of conventional, quantum-resource-intensive approaches. Collectively, these findings establish predictive surrogates as a practical pathway to broadening the impact of advanced quantum processors.', 'abstract_zh': '量子处理器的持续发展正在推动科学发现的突破。尽管取得了这些进展，大规模量子处理器的高昂制造成本意味着它们在未来一段时间内仍将是稀有资源，限制了其广泛应用。为了解决这一瓶颈，我们引入了预测代理的概念，这是一种经典的学习模型，旨在以可证明的计算效率模拟给定量子处理器的平均值行为。特别地，我们提出了两种预测代理，能够在多种实际应用场景中大幅减少对量子处理器的访问需求。为了证明其在推进数字量子模拟方面的潜力，我们使用这些代理来模拟了一个包含多达20个可编程超导量子位的量子处理器，从而能够高效预训练变址量子本证求解器，并识别纵向场伊辛模型的非平衡 Floquet 对称保护拓扑相。实验结果表明，预测代理不仅将测量开销减少了几个数量级，而且在某些情况下还能超越传统、量子资源密集型方法的性能。总之，这些发现确立了预测代理作为一种实用途径，以扩展高级量子处理器的影响。', 'title_zh': '大规模量子处理器的高效预测代理演示'}
{'arxiv_id': 'arXiv:2507.17467', 'title': 'Probing Vision-Language Understanding through the Visual Entailment Task: promises and pitfalls', 'authors': 'Elena Pitta, Tom Kouwenhoven, Tessa Verhoef', 'link': 'https://arxiv.org/abs/2507.17467', 'abstract': "This study investigates the extent to which the Visual Entailment (VE) task serves as a reliable probe of vision-language understanding in multimodal language models, using the LLaMA 3.2 11B Vision model as a test case. Beyond reporting performance metrics, we aim to interpret what these results reveal about the underlying possibilities and limitations of the VE task. We conduct a series of experiments across zero-shot, few-shot, and fine-tuning settings, exploring how factors such as prompt design, the number and order of in-context examples and access to visual information might affect VE performance. To further probe the reasoning processes of the model, we used explanation-based evaluations. Results indicate that three-shot inference outperforms the zero-shot baselines. However, additional examples introduce more noise than they provide benefits. Additionally, the order of the labels in the prompt is a critical factor that influences the predictions. In the absence of visual information, the model has a strong tendency to hallucinate and imagine content, raising questions about the model's over-reliance on linguistic priors. Fine-tuning yields strong results, achieving an accuracy of 83.3% on the e-SNLI-VE dataset and outperforming the state-of-the-art OFA-X model. Additionally, the explanation evaluation demonstrates that the fine-tuned model provides semantically meaningful explanations similar to those of humans, with a BERTScore F1-score of 89.2%. We do, however, find comparable BERTScore results in experiments with limited vision, questioning the visual grounding of this task. Overall, our results highlight both the utility and limitations of VE as a diagnostic task for vision-language understanding and point to directions for refining multimodal evaluation methods.", 'abstract_zh': '本研究探讨视觉蕴含任务（VE）在多模态语言模型中作为视觉-语言理解可靠探针的程度，以LLaMA 3.2 11B Vision模型为测试案例。除了报告性能指标，我们还旨在解释这些结果揭示的视觉蕴含任务潜在能力和局限性。我们在零样本、少样本和微调设置下进行了一系列实验，探索提示设计、上下文示例的数量和顺序以及对视觉信息的访问如何影响视觉蕴含性能。为了进一步探查模型的推理过程，我们使用了解释基评估方法。结果显示，三样本推理优于零样本基准。然而，额外的示例引入的噪声多于提供的益处。此外，提示中标签的顺序是影响预测的关键因素。在缺乏视觉信息的情况下，模型具有强烈的想象内容的趋势，这引发了人们对模型过度依赖语言先验的质疑。微调取得了显著结果，在e-SNLI-VE数据集上准确率达到83.3%，超越了最先进的OFA-X模型。此外，解释评估表明，经过微调的模型提供了与人类相似的语义上意义明确的解释，BERTScore F1得分为89.2%。然而，在视觉受限实验中也获得了可比的BERTScore结果，这质疑了该任务的视觉定位。总体而言，我们的结果突显了视觉蕴含任务作为视觉-语言理解诊断任务的实用性和局限性，并指出了改进多模态评估方法的方向。', 'title_zh': '通过视觉蕴含任务探究视觉-语言理解： promise 和 pitfalls'}
{'arxiv_id': 'arXiv:2507.17448', 'title': 'Reasoning-Driven Retrosynthesis Prediction with Large Language Models via Reinforcement Learning', 'authors': 'Situo Zhang, Hanqi Li, Lu Chen, Zihan Zhao, Xuanze Lin, Zichen Zhu, Bo Chen, Xin Chen, Kai Yu', 'link': 'https://arxiv.org/abs/2507.17448', 'abstract': "Retrosynthesis planning, essential in organic synthesis and drug discovery, has greatly benefited from recent AI-driven advancements. Nevertheless, existing methods frequently face limitations in both applicability and explainability. Traditional graph-based and sequence-to-sequence models often lack generalized chemical knowledge, leading to predictions that are neither consistently accurate nor easily explainable. To address these challenges, we introduce RetroDFM-R, a reasoning-based large language model (LLM) designed specifically for chemical retrosynthesis. Leveraging large-scale reinforcement learning guided by chemically verifiable rewards, RetroDFM-R significantly enhances prediction accuracy and explainability. Comprehensive evaluations demonstrate that RetroDFM-R significantly outperforms state-of-the-art methods, achieving a top-1 accuracy of 65.0% on the USPTO-50K benchmark. Double-blind human assessments further validate the chemical plausibility and practical utility of RetroDFM-R's predictions. RetroDFM-R also accurately predicts multistep retrosynthetic routes reported in the literature for both real-world drug molecules and perovskite materials. Crucially, the model's explicit reasoning process provides human-interpretable insights, thereby enhancing trust and practical value in real-world retrosynthesis applications.", 'abstract_zh': 'retrosynthesis 计划对于有机合成和药物发现至关重要，近年来得益于人工智能驱动的进步。然而，现有方法在适用性和可解释性方面仍面临诸多限制。传统的基于图和序列到序列的模型往往缺乏泛化的化学知识，导致预测既不够准确也不易解释。为了应对这些挑战，我们提出了一种专用的化学 retrosynthesis 原理解释型大型语言模型（LLM）——RetroDFM-R。通过大规模基于化学验证奖励的强化学习，RetroDFM-R 显著提升了预测准确性和可解释性。全面评估表明，RetroDFM-R 显著优于现有最先进的方法，在 USPTO-50K 基准上的顶级准确率达到了 65.0%。双盲的人类评估进一步验证了 RetroDFM-R 预测的化学可行性和实际应用价值。RetroDFM-R 还准确预测了文献中报道的真实药物分子和钙钛矿材料的多步 retrosynthetic 路径。至关重要的是，模型的明确推理过程提供了可由人类理解的见解，从而增强了在实际 retrosynthesis 应用中的信任和实际价值。', 'title_zh': '基于强化学习的大语言模型驱动的逆合成反应预测'}
{'arxiv_id': 'arXiv:2507.17445', 'title': "IndoorBEV: Joint Detection and Footprint Completion of Objects via Mask-based Prediction in Indoor Scenarios for Bird's-Eye View Perception", 'authors': 'Haichuan Li, Changda Tian, Panos Trahanias, Tomi Westerlund', 'link': 'https://arxiv.org/abs/2507.17445', 'abstract': "Detecting diverse objects within complex indoor 3D point clouds presents significant challenges for robotic perception, particularly with varied object shapes, clutter, and the co-existence of static and dynamic elements where traditional bounding box methods falter. To address these limitations, we propose IndoorBEV, a novel mask-based Bird's-Eye View (BEV) method for indoor mobile robots.\nIn a BEV method, a 3D scene is projected into a 2D BEV grid which handles naturally occlusions and provides a consistent top-down view aiding to distinguish static obstacles from dynamic agents. The obtained 2D BEV results is directly usable to downstream robotic tasks like navigation, motion prediction, and planning. Our architecture utilizes an axis compact encoder and a window-based backbone to extract rich spatial features from this BEV map. A query-based decoder head then employs learned object queries to concurrently predict object classes and instance masks in the BEV space. This mask-centric formulation effectively captures the footprint of both static and dynamic objects regardless of their shape, offering a robust alternative to bounding box regression. We demonstrate the effectiveness of IndoorBEV on a custom indoor dataset featuring diverse object classes including static objects\nand dynamic elements like robots and miscellaneous items, showcasing its potential for robust indoor scene understanding.", 'abstract_zh': '室内复杂三维点云中多样物体的检测对机器人感知构成了重大挑战，特别是在物体形状各异、存在杂乱以及静动态元素共存的情况下，传统的边界框方法往往力有未逮。为了解决这些问题，我们提出了一种新的基于掩码的鸟瞰图（BEV）方法——IndoorBEV，适用于室内移动机器人。', 'title_zh': 'IndoorBEV：基于掩码预测的室内场景中物体联合检测与足迹完成的鸟瞰视图感知'}
{'arxiv_id': 'arXiv:2507.17442', 'title': 'Each to Their Own: Exploring the Optimal Embedding in RAG', 'authors': 'Shiting Chen, Zijian Zhao, Jinsong Chen', 'link': 'https://arxiv.org/abs/2507.17442', 'abstract': 'Recently, as Large Language Models (LLMs) have fundamentally impacted various fields, the methods for incorporating up-to-date information into LLMs or adding external knowledge to construct domain-specific models have garnered wide attention. Retrieval-Augmented Generation (RAG), serving as an inference-time scaling method, is notable for its low cost and minimal effort for parameter tuning. However, due to heterogeneous training data and model architecture, the variant embedding models used in RAG exhibit different benefits across various areas, often leading to different similarity calculation results and, consequently, varying response quality from LLMs. To address this problem, we propose and examine two approaches to enhance RAG by combining the benefits of multiple embedding models, named Mixture-Embedding RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects retrievals from multiple embedding models based on standardized similarity; however, it does not outperform vanilla RAG. In contrast, Confident RAG generates responses multiple times using different embedding models and then selects the responses with the highest confidence level, demonstrating average improvements of approximately 10% and 5% over vanilla LLMs and RAG, respectively. The consistent results across different LLMs and embedding models indicate that Confident RAG is an efficient plug-and-play approach for various domains. We will release our code upon publication.', 'abstract_zh': '近期，随着大型语言模型（LLMs）对各个领域产生了根本性影响，将最新信息融入LLMs或通过添加外部知识构建领域特定模型的方法受到了广泛的关注。检索增强生成（RAG）作为一种推理时扩展方法，以其低成本和最小参数调优努力而著称。但由于训练数据和模型架构的异质性，RAG中使用的多种嵌入模型在不同领域表现出不同的优势，导致不同的相似度计算结果和LLMs响应质量的差异。为了解决这一问题，我们提出了两种结合多个嵌入模型优点的方法，分别是混合嵌入RAG和自信RAG。混合嵌入RAG基于标准化相似度简单地排序和选择来自多个嵌入模型的检索结果，但并未超过传统的RAG。相比之下，自信RAG利用不同的嵌入模型多次生成响应，然后选择具有最高置信度的响应，相对于传统的LLMs和RAG分别显示出约10%和5%的平均改进。不同LLMs和嵌入模型的一致性结果表明，自信RAG是一种高效可插拔的方法，适用于各种领域。论文发表后我们将发布我们的代码。', 'title_zh': '各有所适：探究RAG中的最优嵌入方式'}
{'arxiv_id': 'arXiv:2507.17433', 'title': 'Fair Compromises in Participatory Budgeting: a Multi-Agent Deep Reinforcement Learning Approach', 'authors': 'Hugh Adams, Srijoni Majumdar, Evangelos Pournaras', 'link': 'https://arxiv.org/abs/2507.17433', 'abstract': 'Participatory budgeting is a method of collectively understanding and addressing spending priorities where citizens vote on how a budget is spent, it is regularly run to improve the fairness of the distribution of public funds. Participatory budgeting requires voters to make decisions on projects which can lead to ``choice overload". A multi-agent reinforcement learning approach to decision support can make decision making easier for voters by identifying voting strategies that increase the winning proportion of their vote. This novel approach can also support policymakers by highlighting aspects of election design that enable fair compromise on projects. This paper presents a novel, ethically aligned approach to decision support using multi-agent deep reinforcement learning modelling. This paper introduces a novel use of a branching neural network architecture to overcome scalability challenges of multi-agent reinforcement learning in a decentralized way. Fair compromises are found through optimising voter actions towards greater representation of voter preferences in the winning set. Experimental evaluation with real-world participatory budgeting data reveals a pattern in fair compromise: that it is achievable through projects with smaller cost.', 'abstract_zh': '参与式预算是一种公民共同理解并确定支出优先级的方法，通过投票决定预算的分配，定期运行以提高公共资金分配的公平性。参与式预算要求选民在项目决策中做出选择，这可能导致“选择超载”。基于多智能体强化学习的决策支持方法可以通过识别提高投票成功率的投票策略，使选民的决策更加简便。该新方法还可以通过突出选举设计中的公平妥协方面来支持决策者。本文提出了一种新的、符合伦理的决策支持方法，利用多智能体深度强化学习建模。本文介绍了使用分叉神经网络架构的新方法，以分散的方式克服多智能体强化学习的可扩展性挑战。通过优化选民行为以在获胜集中更充分地体现选民偏好来实现公平妥协。实证评价使用真实世界的参与式预算数据揭示了公平妥协的模式：它可以通过成本较小的项目来实现。', 'title_zh': '参与式预算中的公平权衡：多Agent深度强化学习方法'}
{'arxiv_id': 'arXiv:2507.17412', 'title': 'Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for Tumor Flagging and Staging', 'authors': 'Farnaz Khun Jush, Steffen Vogler, Matthias Lenga', 'link': 'https://arxiv.org/abs/2507.17412', 'abstract': "The increasing volume of medical images poses challenges for radiologists in retrieving relevant cases. Content-based image retrieval (CBIR) systems offer potential for efficient access to similar cases, yet lack standardized evaluation and comprehensive studies. Building on prior studies for tumor characterization via CBIR, this study advances CBIR research for volumetric medical images through three key contributions: (1) a framework eliminating reliance on pre-segmented data and organ-specific datasets, aligning with large and unstructured image archiving systems, i.e. PACS in clinical practice; (2) introduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT's contextualized late interaction mechanism for 3D medical imaging; (3) comprehensive evaluation across four tumor sites using three feature extractors and three database configurations. Our evaluations highlight the significant advantages of C-MIR. We demonstrate the successful adaptation of the late interaction principle to volumetric medical images, enabling effective context-aware re-ranking. A key finding is C-MIR's ability to effectively localize the region of interest, eliminating the need for pre-segmentation of datasets and offering a computationally efficient alternative to systems relying on expensive data enrichment steps. C-MIR demonstrates promising improvements in tumor flagging, achieving improved performance, particularly for colon and lung tumors (p<0.05). C-MIR also shows potential for improving tumor staging, warranting further exploration of its capabilities. Ultimately, our work seeks to bridge the gap between advanced retrieval techniques and their practical applications in healthcare, paving the way for improved diagnostic processes.", 'abstract_zh': '医学图像数量的增长为放射学家检索相关病例带来了挑战。基于内容的图像检索（CBIR）系统提供了高效访问相似病例的潜力，但缺乏标准化评估和全面研究。在此基础上，本研究通过三项关键贡献促进了CBIR在体积医学图像中的研究：（1）构建了一种框架，无需依赖预先分割的数据和器官特异性数据集，与临床实践中大规模和非结构化的图像归档系统（如PACS）相契合；（2）引入了C-MIR，这是一种新的体积重排方法，通过适应ColBERT的上下文化晚期交互机制，用于3D医学成像；（3）在四个肿瘤部位上使用三种特征提取器和三种数据库配置进行了全面评估。我们的评估突显了C-MIR的重要优势。我们展示了晚期交互原则在体积医学图像中的成功适配，从而使有效的上下文感知重排成为可能。一个关键发现是C-MIR能够有效地定位感兴趣区域，消除对数据集预分割的需求，并为依赖昂贵数据丰富步骤的系统提供了计算高效的替代方案。C-MIR在肿瘤标记方面表现出有前景的改进，特别是在结肠和肺部肿瘤方面（p<0.05）。C-MIR还显示了在肿瘤分期方面改进的潜力，值得进一步探索其能力。最终，我们的工作旨在填补高级检索技术在医疗保健实际应用中的差距，为改进诊断过程铺平道路。', 'title_zh': '基于内容的3D图像检索及受ColBERT启发的再排序在肿瘤标记和分期中的应用'}
{'arxiv_id': 'arXiv:2507.17399', 'title': 'Millions of $\\text{GeAR}$-s: Extending GraphRAG to Millions of Documents', 'authors': 'Zhili Shen, Chenxin Diao, Pascual Merita, Pavlos Vougiouklis, Jeff Z. Pan', 'link': 'https://arxiv.org/abs/2507.17399', 'abstract': 'Recent studies have explored graph-based approaches to retrieval-augmented generation, leveraging structured or semi-structured information -- such as entities and their relations extracted from documents -- to enhance retrieval. However, these methods are typically designed to address specific tasks, such as multi-hop question answering and query-focused summarisation, and therefore, there is limited evidence of their general applicability across broader datasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG solution: $\\text{GeAR}$ and explore its performance and limitations on the SIGIR 2025 LiveRAG Challenge.', 'abstract_zh': '近年来，有研究表明通过图基于的方法增强生成以检索为基础，利用文档中提取的实体及其关系等结构化或半结构化信息来提升检索效果。然而，这些方法通常针对特定任务，如多跳问答和查询相关的总结，因此在更广泛的數據集上的通用性证据有限。在本文中，我们旨在适应一种最先进的图基于的RAG解决方案：GeAR，并探索其在SIGIR 2025 LiveRAG挑战上的性能和局限性。', 'title_zh': '百万级别的$\\text{GeAR}$：将GraphRAG扩展到百万级文档'}
{'arxiv_id': 'arXiv:2507.17394', 'title': 'HiProbe-VAD: Video Anomaly Detection via Hidden States Probing in Tuning-Free Multimodal LLMs', 'authors': 'Zhaolin Cai, Fan Li, Ziwei Zheng, Yanjun Qin', 'link': 'https://arxiv.org/abs/2507.17394', 'abstract': 'Video Anomaly Detection (VAD) aims to identify and locate deviations from normal patterns in video sequences. Traditional methods often struggle with substantial computational demands and a reliance on extensive labeled datasets, thereby restricting their practical applicability. To address these constraints, we propose HiProbe-VAD, a novel framework that leverages pre-trained Multimodal Large Language Models (MLLMs) for VAD without requiring fine-tuning. In this paper, we discover that the intermediate hidden states of MLLMs contain information-rich representations, exhibiting higher sensitivity and linear separability for anomalies compared to the output layer. To capitalize on this, we propose a Dynamic Layer Saliency Probing (DLSP) mechanism that intelligently identifies and extracts the most informative hidden states from the optimal intermediate layer during the MLLMs reasoning. Then a lightweight anomaly scorer and temporal localization module efficiently detects anomalies using these extracted hidden states and finally generate explanations. Experiments on the UCF-Crime and XD-Violence datasets demonstrate that HiProbe-VAD outperforms existing training-free and most traditional approaches. Furthermore, our framework exhibits remarkable cross-model generalization capabilities in different MLLMs without any tuning, unlocking the potential of pre-trained MLLMs for video anomaly detection and paving the way for more practical and scalable solutions.', 'abstract_zh': '基于预训练多模态大语言模型的无训练视频异常检测（HiProbe-VAD）', 'title_zh': 'HiProbe-VAD: 通过调优无干预的多模态LLM中隐藏状态探测进行视频异常检测'}
{'arxiv_id': 'arXiv:2507.17389', 'title': 'Investigating Training Data Detection in AI Coders', 'authors': 'Tianlin Li, Yunxiang Wei, Zhiming Li, Aishan Liu, Qing Guo, Xianglong Liu, Dongning Sun, Yang Liu', 'link': 'https://arxiv.org/abs/2507.17389', 'abstract': "Recent advances in code large language models (CodeLLMs) have made them indispensable tools in modern software engineering. However, these models occasionally produce outputs that contain proprietary or sensitive code snippets, raising concerns about potential non-compliant use of training data, and posing risks to privacy and intellectual property. To ensure responsible and compliant deployment of CodeLLMs, training data detection (TDD) has become a critical task. While recent TDD methods have shown promise in natural language settings, their effectiveness on code data remains largely underexplored. This gap is particularly important given code's structured syntax and distinct similarity criteria compared to natural language. To address this, we conduct a comprehensive empirical study of seven state-of-the-art TDD methods on source code data, evaluating their performance across eight CodeLLMs. To support this evaluation, we introduce CodeSnitch, a function-level benchmark dataset comprising 9,000 code samples in three programming languages, each explicitly labeled as either included or excluded from CodeLLM training. Beyond evaluation on the original CodeSnitch, we design targeted mutation strategies to test the robustness of TDD methods under three distinct settings. These mutation strategies are grounded in the well-established Type-1 to Type-4 code clone detection taxonomy. Our study provides a systematic assessment of current TDD techniques for code and offers insights to guide the development of more effective and robust detection methods in the future.", 'abstract_zh': 'Recent advances in代码大型语言模型（CodeLLMs）使它们成为现代软件工程中不可或缺的工具。然而，这些模型偶尔会产生包含专有或敏感代码片段的输出，这引发了关于潜在训练数据不合规使用的担忧，并对隐私和知识产权构成了风险。为确保CodeLLMs的负责任和合规部署，训练数据检测（TDD）已成为一项关键任务。尽管最近的TDD方法在自然语言环境中显示了潜力，但它们在代码数据上的有效性仍远未被探索。鉴于代码具有结构化的语法和与自然语言不同的相似性标准，这一差距尤为重要。为解决这一问题，我们在源代码数据上对七种最先进的TDD方法进行了全面的实证研究，评估了它们在八种CodeLLMs上的性能。为了支持这一评估，我们引入了CodeSnitch，这是一个基于功能层次的基准数据集，包含9,000个不同编程语言的代码样本，每个样本都明确标注为包括或排除在CodeLLM训练中。除了在原始CodeSnitch上的评估外，我们还设计了目标驱动的突变策略，以在三种不同的设置下测试TDD方法的鲁棒性。这些突变策略基于成熟的代码克隆检测类型学，从Type-1到Type-4。我们的研究为当前的代码TDD技术提供了系统的评估，并为将来开发更有效和更 robust的检测方法提供了指导。', 'title_zh': '探究AI编码器中的训练数据检测'}
{'arxiv_id': 'arXiv:2507.17373', 'title': 'SFUOD: Source-Free Unknown Object Detection', 'authors': 'Keon-Hee Park, Seun-An Choe, Gyeong-Moon Park', 'link': 'https://arxiv.org/abs/2507.17373', 'abstract': 'Source-free object detection adapts a detector pre-trained on a source domain to an unlabeled target domain without requiring access to labeled source data. While this setting is practical as it eliminates the need for the source dataset during domain adaptation, it operates under the restrictive assumption that only pre-defined objects from the source domain exist in the target domain. This closed-set setting prevents the detector from detecting undefined objects. To ease this assumption, we propose Source-Free Unknown Object Detection (SFUOD), a novel scenario which enables the detector to not only recognize known objects but also detect undefined objects as unknown objects. To this end, we propose CollaPAUL (Collaborative tuning and Principal Axis-based Unknown Labeling), a novel framework for SFUOD. Collaborative tuning enhances knowledge adaptation by integrating target-dependent knowledge from the auxiliary encoder with source-dependent knowledge from the pre-trained detector through a cross-domain attention mechanism. Additionally, principal axes-based unknown labeling assigns pseudo-labels to unknown objects by estimating objectness via principal axes projection and confidence scores from model predictions. The proposed CollaPAUL achieves state-of-the-art performances on SFUOD benchmarks, and extensive experiments validate its effectiveness.', 'abstract_zh': '无源对象检测（Source-Free Object Detection）将预训练在源域上的检测器适应未标记的目标域，而不需要访问源数据的标记者。虽然这种设置在域适应中消除了对源数据集的需求，但它在严格的假设下运行，即目标域中只存在源域中的预定义对象。这种封闭集设置限制了检测器无法检测未定义的对象。为了缓解这一假设，我们提出了无源未知对象检测（Source-Free Unknown Object Detection，SFUOD），这是一种新的场景，使检测器不仅能识别已知对象，还能将未定义的对象检测为未知对象。为此，我们提出了CollaPAUL（协作调优和主轴导向的未知标签），这是一种无源未知对象检测的新框架。协作调优通过跨域注意力机制，将辅助编码器的目标依赖知识与预训练检测器的源依赖知识结合起来，增强知识适应。此外，主轴导向的未知标签通过主轴投影和模型预测的置信分数估计对象性，为未知对象分配伪标签。所提出的CollaPAUL在无源未知对象检测基准测试中达到了最先进的性能，并且广泛的实验验证了其有效性。', 'title_zh': '源代码免费未知物体检测'}
{'arxiv_id': 'arXiv:2507.17365', 'title': 'DynaSearcher: Dynamic Knowledge Graph Augmented Search Agent via Multi-Reward Reinforcement Learning', 'authors': 'Chuzhan Hao, Wenfeng Feng, Yuewei Zhang, Hao Wang', 'link': 'https://arxiv.org/abs/2507.17365', 'abstract': 'Multi-step agentic retrieval systems based on large language models (LLMs) have demonstrated remarkable performance in complex information search tasks. However, these systems still face significant challenges in practical applications, particularly in generating factually inconsistent intermediate queries and inefficient search trajectories, which can lead to reasoning deviations or redundant computations. To address these issues, we propose DynaSearcher, an innovative search agent enhanced by dynamic knowledge graphs and multi-reward reinforcement learning (RL). Specifically, our system leverages knowledge graphs as external structured knowledge to guide the search process by explicitly modeling entity relationships, thereby ensuring factual consistency in intermediate queries and mitigating biases from irrelevant information. Furthermore, we employ a multi-reward RL framework for fine-grained control over training objectives such as retrieval accuracy, efficiency, and response quality. This framework promotes the generation of high-quality intermediate queries and comprehensive final answers, while discouraging unnecessary exploration and minimizing information omissions or redundancy. Experimental results demonstrate that our approach achieves state-of-the-art answer accuracy on six multi-hop question answering datasets, matching frontier LLMs while using only small-scale models and limited computational resources. Furthermore, our approach demonstrates strong generalization and robustness across diverse retrieval environments and larger-scale models, highlighting its broad applicability.', 'abstract_zh': '基于大型语言模型（LLMs）的多步代理检索系统在复杂信息搜索任务中展现了出色的性能。然而，这些系统在实际应用中仍然面临着显著挑战，特别是在生成事实不一致的中间查询和低效的搜索轨迹方面，这可能导致推理偏差或重复计算。为了解决这些问题，我们提出了一种名为DynaSearcher的创新搜索代理，该代理融合了动态知识图和多奖励强化学习（RL）。具体来说，我们的系统利用知识图作为外部结构化知识来指导搜索过程，通过明确建模实体关系确保中间查询的事实一致性，并减轻无关信息带来的偏差。此外，我们采用多奖励RL框架对检索准确性、效率和响应质量等细粒度训练目标进行精确控制。该框架促进了高质量中间查询和全面最终答案的生成，并抑制不必要的探索，减少信息遗漏或冗余。实验结果表明，我们的方法在六个多跳问答数据集上实现了最先进的答案准确性，仅使用小型模型和有限的计算资源就匹配了前沿LLMs。此外，我们的方法在不同检索环境和更大规模模型中的泛化能力和鲁棒性较强，突显了其广泛的应用潜力。', 'title_zh': 'DynaSearcher: 动态知识图谱增强的多奖励强化学习搜索代理'}
{'arxiv_id': 'arXiv:2507.17347', 'title': 'Swin-TUNA : A Novel PEFT Approach for Accurate Food Image Segmentation', 'authors': 'Haotian Chen, Zhiyong Xiao', 'link': 'https://arxiv.org/abs/2507.17347', 'abstract': 'In the field of food image processing, efficient semantic segmentation techniques are crucial for industrial applications. However, existing large-scale Transformer-based models (such as FoodSAM) face challenges in meeting practical deploymentrequirements due to their massive parameter counts and high computational resource demands. This paper introduces TUNable Adapter module (Swin-TUNA), a Parameter Efficient Fine-Tuning (PEFT) method that integrates multiscale trainable adapters into the Swin Transformer architecture, achieving high-performance food image segmentation by updating only 4% of the parameters. The core innovation of Swin-TUNA lies in its hierarchical feature adaptation mechanism: it designs separable convolutions in depth and dimensional mappings of varying scales to address the differences in features between shallow and deep networks, combined with a dynamic balancing strategy for tasks-agnostic and task-specific features. Experiments demonstrate that this method achieves mIoU of 50.56% and 74.94% on the FoodSeg103 and UECFoodPix Complete datasets, respectively, surpassing the fully parameterized FoodSAM model while reducing the parameter count by 98.7% (to only 8.13M). Furthermore, Swin-TUNA exhibits faster convergence and stronger generalization capabilities in low-data scenarios, providing an efficient solution for assembling lightweight food image.', 'abstract_zh': '可调适配器模块在Swin Transformer中的高效细调方法及其在食品图像分割中的应用', 'title_zh': 'Swin-TUNA：一种用于准确食物图像分割的新型PEFT方法'}
{'arxiv_id': 'arXiv:2507.17334', 'title': 'Temporal Point-Supervised Signal Reconstruction: A Human-Annotation-Free Framework for Weak Moving Target Detection', 'authors': 'Weihua Gao, Chunxu Ren, Wenlong Niu, Xiaodong Peng', 'link': 'https://arxiv.org/abs/2507.17334', 'abstract': 'In low-altitude surveillance and early warning systems, detecting weak moving targets remains a significant challenge due to low signal energy, small spatial extent, and complex background clutter. Existing methods struggle with extracting robust features and suffer from the lack of reliable annotations. To address these limitations, we propose a novel Temporal Point-Supervised (TPS) framework that enables high-performance detection of weak targets without any manual this http URL of conventional frame-based detection, our framework reformulates the task as a pixel-wise temporal signal modeling problem, where weak targets manifest as short-duration pulse-like responses. A Temporal Signal Reconstruction Network (TSRNet) is developed under the TPS paradigm to reconstruct these transient this http URL adopts an encoder-decoder architecture and integrates a Dynamic Multi-Scale Attention (DMSAttention) module to enhance its sensitivity to diverse temporal patterns. Additionally, a graph-based trajectory mining strategy is employed to suppress false alarms and ensure temporal this http URL experiments on a purpose-built low-SNR dataset demonstrate that our framework outperforms state-of-the-art methods while requiring no human annotations. It achieves strong detection performance and operates at over 1000 FPS, underscoring its potential for real-time deployment in practical scenarios.', 'abstract_zh': '低-altitude 监控与预警系统中弱移动目标检测仍是一个重大挑战：一种基于时空点监督的新框架', 'title_zh': '基于时间点监督的信号重构：一种无需人工标注的弱移动目标检测框架'}
{'arxiv_id': 'arXiv:2507.17311', 'title': 'EarthLink: Interpreting Climate Signals with Self-Evolving AI Agents', 'authors': 'Zijie Guo, Jiong Wang, Xiaoyu Yue, Wangxu Wei, Zhe Jiang, Wanghan Xu, Ben Fei, Wenlong Zhang, Xinyu Gu, Lijing Cheng, Jing-Jia Luo, Chao Li, Yaqiang Wang, Tao Chen, Wanli Ouyang, Fenghua Ling, Lei Bai', 'link': 'https://arxiv.org/abs/2507.17311', 'abstract': "Modern Earth science is at an inflection point. The vast, fragmented, and complex nature of Earth system data, coupled with increasingly sophisticated analytical demands, creates a significant bottleneck for rapid scientific discovery. Here we introduce EarthLink, the first AI agent designed as an interactive copilot for Earth scientists. It automates the end-to-end research workflow, from planning and code generation to multi-scenario analysis. Unlike static diagnostic tools, EarthLink can learn from user interaction, continuously refining its capabilities through a dynamic feedback loop. We validated its performance on a number of core scientific tasks of climate change, ranging from model-observation comparisons to the diagnosis of complex phenomena. In a multi-expert evaluation, EarthLink produced scientifically sound analyses and demonstrated an analytical competency that was rated as comparable to specific aspects of a human junior researcher's workflow. Additionally, its transparent, auditable workflows and natural language interface empower scientists to shift from laborious manual execution to strategic oversight and hypothesis generation. EarthLink marks a pivotal step towards an efficient, trustworthy, and collaborative paradigm for Earth system research in an era of accelerating global change.", 'abstract_zh': '现代地球科学正处于一个转折点。面向地球科学家的首个AI协作者EarthLink及其在地球系统研究中的应用', 'title_zh': '地球链接：自我进化的AI代理解释气候信号'}
{'arxiv_id': 'arXiv:2507.17309', 'title': 'Confounded Causal Imitation Learning with Instrumental Variables', 'authors': 'Yan Zeng, Shenglan Nie, Feng Xie, Libo Huang, Peng Wu, Zhi Geng', 'link': 'https://arxiv.org/abs/2507.17309', 'abstract': 'Imitation learning from demonstrations usually suffers from the confounding effects of unmeasured variables (i.e., unmeasured confounders) on the states and actions. If ignoring them, a biased estimation of the policy would be entailed. To break up this confounding gap, in this paper, we take the best of the strong power of instrumental variables (IV) and propose a Confounded Causal Imitation Learning (C2L) model. This model accommodates confounders that influence actions across multiple timesteps, rather than being restricted to immediate temporal dependencies. We develop a two-stage imitation learning framework for valid IV identification and policy optimization. In particular, in the first stage, we construct a testing criterion based on the defined pseudo-variable, with which we achieve identifying a valid IV for the C2L models. Such a criterion entails the sufficient and necessary identifiability conditions for IV validity. In the second stage, with the identified IV, we propose two candidate policy learning approaches: one is based on a simulator, while the other is offline. Extensive experiments verified the effectiveness of identifying the valid IV as well as learning the policy.', 'abstract_zh': '受未观测变量混杂效应影响的演示模仿学习通常会导致策略估计产生偏差。为了解决这一混杂问题，本文利用工具变量的强大功能提出了一个包含混杂因素的因果模仿学习（C2L）模型。该模型能够处理影响动作的混杂因素跨多个时间步的影响，而不局限于即时时间依赖性。我们开发了一种双重模仿学习框架，用于有效识别工具变量并优化策略。特别是，在第一阶段，我们基于定义的伪变量构建了一个检验准则，以识别适用于C2L模型的有效工具变量，该准则蕴含了工具变量有效性的充分必要识别条件。在第二阶段，利用识别出的工具变量，我们提出了两种候选策略学习方法：一种基于模拟器，另一种是离线方法。广泛的实验验证了识别有效工具变量以及学习策略的有效性。', 'title_zh': '含有工具变量的混杂因果模仿学习'}
{'arxiv_id': 'arXiv:2507.17303', 'title': 'A Versatile Pathology Co-pilot via Reasoning Enhanced Multimodal Large Language Model', 'authors': 'Zhe Xu, Ziyi Liu, Junlin Hou, Jiabo Ma, Cheng Jin, Yihui Wang, Zhixuan Chen, Zhengyu Zhang, Zhengrui Guo, Fengtao Zhou, Yingxue Xu, Xi Wang, Ronald Cheong Kin Chan, Li Liang, Hao Chen', 'link': 'https://arxiv.org/abs/2507.17303', 'abstract': 'Multimodal large language models (MLLMs) have emerged as powerful tools for computational pathology, offering unprecedented opportunities to integrate pathological images with language context for comprehensive diagnostic analysis. These models hold particular promise for automating complex tasks that traditionally require expert interpretation of pathologists. However, current MLLM approaches in pathology demonstrate significantly constrained reasoning capabilities, primarily due to their reliance on expensive chain-of-thought annotations. Additionally, existing methods remain limited to simplex application of visual question answering (VQA) at region-of-interest (ROI) level, failing to address the full spectrum of diagnostic needs such as ROI classification, detection, segmentation, whole-slide-image (WSI) classification and VQA in clinical practice. In this study, we present SmartPath-R1, a versatile MLLM capable of simultaneously addressing both ROI-level and WSI-level tasks while demonstrating robust pathological reasoning capability. Our framework combines scale-dependent supervised fine-tuning and task-aware reinforcement fine-tuning, which circumvents the requirement for chain-of-thought supervision by leveraging the intrinsic knowledge within MLLM. Furthermore, SmartPath-R1 integrates multiscale and multitask analysis through a mixture-of-experts mechanism, enabling dynamic processing for diverse tasks. We curate a large-scale dataset comprising 2.3M ROI samples and 188K WSI samples for training and evaluation. Extensive experiments across 72 tasks validate the effectiveness and superiority of the proposed approach. This work represents a significant step toward developing versatile, reasoning-enhanced AI systems for precision pathology.', 'abstract_zh': '多模态大型语言模型（MLLMs）在病理学中的应用：一种同时解决ROI级和WSI级任务的智能推理方法', 'title_zh': '一种基于推理增强多模态大型语言模型的通用病理协驾系统'}
{'arxiv_id': 'arXiv:2507.17297', 'title': 'On Temporal Guidance and Iterative Refinement in Audio Source Separation', 'authors': 'Tobias Morocutti, Jonathan Greif, Paul Primus, Florian Schmid, Gerhard Widmer', 'link': 'https://arxiv.org/abs/2507.17297', 'abstract': "Spatial semantic segmentation of sound scenes (S5) involves the accurate identification of active sound classes and the precise separation of their sources from complex acoustic mixtures. Conventional systems rely on a two-stage pipeline - audio tagging followed by label-conditioned source separation - but are often constrained by the absence of fine-grained temporal information critical for effective separation. In this work, we address this limitation by introducing a novel approach for S5 that enhances the synergy between the event detection and source separation stages. Our key contributions are threefold. First, we fine-tune a pre-trained Transformer to detect active sound classes. Second, we utilize a separate instance of this fine-tuned Transformer to perform sound event detection (SED), providing the separation module with detailed, time-varying guidance. Third, we implement an iterative refinement mechanism that progressively enhances separation quality by recursively reusing the separator's output from previous iterations. These advancements lead to significant improvements in both audio tagging and source separation performance, as demonstrated by our system's second-place finish in Task 4 of the DCASE Challenge 2025. Our implementation and model checkpoints are available in our GitHub repository: this https URL .", 'abstract_zh': '声 scene 空间语义分割 (S5) 涉及准确识别活动声类并从复杂声学混合中精确分离其来源。传统系统依赖于两阶段流水线——音频标记后继以基于标签的声源分离，但常受限于缺乏有效分离所必需的细粒度时间信息。在本文中，我们通过引入一种增强事件检测与声源分离阶段协同的新方法来解决这一限制。我们的主要贡献包括三个方面。首先，我们微调了一个预训练的 Transformer 来检测活动声类。其次，我们利用一个单独的微调 Transformer 实例来进行声事件检测 (SED)，为分离模块提供详细的、时间变化的指导。第三，我们实施了一种迭代精炼机制，通过递归重复使用上一次迭代的分离器输出来逐步提升分离质量。这些进步在音频标记和声源分离性能上取得了显著提高，如我们在 2025 年 DCASE 挑战赛 Task 4 中获得第二名所展示的那样。我们的实现和模型检查点可在我们的 GitHub 仓库中获取：this https URL 。', 'title_zh': '基于时间指导与迭代细化的音频源分离'}
{'arxiv_id': 'arXiv:2507.17291', 'title': 'Integrating Belief Domains into Probabilistic Logic Programs', 'authors': 'Damiano Azzolini, Fabrizio Riguzzi, Theresa Swift', 'link': 'https://arxiv.org/abs/2507.17291', 'abstract': 'Probabilistic Logic Programming (PLP) under the Distribution Semantics is a leading approach to practical reasoning under uncertainty. An advantage of the Distribution Semantics is its suitability for implementation as a Prolog or Python library, available through two well-maintained implementations, namely ProbLog and cplint/PITA. However, current formulations of the Distribution Semantics use point-probabilities, making it difficult to express epistemic uncertainty, such as arises from, for example, hierarchical classifications from computer vision models. Belief functions generalize probability measures as non-additive capacities, and address epistemic uncertainty via interval probabilities. This paper introduces interval-based Capacity Logic Programs based on an extension of the Distribution Semantics to include belief functions, and describes properties of the new framework that make it amenable to practical applications.', 'abstract_zh': '基于容度的区间逻辑程序：扩展分布语义纳入信念函数以处理认知不确定性', 'title_zh': '将信念域集成到概率逻辑程序中'}
{'arxiv_id': 'arXiv:2507.17273', 'title': 'Leveraging Knowledge Graphs and LLM Reasoning to Identify Operational Bottlenecks for Warehouse Planning Assistance', 'authors': 'Rishi Parekh, Saisubramaniam Gopalakrishnan, Zishan Ahmad, Anirudh Deodhar', 'link': 'https://arxiv.org/abs/2507.17273', 'abstract': 'Analyzing large, complex output datasets from Discrete Event Simulations (DES) of warehouse operations to identify bottlenecks and inefficiencies is a critical yet challenging task, often demanding significant manual effort or specialized analytical tools. Our framework integrates Knowledge Graphs (KGs) and Large Language Model (LLM)-based agents to analyze complex Discrete Event Simulation (DES) output data from warehouse operations. It transforms raw DES data into a semantically rich KG, capturing relationships between simulation events and entities. An LLM-based agent uses iterative reasoning, generating interdependent sub-questions. For each sub-question, it creates Cypher queries for KG interaction, extracts information, and self-reflects to correct errors. This adaptive, iterative, and self-correcting process identifies operational issues mimicking human analysis. Our DES approach for warehouse bottleneck identification, tested with equipment breakdowns and process irregularities, outperforms baseline methods. For operational questions, it achieves near-perfect pass rates in pinpointing inefficiencies. For complex investigative questions, we demonstrate its superior diagnostic ability to uncover subtle, interconnected issues. This work bridges simulation modeling and AI (KG+LLM), offering a more intuitive method for actionable insights, reducing time-to-insight, and enabling automated warehouse inefficiency evaluation and diagnosis.', 'abstract_zh': '基于知识图谱和大语言模型代理的仓库操作离散事件模拟复杂输出数据分析框架', 'title_zh': '利用知识图谱和大规模语言模型推理识别仓储规划辅助中的运营瓶颈'}
{'arxiv_id': 'arXiv:2507.17264', 'title': 'Understanding Prompt Programming Tasks and Questions', 'authors': 'Jenny T. Liang, Chenyang Yang, Agnia Sergeyuk, Travis D. Breaux, Brad A. Myers', 'link': 'https://arxiv.org/abs/2507.17264', 'abstract': "Prompting foundation models (FMs) like large language models (LLMs) have enabled new AI-powered software features (e.g., text summarization) that previously were only possible by fine-tuning FMs. Now, developers are embedding prompts in software, known as prompt programs. The process of prompt programming requires the developer to make many changes to their prompt. Yet, the questions developers ask to update their prompt is unknown, despite the answers to these questions affecting how developers plan their changes. With the growing number of research and commercial prompt programming tools, it is unclear whether prompt programmers' needs are being adequately addressed. We address these challenges by developing a taxonomy of 25 tasks prompt programmers do and 51 questions they ask, measuring the importance of each task and question. We interview 16 prompt programmers, observe 8 developers make prompt changes, and survey 50 developers. We then compare the taxonomy with 48 research and commercial tools. We find that prompt programming is not well-supported: all tasks are done manually, and 16 of the 51 questions -- including a majority of the most important ones -- remain unanswered. Based on this, we outline important opportunities for prompt programming tools.", 'abstract_zh': 'Prompt编程：任务分类与需求分析', 'title_zh': '理解提示编程任务和问题'}
{'arxiv_id': 'arXiv:2507.17248', 'title': 'Reality Proxy: Fluid Interactions with Real-World Objects in MR via Abstract Representations', 'authors': 'Xiaoan Liu, Difan Jia, Xianhao Carton Liu, Mar Gonzalez-Franco, Chen Zhu-Tian', 'link': 'https://arxiv.org/abs/2507.17248', 'abstract': "Interacting with real-world objects in Mixed Reality (MR) often proves difficult when they are crowded, distant, or partially occluded, hindering straightforward selection and manipulation. We observe that these difficulties stem from performing interaction directly on physical objects, where input is tightly coupled to their physical constraints. Our key insight is to decouple interaction from these constraints by introducing proxies-abstract representations of real-world objects. We embody this concept in Reality Proxy, a system that seamlessly shifts interaction targets from physical objects to their proxies during selection. Beyond facilitating basic selection, Reality Proxy uses AI to enrich proxies with semantic attributes and hierarchical spatial relationships of their corresponding physical objects, enabling novel and previously cumbersome interactions in MR - such as skimming, attribute-based filtering, navigating nested groups, and complex multi object selections - all without requiring new gestures or menu systems. We demonstrate Reality Proxy's versatility across diverse scenarios, including office information retrieval, large-scale spatial navigation, and multi-drone control. An expert evaluation suggests the system's utility and usability, suggesting that proxy-based abstractions offer a powerful and generalizable interaction paradigm for future MR systems.", 'abstract_zh': '在混合现实（MR）中与拥挤、遥远或部分遮挡的实际物体互动往往很困难，这阻碍了直接的选择和操作。我们观察到这些困难源于直接在物理对象上进行交互，其中输入紧密耦合于物理约束。我们的关键洞察是通过引入代理——现实世界物体的抽象表示，来解耦交互与这些约束。我们提出了Reality Proxy系统，该系统在选择过程中无缝地将交互目标从物理对象转移到其代理。超越基本的选择，Reality Proxy利用AI为代理添加对应的物理对象的语义属性和层次空间关系，这在MR中促成了一些以前难以实现的新颖交互，例如浏览、基于属性的筛选、导航嵌套组和复杂的多对象选择，而无需新的手势或菜单系统。我们展示了Reality Proxy在多种场景中的灵活性，包括办公室信息检索、大规模空间导航和多无人机控制。专家评估表明该系统的实用性和易用性，表明基于代理的抽象为未来的MR系统提供了一种强大且通用的交互范式。', 'title_zh': '现实代理：通过抽象表示在MR中与真实世界物体进行流体交互'}
{'arxiv_id': 'arXiv:2507.17245', 'title': 'DistrAttention: An Efficient and Flexible Self-Attention Mechanism on Modern GPUs', 'authors': 'Haolin Jin, Mengbai Xiao, Yuan Yuan, Xiao Zhang, Dongxiao Yu, Guanghui Zhang, Haoliang Wang', 'link': 'https://arxiv.org/abs/2507.17245', 'abstract': 'The Transformer architecture has revolutionized deep learning, delivering the state-of-the-art performance in areas such as natural language processing, computer vision, and time series prediction. However, its core component, self-attention, has the quadratic time complexity relative to input sequence length, which hinders the scalability of Transformers. The exsiting approaches on optimizing self-attention either discard full-contextual information or lack of flexibility. In this work, we design DistrAttention, an effcient and flexible self-attention mechanism with the full context. DistrAttention achieves this by grouping data on the embedding dimensionality, usually referred to as $d$. We realize DistrAttention with a lightweight sampling and fusion method that exploits locality-sensitive hashing to group similar data. A block-wise grouping framework is further designed to limit the errors introduced by locality sensitive hashing. By optimizing the selection of block sizes, DistrAttention could be easily integrated with FlashAttention-2, gaining high-performance on modern GPUs. We evaluate DistrAttention with extensive experiments. The results show that our method is 37% faster than FlashAttention-2 on calculating self-attention. In ViT inference, DistrAttention is the fastest and the most accurate among approximate self-attention mechanisms. In Llama3-1B, DistrAttention still achieves the lowest inference time with only 1% accuray loss.', 'abstract_zh': 'Transformer架构已 revolutionized 深度学习，特别是在自然语言处理、计算机视觉和时间序列预测等领域提供了最先进的性能。然而，其核心组件自注意力的时间复杂度为输入序列长度的平方级，这限制了Transformer的扩展性。现有的自注意力优化方法要么丢弃上下文信息，要么缺乏灵活性。在本工作中，我们设计了DistrAttention，这是一种高效且灵活的具有完整上下文的自注意力机制。DistrAttention 通过按嵌入维度分组数据来实现这一点，通常称为 $d$。我们使用轻量级采样和融合方法，结合局部敏感哈希来分组相似数据。我们进一步设计了一种块级分组框架，以限制局部敏感哈希引入的误差。通过优化块大小的选择，DistrAttention 可以很容易地与 FlashAttention-2 集成，在现代GPU上获得高性能。我们通过广泛实验评估了DistrAttention。结果表明，与FlashAttention-2相比，我们的方法在计算自注意力时速度提高了37%。在ViT推理中，DistrAttention 是最快且最准确的近似自注意力机制。在Llama3-1B中，即使只有1%的准确率损失，DistrAttention 仍然实现了最低的推理时间。', 'title_zh': 'DistrAttention：现代GPU上的高效灵活自我注意力机制'}
{'arxiv_id': 'arXiv:2507.17241', 'title': 'Eco-Friendly AI: Unleashing Data Power for Green Federated Learning', 'authors': 'Mattia Sabella, Monica Vitali', 'link': 'https://arxiv.org/abs/2507.17241', 'abstract': "The widespread adoption of Artificial Intelligence (AI) and Machine Learning (ML) comes with a significant environmental impact, particularly in terms of energy consumption and carbon emissions. This pressing issue highlights the need for innovative solutions to mitigate AI's ecological footprint. One of the key factors influencing the energy consumption of ML model training is the size of the training dataset. ML models are often trained on vast amounts of data continuously generated by sensors and devices distributed across multiple locations. To reduce data transmission costs and enhance privacy, Federated Learning (FL) enables model training without the need to move or share raw data. While FL offers these advantages, it also introduces challenges due to the heterogeneity of data sources (related to volume and quality), computational node capabilities, and environmental impact.\nThis paper contributes to the advancement of Green AI by proposing a data-centric approach to Green Federated Learning. Specifically, we focus on reducing FL's environmental impact by minimizing the volume of training data. Our methodology involves the analysis of the characteristics of federated datasets, the selecting of an optimal subset of data based on quality metrics, and the choice of the federated nodes with the lowest environmental impact. We develop a comprehensive methodology that examines the influence of data-centric factors, such as data quality and volume, on FL training performance and carbon emissions. Building on these insights, we introduce an interactive recommendation system that optimizes FL configurations through data reduction, minimizing environmental impact during training. Applying this methodology to time series classification has demonstrated promising results in reducing the environmental impact of FL tasks.", 'abstract_zh': '人工智能和机器学习的广泛应用带来了显著的环境影响，特别是在能源消耗和碳排放方面。这一紧迫问题强调了需要创新解决方案以减轻人工智能的生态足迹。影响机器学习模型训练能耗的一个关键因素是训练数据集的大小。机器学习模型通常通过来自多个位置的传感器和设备不断生成的大规模数据进行训练。为了降低数据传输成本并增强隐私性，联邦学习（FL）使模型训练无需移动或共享原始数据成为可能。虽然FL带来了这些优势，但也由于数据源的异质性（包括数据量和质量）、计算节点的能力以及环境影响等因素引入了挑战。\n\n本文通过提出一种以数据为中心的绿色联邦学习方法，为绿色人工智能的发展做出贡献。具体来说，我们旨在通过减少训练数据的体积来降低联邦学习的环境影响。我们的方法包括分析联邦数据集的特性、根据质量指标选择最优子集数据以及选择环境影响最低的联邦节点。我们开发了一个全面的方法，评估数据相关因素（如数据质量和体积）对联邦学习训练性能和碳排放的影响。基于这些见解，我们引入了一种交互式推荐系统，通过数据减少来优化联邦学习配置，从而在训练过程中最小化环境影响。将此方法应用于时间序列分类任务中，结果表明这一方法在降低联邦学习任务的环境影响方面颇具成效。', 'title_zh': '环保AI：释放绿色联邦学习的数据力量'}
{'arxiv_id': 'arXiv:2507.17232', 'title': 'A Highly Clean Recipe Dataset with Ingredient States Annotation for State Probing Task', 'authors': 'Mashiro Toyooka, Kiyoharu Aizawa, Yoko Yamakata', 'link': 'https://arxiv.org/abs/2507.17232', 'abstract': "Large Language Models (LLMs) are trained on a vast amount of procedural texts, but they do not directly observe real-world phenomena. In the context of cooking recipes, this poses a challenge, as intermediate states of ingredients are often omitted, making it difficult for models to track ingredient states and understand recipes accurately. In this paper, we apply state probing, a method for evaluating a language model's understanding of the world, to the domain of cooking. We propose a new task and dataset for evaluating how well LLMs can recognize intermediate ingredient states during cooking procedures. We first construct a new Japanese recipe dataset with clear and accurate annotations of ingredient state changes, collected from well-structured and controlled recipe texts. Using this dataset, we design three novel tasks to evaluate whether LLMs can track ingredient state transitions and identify ingredients present at intermediate steps. Our experiments with widely used LLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient state knowledge improves their understanding of cooking processes, achieving performance comparable to commercial LLMs.", 'abstract_zh': '大型语言模型（LLMs）在大量程序性文本上进行训练，但它们并未直接观察到现实世界的现象。在烹饪食谱的背景下，这提出了一个挑战，因为配料的中间状态常被省略，使得模型难以追踪配料的状态并准确理解食谱。在本文中，我们应用状态探查方法，一种评估语言模型对世界理解能力的方法，将其应用于烹饪领域。我们提出一个新的任务和数据集，用于评估LLMs在烹饪程序中识别中间配料状态的能力。我们首先构建了一个新的日语食谱数据集，其中包含了清晰准确的配料状态变化标注，这些标注来自结构良好且受控的食谱文本。使用此数据集，我们设计了三个新型任务，以评估LLMs是否能够追踪配料状态转换并在中间步骤识别出存在的配料。我们的实验表明，学习配料状态知识增强了它们对烹饪过程的理解，性能与商业LLMs相当。', 'title_zh': '一种带有食材状态标注的 highly clean 食材数据集用于状态探查任务'}
{'arxiv_id': 'arXiv:2507.17228', 'title': 'P3SL: Personalized Privacy-Preserving Split Learning on Heterogeneous Edge Devices', 'authors': 'Wei Fan, JinYi Yoon, Xiaochang Li, Huajie Shao, Bo Ji', 'link': 'https://arxiv.org/abs/2507.17228', 'abstract': 'Split Learning (SL) is an emerging privacy-preserving machine learning technique that enables resource constrained edge devices to participate in model training by partitioning a model into client-side and server-side sub-models. While SL reduces computational overhead on edge devices, it encounters significant challenges in heterogeneous environments where devices vary in computing resources, communication capabilities, environmental conditions, and privacy requirements. Although recent studies have explored heterogeneous SL frameworks that optimize split points for devices with varying resource constraints, they often neglect personalized privacy requirements and local model customization under varying environmental conditions. To address these limitations, we propose P3SL, a Personalized Privacy-Preserving Split Learning framework designed for heterogeneous, resource-constrained edge device systems. The key contributions of this work are twofold. First, we design a personalized sequential split learning pipeline that allows each client to achieve customized privacy protection and maintain personalized local models tailored to their computational resources, environmental conditions, and privacy needs. Second, we adopt a bi-level optimization technique that empowers clients to determine their own optimal personalized split points without sharing private sensitive information (i.e., computational resources, environmental conditions, privacy requirements) with the server. This approach balances energy consumption and privacy leakage risks while maintaining high model accuracy. We implement and evaluate P3SL on a testbed consisting of 7 devices including 4 Jetson Nano P3450 devices, 2 Raspberry Pis, and 1 laptop, using diverse model architectures and datasets under varying environmental conditions.', 'abstract_zh': '个性化隐私保护分割学习框架（P3SL）：面向异构资源受限边缘设备系统', 'title_zh': '个性化隐私保护分割学习在异构边缘设备上'}
{'arxiv_id': 'arXiv:2507.17224', 'title': 'HuiduRep: A Robust Self-Supervised Framework for Learning Neural Representations from Extracellular Spikes', 'authors': 'Feng Cao, Zishuo Feng', 'link': 'https://arxiv.org/abs/2507.17224', 'abstract': 'Extracellular recordings are brief voltage fluctuations recorded near neurons, widely used in neuroscience as the basis for decoding brain activity at single-neuron resolution. Spike sorting, which assigns each spike to its source neuron, is a critical step in brain sensing pipelines. However, it remains challenging under low signal-to-noise ratio (SNR), electrode drift, and cross-session variability. In this paper, we propose HuiduRep, a robust self-supervised representation learning framework that extracts discriminative and generalizable features from extracellular spike waveforms. By combining contrastive learning with a denoising autoencoder, HuiduRep learns latent representations that are robust to noise and drift. Built on HuiduRep, we develop a spike sorting pipeline that clusters spike representations without supervision. Experiments on hybrid and real-world datasets demonstrate that HuiduRep achieves strong robustness and the pipeline matches or outperforms state-of-the-art tools such as KiloSort4 and MountainSort5. These findings demonstrate the potential of self-supervised spike representation learning as a foundational tool for robust and generalizable processing of extracellular recordings.', 'abstract_zh': '跨神经元电位的鲁棒自监督表示学习框架HuiduRep及其在尖端排序中的应用', 'title_zh': 'HuiduRep：一种从体外尖峰中学习神经表示的稳健自监督框架'}
{'arxiv_id': 'arXiv:2507.17216', 'title': 'The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models', 'authors': 'Giuseppe Russo, Debora Nozza, Paul Röttger, Dirk Hovy', 'link': 'https://arxiv.org/abs/2507.17216', 'abstract': "People increasingly rely on Large Language Models (LLMs) for moral advice, which may influence humans' decisions. Yet, little is known about how closely LLMs align with human moral judgments. To address this, we introduce the Moral Dilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a distribution of human moral judgments consisting of a binary evaluation and a free-text rationale. We treat this problem as a pluralistic distributional alignment task, comparing the distributions of LLM and human judgments across dilemmas. We find that models reproduce human judgments only under high consensus; alignment deteriorates sharply when human disagreement increases. In parallel, using a 60-value taxonomy built from 3,783 value expressions extracted from rationales, we show that LLMs rely on a narrower set of moral values than humans. These findings reveal a pluralistic moral gap: a mismatch in both the distribution and diversity of values expressed. To close this gap, we introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method that conditions model outputs on human-derived value profiles. DMP improves alignment by 64.3% and enhances value diversity, offering a step toward more pluralistic and human-aligned moral guidance from LLMs.", 'abstract_zh': '人们越来越依赖大型语言模型（LLMs）获取道德建议，这可能影响人类的决策。然而，我们对LLMs与人类道德判断的一致性知之甚少。为此，我们引入了道德困境数据集，这是一个包含1,618个现实世界道德困境及人类道德判断分布（包括二元评估和自由文本理由）的基准。我们将其视为多元分布对齐任务，比较了道德困境中LLMs和人类判断的分布。我们发现，模型仅在高度一致性下才复制人类判断；当人类分歧增加时，对齐急剧恶化。此外，我们使用一个由3,783个从理由中提取的价值表达构建的60值分类法，表明LLMs依赖的价值范畴比人类窄。这些发现揭示了一个多元道德差距：在价值表达的分布和多样性上存在不匹配。为缩小这一差距，我们提出了动态道德画像（DMP）方法，这是一种基于狄利克雷分布的采样方法，根据人类衍生的价值概貌调整模型输出。DMP将对齐提高了64.3%，并增强了价值多样性，为提供更加多元和人类对齐的道德指导迈出了一步。', 'title_zh': '多元的道德差距：理解和人类与大型语言模型之间的判断与价值差异'}
{'arxiv_id': 'arXiv:2507.17202', 'title': 'DesignLab: Designing Slides Through Iterative Detection and Correction', 'authors': 'Jooyeol Yun, Heng Wang, Yotaro Shimose, Jaegul Choo, Shingo Takamatsu', 'link': 'https://arxiv.org/abs/2507.17202', 'abstract': 'Designing high-quality presentation slides can be challenging for non-experts due to the complexity involved in navigating various design choices. Numerous automated tools can suggest layouts and color schemes, yet often lack the ability to refine their own output, which is a key aspect in real-world workflows. We propose DesignLab, which separates the design process into two roles, the design reviewer, who identifies design-related issues, and the design contributor who corrects them. This decomposition enables an iterative loop where the reviewer continuously detects issues and the contributor corrects them, allowing a draft to be further polished with each iteration, reaching qualities that were unattainable. We fine-tune large language models for these roles and simulate intermediate drafts by introducing controlled perturbations, enabling the design reviewer learn design errors and the contributor learn how to fix them. Our experiments show that DesignLab outperforms existing design-generation methods, including a commercial tool, by embracing the iterative nature of designing which can result in polished, professional slides.', 'abstract_zh': '设计Lab：通过拆分设计过程提升演示文稿质量', 'title_zh': 'DesignLab: 通过迭代检测与修正设计幻灯片'}
{'arxiv_id': 'arXiv:2507.17194', 'title': 'Dispatch-Aware Deep Neural Network for Optimal Transmission Switching: Toward Real-Time and Feasibility Guaranteed Operation', 'authors': 'Minsoo Kim, Jip Kim', 'link': 'https://arxiv.org/abs/2507.17194', 'abstract': 'Optimal transmission switching (OTS) improves optimal power flow (OPF) by selectively opening transmission lines, but its mixed-integer formulation increases computational complexity, especially on large grids. To deal with this, we propose a dispatch-aware deep neural network (DA-DNN) that accelerates DC-OTS without relying on pre-solved labels. DA-DNN predicts line states and passes them through a differentiable DC-OPF layer, using the resulting generation cost as the loss function so that all physical network constraints are enforced throughout training and inference. In addition, we adopt a customized weight-bias initialization that keeps every forward pass feasible from the first iteration, which allows stable learning on large grids. Once trained, the proposed DA-DNN produces a provably feasible topology and dispatch pair in the same time as solving the DCOPF, whereas conventional mixed-integer solvers become intractable. As a result, the proposed method successfully captures the economic advantages of OTS while maintaining scalability.', 'abstract_zh': '调度意识深度神经网络加速直流最优传输开关同时保证可行性cessive', 'title_zh': '基于调度感知的深度神经网络最优传输开关决策：迈向实时可行运行'}
{'arxiv_id': 'arXiv:2507.17188', 'title': 'LLM Meets the Sky: Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks', 'authors': 'Lijie Zheng, Ji He, Shih Yu Chang, Yulong Shen, Dusit Niyato', 'link': 'https://arxiv.org/abs/2507.17188', 'abstract': 'This work tackles the physical layer security (PLS) problem of maximizing the secrecy rate in heterogeneous UAV networks (HetUAVNs) under propulsion energy constraints. Unlike prior studies that assume uniform UAV capabilities or overlook energy-security trade-offs, we consider a realistic scenario where UAVs with diverse payloads and computation resources collaborate to serve ground terminals in the presence of eavesdroppers. To manage the complex coupling between UAV motion and communication, we propose a hierarchical optimization framework. The inner layer uses a semidefinite relaxation (SDR)-based S2DC algorithm combining penalty functions and difference-of-convex (d.c.) programming to solve the secrecy precoding problem with fixed UAV positions. The outer layer introduces a Large Language Model (LLM)-guided heuristic multi-agent reinforcement learning approach (LLM-HeMARL) for trajectory optimization. LLM-HeMARL efficiently incorporates expert heuristics policy generated by the LLM, enabling UAVs to learn energy-aware, security-driven trajectories without the inference overhead of real-time LLM calls. The simulation results show that our method outperforms existing baselines in secrecy rate and energy efficiency, with consistent robustness across varying UAV swarm sizes and random seeds.', 'abstract_zh': '本研究解决了动力能量约束下异构无人机网络（HetUAVNs）物理层安全（PLS）问题，旨在最大化保密率。不同于以往研究中假设无人机能力均匀或忽视能量－安全权衡，我们考虑了一个现实场景，在该场景中，配备不同载荷和计算资源的无人机协同工作，为地面终端服务，并同时对抗窃听者。为了处理无人机运动与通信之间的复杂耦合，我们提出了一种分层优化框架。内层采用基于半定 relaxation (SDR) 的带有惩罚函数和凸二次锥（d.c.）编程的 S2DC 算法来解决固定无人机位置下的保密前向编码问题。外层引入了一种由大型语言模型（LLM）引导的启发式多代理强化学习方法（LLM-HeMARL）进行轨迹优化。LLM-HeMARL 有效地整合了由 LLM 生成的专家启发式策略，使无人机能够学习能量意识强且安全导向的轨迹，而无需进行实时 LLM 推理的开销。仿真结果表明，与现有基准方法相比，我们的方法在保密率和能效方面更具优势，并且在不同的无人机群大小和随机种子下表现出一致的鲁棒性。', 'title_zh': 'LLM 接触天空：启发式多代理强化学习在安全异构无人机网络中的应用'}
{'arxiv_id': 'arXiv:2507.17185', 'title': 'Asymmetric Lesion Detection with Geometric Patterns and CNN-SVM Classification', 'authors': 'M. A. Rasel, Sameem Abdul Kareem, Zhenli Kwan, Nik Aimee Azizah Faheem, Winn Hui Han, Rebecca Kai Jan Choong, Shin Shen Yong, Unaizah Obaidellah', 'link': 'https://arxiv.org/abs/2507.17185', 'abstract': 'In dermoscopic images, which allow visualization of surface skin structures not visible to the naked eye, lesion shape offers vital insights into skin diseases. In clinically practiced methods, asymmetric lesion shape is one of the criteria for diagnosing melanoma. Initially, we labeled data for a non-annotated dataset with symmetrical information based on clinical assessments. Subsequently, we propose a supporting technique, a supervised learning image processing algorithm, to analyze the geometrical pattern of lesion shape, aiding non-experts in understanding the criteria of an asymmetric lesion. We then utilize a pre-trained convolutional neural network (CNN) to extract shape, color, and texture features from dermoscopic images for training a multiclass support vector machine (SVM) classifier, outperforming state-of-the-art methods from the literature. In the geometry-based experiment, we achieved a 99.00% detection rate for dermatological asymmetric lesions. In the CNN-based experiment, the best performance is found with 94% Kappa Score, 95% Macro F1-score, and 97% Weighted F1-score for classifying lesion shapes (Asymmetric, Half-Symmetric, and Symmetric).', 'abstract_zh': '在皮肤镜图像中，这些图像允许可视化肉眼不可见的皮肤表面结构，病灶形状为皮肤疾病提供了关键见解。在临床实践中，病灶不对称性是诊断黑色素瘤的标准之一。最初，我们基于临床评估为一个未标注的数据集标注对称信息。随后，我们提出了一个支持技术，即监督学习图像处理算法，用于分析病灶形状的几何模式，帮助非专家理解不对称病灶的标准。我们然后使用预训练的卷积神经网络（CNN）从皮肤镜图像中提取形状、颜色和纹理特征，训练一个多类支持向量机（SVM）分类器，性能超越了文献中的现有方法。在基于几何的实验中，病理性不对称病灶的检测率达到99.00%。在基于CNN的实验中，最佳性能为94%的Kappa分数、95%的宏F1分数和97%加权F1分数，用于分类病灶形状（不对称、半对称和对称）。', 'title_zh': '不对称病灶检测：基于几何模式和CNN-SVM分类'}
{'arxiv_id': 'arXiv:2507.17183', 'title': 'Regret Minimization in Population Network Games: Vanishing Heterogeneity and Convergence to Equilibria', 'authors': 'Die Hu, Shuyue Hu, Chunjiang Mu, Shiqi Fan, Chen Chu, Jinzhuo Liu, Zhen Wang', 'link': 'https://arxiv.org/abs/2507.17183', 'abstract': 'Understanding and predicting the behavior of large-scale multi-agents in games remains a fundamental challenge in multi-agent systems. This paper examines the role of heterogeneity in equilibrium formation by analyzing how smooth regret-matching drives a large number of heterogeneous agents with diverse initial policies toward unified behavior. By modeling the system state as a probability distribution of regrets and analyzing its evolution through the continuity equation, we uncover a key phenomenon in diverse multi-agent settings: the variance of the regret distribution diminishes over time, leading to the disappearance of heterogeneity and the emergence of consensus among agents. This universal result enables us to prove convergence to quantal response equilibria in both competitive and cooperative multi-agent settings. Our work advances the theoretical understanding of multi-agent learning and offers a novel perspective on equilibrium selection in diverse game-theoretic scenarios.', 'abstract_zh': '大规模多智能体在博弈中的行为理解与预测仍然是多智能体系统中的一个基础挑战。本文通过分析平滑遗憾匹配如何引导大量异质智能体朝向统一行为，研究异质性在均衡形成中的作用。通过将系统状态建模为遗憾的概率分布，并通过连续方程分析其演变，我们揭示了一个多智能体系统中普遍的现象：遗憾分布的方差随时间减少，导致异质性的消失和智能体间共识的涌现。这一普遍结果使我们能够证明在竞争性和合作性多智能体设置中收敛于量子响应均衡。我们的工作推进了多智能体学习的理论理解，并为不同博弈论场景中的均衡选择提供了新的视角。', 'title_zh': '群体网络游戏中后悔最小化：异质性的消失与均衡收敛'}
{'arxiv_id': 'arXiv:2507.17178', 'title': 'SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs', 'authors': 'Zhiqiang Liu, Enpei Niu, Yin Hua, Mengshu Sun, Lei Liang, Huajun Chen, Wen Zhang', 'link': 'https://arxiv.org/abs/2507.17178', 'abstract': 'Although large language models (LLMs) have made significant progress in understanding Structured Knowledge (SK) like KG and Table, existing evaluations for SK understanding are non-rigorous (i.e., lacking evaluations of specific capabilities) and focus on a single type of SK. Therefore, we aim to propose a more comprehensive and rigorous structured knowledge understanding benchmark to diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a Structured Knowledge Augmented QA Benchmark that encompasses four widely used structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a three-stage pipeline to construct SKA-Bench instances, which includes a question, an answer, positive knowledge units, and noisy knowledge units. To evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we expand the instances into four fundamental ability testbeds: Noise Robustness, Order Insensitivity, Information Integration, and Negative Rejection. Empirical evaluations on 8 representative LLMs, including the advanced DeepSeek-R1, indicate that existing LLMs still face significant challenges in understanding structured knowledge, and their performance is influenced by factors such as the amount of noise, the order of knowledge units, and hallucination phenomenon. Our dataset and code are available at this https URL.', 'abstract_zh': '尽管大规模语言模型（LLMs）在理解结构化知识（SK）如知识图谱（KG）和表格方面取得了显著进展，现有的SK理解评估缺乏严谨性（即缺乏对特定能力的评估）且主要关注单一类型的SK。因此，我们旨在提出一个更加全面和严谨的结构化知识理解基准，以诊断LLMs的不足之处。在本文中，我们介绍了SKA-Bench，这是一种结构化知识增强问答基准，涵盖了四种广泛使用的结构化知识形式：知识图谱（KG）、表格（Table）、知识图谱+文本（KG+Text）和表格+文本（Table+Text）。我们采用三阶段管道构建SKA-Bench实例，包括一个问题、一个答案、正面的知识单元和噪声的知识单元。为细粒度地评估LLMs的SK理解能力，我们将实例扩展为四项基本能力测试平台：噪声鲁棒性、顺序无关性、信息整合和负事实拒绝。在8个代表性LLM上的实证评估，包括先进的DeepSeek-R1，表明现有的LLM在理解结构化知识方面仍面临重大挑战，其性能受到噪声量、知识单元顺序和幻觉现象等因素的影响。我们的数据集和代码可在以下链接获取。', 'title_zh': 'SKA-Bench: 一种细粒度基准测试，用于评估LLMs的结构化知识理解能力'}
{'arxiv_id': 'arXiv:2507.17161', 'title': 'Tabular Diffusion based Actionable Counterfactual Explanations for Network Intrusion Detection', 'authors': 'Vinura Galwaduge, Jagath Samarabandu', 'link': 'https://arxiv.org/abs/2507.17161', 'abstract': 'Modern network intrusion detection systems (NIDS) frequently utilize the predictive power of complex deep learning models. However, the "black-box" nature of such deep learning methods adds a layer of opaqueness that hinders the proper understanding of detection decisions, trust in the decisions and prevent timely countermeasures against such attacks. Explainable AI (XAI) methods provide a solution to this problem by providing insights into the causes of the predictions. The majority of the existing XAI methods provide explanations which are not convenient to convert into actionable countermeasures. In this work, we propose a novel diffusion-based counterfactual explanation framework that can provide actionable explanations for network intrusion attacks. We evaluated our proposed algorithm against several other publicly available counterfactual explanation algorithms on 3 modern network intrusion datasets. To the best of our knowledge, this work also presents the first comparative analysis of existing counterfactual explanation algorithms within the context of network intrusion detection systems. Our proposed method provide minimal, diverse counterfactual explanations out of the tested counterfactual explanation algorithms in a more efficient manner by reducing the time to generate explanations. We also demonstrate how counterfactual explanations can provide actionable explanations by summarizing them to create a set of global rules. These rules are actionable not only at instance level but also at the global level for intrusion attacks. These global counterfactual rules show the ability to effectively filter out incoming attack queries which is crucial for efficient intrusion detection and defense mechanisms.', 'abstract_zh': '基于扩散的网络入侵攻击可操作反事实解释框架', 'title_zh': '基于表格扩散的可操作反事实解释以用于网络入侵检测'}
{'arxiv_id': 'arXiv:2507.17152', 'title': 'JAM: Keypoint-Guided Joint Prediction after Classification-Aware Marginal Proposal for Multi-Agent Interaction', 'authors': 'Fangze Lin, Ying He, Fei Yu, Hong Zhang', 'link': 'https://arxiv.org/abs/2507.17152', 'abstract': 'Predicting the future motion of road participants is a critical task in autonomous driving. In this work, we address the challenge of low-quality generation of low-probability modes in multi-agent joint prediction. To tackle this issue, we propose a two-stage multi-agent interactive prediction framework named \\textit{keypoint-guided joint prediction after classification-aware marginal proposal} (JAM). The first stage is modeled as a marginal prediction process, which classifies queries by trajectory type to encourage the model to learn all categories of trajectories, providing comprehensive mode information for the joint prediction module. The second stage is modeled as a joint prediction process, which takes the scene context and the marginal proposals from the first stage as inputs to learn the final joint distribution. We explicitly introduce key waypoints to guide the joint prediction module in better capturing and leveraging the critical information from the initial predicted trajectories. We conduct extensive experiments on the real-world Waymo Open Motion Dataset interactive prediction benchmark. The results show that our approach achieves competitive performance. In particular, in the framework comparison experiments, the proposed JAM outperforms other prediction frameworks and achieves state-of-the-art performance in interactive trajectory prediction. The code is available at this https URL to facilitate future research.', 'abstract_zh': '基于分类意识边缘提案和关键点引导的多Agent联合预测框架（JAM）：面向道路参与者的未来运动预测', 'title_zh': 'JAM：分类感知边缘提案指导的关键点引导多Agent交互联合预测'}
{'arxiv_id': 'arXiv:2507.17149', 'title': 'ScSAM: Debiasing Morphology and Distributional Variability in Subcellular Semantic Segmentation', 'authors': 'Bo Fang, Jianan Fan, Dongnan Liu, Hang Chang, Gerald J.Shami, Filip Braet, Weidong Cai', 'link': 'https://arxiv.org/abs/2507.17149', 'abstract': 'The significant morphological and distributional variability among subcellular components poses a long-standing challenge for learning-based organelle segmentation models, significantly increasing the risk of biased feature learning. Existing methods often rely on single mapping relationships, overlooking feature diversity and thereby inducing biased training. Although the Segment Anything Model (SAM) provides rich feature representations, its application to subcellular scenarios is hindered by two key challenges: (1) The variability in subcellular morphology and distribution creates gaps in the label space, leading the model to learn spurious or biased features. (2) SAM focuses on global contextual understanding and often ignores fine-grained spatial details, making it challenging to capture subtle structural alterations and cope with skewed data distributions. To address these challenges, we introduce ScSAM, a method that enhances feature robustness by fusing pre-trained SAM with Masked Autoencoder (MAE)-guided cellular prior knowledge to alleviate training bias from data imbalance. Specifically, we design a feature alignment and fusion module to align pre-trained embeddings to the same feature space and efficiently combine different representations. Moreover, we present a cosine similarity matrix-based class prompt encoder to activate class-specific features to recognize subcellular categories. Extensive experiments on diverse subcellular image datasets demonstrate that ScSAM outperforms state-of-the-art methods.', 'abstract_zh': '子细胞成分之间显著的形态和分布差异对基于学习的器质分割模型构成长期挑战，显著增加了偏向性特征学习的风险。现有方法往往依赖单一映射关系，忽视了特征的多样性，从而导致训练中的偏向性。尽管Segment Anything Model (SAM) 提供了丰富的特征表示，但在子细胞场景中的应用受到两个关键挑战的阻碍：(1) 子细胞形态和分布的差异在标签空间中造成了缺口，导致模型学习虚假或偏向性特征。(2) SAM 专注于全局上下文理解，经常忽略细粒度的空间细节，使其难以捕获细微的结构改变并应对数据分布的偏斜。为应对这些挑战，我们引入了ScSAM 方法，通过将预训练的SAM与Masked Autoencoder (MAE)-引导的细胞先验知识融合，来缓解由数据不平衡引起的训练偏向性。具体而言，我们设计了一个特征对齐和融合模块，以将预训练嵌入对齐到相同的特征空间并有效组合不同的表示。此外，我们提出了基于余弦相似性矩阵的类别提示编码器来激活特定类别的特征以识别子细胞类别。在多种子细胞图像数据集上的广泛实验表明，ScSAM 在性能上优于现有方法。', 'title_zh': 'ScSAM：减轻亚细胞语义分割中形态和分布变异的偏差'}
{'arxiv_id': 'arXiv:2507.17141', 'title': 'Towards Human-level Intelligence via Human-like Whole-Body Manipulation', 'authors': 'Guang Gao, Jianan Wang, Jinbo Zuo, Junnan Jiang, Jingfan Zhang, Xianwen Zeng, Yuejiang Zhu, Lianyang Ma, Ke Chen, Minhua Sheng, Ruirui Zhang, Zhaohui An', 'link': 'https://arxiv.org/abs/2507.17141', 'abstract': "Building general-purpose intelligent robots has long been a fundamental goal of robotics. A promising approach is to mirror the evolutionary trajectory of humans: learning through continuous interaction with the environment, with early progress driven by the imitation of human behaviors. Achieving this goal presents three core challenges: (1) designing safe robotic hardware with human-level physical capabilities; (2) developing an intuitive and scalable whole-body teleoperation interface for data collection; and (3) creating algorithms capable of learning whole-body visuomotor policies from human demonstrations. To address these challenges in a unified framework, we propose Astribot Suite, a robot learning suite for whole-body manipulation aimed at general daily tasks across diverse environments. We demonstrate the effectiveness of our system on a wide range of activities that require whole-body coordination, extensive reachability, human-level dexterity, and agility. Our results show that Astribot's cohesive integration of embodiment, teleoperation interface, and learning pipeline marks a significant step towards real-world, general-purpose whole-body robotic manipulation, laying the groundwork for the next generation of intelligent robots.", 'abstract_zh': '构建通用智能机器人一直是机器人学的基本目标。一种有前景的方法是模仿人类的进化轨迹：通过持续与环境互动学习，早期进展通过模仿人类行为驱动。实现这一目标面临三大核心挑战：（1）设计具有人类级别物理能力的安全机器人硬件；（2）开发直观且可扩展的全身远程操作界面以收集数据；（3）创建能够从人类示范学习全身视听运动策略的算法。为了在一个统一框架中应对这些挑战，我们提出Astribot Suite，一个面向通用日常任务的全身操纵机器人学习套件，适用于多种环境。我们展示了我们的系统在多种需要全身协调、广泛可达性、人类级灵巧性和敏捷性的活动中具有有效性。我们的结果表明，Astribot 有效地整合了本体、远程操作界面和学习管道，标志着向实用化、通用化全身机器人操纵迈出的重要一步，为新一代智能机器人奠定了基础。', 'title_zh': '通过类人全身 Manipulation 追求人类水平的智能'}
{'arxiv_id': 'arXiv:2507.17135', 'title': 'SADA: Stability-guided Adaptive Diffusion Acceleration', 'authors': 'Ting Jiang, Yixiao Wang, Hancheng Ye, Zishan Shao, Jingwei Sun, Jingyang Zhang, Zekai Chen, Jianyi Zhang, Yiran Chen, Hai Li', 'link': 'https://arxiv.org/abs/2507.17135', 'abstract': 'Diffusion models have achieved remarkable success in generative tasks but suffer from high computational costs due to their iterative sampling process and quadratic attention costs. Existing training-free acceleration strategies that reduce per-step computation cost, while effectively reducing sampling time, demonstrate low faithfulness compared to the original baseline. We hypothesize that this fidelity gap arises because (a) different prompts correspond to varying denoising trajectory, and (b) such methods do not consider the underlying ODE formulation and its numerical solution. In this paper, we propose Stability-guided Adaptive Diffusion Acceleration (SADA), a novel paradigm that unifies step-wise and token-wise sparsity decisions via a single stability criterion to accelerate sampling of ODE-based generative models (Diffusion and Flow-matching). For (a), SADA adaptively allocates sparsity based on the sampling trajectory. For (b), SADA introduces principled approximation schemes that leverage the precise gradient information from the numerical ODE solver. Comprehensive evaluations on SD-2, SDXL, and Flux using both EDM and DPM++ solvers reveal consistent $\\ge 1.8\\times$ speedups with minimal fidelity degradation (LPIPS $\\leq 0.10$ and FID $\\leq 4.5$) compared to unmodified baselines, significantly outperforming prior methods. Moreover, SADA adapts seamlessly to other pipelines and modalities: It accelerates ControlNet without any modifications and speeds up MusicLDM by $1.8\\times$ with $\\sim 0.01$ spectrogram LPIPS.', 'abstract_zh': '基于稳定性的自适应扩散加速（SADA）', 'title_zh': 'SADA：基于稳定性引导的自适应扩散加速'}
{'arxiv_id': 'arXiv:2507.17134', 'title': 'Resilient Multi-Agent Negotiation for Medical Supply Chains:Integrating LLMs and Blockchain for Transparent Coordination', 'authors': 'Mariam ALMutairi, Hyungmin Kim', 'link': 'https://arxiv.org/abs/2507.17134', 'abstract': "Global health emergencies, such as the COVID-19 pandemic, have exposed critical weaknesses in traditional medical supply chains, including inefficiencies in resource allocation, lack of transparency, and poor adaptability to dynamic disruptions. This paper presents a novel hybrid framework that integrates blockchain technology with a decentralized, large language model (LLM) powered multi-agent negotiation system to enhance the resilience and accountability of medical supply chains during crises. In this system, autonomous agents-representing manufacturers, distributors, and healthcare institutions-engage in structured, context-aware negotiation and decision-making processes facilitated by LLMs, enabling rapid and ethical allocation of scarce medical resources. The off-chain agent layer supports adaptive reasoning and local decision-making, while the on-chain blockchain layer ensures immutable, transparent, and auditable enforcement of decisions via smart contracts. The framework also incorporates a formal cross-layer communication protocol to bridge decentralized negotiation with institutional enforcement. A simulation environment emulating pandemic scenarios evaluates the system's performance, demonstrating improvements in negotiation efficiency, fairness of allocation, supply chain responsiveness, and auditability. This research contributes an innovative approach that synergizes blockchain trust guarantees with the adaptive intelligence of LLM-driven agents, providing a robust and scalable solution for critical supply chain coordination under uncertainty.", 'abstract_zh': '全球健康紧急事件，如COVID-19疫情，揭示了传统医疗供应链中的关键弱点，包括资源配置效率低下、透明度不足和对动态中断的适应能力差。本文提出了一种结合区块链技术和去中心化大型语言模型（LLM）驱动的多智能体谈判系统的新型集成框架，以增强危机期间医疗供应链的韧性和问责性。在此系统中，代表制造商、分销商和医疗机构的自主智能体在LLM支持下的结构化、情境感知谈判和决策过程中进行交互，从而实现稀缺医疗资源的快速和道德分配。脱链智能体层支持适应性推理和本地决策，而区块链层则通过智能合约确保决策的不变、透明和可审计执行。该框架还整合了一套形式化跨层通信协议，以弥合分散谈判与机构执行之间的差距。模拟环境模仿疫情场景评估系统性能，展示了谈判效率、分配公平性、供应链响应性和可审计性的改善。该研究贡献了一种创新方法，将区块链信任保证与LLM驱动智能体的适应性智能相结合，提供了一种在不确定性条件下实现关键供应链协调的稳健且可扩展的解决方案。', 'title_zh': '具备弹性的多agent谈判机制：结合LLM和区块链实现透明协调'}
{'arxiv_id': 'arXiv:2507.17131', 'title': 'Enabling Self-Improving Agents to Learn at Test Time With Human-In-The-Loop Guidance', 'authors': 'Yufei He, Ruoyu Li, Alex Chen, Yue Liu, Yulin Chen, Yuan Sui, Cheng Chen, Yi Zhu, Luca Luo, Frank Yang, Bryan Hooi', 'link': 'https://arxiv.org/abs/2507.17131', 'abstract': 'Large language model (LLM) agents often struggle in environments where rules and required domain knowledge frequently change, such as regulatory compliance and user risk screening. Current approaches, like offline fine-tuning and standard prompting, are insufficient because they cannot effectively adapt to new knowledge during actual operation. To address this limitation, we propose the Adaptive Reflective Interactive Agent (ARIA), an LLM agent framework designed specifically to continuously learn updated domain knowledge at test time. ARIA assesses its own uncertainty through structured self-dialogue, proactively identifying knowledge gaps and requesting targeted explanations or corrections from human experts. It then systematically updates an internal, timestamped knowledge repository with provided human guidance, detecting and resolving conflicting or outdated knowledge through comparisons and clarification queries. We evaluate ARIA on the realistic customer due diligence name screening task on TikTok Pay, alongside publicly available dynamic knowledge tasks. Results demonstrate significant improvements in adaptability and accuracy compared to baselines using standard offline fine-tuning and existing self-improving agents. ARIA is deployed within TikTok Pay serving over 150 million monthly active users, confirming its practicality and effectiveness for operational use in rapidly evolving environments.', 'abstract_zh': '大型语言模型代理在规则和所需领域知识频繁变化的环境中往往表现不佳，例如合规性和用户风险筛查。当前的方法，如离线微调和标准提示，不足以在实际运行中有效适应新知识。为解决这一局限，我们提出了一种自适应反思交互代理（ARIA），这是一种专门设计用于在测试时持续学习更新领域知识的大型语言模型代理框架。ARIA 通过结构化的自我对话评估自身的不确定性，主动识别知识缺口，并向人类专家请求针对性的解释或更正。然后，它系统地根据提供的用户指导更新内部的时间戳知识库，通过比较和澄清查询检测并解决冲突或过时的知识。我们在抖音支付的真实客户尽职调查姓名筛查任务以及公开可用的动态知识任务上对ARIA 进行评估。结果表明，ARIA 在适应性和准确性方面明显优于使用标准离线微调和现有自我改进代理的基线。ARIA 已部署于抖音支付，服务于超过1.5亿月活跃用户，证实了其在快速变化环境中操作使用中的实用性和有效性。', 'title_zh': '具有人类在环指导的自提升代理在测试时学习-enable'}
{'arxiv_id': 'arXiv:2507.17120', 'title': 'BucketServe: Bucket-Based Dynamic Batching for Smart and Efficient LLM Inference Serving', 'authors': 'Wanyi Zheng, Minxian Xu, Shengye Song, Kejiang Ye', 'link': 'https://arxiv.org/abs/2507.17120', 'abstract': 'Large language models (LLMs) have become increasingly popular in various areas, traditional business gradually shifting from rule-based systems to LLM-based solutions. However, the inference of LLMs is resource-intensive or latency-sensitive, posing significant challenges for serving systems. Existing LLM serving systems often use static or continuous batching strategies, which can lead to inefficient GPU memory utilization and increased latency, especially under heterogeneous workloads. These methods may also struggle to adapt to dynamic workload fluctuations, resulting in suboptimal throughput and potential service level objective (SLO) violations. In this paper, we introduce BucketServe, a bucket-based dynamic batching framework designed to optimize LLM inference performance. By grouping requests into size-homogeneous buckets based on sequence length, BucketServe minimizes padding overhead and optimizes GPU memory usage through real-time batch size adjustments preventing out-of-memory (OOM) errors. It introduces adaptive bucket splitting/merging and priority-aware scheduling to mitigate resource fragmentation and ensure SLO compliance. Experiment shows that BucketServe significantly outperforms UELLM in throughput, achieving up to 3.58x improvement. It can also handle 1.93x more request load under the SLO attainment of 80% compared with DistServe and demonstrates 1.975x higher system load capacity compared to the UELLM.', 'abstract_zh': '基于桶的动态批处理框架BucketServe优化大型语言模型推理性能', 'title_zh': 'BucketServe：基于桶的动态批量处理用于智能高效的LLM推理服务'}
{'arxiv_id': 'arXiv:2507.17107', 'title': 'Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models', 'authors': 'Andrii Balashov', 'link': 'https://arxiv.org/abs/2507.17107', 'abstract': "Reinforcement learning (RL) is a key post-pretraining step for aligning large language models (LLMs) with complex tasks and human preferences. While it is often assumed that RL fine-tuning requires updating most of a model's parameters, we challenge this assumption with a surprising finding: RL fine-tuning consistently modifies only a small subnetwork (typically 5-30% of weights), leaving most parameters unchanged. We call this phenomenon RL-induced parameter update sparsity. It arises naturally, without any sparsity constraints or parameter-efficient tuning, and appears across multiple RL algorithms (e.g., PPO, DPO, SimPO, PRIME) and model families (e.g., OpenAI, Meta, and open-source LLMs). Moreover, the subnetworks updated by RL show substantial overlap across different seeds, datasets, and algorithms-far exceeding chance-suggesting a partially transferable structure in the pretrained model. We show that fine-tuning only this sparse subnetwork recovers full model performance and yields parameters nearly identical to the fully fine-tuned model. Our analysis suggests this sparsity emerges because RL operates near the model's original distribution, requiring only targeted changes. KL penalties, gradient clipping, and on-policy dynamics have limited effect on the sparsity pattern. These findings shed new light on how RL adapts models: not by shifting all weights, but by focusing training on a small, consistently updated subnetwork. This insight enables more efficient RL methods and reframes sparsity through the lens of the lottery ticket hypothesis.", 'abstract_zh': '强化学习（RL）是使大规模语言模型（LLMs）与复杂任务和人类偏好对齐的关键后微调步骤。尽管通常假设RL微调需要更新模型的大部分参数，但我们通过一个出人意料的发现挑战了这一假设：RL微调一致地仅修改一个小的子网络（通常为权重的5-30%），而大多数参数保持不变。我们称这一现象为RL诱导的参数更新稀疏性。这种现象自然产生，无需任何稀疏约束或参数高效微调。它在多种RL算法（例如PPO、DPO、SimPO、PRIME）和模型系列（例如OpenAI、Meta和开源LLMs）中普遍存在。此外，由RL更新的子网络在不同的随机种子、数据集和算法中显示出显著的重叠，远超随机水平，表明预训练模型中存在部分可转移的结构。我们证明，仅微调这一稀疏子网络即可恢复完整模型性能，并产生与完全微调模型几乎相同的参数。我们的分析表明，这种稀疏性的出现是因为RL操作在模型的原始分布附近，只需要进行针对性的调整。KL惩罚项、梯度裁剪和经验策略的动态对稀疏模式的影响有限。这些发现为如何RL适应模型提供了新的见解：并非通过移动所有权重，而是专注于一个小型且一致更新的子网络。这一洞察有助于开发更高效的RL方法，并通过彩票票假说的观点重定义稀疏性。', 'title_zh': '增强学习fine-tune大规模语言模型中的稀疏子网络'}
{'arxiv_id': 'arXiv:2507.17099', 'title': 'Weather-Aware AI Systems versus Route-Optimization AI: A Comprehensive Analysis of AI Applications in Transportation Productivity', 'authors': 'Tatsuru Kikuchi', 'link': 'https://arxiv.org/abs/2507.17099', 'abstract': "While recent research demonstrates that AI route-optimization systems improve taxi driver productivity by 14\\%, this study reveals that such findings capture only a fraction of AI's potential in transportation. We examine comprehensive weather-aware AI systems that integrate deep learning meteorological prediction with machine learning positioning optimization, comparing their performance against traditional operations and route-only AI approaches. Using simulation data from 10,000 taxi operations across varied weather conditions, we find that weather-aware AI systems increase driver revenue by 107.3\\%, compared to 14\\% improvements from route-optimization alone. Weather prediction contributes the largest individual productivity gain, with strong correlations between meteorological conditions and demand ($r=0.575$). Economic analysis reveals annual earnings increases of 13.8 million yen per driver, with rapid payback periods and superior return on investment. These findings suggest that current AI literature significantly underestimates AI's transformative potential by focusing narrowly on routing algorithms, while weather intelligence represents an untapped \\$8.9 billion market opportunity. Our results indicate that future AI implementations should adopt comprehensive approaches that address multiple operational challenges simultaneously rather than optimizing isolated functions.", 'abstract_zh': '虽然近期的研究表明AI路线优化系统能够提高出租车司机生产力14%，但本研究揭示此类发现只捕获了AI在交通运输中潜力的一小部分。我们考察了综合性天气aware AI系统，该系统将深度学习气象预测与机器学习定位优化相结合，并将其性能与传统操作及仅考虑路线优化的AI方法进行了对比。使用10,000次出租车辆在不同天气条件下的模拟数据，我们发现天气aware AI系统能将司机收入提高107.3%，而仅通过路线优化提高的收入增幅仅为14%。气象预测对生产力提升贡献最大，气象条件与需求之间存在显著相关性（r=0.575）。经济分析显示，司机每年收入可增加1380万日元，回报周期短且投资回报率优越。这些发现表明，当前AI文献因过于狭隘地关注路线算法，而严重低估了AI的变革潜力，气象智能代表了一个未充分利用的89亿美元市场机会。我们的结果表明，未来AI实施应采取综合方法，同时解决多种操作挑战，而非单独优化各个功能。', 'title_zh': '气象感知AI系统与路径优化AI：交通运输生产力中AI应用的全面分析'}
{'arxiv_id': 'arXiv:2507.17083', 'title': "SDGOCC: Semantic and Depth-Guided Bird's-Eye View Transformation for 3D Multimodal Occupancy Prediction", 'authors': 'Zaipeng Duan, Chenxu Dang, Xuzhong Hu, Pei An, Junfeng Ding, Jie Zhan, Yunbiao Xu, Jie Ma', 'link': 'https://arxiv.org/abs/2507.17083', 'abstract': 'Multimodal 3D occupancy prediction has garnered significant attention for its potential in autonomous driving. However, most existing approaches are single-modality: camera-based methods lack depth information, while LiDAR-based methods struggle with occlusions. Current lightweight methods primarily rely on the Lift-Splat-Shoot (LSS) pipeline, which suffers from inaccurate depth estimation and fails to fully exploit the geometric and semantic information of 3D LiDAR points. Therefore, we propose a novel multimodal occupancy prediction network called SDG-OCC, which incorporates a joint semantic and depth-guided view transformation coupled with a fusion-to-occupancy-driven active distillation. The enhanced view transformation constructs accurate depth distributions by integrating pixel semantics and co-point depth through diffusion and bilinear discretization. The fusion-to-occupancy-driven active distillation extracts rich semantic information from multimodal data and selectively transfers knowledge to image features based on LiDAR-identified regions. Finally, for optimal performance, we introduce SDG-Fusion, which uses fusion alone, and SDG-KL, which integrates both fusion and distillation for faster inference. Our method achieves state-of-the-art (SOTA) performance with real-time processing on the Occ3D-nuScenes dataset and shows comparable performance on the more challenging SurroundOcc-nuScenes dataset, demonstrating its effectiveness and robustness. The code will be released at this https URL.', 'abstract_zh': '多模态3D占用率预测在自主驾驶领域引起了广泛关注。然而，现有的大多数方法是单模态的：基于摄像头的方法缺乏深度信息，而基于LiDAR的方法在遮挡问题上表现不佳。当前的轻量化方法主要依赖于Lift-Splat-Shoot (LSS) 管道，但该方法存在深度估计不准确的问题，未能充分利用3D LiDAR点的几何和语义信息。因此，我们提出了一种名为SDG-OCC的新颖多模态占用率预测网络，该网络结合了联合语义和深度导向的视图变换以及融合驱动的主动蒸馏。增强的视图变换通过扩散和双线性离散化整合像素语义和共点深度，构建准确的深度分布。融合驱动的主动蒸馏从多模态数据中提取丰富的语义信息，并基于LiDAR标识的区域选择性地向图像特征转移知识。最后，为了实现最佳性能，我们引入了SDG-Fusion（仅使用融合）和SDG-KL（将融合和蒸馏结合起来，以实现更快的推理速度）。我们的方法在Occ3D-nuScenes数据集上实现了实时处理，并在更具挑战性的SurroundOcc-nuScenes数据集上表现出可比拟的性能，展示了其有效性和鲁棒性。代码将在以下几个网址之一发布：https://github.com/username/repo。', 'title_zh': 'SDGOCC: 语义和深度引导的鸟瞰视角变换用于3D多元模式占用预测'}
{'arxiv_id': 'arXiv:2507.17080', 'title': 'VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings', 'authors': 'Ramin Giahi, Kehui Yao, Sriram Kollipara, Kai Zhao, Vahid Mirjalili, Jianpeng Xu, Topojoy Biswas, Evren Korpeoglu, Kannan Achan', 'link': 'https://arxiv.org/abs/2507.17080', 'abstract': 'Multimodal learning plays a critical role in e-commerce recommendation platforms today, enabling accurate recommendations and product understanding. However, existing vision-language models, such as CLIP, face key challenges in e-commerce recommendation systems: 1) Weak object-level alignment, where global image embeddings fail to capture fine-grained product attributes, leading to suboptimal retrieval performance; 2) Ambiguous textual representations, where product descriptions often lack contextual clarity, affecting cross-modal matching; and 3) Domain mismatch, as generic vision-language models may not generalize well to e-commerce-specific data. To address these limitations, we propose a framework, VL-CLIP, that enhances CLIP embeddings by integrating Visual Grounding for fine-grained visual understanding and an LLM-based agent for generating enriched text embeddings. Visual Grounding refines image representations by localizing key products, while the LLM agent enhances textual features by disambiguating product descriptions. Our approach significantly improves retrieval accuracy, multimodal retrieval effectiveness, and recommendation quality across tens of millions of items on one of the largest e-commerce platforms in the U.S., increasing CTR by 18.6%, ATC by 15.5%, and GMV by 4.0%. Additional experimental results show that our framework outperforms vision-language models, including CLIP, FashionCLIP, and GCL, in both precision and semantic alignment, demonstrating the potential of combining object-aware visual grounding and LLM-enhanced text representation for robust multimodal recommendations.', 'abstract_zh': '多模态学习在电子商务推荐平台中发挥着关键作用，通过提高准确的推荐和产品理解。然而，现有的视觉-语言模型，如CLIP，在电子商务推荐系统中面临关键挑战：1）物体级对齐较弱，全局图像嵌入未能捕捉到细粒度的产品属性，导致检索性能不佳；2）文本表示模糊，产品描述经常缺乏上下文清晰度，影响跨模态匹配；3）领域不匹配，通用视觉-语言模型在电子商务特定数据上可能无法很好地泛化。为了解决这些限制，我们提出了一种框架VL-CLIP，通过集成视觉定位以增强细粒度的视觉理解，并结合基于LLM的代理生成丰富的文本嵌入来增强CLIP嵌入。视觉定位通过定位关键产品细化图像表示，而LLM代理通过澄清产品描述来增强文本特征。我们的方法显著提高了检索准确性、多模态检索效果和推荐质量，提升了一个美国最大的电子商务平台上的点击率（CTR）18.6%、添加到购物车率（ATC）15.5%和商品交易总额（GMV）4.0%。额外的实验结果表明，我们的框架在精度和语义对齐方面优于包括CLIP、FashionCLIP和GCL在内的视觉-语言模型，展示了对象感知的视觉定位和LLM增强的文本表示相结合在稳健的多模态推荐中的潜力。', 'title_zh': 'VL-CLIP: 通过视觉定位和LLM增强的CLIP嵌入提高多模态推荐'}
{'arxiv_id': 'arXiv:2507.17070', 'title': 'Advancing Robustness in Deep Reinforcement Learning with an Ensemble Defense Approach', 'authors': 'Adithya Mohan, Dominik Rößle, Daniel Cremers, Torsten Schön', 'link': 'https://arxiv.org/abs/2507.17070', 'abstract': 'Recent advancements in Deep Reinforcement Learning (DRL) have demonstrated its applicability across various domains, including robotics, healthcare, energy optimization, and autonomous driving. However, a critical question remains: How robust are DRL models when exposed to adversarial attacks? While existing defense mechanisms such as adversarial training and distillation enhance the resilience of DRL models, there remains a significant research gap regarding the integration of multiple defenses in autonomous driving scenarios specifically. This paper addresses this gap by proposing a novel ensemble-based defense architecture to mitigate adversarial attacks in autonomous driving. Our evaluation demonstrates that the proposed architecture significantly enhances the robustness of DRL models. Compared to the baseline under FGSM attacks, our ensemble method improves the mean reward from 5.87 to 18.38 (over 213% increase) and reduces the mean collision rate from 0.50 to 0.09 (an 82% decrease) in the highway scenario and merge scenario, outperforming all standalone defense strategies.', 'abstract_zh': 'Recent advancements in深度强化学习(DRL)已经证明其在机器人学、医疗保健、能源优化和自主驾驶等各个领域中的适用性。然而，一个关键问题仍然存在：当暴露于恶意攻击时，DRL模型的稳健性如何？虽然现有的防御机制如恶意训练和蒸馏可以增强DRL模型的抗攻击性，但目前在自主驾驶场景中集成多种防御机制尚存在显著的研究空白。本文通过提出一种新型集成防御架构，旨在缓解自主驾驶场景中的恶意攻击，从而填补这一空白。我们的评估结果表明，该提出的方法显著增强了DRL模型的稳健性。在高架场景和并线场景中，与基线方法相比，在FGSM攻击下，我们的集成方法将平均奖励从5.87提高到18.38（提高了213%），并将平均碰撞率从0.50降低到0.09（降低了82%），优于所有单一防御策略。', 'title_zh': '基于集成防御方法提升深度强化学习的 robustness'}
{'arxiv_id': 'arXiv:2507.17063', 'title': 'Compatibility of Max and Sum Objectives for Committee Selection and $k$-Facility Location', 'authors': 'Yue Han, Elliot Anshelevich', 'link': 'https://arxiv.org/abs/2507.17063', 'abstract': 'We study a version of the metric facility location problem (or, equivalently, variants of the committee selection problem) in which we must choose $k$ facilities in an arbitrary metric space to serve some set of clients $C$. We consider four different objectives, where each client $i\\in C$ attempts to minimize either the sum or the maximum of its distance to the chosen facilities, and where the overall objective either considers the sum or the maximum of the individual client costs. Rather than optimizing a single objective at a time, we study how compatible these objectives are with each other, and show the existence of solutions which are simultaneously close-to-optimum for any pair of the above objectives. Our results show that when choosing a set of facilities or a representative committee, it is often possible to form a solution which is good for several objectives at the same time, instead of sacrificing one desideratum to achieve another.', 'abstract_zh': '我们研究了一种在任意度量空间中选择$k$个设施来服务于客户集$C$的设施位置问题变体（或等价地，委员选择问题的变体）。我们考虑了四种不同的目标函数，其中每个客户$i \\in C$试图最小化其到所选设施的距离之和或最大值，而整体目标则是考虑所有客户的成本之和或最大值。我们不是单独优化每一个目标，而是研究这些目标之间的兼容性，并证明了存在同时对于上述任意一对目标都几乎最优的解。我们的结果表明，在选择设施集合或代表委员时，往往可以找到一个同时满足多个目标的解决方案，而不需要牺牲一个优选条件来实现另一个。', 'title_zh': 'Max和Sum目标在委员会选择和$k$设施选址中的兼容性'}
{'arxiv_id': 'arXiv:2507.17061', 'title': 'Parallelism Meets Adaptiveness: Scalable Documents Understanding in Multi-Agent LLM Systems', 'authors': 'Chengxuan Xia, Qianye Wu, Sixuan Tian, Yilun Hao', 'link': 'https://arxiv.org/abs/2507.17061', 'abstract': 'Large language model (LLM) agents have shown increasing promise for collaborative task completion. However, existing multi-agent frameworks often rely on static workflows, fixed roles, and limited inter-agent communication, reducing their effectiveness in open-ended, high-complexity domains. This paper proposes a coordination framework that enables adaptiveness through three core mechanisms: dynamic task routing, bidirectional feedback, and parallel agent evaluation. The framework allows agents to reallocate tasks based on confidence and workload, exchange structured critiques to iteratively improve outputs, and crucially compete on high-ambiguity subtasks with evaluator-driven selection of the most suitable result. We instantiate these principles in a modular architecture and demonstrate substantial improvements in factual coverage, coherence, and efficiency over static and partially adaptive baselines. Our findings highlight the benefits of incorporating both adaptiveness and structured competition in multi-agent LLM systems.', 'abstract_zh': '大型语言模型代理在协作任务完成中的适应性协调框架', 'title_zh': '并行性遇上自适应性：多代理大语言模型系统中的可扩展文档理解'}
{'arxiv_id': 'arXiv:2507.17056', 'title': 'Pragmatic Policy Development via Interpretable Behavior Cloning', 'authors': 'Anton Matsson, Yaochen Rao, Heather J. Litman, Fredrik D. Johansson', 'link': 'https://arxiv.org/abs/2507.17056', 'abstract': 'Offline reinforcement learning (RL) holds great promise for deriving optimal policies from observational data, but challenges related to interpretability and evaluation limit its practical use in safety-critical domains. Interpretability is hindered by the black-box nature of unconstrained RL policies, while evaluation -- typically performed off-policy -- is sensitive to large deviations from the data-collecting behavior policy, especially when using methods based on importance sampling. To address these challenges, we propose a simple yet practical alternative: deriving treatment policies from the most frequently chosen actions in each patient state, as estimated by an interpretable model of the behavior policy. By using a tree-based model, which is specifically designed to exploit patterns in the data, we obtain a natural grouping of states with respect to treatment. The tree structure ensures interpretability by design, while varying the number of actions considered controls the degree of overlap with the behavior policy, enabling reliable off-policy evaluation. This pragmatic approach to policy development standardizes frequent treatment patterns, capturing the collective clinical judgment embedded in the data. Using real-world examples in rheumatoid arthritis and sepsis care, we demonstrate that policies derived under this framework can outperform current practice, offering interpretable alternatives to those obtained via offline RL.', 'abstract_zh': '基于观测数据推导最优策略的离线强化学习具有巨大潜力，但在安全性关键领域中的实用应用受限于可解释性和评估的挑战。不可约束的RL策略的黑箱性质阻碍了可解释性，而通常离策略进行的评估对数据收集行为策略的偏差非常敏感，尤其是在使用基于重要性采样的方法时。为了解决这些挑战，我们提出了一种简单而实用的替代方案：通过可解释的行为策略模型估计的每个患者状态中被最频繁选择的动作来推导治疗策略。借助专门设计用于利用数据中模式的树状模型，我们自然地分组了与治疗相关的状态。树状结构通过设计保证了可解释性，而考虑的动作数量可以控制与行为策略的重叠程度，从而实现可靠的离策略评估。这种实用的方法标准化了常见的治疗模式，捕获了数据中嵌入的集体临床判断。通过类风湿性关节炎和脓毒症护理的实际案例，我们展示了在该框架下推导出的策略能够优于现有实践，提供了可解释的替代方案，这些替代方案优于传统的离线RL方法。', 'title_zh': '可解释行为克隆导向的 pragmatic 政策开发'}
{'arxiv_id': 'arXiv:2507.17047', 'title': 'Controllable Hybrid Captioner for Improved Long-form Video Understanding', 'authors': 'Kuleen Sasse, Efsun Sarioglu Kayi, Arun Reddy', 'link': 'https://arxiv.org/abs/2507.17047', 'abstract': 'Video data, especially long-form video, is extremely dense and high-dimensional. Text-based summaries of video content offer a way to represent query-relevant content in a much more compact manner than raw video. In addition, textual representations are easily ingested by state-of-the-art large language models (LLMs), which enable reasoning over video content to answer complex natural language queries. To solve this issue, we rely on the progressive construction of a text-based memory by a video captioner operating on shorter chunks of the video, where spatio-temporal modeling is computationally feasible. We explore ways to improve the quality of the activity log comprised solely of short video captions. Because the video captions tend to be focused on human actions, and questions may pertain to other information in the scene, we seek to enrich the memory with static scene descriptions using Vision Language Models (VLMs). Our video understanding system relies on the LaViLa video captioner in combination with a LLM to answer questions about videos. We first explored different ways of partitioning the video into meaningful segments such that the textual descriptions more accurately reflect the structure of the video content. Furthermore, we incorporated static scene descriptions into the captioning pipeline using LLaVA VLM, resulting in a more detailed and complete caption log and expanding the space of questions that are answerable from the textual memory. Finally, we have successfully fine-tuned the LaViLa video captioner to produce both action and scene captions, significantly improving the efficiency of the captioning pipeline compared to using separate captioning models for the two tasks. Our model, controllable hybrid captioner, can alternate between different types of captions according to special input tokens that signals scene changes detected in the video.', 'abstract_zh': '基于视频的高密度和高维度特性，特别是长视频，文本摘要为表示查询相关的内容提供了一种更加紧凑的方式，相比原始视频而言。此外，文本表示易于被最先进的大型语言模型（LLMs）处理，从而可以在视频内容上进行推理以回答复杂的自然语言查询。为了解决这个问题，我们依靠视频标注器逐步构建文本记忆，该标注器在视频的较短片段上操作，从而实现时序建模的可行性。我们探索了提高仅由短视频标注组成的活动日志质量的方法。由于视频标注通常集中在人类动作上，而问题可能涉及场景中的其他信息，我们寻求使用视觉语言模型（VLMs）丰富静态场景描述以增强记忆。我们的视频理解系统依赖LaViLa视频标注器与LLM结合，以回答关于视频的问题。我们首先探索了不同的方法，将视频分割成有意义的片段，从而使文本描述更准确地反映出视频内容的结构。进一步地，我们通过将LLaVA VLM纳入标注管道中，将静态场景描述融入其中，从而生成了更加详细和完整的标注日志，并扩大了可以从文本记忆中解答的问题范围。最后，我们成功微调了LaViLa视频标注器，使其既能生成动作也能生成场景标注，显著提高了标注管道的效率，比使用两个独立模型执行这两种任务的效率更高。我们的模型，可控混合标注器，可根据视频中检测到的场景变化特殊输入标记交替生成不同类型的标注。', 'title_zh': '可控混合 captioner 以提升长视频理解'}
{'arxiv_id': 'arXiv:2507.17043', 'title': 'Computational Performance Bounds Prediction in Quantum Computing with Unstable Noise', 'authors': 'Jinyang Li, Samudra Dasgupta, Yuhong Song, Lei Yang, Travis Humble, Weiwen Jiang', 'link': 'https://arxiv.org/abs/2507.17043', 'abstract': "Quantum computing has significantly advanced in recent years, boasting devices with hundreds of quantum bits (qubits), hinting at its potential quantum advantage over classical computing. Yet, noise in quantum devices poses significant barriers to realizing this supremacy. Understanding noise's impact is crucial for reproducibility and application reuse; moreover, the next-generation quantum-centric supercomputing essentially requires efficient and accurate noise characterization to support system management (e.g., job scheduling), where ensuring correct functional performance (i.e., fidelity) of jobs on available quantum devices can even be higher-priority than traditional objectives. However, noise fluctuates over time, even on the same quantum device, which makes predicting the computational bounds for on-the-fly noise is vital. Noisy quantum simulation can offer insights but faces efficiency and scalability issues. In this work, we propose a data-driven workflow, namely QuBound, to predict computational performance bounds. It decomposes historical performance traces to isolate noise sources and devises a novel encoder to embed circuit and noise information processed by a Long Short-Term Memory (LSTM) network. For evaluation, we compare QuBound with a state-of-the-art learning-based predictor, which only generates a single performance value instead of a bound. Experimental results show that the result of the existing approach falls outside of performance bounds, while all predictions from our QuBound with the assistance of performance decomposition better fit the bounds. Moreover, QuBound can efficiently produce practical bounds for various circuits with over 106 speedup over simulation; in addition, the range from QuBound is over 10x narrower than the state-of-the-art analytical approach.", 'abstract_zh': '量子计算在 recent years 显著进步，展现了其在量子位（qubits）数量达数百的情况下超越经典计算的潜力。然而，量子设备中的噪声构成了实现这一优势的重要障碍。理解噪声的影响对于实现可重复性和应用重用至关重要；此外，下一代以量子为中心的超级计算需要高效且准确地表征噪声以支持系统管理（例如，任务调度），确保在可用量子设备上正确功能性能（即保真度）甚至成为比传统目标更高的优先事项。然而，噪声会随时间波动，即使在同一量子设备上也是如此，因此实时噪声计算能力界限的预测至关重要。嘈杂的量子模拟可以提供见解，但面临效率和可扩展性问题。在本文中，我们提出了一种基于数据的工作流程，即 QuBound，用于预测计算性能界限。它将历史性能轨迹分解以隔离噪声源，并设计了一种新的编码器将量子电路和噪声信息嵌入长短期记忆（LSTM）网络中。在评估中，我们将 QuBound 与一种最先进的基于学习的预测器进行了比较，后者只能生成单一的性能值而不是界限。实验结果表明，现有方法的结果超出了性能界限，而我们的 QuBound 在性能分解的帮助下产生的所有预测均更符合界限。此外，QuBound 可以高效地为各种电路生成实际界限，与模拟相比其速度提升超过 10^6 倍；此外，QuBound 的界限范围比最先进的分析方法窄 10 倍以上。', 'title_zh': '基于不稳定噪声的量子计算中计算性能边界预测'}
{'arxiv_id': 'arXiv:2507.17029', 'title': 'StreamME: Simplify 3D Gaussian Avatar within Live Stream', 'authors': 'Luchuan Song, Yang Zhou, Zhan Xu, Yi Zhou, Deepali Aneja, Chenliang Xu', 'link': 'https://arxiv.org/abs/2507.17029', 'abstract': 'We propose StreamME, a method focuses on fast 3D avatar reconstruction. The StreamME synchronously records and reconstructs a head avatar from live video streams without any pre-cached data, enabling seamless integration of the reconstructed appearance into downstream applications. This exceptionally fast training strategy, which we refer to as on-the-fly training, is central to our approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating the reliance on MLPs in deformable 3DGS and relying solely on geometry, which significantly improves the adaptation speed to facial expression. To further ensure high efficiency in on-the-fly training, we introduced a simplification strategy based on primary points, which distributes the point clouds more sparsely across the facial surface, optimizing points number while maintaining rendering quality. Leveraging the on-the-fly training capabilities, our method protects the facial privacy and reduces communication bandwidth in VR system or online conference. Additionally, it can be directly applied to downstream application such as animation, toonify, and relighting. Please refer to our project page for more details: this https URL.', 'abstract_zh': '我们提出StreamME，一种快速3D角色重建的方法。StreamME无需预缓存数据，即可同步记录并从实时视频流中重建头部角色，从而无缝地将重建的外观集成到下游应用中。我们称之为“即用即训”的这项极其快速的训练策略在我们的方法中起着核心作用。我们的方法基于3D高斯点积（3DGS），消除了可变形3DGS对MLPs的依赖，仅依赖于几何结构，这显著提高了对表情变化的适应速度。为了进一步确保“即用即训”的高效性，我们引入了一种基于主点的简化策略，通过更稀疏地分布在面部表面来优化点的数量，同时保持渲染效果。利用“即用即训”的能力，我们的方法可以保护面部隐私并减少VR系统或在线会议中的通信带宽。此外，它还可以直接应用于下游应用，如动画、卡通化和重新光照。更多信息请参阅我们的项目页面：[this URL]。', 'title_zh': 'StreamME: 简化实时流中的3D高斯Avatar'}
{'arxiv_id': 'arXiv:2507.17025', 'title': 'Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings', 'authors': 'Soumen Sinha, Shahryar Rahnamayan, Azam Asilian Bidgoli', 'link': 'https://arxiv.org/abs/2507.17025', 'abstract': 'Efficient text embedding is crucial for large-scale natural language processing (NLP) applications, where storage and computational efficiency are key concerns. In this paper, we explore how using binary representations (barcodes) instead of real-valued features can be used for NLP embeddings derived from machine learning models such as BERT. Thresholding is a common method for converting continuous embeddings into binary representations, often using a fixed threshold across all features. We propose a Coordinate Search-based optimization framework that instead identifies the optimal threshold for each feature, demonstrating that feature-specific thresholds lead to improved performance in binary encoding. This ensures that the binary representations are both accurate and efficient, enhancing performance across various features. Our optimal barcode representations have shown promising results in various NLP applications, demonstrating their potential to transform text representation. We conducted extensive experiments and statistical tests on different NLP tasks and datasets to evaluate our approach and compare it to other thresholding methods. Binary embeddings generated using using optimal thresholds found by our method outperform traditional binarization methods in accuracy. This technique for generating binary representations is versatile and can be applied to any features, not just limited to NLP embeddings, making it useful for a wide range of domains in machine learning applications.', 'abstract_zh': '高效的文本嵌入对于大规模自然语言处理（NLP）应用至关重要，其中存储和计算效率是关键问题。本文探索了使用二进制表示（条形码）而不是实值特征来为如BERT等机器学习模型生成的NLP嵌入提供方法。阈值化是一种常见的将连续嵌入转换为二进制表示的方法，通常使用固定阈值跨越所有特征。我们提出了一种基于坐标搜索的优化框架，该框架识别每个特征的最佳阈值，表明特征特定的阈值能够改善二进制编码的性能。这确保了二进制表示既准确又高效，增强了各种特征的性能。我们提出的最佳条形码表示在各种NLP应用中显示出有 promise 的结果，展示了它们在文本表示变换方面的潜力。我们在不同的NLP任务和数据集上进行了广泛实验和统计测试，以评估我们的方法并将其与其他阈值化方法进行比较。使用我们方法找到的最优阈值生成的二进制嵌入在准确性上优于传统二进制化方法。此生成二进制表示的技术具有通用性，可以应用于任何特征，不仅限于NLP嵌入，使其在各种机器学习应用领域中具有广泛的应用价值。', 'title_zh': '演化特征阈值化方法用于NLP嵌入的二元表示'}
{'arxiv_id': 'arXiv:2507.17016', 'title': 'Causal Graph Fuzzy LLMs: A First Introduction and Applications in Time Series Forecasting', 'authors': 'Omid Orang, Patricia O. Lucas, Gabriel I. F. Paiva, Petronio C. L. Silva, Felipe Augusto Rocha da Silva, Adriano Alonso Veloso, Frederico Gadelha Guimaraes', 'link': 'https://arxiv.org/abs/2507.17016', 'abstract': 'In recent years, the application of Large Language Models (LLMs) to time series forecasting (TSF) has garnered significant attention among researchers. This study presents a new frame of LLMs named CGF-LLM using GPT-2 combined with fuzzy time series (FTS) and causal graph to predict multivariate time series, marking the first such architecture in the literature. The key objective is to convert numerical time series into interpretable forms through the parallel application of fuzzification and causal analysis, enabling both semantic understanding and structural insight as input for the pretrained GPT-2 model. The resulting textual representation offers a more interpretable view of the complex dynamics underlying the original time series. The reported results confirm the effectiveness of our proposed LLM-based time series forecasting model, as demonstrated across four different multivariate time series datasets. This initiative paves promising future directions in the domain of TSF using LLMs based on FTS.', 'abstract_zh': '近年来，大型语言模型（LLMs）在时间序列预测（TSF）中的应用引起了研究者的广泛关注。本文提出了一种新的LLM框架CGF-LLM，该框架结合了GPT-2、模糊时间序列（FTS）和因果图，用于预测多变量时间序列，这是文献中首个此类架构。关键目标是通过并行应用模糊化和因果分析，将数值时间序列转换为可解释的形式，从而为预训练的GPT-2模型提供兼具语义理解和结构洞察的输入。生成的文本表示提供了对原始时间序列复杂动力学的更可解释的视角。报告的结果证实了我们提出的基于LLM的时间序列预测模型的有效性，在四个不同的多变量时间序列数据集上进行了验证。这一举措为基于FTS的LLMs在TSF领域的未来发展铺平了道路。', 'title_zh': '因果图模糊大语言模型：初步介绍及其在时间序列预测中的应用'}
{'arxiv_id': 'arXiv:2507.17015', 'title': 'Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?', 'authors': 'Arduin Findeis, Floris Weers, Guoli Yin, Ke Ye, Ruoming Pang, Tom Gunter', 'link': 'https://arxiv.org/abs/2507.17015', 'abstract': 'Pairwise preferences over model responses are widely collected to evaluate and provide feedback to large language models (LLMs). Given two alternative model responses to the same input, a human or AI annotator selects the "better" response. This approach can provide feedback for domains where other hard-coded metrics are difficult to obtain (e.g., chat response quality), thereby helping model evaluation or training. However, for some domains high-quality pairwise comparisons can be tricky to obtain - from AI and humans. For example, for responses with many factual statements, annotators may disproportionately weigh writing quality rather than underlying facts. In this work, we explore augmenting standard AI annotator systems with additional tools to improve performance on three challenging response domains: long-form factual, math and code tasks. We propose a tool-using agentic system to provide higher quality feedback on these domains. Our system uses web-search and code execution to ground itself based on external validation, independent of the LLM\'s internal knowledge and biases. We provide extensive experimental results evaluating our method across the three targeted response domains as well as general annotation tasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as three new datasets for domains with saturated pre-existing datasets. Our results indicate that external tools can indeed improve performance in many, but not all, cases. More generally, our experiments highlight the sensitivity of performance to simple parameters (e.g., prompt) and the need for improved (non-saturated) annotator benchmarks. We share our code at this https URL.', 'abstract_zh': '基于模型响应的成对偏好收集广泛用于评估和反馈大型语言模型（LLMs）。', 'title_zh': '外部验证工具能否提高LLM作为法官时的标注质量？'}
{'arxiv_id': 'arXiv:2507.17013', 'title': 'laplax -- Laplace Approximations with JAX', 'authors': 'Tobias Weber, Bálint Mucsányi, Lenard Rommel, Thomas Christie, Lars Kasüschke, Marvin Pförtner, Philipp Hennig', 'link': 'https://arxiv.org/abs/2507.17013', 'abstract': "The Laplace approximation provides a scalable and efficient means of quantifying weight-space uncertainty in deep neural networks, enabling the application of Bayesian tools such as predictive uncertainty and model selection via Occam's razor. In this work, we introduce laplax, a new open-source Python package for performing Laplace approximations with jax. Designed with a modular and purely functional architecture and minimal external dependencies, laplax offers a flexible and researcher-friendly framework for rapid prototyping and experimentation. Its goal is to facilitate research on Bayesian neural networks, uncertainty quantification for deep learning, and the development of improved Laplace approximation techniques.", 'abstract_zh': '拉普拉斯近似提供了一种在深度神经网络中量化权重空间不确定性并应用贝叶斯工具（如预测不确定性与奥卡姆剃刀准则进行模型选择）的可扩展和高效方法。在本文中，我们介绍了一种新的开源Python包laplax，用于通过JAX进行拉普拉斯近似。laplax设计为模块化且纯函数架构，并具有最少的外部依赖项，提供了一种灵活且便于研究人员使用的快速原型设计和实验框架。其目标是促进贝叶斯神经网络、深度学习中的不确定性量化以及改进拉普拉斯近似技术的研究。', 'title_zh': 'Laplax -- Laplace Approximations with JAX'}
{'arxiv_id': 'arXiv:2507.17010', 'title': 'Towards Trustworthy AI: Secure Deepfake Detection using CNNs and Zero-Knowledge Proofs', 'authors': 'H M Mohaimanul Islam, Huynh Q. N. Vo, Aditya Rane', 'link': 'https://arxiv.org/abs/2507.17010', 'abstract': 'In the era of synthetic media, deepfake manipulations pose a significant threat to information integrity. To address this challenge, we propose TrustDefender, a two-stage framework comprising (i) a lightweight convolutional neural network (CNN) that detects deepfake imagery in real-time extended reality (XR) streams, and (ii) an integrated succinct zero-knowledge proof (ZKP) protocol that validates detection results without disclosing raw user data. Our design addresses both the computational constraints of XR platforms while adhering to the stringent privacy requirements in sensitive settings. Experimental evaluations on multiple benchmark deepfake datasets demonstrate that TrustDefender achieves 95.3% detection accuracy, coupled with efficient proof generation underpinned by rigorous cryptography, ensuring seamless integration with high-performance artificial intelligence (AI) systems. By fusing advanced computer vision models with provable security mechanisms, our work establishes a foundation for reliable AI in immersive and privacy-sensitive applications.', 'abstract_zh': '合成媒体时代，深度伪造操作对信息完整性构成重大威胁。为应对这一挑战，我们提出TrustDefender，这一两阶段框架包括（i）一个轻量级卷积神经网络（CNN），用于实时扩展现实（XR）流中深度伪造图像的检测，以及（ii）一个集成的简洁零知识证明（ZKP）协议，用于验证检测结果而不泄露原始用户数据。设计上，我们在满足XR平台的计算约束的同时，还符合敏感环境中严格的数据隐私要求。在多个基准深度伪造数据集上的实验评估表明，TrustDefender实现了95.3%的检测准确率，并通过严格的加密技术高效生成证明，确保能够无缝集成到高性能人工智能（AI）系统中。通过将先进的计算机视觉模型与可证明的安全机制相结合，我们的工作为沉浸式和隐私敏感应用中的可靠AI奠定了基础。', 'title_zh': '可信赖AI：基于CNNs和零知识证明的深伪检测'}
{'arxiv_id': 'arXiv:2507.17008', 'title': 'Bringing Balance to Hand Shape Classification: Mitigating Data Imbalance Through Generative Models', 'authors': 'Gaston Gustavo Rios, Pedro Dal Bianco, Franco Ronchetti, Facundo Quiroga, Oscar Stanchi, Santiago Ponte Ahón, Waldo Hasperué', 'link': 'https://arxiv.org/abs/2507.17008', 'abstract': 'Most sign language handshape datasets are severely limited and unbalanced, posing significant challenges to effective model training. In this paper, we explore the effectiveness of augmenting the training data of a handshape classifier by generating synthetic data. We use an EfficientNet classifier trained on the RWTH German sign language handshape dataset, which is small and heavily unbalanced, applying different strategies to combine generated and real images. We compare two Generative Adversarial Networks (GAN) architectures for data generation: ReACGAN, which uses label information to condition the data generation process through an auxiliary classifier, and SPADE, which utilizes spatially-adaptive normalization to condition the generation on pose information. ReACGAN allows for the generation of realistic images that align with specific handshape labels, while SPADE focuses on generating images with accurate spatial handshape configurations. Our proposed techniques improve the current state-of-the-art accuracy on the RWTH dataset by 5%, addressing the limitations of small and unbalanced datasets. Additionally, our method demonstrates the capability to generalize across different sign language datasets by leveraging pose-based generation trained on the extensive HaGRID dataset. We achieve comparable performance to single-source trained classifiers without the need for retraining the generator.', 'abstract_zh': '基于生成合成数据提升手语手势分类器训练数据有效性的探索', 'title_zh': '通过生成模型缓解手型分类数据不平衡问题'}
{'arxiv_id': 'arXiv:2507.16999', 'title': 'Bayesian preference elicitation for decision support in multiobjective optimization', 'authors': 'Felix Huber, Sebastian Rojas Gonzalez, Raul Astudillo', 'link': 'https://arxiv.org/abs/2507.16999', 'abstract': "We present a novel approach to help decision-makers efficiently identify preferred solutions from the Pareto set of a multi-objective optimization problem. Our method uses a Bayesian model to estimate the decision-maker's utility function based on pairwise comparisons. Aided by this model, a principled elicitation strategy selects queries interactively to balance exploration and exploitation, guiding the discovery of high-utility solutions. The approach is flexible: it can be used interactively or a posteriori after estimating the Pareto front through standard multi-objective optimization techniques. Additionally, at the end of the elicitation phase, it generates a reduced menu of high-quality solutions, simplifying the decision-making process. Through experiments on test problems with up to nine objectives, our method demonstrates superior performance in finding high-utility solutions with a small number of queries. We also provide an open-source implementation of our method to support its adoption by the broader community.", 'abstract_zh': '我们提出了一种新型方法，以帮助决策者高效地从多目标优化问题的帕累托集合中识别出首选解。该方法利用贝叶斯模型根据配对比较来估计决策者的效用函数。借助此模型，一种基于原则性的征询策略可以交互式地选择查询以平衡探索与利用，从而引导高效用解的发现。该方法具有灵活性：它既可以用作交互式工具，也可以在通过标准多目标优化技术估计帕累托前沿之后用作事后工具。此外，在征询阶段结束时，该方法会生成一个高效率解的缩减菜单，简化决策过程。通过最多具有九个目标的测试问题的实验，该方法在少量查询下找到了高效用解，表现出优越的性能。我们还提供了该方法的开源实现，以支持其在更广泛社区中的使用。', 'title_zh': '基于贝叶斯偏好 elicitation 的多目标优化决策支持'}
{'arxiv_id': 'arXiv:2507.16991', 'title': 'PyG 2.0: Scalable Learning on Real World Graphs', 'authors': 'Matthias Fey, Jinu Sunil, Akihiro Nitta, Rishi Puri, Manan Shah, Blaž Stojanovič, Ramona Bendias, Alexandria Barghi, Vid Kocijan, Zecheng Zhang, Xinwei He, Jan Eric Lenssen, Jure Leskovec', 'link': 'https://arxiv.org/abs/2507.16991', 'abstract': "PyG (PyTorch Geometric) has evolved significantly since its initial release, establishing itself as a leading framework for Graph Neural Networks. In this paper, we present Pyg 2.0 (and its subsequent minor versions), a comprehensive update that introduces substantial improvements in scalability and real-world application capabilities. We detail the framework's enhanced architecture, including support for heterogeneous and temporal graphs, scalable feature/graph stores, and various optimizations, enabling researchers and practitioners to tackle large-scale graph learning problems efficiently. Over the recent years, PyG has been supporting graph learning in a large variety of application areas, which we will summarize, while providing a deep dive into the important areas of relational deep learning and large language modeling.", 'abstract_zh': 'PyG（PyTorch Geometric）自初始发布以来发展显著，已成为图神经网络领域的领先框架。在本文中，我们介绍Pyg 2.0（及其后续次要版本），这是一个全面更新的版本，带来了可扩展性和实际应用能力的显著改善。我们详细介绍了框架的增强架构，包括对异构和时序图的支持、可扩展的特征/图存储以及各种优化，使研究人员和实践者能够高效地处理大规模图学习问题。近年来，PyG已在多种应用领域支持图学习，本文将对其进行总结，并深入探讨关系深度学习和大型语言模型等领域。', 'title_zh': 'PyG 2.0: 可扩展的现实世界图学习'}
{'arxiv_id': 'arXiv:2507.16978', 'title': 'Fast and Scalable Gene Embedding Search: A Comparative Study of FAISS and ScaNN', 'authors': 'Mohammad Saleh Refahi, Gavin Hearne, Harrison Muller, Kieran Lynch, Bahrad A. Sokhansanj, James R. Brown, Gail Rosen', 'link': 'https://arxiv.org/abs/2507.16978', 'abstract': 'The exponential growth of DNA sequencing data has outpaced traditional heuristic-based methods, which struggle to scale effectively. Efficient computational approaches are urgently needed to support large-scale similarity search, a foundational task in bioinformatics for detecting homology, functional similarity, and novelty among genomic and proteomic sequences. Although tools like BLAST have been widely used and remain effective in many scenarios, they suffer from limitations such as high computational cost and poor performance on divergent sequences.\nIn this work, we explore embedding-based similarity search methods that learn latent representations capturing deeper structural and functional patterns beyond raw sequence alignment. We systematically evaluate two state-of-the-art vector search libraries, FAISS and ScaNN, on biologically meaningful gene embeddings. Unlike prior studies, our analysis focuses on bioinformatics-specific embeddings and benchmarks their utility for detecting novel sequences, including those from uncharacterized taxa or genes lacking known homologs. Our results highlight both computational advantages (in memory and runtime efficiency) and improved retrieval quality, offering a promising alternative to traditional alignment-heavy tools.', 'abstract_zh': 'DNA测序数据的指数增长已超越了传统的启发式方法，这些方法难以有效扩展。急需高效的计算方法以支持大规模相似性搜索，这是生物信息学中的一个基础任务，用于检测基因组和蛋白质序列之间的同源性、功能相似性和新颖性。虽然BLAST等工具被广泛使用并在许多情况下仍然有效，但它们在计算成本和异源序列性能方面存在局限性。\n\n在本工作中，我们探索基于嵌入的相似性搜索方法，这些方法学习到能够捕捉超越原始序列对齐的深层次结构和功能模式的潜在表示。我们系统地评估了两个最先进的向量搜索库FAISS和ScaNN在生物意义上具有意义的基因嵌入上的性能。与先前研究不同，我们的分析集中在生物信息学特定的嵌入上，并评估了它们在检测新型序列（包括未表征分类群或缺乏已知同源基因的序列）方面的实用性。我们的结果突显了计算上的优势（内存和运行时效率）以及检索质量的提高，为传统的对齐密集型工具提供了有希望的替代方案。', 'title_zh': '快速且可扩展的基因嵌入搜索：FAISS与ScaNN的比较研究'}
{'arxiv_id': 'arXiv:2507.16974', 'title': 'Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain', 'authors': 'Rishemjit Kaur, Arshdeep Singh Bhankhar, Surangika Ranathunga, Jashanpreet Singh Salh, Sudhir Rajput, Vidhi, Kashish Mahendra, Bhavika Berwal, Ritesh Kumar', 'link': 'https://arxiv.org/abs/2507.16974', 'abstract': 'Enabling farmers to access accurate agriculture-related information in their native languages in a timely manner is crucial for the success of the agriculture field. Although large language models (LLMs) can be used to implement Question Answering (QA) systems, simply using publicly available general-purpose LLMs in agriculture typically offer generic advisories, lacking precision in local and multilingual contexts due to insufficient domain-specific training and scarcity of high-quality, region-specific datasets. Our study addresses these limitations by generating multilingual synthetic agricultural datasets (English, Hindi, Punjabi) from agriculture-specific documents and fine-tuning language-specific LLMs. Our evaluation on curated multilingual datasets demonstrates significant improvements in factual accuracy, relevance, and agricultural consensus for the fine-tuned models compared to their baseline counterparts. These results highlight the efficacy of synthetic data-driven, language-specific fine-tuning as an effective strategy to improve the performance of LLMs in agriculture, especially in multilingual and low-resource settings. By enabling more accurate and localized agricultural advisory services, this study provides a meaningful step toward bridging the knowledge gap in AI-driven agricultural solutions for diverse linguistic communities.', 'abstract_zh': '使农民能够及时获取其本土语言的准确农业相关信息对于农业领域的成功至关重要。尽管大型语言模型可以用于实施问答系统，但通常使用通用大型语言模型在农业领域仅能提供泛化的建议，由于缺乏特定领域训练和高质量地区特定数据集的稀缺性，这在本地和多语言背景下缺乏精准性。我们的研究通过从农业特定文档生成多语言合成农业数据集（英语、印地语、旁遮普语）并微调语言特定的大型语言模型来解决这些限制。在编纂的多语言数据集上的评估表明，微调模型在事实准确性、相关性和农业共识方面显著优于基线模型。这些结果强调了合成数据驱动的语言特定微调作为提高大型语言模型在农业中性能的有效策略的有效性，特别是在多语言和低资源环境中。通过提供更准确和本地化的农业咨询服务，本研究为弥合AI驱动农业解决方案中的知识差距提供了有意义的一步，特别是对于多元语言社区。', 'title_zh': '利用合成数据提升多语言大语言模型在农业领域的问答能力'}
{'arxiv_id': 'arXiv:2507.16971', 'title': 'Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning', 'authors': 'Aleksandr Perevalov, Andreas Both', 'link': 'https://arxiv.org/abs/2507.16971', 'abstract': 'Accessing knowledge via multilingual natural-language interfaces is one of the emerging challenges in the field of information retrieval and related ones. Structured knowledge stored in knowledge graphs can be queried via a specific query language (e.g., SPARQL). Therefore, one needs to transform natural-language input into a query to fulfill an information need. Prior approaches mostly focused on combining components (e.g., rule-based or neural-based) that solve downstream tasks and come up with an answer at the end. We introduce mKGQAgent, a human-inspired framework that breaks down the task of converting natural language questions into SPARQL queries into modular, interpretable subtasks. By leveraging a coordinated LLM agent workflow for planning, entity linking, and query refinement - guided by an experience pool for in-context learning - mKGQAgent efficiently handles multilingual KGQA. Evaluated on the DBpedia- and Corporate-based KGQA benchmarks within the Text2SPARQL challenge 2025, our approach took first place among the other participants. This work opens new avenues for developing human-like reasoning systems in multilingual semantic parsing.', 'abstract_zh': '通过多语言自然语言接口访问知识是信息检索及相关领域中的新兴挑战。结构化的知识存储在知识图谱中，可以通过特定的查询语言（例如SPARQL）进行查询。因此，需要将自然语言输入转换为查询以满足信息需求。先前的方法主要集中在结合解决下游任务的组件（例如基于规则或基于神经网络），并在最后得出答案。我们引入了mKGQAgent，这是一种受人类启发的框架，将将自然语言问题转换为SPARQL查询的任务分解为模块化、可解释的子任务。通过利用协调的LLM代理工作流程进行规划、实体链接和查询优化，并由上下文学习的经验池引导，mKGQAgent高效地处理了多语言KGQA。在2025年Text2SPARQL挑战中的DBpedia-和企业基于的知识图谱问答基准测试中，我们的方法在其他参与者中取得了第一名。这项工作为多语言语义解析中开发类似人类的推理系统开辟了新的途径。', 'title_zh': '文本到SPARQL超越英语：通过人类启发式推理在知识图谱上的多语言问答'}
{'arxiv_id': 'arXiv:2507.16952', 'title': 'Evaluating Ensemble and Deep Learning Models for Static Malware Detection with Dimensionality Reduction Using the EMBER Dataset', 'authors': 'Md Min-Ha-Zul Abedin, Tazqia Mehrub', 'link': 'https://arxiv.org/abs/2507.16952', 'abstract': 'This study investigates the effectiveness of several machine learning algorithms for static malware detection using the EMBER dataset, which contains feature representations of Portable Executable (PE) files. We evaluate eight classification models: LightGBM, XGBoost, CatBoost, Random Forest, Extra Trees, HistGradientBoosting, k-Nearest Neighbors (KNN), and TabNet, under three preprocessing settings: original feature space, Principal Component Analysis (PCA), and Linear Discriminant Analysis (LDA). The models are assessed on accuracy, precision, recall, F1 score, and AUC to examine both predictive performance and robustness. Ensemble methods, especially LightGBM and XGBoost, show the best overall performance across all configurations, with minimal sensitivity to PCA and consistent generalization. LDA improves KNN performance but significantly reduces accuracy for boosting models. TabNet, while promising in theory, underperformed under feature reduction, likely due to architectural sensitivity to input structure. The analysis is supported by detailed exploratory data analysis (EDA), including mutual information ranking, PCA or t-SNE visualizations, and outlier detection using Isolation Forest and Local Outlier Factor (LOF), which confirm the discriminatory capacity of key features in the EMBER dataset. The results suggest that boosting models remain the most reliable choice for high-dimensional static malware detection, and that dimensionality reduction should be applied selectively based on model type. This work provides a benchmark for comparing classification models and preprocessing strategies in malware detection tasks and contributes insights that can guide future system development and real-world deployment.', 'abstract_zh': '本研究使用EMBER数据集（包含 Portable Executable (PE) 文件的特征表示）调查了几种机器学习算法在静态恶意软件检测中的有效性，评估了八种分类模型：LightGBM、XGBoost、CatBoost、随机森林、极端随机树、直方图梯度提升、k-最近邻（KNN）和TabNet，在三种预处理设置下：原始特征空间、主成分分析（PCA）和线性判别分析（LDA）。模型在准确性、精确度、召回率、F1分数和AUC等方面进行评估，以考察其预测性能和鲁棒性。集成方法，尤其是LightGBM和XGBoost，在所有配置中表现出最佳的整体表现，对PCA具有较低的敏感性并表现出一致的泛化能力。LDA提升了KNN的表现，但显著降低了提升模型的准确性。TabNet理论上表现有潜力，但在特征维度减少的情况下表现不佳，可能是由于其对输入结构的架构敏感性。研究通过详细的探索性数据分析（EDA），包括互信息排名、PCA或t-SNE可视化以及使用孤立森林和局部异常因子（LOF）进行的离群值检测，证实了EMBER数据集中关键特征的区分能力。结果表明，提升模型仍然是高维静态恶意软件检测中最可靠的选择，特征维度的减少应根据模型类型择优应用。本研究为恶意软件检测任务中分类模型和预处理策略的比较提供了基准，并为未来的系统开发和实际部署提供了启示。', 'title_zh': '使用EMBER数据集的降维评估集成学习和深度学习模型在静态恶意软件检测中的性能'}
{'arxiv_id': 'arXiv:2507.16933', 'title': 'SiLQ: Simple Large Language Model Quantization-Aware Training', 'authors': 'Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, Dharmendra S. Modha', 'link': 'https://arxiv.org/abs/2507.16933', 'abstract': 'Large language models can be quantized to reduce inference time latency, model size, and energy consumption, thereby delivering a better user experience at lower cost. A challenge exists to deliver quantized models with minimal loss of accuracy in reasonable time, and in particular to do so without requiring mechanisms incompatible with specialized inference accelerators. Here, we demonstrate a simple, end-to-end quantization-aware training approach that, with an increase in total model training budget of less than 0.1%, outperforms the leading published quantization methods by large margins on several modern benchmarks, with both base and instruct model variants. The approach easily generalizes across different model architectures, can be applied to activations, cache, and weights, and requires the introduction of no additional operations to the model other than the quantization itself.', 'abstract_zh': '大型语言模型可以通过量化来降低推理时间延迟、模型大小和能源消耗，从而以更低的成本提供更好的用户体验。挑战在于在合理的时间内交付准确度损失最小的量化模型，并且特别是不需要与专用推理加速器不兼容的机制。在此，我们展示了一种简单的一体化量化感知训练方法，该方法在总模型训练预算增加不到0.1%的情况下，在多个现代基准测试中，无论是基础模型还是指令模型变体，都能显著优于现有公布的量化方法。该方法容易跨不同的模型架构进行泛化，可以应用于激活、缓存和权重，并且除了量化本身外，不需要在模型中引入额外的操作。', 'title_zh': 'SiLQ: 简洁的大语言模型量化感知训练'}
{'arxiv_id': 'arXiv:2507.16887', 'title': 'Revisiting Pre-trained Language Models for Vulnerability Detection', 'authors': 'Youpeng Li, Weiliang Qi, Xuyu Wang, Fuxun Yu, Xinda Wang', 'link': 'https://arxiv.org/abs/2507.16887', 'abstract': 'The rapid advancement of pre-trained language models (PLMs) has demonstrated promising results for various code-related tasks. However, their effectiveness in detecting real-world vulnerabilities remains a critical challenge. % for the security community. While existing empirical studies evaluate PLMs for vulnerability detection (VD), their inadequate consideration in data preparation, evaluation setups, and experimental settings undermines the accuracy and comprehensiveness of evaluations. This paper introduces RevisitVD, an extensive evaluation of 17 PLMs spanning smaller code-specific PLMs and large-scale PLMs using newly constructed datasets. Specifically, we compare the performance of PLMs under both fine-tuning and prompt engineering, assess their effectiveness and generalizability across various training and testing settings, and analyze their robustness against code normalization, abstraction, and semantic-preserving transformations.\nOur findings reveal that, for VD tasks, PLMs incorporating pre-training tasks designed to capture the syntactic and semantic patterns of code outperform both general-purpose PLMs and those solely pre-trained or fine-tuned on large code corpora. However, these models face notable challenges in real-world scenarios, such as difficulties in detecting vulnerabilities with complex dependencies, handling perturbations introduced by code normalization and abstraction, and identifying semantic-preserving vulnerable code transformations. Also, the truncation caused by the limited context windows of PLMs can lead to a non-negligible amount of labeling errors. This study underscores the importance of thorough evaluations of model performance in practical scenarios and outlines future directions to help enhance the effectiveness of PLMs for realistic VD applications.', 'abstract_zh': '预训练语言模型在漏洞检测任务中的全面评估：挑战与方向', 'title_zh': '重新审视预训练语言模型在漏洞检测中的应用'}
{'arxiv_id': 'arXiv:2507.16886', 'title': 'Sparser2Sparse: Single-shot Sparser-to-Sparse Learning for Spatial Transcriptomics Imputation with Natural Image Co-learning', 'authors': 'Yaoyu Fang, Jiahe Qian, Xinkun Wang, Lee A. Cooper, Bo Zhou', 'link': 'https://arxiv.org/abs/2507.16886', 'abstract': 'Spatial transcriptomics (ST) has revolutionized biomedical research by enabling high resolution gene expression profiling within tissues. However, the high cost and scarcity of high resolution ST data remain significant challenges. We present Single-shot Sparser-to-Sparse (S2S-ST), a novel framework for accurate ST imputation that requires only a single and low-cost sparsely sampled ST dataset alongside widely available natural images for co-training. Our approach integrates three key innovations: (1) a sparser-to-sparse self-supervised learning strategy that leverages intrinsic spatial patterns in ST data, (2) cross-domain co-learning with natural images to enhance feature representation, and (3) a Cascaded Data Consistent Imputation Network (CDCIN) that iteratively refines predictions while preserving sampled gene data fidelity. Extensive experiments on diverse tissue types, including breast cancer, liver, and lymphoid tissue, demonstrate that our method outperforms state-of-the-art approaches in imputation accuracy. By enabling robust ST reconstruction from sparse inputs, our framework significantly reduces reliance on costly high resolution data, facilitating potential broader adoption in biomedical research and clinical applications.', 'abstract_zh': '单-shot 稀疏转换为稀疏的ST单步插补框架（S2S-ST）：一种仅需单个低成本稀疏采样ST数据集和广泛可用的自然图像进行协同训练的新型高精度ST插补框架', 'title_zh': 'Sparser2Sparse: 单步稀疏到稀疏学习在空间转录组学插补中的自然图像共学习'}
{'arxiv_id': 'arXiv:2507.16884', 'title': 'SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative Modeling', 'authors': 'Yi Guo, Wei Wang, Zhihang Yuan, Rong Cao, Kuan Chen, Zhengyang Chen, Yuanyuan Huo, Yang Zhang, Yuping Wang, Shouda Liu, Yuxuan Wang', 'link': 'https://arxiv.org/abs/2507.16884', 'abstract': 'Generative models like Flow Matching have achieved state-of-the-art performance but are often hindered by a computationally expensive iterative sampling process. To address this, recent work has focused on few-step or one-step generation by learning the average velocity field, which directly maps noise to data. MeanFlow, a leading method in this area, learns this field by enforcing a differential identity that connects the average and instantaneous velocities. In this work, we argue that this differential formulation is a limiting special case of a more fundamental principle. We return to the first principles of average velocity and leverage the additivity property of definite integrals. This leads us to derive a novel, purely algebraic identity we term Interval Splitting Consistency. This identity establishes a self-referential relationship for the average velocity field across different time intervals without resorting to any differential operators. Based on this principle, we introduce SplitMeanFlow, a new training framework that enforces this algebraic consistency directly as a learning objective. We formally prove that the differential identity at the core of MeanFlow is recovered by taking the limit of our algebraic consistency as the interval split becomes infinitesimal. This establishes SplitMeanFlow as a direct and more general foundation for learning average velocity fields. From a practical standpoint, our algebraic approach is significantly more efficient, as it eliminates the need for JVP computations, resulting in simpler implementation, more stable training, and broader hardware compatibility. One-step and two-step SplitMeanFlow models have been successfully deployed in large-scale speech synthesis products (such as Doubao), achieving speedups of 20x.', 'abstract_zh': '基于区间拆分一致性的平均流生成模型', 'title_zh': 'SplitMeanFlow: 几乎步生成建模中的区间划分一致性'}
{'arxiv_id': 'arXiv:2507.16881', 'title': 'Confidence Optimization for Probabilistic Encoding', 'authors': 'Pengjiu Xia, Yidian Huang, Wenchao Wei, Yuwen Tan', 'link': 'https://arxiv.org/abs/2507.16881', 'abstract': 'Probabilistic encoding introduces Gaussian noise into neural networks, enabling a smooth transition from deterministic to uncertain states and enhancing generalization ability. However, the randomness of Gaussian noise distorts point-based distance measurements in classification tasks. To mitigate this issue, we propose a confidence optimization probabilistic encoding (CPE) method that improves distance reliability and enhances representation learning. Specifically, we refine probabilistic encoding with two key strategies: First, we introduce a confidence-aware mechanism to adjust distance calculations, ensuring consistency and reliability in probabilistic encoding classification tasks. Second, we replace the conventional KL divergence-based variance regularization, which relies on unreliable prior assumptions, with a simpler L2 regularization term to directly constrain variance. The method we proposed is model-agnostic, and extensive experiments on natural language classification tasks demonstrate that our method significantly improves performance and generalization on both the BERT and the RoBERTa model.', 'abstract_zh': '概率编码引入高斯噪声到神经网络中， enable 从确定性到不确定状态的平滑过渡并增强泛化能力。然而，高斯噪声的随机性会扭曲分类任务中的点基距离测量。为缓解这一问题，我们提出了一种基于置信度优化的概率编码（CPE）方法，以提高距离的可靠性和增强表示学习。具体而言，我们通过两种关键策略改进概率编码：首先，引入一种置信度感知机制来调整距离计算，确保概率编码分类任务中的一致性和可靠性；其次，用简单的L2正则化项替代依赖于不可靠先验假设的传统KL散度正则化项，直接约束方差。我们提出的方法是模型无关的，针对自然语言分类任务的广泛实验表明，我们的方法在BERT和RoBERTa模型上显著提高了性能和泛化能力。', 'title_zh': '概率编码中的置信优化'}
{'arxiv_id': 'arXiv:2507.16880', 'title': 'Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less Local Than Assumed', 'authors': 'Antoni Kowalczuk, Dominik Hintersdorf, Lukas Struppek, Kristian Kersting, Adam Dziedzic, Franziska Boenisch', 'link': 'https://arxiv.org/abs/2507.16880', 'abstract': 'Text-to-image diffusion models (DMs) have achieved remarkable success in image generation. However, concerns about data privacy and intellectual property remain due to their potential to inadvertently memorize and replicate training data. Recent mitigation efforts have focused on identifying and pruning weights responsible for triggering replication, based on the assumption that memorization can be localized. Our research assesses the robustness of these pruning-based approaches. We demonstrate that even after pruning, minor adjustments to text embeddings of input prompts are sufficient to re-trigger data replication, highlighting the fragility of these defenses. Furthermore, we challenge the fundamental assumption of memorization locality, by showing that replication can be triggered from diverse locations within the text embedding space, and follows different paths in the model. Our findings indicate that existing mitigation strategies are insufficient and underscore the need for methods that truly remove memorized content, rather than attempting to suppress its retrieval. As a first step in this direction, we introduce a novel adversarial fine-tuning method that iteratively searches for replication triggers and updates the model to increase robustness. Through our research, we provide fresh insights into the nature of memorization in text-to-image DMs and a foundation for building more trustworthy and compliant generative AI.', 'abstract_zh': '基于文本到图像的扩散模型（DMs）在图像生成方面取得了显著成功，但数据隐私和知识产权方面的担忧依然存在，因为这些模型有可能无意中记忆并复现训练数据。近期的缓解努力集中在识别并修剪负责触发复现的权重上，前提是假设记忆是局部化的。我们的研究评估了这些修剪方法的鲁棒性。我们证明，即使进行了修剪，对输入提示的文本嵌入进行细微调整也足以重新触发数据复现，强调了这些防御措施的脆弱性。此外，我们通过展示复现可以从文本嵌入空间中不同的位置触发，并且在模型中沿不同的路径进行，挑战了记忆局部化的基本假设。我们的研究结果表明，现有缓解策略是不足的，并强调了需要能够真正移除记忆内容的方法的重要性，而不仅仅是试图抑制其检索。作为这一方向上的第一步，我们提出了一个新颖的对抗微调方法，该方法迭代地搜索复现触发因素并对模型进行更新以提高其鲁棒性。通过我们的研究，我们提供了关于文本到图像DMs中记忆本质的新见解，并为构建更加可靠和合规的生成AI奠定了基础。', 'title_zh': '寻找多里：文本到图像扩散模型中的记忆作用不如假设的那么局部。'}
{'arxiv_id': 'arXiv:2507.16878', 'title': 'CausalStep: A Benchmark for Explicit Stepwise Causal Reasoning in Videos', 'authors': 'Xuchen Li, Xuzhao Li, Shiyu Hu, Kaiqi Huang, Wentao Zhang', 'link': 'https://arxiv.org/abs/2507.16878', 'abstract': 'Recent advances in large language models (LLMs) have improved reasoning in text and image domains, yet achieving robust video reasoning remains a significant challenge. Existing video benchmarks mainly assess shallow understanding and reasoning and allow models to exploit global context, failing to rigorously evaluate true causal and stepwise reasoning. We present CausalStep, a benchmark designed for explicit stepwise causal reasoning in videos. CausalStep segments videos into causally linked units and enforces a strict stepwise question-answer (QA) protocol, requiring sequential answers and preventing shortcut solutions. Each question includes carefully constructed distractors based on error type taxonomy to ensure diagnostic value. The benchmark features 100 videos across six categories and 1,852 multiple-choice QA pairs. We introduce seven diagnostic metrics for comprehensive evaluation, enabling precise diagnosis of causal reasoning capabilities. Experiments with leading proprietary and open-source models, as well as human baselines, reveal a significant gap between current models and human-level stepwise reasoning. CausalStep provides a rigorous benchmark to drive progress in robust and interpretable video reasoning.', 'abstract_zh': 'Recent advances in大型语言模型（LLMs）在文本和图像领域的推理能力取得了进步，但在视频推理的稳健性方面仍面临重大挑战。现有视频基准主要评估表面的理解和推理能力，并允许模型利用全局上下文，未能严格评估真实的因果和逐步推理。我们提出了CausalStep基准，专门用于视频中的显式逐步因果推理。CausalStep将视频分割为因果链接单元，并要求严格的逐步问答（QA）协议，不允许捷径解决方案。每个问题都基于错误类型分类精心构建了干扰项，以确保诊断价值。该基准包含6个类别中的100个视频和1,852个多项选择QA对。我们引入了七个诊断性指标，以进行全面评估，能够精确诊断因果推理能力。对领先的专业和开源模型以及人类基线的实验表明，当前模型与人类级别的逐步推理之间存在显著差距。CausalStep提供了一个严格的基准，以推动稳健和可解释的视频推理的进步。', 'title_zh': 'CausalStep: 一个用于视频中显式分步因果推理的标准基准'}
{'arxiv_id': 'arXiv:2507.16877', 'title': 'ReMeREC: Relation-aware and Multi-entity Referring Expression Comprehension', 'authors': 'Yizhi Hu, Zezhao Tian, Xingqun Qi, Chen Su, Bingkun Yang, Junhui Yin, Muyi Sun, Man Zhang, Zhenan Sun', 'link': 'https://arxiv.org/abs/2507.16877', 'abstract': 'Referring Expression Comprehension (REC) aims to localize specified entities or regions in an image based on natural language descriptions. While existing methods handle single-entity localization, they often ignore complex inter-entity relationships in multi-entity scenes, limiting their accuracy and reliability. Additionally, the lack of high-quality datasets with fine-grained, paired image-text-relation annotations hinders further progress. To address this challenge, we first construct a relation-aware, multi-entity REC dataset called ReMeX, which includes detailed relationship and textual annotations. We then propose ReMeREC, a novel framework that jointly leverages visual and textual cues to localize multiple entities while modeling their inter-relations. To address the semantic ambiguity caused by implicit entity boundaries in language, we introduce the Text-adaptive Multi-entity Perceptron (TMP), which dynamically infers both the quantity and span of entities from fine-grained textual cues, producing distinctive representations. Additionally, our Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and global scene understanding. To further improve language comprehension for fine-grained prompts, we also construct a small-scale auxiliary dataset, EntityText, generated using large language models. Experiments on four benchmark datasets show that ReMeREC achieves state-of-the-art performance in multi-entity grounding and relation prediction, outperforming existing approaches by a large margin.', 'abstract_zh': '基于关系的多实体引用表达理解（ReMeREC）：构建细粒度实体关系推理框架', 'title_zh': 'ReMeREC: 关系-aware 和多实体引用表达理解'}
{'arxiv_id': 'arXiv:2507.16876', 'title': 'Machine learning-based multimodal prognostic models integrating pathology images and high-throughput omic data for overall survival prediction in cancer: a systematic review', 'authors': 'Charlotte Jennings, Andrew Broad, Lucy Godson, Emily Clarke, David Westhead, Darren Treanor', 'link': 'https://arxiv.org/abs/2507.16876', 'abstract': 'Multimodal machine learning integrating histopathology and molecular data shows promise for cancer prognostication. We systematically reviewed studies combining whole slide images (WSIs) and high-throughput omics to predict overall survival. Searches of EMBASE, PubMed, and Cochrane CENTRAL (12/08/2024), plus citation screening, identified eligible studies. Data extraction used CHARMS; bias was assessed with PROBAST+AI; synthesis followed SWiM and PRISMA 2020. Protocol: PROSPERO (CRD42024594745).\nForty-eight studies (all since 2017) across 19 cancer types met criteria; all used The Cancer Genome Atlas. Approaches included regularised Cox regression (n=4), classical ML (n=13), and deep learning (n=31). Reported c-indices ranged 0.550-0.857; multimodal models typically outperformed unimodal ones. However, all studies showed unclear/high bias, limited external validation, and little focus on clinical utility.\nMultimodal WSI-omics survival prediction is a fast-growing field with promising results but needs improved methodological rigor, broader datasets, and clinical evaluation.\nFunded by NPIC, Leeds Teaching Hospitals NHS Trust, UK (Project 104687), supported by UKRI Industrial Strategy Challenge Fund.', 'abstract_zh': '多模态机器学习整合组织病理学和分子数据在癌症预后预测中显示出前景：系统回顾预测总体生存的研究', 'title_zh': '基于机器学习的结合病理图像和高 throughput 组学数据的多模态预后模型：癌症总体生存预测系统的综述'}
{'arxiv_id': 'arXiv:2507.16874', 'title': 'Budget Allocation Policies for Real-Time Multi-Agent Path Finding', 'authors': 'Raz Beck, Roni Stern', 'link': 'https://arxiv.org/abs/2507.16874', 'abstract': 'Multi-Agent Pathfinding (MAPF) is the problem of finding paths for a set of agents such that each agent reaches its desired destination while avoiding collisions with the other agents. Many MAPF solvers are designed to run offline, that is, first generate paths for all agents and then execute them. Real-Time MAPF (RT-MAPF) embodies a realistic MAPF setup in which one cannot wait until a complete path for each agent has been found before they start to move. Instead, planning and execution are interleaved, where the agents must commit to a fixed number of steps in a constant amount of computation time, referred to as the planning budget. Existing solutions to RT-MAPF iteratively call windowed versions of MAPF algorithms in every planning period, without explicitly considering the size of the planning budget. We address this gap and explore different policies for allocating the planning budget in windowed versions of standard MAPF algorithms, namely Prioritized Planning (PrP) and MAPF-LNS2. Our exploration shows that the baseline approach in which all agents draw from a shared planning budget pool is ineffective in over-constrained situations. Instead, policies that distribute the planning budget over the agents are able to solve more problems with a smaller makespan.', 'abstract_zh': '实时多智能体路径规划（RT-MAPF）中规划预算分配策略的研究', 'title_zh': '实时多智能体路径规划中的预算分配策略'}
{'arxiv_id': 'arXiv:2507.16873', 'title': 'HIPPO-Video: Simulating Watch Histories with Large Language Models for Personalized Video Highlighting', 'authors': 'Jeongeun Lee, Youngjae Yu, Dongha Lee', 'link': 'https://arxiv.org/abs/2507.16873', 'abstract': 'The exponential growth of video content has made personalized video highlighting an essential task, as user preferences are highly variable and complex. Existing video datasets, however, often lack personalization, relying on isolated videos or simple text queries that fail to capture the intricacies of user behavior. In this work, we introduce HIPPO-Video, a novel dataset for personalized video highlighting, created using an LLM-based user simulator to generate realistic watch histories reflecting diverse user preferences. The dataset includes 2,040 (watch history, saliency score) pairs, covering 20,400 videos across 170 semantic categories. To validate our dataset, we propose HiPHer, a method that leverages these personalized watch histories to predict preference-conditioned segment-wise saliency scores. Through extensive experiments, we demonstrate that our method outperforms existing generic and query-based approaches, showcasing its potential for highly user-centric video highlighting in real-world scenarios.', 'abstract_zh': '个性化视频摘要的指数增长使得个性化视频亮点提取成为一项必不可少的任务，因为用户偏好高度多样且复杂。现有视频数据集通常缺乏个性化，依赖于孤立的视频或简单的文本查询，未能捕捉用户行为的复杂性。在本工作中，我们引入了HIPPO-Video，这是一个采用基于LLM的用户模拟器生成反映多样化用户偏好的真实观看历史的新数据集。该数据集包含2040个（观看历史，显著性分数）对，覆盖了170个语义类别中的20400个视频。为了验证我们的数据集，我们提出了一种名为HiPHer的方法，该方法利用这些个性化的观看历史来预测条件偏好下的段落显著性分数。通过广泛的实验，我们证明了我们的方法优于现有的通用和查询驱动的方法，展示了其在实际场景中实现高度用户中心的视频摘要的潜力。', 'title_zh': 'HIPPO-视频：使用大型语言模型模拟观看历史以便个性化视频摘要生成'}
{'arxiv_id': 'arXiv:2507.16872', 'title': 'CompLeak: Deep Learning Model Compression Exacerbates Privacy Leakage', 'authors': 'Na Li, Yansong Gao, Hongsheng Hu, Boyu Kuang, Anmin Fu', 'link': 'https://arxiv.org/abs/2507.16872', 'abstract': "Model compression is crucial for minimizing memory storage and accelerating inference in deep learning (DL) models, including recent foundation models like large language models (LLMs). Users can access different compressed model versions according to their resources and budget. However, while existing compression operations primarily focus on optimizing the trade-off between resource efficiency and model performance, the privacy risks introduced by compression remain overlooked and insufficiently understood.\nIn this work, through the lens of membership inference attack (MIA), we propose CompLeak, the first privacy risk evaluation framework examining three widely used compression configurations that are pruning, quantization, and weight clustering supported by the commercial model compression framework of Google's TensorFlow-Lite (TF-Lite) and Facebook's PyTorch Mobile. CompLeak has three variants, given available access to the number of compressed models and original model. CompLeakNR starts by adopting existing MIA methods to attack a single compressed model, and identifies that different compressed models influence members and non-members differently. When the original model and one compressed model are available, CompLeakSR leverages the compressed model as a reference to the original model and uncovers more privacy by combining meta information (e.g., confidence vector) from both models. When multiple compressed models are available with/without accessing the original model, CompLeakMR innovatively exploits privacy leakage info from multiple compressed versions to substantially signify the overall privacy leakage. We conduct extensive experiments on seven diverse model architectures (from ResNet to foundation models of BERT and GPT-2), and six image and textual benchmark datasets.", 'abstract_zh': '模型压缩对于最小化深度学习模型的记忆存储并加速推理至关重要，包括最近的基础模型如大规模语言模型（LLMs）。用户可以根据其资源和预算访问不同压缩模型版本。然而，现有压缩操作主要集中在资源效率与模型性能之间的权衡优化上，而压缩引入的隐私风险却未被充分关注和理解。\n在此项工作中，通过会员推理攻击（MIA）的视角，我们提出了CompLeak，这是首个评估三种广泛使用的压缩配置（剪枝、量化和权重聚类）的隐私风险评估框架，这些配置受到Google的TensorFlow-Lite（TF-Lite）和Facebook的PyTorch Mobile的商业模型压缩框架的支持。CompLeak有三种变体，根据可获得的压缩模型和原始模型数量不同。CompLeakNR首先采用现有的MIA方法攻击单个压缩模型，并发现不同压缩模型对成员和非成员的影响不同。当原始模型和一个压缩模型都可用时，CompLeakSR利用压缩模型作为原始模型的参考，并通过结合两个模型的元信息（如置信向量）揭露更多的隐私。当有多个压缩模型可用但未访问原始模型时，CompLeakMR创新地利用来自多个压缩版本的隐私泄漏信息，显著表明整体的隐私泄漏。我们在七个不同的模型架构（从ResNet到BERT和GPT-2的基础模型）和六个图像和文本基准数据集上进行了广泛的实验。', 'title_zh': 'CompLeak: 深度学习模型压缩加剧了隐私泄露'}
{'arxiv_id': 'arXiv:2507.16867', 'title': 'Diffusion-Modeled Reinforcement Learning for Carbon and Risk-Aware Microgrid Optimization', 'authors': 'Yunyi Zhao, Wei Zhang, Cheng Xiang, Hongyang Du, Dusit Niyato, Shuhua Gao', 'link': 'https://arxiv.org/abs/2507.16867', 'abstract': 'This paper introduces DiffCarl, a diffusion-modeled carbon- and risk-aware reinforcement learning algorithm for intelligent operation of multi-microgrid systems. With the growing integration of renewables and increasing system complexity, microgrid communities face significant challenges in real-time energy scheduling and optimization under uncertainty. DiffCarl integrates a diffusion model into a deep reinforcement learning (DRL) framework to enable adaptive energy scheduling under uncertainty and explicitly account for carbon emissions and operational risk. By learning action distributions through a denoising generation process, DiffCarl enhances DRL policy expressiveness and enables carbon- and risk-aware scheduling in dynamic and uncertain microgrid environments. Extensive experimental studies demonstrate that it outperforms classic algorithms and state-of-the-art DRL solutions, with 2.3-30.1% lower operational cost. It also achieves 28.7% lower carbon emissions than those of its carbon-unaware variant and reduces performance variability. These results highlight DiffCarl as a practical and forward-looking solution. Its flexible design allows efficient adaptation to different system configurations and objectives to support real-world deployment in evolving energy systems.', 'abstract_zh': 'DiffCarl：一种扩散建模的碳和风险意识强化学习算法用于智能多微电网系统操作', 'title_zh': '基于扩散建模的碳排放与风险意识微网优化强化学习方法'}
{'arxiv_id': 'arXiv:2507.16864', 'title': 'Reinforcement Learning in hyperbolic space for multi-step reasoning', 'authors': 'Tao Xu, Dung-Yang Lee, Momiao Xiong', 'link': 'https://arxiv.org/abs/2507.16864', 'abstract': 'Multi-step reasoning is a fundamental challenge in artificial intelligence, with applications ranging from mathematical problem-solving to decision-making in dynamic environments. Reinforcement Learning (RL) has shown promise in enabling agents to perform multi-step reasoning by optimizing long-term rewards. However, conventional RL methods struggle with complex reasoning tasks due to issues such as credit assignment, high-dimensional state representations, and stability concerns. Recent advancements in Transformer architectures and hyperbolic geometry have provided novel solutions to these challenges. This paper introduces a new framework that integrates hyperbolic Transformers into RL for multi-step reasoning. The proposed approach leverages hyperbolic embeddings to model hierarchical structures effectively. We present theoretical insights, algorithmic details, and experimental results that include Frontier Math and nonlinear optimal control problems. Compared to RL with vanilla transformer, the hyperbolic RL largely improves accuracy by (32%~44%) on FrontierMath benchmark, (43%~45%) on nonlinear optimal control benchmark, while achieving impressive reduction in computational time by (16%~32%) on FrontierMath benchmark, (16%~17%) on nonlinear optimal control benchmark. Our work demonstrates the potential of hyperbolic Transformers in reinforcement learning, particularly for multi-step reasoning tasks that involve hierarchical structures.', 'abstract_zh': '基于超曲面变换器的强化学习多步推理框架', 'title_zh': '在双曲空间中进行多步推理的强化学习'}
{'arxiv_id': 'arXiv:2507.16863', 'title': 'Pixels, Patterns, but No Poetry: To See The World like Humans', 'authors': 'Hongcheng Gao, Zihao Huang, Lin Xu, Jingyi Tang, Xinhao Li, Yue Liu, Haoyang Li, Taihang Hu, Minhua Lin, Xinlong Yang, Ge Wu, Balong Bi, Hongyu Chen, Wentao Zhang', 'link': 'https://arxiv.org/abs/2507.16863', 'abstract': "Achieving human-like perception and reasoning in Multimodal Large Language Models (MLLMs) remains a central challenge in artificial intelligence. While recent research has primarily focused on enhancing reasoning capabilities in MLLMs, a fundamental question persists: Can Multimodal Large Language Models truly perceive the world as humans do? This paper shifts focus from reasoning to perception. Rather than constructing benchmarks specifically for reasoning, we introduce the Turing Eye Test (TET), a challenging perception-oriented benchmark comprising four diagnostic tasks that evaluate MLLMs' performance on synthetic images that humans process intuitively. Our findings reveal that state-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks trivial for humans. Both in-context learning and training on language backbone-effective for previous benchmarks-fail to improve performance on our tasks, while fine-tuning the vision tower enables rapid adaptation, suggesting that our benchmark poses challenges for vision tower generalization rather than for the knowledge and reasoning capabilities of the language backbone-a key gap between current MLLMs and human perception. We release a representative subset of TET tasks in this version, and will introduce more diverse tasks and methods to enhance visual generalization in future work.", 'abstract_zh': '在多模态大语言模型（MLLMs）中实现类似人类的感知与推理仍然是人工智能中的一个核心挑战。尽管近期的研究主要集中在增强MLLMs的推理能力上，但一个基本问题仍然存在：多模态大语言模型真能像人类一样感知世界吗？本文将研究重点从推理转向感知。我们构建了图灵眼部测试（TET），这是一个具有四个诊断任务的具有挑战性的感知导向基准，用于评估MLLMs在人类直观处理的合成图像上的表现。我们的发现表明，最先进的MLLMs在人类认为简单的感知任务上表现出灾难性的失败。上下文学习和语言骨干训练——这在过去的一些基准测试中有效——未能提高我们在任务上的表现，而微调视觉塔则能够迅速适应，这表明我们的基准测试对视觉塔的泛化提出了挑战，而不是对语言骨干的知识和推理能力——这是当前MLLMs与人类感知之间的关键差距。我们在此版本中释放了TET任务的一个代表性子集，并将在未来的工作中引入更多样化的任务和方法以增强视觉泛化能力。', 'title_zh': '像素、模式，但没有诗意：以人类方式看待世界'}
{'arxiv_id': 'arXiv:2507.16861', 'title': 'Look Before You Fuse: 2D-Guided Cross-Modal Alignment for Robust 3D Detection', 'authors': 'Xiang Li', 'link': 'https://arxiv.org/abs/2507.16861', 'abstract': "Integrating LiDAR and camera inputs into a unified Bird's-Eye-View (BEV) representation is crucial for enhancing 3D perception capabilities of autonomous vehicles. However, current methods are often affected by misalignment between camera and LiDAR features. This misalignment leads to inaccurate depth supervision in camera branch and erroneous fusion during cross-modal feature aggregation. The root cause of this misalignment lies in projection errors, stemming from minor extrinsic calibration inaccuracies and rolling shutter effect of LiDAR during vehicle motion. In this work, our key insight is that these projection errors are predominantly concentrated at object-background boundaries, which are readily identified by 2D detectors. Based on this, our main motivation is to utilize 2D object priors to pre-align cross-modal features before fusion. To address local misalignment, we propose Prior Guided Depth Calibration (PGDC), which leverages 2D priors to correct local misalignment and preserve correct cross-modal feature pairs. To resolve global misalignment, we introduce Discontinuity Aware Geometric Fusion (DAGF) to process calibrated results from PGDC, suppressing noise and explicitly enhancing sharp transitions at object-background boundaries. To effectively utilize these transition-aware depth representations, we incorporate Structural Guidance Depth Modulator (SGDM), using a gated attention mechanism to efficiently fuse aligned depth and image features. Our proposed method achieves state-of-the-art performance on nuScenes validation dataset, with its mAP and NDS reaching 71.5% and 73.6% respectively.", 'abstract_zh': '将LiDAR和相机输入整合到统一的鸟瞰图（BEV）表示中对于增强自动驾驶车辆的三维感知能力至关重要。然而，目前的方法往往受到相机和LiDAR特征之间的对齐偏差的影响。这种对齐偏差导致了相机分支中的不准确深度监督和跨模态特征聚合时的错误融合。这种对齐偏差的根本原因在于投影误差，这些误差源自于车辆运动过程中外参校准的小幅不准确和LiDAR的卷帘快门效应。在本工作中，我们的关键洞察是这些投影误差主要集中在物体与背景的边界上，这些边界可以被二维检测器轻易识别。基于此，我们的主要动机是利用二维物体先验在融合前对齐跨模态特征。为解决局部对齐偏差，我们提出了先验引导深度校准（PGDC），该方法利用二维先验纠正局部对齐偏差并保留正确的跨模态特征对。为解决全局对齐偏差，我们引入了不连续性感知几何融合（DAGF），对PGDC校准的结果进行处理，抑制噪声并明确增强物体与背景边界处的锐变过渡。为了有效地利用这些过渡感知深度表示，我们引入了结构引导深度调制器（SGDM），使用门控注意力机制高效融合对齐的深度和图像特征。我们提出的方法在nuScenes验证数据集上达到了最先进的性能，其mAP和NDS分别达到了71.5%和73.6%。', 'title_zh': '先看后融合：2D导向的跨模态对齐以实现稳健的3D检测'}
{'arxiv_id': 'arXiv:2507.16859', 'title': 'Leveraging multi-source and heterogeneous signals for fatigue detection', 'authors': 'Luobin Cui, Yanlai Wu, Tang Ying, Weikai Li', 'link': 'https://arxiv.org/abs/2507.16859', 'abstract': 'Fatigue detection plays a critical role in safety-critical applications such as aviation, mining, and long-haul transport. However, most existing methods rely on high-end sensors and controlled environments, limiting their applicability in real world settings. This paper formally defines a practical yet underexplored problem setting for real world fatigue detection, where systems operating with context-appropriate sensors aim to leverage knowledge from differently instrumented sources including those using impractical sensors deployed in controlled environments. To tackle this challenge, we propose a heterogeneous and multi-source fatigue detection framework that adaptively utilizes the available modalities in the target domain while benefiting from the diverse configurations present in source domains. Our experiments, conducted using a realistic field-deployed sensor setup and two publicly available datasets, demonstrate the practicality, robustness, and improved generalization of our approach, paving the practical way for effective fatigue monitoring in sensor-constrained scenarios.', 'abstract_zh': '疲劳检测在航空、采矿和长途运输等安全关键应用中起着至关重要的作用。然而，现有方法大多依赖高端传感器和受控环境，限制了其在现实世界场景中的应用。本文正式定义了一个实际但尚未充分探索的现实世界疲劳检测问题设定，其中系统利用上下文适配的传感器，并从包括在受控环境中部署的不切实际传感器的各种装备来源中获取知识。为了应对这一挑战，我们提出了一个异构的多源疲劳检测框架，该框架能够适应性地利用目标域中的可用模态，并从来源域中存在的多样化配置中受益。我们在一个现实世界部署的传感器配置和两个公开可用的数据集中进行了实验，证明了该方法的实用性、鲁棒性和更好的泛化能力，为传感器受限场景下的有效疲劳监控奠定了实用基础。', 'title_zh': '利用多源异构信号进行疲劳检测'}
{'arxiv_id': 'arXiv:2507.16856', 'title': 'SIA: Enhancing Safety via Intent Awareness for Vision-Language Models', 'authors': 'Youngjin Na, Sangheon Jeong, Youngwan Lee', 'link': 'https://arxiv.org/abs/2507.16856', 'abstract': 'As vision-language models (VLMs) are increasingly deployed in real-world applications, new safety risks arise from the subtle interplay between images and text. In particular, seemingly innocuous inputs can combine to reveal harmful intent, leading to unsafe model responses. Despite increasing attention to multimodal safety, previous approaches based on post hoc filtering or static refusal prompts struggle to detect such latent risks, especially when harmfulness emerges only from the combination of inputs. We propose SIA (Safety via Intent Awareness), a training-free prompt engineering framework that proactively detects and mitigates harmful intent in multimodal inputs. SIA employs a three-stage reasoning process: (1) visual abstraction via captioning, (2) intent inference through few-shot chain-of-thought prompting, and (3) intent-conditioned response refinement. Rather than relying on predefined rules or classifiers, SIA dynamically adapts to the implicit intent inferred from the image-text pair. Through extensive experiments on safety-critical benchmarks including SIUO, MM-SafetyBench, and HoliSafe, we demonstrate that SIA achieves substantial safety improvements, outperforming prior methods. Although SIA shows a minor reduction in general reasoning accuracy on MMStar, the corresponding safety gains highlight the value of intent-aware reasoning in aligning VLMs with human-centric values.', 'abstract_zh': '基于意图感知的多模态安全性提升方法（Safety via Intent Awareness）', 'title_zh': 'SIA：通过意图 Awareness 提高视觉-语言模型的安全性'}
{'arxiv_id': 'arXiv:2507.16854', 'title': 'CLAMP: Contrastive Learning with Adaptive Multi-loss and Progressive Fusion for Multimodal Aspect-Based Sentiment Analysis', 'authors': 'Xiaoqiang He', 'link': 'https://arxiv.org/abs/2507.16854', 'abstract': "Multimodal aspect-based sentiment analysis(MABSA) seeks to identify aspect terms within paired image-text data and determine their fine grained sentiment polarities, representing a fundamental task for improving the effectiveness of applications such as product review systems and public opinion monitoring. Existing methods face challenges such as cross modal alignment noise and insufficient consistency in fine-grained representations. While global modality alignment methods often overlook the connection between aspect terms and their corresponding local visual regions, bridging the representation gap between text and images remains a challenge. To address these limitations, this paper introduces an end to end Contrastive Learning framework with Adaptive Multi-loss and Progressive Attention Fusion(CLAMP). The framework is composed of three novel modules: Progressive Attention Fusion network, Multi-task Contrastive Learning, and Adaptive Multi-loss Aggregation. The Progressive Attention Fusion network enhances fine-grained alignment between textual features and image regions via hierarchical, multi-stage cross modal interactions, effectively suppressing irrelevant visual noise. Secondly, multi-task contrastive learning combines global modal contrast and local granularity alignment to enhance cross modal representation consistency. Adaptive Multi-loss Aggregation employs a dynamic uncertainty based weighting mechanism to calibrate loss contributions according to each task's uncertainty, thereby mitigating gradient interference. Evaluation on standard public benchmarks demonstrates that CLAMP consistently outperforms the vast majority of existing state of the art methods.", 'abstract_zh': '多模态方面级情感分析（MABSA）旨在识别配对的图像-文本数据中的方面术语并确定其细粒度情感极性，这对提高产品评论系统和公共意见监控等应用的效果至关重要。现有方法面临跨模态对齐噪声和细粒度表示不足的挑战。虽然全局模态对齐方法往往忽略了方面术语与其相应的局部视觉区域之间的联系，但在文本和图像之间架桥表示差距仍是一个挑战。为解决这些局限性，本文提出了一种端到端的自适应多损失渐进注意力融合对比学习框架（CLAMP）。该框架由三个新颖模块组成：渐进注意力融合网络、多任务对比学习和自适应多损失聚合。渐进注意力融合网络通过多层次、多阶段跨模态交互来增强文本特征与图像区域之间的细粒度对齐，有效地抑制了无关视觉噪声。其次，多任务对比学习结合了全局模态对比和局部细节对齐，以增强跨模态表示一致性。自适应多损失聚合采用基于动态不确定性权重机制来根据每个任务的不确定性校准损失贡献，从而减轻梯度干扰。在标准公开基准上的评估表明，CLAMP在大多数现有最先进的方法中表现出优越性能。', 'title_zh': 'CLAMP：自适应多损失对比学习与 progressive 融合在多模态方面级情感分析中的应用'}
{'arxiv_id': 'arXiv:2507.16852', 'title': 'SynthCTI: LLM-Driven Synthetic CTI Generation to enhance MITRE Technique Mapping', 'authors': 'Álvaro Ruiz-Ródenas, Jaime Pujante Sáez, Daniel García-Algora, Mario Rodríguez Béjar, Jorge Blasco, José Luis Hernández-Ramos', 'link': 'https://arxiv.org/abs/2507.16852', 'abstract': 'Cyber Threat Intelligence (CTI) mining involves extracting structured insights from unstructured threat data, enabling organizations to understand and respond to evolving adversarial behavior. A key task in CTI mining is mapping threat descriptions to MITRE ATT\\&CK techniques. However, this process is often performed manually, requiring expert knowledge and substantial effort. Automated approaches face two major challenges: the scarcity of high-quality labeled CTI data and class imbalance, where many techniques have very few examples. While domain-specific Large Language Models (LLMs) such as SecureBERT have shown improved performance, most recent work focuses on model architecture rather than addressing the data limitations. In this work, we present SynthCTI, a data augmentation framework designed to generate high-quality synthetic CTI sentences for underrepresented MITRE ATT\\&CK techniques. Our method uses a clustering-based strategy to extract semantic context from training data and guide an LLM in producing synthetic CTI sentences that are lexically diverse and semantically faithful. We evaluate SynthCTI on two publicly available CTI datasets, CTI-to-MITRE and TRAM, using LLMs with different capacity. Incorporating synthetic data leads to consistent macro-F1 improvements: for example, ALBERT improves from 0.35 to 0.52 (a relative gain of 48.6\\%), and SecureBERT reaches 0.6558 (up from 0.4412). Notably, smaller models augmented with SynthCTI outperform larger models trained without augmentation, demonstrating the value of data generation methods for building efficient and effective CTI classification systems.', 'abstract_zh': '基于合成数据的Cyber威胁情报（CTI）矿化解决策ércy威胁情报（CTI）挖掘涉及从非结构化威胁数据中提取结构化洞见，使组织能够理解并响应不断演变的对手行为。CTI挖掘中的一个关键任务是将威胁描述映射到MITRE ATT&CK技术。然而，这一过程通常需要手动完成，依赖专家知识和大量努力。自动化方法面临两个主要挑战：高质量标注CTI数据的稀缺和类别不平衡，其中许多技术具有非常少的示例。尽管领域特定的大语言模型（LLMs）如SecureBERT已经显示出改进性能，但近期大部分工作侧重于模型架构而不是解决数据限制问题。在本项工作中，我们提出了SynthCTI，一种数据增强框架，旨在生成高质量的合成CTI句子以填充MITRE ATT&CK下代表现不足的技术。我们的方法使用基于聚类的策略从训练数据中提取语义上下文，并指导LLM产生词汇上多样且语义上忠实的合成CTI句子。我们在两个公开可用的CTI数据集CTI-to-MITRE和TRAM上使用不同容量的LLM评估SynthCTI。包含合成数据导致宏观经济F1提高：例如，ALBERT从0.35提高到0.52（相对增加48.6%），SecureBERT达到0.6558（从0.4412提高）。值得注意的是，使用SynthCTI增强的较小模型优于未增强的大模型，这表明数据生成方法对于构建高效有效的CTI分类系统具有价值。', 'title_zh': 'SynthCTI: LLM驱动的合成CTI生成以增强MITRE技术映射'}
{'arxiv_id': 'arXiv:2507.16850', 'title': 'Toward a Real-Time Framework for Accurate Monocular 3D Human Pose Estimation with Geometric Priors', 'authors': 'Mohamed Adjel', 'link': 'https://arxiv.org/abs/2507.16850', 'abstract': 'Monocular 3D human pose estimation remains a challenging and ill-posed problem, particularly in real-time settings and unconstrained environments. While direct imageto-3D approaches require large annotated datasets and heavy models, 2D-to-3D lifting offers a more lightweight and flexible alternative-especially when enhanced with prior knowledge. In this work, we propose a framework that combines real-time 2D keypoint detection with geometry-aware 2D-to-3D lifting, explicitly leveraging known camera intrinsics and subject-specific anatomical priors. Our approach builds on recent advances in self-calibration and biomechanically-constrained inverse kinematics to generate large-scale, plausible 2D-3D training pairs from MoCap and synthetic datasets. We discuss how these ingredients can enable fast, personalized, and accurate 3D pose estimation from monocular images without requiring specialized hardware. This proposal aims to foster discussion on bridging data-driven learning and model-based priors to improve accuracy, interpretability, and deployability of 3D human motion capture on edge devices in the wild.', 'abstract_zh': '单目3D人体姿态估计仍然是一个具有挑战性和病态的问题，特别是在实时设置和不受约束的环境中。虽然直接从图像到3D的方法需要大量标注的数据集和复杂的模型，但从2D到3D的提升提供了更轻量级和灵活的选择—尤其是在结合先验知识时。在本文中，我们提出了一种结合实时2D关键点检测与几何感知的2D到3D提升的框架，明确利用已知相机内参和个体特定的解剖先验知识。我们的方法建立在自我校准和生物力学约束逆运动学的最新进展之上，从动捕和合成数据集中生成大规模、合理的2D到3D训练对。我们讨论了这些成分如何能够实现快速、个性化且准确的单目图像3D姿态估计，无需专用硬件。本提案旨在促进关于结合数据驱动学习与模型驱动先验，以提高自然环境中基于边缘设备的人体运动捕捉准确度、可解释性和部署性的讨论。', 'title_zh': '面向实时的基于几何先验的准确单目3D人体姿态估计框架'}
{'arxiv_id': 'arXiv:2507.16849', 'title': 'Post-Disaster Affected Area Segmentation with a Vision Transformer (ViT)-based EVAP Model using Sentinel-2 and Formosat-5 Imagery', 'authors': 'Yi-Shan Chu, Hsuan-Cheng Wei', 'link': 'https://arxiv.org/abs/2507.16849', 'abstract': 'We propose a vision transformer (ViT)-based deep learning framework to refine disaster-affected area segmentation from remote sensing imagery, aiming to support and enhance the Emergent Value Added Product (EVAP) developed by the Taiwan Space Agency (TASA). The process starts with a small set of manually annotated regions. We then apply principal component analysis (PCA)-based feature space analysis and construct a confidence index (CI) to expand these labels, producing a weakly supervised training set. These expanded labels are then used to train ViT-based encoder-decoder models with multi-band inputs from Sentinel-2 and Formosat-5 imagery. Our architecture supports multiple decoder variants and multi-stage loss strategies to improve performance under limited supervision. During the evaluation, model predictions are compared with higher-resolution EVAP output to assess spatial coherence and segmentation consistency. Case studies on the 2022 Poyang Lake drought and the 2023 Rhodes wildfire demonstrate that our framework improves the smoothness and reliability of segmentation results, offering a scalable approach for disaster mapping when accurate ground truth is unavailable.', 'abstract_zh': '基于视景变换器的灾害影响区域遥感影像分割深度学习框架：支持台湾太空署Emergent Value Added Product（EVAP）的发展', 'title_zh': '基于Vision Transformer (ViT) 的EVAP模型：利用Sentinel-2和Formosat-5影像的灾害影响区域分割'}
{'arxiv_id': 'arXiv:2507.16848', 'title': 'Dynamic Simulation Framework for Disinformation Dissemination and Correction With Social Bots', 'authors': 'Boyu Qiao, Kun Li, Wei Zhou, Songlin Hu', 'link': 'https://arxiv.org/abs/2507.16848', 'abstract': 'In the human-bot symbiotic information ecosystem, social bots play key roles in spreading and correcting disinformation. Understanding their influence is essential for risk control and better governance. However, current studies often rely on simplistic user and network modeling, overlook the dynamic behavior of bots, and lack quantitative evaluation of correction strategies. To fill these gaps, we propose MADD, a Multi Agent based framework for Disinformation Dissemination. MADD constructs a more realistic propagation network by integrating the Barabasi Albert Model for scale free topology and the Stochastic Block Model for community structures, while designing node attributes based on real world user data. Furthermore, MADD incorporates both malicious and legitimate bots, with their controlled dynamic participation allows for quantitative analysis of correction strategies. We evaluate MADD using individual and group level metrics. We experimentally verify the real world consistency of MADD user attributes and network structure, and we simulate the dissemination of six disinformation topics, demonstrating the differential effects of fact based and narrative based correction strategies.', 'abstract_zh': '在人类-机器人共生信息生态系统中，社交媒体机器人在传播和纠正虚假信息方面发挥关键作用。理解它们的影响对于风险控制和更有效的治理至关重要。然而，当前的研究往往依赖于简单的用户和网络建模，忽视了机器人的动态行为，并缺乏纠正策略的定量评估。为了弥补这些不足，我们提出了MADD（基于多代理的虚假信息传播框架），该框架通过结合Barabasi Albert模型的无尺度拓扑和Stochastic Block模型的社区结构来构建更现实的传播网络，并基于真实用户的数据设计节点属性。此外，MADD 包括恶意和合法的机器人，并通过控制它们的动态参与来进行纠正策略的定量分析。我们使用个体和群体水平的指标评估 MADD，并实验性地验证 MADD 用户属性和网络结构的现实一致性，同时模拟了六个虚假信息主题的传播，展示了基于事实和基于叙事的纠正策略的差异性效果。', 'title_zh': '带有社会机器人的虚假信息传播与纠偏的动态模拟框架'}
{'arxiv_id': 'arXiv:2507.16843', 'title': 'Weak Supervision Techniques towards Enhanced ASR Models in Industry-level CRM Systems', 'authors': 'Zhongsheng Wang, Sijie Wang, Jia Wang, Yung-I Liang, Yuxi Zhang, Jiamou Liu', 'link': 'https://arxiv.org/abs/2507.16843', 'abstract': 'In the design of customer relationship management (CRM) systems, accurately identifying customer types and offering personalized services are key to enhancing customer satisfaction and loyalty. However, this process faces the challenge of discerning customer voices and intentions, and general pre-trained automatic speech recognition (ASR) models make it difficult to effectively address industry-specific speech recognition tasks. To address this issue, we innovatively proposed a solution for fine-tuning industry-specific ASR models, which significantly improved the performance of the fine-tuned ASR models in industry applications. Experimental results show that our method substantially improves the crucial auxiliary role of the ASR model in industry CRM systems, and this approach has also been adopted in actual industrial applications.', 'abstract_zh': '在客户关系管理（CRM）系统设计中，准确识别顾客类型并提供个性化服务是提升顾客满意度和忠诚度的关键。然而，这一过程面临区分顾客声音和意图的挑战，通用预训练自动语音识别（ASR）模型难以有效应对行业特定的语音识别任务。为解决这一问题，我们创新性地提出了一种针对行业特定ASR模型的微调方案，显著提升了微调后的ASR模型在行业应用中的性能。实验结果表明，我们的方法大幅提升了ASR模型在行业CRM系统中的关键辅助作用，并且该方法已在实际工业应用中得到采用。', 'title_zh': '面向工业级CRM系统的弱监督技术增强ASR模型'}
{'arxiv_id': 'arXiv:2507.16840', 'title': 'CASPER: Contrastive Approach for Smart Ponzi Scheme Detecter with More Negative Samples', 'authors': 'Weijia Yang, Tian Lan, Leyuan Liu, Wei Chen, Tianqing Zhu, Sheng Wen, Xiaosong Zhang', 'link': 'https://arxiv.org/abs/2507.16840', 'abstract': "The rapid evolution of digital currency trading, fueled by the integration of blockchain technology, has led to both innovation and the emergence of smart Ponzi schemes. A smart Ponzi scheme is a fraudulent investment operation in smart contract that uses funds from new investors to pay returns to earlier investors. Traditional Ponzi scheme detection methods based on deep learning typically rely on fully supervised models, which require large amounts of labeled data. However, such data is often scarce, hindering effective model training. To address this challenge, we propose a novel contrastive learning framework, CASPER (Contrastive Approach for Smart Ponzi detectER with more negative samples), designed to enhance smart Ponzi scheme detection in blockchain transactions. By leveraging contrastive learning techniques, CASPER can learn more effective representations of smart contract source code using unlabeled datasets, significantly reducing both operational costs and system complexity. We evaluate CASPER on the XBlock dataset, where it outperforms the baseline by 2.3% in F1 score when trained with 100% labeled data. More impressively, with only 25% labeled data, CASPER achieves an F1 score nearly 20% higher than the baseline under identical experimental conditions. These results highlight CASPER's potential for effective and cost-efficient detection of smart Ponzi schemes, paving the way for scalable fraud detection solutions in the future.", 'abstract_zh': '区块链技术集成推动的数字货币交易快速演变引发了创新和智能庞氏骗局的出现。智能庞氏骗局是一种使用新投资者资金向早期投资者支付回报的欺诈性投资操作，基于智能合约。传统的基于深度学习的庞氏骗局检测方法通常依赖于完全监督模型，需要大量标记数据，但此类数据往往稀缺，阻碍有效模型训练。为解决这一挑战，我们提出了一种新颖的对比学习框架CASPER（基于更多负样本的对比方法用于智能庞氏骗局检测器），旨在增强区块链交易中智能庞氏骗局的检测能力。通过利用对比学习技术，CASPER能够在未标记数据集上学习更有效的智能合约源代码表示，显著降低操作成本和系统复杂性。我们在XBlock数据集上评估了CASPER，结果显示，在使用100%标记数据训练时，CASPER在F1分数上优于基线方法2.3%。更令人印象深刻的是，在仅有25%标记数据的情况下，CASPER在相同实验条件下，F1分数比基线方法高出近20%。这些结果突显了CASPER在有效和经济高效的智能庞氏骗局检测方面的潜力，为未来的可扩展欺诈检测解决方案铺平了道路。', 'title_zh': 'CASPER：基于对比学习的智能庞氏骗局检测器，包含更多负样本'}
{'arxiv_id': 'arXiv:2507.16838', 'title': 'Segmentation-free Goodness of Pronunciation', 'authors': 'Xinwei Cao, Zijian Fan, Torbjørn Svendsen, Giampiero Salvi', 'link': 'https://arxiv.org/abs/2507.16838', 'abstract': 'Mispronunciation detection and diagnosis (MDD) is a significant part in modern computer aided language learning (CALL) systems. Within MDD, phoneme-level pronunciation assessment is key to helping L2 learners improve their pronunciation. However, most systems are based on a form of goodness of pronunciation (GOP) which requires pre-segmentation of speech into phonetic units. This limits the accuracy of these methods and the possibility to use modern CTC-based acoustic models for their evaluation. In this study, we first propose self-alignment GOP (GOP-SA) that enables the use of CTC-trained ASR models for MDD. Next, we define a more general alignment-free method that takes all possible alignments of the target phoneme into account (GOP-AF). We give a theoretical account of our definition of GOP-AF, an implementation that solves potential numerical issues as well as a proper normalization which makes the method applicable with acoustic models with different peakiness over time. We provide extensive experimental results on the CMU Kids and Speechocean762 datasets comparing the different definitions of our methods, estimating the dependency of GOP-AF on the peakiness of the acoustic models and on the amount of context around the target phoneme. Finally, we compare our methods with recent studies over the Speechocean762 data showing that the feature vectors derived from the proposed method achieve state-of-the-art results on phoneme-level pronunciation assessment.', 'abstract_zh': '语音误读检测与诊断（MDD）是现代计算机辅助语言学习（CALL）系统中的一个重要组成部分。在MDD中，音素级发音评估对于帮助二语学习者提高发音水平至关重要。然而，大多数系统依赖于语音优良度（GOP）的一种形式，这要求将语音预先分割成 Phonetic 单位。这限制了这些方法的准确性，并限制了使用现代 CTC 基础声学模型进行评估的可能性。在这项研究中，我们首先提出了自对齐 GOP（GOP-SA），使 CTC 训练的 ASR 模型能够用于 MDD。接着，我们定义了一种更一般的无需对齐的方法（GOP-AF），该方法考虑了目标音素的所有可能对齐方式。我们对 GOP-AF 的定义进行了理论说明，并提供了解决潜在数值问题的实现方法以及适当的归一化，使该方法适用于具有不同时间峰度的声学模型。我们在 CMU Kids 和 Speechocean762 数据集上进行了广泛的实验，比较了不同方法的定义，并估计了 GOP-AF 对声学模型峰度和目标音素周围上下文量的依赖性。最后，我们将我们的方法与 Speechocean762 数据集上的最近研究进行了比较，结果显示从提出的方法得出的特征向量在音素级发音评估上达到了最先进的结果。', 'title_zh': '无分割语音流畅度评估'}
{'arxiv_id': 'arXiv:2507.16834', 'title': 'Towards Robust Speech Recognition for Jamaican Patois Music Transcription', 'authors': 'Jordan Madden, Matthew Stone, Dimitri Johnson, Daniel Geddez', 'link': 'https://arxiv.org/abs/2507.16834', 'abstract': 'Although Jamaican Patois is a widely spoken language, current speech recognition systems perform poorly on Patois music, producing inaccurate captions that limit accessibility and hinder downstream applications. In this work, we take a data-centric approach to this problem by curating more than 40 hours of manually transcribed Patois music. We use this dataset to fine-tune state-of-the-art automatic speech recognition (ASR) models, and use the results to develop scaling laws for the performance of Whisper models on Jamaican Patois audio. We hope that this work will have a positive impact on the accessibility of Jamaican Patois music and the future of Jamaican Patois language modeling.', 'abstract_zh': '尽管牙买加拼方言广泛使用，但当前的语音识别系统在拼方言音乐上的表现不佳，产生不准确的字幕，限制了其可访问性并妨碍了下游应用。在本工作中，我们采取以数据为中心的方法，收集了超过40小时的手动转录拼方言音乐数据。我们利用该数据集微调最先进的自动语音识别（ASR）模型，并使用结果开发了拼方言音频上Whisper模型性能的标度律。我们希望这项工作能够提高拼方言音乐的可访问性，并推动拼方言语言建模的未来。', 'title_zh': '面向健壮的牙买加帕托莫音乐转录的语音识别研究'}
{'arxiv_id': 'arXiv:2507.16829', 'title': "You Don't Bring Me Flowers: Mitigating Unwanted Recommendations Through Conformal Risk Control", 'authors': 'Giovanni De Toni, Erasmo Purificato, Emilia Gómez, Bruno Lepri, Andrea Passerini, Cristian Consonni', 'link': 'https://arxiv.org/abs/2507.16829', 'abstract': "Recommenders are significantly shaping online information consumption. While effective at personalizing content, these systems increasingly face criticism for propagating irrelevant, unwanted, and even harmful recommendations. Such content degrades user satisfaction and contributes to significant societal issues, including misinformation, radicalization, and erosion of user trust. Although platforms offer mechanisms to mitigate exposure to undesired content, these mechanisms are often insufficiently effective and slow to adapt to users' feedback. This paper introduces an intuitive, model-agnostic, and distribution-free method that uses conformal risk control to provably bound unwanted content in personalized recommendations by leveraging simple binary feedback on items. We also address a limitation of traditional conformal risk control approaches, i.e., the fact that the recommender can provide a smaller set of recommended items, by leveraging implicit feedback on consumed items to expand the recommendation set while ensuring robust risk mitigation. Our experimental evaluation on data coming from a popular online video-sharing platform demonstrates that our approach ensures an effective and controllable reduction of unwanted recommendations with minimal effort. The source code is available here: this https URL.", 'abstract_zh': '推荐系统显著影响着在线信息消费。虽然这些系统在个性化内容方面效果显著，但它们也越来越因传播无关、不需要的甚至是有害的推荐而受到批评。这种内容降低了用户满意度，并导致了包括 misinformation、极端化和用户信任损蚀在内的重大社会问题。尽管平台提供了机制来减轻暴露于不需要内容的影响，但这些机制往往不够有效且适应用户反馈的速度较慢。本文介绍了一种直观的、模型无关的、无需分布假设的方法，通过利用项目上的简单二元反馈来使用一致风险控制来证明地限定个性化推荐中的不需要内容。我们还通过利用已消费项目的隐式反馈来扩展推荐集，以缓解传统一致风险控制方法的一个局限性，即推荐器可以提供更小的推荐项目集，同时确保稳健的风险缓解。我们对来自一个流行的在线视频分享平台的数据的实验评估表明，我们的方法确保了在最少努力的情况下实现有效且可控的不需要推荐减少。源代码可在此处获取：this https URL。', 'title_zh': '你不送我花：通过符合性风险控制消除不想要的推荐'}
{'arxiv_id': 'arXiv:2507.16826', 'title': 'A Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing Retrieval-Augmented Generation in Large Language Models', 'authors': 'Qikai Wei, Huansheng Ning, Chunlong Han, Jianguo Ding', 'link': 'https://arxiv.org/abs/2507.16826', 'abstract': "Retrieval Augmented Generation (RAG) has gradually emerged as a promising paradigm for enhancing the accuracy and factual consistency of content generated by large language models (LLMs). However, existing RAG studies primarily focus on retrieving isolated segments using similarity-based matching methods, while overlooking the intrinsic connections between them. This limitation hampers performance in RAG tasks. To address this, we propose QMKGF, a Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing Retrieval Augmented Generation. First, we design prompt templates and employ general-purpose LLMs to extract entities and relations, thereby generating a knowledge graph (KG) efficiently. Based on the constructed KG, we introduce a multi-path subgraph construction strategy that incorporates one-hop relations, multi-hop relations, and importance-based relations, aiming to improve the semantic relevance between the retrieved documents and the user query. Subsequently, we designed a query-aware attention reward model that scores subgraph triples based on their semantic relevance to the query. Then, we select the highest score subgraph and enrich subgraph with additional triples from other subgraphs that are highly semantically relevant to the query. Finally, the entities, relations, and triples within the updated subgraph are utilised to expand the original query, thereby enhancing its semantic representation and improving the quality of LLMs' generation. We evaluate QMKGF on the SQuAD, IIRC, Culture, HotpotQA, and MuSiQue datasets. On the HotpotQA dataset, our method achieves a ROUGE-1 score of 64.98\\%, surpassing the BGE-Rerank approach by 9.72 percentage points (from 55.26\\% to 64.98\\%). Experimental results demonstrate the effectiveness and superiority of the QMKGF approach.", 'abstract_zh': '基于查询aware多路径知识图融合的增强检索增强生成（QMKGF）方法', 'title_zh': '一种基于查询的多路径知识图融合方法，用于增强大型语言模型中的检索增强生成'}
{'arxiv_id': 'arXiv:2507.16820', 'title': 'Disaster Informatics after the COVID-19 Pandemic: Bibliometric and Topic Analysis based on Large-scale Academic Literature', 'authors': 'Ngan Tran, Haihua Chen, Ana Cleveland, Yuhan Zhou', 'link': 'https://arxiv.org/abs/2507.16820', 'abstract': 'This study presents a comprehensive bibliometric and topic analysis of the disaster informatics literature published between January 2020 to September 2022. Leveraging a large-scale corpus and advanced techniques such as pre-trained language models and generative AI, we identify the most active countries, institutions, authors, collaboration networks, emergent topics, patterns among the most significant topics, and shifts in research priorities spurred by the COVID-19 pandemic. Our findings highlight (1) countries that were most impacted by the COVID-19 pandemic were also among the most active, with each country having specific research interests, (2) countries and institutions within the same region or share a common language tend to collaborate, (3) top active authors tend to form close partnerships with one or two key partners, (4) authors typically specialized in one or two specific topics, while institutions had more diverse interests across several topics, and (5) the COVID-19 pandemic has influenced research priorities in disaster informatics, placing greater emphasis on public health. We further demonstrate that the field is converging on multidimensional resilience strategies and cross-sectoral data-sharing collaborations or projects, reflecting a heightened awareness of global vulnerability and interdependency. Collecting and quality assurance strategies, data analytic practices, LLM-based topic extraction and summarization approaches, and result visualization tools can be applied to comparable datasets or solve similar analytic problems. By mapping out the trends in disaster informatics, our analysis offers strategic insights for policymakers, practitioners, and scholars aiming to enhance disaster informatics capacities in an increasingly uncertain and complex risk landscape.', 'abstract_zh': '本研究呈现了2020年1月至2022年9月发表的灾害信息化文献的全面文献计量和主题分析。利用大规模语料库和预训练语言模型、生成式AI等先进技术，我们识别出最活跃的国家、机构、作者、合作网络、新兴主题、最重要的主题之间的模式以及由新冠肺炎疫情引发的研究重点转变。研究发现包括：（1）受新冠肺炎疫情冲击最严重的国家也是最活跃的国家，每国各有特定的研究兴趣；（2）同一地区或共享语言的国家和机构倾向于合作；（3）最活跃的作者通常与一两个关键伙伴形成紧密的合作关系；（4）作者通常专注于一两个特定主题，而机构则在多个主题上有更广泛的兴趣；（5）新冠肺炎疫情已影响灾害信息化的研究重点，更侧重于公共卫生。此外，我们还展示了该领域正朝着多元化的韧性策略和跨部门数据共享合作或项目方向发展，反映了对全球脆弱性和相互依赖性的更高意识。收集和质量保障策略、数据分析实践、基于大语言模型的主题提取与总结方法，以及结果可视化工具可应用于类似数据集或解决类似分析问题。通过绘制灾害信息化的趋势，我们的分析为政策制定者、实践者和学者提供了战略性的见解，以增强在日益不确定和复杂的风险环境中灾害信息化的能力。', 'title_zh': 'COVID-19疫情后的灾难信息化：基于大规模学术文献的文献计量与主题分析'}
{'arxiv_id': 'arXiv:2507.16540', 'title': 'Explainable Vulnerability Detection in C/C++ Using Edge-Aware Graph Attention Networks', 'authors': 'Radowanul Haque, Aftab Ali, Sally McClean, Naveed Khan', 'link': 'https://arxiv.org/abs/2507.16540', 'abstract': 'Detecting security vulnerabilities in source code remains challenging, particularly due to class imbalance in real-world datasets where vulnerable functions are under-represented. Existing learning-based methods often optimise for recall, leading to high false positive rates and reduced usability in development workflows. Furthermore, many approaches lack explainability, limiting their integration into security workflows. This paper presents ExplainVulD, a graph-based framework for vulnerability detection in C/C++ code. The method constructs Code Property Graphs and represents nodes using dual-channel embeddings that capture both semantic and structural information. These are processed by an edge-aware attention mechanism that incorporates edge-type embeddings to distinguish among program relations. To address class imbalance, the model is trained using class-weighted cross-entropy loss. ExplainVulD achieves a mean accuracy of 88.25 percent and an F1 score of 48.23 percent across 30 independent runs on the ReVeal dataset. These results represent relative improvements of 4.6 percent in accuracy and 16.9 percent in F1 score compared to the ReVeal model, a prior learning-based method. The framework also outperforms static analysis tools, with relative gains of 14.0 to 14.1 percent in accuracy and 132.2 to 201.2 percent in F1 score. Beyond improved detection performance, ExplainVulD produces explainable outputs by identifying the most influential code regions within each function, supporting transparency and trust in security triage.', 'abstract_zh': '基于图的 ExplainVulD：C/C++代码中的漏洞检测', 'title_zh': '使用边缘感知图注意网络的可解释性C/C++漏洞检测'}
{'arxiv_id': 'arXiv:2507.10330', 'title': 'Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach', 'authors': 'Mohammed Bouri, Adnane Saoud', 'link': 'https://arxiv.org/abs/2507.10330', 'abstract': 'Despite advancements in Natural Language Processing (NLP), models remain vulnerable to adversarial attacks, such as synonym substitutions. While prior work has focused on improving robustness for feed-forward and convolutional architectures, the robustness of recurrent networks and modern state space models (SSMs), such as S4, remains understudied. These architectures pose unique challenges due to their sequential processing and complex parameter dynamics. In this paper, we introduce a novel regularization technique based on Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the impact of input perturbations on model outputs. We focus on computing the GBM for three architectures: Long Short-Term Memory (LSTM), State Space models (S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance resilience against word substitution attacks, (2) improve generalization on clean text, and (3) providing the first systematic analysis of SSM (S4) robustness. Extensive experiments across multiple architectures and benchmark datasets demonstrate that our method improves adversarial robustness by up to 8.8% over existing baselines. These results highlight the effectiveness of our approach, outperforming several state-of-the-art methods in adversarial defense. Codes are available at this https URL', 'abstract_zh': '尽管自然语言处理（NLP）取得了进展，模型仍然容易受到对抗攻击的影响，如同义词替换。尽管先前的工作主要集中在提高前馈和卷积架构的鲁棒性，但循环网络和现代状态空间模型（SSMs），如S4的鲁棒性尚待研究。这些架构因其顺序处理和复杂的参数动力学而具有独特的挑战。在本文中，我们提出了一种基于生长界矩阵（GBM）的新型正则化技术，通过减少输入扰动对模型输出的影响来提高NLP模型的鲁棒性。我们专注于计算LSTM、SSMs（S4）和卷积神经网络（CNN）这三种架构的GBM。我们的方法旨在（1）增强对单词替换攻击的抵抗力，（2）在干净文本上提高泛化能力，（3）首次对SSMs（S4）的鲁棒性进行系统分析。我们在多种架构和基准数据集上的广泛实验表明，我们的方法在对抗鲁棒性上比现有基线提高了8.8%。这些结果突显了我们方法的有效性，其在对抗防御方面超越了多种最先进的方法。代码可在以下网址获取。', 'title_zh': '基于增长边界矩阵方法提升NLP模型在单词替换攻击下的鲁棒性和泛化能力'}
