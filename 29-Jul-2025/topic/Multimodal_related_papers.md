# Skin-Machine Interface with Multimodal Contact Motion Classifier 

**Title (ZH)**: 多模态接触运动分类的皮肤-机器接口 

**Authors**: Alberto Confente, Takanori Jin, Taisuke Kobayashi, Julio Rogelio Guadarrama-Olvera, Gordon Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2507.19760)  

**Abstract**: This paper proposes a novel framework for utilizing skin sensors as a new operation interface of complex robots. The skin sensors employed in this study possess the capability to quantify multimodal tactile information at multiple contact points. The time-series data generated from these sensors is anticipated to facilitate the classification of diverse contact motions exhibited by an operator. By mapping the classification results with robot motion primitives, a diverse range of robot motions can be generated by altering the manner in which the skin sensors are interacted with. In this paper, we focus on a learning-based contact motion classifier employing recurrent neural networks. This classifier is a pivotal factor in the success of this framework. Furthermore, we elucidate the requisite conditions for software-hardware designs. Firstly, multimodal sensing and its comprehensive encoding significantly contribute to the enhancement of classification accuracy and learning stability. Utilizing all modalities simultaneously as inputs to the classifier proves to be an effective approach. Secondly, it is essential to mount the skin sensors on a flexible and compliant support to enable the activation of three-axis accelerometers. These accelerometers are capable of measuring horizontal tactile information, thereby enhancing the correlation with other modalities. Furthermore, they serve to absorb the noises generated by the robot's movements during deployment. Through these discoveries, the accuracy of the developed classifier surpassed 95 %, enabling the dual-arm mobile manipulator to execute a diverse range of tasks via the Skin-Machine Interface. this https URL 

**Abstract (ZH)**: 本文提出了一种利用皮肤传感器作为复杂机器人新型操作界面的新型框架。研究中使用的皮肤传感器具备在多个接触点量化多模态触觉信息的能力。这些传感器生成的时间序列数据有望便于识别操作员展现的多种接触运动。通过将分类结果映射到机器人运动基本元素，可以通过改变与皮肤传感器的交互方式生成多种多样的机器人运动。在本文中，我们专注于利用循环神经网络的基于学习的接触运动分类器。该分类器是该框架成功的关键因素之一。此外，我们阐述了软硬件设计的必要条件。首先，多模态感知及其综合编码显著提高了分类准确率和学习稳定性。将所有模态同时作为分类器的输入证明是一种有效的方法。其次，将皮肤传感器安装在柔性且顺应性好的支撑上，可以激活三轴加速度计，这些加速度计能够测量水平触觉信息，从而增强与其他模态的关联性。此外，它们还能够吸收机器人在部署过程中产生的噪音。通过这些发现，开发的分类器准确性超过了95%，使得双臂移动操作器能够通过皮肤-机器界面执行多样化的任务。this https.URL 

---
# Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache Compression 

**Title (ZH)**: 增强大尺寸多模态模型的自适应稀疏性和KV缓存压缩 

**Authors**: Te Zhang, Yuheng Li, Junxiang Wang, Lujun Li  

**Link**: [PDF](https://arxiv.org/pdf/2507.20613)  

**Abstract**: Large multimodal models (LMMs) have advanced significantly by integrating visual encoders with extensive language models, enabling robust reasoning capabilities. However, compressing LMMs for deployment on edge devices remains a critical challenge. In this work, we propose an adaptive search algorithm that optimizes sparsity and KV cache compression to enhance LMM efficiency. Utilizing the Tree-structured Parzen Estimator, our method dynamically adjusts pruning ratios and KV cache quantization bandwidth across different LMM layers, using model performance as the optimization objective. This approach uniquely combines pruning with key-value cache quantization and incorporates a fast pruning technique that eliminates the need for additional fine-tuning or weight adjustments, achieving efficient compression without compromising accuracy. Comprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and 13B, demonstrate our method superiority over state-of-the-art techniques such as SparseGPT and Wanda across various compression levels. Notably, our framework automatic allocation of KV cache compression resources sets a new standard in LMM optimization, delivering memory efficiency without sacrificing much performance. 

**Abstract (ZH)**: 大规模多模态模型（LMMs）通过将视觉编码器与广泛的语言模型相结合，显著提升了推理能力。然而，将LMMs压缩以在边缘设备上部署仍然是一个关键挑战。在本文中，我们提出了一种自适应搜索算法，优化稀疏性和KV缓存压缩，以提高LMM效率。利用树结构Parzen估计算法，我们的方法在不同LMM层中动态调整剪枝比率和KV缓存量化带宽，并以模型性能作为优化目标。该方法独特地结合了剪枝和键值缓存量化，并引入了一种快速剪枝技术，无需额外的微调或权重调整，实现了高效压缩而不牺牲准确性。对包括LLaVA-1.5 7B和13B在内的基准数据集的全面评估表明，我们的方法在各种压缩级别上优于SparseGPT和Wanda等最新技术。值得注意的是，我们的框架在LMM优化中的自动分配KV缓存压缩资源树立了新标准，实现了内存效率而不牺牲太多性能。 

---
# STARN-GAT: A Multi-Modal Spatio-Temporal Graph Attention Network for Accident Severity Prediction 

**Title (ZH)**: STARN-GAT：一种用于事故严重程度预测的多模态时空图注意力网络 

**Authors**: Pritom Ray Nobin, Imran Ahammad Rifat  

**Link**: [PDF](https://arxiv.org/pdf/2507.20451)  

**Abstract**: Accurate prediction of traffic accident severity is critical for improving road safety, optimizing emergency response strategies, and informing the design of safer transportation infrastructure. However, existing approaches often struggle to effectively model the intricate interdependencies among spatial, temporal, and contextual variables that govern accident outcomes. In this study, we introduce STARN-GAT, a Multi-Modal Spatio-Temporal Graph Attention Network, which leverages adaptive graph construction and modality-aware attention mechanisms to capture these complex relationships. Unlike conventional methods, STARN-GAT integrates road network topology, temporal traffic patterns, and environmental context within a unified attention-based framework. The model is evaluated on the Fatality Analysis Reporting System (FARS) dataset, achieving a Macro F1-score of 85 percent, ROC-AUC of 0.91, and recall of 81 percent for severe incidents. To ensure generalizability within the South Asian context, STARN-GAT is further validated on the ARI-BUET traffic accident dataset, where it attains a Macro F1-score of 0.84, recall of 0.78, and ROC-AUC of 0.89. These results demonstrate the model's effectiveness in identifying high-risk cases and its potential for deployment in real-time, safety-critical traffic management systems. Furthermore, the attention-based architecture enhances interpretability, offering insights into contributing factors and supporting trust in AI-assisted decision-making. Overall, STARN-GAT bridges the gap between advanced graph neural network techniques and practical applications in road safety analytics. 

**Abstract (ZH)**: 准确预测交通事故严重程度对于提高道路安全、优化应急响应策略和指导更安全的交通基础设施设计至关重要。然而，现有方法往往难以有效地建模空间、时间及上下文变量之间复杂的相互依赖关系。在本研究中，我们引入了STARN-GAT，一种多模态时空图注意网络，该模型利用自适应图构建和模态感知注意机制来捕捉这些复杂的关系。与传统方法不同，STARN-GAT将道路网络拓扑、时间交通模式和环境上下文集成在一个统一的基于注意力的框架中。该模型在致命性事故报告系统（FARS）数据集上进行评估，实现了宏F1分数85%，ROC-AUC 0.91，严重事件召回率81%。为进一步确保在南亚地区的普适性，STARN-GAT在ARI-BUET交通事故数据集上进一步验证，实现了宏F1分数0.84，召回率0.78，ROC-AUC 0.89。这些结果表明该模型在识别高风险案例方面的有效性及其在实时、关键安全交通管理系统中部署的潜力。此外，基于注意力的架构提高了模型的解释性，提供了影响因素的见解并支持对AI辅助决策的信任。总之，STARN-GAT填补了先进的图神经网络技术和道路安全分析实际应用之间的差距。 

---
# HAMLET-FFD: Hierarchical Adaptive Multi-modal Learning Embeddings Transformation for Face Forgery Detection 

**Title (ZH)**: HAMLET-FFD：分层自适应多模态学习嵌入转换对面部伪造检测的研究 

**Authors**: Jialei Cui, Jianwei Du, Yanzhe Li, Lei Gao, Hui Jiang, Chenfu Bao  

**Link**: [PDF](https://arxiv.org/pdf/2507.20913)  

**Abstract**: The rapid evolution of face manipulation techniques poses a critical challenge for face forgery detection: cross-domain generalization. Conventional methods, which rely on simple classification objectives, often fail to learn domain-invariant representations. We propose HAMLET-FFD, a cognitively inspired Hierarchical Adaptive Multi-modal Learning framework that tackles this challenge via bidirectional cross-modal reasoning. Building on contrastive vision-language models such as CLIP, HAMLET-FFD introduces a knowledge refinement loop that iteratively assesses authenticity by integrating visual evidence with conceptual cues, emulating expert forensic analysis. A key innovation is a bidirectional fusion mechanism in which textual authenticity embeddings guide the aggregation of hierarchical visual features, while modulated visual features refine text embeddings to generate image-adaptive prompts. This closed-loop process progressively aligns visual observations with semantic priors to enhance authenticity assessment. By design, HAMLET-FFD freezes all pretrained parameters, serving as an external plugin that preserves CLIP's original capabilities. Extensive experiments demonstrate its superior generalization to unseen manipulations across multiple benchmarks, and visual analyses reveal a division of labor among embeddings, with distinct representations specializing in fine-grained artifact recognition. 

**Abstract (ZH)**: 基于认知的分级适应多模态学习框架（HAMLET-FFD）：面向前所未见的面部伪造跨域泛化功能提升方法 

---
# A Multimodal Architecture for Endpoint Position Prediction in Team-based Multiplayer Games 

**Title (ZH)**: 基于团队多人游戏的端点位置预测多模态架构 

**Authors**: Jonas Peche, Aliaksei Tsishurou, Alexander Zap, Guenter Wallner  

**Link**: [PDF](https://arxiv.org/pdf/2507.20670)  

**Abstract**: Understanding and predicting player movement in multiplayer games is crucial for achieving use cases such as player-mimicking bot navigation, preemptive bot control, strategy recommendation, and real-time player behavior analytics. However, the complex environments allow for a high degree of navigational freedom, and the interactions and team-play between players require models that make effective use of the available heterogeneous input data. This paper presents a multimodal architecture for predicting future player locations on a dynamic time horizon, using a U-Net-based approach for calculating endpoint location probability heatmaps, conditioned using a multimodal feature encoder. The application of a multi-head attention mechanism for different groups of features allows for communication between agents. In doing so, the architecture makes efficient use of the multimodal game state including image inputs, numerical and categorical features, as well as dynamic game data. Consequently, the presented technique lays the foundation for various downstream tasks that rely on future player positions such as the creation of player-predictive bot behavior or player anomaly detection. 

**Abstract (ZH)**: 理解并预测多人游戏中玩家的运动模式对于实现玩家模拟机器人导航、预判性机器人控制、策略推荐以及实时玩家行为分析等用例至关重要。然而，复杂的环境提供了高度的导航自由度，玩家之间的互动和团队协作需要能够有效利用异质性输入数据的模型。本文提出了一种多模态架构，用于在动态时间范围内预测玩家的未来位置，采用基于U-Net的方法计算端点位置概率热图，并使用多模态特征编码进行条件控制。通过为不同的特征组应用多头注意力机制，使代理之间能够进行通信。因此，该架构能够高效利用包括图像输入、数值和分类特征以及动态游戏数据在内的多模态游戏状态。最终，所提出的技术为基础下游任务奠定了基础，这些任务依赖于未来玩家位置，如玩家预测性机器人行为的创建或玩家异常检测。 

---
# TransPrune: Token Transition Pruning for Efficient Large Vision-Language Model 

**Title (ZH)**: TransPrune: Token过渡剪枝以实现高效的大型视觉-语言模型 

**Authors**: Ao Li, Yuxiang Duan, Jinghui Zhang, Congbo Ma, Yutong Xie, Gustavo Carneiro, Mohammad Yaqub, Hu Wang  

**Link**: [PDF](https://arxiv.org/pdf/2507.20630)  

**Abstract**: Large Vision-Language Models (LVLMs) have advanced multimodal learning but face high computational costs due to the large number of visual tokens, motivating token pruning to improve inference efficiency. The key challenge lies in identifying which tokens are truly important. Most existing approaches rely on attention-based criteria to estimate token importance. However, they inherently suffer from certain limitations, such as positional bias. In this work, we explore a new perspective on token importance based on token transitions in LVLMs. We observe that the transition of token representations provides a meaningful signal of semantic information. Based on this insight, we propose TransPrune, a training-free and efficient token pruning method. Specifically, TransPrune progressively prunes tokens by assessing their importance through a combination of Token Transition Variation (TTV)-which measures changes in both the magnitude and direction of token representations-and Instruction-Guided Attention (IGA), which measures how strongly the instruction attends to image tokens via attention. Extensive experiments demonstrate that TransPrune achieves comparable multimodal performance to original LVLMs, such as LLaVA-v1.5 and LLaVA-Next, across eight benchmarks, while reducing inference TFLOPs by more than half. Moreover, TTV alone can serve as an effective criterion without relying on attention, achieving performance comparable to attention-based methods. The code will be made publicly available upon acceptance of the paper at this https URL. 

**Abstract (ZH)**: 基于令牌转换的Large视觉-语言模型的令牌剪枝 

---
# Cognitive Chain-of-Thought: Structured Multimodal Reasoning about Social Situations 

**Title (ZH)**: 认知链式思考：结构化多模态社会情境推理 

**Authors**: Eunkyu Park, Wesley Hanwen Deng, Gunhee Kim, Motahhare Eslami, Maarten Sap  

**Link**: [PDF](https://arxiv.org/pdf/2507.20409)  

**Abstract**: Chain-of-Thought (CoT) prompting helps models think step by step. But what happens when they must see, understand, and judge-all at once? In visual tasks grounded in social context, where bridging perception with norm-grounded judgments is essential, flat CoT often breaks down. We introduce Cognitive Chain-of-Thought (CoCoT), a prompting strategy that scaffolds VLM reasoning through three cognitively inspired stages: perception, situation, and norm. Our experiments show that, across multiple multimodal benchmarks (including intent disambiguation, commonsense reasoning, and safety), CoCoT consistently outperforms CoT and direct prompting (+8\% on average). Our findings demonstrate that cognitively grounded reasoning stages enhance interpretability and social awareness in VLMs, paving the way for safer and more reliable multimodal systems. 

**Abstract (ZH)**: 认知Chain-of-Thought (CoCoT) 提策助力VLM在多模态任务中的推理能力 

---
# Trust the Model: Compact VLMs as In-Context Judges for Image-Text Data Quality 

**Title (ZH)**: 信任模型：紧凑型VLM作为图像-文本数据质量的上下文法官 

**Authors**: Daulet Toibazar, Kesen Wang, Sherif Mohamed, Abdulaziz Al-Badawi, Abdulrahman Alfulayt, Pedro J. Moreno  

**Link**: [PDF](https://arxiv.org/pdf/2507.20156)  

**Abstract**: Vision-language models (VLMs) extend the conventional large language models by integrating visual data, enabling richer multimodal reasoning and significantly broadens the practical applications of AI. However, including visual inputs also brings new challenges in maintaining data quality. Empirical evidence consistently shows that carefully curated and representative training examples often yield superior results compared to simply increasing the quantity of data. Inspired by this observation, we introduce a streamlined data filtration framework that employs a compact VLM, fine-tuned on a high-quality image-caption annotated dataset. This model effectively evaluates and filters potential training samples based on caption and image quality and alignment. Unlike previous approaches, which typically add auxiliary filtration modules on top of existing full-scale VLMs, our method exclusively utilizes the inherent evaluative capability of a purpose-built small VLM. This strategy eliminates the need for extra modules and reduces training overhead. Our lightweight model efficiently filters out inaccurate, noisy web data, improving image-text alignment and caption linguistic fluency. Experimental results show that datasets underwent high-precision filtration using our compact VLM perform on par with, or even surpass, larger and noisier datasets gathered through high-volume web crawling. Thus, our method provides a lightweight yet robust solution for building high-quality vision-language training corpora. \\ \textbf{Availability and implementation:} Our compact VLM filtration model, training data, utility scripts, and Supplementary data (Appendices) are freely available at this https URL. 

**Abstract (ZH)**: 视觉语言模型的数据过滤框架：一种轻量级且高效的高质量视觉语言训练数据构建方法 

---
# Predicting Brain Responses To Natural Movies With Multimodal LLMs 

**Title (ZH)**: 使用多模态大型语言模型预测大脑对自然电影的反应 

**Authors**: Cesar Kadir Torrico Villanueva, Jiaxin Cindy Tu, Mihir Tripathy, Connor Lane, Rishab Iyer, Paul S. Scotti  

**Link**: [PDF](https://arxiv.org/pdf/2507.19956)  

**Abstract**: We present MedARC's team solution to the Algonauts 2025 challenge. Our pipeline leveraged rich multimodal representations from various state-of-the-art pretrained models across video (V-JEPA2), speech (Whisper), text (Llama 3.2), vision-text (InternVL3), and vision-text-audio (Qwen2.5-Omni). These features extracted from the models were linearly projected to a latent space, temporally aligned to the fMRI time series, and finally mapped to cortical parcels through a lightweight encoder comprising a shared group head plus subject-specific residual heads. We trained hundreds of model variants across hyperparameter settings, validated them on held-out movies and assembled ensembles targeted to each parcel in each subject. Our final submission achieved a mean Pearson's correlation of 0.2085 on the test split of withheld out-of-distribution movies, placing our team in fourth place for the competition. We further discuss a last-minute optimization that would have raised us to second place. Our results highlight how combining features from models trained in different modalities, using a simple architecture consisting of shared-subject and single-subject components, and conducting comprehensive model selection and ensembling improves generalization of encoding models to novel movie stimuli. All code is available on GitHub. 

**Abstract (ZH)**: MedARC团队参加Algonauts 2025挑战赛的解决方案 

---
# DeepJIVE: Learning Joint and Individual Variation Explained from Multimodal Data Using Deep Learning 

**Title (ZH)**: DeepJIVE：使用深度学习学习多模态数据中的联合和个体变异解释 

**Authors**: Matthew Drexler, Benjamin Risk, James J Lah, Suprateek Kundu, Deqiang Qiu  

**Link**: [PDF](https://arxiv.org/pdf/2507.19682)  

**Abstract**: Conventional multimodal data integration methods provide a comprehensive assessment of the shared or unique structure within each individual data type but suffer from several limitations such as the inability to handle high-dimensional data and identify nonlinear structures. In this paper, we introduce DeepJIVE, a deep-learning approach to performing Joint and Individual Variance Explained (JIVE). We perform mathematical derivation and experimental validations using both synthetic and real-world 1D, 2D, and 3D datasets. Different strategies of achieving the identity and orthogonality constraints for DeepJIVE were explored, resulting in three viable loss functions. We found that DeepJIVE can successfully uncover joint and individual variations of multimodal datasets. Our application of DeepJIVE to the Alzheimer's Disease Neuroimaging Initiative (ADNI) also identified biologically plausible covariation patterns between the amyloid positron emission tomography (PET) and magnetic resonance (MR) images. In conclusion, the proposed DeepJIVE can be a useful tool for multimodal data analysis. 

**Abstract (ZH)**: DeepJIVE：一种用于执行联合和个体方差解释的深度学习方法 

---
# Efficient Learning for Product Attributes with Compact Multimodal Models 

**Title (ZH)**: 基于紧凑多模态模型的产品属性高效学习 

**Authors**: Mandar Kulkarni  

**Link**: [PDF](https://arxiv.org/pdf/2507.19679)  

**Abstract**: Image-based product attribute prediction in e-commerce is a crucial task with numerous applications. The supervised fine-tuning of Vision Language Models (VLMs) faces significant scale challenges due to the cost of manual or API based annotation. In this paper, we investigate label-efficient semi-supervised fine-tuning strategies for compact VLMs (2B-3B parameters) that leverage unlabeled product listings through Direct Preference Optimization (DPO). Beginning with a small, API-based, annotated, and labeled set, we first employ PEFT to train low-rank adapter modules. To update the adapter weights with unlabeled data, we generate multiple reasoning-and-answer chains per unlabeled sample and segregate these chains into preferred and dispreferred based on self-consistency. We then fine-tune the model with DPO loss and use the updated model for the next iteration. By using PEFT fine-tuning with DPO, our method achieves efficient convergence with minimal compute overhead. On a dataset spanning twelve e-commerce verticals, DPO-based fine-tuning, which utilizes only unlabeled data, demonstrates a significant improvement over the supervised model. Moreover, experiments demonstrate that accuracy with DPO training improves with more unlabeled data, indicating that a large pool of unlabeled samples can be effectively leveraged to improve performance. 

**Abstract (ZH)**: 基于图像的商品属性预测在电子商务中是一项关键任务，具有广泛的應用。通过直接偏好优化（DPO）利用未标记的产品列表进行紧凑视觉语言模型的半监督高效微调策略研究。 

---
# Rainbow Noise: Stress-Testing Multimodal Harmful-Meme Detectors on LGBTQ Content 

**Title (ZH)**: 彩虹噪音：对 LGBTQ 内容的多模态有害 meme 检测器的压力测试 

**Authors**: Ran Tong, Songtao Wei, Jiaqi Liu, Lanruo Wang  

**Link**: [PDF](https://arxiv.org/pdf/2507.19551)  

**Abstract**: Hateful memes aimed at LGBTQ\,+ communities often evade detection by tweaking either the caption, the image, or both. We build the first robustness benchmark for this setting, pairing four realistic caption attacks with three canonical image corruptions and testing all combinations on the PrideMM dataset. Two state-of-the-art detectors, MemeCLIP and MemeBLIP2, serve as case studies, and we introduce a lightweight \textbf{Text Denoising Adapter (TDA)} to enhance the latter's resilience. Across the grid, MemeCLIP degrades more gently, while MemeBLIP2 is particularly sensitive to the caption edits that disrupt its language processing. However, the addition of the TDA not only remedies this weakness but makes MemeBLIP2 the most robust model overall. Ablations reveal that all systems lean heavily on text, but architectural choices and pre-training data significantly impact robustness. Our benchmark exposes where current multimodal safety models crack and demonstrates that targeted, lightweight modules like the TDA offer a powerful path towards stronger defences. 

**Abstract (ZH)**: 针对LGBTQ+社区的仇恨梗经常通过修改标题、图像或两者来规避检测。我们构建了首个针对此情境的鲁棒性基准，结合四种现实的标题攻击与三种经典的图像 Corruption，并在 PrideMM 数据集上测试所有组合。两种最先进的检测器 MemeCLIP 和 MemeBLIP2 作为案例研究，我们引入了一个轻量级的文本去噪适配器（TDA）以增强后者在鲁棒性方面的表现。在整个基准测试中，MemeCLIP 的性能下滑较为温和，而 MemeBLIP2 对扰乱其语言处理的标题编辑尤为敏感。然而，加入 TDA 不仅弥补了这一不足，还使 MemeBLIP2 成为整体上最鲁棒的模型。消融实验表明，所有系统都高度依赖文本，但架构选择和预训练数据显著影响鲁棒性。我们的基准揭示了当前多模态安全模型的脆弱性，并表明像 TDA 这样的针对性轻量级模块提供了增强防御能力的有力途径。 

---
