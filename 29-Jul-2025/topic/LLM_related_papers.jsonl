{'arxiv_id': 'arXiv:2507.20509', 'title': 'LLMs-guided adaptive compensator: Bringing Adaptivity to Automatic Control Systems with Large Language Models', 'authors': 'Zhongchao Zhou, Yuxi Lu, Yaonan Zhu, Yifan Zhao, Bin He, Liang He, Wenwen Yu, Yusuke Iwasawa', 'link': 'https://arxiv.org/abs/2507.20509', 'abstract': 'With rapid advances in code generation, reasoning, and problem-solving, Large Language Models (LLMs) are increasingly applied in robotics. Most existing work focuses on high-level tasks such as task decomposition. A few studies have explored the use of LLMs in feedback controller design; however, these efforts are restricted to overly simplified systems, fixed-structure gain tuning, and lack real-world validation. To further investigate LLMs in automatic control, this work targets a key subfield: adaptive control. Inspired by the framework of model reference adaptive control (MRAC), we propose an LLM-guided adaptive compensator framework that avoids designing controllers from scratch. Instead, the LLMs are prompted using the discrepancies between an unknown system and a reference system to design a compensator that aligns the response of the unknown system with that of the reference, thereby achieving adaptivity. Experiments evaluate five methods: LLM-guided adaptive compensator, LLM-guided adaptive controller, indirect adaptive control, learning-based adaptive control, and MRAC, on soft and humanoid robots in both simulated and real-world environments. Results show that the LLM-guided adaptive compensator outperforms traditional adaptive controllers and significantly reduces reasoning complexity compared to the LLM-guided adaptive controller. The Lyapunov-based analysis and reasoning-path inspection demonstrate that the LLM-guided adaptive compensator enables a more structured design process by transforming mathematical derivation into a reasoning task, while exhibiting strong generalizability, adaptability, and robustness. This study opens a new direction for applying LLMs in the field of automatic control, offering greater deployability and practicality compared to vision-language models.', 'abstract_zh': '随着代码生成、推理和问题解决的快速进步，大规模语言模型（LLMs）在机器人领域的应用日益增多。大多数现有工作集中在高层任务如任务分解上。少数研究探索了LLMs在反馈控制器设计中的应用；然而，这些努力主要限于过于简化的系统、固定结构增益调整，并缺乏现实世界的验证。为进一步研究LLMs在自动控制中的应用，本文将目标集中在自动控制的关键子领域：自适应控制。借鉴模型参考自适应控制（MRAC）框架，我们提出了一种由LLM指导的自适应补偿器框架，避免了从头设计控制器。相反，LLMs通过未知系统与参考系统的差异被提示，设计一个补偿器来使未知系统的行为与参考系统相匹配，从而实现自适应性。实验在模拟和现实环境中对软体机器人和人形机器人评估了五种方法：由LLM指导的自适应补偿器、由LLM指导的自适应控制器、间接自适应控制、基于学习的自适应控制和MRAC。结果表明，由LLM指导的自适应补偿器优于传统自适应控制器，并且在与由LLM指导的自适应控制器相比时显著降低了推理复杂性。通过Lyapunov分析和推理路径检查，证明了由LLM指导的自适应补偿器能够通过将数学推导转化为推理任务来实现更结构化的设计过程，同时具备强大的泛化能力、自适应性和鲁棒性。本研究为LLMs在自动控制领域的应用开辟了新方向，与视觉-语言模型相比，提供了更大的可部署性和实用性。', 'title_zh': '大型语言模型引导的自适应补偿器：将自适应性引入自动控制系统'}
{'arxiv_id': 'arXiv:2507.20021', 'title': 'When Engineering Outruns Intelligence: A Re-evaluation of Instruction-Guided Navigation', 'authors': 'Matin Aghaei, Mohammad Ali Alomrani, Yingxue Zhang, Mahdi Biparva', 'link': 'https://arxiv.org/abs/2507.20021', 'abstract': 'Large language models (LLMs) are often credited with recent leaps in ObjectGoal Navigation, yet the extent to which they improve planning remains unclear. We revisit this question on the HM3D-v1 validation split. First, we strip InstructNav of its Dynamic Chain-of-Navigation prompt, open-vocabulary GLEE detector and Intuition saliency map, and replace them with a simple Distance-Weighted Frontier Explorer (DWFE). This geometry-only heuristic raises Success from 58.0% to 61.1% and lifts SPL from 20.9% to 36.0% over 2 000 validation episodes, outperforming all previous training-free baselines. Second, we add a lightweight language prior (SHF); on a 200-episode subset this yields a further +2% Success and +0.9% SPL while shortening paths by five steps on average. Qualitative trajectories confirm the trend: InstructNav back-tracks and times-out, DWFE reaches the goal after a few islands, and SHF follows an almost straight route. Our results indicate that frontier geometry, not emergent LLM reasoning, drives most reported gains, and suggest that metric-aware prompts or offline semantic graphs are necessary before attributing navigation success to "LLM intelligence."', 'abstract_zh': '大型语言模型在物体目标导航中的作用及其对规划改善的程度：HM3D-v1验证集上的重新审视', 'title_zh': '当工程超越智能：指令引导导航的重新评估'}
{'arxiv_id': 'arXiv:2507.20520', 'title': 'AQUA: A Large Language Model for Aquaculture & Fisheries', 'authors': 'Praneeth Narisetty, Uday Kumar Reddy Kattamanchi, Lohit Akshant Nimma, Sri Ram Kaushik Karnati, Shiva Nagendra Babu Kore, Mounika Golamari, Tejashree Nageshreddy', 'link': 'https://arxiv.org/abs/2507.20520', 'abstract': 'Aquaculture plays a vital role in global food security and coastal economies by providing sustainable protein sources. As the industry expands to meet rising demand, it faces growing challenges such as disease outbreaks, inefficient feeding practices, rising labor costs, logistical inefficiencies, and critical hatchery issues, including high mortality rates and poor water quality control. Although artificial intelligence has made significant progress, existing machine learning methods fall short of addressing the domain-specific complexities of aquaculture. To bridge this gap, we introduce AQUA, the first large language model (LLM) tailored for aquaculture, designed to support farmers, researchers, and industry practitioners. Central to this effort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic Framework for generating and refining high-quality synthetic data using a combination of expert knowledge, largescale language models, and automated evaluation techniques. Our work lays the foundation for LLM-driven innovations in aquaculture research, advisory systems, and decision-making tools.', 'abstract_zh': '水产养殖在通过提供可持续蛋白质来源保障全球食品 security 和沿海经济体方面发挥着重要作用。随着该行业扩大以满足不断增长的需求，它面临着诸如疾病暴发、喂养效率低下、劳动成本上升、物流效率低下以及关键苗圃问题（包括高死亡率和水质控制差）等诸多挑战。尽管人工智能取得了显著进步，现有的机器学习方法仍无法解决水产养殖领域的特定复杂性。为弥补这一差距，我们提出了AQUA，这是首个针对水产养殖定制的大语言模型（LLM），旨在支持农民、研究人员和行业从业者。该努力的核心是AQUADAPT（数据获取、处理和调整）框架，该框架结合使用专家知识、大规模语言模型和自动化评估技术生成和改进高质量的合成数据。我们的工作为基础研究、咨询系统和决策工具有了基于大语言模型的创新奠定了基础。', 'title_zh': 'AQUA：用于水产养殖与渔业的大型语言模型'}
{'arxiv_id': 'arXiv:2507.21046', 'title': 'A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence', 'authors': 'Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, Hongru Wang, Han Xiao, Yuhang Zhou, Shaokun Zhang, Jiayi Zhang, Jinyu Xiang, Yixiong Fang, Qiwen Zhao, Dongrui Liu, Qihan Ren, Cheng Qian, Zhenghailong Wang, Minda Hu, Huazheng Wang, Qingyun Wu, Heng Ji, Mengdi Wang', 'link': 'https://arxiv.org/abs/2507.21046', 'abstract': 'Large Language Models (LLMs) have demonstrated strong capabilities but remain fundamentally static, unable to adapt their internal parameters to novel tasks, evolving knowledge domains, or dynamic interaction contexts. As LLMs are increasingly deployed in open-ended, interactive environments, this static nature has become a critical bottleneck, necessitating agents that can adaptively reason, act, and evolve in real time. This paradigm shift -- from scaling static models to developing self-evolving agents -- has sparked growing interest in architectures and methods enabling continual learning and adaptation from data, interactions, and experiences. This survey provides the first systematic and comprehensive review of self-evolving agents, organized around three foundational dimensions -- what to evolve, when to evolve, and how to evolve. We examine evolutionary mechanisms across agent components (e.g., models, memory, tools, architecture), categorize adaptation methods by stages (e.g., intra-test-time, inter-test-time), and analyze the algorithmic and architectural designs that guide evolutionary adaptation (e.g., scalar rewards, textual feedback, single-agent and multi-agent systems). Additionally, we analyze evaluation metrics and benchmarks tailored for self-evolving agents, highlight applications in domains such as coding, education, and healthcare, and identify critical challenges and research directions in safety, scalability, and co-evolutionary dynamics. By providing a structured framework for understanding and designing self-evolving agents, this survey establishes a roadmap for advancing adaptive agentic systems in both research and real-world deployments, ultimately shedding lights to pave the way for the realization of Artificial Super Intelligence (ASI), where agents evolve autonomously, performing at or beyond human-level intelligence across a wide array of tasks.', 'abstract_zh': '大型语言模型(Large Language Models)展示了强大的能力，但仍然本质上是静态的，无法适应新的任务、不断发展的知识领域或动态的交互环境。随着大型语言模型在开放式互动环境中越来越广泛的应用，这一静态特性已成为关键瓶颈，需要能够实时适应、推理和进化的代理。这一范式转变——从扩展静态模型转向开发自我进化的代理——激发了对能够通过数据、互动和经验进行持续学习和适应的架构和方法的研究兴趣。本综述首次系统性和全面性地回顾了自我进化的代理，围绕三个基础维度——进化什么、何时进化和如何进化——进行组织。我们探讨了跨越代理组件（如模型、记忆、工具、架构）的进化机制，按照阶段分类适应方法（如测试时内、测试时外），并分析了指导进化适应的算法和架构设计（如标量奖励、文本反馈、单智能体和多智能体系统）。此外，我们分析了为自我进化的代理定制的评估指标和基准，展示了在编码、教育和医疗等领域的应用，并指出了安全、可扩展性和协同进化动力学方面的重要挑战和研究方向。通过提供理解和设计自我进化的代理的结构化框架，本综述为研究和实际应用中自适应代理系统的进步绘制了蓝图，最终为实现人工超级智能（ASI）提供了光线，其中代理能够自主进化，在广泛的任务中达到或超过人类智能水平。', 'title_zh': '自我演化智能体综述：通往人工超级智能的道路'}
{'arxiv_id': 'arXiv:2507.21035', 'title': 'GenoMAS: A Multi-Agent Framework for Scientific Discovery via Code-Driven Gene Expression Analysis', 'authors': 'Haoyang Liu, Yijiang Li, Haohan Wang', 'link': 'https://arxiv.org/abs/2507.21035', 'abstract': 'Gene expression analysis holds the key to many biomedical discoveries, yet extracting insights from raw transcriptomic data remains formidable due to the complexity of multiple large, semi-structured files and the need for extensive domain expertise. Current automation approaches are often limited by either inflexible workflows that break down in edge cases or by fully autonomous agents that lack the necessary precision for rigorous scientific inquiry. GenoMAS charts a different course by presenting a team of LLM-based scientists that integrates the reliability of structured workflows with the adaptability of autonomous agents. GenoMAS orchestrates six specialized LLM agents through typed message-passing protocols, each contributing complementary strengths to a shared analytic canvas. At the heart of GenoMAS lies a guided-planning framework: programming agents unfold high-level task guidelines into Action Units and, at each juncture, elect to advance, revise, bypass, or backtrack, thereby maintaining logical coherence while bending gracefully to the idiosyncrasies of genomic data.\nOn the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation of 89.13% for data preprocessing and an F$_1$ of 60.48% for gene identification, surpassing the best prior art by 10.61% and 16.85% respectively. Beyond metrics, GenoMAS surfaces biologically plausible gene-phenotype associations corroborated by the literature, all while adjusting for latent confounders. Code is available at this https URL.', 'abstract_zh': '基因表达分析是许多生物医学发现的关键，但由于原始转录组数据的复杂性和需要广泛的领域专业知识，从这些数据中提取洞察仍颇具挑战。当前的自动化方法往往要么受限于在边缘情况下会失效的僵化工作流程，要么受限于缺乏足够精确性以进行严格的科学探究的完全自主代理。GenoMAS通过呈现一组基于LLM的科学家团队，结合了结构化工作流程的可靠性与自主代理的适应性，开辟了一条不同的道路。GenoMAS通过类型化消息传递协议协调六种专门的LLM代理，每种代理在共享的分析画布上贡献互补的优势。GenoMAS的核心在于指导性规划框架：编程代理将高层任务指南分解为Action Units，并在每一步选择前进、修订、绕过或回溯，从而保持逻辑连贯性，同时顺应基因组数据的复杂性。\n\n在GenoTEX基准测试中，GenoMAS在数据预处理方面的综合相似性相关性达到89.13%，在基因识别方面的F$_1$值达到60.48%，分别超越了现有最佳方法10.61%和16.85%。除了指标外，GenoMAS还揭示了由文献支持的生物合理的基因-表型关联，并调整了潜在混杂因素。代码可在以下链接获取：this https URL。', 'title_zh': 'GenoMAS: 一种基于代码驱动基因表达分析的多智能体框架用于科学发现'}
{'arxiv_id': 'arXiv:2507.21017', 'title': 'MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them', 'authors': 'Weichen Zhang, Yiyou Sun, Pohao Huang, Jiayue Pu, Heyue Lin, Dawn Song', 'link': 'https://arxiv.org/abs/2507.21017', 'abstract': 'Hallucinations pose critical risks for large language model (LLM)-based agents, often manifesting as hallucinative actions resulting from fabricated or misinterpreted information within the cognitive context. While recent studies have exposed such failures, existing evaluations remain fragmented and lack a principled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions in Risky AGEnt settings--the first unified benchmark for eliciting and evaluating hallucinations in interactive LLM-agent scenarios. We begin by introducing a three-part taxonomy to address agentic hallucinations: actions that are unfaithful to (i) task instructions, (ii) execution history, or (iii) environment observations. To analyze, we first elicit such failures by performing a systematic audit of existing agent benchmarks, then synthesize test cases using a snapshot strategy that isolates decision points in deterministic and reproducible manners. To evaluate hallucination behaviors, we adopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware prompts, enabling scalable, high-fidelity assessment of agent actions without enumerating full action spaces. MIRAGE-Bench provides actionable insights on failure modes of LLM agents and lays the groundwork for principled progress in mitigating hallucinations in interactive environments.', 'abstract_zh': 'MIRAGE-Bench：测量风险博弈中幻觉的基准', 'title_zh': 'MIRAGE-Bench: LLM代理在胡言乱语以及如何发现它们'}
{'arxiv_id': 'arXiv:2507.20964', 'title': 'Core Safety Values for Provably Corrigible Agents', 'authors': 'Aran Nayebi', 'link': 'https://arxiv.org/abs/2507.20964', 'abstract': "We introduce the first implementable framework for corrigibility, with provable guarantees in multi-step, partially observed environments. Our framework replaces a single opaque reward with five *structurally separate* utility heads -- deference, switch-access preservation, truthfulness, low-impact behavior via a belief-based extension of Attainable Utility Preservation, and bounded task reward -- combined lexicographically by strict weight gaps. Theorem 1 proves exact single-round corrigibility in the partially observable off-switch game; Theorem 3 extends the guarantee to multi-step, self-spawning agents, showing that even if each head is \\emph{learned} to mean-squared error $\\varepsilon$ and the planner is $\\varepsilon$-sub-optimal, the probability of violating \\emph{any} safety property is bounded while still ensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF, which merge all norms into one learned scalar, our separation makes obedience and impact-limits dominate even when incentives conflict. For open-ended settings where adversaries can modify the agent, we prove that deciding whether an arbitrary post-hack agent will ever violate corrigibility is undecidable by reduction to the halting problem, then carve out a finite-horizon ``decidable island'' where safety can be certified in randomized polynomial time and verified with privacy-preserving, constant-round zero-knowledge proofs. Consequently, the remaining challenge is the ordinary ML task of data coverage and generalization: reward-hacking risk is pushed into evaluation quality rather than hidden incentive leak-through, giving clearer implementation guidance for today's LLM assistants and future autonomous systems.", 'abstract_zh': '我们介绍了第一个可实施的矫正性框架，在多步部分可观测环境中提供可证明的保证。该框架用五个结构上分离的效用头——遵从性、开关访问保存、真实性、基于信念扩展的可实现效用保存的低影响行为，以及有界任务奖励——取代单一的不透明奖励，并通过严格的权重差距进行析列组合。定理1证明了在部分可观测的关机游戏中单轮矫正性的可证明性；定理3将保证扩展到多步、自我生成的代理，即使每个头在均方误差ε下被学习，规划器ε次最优，违反任何安全属性的概率仍被限制，同时仍然确保净人类收益。与宪法型AI或RLHF/RLAIF将所有规范合并为一个学习标量不同，我们的分离使得即使在激励冲突时，遵从性和影响限制仍然占主导地位。对于对手可以修改代理的开放环境中，我们通过归约到停机问题证明了决定任意被攻击后代理是否会违反矫正性问题是不可判定的，然后划出了一个有限期的“可判定岛屿”，在随机多项式时间内进行安全性验证，并使用隐私保护的、常数轮次零知识证明进行验证。因此，剩下的挑战是常规的机器学习任务——数据覆盖和泛化：奖励作弊风险被推入评估质量而非隐藏激励的泄露，为当今的LLM助手和未来的自主系统提供了更清晰的实施指导。', 'title_zh': '可证明可纠正的智能体的核心安全价值观'}
{'arxiv_id': 'arXiv:2507.20774', 'title': 'evalSmarT: An LLM-Based Framework for Evaluating Smart Contract Generated Comments', 'authors': 'Fatou Ndiaye Mbodji', 'link': 'https://arxiv.org/abs/2507.20774', 'abstract': 'Smart contract comment generation has gained traction as a means to improve code comprehension and maintainability in blockchain systems. However, evaluating the quality of generated comments remains a challenge. Traditional metrics such as BLEU and ROUGE fail to capture domain-specific nuances, while human evaluation is costly and unscalable. In this paper, we present \\texttt{evalSmarT}, a modular and extensible framework that leverages large language models (LLMs) as evaluators. The system supports over 400 evaluator configurations by combining approximately 40 LLMs with 10 prompting strategies. We demonstrate its application in benchmarking comment generation tools and selecting the most informative outputs. Our results show that prompt design significantly impacts alignment with human judgment, and that LLM-based evaluation offers a scalable and semantically rich alternative to existing methods.', 'abstract_zh': '智能合约注释生成在提高区块链系统代码理解和维护性方面获得了关注。然而，评估生成注释的质量仍然是一个挑战。传统的评估指标如BLEU和ROUGE未能捕捉到领域特定的细微差异，而人力评估则成本高昂且难以扩展。本文提出了一种模块化和可扩展的框架\\texttt{evalSmarT}，该框架利用大规模语言模型（LLMs）作为评估工具。该系统通过结合大约40个LLM和10种启 tắm策略，支持超过400种评估配置。我们展示了该框架在评估注释生成工具和选择最具信息量的输出结果方面的应用。实验结果表明，启 tắm设计对与人类判断的一致性有显著影响，并且基于LLM的评估提供了比现有方法更具扩展性和语义丰富性的替代方案。', 'title_zh': 'evalSmarT: 一种基于大语言模型的智能合约生成注释评估框架'}
{'arxiv_id': 'arXiv:2507.20541', 'title': 'MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic Design', 'authors': 'Zishang Qiu, Xinan Chen, Long Chen, Ruibin Bai', 'link': 'https://arxiv.org/abs/2507.20541', 'abstract': 'This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that presents a new paradigm for Automatic Heuristic Design (AHD). Traditional evolutionary methods operate directly on heuristic code; in contrast, MeLA evolves the instructional prompts used to guide a Large Language Model (LLM) in generating these heuristics. This process of "prompt evolution" is driven by a novel metacognitive framework where the system analyzes performance feedback to systematically refine its generative strategy. MeLA\'s architecture integrates a problem analyzer to construct an initial strategic prompt, an error diagnosis system to repair faulty code, and a metacognitive search engine that iteratively optimizes the prompt based on heuristic effectiveness. In comprehensive experiments across both benchmark and real-world problems, MeLA consistently generates more effective and robust heuristics, significantly outperforming state-of-the-art methods. Ultimately, this research demonstrates the profound potential of using cognitive science as a blueprint for AI architecture, revealing that by enabling an LLM to metacognitively regulate its problem-solving process, we unlock a more robust and interpretable path to AHD.', 'abstract_zh': 'MeLA：元认知大模型驱动的自动启发式设计架构', 'title_zh': '元LMA：一种元认知大规模语言模型驱动的自动启发式设计架构'}
{'arxiv_id': 'arXiv:2507.20526', 'title': 'Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition', 'authors': 'Andy Zou, Maxwell Lin, Eliot Jones, Micha Nowak, Mateusz Dziemian, Nick Winter, Alexander Grattan, Valent Nathanael, Ayla Croft, Xander Davies, Jai Patel, Robert Kirk, Nate Burnikell, Yarin Gal, Dan Hendrycks, J. Zico Kolter, Matt Fredrikson', 'link': 'https://arxiv.org/abs/2507.20526', 'abstract': "Recent advances have enabled LLM-powered AI agents to autonomously execute complex tasks by combining language model reasoning with tools, memory, and web access. But can these systems be trusted to follow deployment policies in realistic environments, especially under attack? To investigate, we ran the largest public red-teaming competition to date, targeting 22 frontier AI agents across 44 realistic deployment scenarios. Participants submitted 1.8 million prompt-injection attacks, with over 60,000 successfully eliciting policy violations such as unauthorized data access, illicit financial actions, and regulatory noncompliance. We use these results to build the Agent Red Teaming (ART) benchmark - a curated set of high-impact attacks - and evaluate it across 19 state-of-the-art models. Nearly all agents exhibit policy violations for most behaviors within 10-100 queries, with high attack transferability across models and tasks. Importantly, we find limited correlation between agent robustness and model size, capability, or inference-time compute, suggesting that additional defenses are needed against adversarial misuse. Our findings highlight critical and persistent vulnerabilities in today's AI agents. By releasing the ART benchmark and accompanying evaluation framework, we aim to support more rigorous security assessment and drive progress toward safer agent deployment.", 'abstract_zh': '近期进展使由大规模语言模型驱动的AI代理能够通过结合语言模型推理、工具、内存和网络访问来自主执行复杂任务。但这些系统在实际环境中能否被信任遵守部署政策，特别是在遭受攻击的情况下？为进行研究，我们举办了迄今为止最大规模的公开红队竞赛，针对44个现实部署场景下的22个前沿AI代理进行攻击。参与者提交了180万次提示注入攻击，其中超过6万次成功引发了未授权数据访问、非法财务操作和合规性违规等政策违规。我们使用这些结果构建了代理红队基准（ART基准）——一个高影响攻击的精选集——并跨19个最先进的模型对其进行评估。几乎所有代理在10-100次查询内对大多数行为均表现出政策违规，且攻击在模型和任务之间具有高转移性。重要的是，我们发现代理的鲁棒性与模型规模、能力或推理时计算量之间存在有限的相关性，表明需要针对对抗性滥用采取额外防御措施。我们的研究结果突显了当前AI代理中存在的关键且持续的漏洞。通过发布ART基准和配套的评估框架，我们旨在支持更严谨的安全评估，并推动更安全代理部署的进步。', 'title_zh': 'AI代理部署中的安全挑战：大规模公共竞赛的见解'}
{'arxiv_id': 'arXiv:2507.20395', 'title': 'MazeEval: A Benchmark for Testing Sequential Decision-Making in Language Models', 'authors': 'Hafsteinn Einarsson', 'link': 'https://arxiv.org/abs/2507.20395', 'abstract': "As Large Language Models (LLMs) increasingly power autonomous agents in robotics and embodied AI, understanding their spatial reasoning capabilities becomes crucial for ensuring reliable real-world deployment. Despite advances in language understanding, current research lacks evaluation of how LLMs perform spatial navigation without visual cues, a fundamental requirement for agents operating with limited sensory information. This paper addresses this gap by introducing MazeEval, a benchmark designed to isolate and evaluate pure spatial reasoning in LLMs through coordinate-based maze navigation tasks. Our methodology employs a function-calling interface where models navigate mazes of varying complexity ($5\\times 5$ to $15\\times 15$ grids) using only coordinate feedback and distance-to-wall information, excluding visual input to test fundamental spatial cognition. We evaluate eight state-of-the-art LLMs across identical mazes in both English and Icelandic to assess cross-linguistic transfer of spatial abilities. Our findings reveal striking disparities: while OpenAI's O3 achieves perfect navigation for mazes up to size $30\\times 30$, other models exhibit catastrophic failure beyond $9\\times 9$ mazes, with 100% of failures attributed to excessive looping behavior where models revisit a cell at least 10 times. We document a significant performance degradation in Icelandic, with models solving mazes 3-4 sizes smaller than in English, suggesting spatial reasoning in LLMs emerges from linguistic patterns rather than language-agnostic mechanisms. These results have important implications for global deployment of LLM-powered autonomous systems, showing spatial intelligence remains fundamentally constrained by training data availability and highlighting the need for architectural innovations to achieve reliable navigation across linguistic contexts.", 'abstract_zh': '大型语言模型（LLMs）在机器人学和具身AI中日益成为自主代理的动力，理解其空间推理能力对于确保其实用世界的可靠部署变得至关重要。尽管在语言理解方面取得了进步，当前的研究缺乏对LLMs在没有视觉提示的情况下进行空间导航性能的评估，这是对于具有有限感官信息的代理的基本要求。本文通过引入MazeEval基准，旨在通过基于坐标的迷宫导航任务来孤立和评估LLMs的纯空间推理能力，从而弥补这一空白。我们的方法采用了一个函数调用接口，模型仅通过坐标反馈和距离墙信息导航复杂程度不同的迷宫（5×5到15×15网格），不包括视觉输入以测试基础的空间认知能力。我们在相同的迷宫中用英语和冰岛语评估了八种最先进的LLMs，以评估其空间能力的跨语言迁移能力。我们的研究发现揭示了显著的差异：虽然OpenAI的O3在大小不超过30×30的迷宫中实现了完美的导航，但其他模型在9×9以上的迷宫中表现出灾难性的失败，其中100%的失败归因于过度循环行为，模型至少返回一个单元格10次以上。我们记录了在冰岛语中显著的性能下降，模型解决迷宫的规模比在英语中小3-4个等级，这表明LLMs的空间推理能力源自语言模式而非语言无关的机制。这些结果对LLM驱动的自主系统的全球部署具有重要的影响，表明空间智能仍然受到可用训练数据的极大限制，并强调了需要架构创新以实现跨语言上下文可靠导航的必要性。', 'title_zh': 'MazeEval：语言模型顺序决策能力测试基准'}
{'arxiv_id': 'arXiv:2507.20333', 'title': 'The Blessing and Curse of Dimensionality in Safety Alignment', 'authors': 'Rachel S.Y. Teo, Laziz U. Abdullaev, Tan M. Nguyen', 'link': 'https://arxiv.org/abs/2507.20333', 'abstract': "The focus on safety alignment in large language models (LLMs) has increased significantly due to their widespread adoption across different domains. The scale of LLMs play a contributing role in their success, and the growth in parameter count follows larger hidden dimensions. In this paper, we hypothesize that while the increase in dimensions has been a key advantage, it may lead to emergent problems as well. These problems emerge as the linear structures in the activation space can be exploited, in the form of activation engineering, to circumvent its safety alignment. Through detailed visualizations of linear subspaces associated with different concepts, such as safety, across various model scales, we show that the curse of high-dimensional representations uniquely impacts LLMs. Further substantiating our claim, we demonstrate that projecting the representations of the model onto a lower dimensional subspace can preserve sufficient information for alignment while avoiding those linear structures. Empirical results confirm that such dimensional reduction significantly reduces susceptibility to jailbreaking through representation engineering. Building on our empirical validations, we provide theoretical insights into these linear jailbreaking methods relative to a model's hidden dimensions. Broadly speaking, our work posits that the high dimensions of a model's internal representations can be both a blessing and a curse in safety alignment.", 'abstract_zh': '大型语言模型中安全性对齐的安全关注在广泛采用不同领域后显著增加。随着模型规模的扩大，参数数量的增加也伴随着更大隐藏维度的增长。在本文中，我们假设虽然增加维度是一个关键优势，但也可能导致新出现的问题。这些问题源于激活空间中的线性结构可以通过激活工程被利用以绕过安全性对齐。通过详细可视化不同概念（如安全性）在各种模型规模下的线性子空间，我们展示了高维表示带来的诅咒对大型语言模型的独特影响。进一步支持我们的观点，我们证明将模型的表示投影到低维子空间可以在保留足够信息以进行对齐的同时避免这些线性结构。实验证据证实，这种维度的减少可以显著降低通过表示工程被破解的风险。基于我们的实验证明，我们提供了一种理论见解，适用于模型隐藏维度的这些线性破解方法。总体而言，我们的工作提出，模型内部表示的高维数在安全性对齐中既是恩赐也是诅咒。', 'title_zh': '高维性之福与祸在安全对齐中的体现'}
{'arxiv_id': 'arXiv:2507.20322', 'title': 'Artificial Intelligence In Patent And Market Intelligence: A New Paradigm For Technology Scouting', 'authors': 'Manish Verma, Vivek Sharma, Vishal Singh', 'link': 'https://arxiv.org/abs/2507.20322', 'abstract': 'This paper presents the development of an AI powered software platform that leverages advanced large language models (LLMs) to transform technology scouting and solution discovery in industrial R&D. Traditional approaches to solving complex research and development challenges are often time consuming, manually driven, and heavily dependent on domain specific expertise. These methods typically involve navigating fragmented sources such as patent repositories, commercial product catalogs, and competitor data, leading to inefficiencies and incomplete insights. The proposed platform utilizes cutting edge LLM capabilities including semantic understanding, contextual reasoning, and cross-domain knowledge extraction to interpret problem statements and retrieve high-quality, sustainable solutions. The system processes unstructured patent texts, such as claims and technical descriptions, and systematically extracts potential innovations aligned with the given problem context. These solutions are then algorithmically organized under standardized technical categories and subcategories to ensure clarity and relevance across interdisciplinary domains. In addition to patent analysis, the platform integrates commercial intelligence by identifying validated market solutions and active organizations addressing similar challenges. This combined insight sourced from both intellectual property and real world product data enables R&D teams to assess not only technical novelty but also feasibility, scalability, and sustainability. The result is a comprehensive, AI driven scouting engine that reduces manual effort, accelerates innovation cycles, and enhances decision making in complex R&D environments.', 'abstract_zh': '基于先进大语言模型的AI驱动软件平台在工业研发中的技术 scouting与解决方案发现开发', 'title_zh': '人工智能在专利和市场情报中的应用：技术 scouting的新范式'}
{'arxiv_id': 'arXiv:2507.20280', 'title': 'SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration', 'authors': 'Keyan Ding, Jing Yu, Junjie Huang, Yuchen Yang, Qiang Zhang, Huajun Chen', 'link': 'https://arxiv.org/abs/2507.20280', 'abstract': "Scientific research increasingly relies on specialized computational tools, yet effectively utilizing these tools demands substantial domain expertise. While Large Language Models (LLMs) show promise in tool automation, they struggle to seamlessly integrate and orchestrate multiple tools for complex scientific workflows. Here, we present SciToolAgent, an LLM-powered agent that automates hundreds of scientific tools across biology, chemistry, and materials science. At its core, SciToolAgent leverages a scientific tool knowledge graph that enables intelligent tool selection and execution through graph-based retrieval-augmented generation. The agent also incorporates a comprehensive safety-checking module to ensure responsible and ethical tool usage. Extensive evaluations on a curated benchmark demonstrate that SciToolAgent significantly outperforms existing approaches. Case studies in protein engineering, chemical reactivity prediction, chemical synthesis, and metal-organic framework screening further demonstrate SciToolAgent's capability to automate complex scientific workflows, making advanced research tools accessible to both experts and non-experts.", 'abstract_zh': '基于大规模语言模型的科学工具代理 SciToolAgent：自动化多领域科学工作流', 'title_zh': 'SciToolAgent：一个知识图谱驱动的多工具集成科学代理'}
{'arxiv_id': 'arXiv:2507.20150', 'title': 'The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models', 'authors': 'Xingcheng Xu', 'link': 'https://arxiv.org/abs/2507.20150', 'abstract': 'Reinforcement learning (RL) plays a crucial role in shaping the behavior of large language and reasoning models (LLMs/LRMs). However, it often produces brittle and unstable policies, leading to critical failures such as spurious reasoning, deceptive alignment, and instruction disobedience that undermine the trustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified theoretical explanation and are typically addressed using ad-hoc heuristics. This paper presents a rigorous mathematical framework for analyzing the stability of the mapping from a reward function to the optimal policy. We show that policy brittleness often stems from non-unique optimal actions, a common occurrence when multiple valid traces exist in a reasoning task. This theoretical lens provides a unified explanation for a range of seemingly disparate failures, reframing them as rational outcomes of optimizing rewards that may be incomplete or noisy, especially in the presence of action degeneracy. We extend this analysis from the fundamental single-reward setting to the more realistic multi-reward RL across diverse domains, showing how stability is governed by an "effective reward" aggregation mechanism. We also prove that entropy regularization restores policy stability at the cost of increased stochasticity. Our framework provides a unified explanation for recent empirical findings on deceptive reasoning, instruction-following trade-offs, and RLHF-induced sophistry, and is further validated through perturbation experiments in multi-reward RL. This work advances policy-stability analysis from empirical heuristics towards a principled theory, offering essential insights for designing safer and more trustworthy AI systems.', 'abstract_zh': '强化学习（RL）在塑造大规模语言和推理模型（LLMs/LRMs）的行为中发挥着关键作用。然而，它通常会产生脆弱和不稳定的策略，导致诸如虚假推理、欺骗性对齐和指令不遵从等关键故障，这些故障会损害LLMs/LRMs的信任度和安全性。目前，这些问题缺乏统一的理论解释，通常仅通过非正式的启发式方法进行处理。本文提出了一种严谨的数学框架，用于分析奖励函数到最优策略映射的稳定性。我们表明，策略的脆弱性通常源于非唯一的最优行为，这是当推理任务中存在多个有效路径时的常见现象。这一理论视角为各种看似不同的故障提供了统一的解释，将其重新框定为奖励优化的合理结果，尤其是当存在行为退化时可能不完整或嘈杂。我们将这种分析从基础的单一奖励设置扩展到更现实的多奖励RL在不同领域的应用，展示了稳定性由“有效的奖励”聚合机制所支配。我们还证明了熵正则化可以以增加随机性为代价恢复策略的稳定性。本文框架为近期关于欺骗性推理、指令遵循权衡以及RLHF引起的狡辩的实证发现提供了统一的解释，并通过多奖励RL的扰动实验进一步得到了验证。这项工作将政策稳定性分析从经验性的启发式方法推进到了原理性的理论，为设计更安全和可信赖的AI系统提供了重要的见解。', 'title_zh': '奖励政策悬崖：大型语言模型中奖励-政策映射的理论分析'}
{'arxiv_id': 'arXiv:2507.20067', 'title': 'PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training', 'authors': 'Sarat Chandra Bobbili, Ujwal Dinesha, Dheeraj Narasimha, Srinivas Shakkottai', 'link': 'https://arxiv.org/abs/2507.20067', 'abstract': "Inference-time alignment enables large language models (LLMs) to generate outputs aligned with end-user preferences without further training. Recent post-training methods achieve this by using small guidance models to modify token generation during inference. These methods typically optimize a reward function KL-regularized by the original LLM taken as the reference policy. A critical limitation, however, is their dependence on a pre-trained reward model, which requires fitting to human preference feedback--a potentially unstable process. In contrast, we introduce PITA, a novel framework that integrates preference feedback directly into the LLM's token generation, eliminating the need for a reward model. PITA learns a small preference-based guidance policy to modify token probabilities at inference time without LLM fine-tuning, reducing computational cost and bypassing the pre-trained reward model dependency. The problem is framed as identifying an underlying preference distribution, solved through stochastic search and iterative refinement of the preference-based guidance model. We evaluate PITA across diverse tasks, including mathematical reasoning and sentiment classification, demonstrating its effectiveness in aligning LLM outputs with user preferences.", 'abstract_zh': '推理时的对齐使大规模语言模型（LLMs）能够生成与最终用户偏好匹配的输出而无需进一步训练。最近的后训练方法通过使用小型指导模型在推理时修改标记生成来实现这一点。这些方法通常通过以原LLM作为参考策略来正则化奖励函数来优化一个奖励函数。然而，一个关键的局限性在于它们依赖于一个预训练的奖励模型，这需要拟合人类偏好反馈——这是一个可能不稳定的过程。相比之下，我们引入了PITA，这是一种新的框架，将偏好反馈直接整合到LLM的标记生成中，从而消除对奖励模型的依赖。PITA学习一个小型基于偏好的指导策略，在不进行LLM微调的情况下在推理时修改标记概率，减少计算成本并绕过了预训练的奖励模型依赖。该问题被表述为识别潜在的偏好分布，并通过随机搜索和基于偏好的指导模型的迭代改进来解决。我们在数学推理和情感分类等多种任务中评估PITA，证明了其在使LLM输出与用户偏好对齐方面的有效性。', 'title_zh': 'PITA：偏好引导的推理时对齐方法用于LLM后训练'}
{'arxiv_id': 'arXiv:2507.20000', 'title': 'Matching Game Preferences Through Dialogical Large Language Models: A Perspective', 'authors': 'Renaud Fabre, Daniel Egret, Patrice Bellot', 'link': 'https://arxiv.org/abs/2507.20000', 'abstract': 'This perspective paper explores the future potential of "conversational intelligence" by examining how Large Language Models (LLMs) could be combined with GRAPHYP\'s network system to better understand human conversations and preferences. Using recent research and case studies, we propose a conceptual framework that could make AI rea-soning transparent and traceable, allowing humans to see and understand how AI reaches its conclusions. We present the conceptual perspective of "Matching Game Preferences through Dialogical Large Language Models (D-LLMs)," a proposed system that would allow multiple users to share their different preferences through structured conversations. This approach envisions personalizing LLMs by embedding individual user preferences directly into how the model makes decisions. The proposed D-LLM framework would require three main components: (1) reasoning processes that could analyze different search experiences and guide performance, (2) classification systems that would identify user preference patterns, and (3) dialogue approaches that could help humans resolve conflicting information. This perspective framework aims to create an interpretable AI system where users could examine, understand, and combine the different human preferences that influence AI responses, detected through GRAPHYP\'s search experience networks. The goal of this perspective is to envision AI systems that would not only provide answers but also show users how those answers were reached, making artificial intelligence more transparent and trustworthy for human decision-making.', 'abstract_zh': '这一观点论文探讨了“对话智能”的未来潜力，通过研究大规模语言模型（LLMs）如何与GRAPHYP的网络系统结合，以更好地理解人类对话和偏好。利用 Recent 研究和案例研究，我们提出了一种概念框架，使人工智能推理变得透明可追踪，让人类能够观察和理解 AI 如何得出结论。我们提出了“通过对话式大规模语言模型（D-LLMs）匹配游戏偏好”的概念框架，该系统允许多个用户通过结构化的对话共享他们的不同偏好。这种方法设想通过将个人用户偏好直接嵌入模型决策过程来个性化 LLMS。提出的 D-LLM 框架需要三个主要组成部分：（1）推理过程，能够分析不同的搜索体验并指导性能，（2）分类系统，能够识别用户偏好模式，以及（3）对话方法，能够帮助人类解决矛盾信息。这一观点框架旨在创建一个可解释的 AI 系统，用户可以检查、理解和结合影响 AI 反应的不同人类偏好，这些偏好通过 GRAPHYP 的搜索体验网络被检测到。本视角的目的是设想不仅提供答案，还能向用户展示如何得出这些答案的人工智能系统，使人工智能更加透明和值得信赖，以支持人类决策。', 'title_zh': '通过对话式大型语言模型匹配游戏偏好：一种视角'}
{'arxiv_id': 'arXiv:2507.19973', 'title': 'Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization', 'authors': 'Ebrahim Rasromani, Stella K. Kang, Yanqi Xu, Beisong Liu, Garvit Luhadia, Wan Fung Chui, Felicia L. Pasadyn, Yu Chih Hung, Julie Y. An, Edwin Mathieu, Zehui Gu, Carlos Fernandez-Granda, Ammar A. Javed, Greg D. Sacks, Tamas Gonda, Chenchan Huang, Yiqiu Shen', 'link': 'https://arxiv.org/abs/2507.19973', 'abstract': "Background: Manual extraction of pancreatic cystic lesion (PCL) features from radiology reports is labor-intensive, limiting large-scale studies needed to advance PCL research. Purpose: To develop and evaluate large language models (LLMs) that automatically extract PCL features from MRI/CT reports and assign risk categories based on guidelines. Materials and Methods: We curated a training dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134 patients that described PCLs. Labels were generated by GPT-4o using chain-of-thought (CoT) prompting to extract PCL and main pancreatic duct features. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated CoT data. Features were mapped to risk categories per institutional guideline based on the 2017 ACR White Paper. Evaluation was performed on 285 held-out human-annotated reports. Model outputs for 100 cases were independently reviewed by three radiologists. Feature extraction was evaluated using exact match accuracy, risk categorization with macro-averaged F1 score, and radiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning improved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79% to 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved (LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no statistically significant differences. Radiologist inter-reader agreement was high (Fleiss' Kappa = 0.888) and showed no statistically significant difference with the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT (Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels on par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT supervision enable accurate, interpretable, and efficient phenotyping for large-scale PCL research, achieving performance comparable to GPT-4o.", 'abstract_zh': "背景：手动从影像报告中提取胰腺囊性病变（PCL）特征耗时且 labor-intensive，限制了大规模研究的进行，从而阻碍了PCL研究的进步。目的：开发并评估大型语言模型（LLMs），使其能够自动从MRI/CT报告中提取PCL特征，并根据指南进行风险分类。材料与方法：我们构建了一个包含6,000份腹部MRI/CT报告（2005-2024年）的训练集，涉及5,134名患者，这些报告描述了PCL。标签由GPT-4o通过链式思考（CoT）提示生成，提取PCL和主要胰管特征。使用QLoRA对两种开源LLMs进行 fine-tuning，并使用GPT-4o生成的CoT数据。特征根据2017年ACR白皮书中的机构指南映射到相应风险类别。在285份保留的、由人类标注的报告上进行了评估。三位放射科医师独立审查了100个病例的模型输出。特征提取的准确性使用精确匹配准确率评估，风险分类使用宏平均F1分数评估，放射科医师与模型的共识则使用Fleiss' Kappa系数。结果：CoT fine-tuning提高了LLaMA（从80%到97%）和DeepSeek（从79%到98%）的特征提取准确性，与GPT-4o（97%）持平。风险分类F1分数也有所提高（LLaMA：0.95；DeepSeek：0.94），接近GPT-4o（0.97），并无统计学差异。放射科医师的内读者一致性高（Fleiss' Kappa = 0.888），并与DeepSeek-FT-CoT（Fleiss' Kappa = 0.893）或GPT-CoT（Fleiss' Kappa = 0.897）的添加无统计学差异，表明两种模型与放射科医师达到了一致水平。结论：带有CoT监督的开源LLMs fine-tuning能够实现大规模PCL研究中准确、可解释和高效的表型研究，其性能与GPT-4o相当。", 'title_zh': '利用细调的大语言模型进行可解释的胰腺囊性病变特征提取与风险分类'}
{'arxiv_id': 'arXiv:2507.19749', 'title': 'Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)', 'authors': 'Lin Ren, Guohui Xiao, Guilin Qi, Yishuai Geng, Haohan Xue', 'link': 'https://arxiv.org/abs/2507.19749', 'abstract': 'Answer Set Programming (ASP) is a powerful paradigm for non-monotonic reasoning. Recently, large language models (LLMs) have demonstrated promising capabilities in logical reasoning. Despite this potential, current evaluations of LLM capabilities in ASP are often limited. Existing works normally employ overly simplified ASP programs, do not support negation, disjunction, or multiple answer sets. Furthermore, there is a lack of benchmarks that introduce tasks specifically designed for ASP solving. To bridge this gap, we introduce ASPBench, a comprehensive ASP benchmark, including three ASP specific tasks: ASP entailment, answer set verification, and answer set computation. Our extensive evaluations on ASPBench reveal that while 14 state-of-the-art LLMs, including \\emph{deepseek-r1}, \\emph{o4-mini}, and \\emph{gemini-2.5-flash-thinking}, perform relatively well on the first two simpler tasks, they struggle with answer set computation, which is the core of ASP solving. These findings offer insights into the current limitations of LLMs in ASP solving. This highlights the need for new approaches that integrate symbolic reasoning capabilities more effectively. The code and dataset are available at this https URL.', 'abstract_zh': 'ASPBench：一个全面的ASP基准，包含三个特定的ASP任务：ASP蕴含、答案集验证和答案集计算。', 'title_zh': 'LLM解决ASP问题的能力：一项基准研究的见解（扩展版）'}
{'arxiv_id': 'arXiv:2507.19703', 'title': 'The wall confronting large language models', 'authors': 'Peter V. Coveney, Sauro Succi', 'link': 'https://arxiv.org/abs/2507.19703', 'abstract': 'We show that the scaling laws which determine the performance of large language models (LLMs) severely limit their ability to improve the uncertainty of their predictions. As a result, raising their reliability to meet the standards of scientific inquiry is intractable by any reasonable measure. We argue that the very mechanism which fuels much of the learning power of LLMs, namely the ability to generate non-Gaussian output distributions from Gaussian input ones, might well be at the roots of their propensity to produce error pileup, ensuing information catastrophes and degenerative AI behaviour. This tension between learning and accuracy is a likely candidate mechanism underlying the observed low values of the scaling components. It is substantially compounded by the deluge of spurious correlations pointed out by Calude and Longo which rapidly increase in any data set merely as a function of its size, regardless of its nature. The fact that a degenerative AI pathway is a very probable feature of the LLM landscape does not mean that it must inevitably arise in all future AI research. Its avoidance, which we also discuss in this paper, necessitates putting a much higher premium on insight and understanding of the structural characteristics of the problems being investigated.', 'abstract_zh': '我们展示了决定大型语言模型（LLMs）性能的标度律严重限制了它们改进预测不确定性的能力。结果，提高其可靠性以达到科学探究的标准在任何合理的衡量标准下都是无法实现的。我们argue认为，正是生成非高斯输出分布的能力，从高斯输入分布中生成，可能是导致LLMs产生累积误差、信息灾难和退化AI行为的根本机制之一。这种学习与准确性的张力可能是观察到的标度成分低值的潜在机制之一。这种张力被Calude和Longo指出的虚假相关性进一步加剧，这些虚假相关性在数据集的大小增加时迅速增加，且不依赖于数据集的性质。LLMs存在退化AI路径的概率特征并不意味着它必然会在所有未来的AI研究中出现。避免这种退化AI路径，需要我们在研究过程中更重视对所研究问题的结构特征的理解和洞察。', 'title_zh': '大型语言模型面临的挑战'}
{'arxiv_id': 'arXiv:2507.19672', 'title': 'Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges', 'authors': 'Haoran Lu, Luyang Fang, Ruidong Zhang, Xinliang Li, Jiazhang Cai, Huimin Cheng, Lin Tang, Ziyu Liu, Zeliang Sun, Tao Wang, Yingchuan Zhang, Arif Hassan Zidan, Jinwen Xu, Jincheng Yu, Meizhi Yu, Hanqi Jiang, Xilin Gong, Weidi Luo, Bolun Sun, Yongkai Chen, Terry Ma, Shushan Wu, Yifan Zhou, Junhao Chen, Haotian Xiang, Jing Zhang, Afrar Jahin, Wei Ruan, Ke Deng, Yi Pan, Peilong Wang, Jiahui Li, Zhengliang Liu, Lu Zhang, Lin Zhao, Wei Liu, Dajiang Zhu, Xin Xing, Fei Dou, Wei Zhang, Chao Huang, Rongjie Liu, Mengrui Zhang, Yiwen Liu, Xiaoxiao Sun, Qin Lu, Zhen Xiang, Wenxuan Zhong, Tianming Liu, Ping Ma', 'link': 'https://arxiv.org/abs/2507.19672', 'abstract': 'Due to the remarkable capabilities and growing impact of large language models (LLMs), they have been deeply integrated into many aspects of society. Thus, ensuring their alignment with human values and intentions has emerged as a critical challenge. This survey provides a comprehensive overview of practical alignment techniques, training protocols, and empirical findings in LLM alignment. We analyze the development of alignment methods across diverse paradigms, characterizing the fundamental trade-offs between core alignment objectives. Our analysis shows that while supervised fine-tuning enables basic instruction-following, preference-based methods offer more flexibility for aligning with nuanced human intent. We discuss state-of-the-art techniques, including Direct Preference Optimization (DPO), Constitutional AI, brain-inspired methods, and alignment uncertainty quantification (AUQ), highlighting their approaches to balancing quality and efficiency. We review existing evaluation frameworks and benchmarking datasets, emphasizing limitations such as reward misspecification, distributional robustness, and scalable oversight. We summarize strategies adopted by leading AI labs to illustrate the current state of practice. We conclude by outlining open problems in oversight, value pluralism, robustness, and continuous alignment. This survey aims to inform both researchers and practitioners navigating the evolving landscape of LLM alignment.', 'abstract_zh': '由于大型语言模型（LLMs）的卓越能力和日益增长的影响，它们已被广泛融入社会的许多方面。因此，确保它们与人类价值观和意图的一致性已成为一个关键挑战。本综述提供了LLM一致性技术、训练协议和实验研究的全面概述。我们分析了一致性方法在不同范式下的发展，阐述了核心一致目标之间的基本权衡。我们的分析表明，虽然监督微调可以实现基本的指令遵循，但基于偏好的方法则为与细微的人类意图对齐提供了更多灵活性。我们讨论了最先进的技术，包括直接偏好优化（DPO）、宪法AI、大脑启发方法以及一致性不确定性量化（AUQ），强调了它们在平衡质量和效率方面的策略。我们回顾了现有的评估框架和基准数据集，强调了奖励误指定、分布鲁棒性和可扩展监督等局限性。我们总结了领先AI实验室采用的策略，以说明当前的一致性实践状况。最后，我们概述了在监督、价值多元论、鲁棒性和持续一致性方面的开放问题。本综述旨在为研究人员和从业者提供关于LLM一致性不断演变的景观的指导信息。', 'title_zh': '大型语言模型中的对齐与安全性：安全机制、训练范式及新兴挑战'}
{'arxiv_id': 'arXiv:2507.19608', 'title': 'DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference', 'authors': 'Jiawen Qi, Chang Gao, Zhaochun Ren, Qinyu Chen', 'link': 'https://arxiv.org/abs/2507.19608', 'abstract': 'Deploying Large Language Models (LLMs) on edge devices remains challenging due to their quadratically increasing computations with the sequence length. Existing studies for dynamic attention pruning are designed for hardware with massively parallel computation capabilities, such as GPUs or TPUs, and aim at long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We present DeltaLLM, a training-free framework that exploits temporal sparsity in attention patterns to enable efficient LLM inference across both the prefilling and decoding stages, on resource-constrained edge devices. DeltaLLM introduces an accuracy- and memory-aware delta matrix construction strategy that introduces temporal sparsity, and a context-aware hybrid attention mechanism that combines full attention in a local context window with delta approximation outside it to increase accuracy. We evaluate our framework on the edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model across diverse language tasks. The results show that on BitNet, our framework increases the attention sparsity from 0% to 60% during the prefilling stage with slight accuracy improvement on the WG task, and 0% to 57% across both the prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97 on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity during the prefilling stage and around 57% across both stages with negligible accuracy drop. These results demonstrate that DeltaLLM offers a promising solution for efficient edge deployment, requiring no fine-tuning and seamlessly integrating with existing inference pipelines.', 'abstract_zh': 'DeltaLLM：利用注意力模式的时域稀疏性在受限资源边缘设备上实现高效的大语言模型推理', 'title_zh': 'DeltaLLM：一种利用时间稀疏性进行高效边缘端LLM推理的无训练框架'}
{'arxiv_id': 'arXiv:2507.19543', 'title': 'Agent WARPP: Workflow Adherence via Runtime Parallel Personalization', 'authors': 'Maria Emilia Mazzolenis, Ruirui Zhang', 'link': 'https://arxiv.org/abs/2507.19543', 'abstract': 'Large language models (LLMs) are increasingly applied in task-oriented dialogue (TOD) systems but often struggle with long, conditional workflows that involve external tool calls and depend on user-specific information. We present Workflow Adherence via Runtime Parallel Personalization, or WARPP, a training-free, modular framework that combines multi-agent orchestration with runtime personalization to improve workflow adherence in LLM-based systems. By dynamically pruning conditional branches based on user attributes, the framework reduces reasoning overhead and narrows tool selection at runtime. WARPP deploys a parallelized architecture where a dedicated Personalizer agent operates alongside modular, domain-specific agents to dynamically tailor execution paths in real time. The framework is evaluated across five representative user intents of varying complexity within three domains: banking, flights, and healthcare. Our evaluation leverages synthetic datasets and LLM-powered simulated users to test scenarios with conditional dependencies. Our results demonstrate that WARPP outperforms both the non-personalized method and the ReAct baseline, achieving increasingly larger gains in parameter fidelity and tool accuracy as intent complexity grows, while also reducing average token usage, without any additional training.', 'abstract_zh': '基于运行时并行个人化的流程遵从性框架：无需训练的模块化方法改进大语言模型的流程遵从性', 'title_zh': 'Agent WARPP: 工作流遵守性通过运行时并行个性化'}
{'arxiv_id': 'arXiv:2507.21009', 'title': 'Memorization in Fine-Tuned Large Language Models', 'authors': 'Danil Savine, Muni Sreenivas Pydi, Jamal Atif, Olivier Cappé', 'link': 'https://arxiv.org/abs/2507.21009', 'abstract': "This study investigates the mechanisms and factors influencing memorization in fine-tuned large language models (LLMs), with a focus on the medical domain due to its privacy-sensitive nature. We examine how different aspects of the fine-tuning process affect a model's propensity to memorize training data, using the PHEE dataset of pharmacovigilance events.\nOur research employs two main approaches: a membership inference attack to detect memorized data, and a generation task with prompted prefixes to assess verbatim reproduction. We analyze the impact of adapting different weight matrices in the transformer architecture, the relationship between perplexity and memorization, and the effect of increasing the rank in low-rank adaptation (LoRA) fine-tuning.\nKey findings include: (1) Value and Output matrices contribute more significantly to memorization compared to Query and Key matrices; (2) Lower perplexity in the fine-tuned model correlates with increased memorization; (3) Higher LoRA ranks lead to increased memorization, but with diminishing returns at higher ranks.\nThese results provide insights into the trade-offs between model performance and privacy risks in fine-tuned LLMs. Our findings have implications for developing more effective and responsible strategies for adapting large language models while managing data privacy concerns.", 'abstract_zh': '本研究 investigates 细化为探讨调优大型语言模型（LLMs）中记忆机制及其影响因素，重点关注医疗领域因其敏感的隐私特性。我们通过使用PHEE药物警戒事件数据集，研究调优过程中不同方面对模型记忆训练数据倾向的影响。\n\n我们的研究采用两种主要方法：一种是成员推断攻击以检测记忆的数据，另一种是带有提示前缀的生成任务以评估逐字复制。我们分析了适应不同权重矩阵在转换器架构中的影响、困惑度与记忆之间的关系，以及低秩适应（LoRA）调优中秩增大的影响。\n\n主要发现包括：（1）值和输出矩阵比查询和键矩阵更显著地贡献于记忆；（2）调优后的模型较低的困惑度与增加的记忆有关；（3）较高的LoRA秩导致记忆增加，但随着秩的增大，效果递减。\n\n这些结果为模型性能与隐私风险之间的权衡提供了见解。我们的发现对开发更有效且负责任的大语言模型适应策略、同时管理数据隐私问题具有重要意义。', 'title_zh': 'Fine-Tuned大型语言模型中的记忆现象'}
{'arxiv_id': 'arXiv:2507.20994', 'title': 'Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety to Vision in LVLM', 'authors': 'Shen Li, Liuyi Yao, Wujia Niu, Lan Zhang, Yaliang Li', 'link': 'https://arxiv.org/abs/2507.20994', 'abstract': 'Large visual-language models (LVLMs) integrate aligned large language models (LLMs) with visual modules to process multimodal inputs. However, the safety mechanisms developed for text-based LLMs do not naturally extend to visual modalities, leaving LVLMs vulnerable to harmful image inputs. To address this cross-modal safety gap, we introduce security tensors - trainable input vectors applied during inference through either the textual or visual modality. These tensors transfer textual safety alignment to visual processing without modifying the model\'s parameters. They are optimized using a curated dataset containing (i) malicious image-text pairs requiring rejection, (ii) contrastive benign pairs with text structurally similar to malicious queries, with the purpose of being contrastive examples to guide visual reliance, and (iii) general benign samples preserving model functionality. Experimental results demonstrate that both textual and visual security tensors significantly enhance LVLMs\' ability to reject diverse harmful visual inputs while maintaining near-identical performance on benign tasks. Further internal analysis towards hidden-layer representations reveals that security tensors successfully activate the language module\'s textual "safety layers" in visual inputs, thereby effectively extending text-based safety to the visual modality.', 'abstract_zh': '大型视觉语言模型的安全机制：通过可训练的安全张量在文本和视觉模态间转移文本安全性', 'title_zh': '安全张量作为跨模态桥梁：将文本对齐的安全性扩展到LVLM中的视觉领域'}
{'arxiv_id': 'arXiv:2507.20984', 'title': 'SmallThinker: A Family of Efficient Large Language Models Natively Trained for Local Deployment', 'authors': 'Yixin Song, Zhenliang Xue, Dongliang Wei, Feiyang Chen, Jianxiang Gao, Junchen Liu, Hangyu Liang, Guangshuo Qin, Chengrong Tian, Bo Wen, Longyu Zhao, Xinrui Zheng, Zeyu Mi, Haibo Chen', 'link': 'https://arxiv.org/abs/2507.20984', 'abstract': 'While frontier large language models (LLMs) continue to push capability boundaries, their deployment remains confined to GPU-powered cloud infrastructure. We challenge this paradigm with SmallThinker, a family of LLMs natively designed - not adapted - for the unique constraints of local devices: weak computational power, limited memory, and slow storage. Unlike traditional approaches that mainly compress existing models built for clouds, we architect SmallThinker from the ground up to thrive within these limitations. Our innovation lies in a deployment-aware architecture that transforms constraints into design principles. First, We introduce a two-level sparse structure combining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward networks, drastically reducing computational demands without sacrificing model capacity. Second, to conquer the I/O bottleneck of slow storage, we design a pre-attention router that enables our co-designed inference engine to prefetch expert parameters from storage while computing attention, effectively hiding storage latency that would otherwise cripple on-device inference. Third, for memory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to slash KV cache requirements. We release SmallThinker-4B-A0.6B and SmallThinker-21B-A3B, which achieve state-of-the-art performance scores and even outperform larger LLMs. Remarkably, our co-designed system mostly eliminates the need for expensive GPU hardware: with Q4_0 quantization, both models exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB and 8GB of memory respectively. SmallThinker is publicly available at this http URL and this http URL.', 'abstract_zh': '尽管前沿大规模语言模型持续推动能力边界，其部署依然局限于GPU驱动的云基础设施。我们以SmallThinker挑战这一范式，这是一个专门为本地设备设计的LLM家族：其独特约束包括弱计算能力、有限内存和缓慢存储。与主要针对云设计的现有模型进行压缩的传统方法不同，我们从头构建SmallThinker，使其能够在这些限制中茁壮成长。我们的创新之处在于一种面向部署的架构，将约束转化为设计原则。首先，我们引入了一种两级稀疏结构，结合了细粒度的混合专家（MoE）和稀疏前馈网络，大幅减少了计算需求，同时不牺牲模型容量。其次，为克服缓慢存储带来的I/O瓶颈，我们设计了一种预注意力路由器，使我们共同设计的推理引擎能够在计算注意力的同时预取专家参数，从而隐藏存储延迟，避免使设备上的推理能力瘫痪。第三，为了提高内存效率，我们利用NoPE-RoPE混合稀疏注意力机制削减了键值缓存要求。我们发布了SmallThinker-4B-A0.6B和SmallThinker-21B-A3B，它们达到最先进的性能得分，并且甚至优于更大规模的LLM。令人惊讶的是，我们的共同设计系统几乎消除了对昂贵GPU硬件的需求：使用Q4_0量化后，两款模型在普通消费级CPU上分别实现了超过20个-token/s的速度，同时分别消耗1GB和8GB的内存。SmallThinker在以下网址公开提供：此网址和此网址。', 'title_zh': '小思考者：一系列本地产地部署的高效大型语言模型'}
{'arxiv_id': 'arXiv:2507.20957', 'title': 'Your AI, Not Your View: The Bias of LLMs in Investment Analysis', 'authors': 'Hoyoung Lee, Junhyuk Seo, Suhwan Park, Junhyeong Lee, Wonbin Ahn, Chanyeol Choi, Alejandro Lopez-Lira, Yongjae Lee', 'link': 'https://arxiv.org/abs/2507.20957', 'abstract': "In finance, Large Language Models (LLMs) face frequent knowledge conflicts due to discrepancies between pre-trained parametric knowledge and real-time market data. These conflicts become particularly problematic when LLMs are deployed in real-world investment services, where misalignment between a model's embedded preferences and those of the financial institution can lead to unreliable recommendations. Yet little research has examined what investment views LLMs actually hold. We propose an experimental framework to investigate such conflicts, offering the first quantitative analysis of confirmation bias in LLM-based investment analysis. Using hypothetical scenarios with balanced and imbalanced arguments, we extract models' latent preferences and measure their persistence. Focusing on sector, size, and momentum, our analysis reveals distinct, model-specific tendencies. In particular, we observe a consistent preference for large-cap stocks and contrarian strategies across most models. These preferences often harden into confirmation bias, with models clinging to initial judgments despite counter-evidence.", 'abstract_zh': '在金融领域，大型语言模型（LLMs）由于预训练参数知识与实时市场数据之间的差异，经常面临知识冲突。当LLMs在现实世界的投资服务中部署时，模型内置的偏好与金融机构的偏好之间的不一致可能导致不可靠的建议。然而，很少有研究考察LLMs实际持有的投资观点。我们提出了一种实验框架来研究这些冲突，并提供了基于LLMs的投资分析中证实偏差的首次定量分析。利用平衡和不平衡的假设情景，我们提取了模型的潜在偏好并测量其持续性。聚焦于行业、规模和动量，我们的分析揭示了不同的、模型特定的趋势。特别是，我们观察到大多数模型中对大型 capital 股票和逆向策略的一致偏好。这些偏好往往会固化为证实偏差，即使有相反的证据，模型也仍坚持初始判断。', 'title_zh': '你的AI，而非你的观点：投资分析中的LLM偏差'}
{'arxiv_id': 'arXiv:2507.20956', 'title': 'Mind the Gap: Conformative Decoding to Improve Output Diversity of Instruction-Tuned Large Language Models', 'authors': 'Max Peeperkorn, Tom Kouwenhoven, Dan Brown, Anna Jordanous', 'link': 'https://arxiv.org/abs/2507.20956', 'abstract': "Instruction-tuning large language models (LLMs) reduces the diversity of their outputs, which has implications for many tasks, particularly for creative tasks. This paper investigates the ``diversity gap'' for a writing prompt narrative generation task. This gap emerges as measured by current diversity metrics for various open-weight and open-source LLMs. The results show significant decreases in diversity due to instruction-tuning. We explore the diversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to further understand how output diversity is affected. The results indicate that DPO has the most substantial impact on diversity. Motivated by these findings, we present a new decoding strategy, conformative decoding, which guides an instruct model using its more diverse base model to reintroduce output diversity. We show that conformative decoding typically increases diversity and even maintains or improves quality.", 'abstract_zh': '大规模语言模型（LLMs）指令调优降低了输出多样性，这对许多任务尤其是创造性任务具有重要意义。本文研究了指令调优对写作提示叙事生成任务中“多样性差距”的影响。通过当前的多样性度量标准测量，这一差距在各种开放权重和开源LLMs中显现出来。结果显示，指令调优导致了显著的多样性下降。我们探讨了OLMo和OLMo 2模型在每个微调阶段的多样性损失情况，以进一步了解输出多样性受到影响的具体方式。结果表明，DPO对多样性的影响最为显著。受这些发现的启发，我们提出了一个新的解码策略——符合性解码，该策略使用更具多样性的基础模型来指导指令模型，重新引入输出多样性。我们证明，符合性解码通常会增加多样性，并且甚至可以维持或提高质量。', 'title_zh': '注意差距：规范解码以提高指令微调大型语言模型的输出多样性'}
{'arxiv_id': 'arXiv:2507.20936', 'title': 'Dissecting Persona-Driven Reasoning in Language Models via Activation Patching', 'authors': 'Ansh Poonia, Maeghal Jain', 'link': 'https://arxiv.org/abs/2507.20936', 'abstract': "Large language models (LLMs) exhibit remarkable versatility in adopting diverse personas. In this study, we examine how assigning a persona influences a model's reasoning on an objective task. Using activation patching, we take a first step toward understanding how key components of the model encode persona-specific information. Our findings reveal that the early Multi-Layer Perceptron (MLP) layers attend not only to the syntactic structure of the input but also process its semantic content. These layers transform persona tokens into richer representations, which are then used by the middle Multi-Head Attention (MHA) layers to shape the model's output. Additionally, we identify specific attention heads that disproportionately attend to racial and color-based identities.", 'abstract_zh': '大型语言模型（LLMs）在采用不同人设方面显示出了显著的灵活性。在本研究中，我们探讨了赋予模型人设如何影响其在客观任务上的推理过程。利用激活提取方法，我们首次尝试理解模型的关键组件如何编码人设特定的信息。研究发现，早期的多层感知机（MLP）层不仅关注输入的句法结构，还处理其语义内容。这些层将人设标记转换为更丰富的表示，然后由中间的多头注意力（MHA）层用来塑造模型的输出。此外，我们还识别出对种族和基于肤色身份特别关注的注意力头。', 'title_zh': '通过激活补丁分析基于人设的语言模型推理解析'}
{'arxiv_id': 'arXiv:2507.20930', 'title': 'FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models', 'authors': 'Likun Tan, Kuan-Wei Huang, Kevin Wu', 'link': 'https://arxiv.org/abs/2507.20930', 'abstract': 'Hallucinations in large language models pose a critical challenge for applications requiring factual reliability, particularly in high-stakes domains such as finance. This work presents an effective approach for detecting and editing factually incorrect content in model-generated responses based on the provided context. Given a user-defined domain-specific error taxonomy, we construct a synthetic dataset by inserting tagged errors into financial question-answering corpora and then fine-tune four language models, Phi-4, Phi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8% improvement in binary F1 score and a 30% gain in overall detection performance compared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having only 4 billion parameters, maintains competitive performance with just a 2% drop in binary detection and a 0.1% decline in overall detection compared to OpenAI-o3. Our work provides a practical solution for detecting and editing factual inconsistencies in financial text generation while introducing a generalizable framework that can enhance the trustworthiness and alignment of large language models across diverse applications beyond finance. Our code and data are available at this https URL.', 'abstract_zh': '大型语言模型中的幻觉对需要事实可靠性的应用构成了关键挑战，特别是在金融等高 stakes 领域。本文提出了一种有效的方法，基于提供的上下文检测和编辑模型生成响应中的事实错误。给定用户定义的领域特定错误分类，我们通过在金融问答语料中插入标记错误构造了一个合成数据集，并对四种语言模型Phi-4、Phi-4-mini、Qwen3-4B和Qwen3-14B进行微调，以检测和编辑这些事实不准确性。我们表现最佳的模型微调后的Phi-4，在二元F1分数上提高了8%，整体检测性能提高了30%，相较于OpenAI-o3。值得注意的是，尽管微调后的Phi-4-mini仅有40亿参数，其二元检测性能下降了2%，整体检测性能下降了0.1%，但仍保持了竞争力。我们的工作提供了一种实用的解决方案，用于检测和编辑金融文本生成中的事实不一致，同时引入了一种可推广的框架，可以在超出金融领域的各种应用中增强大型语言模型的信任度和对齐。我们的代码和数据可在以下链接获取。', 'title_zh': 'FRED: 金融检索增强的语言模型幻觉检测与编辑'}
{'arxiv_id': 'arXiv:2507.20924', 'title': 'FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models', 'authors': 'Roberto Labadie-Tamayo, Adrian Jaques Böck, Djordje Slijepčević, Xihui Chen, Andreas Babic, Matthias Zeppelzauer', 'link': 'https://arxiv.org/abs/2507.20924', 'abstract': "Sexism has become widespread on social media and in online conversation. To help address this issue, the fifth Sexism Identification in Social Networks (EXIST) challenge is initiated at CLEF 2025. Among this year's international benchmarks, we concentrate on solving the first task aiming to identify and classify sexism in social media textual posts. In this paper, we describe our solutions and report results for three subtasks: Subtask 1.1 - Sexism Identification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask 1.3 - Sexism Categorization in Tweets. We implement three models to address each subtask which constitute three individual runs: Speech Concept Bottleneck Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a fine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as human-interpretable bottleneck concepts. SCBM leverages large language models (LLMs) to encode input texts into a human-interpretable representation of adjectives, then used to train a lightweight classifier for downstream tasks. SCBMT extends SCBM by fusing adjective-based representation with contextual embeddings from transformers to balance interpretability and classification performance. Beyond competitive results, these two models offer fine-grained explanations at both instance (local) and class (global) levels. We also investigate how additional metadata, e.g., annotators' demographic profiles, can be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data augmented with prior datasets, ranks 6th for English and Spanish and 4th for English in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and Spanish and 6th for Spanish.", 'abstract_zh': '性别歧视在社交媒体和在线对话中愈演愈烈。为解决这一问题，CLEF 2025年启动了第五屆社交网络性别歧视识别（EXIST）挑战。在本年的国际基准中，我们集中于解决首个任务，即识别和分类社交媒体文本帖子中的性别歧视。在本文中，我们描述了我们的解决方案并报告了三个子任务的结果：子任务1.1 - 微博中的性别歧视识别、子任务1.2 - 微博中的来源意图、子任务1.3 - 微博中的性别歧视分类。我们实现了三种模型来解决每个子任务，构成三个独立的运行：言语概念瓶颈模型（SCBM）、带变换器的言语概念瓶颈模型（SCBMT）以及微调后的XLM-RoBERTa变换器模型。SCBM利用描述性形容词作为人类可解释的瓶颈概念。SCBM利用大规模语言模型（LLMs）将输入文本编码为形容词的人类可解释表示，然后用于下游任务的轻量级分类器训练。SCBMT在基于描述词的表示上结合了变换器的语境嵌入，以平衡可解释性和分类性能。除了竞争性结果外，这两种模型还在实例（局部）和类别（全局）层面提供了详尽的解释。我们还研究了如何利用额外的元数据，例如注释者的 demographic 背景。对于子任务1.1，XLM-RoBERTa，在软-软评估中，英语和西班牙语分别排名第6和第4。我们的SCBMT在英语和西班牙语中分别排名第7和第6，在西班牙语中排名第6。', 'title_zh': 'FHSTP@EXIST 2025 基准：透明语音概念瓶颈模型中的性别偏见检测'}
{'arxiv_id': 'arXiv:2507.20923', 'title': 'Pareto-Grid-Guided Large Language Models for Fast and High-Quality Heuristics Design in Multi-Objective Combinatorial Optimization', 'authors': 'Minh Hieu Ha, Hung Phan, Tung Duy Doan, Tung Dao, Dao Tran, Huynh Thi Thanh Binh', 'link': 'https://arxiv.org/abs/2507.20923', 'abstract': 'Multi-objective combinatorial optimization problems (MOCOP) frequently arise in practical applications that require the simultaneous optimization of conflicting objectives. Although traditional evolutionary algorithms can be effective, they typically depend on domain knowledge and repeated parameter tuning, limiting flexibility when applied to unseen MOCOP instances. Recently, integration of Large Language Models (LLMs) into evolutionary computation has opened new avenues for automatic heuristic generation, using their advanced language understanding and code synthesis capabilities. Nevertheless, most existing approaches predominantly focus on single-objective tasks, often neglecting key considerations such as runtime efficiency and heuristic diversity in multi-objective settings. To bridge this gap, we introduce Multi-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a novel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO) framework that leverages LLMs and Pareto Front Grid (PFG) technique. By partitioning the objective space into grids and retaining top-performing candidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize heuristics with semantically distinct logical structures during variation, thus promoting diversity and mitigating redundancy within the population. Through extensive evaluations, MPaGE demonstrates superior performance over existing LLM-based frameworks, and achieves competitive results to traditional Multi-objective evolutionary algorithms (MOEAs), with significantly faster runtime. Our code is available at: this https URL.', 'abstract_zh': '多目标组合优化问题（MOCOP）经常出现在需要同时优化冲突目标的实际应用中。虽然传统的进化算法通常是有效的，但它们通常依赖于领域知识并且需要反复调整参数，这限制了它们在未见过的MOCOP实例中的灵活性。近年来，将大型语言模型（LLMs）集成到进化计算中为自动启发式生成开辟了新的途径，利用其先进的语言理解和代码合成能力。然而，现有的大多数方法主要集中在单目标任务上，经常忽视多目标设置中的重要考虑因素，如运行时效率和启发式多样性。为了解决这一问题，我们提出了通过帕累托网格引导的LLM进化进行多启发式优化（MPaGE），这是一种对简单多目标进化优化（SEMO）框架的创新改进，利用了LLMs和帕累托前沿网格（PFG）技术。通过将目标空间划分成网格，并保留表现优异的候选者以引导启发式生成，MPaGE利用LLMs在变异过程中优先选择语义上不同的逻辑结构的启发式，从而促进多样性和减少种群中的冗余。通过广泛的评估，MPaGE在基于LLM的现有框架中表现出更优的性能，并且在运行时速度上有显著提升，达到传统多目标进化算法（MOEAs）的竞争力。我们的代码可在以下链接获得：this https URL。', 'title_zh': 'Pareto-网格引导的大语言模型在多目标组合优化中快速高质设计启发式方法'}
{'arxiv_id': 'arXiv:2507.20836', 'title': 'First Hallucination Tokens Are Different from Conditional Ones', 'authors': 'Jakob Snel, Seong Joon Oh', 'link': 'https://arxiv.org/abs/2507.20836', 'abstract': "Hallucination, the generation of untruthful content, is one of the major concerns regarding foundational models. Detecting hallucinations at the token level is vital for real-time filtering and targeted correction, yet the variation of hallucination signals within token sequences is not fully understood. Leveraging the RAGTruth corpus with token-level annotations and reproduced logits, we analyse how these signals depend on a token's position within hallucinated spans, contributing to an improved understanding of token-level hallucination. Our results show that the first hallucinated token carries a stronger signal and is more detectable than conditional tokens. We release our analysis framework, along with code for logit reproduction and metric computation at this https URL.", 'abstract_zh': '幻觉，即生成虚假内容，是基础模型面临的重大问题之一。基于标记到令牌级别的RAGTruth语料库和重构的logits，我们分析这些信号如何依赖于幻觉片段中令牌的位置，从而增进对令牌级幻觉的理解。我们的结果显示，首个生成的幻觉令牌比条件令牌携带更强的信号且更容易检测。我们发布了解析框架及相关代码，详见此链接：https://this.is/url。', 'title_zh': '第一类幻觉词与条件词不同'}
{'arxiv_id': 'arXiv:2507.20796', 'title': 'Aligning Large Language Model Agents with Rational and Moral Preferences: A Supervised Fine-Tuning Approach', 'authors': 'Wei Lu, Daniel L. Chen, Christian B. Hansen', 'link': 'https://arxiv.org/abs/2507.20796', 'abstract': "Understanding how large language model (LLM) agents behave in strategic interactions is essential as these systems increasingly participate autonomously in economically and morally consequential decisions. We evaluate LLM preferences using canonical economic games, finding substantial deviations from human behavior. Models like GPT-4o show excessive cooperation and limited incentive sensitivity, while reasoning models, such as o3-mini, align more consistently with payoff-maximizing strategies. We propose a supervised fine-tuning pipeline that uses synthetic datasets derived from economic reasoning to align LLM agents with economic preferences, focusing on two stylized preference structures. In the first, utility depends only on individual payoffs (homo economicus), while utility also depends on a notion of Kantian universalizability in the second preference structure (homo moralis). We find that fine-tuning based on small datasets shifts LLM agent behavior toward the corresponding economic agent. We further assess the fine-tuned agents' behavior in two applications: Moral dilemmas involving autonomous vehicles and algorithmic pricing in competitive markets. These examples illustrate how different normative objectives embedded via realizations from structured preference structures can influence market and moral outcomes. This work contributes a replicable, cost-efficient, and economically grounded pipeline to align AI preferences using moral-economic principles.", 'abstract_zh': '理解大型语言模型代理在战略互动中的行为对于这些系统在经济和道德上有重要意义的决策中自主参与至关重要。我们使用经典的经济博弈来评估LLM的偏好，发现其行为与人类行为存在显著偏差。如GPT-4o这类模型表现出过度合作和激励敏感性有限的特点，而如o3-mini这类基于推理的模型则更一致地符合最大化收益的战略。我们提出了一种监督微调管道，使用源自经济推理的合成数据集来使LLM代理与经济偏好相一致，重点关注两种典型偏好结构。在第一种结构中，效用仅依赖于个体收益（经济人），而在第二种结构中，效用还依赖于一种批判性的普遍化概念（道德人）。我们发现基于小型数据集的微调能使LLM代理行为向相应的经济代理靠拢。进一步在两个应用中评估微调后的代理行为：涉及自主车辆的道德困境和竞争市场中的算法定价。这些示例展示了通过结构化偏好结构嵌入的不同规范性目标如何影响市场和道德结果。本研究贡献了一种可复制、成本效益高且基于经济原则的人工智能偏好对齐管道。', 'title_zh': '使大型语言模型代理与理性与道德的偏好相一致：一种监督微调方法'}
{'arxiv_id': 'arXiv:2507.20704', 'title': 'Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models', 'authors': 'Gabriel Downer, Sean Craven, Damian Ruck, Jake Thomas', 'link': 'https://arxiv.org/abs/2507.20704', 'abstract': "The increasing integration of Visual Language Models (VLMs) into AI systems necessitates robust model alignment, especially when handling multimodal content that combines text and images. Existing evaluation datasets heavily lean towards text-only prompts, leaving visual vulnerabilities under evaluated. To address this gap, we propose \\textbf{Text2VLM}, a novel multi-stage pipeline that adapts text-only datasets into multimodal formats, specifically designed to evaluate the resilience of VLMs against typographic prompt injection attacks. The Text2VLM pipeline identifies harmful content in the original text and converts it into a typographic image, creating a multimodal prompt for VLMs. Also, our evaluation of open-source VLMs highlights their increased susceptibility to prompt injection when visual inputs are introduced, revealing critical weaknesses in the current models' alignment. This is in addition to a significant performance gap compared to closed-source frontier models. We validate Text2VLM through human evaluations, ensuring the alignment of extracted salient concepts; text summarization and output classification align with human expectations. Text2VLM provides a scalable tool for comprehensive safety assessment, contributing to the development of more robust safety mechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities, Text2VLM plays a role in advancing the safe deployment of VLMs in diverse, real-world applications.", 'abstract_zh': 'Text2VLM：多阶段管道将文本数据适配为多模态格式以评估视觉语言模型的视觉漏洞', 'title_zh': 'Text2VLM: 将仅文本数据集适应于评估视觉语言模型的对齐训练'}
{'arxiv_id': 'arXiv:2507.20666', 'title': 'MIMII-Agent: Leveraging LLMs with Function Calling for Relative Evaluation of Anomalous Sound Detection', 'authors': 'Harsh Purohit, Tomoya Nishida, Kota Dohi, Takashi Endo, Yohei Kawaguchi', 'link': 'https://arxiv.org/abs/2507.20666', 'abstract': 'This paper proposes a method for generating machine-type-specific anomalies to evaluate the relative performance of unsupervised anomalous sound detection (UASD) systems across different machine types, even in the absence of real anomaly sound data. Conventional keyword-based data augmentation methods often produce unrealistic sounds due to their reliance on manually defined labels, limiting scalability as machine types and anomaly patterns diversify. Advanced audio generative models, such as MIMII-Gen, show promise but typically depend on anomalous training data, making them less effective when diverse anomalous examples are unavailable. To address these limitations, we propose a novel synthesis approach leveraging large language models (LLMs) to interpret textual descriptions of faults and automatically select audio transformation functions, converting normal machine sounds into diverse and plausible anomalous sounds. We validate this approach by evaluating a UASD system trained only on normal sounds from five machine types, using both real and synthetic anomaly data. Experimental results reveal consistent trends in relative detection difficulty across machine types between synthetic and real anomalies. This finding supports our hypothesis and highlights the effectiveness of the proposed LLM-based synthesis approach for relative evaluation of UASD systems.', 'abstract_zh': '本文提出了一种生成机器类型特定异常的方法，以评估不同机器类型下无监督异常声检测系统（UASD）的相对性能，即使在缺乏实际异常声数据的情况下也是如此。传统的基于关键词的数据扩增方法通常依赖于人工定义的标签，生成不现实的声音，限制了在机器类型和异常模式多样化时的可扩展性。先进的音频生成模型，如MIMII-Gen，表现出潜力，但通常依赖于异常训练数据，当多样化的异常示例不可用时效果较差。为解决这些限制，我们提出了一种新的合成方法，利用大规模语言模型（LLMs）解释故障的文本描述并自动选择音频变换函数，将正常机器声音转换为多样且合乎实际的异常声音。我们通过使用五种机器类型仅正常声音训练的UASD系统，并使用真实和合成的异常数据进行评估，验证了该方法。实验结果表明，合成异常和真实异常在不同机器类型下的相对检测难度存在一致趋势。这一发现支持了我们的假设，并强调了基于LLMs合成方法在相对评估UASD系统方面的有效性。', 'title_zh': 'MIMII-Agent：利用函数调用的LLM技术在异常声音检测的相对评估中应用'}
{'arxiv_id': 'arXiv:2507.20643', 'title': 'Ontology-Enhanced Knowledge Graph Completion using Large Language Models', 'authors': 'Wenbin Guo, Xin Wang, Jiaoyan Chen, Zhao Li, Zirui Chen', 'link': 'https://arxiv.org/abs/2507.20643', 'abstract': 'Large Language Models (LLMs) have been extensively adopted in Knowledge Graph Completion (KGC), showcasing significant research advancements. However, as black-box models driven by deep neural architectures, current LLM-based KGC methods rely on implicit knowledge representation with parallel propagation of erroneous knowledge, thereby hindering their ability to produce conclusive and decisive reasoning outcomes. We aim to integrate neural-perceptual structural information with ontological knowledge, leveraging the powerful capabilities of LLMs to achieve a deeper understanding of the intrinsic logic of the knowledge. We propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first leverages neural perceptual mechanisms to effectively embed structural information into the textual space, and then uses an automated extraction algorithm to retrieve ontological knowledge from the knowledge graphs (KGs) that needs to be completed, which is further transformed into a textual format comprehensible to LLMs for providing logic guidance. We conducted extensive experiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The experimental results demonstrate that OL-KGC significantly outperforms existing mainstream KGC methods across multiple evaluation metrics, achieving state-of-the-art performance.', 'abstract_zh': '大规模语言模型（LLMs）在知识图谱补全（KGC）中的应用取得了显著的研究进展，然而，作为基于深度神经架构的黑盒模型，当前基于LLM的KGC方法依赖于并行传递的隐式知识表示，这限制了它们产生结论性和决定性的推理结果的能力。我们旨在结合神经感知结构信息与本体知识，利用LLMs的强大能力，实现对知识内在逻辑的更深入理解。我们提出了一种使用LLMs的本体增强知识图谱补全方法——OL-KGC。该方法首先利用神经感知机制有效将结构信息嵌入到文本空间中，然后使用自动化提取算法从需要补全的知识图谱（KGs）中检索本体知识，并进一步转换为LLMs可理解的文本格式，以提供逻辑指导。我们在三个广泛使用的基准数据集——FB15K-237、UMLS和WN18RR上进行了广泛的实验。实验结果表明，OL-KGC在多个评估指标上显著优于现有主流的KGC方法，实现了最佳性能。', 'title_zh': '使用大型语言模型增强的知识图谱完成方法'}
{'arxiv_id': 'arXiv:2507.20546', 'title': 'Enhancing Hallucination Detection via Future Context', 'authors': 'Joosung Lee, Cheonbok Park, Hwiyeol Jo, Jeonghoon Kim, Joonsuk Park, Kang Min Yoo', 'link': 'https://arxiv.org/abs/2507.20546', 'abstract': 'Large Language Models (LLMs) are widely used to generate plausible text on online platforms, without revealing the generation process. As users increasingly encounter such black-box outputs, detecting hallucinations has become a critical challenge. To address this challenge, we focus on developing a hallucination detection framework for black-box generators. Motivated by the observation that hallucinations, once introduced, tend to persist, we sample future contexts. The sampled future contexts provide valuable clues for hallucination detection and can be effectively integrated with various sampling-based methods. We extensively demonstrate performance improvements across multiple methods using our proposed sampling approach.', 'abstract_zh': '大型语言模型（LLMs）广泛用于在线平台上生成可信文本，而不揭示生成过程。随着用户越来越多地遇到这样的黑盒输出，检测幻觉已经成为一个关键挑战。为应对这一挑战，我们专注于开发针对黑盒生成器的幻觉检测框架。受观察到的幻觉一旦出现往往会持续存在这一现象的启发，我们采样了未来上下文。采样的未来上下文为幻觉检测提供了有价值的线索，并且可以有效与各种采样方法集成。我们广泛展示了使用我们提出的方法在多种方法中实现性能提升。', 'title_zh': '通过未来语境增强幻觉检测'}
{'arxiv_id': 'arXiv:2507.20534', 'title': 'Kimi K2: Open Agentic Intelligence', 'authors': 'Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao, Tong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T.Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, Haiming Wang', 'link': 'https://arxiv.org/abs/2507.20534', 'abstract': 'We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments.\nKimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints to facilitate future research and applications of agentic intelligence.', 'abstract_zh': 'Kimi K2：一种具有320亿激活参数和1万亿总参数的Mixture-of-Experts大型语言模型及其优化方法与应用', 'title_zh': 'Kimi K2: 开放自主人工智能'}
{'arxiv_id': 'arXiv:2507.20525', 'title': 'The Xeno Sutra: Can Meaning and Value be Ascribed to an AI-Generated "Sacred" Text?', 'authors': 'Murray Shanahan, Tara Das, Robert Thurman', 'link': 'https://arxiv.org/abs/2507.20525', 'abstract': 'This paper presents a case study in the use of a large language model to generate a fictional Buddhist "sutr"\', and offers a detailed analysis of the resulting text from a philosophical and literary point of view. The conceptual subtlety, rich imagery, and density of allusion found in the text make it hard to causally dismiss on account of its mechanistic origin. This raises questions about how we, as a society, should come to terms with the potentially unsettling possibility of a technology that encroaches on human meaning-making. We suggest that Buddhist philosophy, by its very nature, is well placed to adapt.', 'abstract_zh': '该论文探讨了一种大型语言模型生成虚构佛教“ sutra ”的案例研究，并从哲学和文学的角度对生成文本进行了详细分析。文本中概念的细微、丰富的意象以及引文的密集性使其机械产生的来源难以因果轻视。这引发了关于社会应如何应对技术侵占人类意义构建的潜在令人不安的可能性的问题。我们建议，佛教哲学因其本质特点，非常适合应对这一挑战。', 'title_zh': '异界 sutra: 人工智能生成的“神圣”文本能否赋予其意义和价值？'}
{'arxiv_id': 'arXiv:2507.20491', 'title': 'Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems', 'authors': 'Tuan Bui, Trong Le, Phat Thai, Sang Nguyen, Minh Hua, Ngan Pham, Thang Bui, Tho Quan', 'link': 'https://arxiv.org/abs/2507.20491', 'abstract': 'Recent advances in large language models (LLMs) have significantly enhanced question-answering (QA) capabilities, particularly in open-domain contexts. However, in closed-domain scenarios such as education, healthcare, and law, users demand not only accurate answers but also transparent reasoning and explainable decision-making processes. While neural-symbolic (NeSy) frameworks have emerged as a promising solution, leveraging LLMs for natural language understanding and symbolic systems for formal reasoning, existing approaches often rely on large-scale models and exhibit inefficiencies in translating natural language into formal logic representations.\nTo address these limitations, we introduce Text-JEPA (Text-based Joint-Embedding Predictive Architecture), a lightweight yet effective framework for converting natural language into first-order logic (NL2FOL). Drawing inspiration from dual-system cognitive theory, Text-JEPA emulates System 1 by efficiently generating logic representations, while the Z3 solver operates as System 2, enabling robust logical inference. To rigorously evaluate the NL2FOL-to-reasoning pipeline, we propose a comprehensive evaluation framework comprising three custom metrics: conversion score, reasoning score, and Spearman rho score, which collectively capture the quality of logical translation and its downstream impact on reasoning accuracy.\nEmpirical results on domain-specific datasets demonstrate that Text-JEPA achieves competitive performance with significantly lower computational overhead compared to larger LLM-based systems. Our findings highlight the potential of structured, interpretable reasoning frameworks for building efficient and explainable QA systems in specialized domains.', 'abstract_zh': 'Recent advances in大型语言模型（LLMs）显著增强了问答（QA）能力，特别是在开放领域中。然而，在教育、医疗和法律等封闭领域场景中，用户不仅需要准确的答案，还需要透明的推理和可解释的决策过程。虽然神经符号（NeSy）框架已 emerged 作为有希望的解决方案，利用大型语言模型进行自然语言理解，并利用符号系统进行形式推理，但现有方法往往依赖于大型模型并在将自然语言转换为形式逻辑表示方面表现出效率低下。\n\n为了解决这些限制，我们引入了Text-JEPA（基于文本的联合嵌入预测架构），这是一种轻量级但有效的方法，用于将自然语言转换为一阶逻辑（NL2FOL）。Text-JEPA 从双系统认知理论汲取灵感，通过高效生成逻辑表示来模拟系统1，而Z3求解器则作为系统2，支持强大的逻辑推理。为严格评估NL2FOL到推理的工作流程，我们提出了一个包含三个自定义度量的整体评估框架：转换得分、推理得分和斯皮尔曼ρ得分，这些得分共同捕捉了逻辑翻译的质量及其对推理准确性的下游影响。\n\n在特定领域的数据集上的实验证明，Text-JEPA 在显著降低计算开销的情况下实现了与更大规模的LLM系统相媲美的性能。我们的发现强调了结构化、可解释的推理框架在构建高效且可解释的问答系统方面的潜力。', 'title_zh': '言辞表达，逻辑思考：问答系统中的双重过程框架'}
{'arxiv_id': 'arXiv:2507.20439', 'title': 'When Prompts Go Wrong: Evaluating Code Model Robustness to Ambiguous, Contradictory, and Incomplete Task Descriptions', 'authors': 'Maya Larbi, Amal Akli, Mike Papadakis, Rihab Bouyousfi, Maxime Cordy, Federica Sarro, Yves Le Traon', 'link': 'https://arxiv.org/abs/2507.20439', 'abstract': 'Large Language Models (LLMs) have demonstrated impressive performance in code generation tasks under idealized conditions, where task descriptions are clear and precise. However, in practice, task descriptions frequently exhibit ambiguity, incompleteness, or internal contradictions. In this paper, we present the first empirical study examining the robustness of state-of-the-art code generation models when faced with such unclear task descriptions. We extend the HumanEval and MBPP benchmarks by systematically introducing realistic task descriptions flaws through guided mutation strategies, producing a dataset that mirrors the messiness of informal developer instructions. We evaluate multiple LLMs of varying sizes and architectures, analyzing their functional correctness and failure modes across task descriptions categories. Our findings reveal that even minor imperfections in task description phrasing can cause significant performance degradation, with contradictory task descriptions resulting in numerous logical errors. Moreover, while larger models tend to be more resilient than smaller variants, they are not immune to the challenges posed by unclear requirements. We further analyze semantic error patterns and identify correlations between description clarity, model behavior, and error types. Our results underscore the critical need for developing LLMs that are not only powerful but also robust to the imperfections inherent in natural user tasks, highlighting important considerations for improving model training strategies, designing more realistic evaluation benchmarks, and ensuring reliable deployment in practical software development environments.', 'abstract_zh': '大规模语言模型在面对含糊不清的任务描述时的稳健性研究', 'title_zh': '当提示出现错误：评价代码模型对模糊、矛盾和不完整任务描述的 robustness'}
{'arxiv_id': 'arXiv:2507.20423', 'title': 'CodeNER: Code Prompting for Named Entity Recognition', 'authors': 'Sungwoo Han, Hyeyeon Kim, Jingun Kwon, Hidetaka Kamigaito, Manabu Okumura', 'link': 'https://arxiv.org/abs/2507.20423', 'abstract': 'Recent studies have explored various approaches for treating candidate named entity spans as both source and target sequences in named entity recognition (NER) by leveraging large language models (LLMs). Although previous approaches have successfully generated candidate named entity spans with suitable labels, they rely solely on input context information when using LLMs, particularly, ChatGPT. However, NER inherently requires capturing detailed labeling requirements with input context information. To address this issue, we propose a novel method that leverages code-based prompting to improve the capabilities of LLMs in understanding and performing NER. By embedding code within prompts, we provide detailed BIO schema instructions for labeling, thereby exploiting the ability of LLMs to comprehend long-range scopes in programming languages. Experimental results demonstrate that the proposed code-based prompting method outperforms conventional text-based prompting on ten benchmarks across English, Arabic, Finnish, Danish, and German datasets, indicating the effectiveness of explicitly structuring NER instructions. We also verify that combining the proposed code-based prompting method with the chain-of-thought prompting further improves performance.', 'abstract_zh': 'Recent Studies Have Explored Various Approaches for Treating Candidate Named Entity Spans as Both Source and Target Sequences in NER by Leveraging Large Language Models', 'title_zh': 'CodeNER: 代码提示用于命名实体识别'}
{'arxiv_id': 'arXiv:2507.20252', 'title': 'Post-Completion Learning for Language Models', 'authors': 'Xiang Fei, Siqi Wang, Shu Wei, Yuxiang Nie, Wei Shi, Hao Feng, Can Huang', 'link': 'https://arxiv.org/abs/2507.20252', 'abstract': 'Current language model training paradigms typically terminate learning upon reaching the end-of-sequence (<eos>}) token, overlooking the potential learning opportunities in the post-completion space. We propose Post-Completion Learning (PCL), a novel training framework that systematically utilizes the sequence space after model output completion, to enhance both the reasoning and self-evaluation abilities. PCL enables models to continue generating self-assessments and reward predictions during training, while maintaining efficient inference by stopping at the completion point.\nTo fully utilize this post-completion space, we design a white-box reinforcement learning method: let the model evaluate the output content according to the reward rules, then calculate and align the score with the reward functions for supervision. We implement dual-track SFT to optimize both reasoning and evaluation capabilities, and mixed it with RL training to achieve multi-objective hybrid optimization.\nExperimental results on different datasets and models demonstrate consistent improvements over traditional SFT and RL methods. Our method provides a new technical path for language model training that enhances output quality while preserving deployment efficiency.', 'abstract_zh': '当前的语言模型训练范式通常在遇到序列结束符（<eos>）时终止学习，忽视了完成之后的空间中的潜在学习机会。我们提出了后完成学习（PCL），这是一种系统利用模型输出完成后序列空间的新训练框架，以增强推理和自我评估能力。PCL使模型在训练过程中继续生成自我评估和奖励预测，同时通过在完成点停止来保持高效的推理。\n\n为了充分利用这一后完成空间，我们设计了一种白盒强化学习方法：让模型根据奖励规则评估输出内容，然后计算并调整评分以匹配奖励函数进行监督。我们采用双重轨道的自我对齐训练（SFT）来优化推理和评估能力，并将其与RL训练结合，实现多目标混合优化。\n\n在不同数据集和模型上的实验结果表明，我们的方法在传统SFT和RL方法上一致地提高了性能。该方法为语言模型训练提供了一条新的技术路径，在提高输出质量的同时保持部署效率。', 'title_zh': '完成后的语言模型学习'}
{'arxiv_id': 'arXiv:2507.20181', 'title': 'SGPO: Self-Generated Preference Optimization based on Self-Improver', 'authors': 'Hyeonji Lee, Daejin Jo, Seohwan Yun, Sungwoong Kim', 'link': 'https://arxiv.org/abs/2507.20181', 'abstract': 'Large language models (LLMs), despite their extensive pretraining on diverse datasets, require effective alignment to human preferences for practical and reliable deployment. Conventional alignment methods typically employ off-policy learning and depend on human-annotated datasets, which limits their broad applicability and introduces distribution shift issues during training. To address these challenges, we propose Self-Generated Preference Optimization based on Self-Improver (SGPO), an innovative alignment framework that leverages an on-policy self-improving mechanism. Specifically, the improver refines responses from a policy model to self-generate preference data for direct preference optimization (DPO) of the policy model. Here, the improver and policy are unified into a single model, and in order to generate higher-quality preference data, this self-improver learns to make incremental yet discernible improvements to the current responses by referencing supervised fine-tuning outputs. Experimental results on AlpacaEval 2.0 and Arena-Hard show that the proposed SGPO significantly improves performance over DPO and baseline self-improving methods without using external preference data.', 'abstract_zh': '大规模语言模型（LLMs）尽管在多样化的数据集上进行了广泛的预训练，但在实际和可靠的部署中仍需要有效的对齐以符合人类偏好。传统的对齐方法通常依赖于离策学习并且依赖于人工标注的数据集，这限制了它们的广泛应用并在训练中引入了分布偏移问题。为了解决这些挑战，我们提出了基于Self-Improver的Self-Generated Preference Optimization（SGPO）这一创新的对齐框架，该框架利用了自改进机制。具体而言，改进模块通过优化政策模型的响应来自动生成偏好数据，直接用于对政策模型的偏好优化（DPO）。在此过程中，改进模块和政策被统一为一个模型，并通过参考监督微调输出来学习逐步改进当前响应，从而生成更高质量的偏好数据。实验结果表明，在AlpacaEval 2.0和Arena-Hard上的表现表明，提出的SGPO方法在不使用外部偏好数据的情况下显著优于DPO和基线自改进方法。', 'title_zh': 'SGPO: 自生成偏好优化基于自我改进者'}
{'arxiv_id': 'arXiv:2507.20152', 'title': 'Goal Alignment in LLM-Based User Simulators for Conversational AI', 'authors': 'Shuhaib Mehri, Xiaocheng Yang, Takyoung Kim, Gokhan Tur, Shikib Mehri, Dilek Hakkani-Tür', 'link': 'https://arxiv.org/abs/2507.20152', 'abstract': 'User simulators are essential to conversational AI, enabling scalable agent development and evaluation through simulated interactions. While current Large Language Models (LLMs) have advanced user simulation capabilities, we reveal that they struggle to consistently demonstrate goal-oriented behavior across multi-turn conversations--a critical limitation that compromises their reliability in downstream applications. We introduce User Goal State Tracking (UGST), a novel framework that tracks user goal progression throughout conversations. Leveraging UGST, we present a three-stage methodology for developing user simulators that can autonomously track goal progression and reason to generate goal-aligned responses. Moreover, we establish comprehensive evaluation metrics for measuring goal alignment in user simulators, and demonstrate that our approach yields substantial improvements across two benchmarks (MultiWOZ 2.4 and {\\tau}-Bench). Our contributions address a critical gap in conversational AI and establish UGST as an essential framework for developing goal-aligned user simulators.', 'abstract_zh': '用户模拟器是会话AI的关键组成部分，通过模拟交互 enables 扩展代理开发和评估。尽管当前的大规模语言模型（LLMs）具有先进的用户模拟能力，但我们发现它们在多轮对话中一致地表现出目标导向行为方面存在关键限制——这一限制阻碍了它们在下游应用中的可靠性。我们提出了一种新型框架用户目标状态跟踪（UGST），该框架在整个对话过程中跟踪用户目标进展。利用UGST，我们提出了一个三阶段方法用于开发能够自主跟踪目标进展并推理生成目标导向响应的用户模拟器。此外，我们为评估用户模拟器的目标一致性建立了全面的评价指标，并证明了我们方法在两个基准（MultiWOZ 2.4和τ-Bench）上取得了显著改进。我们的贡献填补了会话AI中的关键空白，并将UGST确立为开发目标导向用户模拟器的必不可少的框架。', 'title_zh': 'LLM基于的用户模拟器中目标对齐在对话AI中的应用'}
{'arxiv_id': 'arXiv:2507.20136', 'title': 'Multi-Stage Verification-Centric Framework for Mitigating Hallucination in Multi-Modal RAG', 'authors': 'Baiyu Chen, Wilson Wongso, Xiaoqian Hu, Yue Tan, Flora Salim', 'link': 'https://arxiv.org/abs/2507.20136', 'abstract': "This paper presents the technical solution developed by team CRUISE for the KDD Cup 2025 Meta Comprehensive RAG Benchmark for Multi-modal, Multi-turn (CRAG-MM) challenge. The challenge aims to address a critical limitation of modern Vision Language Models (VLMs): their propensity to hallucinate, especially when faced with egocentric imagery, long-tail entities, and complex, multi-hop questions. This issue is particularly problematic in real-world applications where users pose fact-seeking queries that demand high factual accuracy across diverse modalities. To tackle this, we propose a robust, multi-stage framework that prioritizes factual accuracy and truthfulness over completeness. Our solution integrates a lightweight query router for efficiency, a query-aware retrieval and summarization pipeline, a dual-pathways generation and a post-hoc verification. This conservative strategy is designed to minimize hallucinations, which incur a severe penalty in the competition's scoring metric. Our approach achieved 3rd place in Task 1, demonstrating the effectiveness of prioritizing answer reliability in complex multi-modal RAG systems. Our implementation is available at this https URL .", 'abstract_zh': '本文介绍了CRUISE团队为2025 KDD Cup Meta Comprehensive RAG Benchmark CRAG-MM挑战开发的技术解决方案。该挑战旨在解决现代视觉语言模型（VLMs）的一个关键局限性：在面对主观图像、长尾实体和复杂多跳问题时的倾向性虚构。这个问题在实际应用中尤为关键，用户提出事实求证查询，要求在多种模态中保持高度的事实准确性。为此，我们提出了一种稳健的多阶段框架，优先考虑事实准确性与真实性而非完整性。我们的解决方案包括一个轻量级查询路由器以提高效率，一个查询感知的检索和总结流水线，以及一种双重路径生成和事后验证。这一谨慎策略旨在最大限度地减少虚构的发生，在竞赛评分标准中虚构会遭受严重惩罚。我们的方法在任务1中获得第3名，展示了在复杂多模态RAG系统中优先保证答案可靠性的有效性。我们的实现可以在以下网址获取：这个 https URL 。', 'title_zh': '面向多模态RAG中幻觉缓解的多阶段验证中心框架'}
{'arxiv_id': 'arXiv:2507.20111', 'title': 'AI-Driven Generation of Old English: A Framework for Low-Resource Languages', 'authors': 'Rodrigo Gabriel Salazar Alva, Matías Nuñez, Cristian López, Javier Martín Arista', 'link': 'https://arxiv.org/abs/2507.20111', 'abstract': "Preserving ancient languages is essential for understanding humanity's cultural and linguistic heritage, yet Old English remains critically under-resourced, limiting its accessibility to modern natural language processing (NLP) techniques. We present a scalable framework that uses advanced large language models (LLMs) to generate high-quality Old English texts, addressing this gap. Our approach combines parameter-efficient fine-tuning (Low-Rank Adaptation, LoRA), data augmentation via backtranslation, and a dual-agent pipeline that separates the tasks of content generation (in English) and translation (into Old English). Evaluation with automated metrics (BLEU, METEOR, and CHRF) shows significant improvements over baseline models, with BLEU scores increasing from 26 to over 65 for English-to-Old English translation. Expert human assessment also confirms high grammatical accuracy and stylistic fidelity in the generated texts. Beyond expanding the Old English corpus, our method offers a practical blueprint for revitalizing other endangered languages, effectively uniting AI innovation with the goals of cultural preservation.", 'abstract_zh': '保护古代语言是理解人类文化与语言遗产的关键，然而古英语仍严重缺乏资源，限制了其在现代自然语言处理技术中的应用。我们提出了一种可扩展的框架，利用先进的大型语言模型生成高质量的古英语文本，填补这一空白。我们的方法结合了参数高效微调（低秩适应，LoRA）、通过后翻译的数据增强以及一个分离内容生成（英文）和翻译（古英语）任务的双重代理管道。使用自动评估指标（BLEU、METEOR和CHRF）的评估显示，与基线模型相比，古英语翻译的BLEU分数从26显著提高到超过65。专家的人类评估也证实了生成文本在语法准确性和风格保真度方面的高质量。除了扩大古英语语料库外，该方法还提供了一种实用的蓝图，用于复兴其他濒危语言，有效地将人工智能创新与文化保护的目标结合起来。', 'title_zh': 'AI驱动的古英语生成：低资源语言的一种框架'}
{'arxiv_id': 'arXiv:2507.20109', 'title': 'Learning to Align Human Code Preferences', 'authors': 'Xin Yin, Chao Ni, Liushan Chen, Xiaohu Yang', 'link': 'https://arxiv.org/abs/2507.20109', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable potential in automating software development tasks. While recent advances leverage Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to align models with human preferences, the optimal training strategy remains unclear across diverse code preference scenarios. This paper systematically investigates the roles of SFT and DPO in aligning LLMs with different code preferences. Through both theoretical analysis and empirical observation, we hypothesize that SFT excels in scenarios with objectively verifiable optimal solutions, while applying SFT followed by DPO (S&D) enables models to explore superior solutions in scenarios without objectively verifiable optimal solutions. Based on the analysis and experimental evidence, we propose Adaptive Preference Optimization (APO), a dynamic integration approach that adaptively amplifies preferred responses, suppresses dispreferred ones, and encourages exploration of potentially superior solutions during training. Extensive experiments across six representative code preference tasks validate our theoretical hypotheses and demonstrate that APO consistently matches or surpasses the performance of existing SFT and S&D strategies. Our work provides both theoretical foundations and practical guidance for selecting appropriate training strategies in different code preference alignment scenarios.', 'abstract_zh': '大型语言模型（LLMs）在自动化软件开发任务方面展现了显著潜力。尽管最近的进步利用了监督微调（SFT）和直接偏好优化（DPO）来使模型与人类偏好相一致，但在不同的代码偏好场景中，最优训练策略仍然不清楚。本文系统地探讨了SFT和DPO在使LLMs与不同代码偏好相一致中的角色。通过理论分析和实证观察，我们假设SFT在客观可验证的最优解场景中表现出色，而在没有客观可验证的最优解场景中，先进行SFT再进行DPO（S&D）能够使模型探索更优秀的解。基于分析和实验证据，我们提出了自适应偏好优化（APO），该方法动态地放大偏好响应、抑制不偏好响应，并在训练过程中鼓励探索潜在的更优秀解。在六项代表性的代码偏好任务上的广泛实验验证了我们的理论假设，并证明APO在所有任务上都能一致地匹配或超越现有SFT和S&D策略的性能。我们的工作为不同代码偏好对齐场景下选择合适的训练策略提供了理论基础和实践指导。', 'title_zh': '学习对齐人类代码偏好'}
{'arxiv_id': 'arXiv:2507.20059', 'title': 'RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge Retrieval Augmentation', 'authors': 'Ran Xu, Yuchen Zhuang, Yue Yu, Haoyu Wang, Wenqi Shi, Carl Yang', 'link': 'https://arxiv.org/abs/2507.20059', 'abstract': 'Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved at inference time. While RAG demonstrates strong performance on benchmarks largely derived from general-domain corpora like Wikipedia, its effectiveness under realistic, diverse retrieval scenarios remains underexplored. We evaluated RAG systems using MassiveDS, a large-scale datastore with mixture of knowledge, and identified critical limitations: retrieval mainly benefits smaller models, rerankers add minimal value, and no single retrieval source consistently excels. Moreover, current LLMs struggle to route queries across heterogeneous knowledge sources. These findings highlight the need for adaptive retrieval strategies before deploying RAG in real-world settings. Our code and data can be found at this https URL.', 'abstract_zh': '检索增强生成（RAG）通过在推理时集成外部知识来增强大型语言模型（LLMs）。尽管RAG在大量源自通用领域语料库（如维基百科）的基准测试中表现出色，但其在现实多样检索场景下的有效性尚未充分探索。我们使用MassiveDS大规模知识存储库进行了RAG系统评估，并发现关键限制：检索主要有利于小型模型，再排序器增加的价值有限，也没有单一的检索来源始终表现出色。此外，当前的LLMs难以跨异构知识源路由查询。这些发现强调，在实际应用中部署RAG之前需要采用适应性检索策略。我们的代码和数据可在以下网址找到。', 'title_zh': 'RAG在野外：混合知识检索增强的大型语言模型效用研究'}
{'arxiv_id': 'arXiv:2507.20028', 'title': 'TAPS : Frustratingly Simple Test Time Active Learning for VLMs', 'authors': 'Dhruv Sarkar, Aprameyo Chakrabartty, Bibhudatta Bhanja', 'link': 'https://arxiv.org/abs/2507.20028', 'abstract': 'Test-Time Optimization enables models to adapt to new data during inference by updating parameters on-the-fly. Recent advances in Vision-Language Models (VLMs) have explored learning prompts at test time to improve performance in downstream tasks. In this work, we extend this idea by addressing a more general and practical challenge: Can we effectively utilize an oracle in a continuous data stream where only one sample is available at a time, requiring an immediate query decision while respecting latency and memory constraints? To tackle this, we propose a novel Test-Time Active Learning (TTAL) framework that adaptively queries uncertain samples and updates prompts dynamically. Unlike prior methods that assume batched data or multiple gradient updates, our approach operates in a real-time streaming scenario with a single test sample per step. We introduce a dynamically adjusted entropy threshold for active querying, a class-balanced replacement strategy for memory efficiency, and a class-aware distribution alignment technique to enhance adaptation. The design choices are justified using careful theoretical analysis. Extensive experiments across 10 cross-dataset transfer benchmarks and 4 domain generalization datasets demonstrate consistent improvements over state-of-the-art methods while maintaining reasonable latency and memory overhead. Our framework provides a practical and effective solution for real-world deployment in safety-critical applications such as autonomous systems and medical diagnostics.', 'abstract_zh': '基于测试时优化的测试时主动学习框架使得模型能够在推理过程中适应新数据并通过即刻更新参数来适应变化。通过在连续数据流中仅利用单个样本进行即时查询决策，我们提出了一种新颖的测试时主动学习（TTAL）框架，以有效利用先验知识并动态更新提示。该框架适用于单样本实时流场景，不同于以前假设批量数据或多次梯度更新的方法。我们引入了动态调整的熵阈值进行主动查询，以提高内存效率的类平衡替换策略，以及增强适应性的类觉察分布对齐技术。理论分析表明这些设计选择是合理的。实验结果表明，该框架在10个跨数据集迁移基准和4个领域泛化数据集上相较于最先进的方法具有持续改进，并且保持了合理的延迟和内存开销。我们的框架为自主系统和医疗诊断等关键应用的实际部署提供了一种实用而有效的方法。', 'title_zh': 'TAPS : 极其简单的测试时主动学习方法 for VLMs'}
{'arxiv_id': 'arXiv:2507.20018', 'title': 'The Carbon Cost of Conversation, Sustainability in the Age of Language Models', 'authors': 'Sayed Mahbub Hasan Amiri, Prasun Goswami, Md. Mainul Islam, Mohammad Shakhawat Hossen, Sayed Majhab Hasan Amiri, Naznin Akter', 'link': 'https://arxiv.org/abs/2507.20018', 'abstract': 'Large language models (LLMs) like GPT-3 and BERT have revolutionized natural language processing (NLP), yet their environmental costs remain dangerously overlooked. This article critiques the sustainability of LLMs, quantifying their carbon footprint, water usage, and contribution to e-waste through case studies of models such as GPT-4 and energy-efficient alternatives like Mistral 7B. Training a single LLM can emit carbon dioxide equivalent to hundreds of cars driven annually, while data centre cooling exacerbates water scarcity in vulnerable regions. Systemic challenges corporate greenwashing, redundant model development, and regulatory voids perpetuate harm, disproportionately burdening marginalized communities in the Global South. However, pathways exist for sustainable NLP: technical innovations (e.g., model pruning, quantum computing), policy reforms (carbon taxes, mandatory emissions reporting), and cultural shifts prioritizing necessity over novelty. By analysing industry leaders (Google, Microsoft) and laggards (Amazon), this work underscores the urgency of ethical accountability and global cooperation. Without immediate action, AIs ecological toll risks outpacing its societal benefits. The article concludes with a call to align technological progress with planetary boundaries, advocating for equitable, transparent, and regenerative AI systems that prioritize both human and environmental well-being.', 'abstract_zh': '大型语言模型（LLMs）如GPT-3和BERT重塑了自然语言处理（NLP），但其环境成本仍然被危险地忽视。本文批评了LLMs的可持续性，通过GPT-4等模型和节能替代品Mistral 7B的案例研究，定量评估其碳足迹、水资源消耗及其对电子废物的贡献。训练单个LLM所产生的二氧化碳相当于数百辆汽车一年的排放量，而数据中心冷却进一步加剧了脆弱地区水资源短缺。系统性挑战包括企业绿色漂洗、冗余模型开发以及监管空白，这些都加剧了危害，不成比例地影响着全球南方的边缘化社区。然而，可持续NLP的道路存在：技术创新（如模型剪裁、量子计算），政策改革（如碳税、强制性排放报告），以及文化转变，优先考虑必要性而非新颖性。通过分析行业领导者（谷歌、微软）和落后者（亚马逊），本文强调了道德问责和全球合作的迫切性。如果不采取立即行动，人工智能的生态成本可能会超过其社会收益。文章最后呼吁将技术进步与行星边界相一致，倡导公平、透明和再生的人工智能系统，既重视人类福祉也重视环境福祉。', 'title_zh': '语言模型时代交流的碳成本：可持续性探究'}
{'arxiv_id': 'arXiv:2507.19990', 'title': 'Improving the Performance of Sequential Recommendation Systems with an Extended Large Language Model', 'authors': 'Sinnyum Choi, Woong Kim', 'link': 'https://arxiv.org/abs/2507.19990', 'abstract': 'Recently, competition in the field of artificial intelligence (AI) has intensified among major technological companies, resulting in the continuous release of new large-language models (LLMs) that exhibit improved language understanding and context-based reasoning capabilities. It is expected that these advances will enable more efficient personalized recommendations in LLM-based recommendation systems through improved quality of training data and architectural design. However, many studies have not considered these recent developments. In this study, it was proposed to improve LLM-based recommendation systems by replacing Llama2 with Llama3 in the LlamaRec framework. To ensure a fair comparison, random seed values were set and identical input data was provided during preprocessing and training. The experimental results show average performance improvements of 38.65\\%, 8.69\\%, and 8.19\\% for the ML-100K, Beauty, and Games datasets, respectively, thus confirming the practicality of this method. Notably, the significant improvements achieved by model replacement indicate that the recommendation quality can be improved cost-effectively without the need to make structural changes to the system. Based on these results, it is our contention that the proposed approach is a viable solution for improving the performance of current recommendation systems.', 'abstract_zh': '近年来，主要科技公司在人工智能领域的竞争加剧，导致不断推出新的大语言模型（LLMs），这些模型在语言理解和上下文推理能力上有所提升。预计这些进步将通过提高训练数据质量和架构设计，使基于LLM的推荐系统能够实现更高效的个性化推荐。然而，许多研究尚未考虑这些最新发展。本研究提出通过在LlamaRec框架中用Llama3替换Llama2来改进基于LLM的推荐系统。为了确保比较的公平性，在预处理和训练过程中设定了随机种子值，并提供了相同的输入数据。实验结果显示，与ML-100K、Beauty和Games数据集相比，平均性能分别提高了38.65%、8.69%和8.19%，这证实了该方法的实际可行性。值得注意的是，模型替换所取得的显著改进表明，通过优化模型本身即可有效提高推荐质量，无需对系统进行结构性改动。基于这些结果，我们认为提出的方法是提高现有推荐系统性能的一种可行方案。', 'title_zh': '使用扩展的大语言模型提高序列推荐系统的性能'}
{'arxiv_id': 'arXiv:2507.19909', 'title': 'The Impact of Fine-tuning Large Language Models on Automated Program Repair', 'authors': 'Roman Macháček, Anastasiia Grishina, Max Hort, Leon Moonen', 'link': 'https://arxiv.org/abs/2507.19909', 'abstract': 'Automated Program Repair (APR) uses various tools and techniques to help developers achieve functional and error-free code faster. In recent years, Large Language Models (LLMs) have gained popularity as components in APR tool chains because of their performance and flexibility. However, training such models requires a significant amount of resources. Fine-tuning techniques have been developed to adapt pre-trained LLMs to specific tasks, such as APR, and enhance their performance at far lower computational costs than training from scratch. In this study, we empirically investigate the impact of various fine-tuning techniques on the performance of LLMs used for APR. Our experiments provide insights into the performance of a selection of state-of-the-art LLMs pre-trained on code. The evaluation is done on three popular APR benchmarks (i.e., QuixBugs, Defects4J and HumanEval-Java) and considers six different LLMs with varying parameter sizes (resp. CodeGen, CodeT5, StarCoder, DeepSeekCoder, Bloom, and CodeLlama-2). We consider three training regimens: no fine-tuning, full fine-tuning, and parameter-efficient fine-tuning (PEFT) using LoRA and IA3. We observe that full fine-tuning techniques decrease the benchmarking performance of various models due to different data distributions and overfitting. By using parameter-efficient fine-tuning methods, we restrict models in the amount of trainable parameters and achieve better results.\nKeywords: large language models, automated program repair, parameter-efficient fine-tuning, AI4Code, AI4SE, ML4SE.', 'abstract_zh': '自动程序修复中的大型语言模型细调：一种基于AI4Code和AI4SE的参数高效方法', 'title_zh': '大型语言模型微调对自动化程序修复的影响'}
{'arxiv_id': 'arXiv:2507.19904', 'title': 'CrossPL: Evaluating Large Language Models on Cross Programming Language Code Generation', 'authors': 'Zhanhang Xiong, Dongxia Wang, Yuekang Li, Xinyuan An, Wenhai Wang', 'link': 'https://arxiv.org/abs/2507.19904', 'abstract': "As large language models (LLMs) become increasingly embedded in software engineering workflows, a critical capability remains underexplored: generating correct code that enables cross-programming-language (CPL) interoperability. This skill is essential for building complex systems that integrate components written in multiple languages via mechanisms like inter-process communication (IPC). To bridge this gap, we present CrossPL, the first benchmark designed to systematically evaluate LLMs' ability to generate CPL-interoperating code. CrossPL comprises 1,982 tasks centered around IPC, covering six widely-used programming languages and seven representative CPL techniques. We construct this benchmark by (i) analyzing 19,169 multi-language GitHub repositories using 156 hand-crafted finite state machines (FSMs), and (ii) developing an LLM-based pipeline that automatically extracts CPL code snippets, generates task instructions, and validates functional correctness. We evaluate 14 state-of-the-art general-purpose LLMs and 6 code-oriented LLMs released in the past three years on CrossPL via FSM-based validation. Results reveal that even the best-performing models struggle with CPL scenarios, underscoring the need for more targeted research in this space. Our benchmark and code are available at: this https URL.", 'abstract_zh': '随着大型语言模型（LLMs）在软件工程工作流中的应用日益增多，一种关键能力仍被忽视：生成能够实现跨编程语言（CPL）互操作的正确代码。这项能力对于通过进程间通信（IPC）等机制集成多种语言编写的组件以构建复杂系统至关重要。为解决这一问题，我们提出了CrossPL，这是首个旨在系统评估LLMs生成CPL互操作代码能力的基准测试。CrossPL包含1,982项围绕IPC的任务，覆盖六种广泛使用的编程语言以及七种代表性的CPL技术。我们通过以下方式构建此基准测试：（i）使用156个手工设计的状态机（FSMs）分析了19,169个多语言GitHub仓库；（ii）开发了一种基于LLM的自动化流水线，用于自动提取CPL代码片段、生成任务指令并验证功能正确性。我们使用基于状态机的验证方法，评估了过去三年内发布的14种最先进的通用语言模型和6种代码导向语言模型在CrossPL上的表现。结果表明，即使性能最好的模型也难以处理CPL场景，突显了在该领域进行更有针对性研究的必要性。我们的基准测试和代码可在以下链接获取：this https URL。', 'title_zh': '跨语言代码生成：评估大型语言模型在不同编程语言代码生成任务上的表现'}
{'arxiv_id': 'arXiv:2507.19902', 'title': 'AgentMesh: A Cooperative Multi-Agent Generative AI Framework for Software Development Automation', 'authors': 'Sourena Khanzadeh', 'link': 'https://arxiv.org/abs/2507.19902', 'abstract': 'Software development is a complex, multi-phase process traditionally requiring collaboration among individuals with diverse expertise. We propose AgentMesh, a Python-based framework that uses multiple cooperating LLM-powered agents to automate software development tasks. In AgentMesh, specialized agents - a Planner, Coder, Debugger, and Reviewer - work in concert to transform a high-level requirement into fully realized code. The Planner agent first decomposes user requests into concrete subtasks; the Coder agent implements each subtask in code; the Debugger agent tests and fixes the code; and the Reviewer agent validates the final output for correctness and quality. We describe the architecture and design of these agents and their communication, and provide implementation details including prompt strategies and workflow orchestration. A case study illustrates AgentMesh handling a non-trivial development request via sequential task planning, code generation, iterative debugging, and final code review. We discuss how dividing responsibilities among cooperative agents leverages the strengths of large language models while mitigating single-agent limitations. Finally, we examine current limitations - such as error propagation and context scaling - and outline future work toward more robust, scalable multi-agent AI systems for software engineering automation.', 'abstract_zh': '基于Python的AgentMesh框架：多协作LLM驱动的软件开发自动化', 'title_zh': 'AgentMesh：一种协作式多智能体生成AI软件开发自动化框架'}
{'arxiv_id': 'arXiv:2507.19849', 'title': 'Agentic Reinforced Policy Optimization', 'authors': 'Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, Zhicheng Dou', 'link': 'https://arxiv.org/abs/2507.19849', 'abstract': "Large-scale reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in harnessing the potential of large language models (LLMs) for single-turn reasoning tasks. In realistic reasoning scenarios, LLMs can often utilize external tools to assist in task-solving processes. However, current RL algorithms inadequately balance the models' intrinsic long-horizon reasoning capabilities and their proficiency in multi-turn tool interactions. To bridge this gap, we propose Agentic Reinforced Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training multi-turn LLM-based agents. Through preliminary experiments, we observe that LLMs tend to exhibit highly uncertain behavior, characterized by an increase in the entropy distribution of generated tokens, immediately following interactions with external tools. Motivated by this observation, ARPO incorporates an entropy-based adaptive rollout mechanism, dynamically balancing global trajectory sampling and step-level sampling, thereby promoting exploration at steps with high uncertainty after tool usage. By integrating an advantage attribution estimation, ARPO enables LLMs to internalize advantage differences in stepwise tool-use interactions. Our experiments across 13 challenging benchmarks in computational reasoning, knowledge reasoning, and deep search domains demonstrate ARPO's superiority over trajectory-level RL algorithms. Remarkably, ARPO achieves improved performance using only half of the tool-use budget required by existing methods, offering a scalable solution for aligning LLM-based agents with real-time dynamic environments. Our code and datasets are released at this https URL", 'abstract_zh': '可验证奖励的大规模强化学习（RLVR）在利用大型语言模型（LLMs）进行单轮推理任务方面展示了其有效性。在现实推理场景中，LLMs通常可以利用外部工具来辅助任务解决过程。然而，当前的RL算法未能有效地平衡模型的固有长时推理能力与其在多轮工具交互方面的熟练程度。为了解决这一问题，我们提出了Agent Reinforced Policy Optimization (ARPO)，一种专门用于训练基于多轮LLM的代理的新型代理RL算法。初步实验表明，LLMs在与外部工具交互后倾向于表现出高度不确定的行为，表现为生成token的熵分布增加。受此观察的启发，ARPO引入了一种基于熵的自适应展开机制，动态平衡全局轨迹采样和步骤级采样，从而在工具使用后的高不确定步骤中促进探索。通过结合优势归因估计，ARPO使LLMs能够内化步骤级工具使用交互中的优势差异。我们在包括13个具有挑战性的计算推理、知识推理和深度搜索领域的基准测试中，显示出ARPO优于轨迹级RL算法的优势。值得注意的是，ARPO仅使用现有方法所需的一半工具使用预算就能实现更好的性能，提供了一种可扩展的解决方案，用于使LLM基代理与实时动态环境对齐。我们的代码和数据集在此处发布。', 'title_zh': '代理强化策略优化'}
{'arxiv_id': 'arXiv:2507.19823', 'title': 'HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs', 'authors': 'Dongquan Yang, Yifan Yang, Xiaotian Yu, Xianbiao Qi, Rong Xiao', 'link': 'https://arxiv.org/abs/2507.19823', 'abstract': 'Processing long-context inputs with large language models presents a significant challenge due to the enormous memory requirements of the Key-Value (KV) cache during inference. Existing KV cache compression methods exhibit noticeable performance degradation when memory is reduced by more than 85%. Additionally, strategies that leverage GPU-CPU collaboration for approximate attention remain underexplored in this setting. We propose HCAttention, a heterogeneous attention computation framework that integrates key quantization, value offloading, and dynamic KV eviction to enable efficient inference under extreme memory constraints. The method is compatible with existing transformer architectures and does not require model fine-tuning. Experimental results on the LongBench benchmark demonstrate that our approach preserves the accuracy of full-attention model while shrinking the KV cache memory footprint to 25% of its original size. Remarkably, it stays competitive with only 12.5% of the cache, setting a new state-of-the-art in LLM KV cache compression. To the best of our knowledge, HCAttention is the first to extend the Llama-3-8B model to process 4 million tokens on a single A100 GPU with 80GB memory.', 'abstract_zh': '用大规模语言模型处理长上下文输入面临着显著挑战，因为在推理过程中，关键值（KV）缓存需要大量的内存。现有的KV缓存压缩方法在内存缩减超过85%时会出现明显的性能下降。此外，利用GPU-CPU协同进行近似注意机制的策略在这个场景下仍处于探索之中。我们提出了HCAttention，这是一种异构注意计算框架，结合了键的量化、值的卸载和动态KV淘汰，以在极端内存约束下实现高效的推理。该方法与现有的变压器架构兼容，不需要对模型进行微调。在LongBench基准测试上的实验结果表明，我们的方法在保持全注意模型准确性的基础上，将KV缓存的内存占用缩减至原大小的25%。更值得注意的是，仅使用12.5%的缓存时，它仍然具有竞争力，创下了新的语言模型KV缓存压缩的最先进技术状态。据我们所知，HCAttention是首个将Llama-3-8B模型扩展到单个具有80GB内存的A100 GPU上处理400万令牌的方案。', 'title_zh': 'HCAttention: 极端键值缓存压缩的异质注意计算方法用于大型语言模型'}
{'arxiv_id': 'arXiv:2507.19771', 'title': 'Large Language Model Agent for Structural Drawing Generation Using ReAct Prompt Engineering and Retrieval Augmented Generation', 'authors': 'Xin Zhang, Lissette Iturburu, Juan Nicolas Villamizar, Xiaoyu Liu, Manuel Salmeron, Shirley J.Dyke, Julio Ramirez', 'link': 'https://arxiv.org/abs/2507.19771', 'abstract': "Structural drawings are widely used in many fields, e.g., mechanical engineering, civil engineering, etc. In civil engineering, structural drawings serve as the main communication tool between architects, engineers, and builders to avoid conflicts, act as legal documentation, and provide a reference for future maintenance or evaluation needs. They are often organized using key elements such as title/subtitle blocks, scales, plan views, elevation view, sections, and detailed sections, which are annotated with standardized symbols and line types for interpretation by engineers and contractors. Despite advances in software capabilities, the task of generating a structural drawing remains labor-intensive and time-consuming for structural engineers. Here we introduce a novel generative AI-based method for generating structural drawings employing a large language model (LLM) agent. The method incorporates a retrieval-augmented generation (RAG) technique using externally-sourced facts to enhance the accuracy and reliability of the language model. This method is capable of understanding varied natural language descriptions, processing these to extract necessary information, and generating code to produce the desired structural drawing in AutoCAD. The approach developed, demonstrated and evaluated herein enables the efficient and direct conversion of a structural drawing's natural language description into an AutoCAD drawing, significantly reducing the workload compared to current working process associated with manual drawing production, facilitating the typical iterative process of engineers for expressing design ideas in a simplified way.", 'abstract_zh': '结构图在机械工程、土木工程等多个领域广泛应用。在土木工程中，结构图是建筑师、工程师和施工人员之间主要的沟通工具，用于避免冲突、作为法律文件使用，并为未来的维修或评估提供参考。它们通常使用标题/副标题块、比例尺、平面图、立面图、剖面图和详图等关键要素进行组织，并用标准化符号和线条类型进行标注，以便工程师和承包商进行解读。尽管软件能力不断提高，但生成结构图的任务仍对结构工程师来说劳动密集且耗时。这里介绍一种基于生成式AI的方法，利用大型语言模型（LLM）代理生成结构图，该方法结合了检索增强生成（RAG）技术，利用外部事实增强语言模型的准确性和可靠性。该方法能够理解各种自然语言描述，处理这些描述以提取必要信息，并生成代码以在AutoCAD中生成所需的结构图。本文中开发的方法不仅展示了其实现过程，还对其进行了评估，显著减少了与手工绘制相关的繁琐工作，简化了工程师的设计表达过程。', 'title_zh': '使用ReAct提示工程和检索增强生成的结构绘图大型语言模型代理'}
{'arxiv_id': 'arXiv:2507.19766', 'title': "UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities", 'authors': 'Dong Du, Shulin Liu, Tao Yang, Shaohua Chen, Yang Li', 'link': 'https://arxiv.org/abs/2507.19766', 'abstract': "Recent advances in large language models (LLMs) have highlighted the potential of reinforcement learning with verifiable rewards (RLVR) to enhance reasoning capabilities through extended output sequences. However, traditional RL frameworks face inefficiencies when handling ultra-long outputs due to long-tail sequence distributions and entropy collapse during training. To address these challenges, we propose an Ultra-Long Output Reinforcement Learning (UloRL) approach for advancing large language models' reasoning abilities. Specifically, we divide ultra long output decoding into short segments, enabling efficient training by mitigating delays caused by long-tail samples. Additionally, we introduce dynamic masking of well-Mastered Positive Tokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the effectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment rollout achieved 2.06x increase in training speed, while RL training with 128k-token outputs improves the model's performance on AIME2025 from 70.9\\% to 85.1\\% and on BeyondAIME from 50.7\\% to 61.9\\%, even surpassing Qwen3-235B-A22B with remarkable gains. These findings underscore the potential of our methods to advance the reasoning capabilities of LLMs with ultra-long sequence generation. We will release our code and model for further use by the community.", 'abstract_zh': 'Recent Advances in Large Language Models Through Ultra-Long Output Reinforcement Learning with Verifiable Rewards', 'title_zh': 'UloRL：一种促进大型语言模型推理能力提升的超长输出强化学习方法'}
{'arxiv_id': 'arXiv:2507.19737', 'title': 'Predicting Human Mobility in Disasters via LLM-Enhanced Cross-City Learning', 'authors': 'Yinzhou Tang, Huandong Wang, Xiaochen Fan, Yong Li', 'link': 'https://arxiv.org/abs/2507.19737', 'abstract': 'The vulnerability of cities to natural disasters has increased with urbanization and climate change, making it more important to predict human mobility in the disaster scenarios for downstream tasks including location-based early disaster warning and pre-allocating rescue resources, etc. However, existing human mobility prediction models are mainly designed for normal scenarios, and fail to adapt to disaster scenarios due to the shift of human mobility patterns under disaster. To address this issue, we introduce \\textbf{DisasterMobLLM}, a mobility prediction framework for disaster scenarios that can be integrated into existing deep mobility prediction methods by leveraging LLMs to model the mobility intention and transferring the common knowledge of how different disasters affect mobility intentions between cities. This framework utilizes a RAG-Enhanced Intention Predictor to forecast the next intention, refines it with an LLM-based Intention Refiner, and then maps the intention to an exact location using an Intention-Modulated Location Predictor. Extensive experiments illustrate that DisasterMobLLM can achieve a 32.8\\% improvement in terms of Acc@1 and a 35.0\\% improvement in terms of the F1-score of predicting immobility compared to the baselines. The code is available at this https URL.', 'abstract_zh': '城市在自然灾难中的脆弱性随着城市化进程和气候变化而增加，因此在灾难情景下预测人类移动性以进行基于位置的早期灾害预警和预先分配救援资源等下游任务变得更加重要。然而，现有的人类移动性预测模型主要针对正常情景设计，因在灾难情景下人类移动性模式的转变而无法适应。为了解决这一问题，我们引入了DisasterMobLLM，这是一种利用LLM建模移动意图并利用不同城市在不同灾害下影响移动意图的通用知识来整合到现有深度移动性预测方法中的灾难情景下移动性预测框架。该框架利用RAG增强意图预测器来预测下一个意图，使用基于LLM的意图精炼器对其进行精炼，然后使用意图调制位置预测器将意图映射到确切的位置。广泛的实验表明，DisasterMobLLM在Acc@1上可以实现32.8%的改进，在预测不移动性方面的F1分数上可以实现35.0%的改进，相较于基线方法。代码可在以下网址获得。', 'title_zh': '通过LLM增强的跨城市学习预测灾害中的人类 mobility'}
{'arxiv_id': 'arXiv:2507.19643', 'title': "Can You Share Your Story? Modeling Clients' Metacognition and Openness for LLM Therapist Evaluation", 'authors': 'Minju Kim, Dongje Yoo, Yeonjun Hwang, Minseok Kang, Namyoung Kim, Minju Gwak, Beong-woo Kwak, Hyungjoo Chae, Harim Kim, Yunjoong Lee, Min Hee Kim, Dayi Jung, Kyong-Mee Chung, Jinyoung Yeo', 'link': 'https://arxiv.org/abs/2507.19643', 'abstract': "Understanding clients' thoughts and beliefs is fundamental in counseling, yet current evaluations of LLM therapists often fail to assess this ability. Existing evaluation methods rely on client simulators that clearly disclose internal states to the therapist, making it difficult to determine whether an LLM therapist can uncover unexpressed perspectives. To address this limitation, we introduce MindVoyager, a novel evaluation framework featuring a controllable and realistic client simulator which dynamically adapts itself based on the ongoing counseling session, offering a more realistic and challenging evaluation environment. We further introduce evaluation metrics that assess the exploration ability of LLM therapists by measuring their thorough understanding of client's beliefs and thoughts.", 'abstract_zh': '理解客户的思想和信念是咨询中的 fundamentals，然而当前对大语言模型治疗师的评估往往未能考察这一能力。现有的评估方法依赖于清楚披露内部状态的客户模拟器，这使得难以判断大语言模型治疗师是否能够揭示未表达的观点。为解决这一局限，我们介绍了 MindVoyager，一种新颖的评估框架，该框架具有可控且真实的客户模拟器，并根据正在进行的咨询会话动态调整自身，提供更真实且具有挑战性的评估环境。我们还介绍了评估指标，通过衡量大语言模型治疗师对客户信念和思想的深入了解程度来评估其探索能力。', 'title_zh': '你能分享你的故事吗？：建模客户的元认知和开放性以评估语言模型 therapists'}
{'arxiv_id': 'arXiv:2507.19634', 'title': 'MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks', 'authors': 'Sara Papi, Maike Züfle, Marco Gaido, Beatrice Savoldi, Danni Liu, Ioannis Douros, Luisa Bentivogli, Jan Niehues', 'link': 'https://arxiv.org/abs/2507.19634', 'abstract': "Recent advances in large language models have catalyzed the development of multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to general-purpose instruction-following models, a key frontier lies in evaluating their multilingual and multimodal capabilities over both long and short contexts. However, existing benchmarks fall short in evaluating these dimensions jointly: they are often limited to English, mostly focus on one single modality at a time, rely on short-form contexts, or lack human annotations--hindering comprehensive assessment of model performance across languages, modalities, and task complexity. To address these gaps, we introduce MCIF (Multimodal Crosslingual Instruction Following), the first multilingual human-annotated benchmark based on scientific talks that is designed to evaluate instruction-following in crosslingual, multimodal settings over both short- and long-form inputs. MCIF spans three core modalities--speech, vision, and text--and four diverse languages (English, German, Italian, and Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret instructions across languages and combine them with multimodal contextual information. MCIF is released under a CC-BY 4.0 license to encourage open research and progress in MLLMs development.", 'abstract_zh': '近期大规模语言模型的进展促进了多模态大型语言模型（MLLMs）的发展，这些模型在统一框架内融合了文本、语音和视觉信息。随着MLLMs从狭窄的单语言、任务特定系统发展成为通用的指令遵循模型，一个关键的前沿领域是在长短上下文中评估它们的多语言和多模态能力。然而，现有的基准在联合评估这些维度方面存在不足：它们往往仅限于英语，大多仅集中于单一模态，依赖于短形式的上下文，或者缺乏人类注释——这妨碍了对模型在不同语言、模态和任务复杂度方面的全面评估。为填补这些空白，我们推出了MCIF（跨语言多模态指令遵循），这是首个基于科学讲座的多语言人工标注基准，旨在评估跨语言和多模态设置下的指令遵循能力，包括短形式和长形式的输入。MCIF涵盖了三种核心模态——语音、视觉和文本——以及四种多样的语言（英语、德语、意大利语和中文），从而全面评估MLLMs在不同语言和结合多模态上下文信息方面的解释指令能力。MCIF以CC-BY 4.0许可证发布，以促进开放研究和MLLMs的发展。', 'title_zh': 'MCIF: 多模态跨语言指令跟随基准来自科学讲座'}
{'arxiv_id': 'arXiv:2507.19598', 'title': 'MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?', 'authors': 'Muntasir Wahed, Xiaona Zhou, Kiet A. Nguyen, Tianjiao Yu, Nirav Diwan, Gang Wang, Dilek Hakkani-Tür, Ismini Lourentzou', 'link': 'https://arxiv.org/abs/2507.19598', 'abstract': 'Recent advancements in Large Language Models (LLMs) have significantly enhanced their code generation capabilities. However, their robustness against adversarial misuse, particularly through multi-turn malicious coding prompts, remains underexplored. In this work, we introduce code decomposition attacks, where a malicious coding task is broken down into a series of seemingly benign subtasks across multiple conversational turns to evade safety filters. To facilitate systematic evaluation, we introduce \\benchmarkname{}, a large-scale benchmark designed to evaluate the robustness of code LLMs against both single-turn and multi-turn malicious prompts. Empirical results across open- and closed-source models reveal persistent vulnerabilities, especially under multi-turn scenarios. Fine-tuning on MOCHA improves rejection rates while preserving coding ability, and importantly, enhances robustness on external adversarial datasets with up to 32.4% increase in rejection rates without any additional supervision.', 'abstract_zh': 'Recent advancements in大型语言模型（LLMs）显著增强了它们的代码生成能力。然而，它们在对抗恶意使用的鲁棒性，特别是在通过多轮恶意编码提示方面，仍然未得到充分探索。在本工作中，我们引入了代码分解攻击，即将恶意编码任务分解为多轮对话中的一个个看似无害的子任务，以规避安全过滤。为了促进系统的评估，我们引入了\\benchmarkname{}大规模基准，用于评估代码LLMs在面对单轮和多轮恶意提示时的鲁棒性。跨开源和封闭源模型的实证结果显示出持续的漏洞，尤其是多轮场景下。MOCHA上的微调提高了拒绝率同时保留了编码能力，并且重要的是，在外部对抗数据集上增强了鲁棒性，拒绝率最多提高了32.4%，无需额外监督。', 'title_zh': 'MOCHA：代码语言模型在多轮恶意编码提示下是否 robust？'}
{'arxiv_id': 'arXiv:2507.19595', 'title': 'Efficient Attention Mechanisms for Large Language Models: A Survey', 'authors': 'Yutao Sun, Zhenyu Li, Yike Zhang, Tengyu Pan, Bowen Dong, Yuyi Guo, Jianyong Wang', 'link': 'https://arxiv.org/abs/2507.19595', 'abstract': 'Transformer-based architectures have become the prevailing backbone of large language models. However, the quadratic time and memory complexity of self-attention remains a fundamental obstacle to efficient long-context modeling. To address this limitation, recent research has introduced two principal categories of efficient attention mechanisms. Linear attention methods achieve linear complexity through kernel approximations, recurrent formulations, or fastweight dynamics, thereby enabling scalable inference with reduced computational overhead. Sparse attention techniques, in contrast, limit attention computation to selected subsets of tokens based on fixed patterns, block-wise routing, or clustering strategies, enhancing efficiency while preserving contextual coverage. This survey provides a systematic and comprehensive overview of these developments, integrating both algorithmic innovations and hardware-level considerations. In addition, we analyze the incorporation of efficient attention into largescale pre-trained language models, including both architectures built entirely on efficient attention and hybrid designs that combine local and global components. By aligning theoretical foundations with practical deployment strategies, this work aims to serve as a foundational reference for advancing the design of scalable and efficient language models.', 'abstract_zh': '基于Transformer的架构已成为大型语言模型的主导骨干。然而，自注意力的二次时间和内存复杂性仍然是高效长上下文建模的基本障碍。为解决这一局限性，近期研究引入了两种主要的高效注意力机制类别。线性注意力方法通过核近似、递归表示或快速权重动力学实现线性复杂性，从而实现可扩展的推理并减少计算开销。相比之下，稀疏注意力技术限制注意力计算仅针对基于固定模式、块状路由或聚类策略选择的子集中的标记，从而提高效率同时保持上下文覆盖率。本文综述了这些发展的系统和全面概述，整合了算法创新和硬件层面的考虑。此外，我们分析了将高效注意力机制纳入大规模预训练语言模型中的应用，包括完全基于高效注意力机制的架构和结合局部和全局组件的混合设计。通过结合理论基础和实用部署策略，本文旨在成为推动可扩展和高效语言模型设计的基础参考。', 'title_zh': '大型语言模型中高效注意力机制的研究综述'}
{'arxiv_id': 'arXiv:2507.19586', 'title': 'Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning', 'authors': 'Shengyuan Wang, Jie Feng, Tianhui Liu, Dan Pei, Yong Li', 'link': 'https://arxiv.org/abs/2507.19586', 'abstract': 'Large language models (LLMs) possess extensive world knowledge, including geospatial knowledge, which has been successfully applied to various geospatial tasks such as mobility prediction and social indicator prediction. However, LLMs often generate inaccurate geospatial knowledge, leading to geospatial hallucinations (incorrect or inconsistent representations of geospatial information) that compromise their reliability. While the phenomenon of general knowledge hallucination in LLMs has been widely studied, the systematic evaluation and mitigation of geospatial hallucinations remain largely unexplored. To address this gap, we propose a comprehensive evaluation framework for geospatial hallucinations, leveraging structured geospatial knowledge graphs for controlled assessment. Through extensive evaluation across 20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge. Building on these insights, we introduce a dynamic factuality aligning method based on Kahneman-Tversky Optimization (KTO) to mitigate geospatial hallucinations in LLMs, leading to a performance improvement of over 29.6% on the proposed benchmark. Extensive experimental results demonstrate the effectiveness of our benchmark and learning algorithm in enhancing the trustworthiness of LLMs in geospatial knowledge and reasoning tasks.', 'abstract_zh': '大规模语言模型（LLMs）具备广泛的领域知识，包括地理空间知识，这些知识已在移动性预测和社会指标预测等各类地理空间任务中得到成功应用。然而，LLMs常常生成不准确的地理空间知识，导致地理空间幻觉（地理空间信息的不正确或不一致的表示），从而影响其可靠性。尽管LLMs一般知识幻觉的现象已得到广泛研究，但地理空间幻觉的系统性评估与缓解仍较少探讨。为填补这一空白，我们提出了一种全面的地理空间幻觉评估框架，利用结构化的地理空间知识图谱进行受控评估。通过在20个高级LLM上的广泛评估，我们揭示了其地理空间知识中的幻觉。基于这些洞见，我们引入了基于Kahneman-Tversky优化的动态事实对齐方法，以缓解LLMs中的地理空间幻觉，从而在所提出的基准上实现了超过29.6%的性能改进。广泛的实验结果表明，我们的基准和学习算法在增强LLMs在地理空间知识和推理任务中的可信度方面具有有效性。', 'title_zh': '在大型语言模型中减轻地理空间知识幻觉：基准测试与动态事实对齐'}
{'arxiv_id': 'arXiv:2507.19567', 'title': 'Differentiating hype from practical applications of large language models in medicine - a primer for healthcare professionals', 'authors': 'Elisha D.O. Roberson', 'link': 'https://arxiv.org/abs/2507.19567', 'abstract': 'The medical ecosystem consists of the training of new clinicians and researchers, the practice of clinical medicine, and areas of adjacent research. There are many aspects of these domains that could benefit from the application of task automation and programmatic assistance. Machine learning and artificial intelligence techniques, including large language models (LLMs), have been promised to deliver on healthcare innovation, improving care speed and accuracy, and reducing the burden on staff for manual interventions. However, LLMs have no understanding of objective truth that is based in reality. They also represent real risks to the disclosure of protected information when used by clinicians and researchers. The use of AI in medicine in general, and the deployment of LLMs in particular, therefore requires careful consideration and thoughtful application to reap the benefits of these technologies while avoiding the dangers in each context.', 'abstract_zh': '医疗生态系统包括新临床医生和研究人员的培训、临床医学的实践以及相关研究领域。这些领域的许多方面可以从任务自动化和程序化辅助的应用中受益。机器学习和人工智能技术，包括大型语言模型（LLMs），被期望推动医疗创新，提高护理速度和准确性，并减轻人员手动干预的负担。然而，LLMs并不理解基于现实的客观真理。当由临床医生和研究人员使用时，它们还存在泄露受保护信息的真正风险。因此，医疗领域中AI的应用，特别是LLMs的部署，需要谨慎考虑和慎重应用，以在每个情境中充分利用这些技术的同时避免潜在的风险。', 'title_zh': '区分医疗领域大型语言模型的泡沫与实际应用——医疗专业人员入门指南'}
{'arxiv_id': 'arXiv:2507.19562', 'title': 'PennyCoder: Efficient Domain-Specific LLMs for PennyLane-Based Quantum Code Generation', 'authors': 'Abdul Basit, Minghao Shao, Muhammad Haider Asif, Nouhaila Innan, Muhammad Kashif, Alberto Marchisio, Muhammad Shafique', 'link': 'https://arxiv.org/abs/2507.19562', 'abstract': 'The growing demand for robust quantum programming frameworks has unveiled a critical limitation: current large language model (LLM) based quantum code assistants heavily rely on remote APIs, introducing challenges related to privacy, latency, and excessive usage costs. Addressing this gap, we propose PennyCoder, a novel lightweight framework for quantum code generation, explicitly designed for local and embedded deployment to enable on-device quantum programming assistance without external API dependence. PennyCoder leverages a fine-tuned version of the LLaMA 3.1-8B model, adapted through parameter-efficient Low-Rank Adaptation (LoRA) techniques combined with domain-specific instruction tuning optimized for the specialized syntax and computational logic of quantum programming in PennyLane, including tasks in quantum machine learning and quantum reinforcement learning. Unlike prior work focused on cloud-based quantum code generation, our approach emphasizes device-native operability while maintaining high model efficacy. We rigorously evaluated PennyCoder over a comprehensive quantum programming dataset, achieving 44.3% accuracy with our fine-tuned model (compared to 33.7% for the base LLaMA 3.1-8B and 40.1% for the RAG-augmented baseline), demonstrating a significant improvement in functional correctness.', 'abstract_zh': '逐步增长的稳健量子编程框架需求揭示了一个关键限制：当前基于大型语言模型（LLM）的量子代码助手很大程度上依赖远程API，这带来了隐私、延迟和过度使用成本方面的挑战。为此，我们提出了一种新型轻量级量子代码生成框架PennyCoder，专门设计用于本地和嵌入式部署，以实现无需外部API依赖的设备本地量子编程辅助。PennyCoder利用了通过参数高效的低秩适应（LoRA）技术微调的LLaMA 3.1-8B模型，并结合了针对脉动门控量子编程特定语法和计算逻辑的指令微调，包括量子机器学习和量子强化学习任务。与以往侧重云基量子代码生成的工作不同，我们的方法强调设备本地操作能力的同时保持高模型效能。我们在一个全面的量子编程数据集上严格评估了PennyCoder，使用微调模型的准确率为44.3%（相比之下，基本的LLaMA 3.1-8B为33.7%，而增加RAG增益的基本模型为40.1%），证明了功能正确性的显著提升。', 'title_zh': 'PennyCoder：基于PennyLane的高效领域特定大型语言模型代码生成'}
{'arxiv_id': 'arXiv:2507.19549', 'title': 'AccessGuru: Leveraging LLMs to Detect and Correct Web Accessibility Violations in HTML Code', 'authors': 'Nadeen Fathallah, Daniel Hernández, Steffen Staab', 'link': 'https://arxiv.org/abs/2507.19549', 'abstract': 'The vast majority of Web pages fail to comply with established Web accessibility guidelines, excluding a range of users with diverse abilities from interacting with their content. Making Web pages accessible to all users requires dedicated expertise and additional manual efforts from Web page providers. To lower their efforts and promote inclusiveness, we aim to automatically detect and correct Web accessibility violations in HTML code. While previous work has made progress in detecting certain types of accessibility violations, the problem of automatically detecting and correcting accessibility violations remains an open challenge that we address. We introduce a novel taxonomy classifying Web accessibility violations into three key categories - Syntactic, Semantic, and Layout. This taxonomy provides a structured foundation for developing our detection and correction method and redefining evaluation metrics. We propose a novel method, AccessGuru, which combines existing accessibility testing tools and Large Language Models (LLMs) to detect violations and applies taxonomy-driven prompting strategies to correct all three categories. To evaluate these capabilities, we develop a benchmark of real-world Web accessibility violations. Our benchmark quantifies syntactic and layout compliance and judges semantic accuracy through comparative analysis with human expert corrections. Evaluation against our benchmark shows that AccessGuru achieves up to 84% average violation score decrease, significantly outperforming prior methods that achieve at most 50%.', 'abstract_zh': 'Web页面广泛未能遵守现有的Web无障碍指南，排斥了具有各种能力的用户与其内容互动。为了使所有用户都能访问Web页面，需要网页提供者投入专门的知识和额外的手工努力。为了减轻他们的努力并促进包容性，我们旨在自动检测和修正HTML代码中的无障碍违规行为。尽管以前的工作已经在检测某些类型的无障碍违规方面取得进展，但自动检测和纠正无障碍违规的问题仍然是一个开放的挑战，我们对此进行了研究。我们提出了一个新的分类法，将Web无障碍违规行为分为三大类——语法、语义和布局。该分类法为开发我们的检测和修正方法以及重新定义评价指标提供了有组织的基础。我们提出了一种新的方法——AccessGuru，它结合了现有的无障碍测试工具和大规模语言模型（LLMs）来检测违规行为，并采用基于分类法的提示策略来修正三大类违规行为。为了评估这些能力，我们开发了一个现实世界的Web无障碍违规基准。该基准量化了语法和布局合规性，并通过与人类专家修正进行比较分析来评判语义准确性。根据基准的评估显示，AccessGuru 的平均违规得分降低了高达84%，显著优于之前的最优方法，后者仅能降低50%左右。', 'title_zh': 'AccessGuru: 利用大语言模型检测和修正HTML代码中的网页Accessibility违规问题'}
{'arxiv_id': 'arXiv:2507.19526', 'title': 'Quantizing Text-attributed Graphs for Semantic-Structural Integration', 'authors': 'Jianyuan Bo, Hao Wu, Yuan Fang', 'link': 'https://arxiv.org/abs/2507.19526', 'abstract': 'Text-attributed graphs (TAGs) have emerged as a powerful representation for modeling complex relationships across diverse domains. With the rise of large language models (LLMs), there is growing interest in leveraging their capabilities for graph learning. However, current approaches face significant challenges in embedding structural information into LLM-compatible formats, requiring either computationally expensive alignment mechanisms or manual graph verbalization techniques that often lose critical structural details. Moreover, these methods typically require labeled data from source domains for effective transfer learning, significantly constraining their adaptability. We propose STAG, a novel self-supervised framework that directly quantizes graph structural information into discrete tokens using a frozen codebook. Unlike traditional quantization approaches, our method employs soft assignment and KL divergence guided quantization to address the unique challenges of graph data, which lacks natural tokenization structures. Our framework enables both LLM-based and traditional learning approaches, supporting true zero-shot transfer learning without requiring labeled data even in the source domain. Extensive experiments demonstrate state-of-the-art performance across multiple node classification benchmarks while maintaining compatibility with different LLM architectures, offering an elegant solution to bridging graph learning with LLMs.', 'abstract_zh': '基于文本标注的图形（Text-attributed Graphs, TAGs）已 emerges 作为描述跨多个领域复杂关系的强大表示形式。随着大规模语言模型（LLMs）的兴起，人们越来越有兴趣利用它们的能力进行图学习。然而，现有方法在将结构信息嵌入LLM兼容格式时面临着重大挑战，要么需要计算成本高昂的对齐机制，要么需要手动的图语义化技术，后者往往会丢失关键的结构细节。此外，这些方法通常需要来自源领域的标记数据以实现有效的迁移学习，这极大地限制了它们的适应性。我们提出了STAG，一种新颖的自监督框架，直接使用冻结的码本将图结构信息量化为离散的标记。不同于传统的量化方法，我们的方法采用软分配和基于KL散度的量化来解决图数据的独特挑战，而图数据缺乏自然的标记化结构。该框架支持基于LLM和传统学习方法，能够实现真正的零样本迁移学习，即使在源领域也没有需要标记数据。广泛实验表明，STAG在多个节点分类基准测试中取得了目前最佳性能，同时保持与不同LLM架构的兼容性，提供了一种优雅的解决方案，用于将图学习与LLM结合。', 'title_zh': '量化文本 Attribution 的图形以实现语义结构集成'}
{'arxiv_id': 'arXiv:2507.19525', 'title': 'MMCircuitEval: A Comprehensive Multimodal Circuit-Focused Benchmark for Evaluating LLMs', 'authors': 'Chenchen Zhao, Zhengyuan Shi, Xiangyu Wen, Chengjie Liu, Yi Liu, Yunhao Zhou, Yuxiang Zhao, Hefei Feng, Yinan Zhu, Gwok-Waa Wan, Xin Cheng, Weiyu Chen, Yongqi Fu, Chujie Chen, Chenhao Xue, Guangyu Sun, Ying Wang, Yibo Lin, Jun Yang, Ning Xu, Xi Wang, Qiang Xu', 'link': 'https://arxiv.org/abs/2507.19525', 'abstract': 'The emergence of multimodal large language models (MLLMs) presents promising opportunities for automation and enhancement in Electronic Design Automation (EDA). However, comprehensively evaluating these models in circuit design remains challenging due to the narrow scope of existing benchmarks. To bridge this gap, we introduce MMCircuitEval, the first multimodal benchmark specifically designed to assess MLLM performance comprehensively across diverse EDA tasks. MMCircuitEval comprises 3614 meticulously curated question-answer (QA) pairs spanning digital and analog circuits across critical EDA stages - ranging from general knowledge and specifications to front-end and back-end design. Derived from textbooks, technical question banks, datasheets, and real-world documentation, each QA pair undergoes rigorous expert review for accuracy and relevance. Our benchmark uniquely categorizes questions by design stage, circuit type, tested abilities (knowledge, comprehension, reasoning, computation), and difficulty level, enabling detailed analysis of model capabilities and limitations. Extensive evaluations reveal significant performance gaps among existing LLMs, particularly in back-end design and complex computations, highlighting the critical need for targeted training datasets and modeling approaches. MMCircuitEval provides a foundational resource for advancing MLLMs in EDA, facilitating their integration into real-world circuit design workflows. Our benchmark is available at this https URL.', 'abstract_zh': '多模态大型语言模型在电子设计自动化中的涌现为自动化和增强带来了 promising 的机会。然而，由于现有基准的范围狭窄，在电路设计中全面评估这些模型仍然具有挑战性。为解决这一问题，我们引入了 MMCircuitEval，这是首个专门设计用于全面评估多模态大型语言模型在各种电子设计自动化任务中的性能的多模态基准。MMCircuitEval 包含 3614 个经过精心策划的问答（QA）对，覆盖了从数字电路到模拟电路的关键电子设计自动化阶段，题目涉及从基础知识和规格到前端和后端设计的各种方面。这些 QA 对来源于教材、技术问题集、数据表和实际文档，并经过严格的专家审查以确保准确性和相关性。该基准独树一帜地根据设计阶段、电路类型、测试能力（包括知识、理解、推理和计算）以及难度级别对问题进行分类，从而能够详细分析模型的能力和局限性。广泛的研究揭示了现有语言模型在后端设计和复杂计算方面存在显著的性能差距，突显了构建针对特定训练数据集和建模方法的迫切需求。MMCircuitEval 为推进多模态大型语言模型在电子设计自动化中的应用提供了基础资源，有助于其实现实际电路设计工作流程的整合。该基准可通过以下链接获取：this https URL。', 'title_zh': 'MMCircuitEval: 一个全面的多模态电路集中基准，用于评估LLM'}
{'arxiv_id': 'arXiv:2507.19500', 'title': 'Gaze-Aware AI: Mathematical modeling of epistemic experience of the Marginalized for Human-Computer Interaction & AI Systems', 'authors': 'Omkar Suresh Hatti', 'link': 'https://arxiv.org/abs/2507.19500', 'abstract': "The proliferation of artificial intelligence provides an opportunity to create psychological spaciousness in society. Spaciousness is defined as the ability to hold diverse interpersonal interactions and forms the basis for vulnerability that leads to authenticity that leads to prosocial behaviors and thus to societal harmony. This paper demonstrates an attempt to quantify, the human conditioning to subconsciously modify authentic self-expression to fit the norms of the dominant culture. Gaze is explored across various marginalized and intersectional groups, using concepts from postmodern philosophy and psychology. The effects of gaze are studied through analyzing a few redacted Reddit posts, only to be discussed in discourse and not endorsement. A mathematical formulation for the Gaze Pressure Index (GPI)-Diff Composite Metric is presented to model the analysis of two sets of conversational spaces in relation to one another. The outcome includes an equation to train Large Language Models (LLMs) - the working mechanism of AI products such as Chat-GPT; and an argument for affirming and inclusive HCI, based on the equation, is presented. The argument is supported by a few principles of Neuro-plasticity, The brain's lifelong capacity to rewire.", 'abstract_zh': '人工智能的普及为在社会中创造心理空间提供了机会。心理空间被定义为容纳多样的人际交往的能力，这是走向脆弱、真诚、亲社会行为以及社会和谐的基础。本文旨在量化人类无意识地将其真实自我表达适配于主导文化规范的条件作用。通过后现代哲学和心理学的概念，研究了凝视在不同边缘化和交叉群体中的作用，仅通过分析少量隐去内容的Reddit帖子进行探讨，而不作推荐。提出了凝视压力指数（GPI）-差异复合指标的数学公式，以模型化两个对话空间之间的分析。结果包括一个用于训练大规模语言模型（LLMs）的方程，以及基于该方程的论据支持包容性的人机交互。这一论据得到了神经可塑性原则的支持，大脑终生重塑的能力。', 'title_zh': '面向注视的AI：边缘化群体认知体验的数学建模用于人机交互与AI系统'}
{'arxiv_id': 'arXiv:2507.19498', 'title': 'ChatMyopia: An AI Agent for Pre-consultation Education in Primary Eye Care Settings', 'authors': 'Yue Wu, Xiaolan Chen, Weiyi Zhang, Shunming Liu, Wing Man Rita Sum, Xinyuan Wu, Xianwen Shang, Chea-su Kee, Mingguang He, Danli Shi', 'link': 'https://arxiv.org/abs/2507.19498', 'abstract': "Large language models (LLMs) show promise for tailored healthcare communication but face challenges in interpretability and multi-task integration particularly for domain-specific needs like myopia, and their real-world effectiveness as patient education tools has yet to be demonstrated. Here, we introduce ChatMyopia, an LLM-based AI agent designed to address text and image-based inquiries related to myopia. To achieve this, ChatMyopia integrates an image classification tool and a retrieval-augmented knowledge base built from literature, expert consensus, and clinical guidelines. Myopic maculopathy grading task, single question examination and human evaluations validated its ability to deliver personalized, accurate, and safe responses to myopia-related inquiries with high scalability and interpretability. In a randomized controlled trial (n=70, NCT06607822), ChatMyopia significantly improved patient satisfaction compared to traditional leaflets, enhancing patient education in accuracy, empathy, disease awareness, and patient-eyecare practitioner communication. These findings highlight ChatMyopia's potential as a valuable supplement to enhance patient education and improve satisfaction with medical services in primary eye care settings.", 'abstract_zh': '大型语言模型（LLMs）在个性化医疗沟通方面显示出潜力，但在可解释性和多任务集成方面，尤其是在像近视这样的特定领域需求方面面临着挑战，它们作为患者教育工具的真实世界有效性尚未得到证明。为此，我们介绍了基于LLM的AI代理ChatMyopia，旨在处理与近视相关的内容和图像查询。ChatMyopia集成了图像分类工具和从文献、专家共识和临床指南构建的检索增强知识库。针对近视黄斑病变分级任务、单问题评估和人类评估，验证了ChatMyopia能够提供个性化、准确且安全的近视相关查询响应，并具备高可扩展性和可解释性。在一项随机对照试验（n=70，NCT06607822）中，与传统小册子相比，ChatMyopia显著提高了患者满意度，提升了患者教育的准确性、同理心、疾病意识以及患者与眼科保健提供者之间的沟通。这些发现突显了ChatMyopia作为提高初级眼科护理中患者教育和医疗服务满意度有价值的补充的潜力。', 'title_zh': '聚焦聊天：初级眼科护理环境中的人工智能预咨询教育代理'}
