{'arxiv_id': 'arXiv:2507.20589', 'title': 'Methods for the Segmentation of Reticular Structures Using 3D LiDAR Data: A Comparative Evaluation', 'authors': 'Francisco J. Soler Mora, Adrián Peidró Vidal, Marc Fabregat-Jaén, Luis Payá Castelló, Óscar Reinoso García', 'link': 'https://arxiv.org/abs/2507.20589', 'abstract': 'Reticular structures form the backbone of major infrastructure like bridges, pylons, and airports, but their inspection and maintenance are costly and hazardous, often requiring human intervention. While prior research has focused on fault detection via images or robotic platform design, the autonomous navigation of robots within these structures is less explored. This study addresses that gap by proposing methods to detect navigable surfaces in truss structures, enhancing the autonomy of climbing robots. The paper introduces several approaches for binary segmentation of navigable surfaces versus background from 3D point clouds of metallic trusses. These methods fall into two categories: analytical algorithms and deep learning models. The analytical approach features a custom algorithm that segments structures by analyzing the eigendecomposition of planar patches in the point cloud. In parallel, advanced deep learning models PointNet, PointNet++, MinkUNet34C, and PointTransformerV3 are trained and evaluated for the same task. Comparative analysis shows that the analytical algorithm offers easier parameter tuning and performance comparable to deep learning models, which, while more computationally intensive, excel in segmentation accuracy. Notably, PointTransformerV3 achieves a Mean Intersection Over Union (mIoU) of about 97%. The study demonstrates the promise of both analytical and deep learning methods for improving autonomous navigation in complex truss environments. The results highlight the trade-offs between computational efficiency and segmentation performance, providing valuable guidance for future research and practical applications in autonomous infrastructure inspection and maintenance.', 'abstract_zh': '网状结构构成桥梁、电线杆和机场等重大基础设施的骨干，但其检测和维护成本高且危险，常需人工干预。尽管以往研究主要集中在通过图像进行故障检测或机器人平台设计，但在这些结构中自主导航机器人的探索较少。本研究通过提出检测桁架结构中可通行表面的方法，增强了攀爬机器人的自主性。论文介绍了从金属桁架的3D点云中分割可通行表面与背景的二元分割方法，这些方法分为两类：解析算法和深度学习模型。解析方法包含一个定制算法，通过分析点云中平面片段的特征值分解来进行结构分割。同时，PointNet、PointNet++、MinkUNet34C 和 PointTransformerV3 等高级深度学习模型被训练和评估以完成相同任务。比较分析表明，解析算法具有更易调参且性能与深度学习模型相当的特点，尽管深度学习模型在计算上更为密集，但在分割准确性方面表现出色。特别地，PointTransformerV3 达到了约 97% 的平均交并比 (mIoU)。研究展示了解析方法和深度学习方法在复杂桁架环境下的自主导航中的潜力。结果突显了计算效率与分割性能之间的权衡，为未来研究和自主基础设施检测与维护的实际应用提供了宝贵指导。', 'title_zh': '使用3D LiDAR数据分割网状结构的方法：一种比较评估'}
{'arxiv_id': 'arXiv:2507.19701', 'title': 'PhysVarMix: Physics-Informed Variational Mixture Model for Multi-Modal Trajectory Prediction', 'authors': 'Haichuan Li, Tomi Westerlund', 'link': 'https://arxiv.org/abs/2507.19701', 'abstract': 'Accurate prediction of future agent trajectories is a critical challenge for ensuring safe and efficient autonomous navigation, particularly in complex urban environments characterized by multiple plausible future scenarios. In this paper, we present a novel hybrid approach that integrates learning-based with physics-based constraints to address the multi-modality inherent in trajectory prediction. Our method employs a variational Bayesian mixture model to effectively capture the diverse range of potential future behaviors, moving beyond traditional unimodal assumptions. Unlike prior approaches that predominantly treat trajectory prediction as a data-driven regression task, our framework incorporates physical realism through sector-specific boundary conditions and Model Predictive Control (MPC)-based smoothing. These constraints ensure that predicted trajectories are not only data-consistent but also physically plausible, adhering to kinematic and dynamic principles. Furthermore, our method produces interpretable and diverse trajectory predictions, enabling enhanced downstream decision-making and planning in autonomous driving systems. We evaluate our approach on two benchmark datasets, demonstrating superior performance compared to existing methods. Comprehensive ablation studies validate the contributions of each component and highlight their synergistic impact on prediction accuracy and reliability. By balancing data-driven insights with physics-informed constraints, our approach offers a robust and scalable solution for navigating the uncertainties of real-world urban environments.', 'abstract_zh': '基于学习与物理约束的未来代理轨迹准确预测方法：实现复杂城市环境下的高效自主导航', 'title_zh': 'PhysVarMix：物理知情的变分混合模型多模态轨迹预测'}
{'arxiv_id': 'arXiv:2507.20772', 'title': 'Beyond Line-of-Sight: Cooperative Localization Using Vision and V2X Communication', 'authors': 'Annika Wong, Zhiqi Tang, Frank J. Jiang, Karl H. Johansson, Jonas Mårtensson', 'link': 'https://arxiv.org/abs/2507.20772', 'abstract': 'Accurate and robust localization is critical for the safe operation of Connected and Automated Vehicles (CAVs), especially in complex urban environments where Global Navigation Satellite System (GNSS) signals are unreliable. This paper presents a novel vision-based cooperative localization algorithm that leverages onboard cameras and Vehicle-to-Everything (V2X) communication to enable CAVs to estimate their poses, even in occlusion-heavy scenarios such as busy intersections. In particular, we propose a novel decentralized observer for a group of connected agents that includes landmark agents (static or moving) in the environment with known positions and vehicle agents that need to estimate their poses (both positions and orientations). Assuming that (i) there are at least three landmark agents in the environment, (ii) each vehicle agent can measure its own angular and translational velocities as well as relative bearings to at least three neighboring landmarks or vehicles, and (iii) neighboring vehicles can communicate their pose estimates, each vehicle can estimate its own pose using the proposed decentralized observer. We prove that the origin of the estimation error is locally exponentially stable under the proposed observer, provided that the minimal observability conditions are satisfied. Moreover, we evaluate the proposed approach through experiments with real 1/10th-scale connected vehicles and large-scale simulations, demonstrating its scalability and validating the theoretical guarantees in practical scenarios.', 'abstract_zh': '基于视觉的合作定位算法：面向复杂城市环境中的连接和自动驾驶车辆的鲁棒定位', 'title_zh': '超越视线：基于视觉和V2X通信的协同定位'}
{'arxiv_id': 'arXiv:2507.19691', 'title': 'Co-Win: Joint Object Detection and Instance Segmentation in LiDAR Point Clouds via Collaborative Window Processing', 'authors': 'Haichuan Li, Tomi Westerlund', 'link': 'https://arxiv.org/abs/2507.19691', 'abstract': "Accurate perception and scene understanding in complex urban environments is a critical challenge for ensuring safe and efficient autonomous navigation. In this paper, we present Co-Win, a novel bird's eye view (BEV) perception framework that integrates point cloud encoding with efficient parallel window-based feature extraction to address the multi-modality inherent in environmental understanding. Our method employs a hierarchical architecture comprising a specialized encoder, a window-based backbone, and a query-based decoder head to effectively capture diverse spatial features and object relationships. Unlike prior approaches that treat perception as a simple regression task, our framework incorporates a variational approach with mask-based instance segmentation, enabling fine-grained scene decomposition and understanding. The Co-Win architecture processes point cloud data through progressive feature extraction stages, ensuring that predicted masks are both data-consistent and contextually relevant. Furthermore, our method produces interpretable and diverse instance predictions, enabling enhanced downstream decision-making and planning in autonomous driving systems.", 'abstract_zh': '准确感知和理解复杂城市环境是确保自主导航安全和高效的关键挑战。本文提出了一种新颖的鸟瞰视图（BEV）感知框架Co-Win，该框架结合点云编码与高效的并行窗口特征提取，以应对环境理解中的多模态性。本文方法采用包含专用编码器、基于窗口的主干和查询式解码头的分层架构，以有效地捕捉多样的空间特征和对象关系。与以往将感知简单视为回归任务的方法不同，我们的框架结合了变分方法和基于掩码的实例分割，实现精细粒度的场景分解和理解。Co-Win架构通过逐步特征提取阶段处理点云数据，确保预测掩码既与数据一致又具有上下文相关性。此外，我们的方法能够生成可解释且多样的实例预测，从而增强自主驾驶系统中的下游决策和规划能力。', 'title_zh': 'Co-Win: 联合点云目标检测与实例分割的协作窗口处理方法'}
{'arxiv_id': 'arXiv:2507.20987', 'title': 'JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech Generation Version 1', 'authors': 'Xinhan Di, Kristin Qi, Pengqian Yu', 'link': 'https://arxiv.org/abs/2507.20987', 'abstract': 'Recent advances in diffusion-based video generation have enabled photo-realistic short clips, but current methods still struggle to achieve multi-modal consistency when jointly generating whole-body motion and natural speech. Current approaches lack comprehensive eval- uation frameworks that assess both visual and audio quality, and there are insufficient benchmarks for region- specific performance analysis. To address these gaps, we introduce the Joint Whole-Body Talking Avatar and Speech Generation Version I(JWB-DH-V1), comprising a large-scale multi-modal dataset with 10,000 unique identities across 2 million video samples, and an evalua- tion protocol for assessing joint audio-video generation of whole-body animatable avatars. Our evaluation of SOTA models reveals consistent performance disparities between face/hand-centric and whole-body performance, which incidates essential areas for future research. The dataset and evaluation tools are publicly available at this https URL.', 'abstract_zh': '基于扩散的视频生成最近取得了进展，能够生成 PHOTO-REALISTIC 短片段，但当前方法在联合生成全身运动和自然语音时仍难以实现多模态一致性。当前的方法缺乏综合评估框架，无法同时评估视觉和音频质量，也缺乏针对特定区域性能分析的基准数据集。为解决这些不足，我们引入了包含 10,000 个独特身份和 200 万视频样本的大型多模态数据集 JWB-DH-V1 及其评估协议，用于评估可全身动画化的化身的联合音频-视频生成性能。我们的研究表明，人脸/手部为中心的方法与全身方法之间存在一致的性能差异，这表明未来研究的重要领域。数据集和评估工具可在以下网址公开获取：this https URL。', 'title_zh': 'JWB-DH-V1：全面身体对话角色和语音生成基准1.0'}
{'arxiv_id': 'arXiv:2507.20853', 'title': 'Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces', 'authors': 'Saket Tiwari, Omer Gottesman, George Konidaris', 'link': 'https://arxiv.org/abs/2507.20853', 'abstract': 'Advances in reinforcement learning (RL) have led to its successful application in complex tasks with continuous state and action spaces. Despite these advances in practice, most theoretical work pertains to finite state and action spaces. We propose building a theoretical understanding of continuous state and action spaces by employing a geometric lens to understand the locally attained set of states. The set of all parametrised policies learnt through a semi-gradient based approach induces a set of attainable states in RL. We show that the training dynamics of a two-layer neural policy induce a low dimensional manifold of attainable states embedded in the high-dimensional nominal state space trained using an actor-critic algorithm. We prove that, under certain conditions, the dimensionality of this manifold is of the order of the dimensionality of the action space. This is the first result of its kind, linking the geometry of the state space to the dimensionality of the action space. We empirically corroborate this upper bound for four MuJoCo environments and also demonstrate the results in a toy environment with varying dimensionality. We also show the applicability of this theoretical result by introducing a local manifold learning layer to the policy and value function networks to improve the performance in control environments with very high degrees of freedom by changing one layer of the neural network to learn sparse representations.', 'abstract_zh': '强化学习（RL）的进步使其成功应用于具有连续状态和动作空间的复杂任务。尽管在实际应用中取得了这些进展，大多数理论工作仍集中在有限状态和动作空间。我们提出通过几何视角理解局部达到的状态集来建立对连续状态和动作空间的理论理解。通过半梯度方法学习的参数化策略集在RL中诱导出可达到的状态集。我们证明，使用actor-critic算法训练两层神经策略的动力学诱导出嵌入在高维名义状态空间中的低维流形。在满足某些条件下，该流形的维数与动作空间的维数成比例。这是这一领域的首个结果，将状态空间的几何结构与动作空间的维数联系起来。我们通过四个MuJoCo环境的经验验证确认了这一上界，并在不同维数的玩具环境中也证明了该结果。此外，通过在策略和价值函数网络中引入局部流形学习层，我们展示了这一理论结果的应用性，通过改变神经网络的一层来学习稀疏表示，从而提高具有非常高自由度的控制环境中的性能。', 'title_zh': '几何视角下的神经强化学习在连续状态和行动空间中的研究'}
{'arxiv_id': 'arXiv:2507.20757', 'title': 'Learning to See Inside Opaque Liquid Containers using Speckle Vibrometry', 'authors': 'Matan Kichler, Shai Bagon, Mark Sheinin', 'link': 'https://arxiv.org/abs/2507.20757', 'abstract': 'Computer vision seeks to infer a wide range of information about objects and events. However, vision systems based on conventional imaging are limited to extracting information only from the visible surfaces of scene objects. For instance, a vision system can detect and identify a Coke can in the scene, but it cannot determine whether the can is full or empty. In this paper, we aim to expand the scope of computer vision to include the novel task of inferring the hidden liquid levels of opaque containers by sensing the tiny vibrations on their surfaces. Our method provides a first-of-a-kind way to inspect the fill level of multiple sealed containers remotely, at once, without needing physical manipulation and manual weighing. First, we propose a novel speckle-based vibration sensing system for simultaneously capturing scene vibrations on a 2D grid of points. We use our system to efficiently and remotely capture a dataset of vibration responses for a variety of everyday liquid containers. Then, we develop a transformer-based approach for analyzing the captured vibrations and classifying the container type and its hidden liquid level at the time of measurement. Our architecture is invariant to the vibration source, yielding correct liquid level estimates for controlled and ambient scene sound sources. Moreover, our model generalizes to unseen container instances within known classes (e.g., training on five Coke cans of a six-pack, testing on a sixth) and fluid levels. We demonstrate our method by recovering liquid levels from various everyday containers.', 'abstract_zh': '计算机视觉旨在推断物体和事件的广泛信息。然而，基于传统成像的视觉系统仅能从场景物体的可见表面提取信息。例如，一个视觉系统可以检测并识别场景中的可乐罐，但无法确定罐子是否装满或为空。本文旨在扩展计算机视觉的范围，通过感知不透明容器表面微小的振动来推断其内部液位，从而实现新的任务。我们的方法提供了首款无需物理操作和手动称重即可远程、同时检查多个密封容器液位的方案。首先，我们提出了一种新的基于斑点的振动感知系统，用于在二维点网格上同时捕捉场景振动。我们使用该系统高效且远程地收集了各种日常液体容器的振动响应数据集。然后，我们开发了一种基于变压器的方法，用于分析捕捉到的振动信号，并在测量时对容器类型及其隐藏液位进行分类。我们的架构对振动源具有不变性，适用于控制场景和环境场景声源。此外，我们的模型可以泛化到已知类别的未见过的容器实例（例如，在六罐装中进行五罐训练，在第六罐上进行测试）和液位。我们通过从各种日常容器中恢复液位来展示该方法。', 'title_zh': '使用斑点振动技术观察不透明液体容器内部'}
{'arxiv_id': 'arXiv:2507.20627', 'title': 'Controllable Video-to-Music Generation with Multiple Time-Varying Conditions', 'authors': 'Junxian Wu, Weitao You, Heda Zuo, Dengming Zhang, Pei Chen, Lingyun Sun', 'link': 'https://arxiv.org/abs/2507.20627', 'abstract': "Music enhances video narratives and emotions, driving demand for automatic video-to-music (V2M) generation. However, existing V2M methods relying solely on visual features or supplementary textual inputs generate music in a black-box manner, often failing to meet user expectations. To address this challenge, we propose a novel multi-condition guided V2M generation framework that incorporates multiple time-varying conditions for enhanced control over music generation. Our method uses a two-stage training strategy that enables learning of V2M fundamentals and audiovisual temporal synchronization while meeting users' needs for multi-condition control. In the first stage, we introduce a fine-grained feature selection module and a progressive temporal alignment attention mechanism to ensure flexible feature alignment. For the second stage, we develop a dynamic conditional fusion module and a control-guided decoder module to integrate multiple conditions and accurately guide the music composition process. Extensive experiments demonstrate that our method outperforms existing V2M pipelines in both subjective and objective evaluations, significantly enhancing control and alignment with user expectations.", 'abstract_zh': '音乐增强视频叙事和情感，推动了自动视频到音乐（V2M）生成的需求。然而，现有的仅依赖视觉特征或补充文本输入的V2M方法以黑箱方式生成音乐，往往无法满足用户期望。为此，我们提出了一种新的多条件引导V2M生成框架，结合了多种时间变化条件以增强音乐生成的控制。我们的方法采用两阶段训练策略，既能学习V2M的基本原理和音视频时间同步，又能满足用户对多条件控制的需求。在第一阶段，我们引入了精细特征选择模块和渐进时间对齐注意力机制，以确保灵活的特征对齐。在第二阶段，我们开发了动态条件融合模块和控制引导解码器模块，以整合多种条件并准确引导音乐创作过程。 extensive实验表明，我们的方法在主观和客观评价中均优于现有V2M管道，显著增强了控制能力和与用户期望的契合度。', 'title_zh': '具有多种时间变异条件的可控视频到音乐生成'}
{'arxiv_id': 'arXiv:2507.20575', 'title': 'Implicit Spatiotemporal Bandwidth Enhancement Filter by Sine-activated Deep Learning Model for Fast 3D Photoacoustic Tomography', 'authors': 'I Gede Eka Sulistyawan, Takuro Ishii, Riku Suzuki, Yoshifumi Saijo', 'link': 'https://arxiv.org/abs/2507.20575', 'abstract': '3D photoacoustic tomography (3D-PAT) using high-frequency hemispherical transducers offers near-omnidirectional reception and enhanced sensitivity to the finer structural details encoded in the high-frequency components of the broadband photoacoustic (PA) signal. However, practical constraints such as limited number of channels with bandlimited sampling rate often result in sparse and bandlimited sensors that degrade image quality. To address this, we revisit the 2D deep learning (DL) approach applied directly to sensor-wise PA radio-frequency (PARF) data. Specifically, we introduce sine activation into the DL model to restore the broadband nature of PARF signals given the observed band-limited and high-frequency PARF data. Given the scarcity of 3D training data, we employ simplified training strategies by simulating random spherical absorbers. This combination of sine-activated model and randomized training is designed to emphasize bandwidth learning over dataset memorization. Our model was evaluated on a leaf skeleton phantom, a micro-CT-verified 3D spiral phantom and in-vivo human palm vasculature. The results showed that the proposed training mechanism on sine-activated model was well-generalized across the different tests by effectively increasing the sensor density and recovering the spatiotemporal bandwidth. Qualitatively, the sine-activated model uniquely enhanced high-frequency content that produces clearer vascular structure with fewer artefacts. Quantitatively, the sine-activated model exhibits full bandwidth at -12 dB spectrum and significantly higher contrast-to-noise ratio with minimal loss of structural similarity index. Lastly, we optimized our approach to enable fast enhanced 3D-PAT at 2 volumes-per-second for better practical imaging of a free-moving targets.', 'abstract_zh': '基于高频率半球形换能器的3D光声成像（3D-PAT）利用近全向接收和高频率成分的高灵敏度以编码精细结构细节。然而，有限的带限采样率通道数量等实际限制常常导致稀疏且带限的传感器，从而降低图像质量。为此，我们重新审视了直接应用于传感器级光声射频（PARF）数据的2D深度学习（DL）方法。具体而言，我们引入了正弦激活到DL模型中，以恢复PARF信号的宽带特性，给定观察到的带限和高频率PARF数据。鉴于3D训练数据的稀缺性，我们通过模拟随机球形吸收体来采用简化的训练策略。这种正弦激活模型与随机化训练的结合旨在强调宽带学习而非数据集记忆。我们的模型在榆叶骨架仿真器、微CT验证的3D螺旋仿真器以及活体人类手掌血管中进行了评估。结果表明，提出的基于正弦激活模型的训练机制在不同测试中表现出良好的泛化能力，通过有效增加传感器密度并恢复时空带宽。定性上，正弦激活模型独特地增强了高频成分，从而生成更清晰的血管结构并减少了伪影。定量上，正弦激活模型在-12 dB频谱中表现出完整带宽，并具有显著更高的信噪比，同时保持结构相似性指数的最小损失。最后，我们优化了我们的方法以实现每秒2个体素的快速增强3D-PAT，以便更好地进行自由移动目标的实用成像。', 'title_zh': '基于正弦激活深度学习模型的快速三维光声断层成像隐式空时带宽增强滤波器'}
{'arxiv_id': 'arXiv:2507.20568', 'title': 'Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven 3D Facial Animation', 'authors': 'Hyung Kyu Kim, Hak Gu Kim', 'link': 'https://arxiv.org/abs/2507.20568', 'abstract': 'Speech-driven 3D facial animation aims to generate realistic facial movements synchronized with audio. Traditional methods primarily minimize reconstruction loss by aligning each frame with ground-truth. However, this frame-wise approach often fails to capture the continuity of facial motion, leading to jittery and unnatural outputs due to coarticulation. To address this, we propose a novel phonetic context-aware loss, which explicitly models the influence of phonetic context on viseme transitions. By incorporating a viseme coarticulation weight, we assign adaptive importance to facial movements based on their dynamic changes over time, ensuring smoother and perceptually consistent animations. Extensive experiments demonstrate that replacing the conventional reconstruction loss with ours improves both quantitative metrics and visual quality. It highlights the importance of explicitly modeling phonetic context-dependent visemes in synthesizing natural speech-driven 3D facial animation. Project page: this https URL', 'abstract_zh': '基于语音的3D面部动画旨在生成与音频同步的逼真面部动作。传统的方法主要通过将每一帧与ground-truth对齐来最小化重建损失，但这种方法往往无法捕捉面部动作的连贯性，导致输出不流畅且不自然，这是因为协同发音的影响。为了解决这一问题，我们提出了一种新的音系上下文感知损失，该损失明确地建模了音系上下文对音素转换的影响。通过引入协同发音权重，我们根据面部动作随时间动态变化的情况为面部动作分配适应性的权重，确保动画更加平滑且具感知一致性。大量实验表明，用我们的损失替换传统的重建损失可以提高定量指标和视觉质量。这突显了在合成自然的语音驱动3D面部动画时明确建模音系上下文依赖的音素的重要性。项目页面: 这里.getJSONObject("result").getString("url")', 'title_zh': '基于音素上下文依赖的语音驱动3D面部动画增强中的发音音视素学习'}
{'arxiv_id': 'arXiv:2507.20562', 'title': 'MemoryTalker: Personalized Speech-Driven 3D Facial Animation via Audio-Guided Stylization', 'authors': 'Hyung Kyu Kim, Sangmin Lee, Hak Gu Kim', 'link': 'https://arxiv.org/abs/2507.20562', 'abstract': "Speech-driven 3D facial animation aims to synthesize realistic facial motion sequences from given audio, matching the speaker's speaking style. However, previous works often require priors such as class labels of a speaker or additional 3D facial meshes at inference, which makes them fail to reflect the speaking style and limits their practical use. To address these issues, we propose MemoryTalker which enables realistic and accurate 3D facial motion synthesis by reflecting speaking style only with audio input to maximize usability in applications. Our framework consists of two training stages: 1-stage is storing and retrieving general motion (i.e., Memorizing), and 2-stage is to perform the personalized facial motion synthesis (i.e., Animating) with the motion memory stylized by the audio-driven speaking style feature. In this second stage, our model learns about which facial motion types should be emphasized for a particular piece of audio. As a result, our MemoryTalker can generate a reliable personalized facial animation without additional prior information. With quantitative and qualitative evaluations, as well as user study, we show the effectiveness of our model and its performance enhancement for personalized facial animation over state-of-the-art methods.", 'abstract_zh': '基于语音的3D面部动画旨在从给定的音频中合成逼真的面部运动序列，匹配说话人的说话风格。然而，先前的工作往往需要诸如说话者类别标签或额外的3D面部网格等先验信息，这使得它们难以反映说话风格并限制了其实用性。为了解决这些问题，我们提出了MemoryTalker，仅通过语音输入反映说话风格来实现逼真和准确的3D面部运动合成，以最大化其在应用中的适用性。我们的框架包括两个训练阶段：第一阶段是存储和检索通用运动（即记忆），第二阶段是通过由语音驱动的说话风格特征进行个性化面部运动合成（即动画）。在第二阶段中，我们的模型学会了特定音频片段中应强调哪些面部运动类型。因此，我们的MemoryTalker可以在无需额外先验信息的情况下生成可靠的个性化面部动画。通过定量和定性评估以及用户研究，我们展示了我们模型的有效性及其在个性化面部动画方面的性能提升，超过了当前最先进的方法。', 'title_zh': 'MemoryTalker: 个人化音源驱动的3D面部动画通过音频引导的风格化'}
{'arxiv_id': 'arXiv:2507.20529', 'title': 'Enhancing Spatial Reasoning through Visual and Textual Thinking', 'authors': 'Xun Liang, Xin Guo, Zhongming Jin, Weihang Pan, Penghui Shang, Deng Cai, Binbin Lin, Jieping Ye', 'link': 'https://arxiv.org/abs/2507.20529', 'abstract': "The spatial reasoning task aims to reason about the spatial relationships in 2D and 3D space, which is a fundamental capability for Visual Question Answering (VQA) and robotics. Although vision language models (VLMs) have developed rapidly in recent years, they are still struggling with the spatial reasoning task. In this paper, we introduce a method that can enhance Spatial reasoning through Visual and Textual thinking Simultaneously (SpatialVTS). In the spatial visual thinking phase, our model is trained to generate location-related specific tokens of essential targets automatically. Not only are the objects mentioned in the problem addressed, but also the potential objects related to the reasoning are considered. During the spatial textual thinking phase, Our model conducts long-term thinking based on visual cues and dialogues, gradually inferring the answers to spatial reasoning problems. To effectively support the model's training, we perform manual corrections to the existing spatial reasoning dataset, eliminating numerous incorrect labels resulting from automatic annotation, restructuring the data input format to enhance generalization ability, and developing thinking processes with logical reasoning details. Without introducing additional information (such as masks or depth), our model's overall average level in several spatial understanding tasks has significantly improved compared with other models.", 'abstract_zh': '空间推理任务旨在探究二维和三维空间中的空间关系，这是视觉问答（VQA）和机器人技术中的基本能力。尽管视觉语言模型（VLMs）近年来取得了 rapid 的发展，但在空间推理任务上仍然面临挑战。本文介绍了一种通过同时进行视觉和文本思考来增强空间推理的方法（SpatialVTS）。在空间视觉思考阶段，我们的模型被训练为能够自动生成与关键目标相关的位置特定标记。不仅提及了问题中的对象，还考虑了与推理相关的潜在对象。在空间文本思考阶段，模型基于视觉线索和对话进行长期思考，逐步推导出空间推理问题的答案。为了有效支持模型的训练，我们对手头的空间推理数据集进行了手动修正，消除了大量自动标注导致的错误标签，重新结构化了数据输入格式以增强泛化能力，并发展了具有逻辑推理细节的思考过程。在不引入额外信息（如遮罩或深度）的情况下，与其它模型相比，我们的模型在多个空间理解任务上的整体平均水平有显著提升。', 'title_zh': '通过视觉与文本思维增强空间推理能力'}
{'arxiv_id': 'arXiv:2507.20408', 'title': 'A Multi-Stage Hybrid CNN-Transformer Network for Automated Pediatric Lung Sound Classification', 'authors': 'Samiul Based Shuvo, Taufiq Hasan', 'link': 'https://arxiv.org/abs/2507.20408', 'abstract': 'Automated analysis of lung sound auscultation is essential for monitoring respiratory health, especially in regions facing a shortage of skilled healthcare workers. While respiratory sound classification has been widely studied in adults, its ap plication in pediatric populations, particularly in children aged <6 years, remains an underexplored area. The developmental changes in pediatric lungs considerably alter the acoustic proper ties of respiratory sounds, necessitating specialized classification approaches tailored to this age group. To address this, we propose a multistage hybrid CNN-Transformer framework that combines CNN-extracted features with an attention-based architecture to classify pediatric respiratory diseases using scalogram images from both full recordings and individual breath events. Our model achieved an overall score of 0.9039 in binary event classifi cation and 0.8448 in multiclass event classification by employing class-wise focal loss to address data imbalance. At the recording level, the model attained scores of 0.720 for ternary and 0.571 for multiclass classification. These scores outperform the previous best models by 3.81% and 5.94%, respectively. This approach offers a promising solution for scalable pediatric respiratory disease diagnosis, especially in resource-limited settings.', 'abstract_zh': '自动化分析肺音对于监测呼吸健康，尤其是在医护人员不足的地区，至关重要。虽然呼吸道声音分类在成人中已被广泛研究，但其在儿科人群中的应用，特别是在<6岁儿童中，仍是一个未充分探索的领域。儿童肺部的发育变化显著改变了呼吸声音的声学特性，需要针对这一年龄段的专门分类方法。为此，我们提出了一种多阶段混合CNN-Transformer框架，结合CNN提取的特征与基于注意力的架构，使用小波图像对完整记录和单个呼吸事件进行儿童呼吸疾病分类。我们的模型通过采用类别平衡的焦点损失实现了二分类0.9039和多分类0.8448的评分。在记录层面，模型分别实现了三分类0.720和多分类0.571的评分，这些评分分别比之前最佳模型高出3.81%和5.94%。该方法为资源有限的环境中可扩展的儿童呼吸疾病诊断提供了一个有前景的解决方案。', 'title_zh': '多阶段混合CNN-Transformer网络在自动化儿童肺音分类中的应用'}
{'arxiv_id': 'arXiv:2507.20389', 'title': 'Solving Scene Understanding for Autonomous Navigation in Unstructured Environments', 'authors': 'Naveen Mathews Renji, Kruthika K, Manasa Keshavamurthy, Pooja Kumari, S. Rajarajeswari', 'link': 'https://arxiv.org/abs/2507.20389', 'abstract': 'Autonomous vehicles are the next revolution in the automobile industry and they are expected to revolutionize the future of transportation. Understanding the scenario in which the autonomous vehicle will operate is critical for its competent functioning. Deep Learning has played a massive role in the progress that has been made till date. Semantic Segmentation, the process of annotating every pixel of an image with an object class, is one crucial part of this scene comprehension using Deep Learning. It is especially useful in Autonomous Driving Research as it requires comprehension of drivable and non-drivable areas, roadside objects and the like. In this paper semantic segmentation has been performed on the Indian Driving Dataset which has been recently compiled on the urban and rural roads of Bengaluru and Hyderabad. This dataset is more challenging compared to other datasets like Cityscapes, since it is based on unstructured driving environments. It has a four level hierarchy and in this paper segmentation has been performed on the first level. Five different models have been trained and their performance has been compared using the Mean Intersection over Union. These are UNET, UNET+RESNET50, DeepLabsV3, PSPNet and SegNet. The highest MIOU of 0.6496 has been achieved. The paper discusses the dataset, exploratory data analysis, preparation, implementation of the five models and studies the performance and compares the results achieved in the process.', 'abstract_zh': '自主驾驶车辆是汽车工业的下一次革命，它们有望重塑交通的未来。理解自主车辆将要运行的场景对于其有效运行至关重要。深度学习在迄今为止取得的进步中发挥了巨大作用。语义分割，即为图像中的每个像素赋予对象类别的过程，是使用深度学习进行场景理解的一个关键组成部分。在自主驾驶研究中尤其有用，因为它要求理解可行驶和不可行驶区域、路边物体等。本文在印度驾驶数据集上进行了语义分割，该数据集是最近在班加罗尔和海得拉巴的城乡道路上编译的。与Cityscapes等其他数据集相比，该数据集更具挑战性，因为它基于非结构化的驾驶环境。该数据集采用了四层层级结构，在本文中对第一层进行了分割。训练了五种不同的模型，并使用平均交并比（Mean Intersection over Union）比较了它们的性能。这些模型包括UNET、UNET+RESNET50、DeepLabsV3、PSPNet和SegNet。本文最高达到了0.6496的平均交并比。本文讨论了数据集、探索性数据分析、模型的准备和实施、以及研究性能并比较了整个过程中实现的结果。', 'title_zh': '解决自主导航于未结构化环境中的场景理解问题'}
{'arxiv_id': 'arXiv:2507.20221', 'title': 'Multi-Attention Stacked Ensemble for Lung Cancer Detection in CT Scans', 'authors': 'Uzzal Saha, Surya Prakash', 'link': 'https://arxiv.org/abs/2507.20221', 'abstract': 'In this work, we address the challenge of binary lung nodule classification (benign vs malignant) using CT images by proposing a multi-level attention stacked ensemble of deep neural networks. Three pretrained backbones - EfficientNet V2 S, MobileViT XXS, and DenseNet201 - are each adapted with a custom classification head tailored to 96 x 96 pixel inputs. A two-stage attention mechanism learns both model-wise and class-wise importance scores from concatenated logits, and a lightweight meta-learner refines the final prediction. To mitigate class imbalance and improve generalization, we employ dynamic focal loss with empirically calculated class weights, MixUp augmentation during training, and test-time augmentation at inference. Experiments on the LIDC-IDRI dataset demonstrate exceptional performance, achieving 98.09 accuracy and 0.9961 AUC, representing a 35 percent reduction in error rate compared to state-of-the-art methods. The model exhibits balanced performance across sensitivity (98.73) and specificity (98.96), with particularly strong results on challenging cases where radiologist disagreement was high. Statistical significance testing confirms the robustness of these improvements across multiple experimental runs. Our approach can serve as a robust, automated aid for radiologists in lung cancer screening.', 'abstract_zh': '基于多层注意力堆叠ensemble的深度神经网络在CT图像上实现二元肺结节分类（良性 vs 恶性）', 'title_zh': '基于CT扫描的肺癌检测的多注意力堆叠集成方法'}
{'arxiv_id': 'arXiv:2507.20197', 'title': 'Color histogram equalization and fine-tuning to improve expression recognition of (partially occluded) faces on sign language datasets', 'authors': 'Fabrizio Nunnari, Alakshendra Jyotsnaditya Ramkrishna Singh, Patrick Gebhard', 'link': 'https://arxiv.org/abs/2507.20197', 'abstract': 'The goal of this investigation is to quantify to what extent computer vision methods can correctly classify facial expressions on a sign language dataset. We extend our experiments by recognizing expressions using only the upper or lower part of the face, which is needed to further investigate the difference in emotion manifestation between hearing and deaf subjects. To take into account the peculiar color profile of a dataset, our method introduces a color normalization stage based on histogram equalization and fine-tuning. The results show the ability to correctly recognize facial expressions with 83.8% mean sensitivity and very little variance (.042) among classes. Like for humans, recognition of expressions from the lower half of the face (79.6%) is higher than that from the upper half (77.9%). Noticeably, the classification accuracy from the upper half of the face is higher than human level.', 'abstract_zh': '本研究的目的是量化计算机视觉方法在手语数据集中正确分类面部表情的程度。我们通过仅识别面部上部或下部的表情来扩展我们的实验，以进一步调查 Hearing 和 Deaf 受试者在情绪表达方面的差异。为了考虑到数据集的特殊颜色特征，我们的方法引入了基于直方图均衡和微调的色彩归一化阶段。结果显示，正确识别面部表情的能力达到了 83.8% 的平均灵敏度，并且不同类别的方差很小（0.042）。如同人类一样，来自面部下部（79.6%）的表情识别准确性高于来自上部（77.9%）。值得注意的是，来自面部上部的表情分类准确性高于人类水平。', 'title_zh': '基于颜色直方图均衡化和微调以提高手语图像中（部分遮挡的）面部表情识别'}
{'arxiv_id': 'arXiv:2507.20110', 'title': 'NeuroVoxel-LM: Language-Aligned 3D Perception via Dynamic Voxelization and Meta-Embedding', 'authors': 'Shiyu Liu, Lianlei Shan', 'link': 'https://arxiv.org/abs/2507.20110', 'abstract': 'Recent breakthroughs in Visual Language Models (VLMs) and Multimodal Large Language Models (MLLMs) have significantly advanced 3D scene perception towards language-driven cognition. However, existing 3D language models struggle with sparse, large-scale point clouds due to slow feature extraction and limited representation accuracy. To address these challenges, we propose NeuroVoxel-LM, a novel framework that integrates Neural Radiance Fields (NeRF) with dynamic resolution voxelization and lightweight meta-embedding. Specifically, we introduce a Dynamic Resolution Multiscale Voxelization (DR-MSV) technique that adaptively adjusts voxel granularity based on geometric and structural complexity, reducing computational cost while preserving reconstruction fidelity. In addition, we propose the Token-level Adaptive Pooling for Lightweight Meta-Embedding (TAP-LME) mechanism, which enhances semantic representation through attention-based weighting and residual fusion. Experimental results demonstrate that DR-MSV significantly improves point cloud feature extraction efficiency and accuracy, while TAP-LME outperforms conventional max-pooling in capturing fine-grained semantics from NeRF weights.', 'abstract_zh': '近期视觉语言模型（VLMs）和多模态大型语言模型（MLLMs）的突破性进展极大地推动了3D场景感知向语言驱动的认知发展。然而，现有3D语言模型在处理稀疏的大规模点云时遇到困难，主要由于特征提取速度慢和表示精度有限。为了解决这些问题，我们提出了NeuroVoxel-LM，该框架将神经辐射场（NeRF）与动态分辨率体素化和轻量级元嵌入相结合。具体而言，我们引入了一种动态分辨率多尺度体素化（DR-MSV）技术，该技术根据几何和结构复杂性自适应调整体素粒度，从而在保持重建保真度的同时降低计算成本。此外，我们提出了基于注意力加权和残差融合的Token级自适应聚类（TAP-LME）机制，以增强语义表示。实验结果表明，DR-MSV显著提高了点云特征提取的效率和准确性，而TAP-LME在捕捉NeRF权重中的细粒度语义方面优于传统的最大池化方法。', 'title_zh': 'NeuroVoxel-LM: 语言对齐的动态体素化与元嵌入三维感知'}
{'arxiv_id': 'arXiv:2507.20094', 'title': 'Local Prompt Adaptation for Style-Consistent Multi-Object Generation in Diffusion Models', 'authors': 'Ankit Sanjyal', 'link': 'https://arxiv.org/abs/2507.20094', 'abstract': "Diffusion models have become a powerful backbone for text-to-image generation, enabling users to synthesize high-quality visuals from natural language prompts. However, they often struggle with complex prompts involving multiple objects and global or local style specifications. In such cases, the generated scenes tend to lack style uniformity and spatial coherence, limiting their utility in creative and controllable content generation. In this paper, we propose a simple, training-free architectural method called Local Prompt Adaptation (LPA). Our method decomposes the prompt into content and style tokens, and injects them selectively into the U-Net's attention layers at different stages. By conditioning object tokens early and style tokens later in the generation process, LPA enhances both layout control and stylistic consistency. We evaluate our method on a custom benchmark of 50 style-rich prompts across five categories and compare against strong baselines including Composer, MultiDiffusion, Attend-and-Excite, LoRA, and SDXL. Our approach outperforms prior work on both CLIP score and style consistency metrics, offering a new direction for controllable, expressive diffusion-based generation.", 'abstract_zh': '局部提示适应（LPA）在文本到图像生成中的应用', 'title_zh': '局部提示适配以实现风格一致的多对象生成在扩散模型中'}
{'arxiv_id': 'arXiv:2507.20056', 'title': 'FaRMamba: Frequency-based learning and Reconstruction aided Mamba for Medical Segmentation', 'authors': 'Ze Rong, ZiYue Zhao, Zhaoxin Wang, Lei Ma', 'link': 'https://arxiv.org/abs/2507.20056', 'abstract': 'Accurate medical image segmentation remains challenging due to blurred lesion boundaries (LBA), loss of high-frequency details (LHD), and difficulty in modeling long-range anatomical structures (DC-LRSS). Vision Mamba employs one-dimensional causal state-space recurrence to efficiently model global dependencies, thereby substantially mitigating DC-LRSS. However, its patch tokenization and 1D serialization disrupt local pixel adjacency and impose a low-pass filtering effect, resulting in Local High-frequency Information Capture Deficiency (LHICD) and two-dimensional Spatial Structure Degradation (2D-SSD), which in turn exacerbate LBA and LHD. In this work, we propose FaRMamba, a novel extension that explicitly addresses LHICD and 2D-SSD through two complementary modules. A Multi-Scale Frequency Transform Module (MSFM) restores attenuated high-frequency cues by isolating and reconstructing multi-band spectra via wavelet, cosine, and Fourier transforms. A Self-Supervised Reconstruction Auxiliary Encoder (SSRAE) enforces pixel-level reconstruction on the shared Mamba encoder to recover full 2D spatial correlations, enhancing both fine textures and global context. Extensive evaluations on CAMUS echocardiography, MRI-based Mouse-cochlea, and Kvasir-Seg endoscopy demonstrate that FaRMamba consistently outperforms competitive CNN-Transformer hybrids and existing Mamba variants, delivering superior boundary accuracy, detail preservation, and global coherence without prohibitive computational overhead. This work provides a flexible frequency-aware framework for future segmentation models that directly mitigates core challenges in medical imaging.', 'abstract_zh': '准确的医学图像分割仍面临挑战：模糊的病灶边界（LBA）、高频细节丢失（LHD）以及长距离解剖结构建模困难（DC-LRSS）。Vision Mamba通过一维因果状态空间递归有效地建模全局依赖性，从而显著减轻DC-LRSS。然而，其片段化token化和一维序列化破坏了局部像素邻接关系，并施加了低通滤波效应，导致局部高频信息捕获不足（LHICD）和二维空间结构降解（2D-SSD），进而加剧了LBA和LHD。本文提出FaRMamba，一种新颖的扩展，通过两个互补模块显式解决LHICD和2D-SSD。多尺度频率变换模块（MSFM）通过小波、余弦和傅里叶变换隔离和重构多带谱，恢复衰减的高频线索。自我监督重建辅助编码器（SSRAE）在共享的Mamba编码器上施加像素级重建，以恢复完整的二维空间相关性，增强精细纹理和全局上下文。在CAMUS超声心动图、MRI基鼠耳蜗和Kvasir-Seg内窥镜上的广泛评估表明，FaRMamba在边界准确性、细节保留和全局连贯性方面均优于竞争性的CNN-Transformer混合模型和现有Mamba变体，且不增加显著的计算开销。本文提供了一种灵活的频率感知框架，可为未来的分割模型直接减轻医学影像中的核心挑战。', 'title_zh': '基于频率学习和重建辅助的Mamba医疗分割方法'}
{'arxiv_id': 'arXiv:2507.19961', 'title': 'Pic2Diagnosis: A Method for Diagnosis of Cardiovascular Diseases from the Printed ECG Pictures', 'authors': 'Oğuzhan Büyüksolak, İlkay Öksüz', 'link': 'https://arxiv.org/abs/2507.19961', 'abstract': 'The electrocardiogram (ECG) is a vital tool for diagnosing heart diseases. However, many disease patterns are derived from outdated datasets and traditional stepwise algorithms with limited accuracy. This study presents a method for direct cardiovascular disease (CVD) diagnosis from ECG images, eliminating the need for digitization. The proposed approach utilizes a two-step curriculum learning framework, beginning with the pre-training of a classification model on segmentation masks, followed by fine-tuning on grayscale, inverted ECG images. Robustness is further enhanced through an ensemble of three models with averaged outputs, achieving an AUC of 0.9534 and an F1 score of 0.7801 on the BHF ECG Challenge dataset, outperforming individual models. By effectively handling real-world artifacts and simplifying the diagnostic process, this method offers a reliable solution for automated CVD diagnosis, particularly in resource-limited settings where printed or scanned ECG images are commonly used. Such an automated procedure enables rapid and accurate diagnosis, which is critical for timely intervention in CVD cases that often demand urgent care.', 'abstract_zh': '电 cardiogram (ECG) 是诊断心脏疾病的重要工具。然而，许多疾病模式源于过时的数据集和传统步进算法，这些算法的准确性有限。本研究提出了一种直接从 ECG 图像诊断心血管疾病 (CVD) 的方法，无需进行数字化处理。所提出的方法利用了一个两步 Curriculum 学习框架，首先在分割掩码上预训练分类模型，然后在灰度反转的 ECG 图像上进行微调。通过三种模型的集成，输出平均值进一步增强了鲁棒性，在 BHF ECG 挑战数据集上实现了 AUC 为 0.9534 和 F1 分数为 0.7801 的性能，优于单个模型。通过有效处理实际的图像伪影并简化诊断过程，该方法在资源有限的环境中提供了一种可靠的自动化 CVD 诊断解决方案，这些环境中常常使用打印或扫描的 ECG 图像。这种自动化的操作流程能够实现快速且准确的诊断，这对于需要及时干预的心血管疾病病例至关重要。', 'title_zh': 'Pic2Diagnosis: 一种基于打印心电图图片的心血管疾病诊断方法'}
{'arxiv_id': 'arXiv:2507.19950', 'title': 'RARE: Refine Any Registration of Pairwise Point Clouds via Zero-Shot Learning', 'authors': 'Chengyu Zheng, Jin Huang, Honghua Chen, Mingqiang Wei', 'link': 'https://arxiv.org/abs/2507.19950', 'abstract': 'Recent research leveraging large-scale pretrained diffusion models has demonstrated the potential of using diffusion features to establish semantic correspondences in images. Inspired by advancements in diffusion-based techniques, we propose a novel zero-shot method for refining point cloud registration algorithms. Our approach leverages correspondences derived from depth images to enhance point feature representations, eliminating the need for a dedicated training dataset. Specifically, we first project the point cloud into depth maps from multiple perspectives and extract implicit knowledge from a pretrained diffusion network as depth diffusion features. These features are then integrated with geometric features obtained from existing methods to establish more accurate correspondences between point clouds. By leveraging these refined correspondences, our approach achieves significantly improved registration accuracy. Extensive experiments demonstrate that our method not only enhances the performance of existing point cloud registration techniques but also exhibits robust generalization capabilities across diverse datasets. Codes are available at this https URL.', 'abstract_zh': '近期研究利用大规模预训练扩散模型展示了使用扩散特征建立图像语义对应关系的潜力。受扩散基技术进展的启发，我们提出了一种新的零样本方法，用于细化点云配准算法。该方法利用来自深度图的对应关系增强点特征表示，从而避免了专用训练数据集的需求。具体地，我们首先将点云投影到多视角的深度图中，并从预训练的扩散网络中提取隐含知识作为深度扩散特征。随后，将这些特征与现有方法获得的几何特征结合，以建立更准确的点云对应关系。通过利用这些细化的对应关系，我们的方法实现了显著提高的配准精度。大量实验表明，我们的方法不仅提升了现有点云配准技术的性能，还展示了在多种数据集上具有稳健的泛化能力。代码可在以下链接获取：this https URL。', 'title_zh': 'RARE: 通过零样本学习精炼配对点云注册'}
{'arxiv_id': 'arXiv:2507.19891', 'title': 'Interpretable Open-Vocabulary Referring Object Detection with Reverse Contrast Attention', 'authors': 'Drandreb Earl O. Juanico, Rowel O. Atienza, Jeffrey Kenneth Go', 'link': 'https://arxiv.org/abs/2507.19891', 'abstract': 'We propose Reverse Contrast Attention (RCA), a plug-in method that enhances object localization in vision-language transformers without retraining. RCA reweights final-layer attention by suppressing extremes and amplifying mid-level activations to let semantically relevant but subdued tokens guide predictions. We evaluate it on Open Vocabulary Referring Object Detection (OV-RefOD), introducing FitAP, a confidence-free average precision metric based on IoU and box area. RCA improves FitAP in 11 out of 15 open-source VLMs, with gains up to $+26.6\\%$. Effectiveness aligns with attention sharpness and fusion timing; while late-fusion models benefit consistently, models like $\\texttt{DeepSeek-VL2}$ also improve, pointing to capacity and disentanglement as key factors. RCA offers both interpretability and performance gains for multimodal transformers.', 'abstract_zh': '我们提出反向对比注意（RCA），一种无需重新训练的插件方法，用于增强视觉-语言Transformer中的对象定位。RCA 通过抑制极端值并放大中间层激活来重新权重新层注意，让语义相关但被压制的标记引导预测。我们在开放词汇量物体引用检测（OV-RefOD）上进行了评估，引入了基于IoU和框面积的无信心度平均精度指标FitAP。RCA 在15个开源VLM中有11个上显示出改进，增幅最高达26.6%。效果与注意力锐度和融合时机一致；虽然晚期融合模型持续受益，如DeepSeek-VL2等模型也有所改进，表明容量和解耦是关键因素。RCA 为多模态Transformer提供了解释性和性能双重增益。', 'title_zh': '可解释的开放词汇对象检测与逆对比注意力'}
{'arxiv_id': 'arXiv:2507.19856', 'title': 'RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues for 3D Object Detection', 'authors': 'Xiaokai Bai, Chenxu Zhou, Lianqing Zheng, Si-Yuan Cao, Jianan Liu, Xiaohan Zhang, Zhengzhuang Zhang, Hui-liang Shen', 'link': 'https://arxiv.org/abs/2507.19856', 'abstract': '4D millimeter-wave radar has emerged as a promising sensor for autonomous driving, but effective 3D object detection from both 4D radar and monocular images remains a challenge. Existing fusion approaches typically rely on either instance-based proposals or dense BEV grids, which either lack holistic scene understanding or are limited by rigid grid structures. To address these, we propose RaGS, the first framework to leverage 3D Gaussian Splatting (GS) as representation for fusing 4D radar and monocular cues in 3D object detection. 3D GS naturally suits 3D object detection by modeling the scene as a field of Gaussians, dynamically allocating resources on foreground objects and providing a flexible, resource-efficient solution. RaGS uses a cascaded pipeline to construct and refine the Gaussian field. It starts with the Frustum-based Localization Initiation (FLI), which unprojects foreground pixels to initialize coarse 3D Gaussians positions. Then, the Iterative Multimodal Aggregation (IMA) fuses semantics and geometry, refining the limited Gaussians to the regions of interest. Finally, the Multi-level Gaussian Fusion (MGF) renders the Gaussians into multi-level BEV features for 3D object detection. By dynamically focusing on sparse objects within scenes, RaGS enable object concentrating while offering comprehensive scene perception. Extensive experiments on View-of-Delft, TJ4DRadSet, and OmniHD-Scenes benchmarks demonstrate its state-of-the-art performance. Code will be released.', 'abstract_zh': '4D毫米波雷达已成为自主驾驶领域有前景的传感器，但在从4D雷达和单目图像中进行有效的3D对象检测方面仍存在挑战。现有融合方法通常依赖于实例级提议或密集的鸟瞰图网格，要么缺乏全局场景理解，要么受限于刚性的网格结构。为了解决这些问题，我们提出了RaGS，这是首个利用3D高斯斑图化（GS）作为表示方法，将4D雷达和单目视觉线索融合到3D对象检测中的框架。3D GS自然适用于3D对象检测，通过将场景建模为高斯场，动态分配资源到前景对象上，并提供一种灵活且资源高效的解决方案。RaGS采用级联流水线来构建和精炼高斯场。首先，通过束基定位初始化（FLI），将前景像素反投影以初始化粗略的3D高斯位置。然后，通过迭代多模态聚合（IMA）融合语义和几何信息，精炼有限的高斯到感兴趣区域。最后，多级高斯融合（MGF）将高斯渲染为多级BEV特征，用于3D对象检测。通过动态聚焦场景中的稀疏对象，RaGS实现了对象集中化的同时提供了全面的场景感知。在View-of-Delft、TJ4DRadSet和OmniHD-Scenes基准测试上进行的广泛实验展示了其优越性能。代码将开源。', 'title_zh': 'RaGS: 从4D雷达和单目线索释放3D高斯采样以进行3D物体检测'}
{'arxiv_id': 'arXiv:2507.19836', 'title': 'ChoreoMuse: Robust Music-to-Dance Video Generation with Style Transfer and Beat-Adherent Motion', 'authors': 'Xuanchen Wang, Heng Wang, Weidong Cai', 'link': 'https://arxiv.org/abs/2507.19836', 'abstract': 'Modern artistic productions increasingly demand automated choreography generation that adapts to diverse musical styles and individual dancer characteristics. Existing approaches often fail to produce high-quality dance videos that harmonize with both musical rhythm and user-defined choreography styles, limiting their applicability in real-world creative contexts. To address this gap, we introduce ChoreoMuse, a diffusion-based framework that uses SMPL format parameters and their variation version as intermediaries between music and video generation, thereby overcoming the usual constraints imposed by video resolution. Critically, ChoreoMuse supports style-controllable, high-fidelity dance video generation across diverse musical genres and individual dancer characteristics, including the flexibility to handle any reference individual at any resolution. Our method employs a novel music encoder MotionTune to capture motion cues from audio, ensuring that the generated choreography closely follows the beat and expressive qualities of the input music. To quantitatively evaluate how well the generated dances match both musical and choreographic styles, we introduce two new metrics that measure alignment with the intended stylistic cues. Extensive experiments confirm that ChoreoMuse achieves state-of-the-art performance across multiple dimensions, including video quality, beat alignment, dance diversity, and style adherence, demonstrating its potential as a robust solution for a wide range of creative applications. Video results can be found on our project page: this https URL.', 'abstract_zh': '现代艺术创作日益需求能够适应多样音乐风格和个体舞者特性的自动化编舞生成。现有方法往往无法生成与音乐节奏和用户定义的编舞风格和谐一致的高质量舞蹈视频，限制了其在实际创作环境中的应用。为解决这一问题，我们引入了ChoreoMuse，这是一种基于发散模型的框架，利用SMPL格式参数及其变体作为音乐与视频生成之间的中介，从而克服了传统视频分辨率限制。关键的是，ChoreoMuse支持跨多样音乐流派和个体舞者特性的风格可控、高保真舞蹈视频生成，包括处理任意分辨率参考个体的能力。我们的方法采用新型音乐编码器MotionTune捕获音频中的运动线索，确保生成的编舞紧密跟随输入音乐的节奏和表达特质。为了定量评估生成舞蹈与音乐和编舞风格的匹配程度，我们引入了两个新的度量标准，用于衡量风格意图的对齐情况。大量实验表明，ChoreoMuse在视频质量、节奏对齐、舞蹈多样性和风格遵循方面均达到最先进的性能，展示了其在多种创意应用中的潜在优势。视频结果可在我们的项目页面上找到：this https URL。', 'title_zh': 'ChoreoMuse：基于风格转换和节拍适配的 robust 音乐到舞蹈视频生成'}
{'arxiv_id': 'arXiv:2507.19730', 'title': 'Quaternion-Based Robust PCA for Efficient Moving Target Detection and Background Recovery in Color Videos', 'authors': 'Liyang Wang, Shiqian Wu, Shun Fang, Qile Zhu, Jiaxin Wu, Sos Again', 'link': 'https://arxiv.org/abs/2507.19730', 'abstract': 'Moving target detection is a challenging computer vision task aimed at generating accurate segmentation maps in diverse in-the-wild color videos captured by static cameras. If backgrounds and targets can be simultaneously extracted and recombined, such synthetic data can significantly enrich annotated in-the-wild datasets and enhance the generalization ability of deep models. Quaternion-based RPCA (QRPCA) is a promising unsupervised paradigm for color image processing. However, in color video processing, Quaternion Singular Value Decomposition (QSVD) incurs high computational costs, and rank-1 quaternion matrix fails to yield rank-1 color channels. In this paper, we reduce the computational complexity of QSVD to o(1) by utilizing a quaternion Riemannian manifold. Furthermor, we propose the universal QRPCA (uQRPCA) framework, which achieves a balance in simultaneously segmenting targets and recovering backgrounds from color videos. Moreover, we expand to uQRPCA+ by introducing the Color Rank-1 Batch (CR1B) method to further process and obtain the ideal low-rank background across color channels. Experiments demonstrate our uQRPCA+ achieves State Of The Art (SOTA) performance on moving target detection and background recovery tasks compared to existing open-source methods. Our implementation is publicly available on GitHub at this https URL', 'abstract_zh': '彩色视频中的移动目标检测是一项具有挑战性的计算机视觉任务，旨在生成 diverse in-the-wild 颜色视频中静态摄像头捕获的准确分割图。如果背景和目标能够同时提取和重组，此类合成数据可以显著丰富标注的在野数据集，并增强深度模型的泛化能力。四元数基于 RPCA (QRPCA) 是一种有前景的无监督彩色图像处理范式。但在彩色视频处理中，四元数奇异值分解 (QSVD) 引起高计算成本，且 rank-1 四元数矩阵无法产生 rank-1 彩色通道。本文通过利用四元数黎曼流形将 QSVD 的计算复杂度降低到 o(1)。此外，我们提出了一种通用的 QRPCA (uQRPCA) 框架，该框架能够在彩色视频中同时分割目标和恢复背景时取得平衡。进一步地，通过引入彩色秩-1 批处理 (CR1B) 方法，我们拓展了 uQRPCA+，以进一步处理并获得理想低秩背景。实验表明，我们的 uQRPCA+ 在移动目标检测和背景恢复任务上达到了现有开源方法的最优性能。我们的实现已在 GitHub 公开发布。', 'title_zh': '基于四元数的鲁棒PCA方法在彩色视频中高效的目标检测与背景恢复'}
