{'arxiv_id': 'arXiv:2504.06124', 'title': 'Safe Interaction via Monte Carlo Linear-Quadratic Games', 'authors': 'Benjamin A. Christie, Dylan P. Losey', 'link': 'https://arxiv.org/abs/2504.06124', 'abstract': "Safety is critical during human-robot interaction. But -- because people are inherently unpredictable -- it is often difficult for robots to plan safe behaviors. Instead of relying on our ability to anticipate humans, here we identify robot policies that are robust to unexpected human decisions. We achieve this by formulating human-robot interaction as a zero-sum game, where (in the worst case) the human's actions directly conflict with the robot's objective. Solving for the Nash Equilibrium of this game provides robot policies that maximize safety and performance across a wide range of human actions. Existing approaches attempt to find these optimal policies by leveraging Hamilton-Jacobi analysis (which is intractable) or linear-quadratic approximations (which are inexact). By contrast, in this work we propose a computationally efficient and theoretically justified method that converges towards the Nash Equilibrium policy. Our approach (which we call MCLQ) leverages linear-quadratic games to obtain an initial guess at safe robot behavior, and then iteratively refines that guess with a Monte Carlo search. Not only does MCLQ provide real-time safety adjustments, but it also enables the designer to tune how conservative the robot is -- preventing the system from focusing on unrealistic human behaviors. Our simulations and user study suggest that this approach advances safety in terms of both computation time and expected performance. See videos of our experiments here: this https URL.", 'abstract_zh': '人类-机器人交互中的安全性至关重要。但是，由于人类行为固有的不可预测性，机器人很难规划安全行为。我们不依赖于预测人类能力，而是识别出对意外人类决策具有鲁棒性的机器人策略。通过将人类-机器人交互视为零和博弈（在最坏情况下，人类的行为直接与机器人的目标冲突）来实现这一目标。求解此博弈的纳什均衡提供了适用于各种人类行为的最优安全和性能策略。现有方法试图通过哈密尔顿-雅可比分析（该方法不可行）或线性二次近似（不准确）来找到这些最优策略。与此相反，本项工作中我们提出了一种计算高效且具有理论支持的方法，该方法能够收敛到纳什均衡策略。我们提出的方法（我们称之为MCLQ）利用线性二次博弈获得安全的机器人行为的初始猜测，并通过蒙特卡洛搜索逐步优化该猜测。MCLQ不仅提供实时安全性调整，还使设计者能够调整机器人的保守程度，防止系统专注于不现实的人类行为。我们的仿真和用户研究结果表明，该方法在计算时间和预期性能方面均提升了安全性。更多实验视频请见：this https URL。', 'title_zh': '基于蒙特卡洛线性-二次游戏的安全交互'}
{'arxiv_id': 'arXiv:2504.05552', 'title': 'Lazy-DaSH: Lazy Approach for Hypergraph-based Multi-robot Task and Motion Planning', 'authors': 'Seongwon Lee, James Motes, Isaac Ngui, Marco Morales, Nancy M. Amato', 'link': 'https://arxiv.org/abs/2504.05552', 'abstract': 'We introduce Lazy-DaSH, an improvement over the recent state of the art multi-robot task and motion planning method DaSH, which scales to more than double the number of robots and objects compared to the original method and achieves an order of magnitude faster planning time when applied to a multi-manipulator object rearrangement problem. We achieve this improvement through a hierarchical approach, where a high-level task planning layer identifies planning spaces required for task completion, and motion feasibility is validated lazily only within these spaces. In contrast, DaSH precomputes the motion feasibility of all possible actions, resulting in higher costs for constructing state space representations. Lazy-DaSH maintains efficient query performance by utilizing a constraint feedback mechanism within its hierarchical structure, ensuring that motion feasibility is effectively conveyed to the query process. By maintaining smaller state space representations, our method significantly reduces both representation construction time and query time. We evaluate Lazy-DaSH in four distinct scenarios, demonstrating its scalability to increasing numbers of robots and objects, as well as its adaptability in resolving conflicts through the constraint feedback mechanism.', 'abstract_zh': '懒加载DaSH：一种多机器人任务与运动规划的改进方法', 'title_zh': 'Lazy-DaSH：基于超图的多机器人任务与运动规划的懒惰方法'}
{'arxiv_id': 'arXiv:2504.05550', 'title': 'Path Database Guidance for Motion Planning', 'authors': 'Amnon Attali, Praval Telagi, Marco Morales, Nancy M. Amato', 'link': 'https://arxiv.org/abs/2504.05550', 'abstract': 'One approach to using prior experience in robot motion planning is to store solutions to previously seen problems in a database of paths. Methods that use such databases are characterized by how they query for a path and how they use queries given a new problem. In this work we present a new method, Path Database Guidance (PDG), which innovates on existing work in two ways. First, we use the database to compute a heuristic for determining which nodes of a search tree to expand, in contrast to prior work which generally pastes the (possibly transformed) queried path or uses it to bias a sampling distribution. We demonstrate that this makes our method more easily composable with other search methods by dynamically interleaving exploration according to a baseline algorithm with exploitation of the database guidance. Second, in contrast to other methods that treat the database as a single fixed prior, our database (and thus our queried heuristic) updates as we search the implicitly defined robot configuration space. We experimentally demonstrate the effectiveness of PDG in a variety of explicitly defined environment distributions in simulation.', 'abstract_zh': '一种在机器人运动规划中利用先验经验的方法是将以往解决的问题路径存储在一个路径数据库中。本研究提出了一种名为路径数据库引导（PDG）的新方法，该方法在两方面创新了现有工作。首先，我们利用数据库来计算一个启发式函数，用于确定搜索树中需要扩展的节点，与以往工作不同，以往工作通常会直接应用（可能经过变换的）查询路径或将其用于偏倚采样分布。我们通过动态交替进行基线算法的探索和数据库引导的利用，证明这种方法更容易与其他搜索方法组合。其次，与其他将数据库视为单一固定先验的方法不同，我们的数据库（以及因此我们的查询启发式）在搜索隐式定义的机器人配置空间时会进行更新。我们在仿真中对PDG的有效性进行了实验性验证。', 'title_zh': '路径数据库指导的运动规划'}
{'arxiv_id': 'arXiv:2504.05407', 'title': 'TRATSS: Transformer-Based Task Scheduling System for Autonomous Vehicles', 'authors': 'Yazan Youssef, Paulo Ricardo Marques de Araujo, Aboelmagd Noureldin, Sidney Givigi', 'link': 'https://arxiv.org/abs/2504.05407', 'abstract': "Efficient scheduling remains a critical challenge in various domains, requiring solutions to complex NP-hard optimization problems to achieve optimal resource allocation and maximize productivity. In this paper, we introduce a framework called Transformer-Based Task Scheduling System (TRATSS), designed to address the intricacies of single agent scheduling in graph-based environments. By integrating the latest advancements in reinforcement learning and transformer architecture, TRATSS provides a novel system that outputs optimized task scheduling decisions while dynamically adapting to evolving task requirements and resource availability. Leveraging the self-attention mechanism in transformers, TRATSS effectively captures complex task dependencies, thereby providing solutions with enhanced resource utilization and task completion efficiency. Experimental evaluations on benchmark datasets demonstrate TRATSS's effectiveness in providing high-quality solutions to scheduling problems that involve multiple action profiles.", 'abstract_zh': '基于Transformer的任务调度系统（TRATSS）：图环境单代理调度的高效解决方案', 'title_zh': 'TRATSS: 基于 Transformer 的自主车辆任务调度系统'}
{'arxiv_id': 'arXiv:2504.06091', 'title': 'Real-Time LaCAM', 'authors': 'Runzhe Liang, Rishi Veerapaneni, Daniel Harabor, Jiaoyang Li, Maxim Likhachev', 'link': 'https://arxiv.org/abs/2504.06091', 'abstract': 'The vast majority of Multi-Agent Path Finding (MAPF) methods with completeness guarantees require planning full horizon paths. However, planning full horizon paths can take too long and be impractical in real-world applications. Instead, real-time planning and execution, which only allows the planner a finite amount of time before executing and replanning, is more practical for real world multi-agent systems. Several methods utilize real-time planning schemes but none are provably complete, which leads to livelock or deadlock. Our main contribution is to show the first Real-Time MAPF method with provable completeness guarantees. We do this by leveraging LaCAM (Okumura 2023) in an incremental fashion. Our results show how we can iteratively plan for congested environments with a cutoff time of milliseconds while still maintaining the same success rate as full horizon LaCAM. We also show how it can be used with a single-step learned MAPF policy. The proposed Real-Time LaCAM also provides us with a general mechanism for using iterative constraints for completeness in future real-time MAPF algorithms.', 'abstract_zh': '实时 Multi-Agent Path Finding 方法的完备性保证研究', 'title_zh': '实时LaCAM'}
{'arxiv_id': 'arXiv:2504.06188', 'title': 'SkillFlow: Efficient Skill and Code Transfer Through Communication in Adapting AI Agents', 'authors': 'Pagkratios Tagkopoulos, Fangzhou Li, Ilias Tagkopoulos', 'link': 'https://arxiv.org/abs/2504.06188', 'abstract': "AI agents are autonomous systems that can execute specific tasks based on predefined programming. Here, we present SkillFlow, a modular, technology-agnostic framework that allows agents to expand their functionality in an ad-hoc fashion by acquiring new skills from their environment or other agents. We present a theoretical model that examines under which conditions this framework would be beneficial, and we then explore SkillFlow's ability to accelerate task completion and lead to lower cumulative costs in a real-world application, namely scheduling agents for calendar events. We demonstrate that within a few iterations, SkillFlow leads to considerable (24.8%, p-value = $6.4\\times10^{-3}$) gains in time and cost, especially when the communication cost is high. Finally, we draw analogies from well-studied biological systems and compare this framework to that of lateral gene transfer, a significant process of adaptation and evolution in novel environments.", 'abstract_zh': '基于预定义编程的AI代理是自主系统，能够根据特定编程执行特定任务。本文介绍了SkillFlow，这是一种模块化、技术无关的框架，允许代理通过从环境或其他代理获取新技能以即兴方式扩展其功能。我们提出了一种理论模型，探讨了在何种条件下该框架会有益，并研究了SkillFlow在实际应用中加速任务完成以及降低累积成本的能力，具体应用为日历事件调度代理。结果显示，在几轮迭代后，SkillFlow在时间和成本方面取得了显著（24.8%，p值=6.4×10^-3）的提升，尤其是在通信成本较高时更为明显。最后，我们从研究较为成熟的生物系统中汲取灵感，将此框架与水平基因转移进行类比，水平基因转移是适应和演化的重要过程，尤其在新型环境中。', 'title_zh': 'SkillFlow：通过适应性AI代理沟通实现高效技能和代码转移'}
{'arxiv_id': 'arXiv:2504.06135', 'title': 'Decentralizing AI Memory: SHIMI, a Semantic Hierarchical Memory Index for Scalable Agent Reasoning', 'authors': 'Tooraj Helmi', 'link': 'https://arxiv.org/abs/2504.06135', 'abstract': "Retrieval-Augmented Generation (RAG) and vector-based search have become foundational tools for memory in AI systems, yet they struggle with abstraction, scalability, and semantic precision - especially in decentralized environments. We present SHIMI (Semantic Hierarchical Memory Index), a unified architecture that models knowledge as a dynamically structured hierarchy of concepts, enabling agents to retrieve information based on meaning rather than surface similarity. SHIMI organizes memory into layered semantic nodes and supports top-down traversal from abstract intent to specific entities, offering more precise and explainable retrieval. Critically, SHIMI is natively designed for decentralized ecosystems, where agents maintain local memory trees and synchronize them asynchronously across networks. We introduce a lightweight sync protocol that leverages Merkle-DAG summaries, Bloom filters, and CRDT-style conflict resolution to enable partial synchronization with minimal overhead. Through benchmark experiments and use cases involving decentralized agent collaboration, we demonstrate SHIMI's advantages in retrieval accuracy, semantic fidelity, and scalability - positioning it as a core infrastructure layer for decentralized cognitive systems.", 'abstract_zh': 'SHIMI：语义层次记忆索引', 'title_zh': '分散化AI记忆：SHIMI，一种语义分层记忆索引，用于可扩展的智能体推理'}
{'arxiv_id': 'arXiv:2504.06020', 'title': 'Information-Theoretic Reward Decomposition for Generalizable RLHF', 'authors': 'Liyuan Mao, Haoran Xu, Amy Zhang, Weinan Zhang, Chenjia Bai', 'link': 'https://arxiv.org/abs/2504.06020', 'abstract': 'A generalizable reward model is crucial in Reinforcement Learning from Human Feedback (RLHF) as it enables correctly evaluating unseen prompt-response pairs. However, existing reward models lack this ability, as they are typically trained by increasing the reward gap between chosen and rejected responses, while overlooking the prompts that the responses are conditioned on. Consequently, when the trained reward model is evaluated on prompt-response pairs that lie outside the data distribution, neglecting the effect of prompts may result in poor generalization of the reward model. To address this issue, we decompose the reward value into two independent components: prompt-free reward and prompt-related reward. Prompt-free reward represents the evaluation that is determined only by responses, while the prompt-related reward reflects the reward that derives from both the prompt and the response. We extract these two components from an information-theoretic perspective, which requires no extra models. Subsequently, we propose a new reward learning algorithm by prioritizing data samples based on their prompt-free reward values. Through toy examples, we demonstrate that the extracted prompt-free and prompt-related rewards effectively characterize two parts of the reward model. Further, standard evaluations show that our method improves both the alignment performance and the generalization capability of the reward model.', 'abstract_zh': '可泛化奖励模型对于从人类反馈强化学习（RLHF）至关重要，因为它使得正确评估未见过的提示-响应对成为可能。然而，现有的奖励模型缺乏这一能力，因为它们通常通过增加所选和拒绝响应之间的奖励差距来进行训练，而忽视了响应所依赖的提示。因此，当训练好的奖励模型应用于数据分布之外的提示-响应对时，忽视提示的影响可能导致奖励模型泛化能力较差。为解决这一问题，我们将奖励价值分解为两个独立的组成部分：与提示无关的奖励和与提示相关的奖励。与提示无关的奖励仅由响应决定的评估，而与提示相关的奖励反映了来自提示和响应两者的奖励。我们从信息论的角度提取这两个组成部分，无需额外模型。随后，我们提出了一种新的奖励学习算法，根据数据样本的与提示无关的奖励值优先处理数据样本。通过玩具示例，我们展示了提取的与提示无关和与提示相关的奖励有效地表征了奖励模型的两部分。进一步的标准评估表明，我们的方法提高了奖励模型的对齐性能和泛化能力。', 'title_zh': '信息论奖励分解促进通用的RLHF'}
{'arxiv_id': 'arXiv:2504.05951', 'title': 'Representing Normative Regulations in OWL DL for Automated Compliance Checking Supported by Text Annotation', 'authors': 'Ildar Baimuratov, Denis Turygin', 'link': 'https://arxiv.org/abs/2504.05951', 'abstract': 'Compliance checking is the process of determining whether a regulated entity adheres to these regulations. Currently, compliance checking is predominantly manual, requiring significant time and highly skilled experts, while still being prone to errors caused by the human factor. Various approaches have been explored to automate compliance checking, however, representing regulations in OWL DL language which enables compliance checking through OWL reasoning has not been adopted. In this work, we propose an annotation schema and an algorithm that transforms text annotations into machine-interpretable OWL DL code. The proposed approach is validated through a proof-of-concept implementation applied to examples from the building construction domain.', 'abstract_zh': '合规检查是确定受监管实体是否遵守相关法规的过程。目前，合规检查主要依赖人工进行，耗时且需要高度专业的专家，仍然容易受到人为因素导致的错误。尽管已经探索了多种自动化合规检查的方法，但通过OWL DL语言表示法规以利用OWL推理进行合规检查的方法尚未被采用。本文提出了一种标注方案和算法，将文本标注转换为机器可解释的OWL DL代码。所提出的方法通过建筑施工领域的实例概念证明实现进行了验证。', 'title_zh': '基于文本标注支持的自动合规检查的OWL DL中规范性规则表示'}
{'arxiv_id': 'arXiv:2504.05950', 'title': 'AEGIS: Human Attention-based Explainable Guidance for Intelligent Vehicle Systems', 'authors': 'Zhuoli Zhuang, Cheng-You Lu, Yu-Cheng Fred Chang, Yu-Kai Wang, Thomas Do, Chin-Teng Lin', 'link': 'https://arxiv.org/abs/2504.05950', 'abstract': 'Improving decision-making capabilities in Autonomous Intelligent Vehicles (AIVs) has been a heated topic in recent years. Despite advancements, training machines to capture regions of interest for comprehensive scene understanding, like human perception and reasoning, remains a significant challenge. This study introduces a novel framework, Human Attention-based Explainable Guidance for Intelligent Vehicle Systems (AEGIS). AEGIS utilizes human attention, converted from eye-tracking, to guide reinforcement learning (RL) models to identify critical regions of interest for decision-making. AEGIS uses a pre-trained human attention model to guide RL models to identify critical regions of interest for decision-making. By collecting 1.2 million frames from 20 participants across six scenarios, AEGIS pre-trains a model to predict human attention patterns.', 'abstract_zh': '提高自主智能车辆（AIVs）的决策能力一直是近年来的一个热点话题。尽管取得了进展，但训练机器捕捉对全面场景理解至关重要的区域，如人类感知和推理，仍然是一个重大挑战。本研究提出了一种新的框架，基于人类注意力的可解释引导自主智能车辆系统（AEGIS）。AEGIS 利用从眼动追踪转换而来的人类注意力来引导强化学习（RL）模型识别决策所需的关键区域。AEGIS 使用预训练的人类注意力模型来引导 RL 模型识别决策所需的关键区域。通过来自六个场景的 20 名参与者收集的 120 万帧数据，AEGIS 预训练了一个人类注意力模式预测模型。', 'title_zh': 'AEGIS：基于人类注意力的可解释指导智能车辆系统'}
{'arxiv_id': 'arXiv:2504.05874', 'title': 'Systematic Parameter Decision in Approximate Model Counting', 'authors': 'Jinping Lei, Toru Takisaka, Junqiang Peng, Mingyu Xiao', 'link': 'https://arxiv.org/abs/2504.05874', 'abstract': "This paper proposes a novel approach to determining the internal parameters of the hashing-based approximate model counting algorithm $\\mathsf{ApproxMC}$. In this problem, the chosen parameter values must ensure that $\\mathsf{ApproxMC}$ is Probably Approximately Correct (PAC), while also making it as efficient as possible. The existing approach to this problem relies on heuristics; in this paper, we solve this problem by formulating it as an optimization problem that arises from generalizing $\\mathsf{ApproxMC}$'s correctness proof to arbitrary parameter values.\nOur approach separates the concerns of algorithm soundness and optimality, allowing us to address the former without the need for repetitive case-by-case argumentation, while establishing a clear framework for the latter. Furthermore, after reduction, the resulting optimization problem takes on an exceptionally simple form, enabling the use of a basic search algorithm and providing insight into how parameter values affect algorithm performance. Experimental results demonstrate that our optimized parameters improve the runtime performance of the latest $\\mathsf{ApproxMC}$ by a factor of 1.6 to 2.4, depending on the error tolerance.", 'abstract_zh': '本文提出了一种确定基于哈希的近似模型计数算法 $\\mathsf{ApproxMC}$ 内部参数的新型方法。在这种情况下，选择的参数值必须确保 $\\mathsf{ApproxMC}$ 达到可能近似正确的（PAC）标准，同时也要使其尽可能高效。现有方法依赖于启发式方法；本文通过将 $\\mathsf{ApproxMC}$ 的正确性证明推广到任意参数值，将其问题形式化为一个优化问题来解决这一问题。', 'title_zh': '近似模型计数中的系统参数决策'}
{'arxiv_id': 'arXiv:2504.05806', 'title': 'Meta-Continual Learning of Neural Fields', 'authors': 'Seungyoon Woo, Junhyeog Yun, Gunhee Kim', 'link': 'https://arxiv.org/abs/2504.05806', 'abstract': "Neural Fields (NF) have gained prominence as a versatile framework for complex data representation. This work unveils a new problem setting termed \\emph{Meta-Continual Learning of Neural Fields} (MCL-NF) and introduces a novel strategy that employs a modular architecture combined with optimization-based meta-learning. Focused on overcoming the limitations of existing methods for continual learning of neural fields, such as catastrophic forgetting and slow convergence, our strategy achieves high-quality reconstruction with significantly improved learning speed. We further introduce Fisher Information Maximization loss for neural radiance fields (FIM-NeRF), which maximizes information gains at the sample level to enhance learning generalization, with proved convergence guarantee and generalization bound. We perform extensive evaluations across image, audio, video reconstruction, and view synthesis tasks on six diverse datasets, demonstrating our method's superiority in reconstruction quality and speed over existing MCL and CL-NF approaches. Notably, our approach attains rapid adaptation of neural fields for city-scale NeRF rendering with reduced parameter requirement.", 'abstract_zh': '基于神经场的元持续学习（MCL-NF）：模块化架构结合优化元学习的新策略', 'title_zh': '元持续学习的神经场学习'}
{'arxiv_id': 'arXiv:2504.05728', 'title': 'AI-Driven Prognostics for State of Health Prediction in Li-ion Batteries: A Comprehensive Analysis with Validation', 'authors': 'Tianqi Ding, Dawei Xiang, Tianyao Sun, YiJiashum Qi, Zunduo Zhao', 'link': 'https://arxiv.org/abs/2504.05728', 'abstract': 'This paper presents a comprehensive review of AI-driven prognostics for State of Health (SoH) prediction in lithium-ion batteries. We compare the effectiveness of various AI algorithms, including FFNN, LSTM, and BiLSTM, across multiple datasets (CALCE, NASA, UDDS) and scenarios (e.g., varying temperatures and driving conditions). Additionally, we analyze the factors influencing SoH fluctuations, such as temperature and charge-discharge rates, and validate our findings through simulations. The results demonstrate that BiLSTM achieves the highest accuracy, with an average RMSE reduction of 15% compared to LSTM, highlighting its robustness in real-world applications.', 'abstract_zh': '本论文对AI驱动的锂离子电池健康状态（SoH）预测进行了全面综述，并比较了各种AI算法（包括FFNN、LSTM和BiLSTM）在多个数据集（CALCE、NASA、UDDS）和不同场景（如温度和驱动条件变化）下的有效性。此外，我们分析了影响SoH波动的因素，如温度和充放电速率，并通过仿真验证了研究结果。研究结果表明，BiLSTM在准确性方面表现最佳，与LSTM相比，平均RMSE降低了15%，突显了其在实际应用中的稳健性。', 'title_zh': '基于AI驱动的锂离子电池健康状态预测 prognostics：一种综合验证分析'}
{'arxiv_id': 'arXiv:2504.05691', 'title': 'StayLTC: A Cost-Effective Multimodal Framework for Hospital Length of Stay Forecasting', 'authors': 'Sudeshna Jana, Manjira Sinha, Tirthankar Dasgupta', 'link': 'https://arxiv.org/abs/2504.05691', 'abstract': 'Accurate prediction of Length of Stay (LOS) in hospitals is crucial for improving healthcare services, resource management, and cost efficiency. This paper presents StayLTC, a multimodal deep learning framework developed to forecast real-time hospital LOS using Liquid Time-Constant Networks (LTCs). LTCs, with their continuous-time recurrent dynamics, are evaluated against traditional models using structured data from Electronic Health Records (EHRs) and clinical notes. Our evaluation, conducted on the MIMIC-III dataset, demonstrated that LTCs significantly outperform most of the other time series models, offering enhanced accuracy, robustness, and efficiency in resource utilization. Additionally, LTCs demonstrate a comparable performance in LOS prediction compared to time series large language models, while requiring significantly less computational power and memory, underscoring their potential to advance Natural Language Processing (NLP) tasks in healthcare.', 'abstract_zh': '准确预测医院住院时间对于提高医疗服务质量、资源管理及成本效率至关重要。本文介绍了一种基于液态时间常数网络（LTCs）的多模态深度学习框架StayLTC，用于实时预测医院住院时间。我们在MIMIC-III数据集上的评估显示，LTCs在时间序列预测任务中显著优于其他传统模型，提供了更高的准确性和资源利用效率。此外，LTCs在住院时间预测任务上的性能与时间序列大语言模型相当，但所需计算资源和内存远少于后者，突显了其在医疗自然语言处理任务中的应用潜力。', 'title_zh': 'StayLTC：一种成本效益高的多模态医院住院时长预测框架'}
{'arxiv_id': 'arXiv:2504.05419', 'title': "Reasoning Models Know When They're Right: Probing Hidden States for Self-Verification", 'authors': 'Anqi Zhang, Yulin Chen, Jane Pan, Chen Zhao, Aurojit Panda, Jinyang Li, He He', 'link': 'https://arxiv.org/abs/2504.05419', 'abstract': "Reasoning models have achieved remarkable performance on tasks like math and logical reasoning thanks to their ability to search during reasoning. However, they still suffer from overthinking, often performing unnecessary reasoning steps even after reaching the correct answer. This raises the question: can models evaluate the correctness of their intermediate answers during reasoning? In this work, we study whether reasoning models encode information about answer correctness through probing the model's hidden states. The resulting probe can verify intermediate answers with high accuracy and produces highly calibrated scores. Additionally, we find models' hidden states encode correctness of future answers, enabling early prediction of the correctness before the intermediate answer is fully formulated. We then use the probe as a verifier to decide whether to exit reasoning at intermediate answers during inference, reducing the number of inference tokens by 24\\% without compromising performance. These findings confirm that reasoning models do encode a notion of correctness yet fail to exploit it, revealing substantial untapped potential to enhance their efficiency.", 'abstract_zh': '推理模型通过探测模型隐藏状态来编码答案正确性的信息，从而在推理过程中评估中间答案的正确性，提高推理效率', 'title_zh': '推理模型知其所以然：探查隐藏状态实现自我验证'}
{'arxiv_id': 'arXiv:2504.06235', 'title': 'Decentralized Federated Domain Generalization with Style Sharing: A Formal Modeling and Convergence Analysis', 'authors': 'Shahryar Zehtabi, Dong-Jun Han, Seyyedali Hosseinalipour, Christopher G. Brinton', 'link': 'https://arxiv.org/abs/2504.06235', 'abstract': 'Much of the federated learning (FL) literature focuses on settings where local dataset statistics remain the same between training and testing time. Recent advances in domain generalization (DG) aim to use data from source (training) domains to train a model that generalizes well to data from unseen target (testing) domains. In this paper, we are motivated by two major gaps in existing work on FL and DG: (1) the lack of formal mathematical analysis of DG objectives and training processes; and (2) DG research in FL being limited to the conventional star-topology architecture. Addressing the second gap, we develop $\\textit{Decentralized Federated Domain Generalization with Style Sharing}$ ($\\texttt{StyleDDG}$), a fully decentralized DG algorithm designed to allow devices in a peer-to-peer network to achieve DG based on sharing style information inferred from their datasets. Additionally, we fill the first gap by providing the first systematic approach to mathematically analyzing style-based DG training optimization. We cast existing centralized DG algorithms within our framework, and employ their formalisms to model $\\texttt{StyleDDG}$. Based on this, we obtain analytical conditions under which a sub-linear convergence rate of $\\texttt{StyleDDG}$ can be obtained. Through experiments on two popular DG datasets, we demonstrate that $\\texttt{StyleDDG}$ can obtain significant improvements in accuracy across target domains with minimal added communication overhead compared to decentralized gradient methods that do not employ style sharing.', 'abstract_zh': '去中心化联邦风格共享领域泛化算法', 'title_zh': '去中心化联邦领域泛化中的风格共享：正式建模与收敛性分析'}
{'arxiv_id': 'arXiv:2504.06207', 'title': 'An experimental survey and Perspective View on Meta-Learning for Automated Algorithms Selection and Parametrization', 'authors': 'Moncef Garouani', 'link': 'https://arxiv.org/abs/2504.06207', 'abstract': 'Considerable progress has been made in the recent literature studies to tackle the Algorithms Selection and Parametrization (ASP) problem, which is diversified in multiple meta-learning setups. Yet there is a lack of surveys and comparative evaluations that critically analyze, summarize and assess the performance of existing methods. In this paper, we provide an overview of the state of the art in this continuously evolving field. The survey sheds light on the motivational reasons for pursuing classifiers selection through meta-learning. In this regard, Automated Machine Learning (AutoML) is usually treated as an ASP problem under the umbrella of the democratization of machine learning. Accordingly, AutoML makes machine learning techniques accessible to domain scientists who are interested in applying advanced analytics but lack the required expertise. It can ease the task of manually selecting ML algorithms and tuning related hyperparameters. We comprehensively discuss the different phases of classifiers selection based on a generic framework that is formed as an outcome of reviewing prior works. Subsequently, we propose a benchmark knowledge base of 4 millions previously learned models and present extensive comparative evaluations of the prominent methods for classifiers selection based on 08 classification algorithms and 400 benchmark datasets. The comparative study quantitatively assesses the performance of algorithms selection methods along while emphasizing the strengths and limitations of existing studies.', 'abstract_zh': '近期文献在处理算法选择与参数化（ASP）问题方面取得了显著进展，该问题在多种元学习设置中表现出多样性。然而，缺乏对现有方法进行批判性分析、总结和评估的综述和比较评估。本文提供了该不断发展的领域的一种综述，探讨了通过元学习进行分类器选择的动机原因。在此方面，自动化机器学习（AutoML）通常被视作ASP问题，作为使机器学习技术普惠的手段，使希望应用先进分析但缺乏所需专业知识的领域科学家能够利用机器学习技术。AutoML可以简化手动选择ML算法及其相关超参数调整的任务。本文基于审阅先前工作形成的一般框架，全面讨论了分类器选择的不同阶段。随后，我们提出一个包含400万之前学习模型的基准知识库，并基于8种分类算法和400个基准数据集，对分类器选择的主导方法进行了详尽的比较评估。比较研究不仅定量评估了算法选择方法的性能，还强调了现有研究的优缺点。', 'title_zh': '元学习在自动化算法选择与参数化中的实验调查与视角分析'}
{'arxiv_id': 'arXiv:2504.06193', 'title': 'Heuristic Methods are Good Teachers to Distill MLPs for Graph Link Prediction', 'authors': 'Zongyue Qin, Shichang Zhang, Mingxuan Ju, Tong Zhao, Neil Shah, Yizhou Sun', 'link': 'https://arxiv.org/abs/2504.06193', 'abstract': 'Link prediction is a crucial graph-learning task with applications including citation prediction and product recommendation. Distilling Graph Neural Networks (GNNs) teachers into Multi-Layer Perceptrons (MLPs) students has emerged as an effective approach to achieve strong performance and reducing computational cost by removing graph dependency. However, existing distillation methods only use standard GNNs and overlook alternative teachers such as specialized model for link prediction (GNN4LP) and heuristic methods (e.g., common neighbors). This paper first explores the impact of different teachers in GNN-to-MLP distillation. Surprisingly, we find that stronger teachers do not always produce stronger students: MLPs distilled from GNN4LP can underperform those distilled from simpler GNNs, while weaker heuristic methods can teach MLPs to near-GNN performance with drastically reduced training costs. Building on these insights, we propose Ensemble Heuristic-Distilled MLPs (EHDM), which eliminates graph dependencies while effectively integrating complementary signals via a gating mechanism. Experiments on ten datasets show an average 7.93% improvement over previous GNN-to-MLP approaches with 1.95-3.32 times less training time, indicating EHDM is an efficient and effective link prediction method.', 'abstract_zh': '链接预测是包括引文预测和产品推荐在内的图学习关键任务。将图神经网络（GNNs）教师提炼成多层感知器（MLPs）学生已成为一种有效的方法，通过去除图依赖来实现强大的性能并减少计算成本。然而，现有的提炼方法仅使用标准的GNNs，忽略了专为链接预测设计的模型（如GNN4LP）和启发式方法（如共同邻居）等其他教师的影响。本文首先探讨了不同教师对GNN-to-MLP提炼的影响。令人惊讶的是，更强的教师并不总是产生更强的学生：从GNN4LP提炼的MLPs可能会逊色于从简单GNN提炼的学生，而较弱的启发式方法能在大幅减少训练成本的情况下将MLPs训练到接近GNN性能。基于这些见解，我们提出了集成启发式提炼的MLPs（EHDM），通过门控机制有效地整合互补信号，消除了图依赖。在十个数据集上的实验表明，与之前的GNN-to-MLP方法相比，EHDM平均提高了7.93%的性能，并且训练时间减少了1.95-3.32倍，表明EHDM是一种高效且有效的链接预测方法。', 'title_zh': '启发式方法是良好的教师，用于提炼用于图链接预测的MLP模型'}
{'arxiv_id': 'arXiv:2504.06176', 'title': 'A Self-Supervised Framework for Space Object Behaviour Characterisation', 'authors': 'Ian Groves, Andrew Campbell, James Fernandes, Diego Rodriguez, Paul Murray, Massimiliano Vasile, Victoria Nockles', 'link': 'https://arxiv.org/abs/2504.06176', 'abstract': 'Foundation Models, pre-trained on large unlabelled datasets before task-specific fine-tuning, are increasingly being applied to specialised domains. Recent examples include ClimaX for climate and Clay for satellite Earth observation, but a Foundation Model for Space Object Behavioural Analysis has not yet been developed. As orbital populations grow, automated methods for characterising space object behaviour are crucial for space safety. We present a Space Safety and Sustainability Foundation Model focusing on space object behavioural analysis using light curves (LCs). We implemented a Perceiver-Variational Autoencoder (VAE) architecture, pre-trained with self-supervised reconstruction and masked reconstruction on 227,000 LCs from the MMT-9 observatory. The VAE enables anomaly detection, motion prediction, and LC generation. We fine-tuned the model for anomaly detection & motion prediction using two independent LC simulators (CASSANDRA and GRIAL respectively), using CAD models of boxwing, Sentinel-3, SMOS, and Starlink platforms. Our pre-trained model achieved a reconstruction error of 0.01%, identifying potentially anomalous light curves through reconstruction difficulty. After fine-tuning, the model scored 88% and 82% accuracy, with 0.90 and 0.95 ROC AUC scores respectively in both anomaly detection and motion mode prediction (sun-pointing, spin, etc.). Analysis of high-confidence anomaly predictions on real data revealed distinct patterns including characteristic object profiles and satellite glinting. Here, we demonstrate how self-supervised learning can simultaneously enable anomaly detection, motion prediction, and synthetic data generation from rich representations learned in pre-training. Our work therefore supports space safety and sustainability through automated monitoring and simulation capabilities.', 'abstract_zh': '基于大型未标注数据集预训练后再进行任务特定微调的基础模型正 increasingly被应用于专门领域。近期的例子包括用于气候分析的ClimaX和用于卫星地球观测的Clay，但尚未开发出用于太空物体行为分析的基础模型。随着轨道物体数量的增长，自动化的太空物体行为表征方法对于太空安全至关重要。我们提出了一种专注于太空物体行为分析的太空安全与可持续性基础模型，采用光变曲线（LC）。我们实现了Perceiver-变分自编码器（VAE）架构，使用自监督重建和掩码重建在MMT-9观测站的227,000个LC上进行预训练。VAE能够实现异常检测、运动预测和光变曲线生成。我们使用两个独立的光变曲线模拟器（CASSANDRA和GRIAL）分别对模型进行了微调，使用盒翼、Sentinel-3、SMOS和Starlink平台的CAD模型。预训练模型的重建误差为0.01%，并通过重建难度识别出潜在异常的光变曲线。微调后，模型在异常检测和运动模式预测（太阳定向、自旋等）中的准确率分别为88%和82%，ROC AUC分数分别为0.90和0.95。对真实数据中高置信度异常预测的分析揭示了不同的模式，包括特征物体配置和卫星反射现象。通过自监督学习，我们展示了如何同时实现异常检测、运动预测和从丰富表示学习中生成合成数据的能力。因此，我们的工作通过自动监测和模拟能力支持了太空安全与可持续性。', 'title_zh': '自我监督的空间目标行为特征化框架'}
{'arxiv_id': 'arXiv:2504.06173', 'title': 'Multi-Modality Sensing in mmWave Beamforming for Connected Vehicles Using Deep Learning', 'authors': 'Muhammad Baqer Mollah, Honggang Wang, Mohammad Ataul Karim, Hua Fang', 'link': 'https://arxiv.org/abs/2504.06173', 'abstract': 'Beamforming techniques are considered as essential parts to compensate severe path losses in millimeter-wave (mmWave) communications. In particular, these techniques adopt large antenna arrays and formulate narrow beams to obtain satisfactory received powers. However, performing accurate beam alignment over narrow beams for efficient link configuration by traditional standard defined beam selection approaches, which mainly rely on channel state information and beam sweeping through exhaustive searching, imposes computational and communications overheads. And, such resulting overheads limit their potential use in vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communications involving highly dynamic scenarios. In comparison, utilizing out-of-band contextual information, such as sensing data obtained from sensor devices, provides a better alternative to reduce overheads. This paper presents a deep learning-based solution for utilizing the multi-modality sensing data for predicting the optimal beams having sufficient mmWave received powers so that the best V2I and V2V line-of-sight links can be ensured proactively. The proposed solution has been tested on real-world measured mmWave sensing and communication data, and the results show that it can achieve up to 98.19% accuracies while predicting top-13 beams. Correspondingly, when compared to existing been sweeping approach, the beam sweeping searching space and time overheads are greatly shortened roughly by 79.67% and 91.89%, respectively which confirm a promising solution for beamforming in mmWave enabled communications.', 'abstract_zh': '基于深度学习的多模态感知数据应用于预测最优毫米波波束以确保车辆到基础设施和车辆到车辆的直线视距连接', 'title_zh': '基于深学习的连接车辆毫米波波束形成多模态感知'}
{'arxiv_id': 'arXiv:2504.06016', 'title': 'The Hall of AI Fears and Hopes: Comparing the Views of AI Influencers and those of Members of the U.S. Public Through an Interactive Platform', 'authors': 'Gustavo Moreira, Edyta Paulina Bogucka, Marios Constantinides, Daniele Quercia', 'link': 'https://arxiv.org/abs/2504.06016', 'abstract': "AI development is shaped by academics and industry leaders - let us call them ``influencers'' - but it is unclear how their views align with those of the public. To address this gap, we developed an interactive platform that served as a data collection tool for exploring public views on AI, including their fears, hopes, and overall sense of hopefulness. We made the platform available to 330 participants representative of the U.S. population in terms of age, sex, ethnicity, and political leaning, and compared their views with those of 100 AI influencers identified by Time magazine. The public fears AI getting out of control, while influencers emphasize regulation, seemingly to deflect attention from their alleged focus on monetizing AI's potential. Interestingly, the views of AI influencers from underrepresented groups such as women and people of color often differ from the views of underrepresented groups in the public.", 'abstract_zh': 'AI的发展受到学术界和工业界领导者的塑造——让我们称之为“影响者”——但他们的观点与公众的观点尚不清楚。为了填补这一空白，我们开发了一个交互平台，作为收集公众对AI看法的数据工具，包括他们的恐惧、希望以及总体的乐观感。我们将这一平台提供给330名在年龄、性别、种族和政治倾向上具有代表性的美国人口参与者，并将他们的观点与《时代》杂志认定的100名AI影响者的观点进行了比较。公众担心AI失控，而影响者则强调监管，似乎是为了转移公众对他们可能利用AI潜在价值的关注。有趣的是，代表性不足群体（如女性和有色人种）的影响者与公众中代表性不足群体的观点往往存在差异。', 'title_zh': 'AI恐惧与希望之堂：通过交互平台对比AI影响者与美国公众的观点'}
{'arxiv_id': 'arXiv:2504.05945', 'title': 'CKGAN: Training Generative Adversarial Networks Using Characteristic Kernel Integral Probability Metrics', 'authors': 'Kuntian Zhang, Simin Yu, Yaoshu Wang, Makoto Onizuka, Chuan Xiao', 'link': 'https://arxiv.org/abs/2504.05945', 'abstract': 'In this paper, we propose CKGAN, a novel generative adversarial network (GAN) variant based on an integral probability metrics framework with characteristic kernel (CKIPM). CKIPM, as a distance between two probability distributions, is designed to optimize the lowerbound of the maximum mean discrepancy (MMD) in a reproducing kernel Hilbert space, and thus can be used to train GANs. CKGAN mitigates the notorious problem of mode collapse by mapping the generated images back to random noise. To save the effort of selecting the kernel function manually, we propose a soft selection method to automatically learn a characteristic kernel function. The experimental evaluation conducted on a set of synthetic and real image benchmarks (MNIST, CelebA, etc.) demonstrates that CKGAN generally outperforms other MMD-based GANs. The results also show that at the cost of moderately more training time, the automatically selected kernel function delivers very close performance to the best of manually fine-tuned one on real image benchmarks and is able to improve the performances of other MMD-based GANs.', 'abstract_zh': '基于特征内积概率度量的CKGAN生成式对抗网络', 'title_zh': 'CKGAN：使用特征核积分概率度量训练生成对抗网络'}
{'arxiv_id': 'arXiv:2504.05923', 'title': 'Uncovering Fairness through Data Complexity as an Early Indicator', 'authors': 'Juliett Suárez Ferreira, Marija Slavkovik, Jorge Casillas', 'link': 'https://arxiv.org/abs/2504.05923', 'abstract': 'Fairness constitutes a concern within machine learning (ML) applications. Currently, there is no study on how disparities in classification complexity between privileged and unprivileged groups could influence the fairness of solutions, which serves as a preliminary indicator of potential unfairness. In this work, we investigate this gap, specifically, we focus on synthetic datasets designed to capture a variety of biases ranging from historical bias to measurement and representational bias to evaluate how various complexity metrics differences correlate with group fairness metrics. We then apply association rule mining to identify patterns that link disproportionate complexity differences between groups with fairness-related outcomes, offering data-centric indicators to guide bias mitigation. Our findings are also validated by their application in real-world problems, providing evidence that quantifying group-wise classification complexity can uncover early indicators of potential fairness challenges. This investigation helps practitioners to proactively address bias in classification tasks.', 'abstract_zh': '公平性是机器学习（ML）应用中的一个关注点。目前，尚未有研究探讨分类复杂度在特权和非特权群体之间的差异如何影响解决方案的公平性，这被视为潜在不公平性的初步指标。在这项工作中，我们探讨了这一差距，具体而言，我们关注设计用于捕捉历史偏差、测量偏差和表征偏差等多种偏差的合成数据集，以评估各种复杂度度量差异与群体公平性度量之间的关联。然后，我们应用关联规则挖掘来识别将群体之间不均衡的复杂度差异与公平性相关结果联系起来的模式，提供以数据为中心的指标来指导偏差缓解。我们的发现也通过应用于实际问题得到了验证，提供了证据表明量化群体间的分类复杂性可以揭示潜在公平性挑战的早期指标。这项调查有助于实践者在分类任务中前瞻性地应对偏差。', 'title_zh': '通过数据复杂性作为早期指标发现公平性'}
{'arxiv_id': 'arXiv:2504.05908', 'title': 'PRIMEDrive-CoT: A Precognitive Chain-of-Thought Framework for Uncertainty-Aware Object Interaction in Driving Scene Scenario', 'authors': 'Sriram Mandalika, Lalitha V, Athira Nambiar', 'link': 'https://arxiv.org/abs/2504.05908', 'abstract': 'Driving scene understanding is a critical real-world problem that involves interpreting and associating various elements of a driving environment, such as vehicles, pedestrians, and traffic signals. Despite advancements in autonomous driving, traditional pipelines rely on deterministic models that fail to capture the probabilistic nature and inherent uncertainty of real-world driving. To address this, we propose PRIMEDrive-CoT, a novel uncertainty-aware model for object interaction and Chain-of-Thought (CoT) reasoning in driving scenarios. In particular, our approach combines LiDAR-based 3D object detection with multi-view RGB references to ensure interpretable and reliable scene understanding. Uncertainty and risk assessment, along with object interactions, are modelled using Bayesian Graph Neural Networks (BGNNs) for probabilistic reasoning under ambiguous conditions. Interpretable decisions are facilitated through CoT reasoning, leveraging object dynamics and contextual cues, while Grad-CAM visualizations highlight attention regions. Extensive evaluations on the DriveCoT dataset demonstrate that PRIMEDrive-CoT outperforms state-of-the-art CoT and risk-aware models.', 'abstract_zh': 'PRIMEDrive-CoT：一种用于驾驶场景的认知推理和不确定性感知模型', 'title_zh': 'PRIMEDrive-CoT: 一种用于驾驶场景中不确定性aware对象交互的先知性链思考框架'}
{'arxiv_id': 'arXiv:2504.05840', 'title': 'Momentum Boosted Episodic Memory for Improving Learning in Long-Tailed RL Environments', 'authors': 'Dolton Fernandes, Pramod Kaushik, Harsh Shukla, Bapi Raju Surampudi', 'link': 'https://arxiv.org/abs/2504.05840', 'abstract': 'Traditional Reinforcement Learning (RL) algorithms assume the distribution of the data to be uniform or mostly uniform. However, this is not the case with most real-world applications like autonomous driving or in nature where animals roam. Some experiences are encountered frequently, and most of the remaining experiences occur rarely; the resulting distribution is called Zipfian. Taking inspiration from the theory of complementary learning systems, an architecture for learning from Zipfian distributions is proposed where important long tail trajectories are discovered in an unsupervised manner. The proposal comprises an episodic memory buffer containing a prioritised memory module to ensure important rare trajectories are kept longer to address the Zipfian problem, which needs credit assignment to happen in a sample efficient manner. The experiences are then reinstated from episodic memory and given weighted importance forming the trajectory to be executed. Notably, the proposed architecture is modular, can be incorporated in any RL architecture and yields improved performance in multiple Zipfian tasks over traditional architectures. Our method outperforms IMPALA by a significant margin on all three tasks and all three evaluation metrics (Zipfian, Uniform, and Rare Accuracy) and also gives improvements on most Atari environments that are considered challenging', 'abstract_zh': '源自Zipfian分发的无监督学习架构：一种处理稀有重要轨迹的强化学习方法', 'title_zh': '动量增强的 episodic 记忆以改善长尾 RL 环境中的学习'}
{'arxiv_id': 'arXiv:2504.05838', 'title': 'Mind the Trojan Horse: Image Prompt Adapter Enabling Scalable and Deceptive Jailbreaking', 'authors': 'Junxi Chen, Junhao Dong, Xiaohua Xie', 'link': 'https://arxiv.org/abs/2504.05838', 'abstract': "Recently, the Image Prompt Adapter (IP-Adapter) has been increasingly integrated into text-to-image diffusion models (T2I-DMs) to improve controllability. However, in this paper, we reveal that T2I-DMs equipped with the IP-Adapter (T2I-IP-DMs) enable a new jailbreak attack named the hijacking attack. We demonstrate that, by uploading imperceptible image-space adversarial examples (AEs), the adversary can hijack massive benign users to jailbreak an Image Generation Service (IGS) driven by T2I-IP-DMs and mislead the public to discredit the service provider. Worse still, the IP-Adapter's dependency on open-source image encoders reduces the knowledge required to craft AEs. Extensive experiments verify the technical feasibility of the hijacking attack. In light of the revealed threat, we investigate several existing defenses and explore combining the IP-Adapter with adversarially trained models to overcome existing defenses' limitations. Our code is available at this https URL.", 'abstract_zh': '最近，Image Prompt Adapter (IP-Adapter) 越来越多地被集成到文本到图像扩散模型（T2I-DMs）中以提高可控性。然而，在本文中，我们揭示出配备 IP-Adapter 的 T2I-DMs（T2I-IP-DMs）能够启用一种新的 Jailbreak 攻击——劫持攻击。我们证明通过上传不可感知的图像空间 adversarial examples (AEs)，攻击者可以劫持大量无辜用户，使他们 Jailbreak 由 T2I-IP-DMs 驱动的图像生成服务（IGS），误导公众对服务提供商产生不良看法。更糟的是，IP-Adapter 对开源图像编码器的依赖降低了构造 AEs 所需的知识。广泛实验验证了劫持攻击的技术可行性。鉴于揭示出的威胁，我们调查了几种现有防御措施，并探讨将 IP-Adapter 与对抗训练模型结合以克服现有防御措施的局限性。我们的代码可在以下网址获取。', 'title_zh': '警惕木马 Horse：图像提示适配器实现可扩展且欺骗性的 Jailbreaking'}
{'arxiv_id': 'arXiv:2504.05815', 'title': 'Parasite: A Steganography-based Backdoor Attack Framework for Diffusion Models', 'authors': 'Jiahao Chen, Yu Pan, Yi Du, Chunkai Wu, Lin Wang', 'link': 'https://arxiv.org/abs/2504.05815', 'abstract': 'Recently, the diffusion model has gained significant attention as one of the most successful image generation models, which can generate high-quality images by iteratively sampling noise. However, recent studies have shown that diffusion models are vulnerable to backdoor attacks, allowing attackers to enter input data containing triggers to activate the backdoor and generate their desired output. Existing backdoor attack methods primarily focused on target noise-to-image and text-to-image tasks, with limited work on backdoor attacks in image-to-image tasks. Furthermore, traditional backdoor attacks often rely on a single, conspicuous trigger to generate a fixed target image, lacking concealability and flexibility. To address these limitations, we propose a novel backdoor attack method called "Parasite" for image-to-image tasks in diffusion models, which not only is the first to leverage steganography for triggers hiding, but also allows attackers to embed the target content as a backdoor trigger to achieve a more flexible attack. "Parasite" as a novel attack method effectively bypasses existing detection frameworks to execute backdoor attacks. In our experiments, "Parasite" achieved a 0 percent backdoor detection rate against the mainstream defense frameworks. In addition, in the ablation study, we discuss the influence of different hiding coefficients on the attack results. You can find our code at this https URL.', 'abstract_zh': 'Recent 差生攻击：一种高效的图像生成模型中的隐秘后门攻击方法', 'title_zh': '寄生虫：一种基于隐写术的扩散模型后门攻击框架'}
{'arxiv_id': 'arXiv:2504.05768', 'title': 'Temporal Dynamic Embedding for Irregularly Sampled Time Series', 'authors': 'Mincheol Kim, Soo-Yong Shin', 'link': 'https://arxiv.org/abs/2504.05768', 'abstract': 'In several practical applications, particularly healthcare, clinical data of each patient is individually recorded in a database at irregular intervals as required. This causes a sparse and irregularly sampled time series, which makes it difficult to handle as a structured representation of the prerequisites of neural network models. We therefore propose temporal dynamic embedding (TDE), which enables neural network models to receive data that change the number of variables over time. TDE regards each time series variable as an embedding vector evolving over time, instead of a conventional fixed structured representation, which causes a critical missing problem. For each time step, TDE allows for the selective adoption and aggregation of only observed variable subsets and represents the current status of patient based on current observations. The experiment was conducted on three clinical datasets: PhysioNet 2012, MIMIC-III, and PhysioNet 2019. The TDE model performed competitively or better than the imputation-based baseline and several recent state-of-the-art methods with reduced training runtime.', 'abstract_zh': '在几个实际应用中，尤其是在医疗健康领域，每位患者的临床数据会在不规则的时间间隔内以个体化的方式记录在数据库中。这会产生稀疏且不规则采样的时间序列，使得将其作为神经网络模型结构化表示处理变得困难。因此，我们提出了时间动态嵌入（TDE），该方法使神经网络模型能够接收随时间变化的变量数量的数据。TDE 将每个时间序列变量视为随时间演化的嵌入向量，而不是传统的固定结构表示，从而解决了关键的缺失问题。对于每个时间步，TDE 只允许选择性地采用和聚合仅观察到的变量子集，并基于当前观察来表示患者当前的状态。在三项临床数据集中对 TDE 模型进行了实验：PhysioNet 2012、MIMIC-III 和 PhysioNet 2019。实验结果表明，与基于插补的基线方法以及几种最新的最先进方法相比，TDE 模型在减少训练运行时间的情况下表现竞争力或更佳。', 'title_zh': '不规则采样时间序列的时序动态嵌入'}
{'arxiv_id': 'arXiv:2504.05755', 'title': 'Unraveling Human-AI Teaming: A Review and Outlook', 'authors': 'Bowen Lou, Tian Lu, Raghu Santanam, Yingjie Zhang', 'link': 'https://arxiv.org/abs/2504.05755', 'abstract': "Artificial Intelligence (AI) is advancing at an unprecedented pace, with clear potential to enhance decision-making and productivity. Yet, the collaborative decision-making process between humans and AI remains underdeveloped, often falling short of its transformative possibilities. This paper explores the evolution of AI agents from passive tools to active collaborators in human-AI teams, emphasizing their ability to learn, adapt, and operate autonomously in complex environments. This paradigm shifts challenges traditional team dynamics, requiring new interaction protocols, delegation strategies, and responsibility distribution frameworks. Drawing on Team Situation Awareness (SA) theory, we identify two critical gaps in current human-AI teaming research: the difficulty of aligning AI agents with human values and objectives, and the underutilization of AI's capabilities as genuine team members. Addressing these gaps, we propose a structured research outlook centered on four key aspects of human-AI teaming: formulation, coordination, maintenance, and training. Our framework highlights the importance of shared mental models, trust-building, conflict resolution, and skill adaptation for effective teaming. Furthermore, we discuss the unique challenges posed by varying team compositions, goals, and complexities. This paper provides a foundational agenda for future research and practical design of sustainable, high-performing human-AI teams.", 'abstract_zh': '人工智能（AI）的发展速度史无前例，具有增强决策和生产力的潜力。然而，人类与AI的合作决策过程仍处于初级阶段，往往未能充分发挥其变革潜力。本文探讨了AI代理从被动工具向人类-AI团队中主动合作者的演变，强调其在复杂环境中的学习、适应和自主操作能力。这一范式转变挑战了传统的团队动态，要求新的交互协议、委派策略和责任分配框架。基于团队态势意识（SA）理论，我们指出了当前人类-AI团队研究中的两个关键缺口：AI代理与人类价值观和目标的对齐困难，以及AI作为真正团队成员潜力的未充分利用。针对这些缺口，我们提出了一种以人类-AI团队四方核心方面为焦点的整体研究方向：建模、协调、维护和训练。我们的框架强调了共享认知模型、信任建立、冲突解决和技能适应对于有效团队合作的重要性。此外，我们讨论了不同团队组成、目标和复杂性带来的独特挑战。本文为未来研究和可持续、高性能的人类-AI团队的实际设计提供了基础议程。', 'title_zh': '探究人机协作：综述与展望'}
{'arxiv_id': 'arXiv:2504.05741', 'title': 'DDT: Decoupled Diffusion Transformer', 'authors': 'Shuai Wang, Zhi Tian, Weilin Huang, Limin Wang', 'link': 'https://arxiv.org/abs/2504.05741', 'abstract': 'Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding low-frequency semantics necessitates reducing high-frequency components, creating tension between semantic encoding and high-frequency decoding. To resolve this challenge, we propose a new \\textbf{\\color{ddt}D}ecoupled \\textbf{\\color{ddt}D}iffusion \\textbf{\\color{ddt}T}ransformer~(\\textbf{\\color{ddt}DDT}), with a decoupled design of a dedicated condition encoder for semantic extraction alongside a specialized velocity decoder. Our experiments reveal that a more substantial encoder yields performance improvements as model size increases. For ImageNet $256\\times256$, Our DDT-XL/2 achieves a new state-of-the-art performance of {1.31 FID}~(nearly $4\\times$ faster training convergence compared to previous diffusion transformers). For ImageNet $512\\times512$, Our DDT-XL/2 achieves a new state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our decoupled architecture enhances inference speed by enabling the sharing self-condition between adjacent denoising steps. To minimize performance degradation, we propose a novel statistical dynamic programming approach to identify optimal sharing strategies.', 'abstract_zh': '解耦扩散变压器（DDT）：解耦设计的专用语义编码器与专门的速度解码器', 'title_zh': 'DDT: 解耦扩散变换器'}
{'arxiv_id': 'arXiv:2504.05695', 'title': 'Architecture independent generalization bounds for overparametrized deep ReLU networks', 'authors': 'Thomas Chen, Chun-Kai Kevin Chien, Patricia Muñoz Ewald, Andrew G. Moore', 'link': 'https://arxiv.org/abs/2504.05695', 'abstract': 'We prove that overparametrized neural networks are able to generalize with a test error that is independent of the level of overparametrization, and independent of the Vapnik-Chervonenkis (VC) dimension. We prove explicit bounds that only depend on the metric geometry of the test and training sets, on the regularity properties of the activation function, and on the operator norms of the weights and norms of biases. For overparametrized deep ReLU networks with a training sample size bounded by the input space dimension, we explicitly construct zero loss minimizers without use of gradient descent, and prove that the generalization error is independent of the network architecture.', 'abstract_zh': '我们证明，过参数化的神经网络能够实现与过参数化程度和Vapnik-Chervonenkis维数无关的测试误差泛化。我们推导了仅依赖于测试集和训练集的度量几何特性、激活函数的正则性质、权重的操作范数以及偏置的范数的显式边界。对于输入空间维数受限的训练样本大小的过参数化深层ReLU网络，我们明确构造了无需使用梯度下降的零损失最小化器，并证明了泛化误差与网络架构无关。', 'title_zh': '独立于架构的一般化界面对过参数化深ReLU网络的分析'}
{'arxiv_id': 'arXiv:2504.05657', 'title': 'Nes2Net: A Lightweight Nested Architecture for Foundation Model Driven Speech Anti-spoofing', 'authors': 'Tianchi Liu, Duc-Tuan Truong, Rohan Kumar Das, Kong Aik Lee, Haizhou Li', 'link': 'https://arxiv.org/abs/2504.05657', 'abstract': "Speech foundation models have significantly advanced various speech-related tasks by providing exceptional representation capabilities. However, their high-dimensional output features often create a mismatch with downstream task models, which typically require lower-dimensional inputs. A common solution is to apply a dimensionality reduction (DR) layer, but this approach increases parameter overhead, computational costs, and risks losing valuable information. To address these issues, we propose Nested Res2Net (Nes2Net), a lightweight back-end architecture designed to directly process high-dimensional features without DR layers. The nested structure enhances multi-scale feature extraction, improves feature interaction, and preserves high-dimensional information. We first validate Nes2Net on CtrSVDD, a singing voice deepfake detection dataset, and report a 22% performance improvement and an 87% back-end computational cost reduction over the state-of-the-art baseline. Additionally, extensive testing across four diverse datasets: ASVspoof 2021, ASVspoof 5, PartialSpoof, and In-the-Wild, covering fully spoofed speech, adversarial attacks, partial spoofing, and real-world scenarios, consistently highlights Nes2Net's superior robustness and generalization capabilities. The code package and pre-trained models are available at this https URL.", 'abstract_zh': '基于演讲的基础模型显著推进了各种演讲相关任务，但其高维输出特征经常会与下游任务模型所需的低维输入产生不匹配。一种常见的解决方案是应用降维层，但这会增加参数开销、计算成本，并有损失有价值信息的风险。为解决这些问题，我们提出了一种轻量级后端架构Nested Res2Net（Nes2Net），该架构可以直接处理高维特征而不使用降维层。嵌套结构增强了多尺度特征提取、提高了特征交互并保留了高维信息。我们在 singing voice deepfake 检测数据集 CtrSVDD 上验证了 Nes2Net，并报告了相比最先进的基线提高了22%的性能和87%的后端计算成本降低。此外，跨四个不同数据集（ASVspoof 2021、ASVspoof 5、PartialSpoof 和 In-the-Wild，涵盖完全欺骗性语音、对抗性攻击、部分欺骗性和真实场景）的广泛测试一致突出了 Nes2Net 优越的鲁棒性和泛化能力。 code package 和预训练模型可从此链接下载。', 'title_zh': 'Nes2Net：一种轻量级嵌套架构，用于基础模型驱动的语音防 spoofing'}
{'arxiv_id': 'arXiv:2504.05646', 'title': 'Lattice: Learning to Efficiently Compress the Memory', 'authors': 'Mahdi Karami, Vahab Mirrokni', 'link': 'https://arxiv.org/abs/2504.05646', 'abstract': 'Attention mechanisms have revolutionized sequence learning but suffer from quadratic computational complexity. This paper introduces Lattice, a novel recurrent neural network (RNN) mechanism that leverages the inherent low-rank structure of K-V matrices to efficiently compress the cache into a fixed number of memory slots, achieving sub-quadratic complexity. We formulate this compression as an online optimization problem and derive a dynamic memory update rule based on a single gradient descent step. The resulting recurrence features a state- and input-dependent gating mechanism, offering an interpretable memory update process. The core innovation is the orthogonal update: each memory slot is updated exclusively with information orthogonal to its current state hence incorporation of only novel, non-redundant data, which minimizes the interference with previously stored information. The experimental results show that Lattice achieves the best perplexity compared to all baselines across diverse context lengths, with performance improvement becoming more pronounced as the context length increases.', 'abstract_zh': '注意力机制革新了序列学习但遭受着二次时间复杂度的困扰。本文提出了一种新颖的递归神经网络机制Lattice，利用K-V矩阵固有的低秩结构高效压缩缓存至固定数量的记忆单元，实现次二次时间复杂度。我们将这种压缩形式化为在线优化问题，并基于单步梯度下降推导出动态内存更新规则。该递归机制包含状态和输入依赖的门控机制，提供可解释的记忆更新过程。核心创新在于正交更新：每个记忆单元仅根据与当前状态正交的信息进行更新，从而仅 Incorporate 新颖且无冗余的数据，最大限度地减少对已存储信息的干扰。实验结果表明，Lattice 在各种上下文长度下的困惑度均优于所有基线模型，且随着上下文长度增加，性能提升更为显著。', 'title_zh': 'Lattice: 学习高效压缩内存'}
{'arxiv_id': 'arXiv:2504.05639', 'title': 'DBOT: Artificial Intelligence for Systematic Long-Term Investing', 'authors': 'Vasant Dhar, João Sedoc', 'link': 'https://arxiv.org/abs/2504.05639', 'abstract': "Long-term investing was previously seen as requiring human judgment. With the advent of generative artificial intelligence (AI) systems, automated systematic long-term investing is now feasible. In this paper, we present DBOT, a system whose goal is to reason about valuation like Aswath Damodaran, who is a unique expert in the investment arena in terms of having published thousands of valuations on companies in addition to his numerous writings on the topic, which provide ready training data for an AI system. DBOT can value any publicly traded company. DBOT can also be back-tested, making its behavior and performance amenable to scientific inquiry. We compare DBOT to its analytic parent, Damodaran, and highlight the research challenges involved in raising its current capability to that of Damodaran's. Finally, we examine the implications of DBOT-like AI agents for the financial industry, especially how they will impact the role of human analysts in valuation.", 'abstract_zh': '长线投资 previously viewed as requiring human判断，现在借助生成型人工智能系统变得可行。本文介绍DBOT系统，其目标是像拥有数千家企业估值及大量相关著作的Aswath Damodaran一样进行估值推理。DBOT可以估值任何公开交易的公司，并可进行回测，使其行为和表现易于科学探究。我们将DBOT与其分析型前辈Damodaran进行比较，并强调提高DBOT当前能力至Damodaran水平的研究挑战。最后，我们探讨类似DBOT的AI代理对金融行业的影响，尤其是它们如何影响估值中的分析师角色。', 'title_zh': 'DBOT：系统长期投资的人工智能方法'}
{'arxiv_id': 'arXiv:2504.05618', 'title': 'Technical Report: Full Version of Analyzing and Optimizing Perturbation of DP-SGD Geometrically', 'authors': 'Jiawei Duan, Haibo Hu, Qingqing Ye, Xinyue Sun', 'link': 'https://arxiv.org/abs/2504.05618', 'abstract': 'Differential privacy (DP) has become a prevalent privacy model in a wide range of machine learning tasks, especially after the debut of DP-SGD. However, DP-SGD, which directly perturbs gradients in the training iterations, fails to mitigate the negative impacts of noise on gradient direction. As a result, DP-SGD is often inefficient. Although various solutions (e.g., clipping to reduce the sensitivity of gradients and amplifying privacy bounds to save privacy budgets) are proposed to trade privacy for model efficiency, the root cause of its inefficiency is yet unveiled.\nIn this work, we first generalize DP-SGD and theoretically derive the impact of DP noise on the training process. Our analysis reveals that, in terms of a perturbed gradient, only the noise on direction has eminent impact on the model efficiency while that on magnitude can be mitigated by optimization techniques, i.e., fine-tuning gradient clipping and learning rate. Besides, we confirm that traditional DP introduces biased noise on the direction when adding unbiased noise to the gradient itself. Overall, the perturbation of DP-SGD is actually sub-optimal from a geometric perspective. Motivated by this, we design a geometric perturbation strategy GeoDP within the DP framework, which perturbs the direction and the magnitude of a gradient, respectively. By directly reducing the noise on the direction, GeoDP mitigates the negative impact of DP noise on model efficiency with the same DP guarantee. Extensive experiments on two public datasets (i.e., MNIST and CIFAR-10), one synthetic dataset and three prevalent models (i.e., Logistic Regression, CNN and ResNet) confirm the effectiveness and generality of our strategy.', 'abstract_zh': '差分隐私（DP）已成为广泛机器学习任务中的一种主流隐私模型，尤其是在DP-SGD出现之后。然而，DP-SGD直接在训练迭代中扰动梯度，未能减轻噪声对梯度方向的负面影响，导致其效率低下。尽管提出了多种解决方案（例如梯度裁剪以降低梯度敏感性、放大隐私边界以节省隐私预算）来权衡隐私与模型效率，但其效率低下的根本原因仍未揭开。\n\n在本文中，我们首先泛化DP-SGD，并从理论上推导出DP噪声对训练过程的影响。我们的分析表明，在扰动梯度的情况下，仅噪声的方向部分对模型效率产生了显着影响，而幅度噪声则可以通过优化技术（如精细调节梯度裁剪和学习率）来缓解。此外，我们确认当对梯度本身添加无偏噪声时，传统DP会引入偏置方向噪声。整体而言，从几何角度看，DP-SGD的扰动实际上是次优的。受此启发，我们在DP框架内设计了一种几何扰动策略GeoDP，分别扰动梯度的方向和幅度。通过直接减少方向上的噪声，GeoDP在保留相同DP保证的情况下，减轻了DP噪声对模型效率的负面影响。在两个公开数据集（即MNIST和CIFAR-10）、一个合成数据集和三种常见模型（即逻辑回归、卷积神经网络和残差网络）上的广泛实验确认了我们策略的有效性和普适性。', 'title_zh': '技术报告：几何角度分析与优化DP-SGD扰动的完整版本'}
{'arxiv_id': 'arXiv:2504.05615', 'title': 'FedEFC: Federated Learning Using Enhanced Forward Correction Against Noisy Labels', 'authors': 'Seunghun Yu, Jin-Hyun Ahn, Joonhyuk Kang', 'link': 'https://arxiv.org/abs/2504.05615', 'abstract': 'Federated Learning (FL) is a powerful framework for privacy-preserving distributed learning. It enables multiple clients to collaboratively train a global model without sharing raw data. However, handling noisy labels in FL remains a major challenge due to heterogeneous data distributions and communication constraints, which can severely degrade model performance. To address this issue, we propose FedEFC, a novel method designed to tackle the impact of noisy labels in FL. FedEFC mitigates this issue through two key techniques: (1) prestopping, which prevents overfitting to mislabeled data by dynamically halting training at an optimal point, and (2) loss correction, which adjusts model updates to account for label noise. In particular, we develop an effective loss correction tailored to the unique challenges of FL, including data heterogeneity and decentralized training. Furthermore, we provide a theoretical analysis, leveraging the composite proper loss property, to demonstrate that the FL objective function under noisy label distributions can be aligned with the clean label distribution. Extensive experimental results validate the effectiveness of our approach, showing that it consistently outperforms existing FL techniques in mitigating the impact of noisy labels, particularly under heterogeneous data settings (e.g., achieving up to 41.64% relative performance improvement over the existing loss correction method).', 'abstract_zh': '联邦学习中的噪声标签应对方法：FedEFC', 'title_zh': 'FedEFC: 面向噪声标签的增强前向纠错联邦学习'}
{'arxiv_id': 'arXiv:2504.05591', 'title': 'Class Imbalance Correction for Improved Universal Lesion Detection and Tagging in CT', 'authors': 'Peter D. Erickson, Tejas Sudharshan Mathai, Ronald M. Summers', 'link': 'https://arxiv.org/abs/2504.05591', 'abstract': "Radiologists routinely detect and size lesions in CT to stage cancer and assess tumor burden. To potentially aid their efforts, multiple lesion detection algorithms have been developed with a large public dataset called DeepLesion (32,735 lesions, 32,120 CT slices, 10,594 studies, 4,427 patients, 8 body part labels). However, this dataset contains missing measurements and lesion tags, and exhibits a severe imbalance in the number of lesions per label category. In this work, we utilize a limited subset of DeepLesion (6\\%, 1331 lesions, 1309 slices) containing lesion annotations and body part label tags to train a VFNet model to detect lesions and tag them. We address the class imbalance by conducting three experiments: 1) Balancing data by the body part labels, 2) Balancing data by the number of lesions per patient, and 3) Balancing data by the lesion size. In contrast to a randomly sampled (unbalanced) data subset, our results indicated that balancing the body part labels always increased sensitivity for lesions >= 1cm for classes with low data quantities (Bone: 80\\% vs. 46\\%, Kidney: 77\\% vs. 61\\%, Soft Tissue: 70\\% vs. 60\\%, Pelvis: 83\\% vs. 76\\%). Similar trends were seen for three other models tested (FasterRCNN, RetinaNet, FoveaBox). Balancing data by lesion size also helped the VFNet model improve recalls for all classes in contrast to an unbalanced dataset. We also provide a structured reporting guideline for a ``Lesions'' subsection to be entered into the ``Findings'' section of a radiology report. To our knowledge, we are the first to report the class imbalance in DeepLesion, and have taken data-driven steps to address it in the context of joint lesion detection and tagging.", 'abstract_zh': '放射ologist在CT中常规检测和评估病灶以分期癌症和评估肿瘤负荷。为了潜在地辅助他们的工作，开发了多种病灶检测算法，并使用了一个名为DeepLesion的大公开数据集（32,735个病灶，32,120个CT切片，10,594项研究，4,427名患者，8个解剖部位标签）。然而，该数据集包含缺失的测量值和病灶标签，并在病灶数量方面表现出严重的类别不平衡。本文利用一个有限子集的DeepLesion数据集（6%，1,331个病灶，1,309个切片，包含病灶标注和解剖部位标签），训练VFNet模型进行病灶检测和标签标注。我们通过三种实验解决类别不平衡问题：1）按解剖部位标签平衡数据，2）按患者病灶数量平衡数据，3）按病灶大小平衡数据。与随机采样的（不平衡的）数据子集相比，我们的结果显示，按解剖部位标签平衡数据总是提高了对病灶≥1cm的灵敏度（骨骼：80% vs. 46%，肾脏：77% vs. 61%，软组织：70% vs. 60%，骨盆：83% vs. 76%）。其他三种模型（FasterRCNN、RetinaNet、FoveaBox）也观察到类似的趋势。此外，按病灶大小平衡数据也有助于提高交回模型在所有类别的召回率。我们还提供了一份在放射学报告中“病灶”部分结构化报告指南。据我们所知，我们是第一个报告DeepLesion类别不平衡的研究，并且采取了数据驱动的方法来解决这一问题，以联合病灶检测和标签标注的背景中。', 'title_zh': 'CT中改进的通用病灶检测和标记的类别不平衡校正'}
{'arxiv_id': 'arXiv:2504.05586', 'title': 'Finding Fantastic Experts in MoEs: A Unified Study for Expert Dropping Strategies and Observations', 'authors': 'Ajay Jaiswal, Jianyu Wang, Yixiao Li, Pingzhi Li, Tianlong Chen, Zhangyang Wang, Chong Wang, Ruoming Pang, Xianzhi Du', 'link': 'https://arxiv.org/abs/2504.05586', 'abstract': "Sparsely activated Mixture-of-Experts (SMoE) has shown promise in scaling up the learning capacity of neural networks. However, vanilla SMoEs have issues such as expert redundancy and heavy memory requirements, making them inefficient and non-scalable, especially for resource-constrained scenarios. Expert-level sparsification of SMoEs involves pruning the least important experts to address these limitations. In this work, we aim to address three questions: (1) What is the best recipe to identify the least knowledgeable subset of experts that can be dropped with minimal impact on performance? (2) How should we perform expert dropping (one-shot or iterative), and what correction measures can we undertake to minimize its drastic impact on SMoE subnetwork capabilities? (3) What capabilities of full-SMoEs are severely impacted by the removal of the least dominant experts, and how can we recover them? Firstly, we propose MoE Experts Compression Suite (MC-Suite), which is a collection of some previously explored and multiple novel recipes to provide a comprehensive benchmark for estimating expert importance from diverse perspectives, as well as unveil numerous valuable insights for SMoE experts. Secondly, unlike prior works with a one-shot expert pruning approach, we explore the benefits of iterative pruning with the re-estimation of the MC-Suite criterion. Moreover, we introduce the benefits of task-agnostic fine-tuning as a correction mechanism during iterative expert dropping, which we term MoE Lottery Subnetworks. Lastly, we present an experimentally validated conjecture that, during expert dropping, SMoEs' instruction-following capabilities are predominantly hurt, which can be restored to a robust level subject to external augmentation of instruction-following capabilities using k-shot examples and supervised fine-tuning.", 'abstract_zh': '稀疏激活专家混合模型（SMoE）在扩展神经网络的learning容量方面展现了潜力。然而，vanilla SMoE存在专家冗余和高内存需求等问题，导致其效率低下且不具扩展性，特别是在资源受限的场景中更为明显。通过剪枝最不重要的专家，专家级SMoE的稀疏化旨在解决这些问题。本文旨在回答三个问题：（1）什么是识别对性能影响最小的最不重要的专家的最佳方法？（2）我们应该采用一次性还是迭代方式剪枝专家，以及在迭代剪枝过程中可以采取哪些措施来最小化对SMoE子网络能力的严重影响？（3）删除最不占主导地位的专家会对全SMoE的哪些能力产生严重影响，我们如何恢复这些能力？首先，我们提出MoE专家压缩套件（MC-Suite），这是一个包含一些先前探索和多种新方法的集合，用于从多角度提供专家重要性的全面基准，并揭示大量关于SMoE专家的有价值见解。其次，与以往采用一次性专家剪枝的方法不同，我们探索了重新估计MC-Suite标准的迭代剪枝优势。此外，我们引入了任务无关的微调作为一种迭代专家剪枝过程中的校正机制，称之为MoE彩票子网络。最后，我们提出一个经实验验证的猜测：在专家剪枝过程中，SMoE的指令遵循能力受到主要损害，但可以通过使用k-shot示例和监督微调的方式对外部增加指令遵循能力来恢复到稳健的水平。', 'title_zh': 'MoEs中寻找卓越专家：专家丢弃策略与观察的统一研究'}
{'arxiv_id': 'arXiv:2504.05585', 'title': 'TW-CRL: Time-Weighted Contrastive Reward Learning for Efficient Inverse Reinforcement Learning', 'authors': 'Yuxuan Li, Ning Yang, Stephen Xia', 'link': 'https://arxiv.org/abs/2504.05585', 'abstract': 'Episodic tasks in Reinforcement Learning (RL) often pose challenges due to sparse reward signals and high-dimensional state spaces, which hinder efficient learning. Additionally, these tasks often feature hidden "trap states" -- irreversible failures that prevent task completion but do not provide explicit negative rewards to guide agents away from repeated errors. To address these issues, we propose Time-Weighted Contrastive Reward Learning (TW-CRL), an Inverse Reinforcement Learning (IRL) framework that leverages both successful and failed demonstrations. By incorporating temporal information, TW-CRL learns a dense reward function that identifies critical states associated with success or failure. This approach not only enables agents to avoid trap states but also encourages meaningful exploration beyond simple imitation of expert trajectories. Empirical evaluations on navigation tasks and robotic manipulation benchmarks demonstrate that TW-CRL surpasses state-of-the-art methods, achieving improved efficiency and robustness.', 'abstract_zh': '时间加权对比奖励学习（TW-CRL）：一种集成成功与失败示例的逆强化学习框架', 'title_zh': '时间加权对比奖励学习：高效逆强化学习'}
{'arxiv_id': 'arXiv:2504.05576', 'title': 'SoundVista: Novel-View Ambient Sound Synthesis via Visual-Acoustic Binding', 'authors': 'Mingfei Chen, Israel D. Gebru, Ishwarya Ananthabhotla, Christian Richardt, Dejan Markovic, Jake Sandakly, Steven Krenn, Todd Keebler, Eli Shlizerman, Alexander Richard', 'link': 'https://arxiv.org/abs/2504.05576', 'abstract': 'We introduce SoundVista, a method to generate the ambient sound of an arbitrary scene at novel viewpoints. Given a pre-acquired recording of the scene from sparsely distributed microphones, SoundVista can synthesize the sound of that scene from an unseen target viewpoint. The method learns the underlying acoustic transfer function that relates the signals acquired at the distributed microphones to the signal at the target viewpoint, using a limited number of known recordings. Unlike existing works, our method does not require constraints or prior knowledge of sound source details. Moreover, our method efficiently adapts to diverse room layouts, reference microphone configurations and unseen environments. To enable this, we introduce a visual-acoustic binding module that learns visual embeddings linked with local acoustic properties from panoramic RGB and depth data. We first leverage these embeddings to optimize the placement of reference microphones in any given scene. During synthesis, we leverage multiple embeddings extracted from reference locations to get adaptive weights for their contribution, conditioned on target viewpoint. We benchmark the task on both publicly available data and real-world settings. We demonstrate significant improvements over existing methods.', 'abstract_zh': 'SoundVista：生成任意场景在新型视角下的环境声音的方法', 'title_zh': 'SoundVista: 基于视听结合的新型视角环境声合成'}
{'arxiv_id': 'arXiv:2504.05573', 'title': 'MicroNN: An On-device Disk-resident Updatable Vector Database', 'authors': 'Jeffrey Pound, Floris Chabert, Arjun Bhushan, Ankur Goswami, Anil Pacaci, Shihabur Rahman Chowdhury', 'link': 'https://arxiv.org/abs/2504.05573', 'abstract': 'Nearest neighbour search over dense vector collections has important applications in information retrieval, retrieval augmented generation (RAG), and content ranking. Performing efficient search over large vector collections is a well studied problem with many existing approaches and open source implementations. However, most state-of-the-art systems are generally targeted towards scenarios using large servers with an abundance of memory, static vector collections that are not updatable, and nearest neighbour search in isolation of other search criteria. We present Micro Nearest Neighbour (MicroNN), an embedded nearest-neighbour vector search engine designed for scalable similarity search in low-resource environments. MicroNN addresses the problem of on-device vector search for real-world workloads containing updates and hybrid search queries that combine nearest neighbour search with structured attribute filters. In this scenario, memory is highly constrained and disk-efficient index structures and algorithms are required, as well as support for continuous inserts and deletes. MicroNN is an embeddable library that can scale to large vector collections with minimal resources. MicroNN is used in production and powers a wide range of vector search use-cases on-device. MicroNN takes less than 7 ms to retrieve the top-100 nearest neighbours with 90% recall on publicly available million-scale vector benchmark while using ~10 MB of memory.', 'abstract_zh': '密集向量集合的最近邻搜索在信息检索、检索增强生成（RAG）和内容排名中具有重要应用。针对低资源环境下的可扩展相似性搜索，我们提出了嵌入式最近邻向量搜索引擎Micro Nearest Neighbour (MicroNN)。MicroNN 解决了包含更新和结合最近邻搜索与结构化属性过滤器的混合搜索查询的设备端向量搜索问题。在这一场景中，内存高度受限，需要高效的磁盘索引结构和算法，以及连续插入和删除的支持。MicroNN 是一个可嵌入的库，能够在最小资源消耗的情况下扩展到大规模向量集合。MicroNN 在生产环境中使用，并支持多种向量搜索用例。在使用约 10 MB 内存的情况下，MicroNN 能在公开的百万规模向量基准上以小于 7 ms 的时间检索 90% 召回率的 top-100 最近邻。', 'title_zh': 'MicroNN: 一种设备本地磁盘驻留可更新向量数据库'}
{'arxiv_id': 'arXiv:2504.05530', 'title': 'FORCE: Feature-Oriented Representation with Clustering and Explanation', 'authors': 'Rishav Mukherjee, Jeffrey Ahearn Thompson', 'link': 'https://arxiv.org/abs/2504.05530', 'abstract': 'Learning about underlying patterns in data using latent unobserved structures to improve the accuracy of predictive models has become an active avenue of deep learning research. Most approaches cluster the original features to capture certain latent structures. However, the information gained in the process can often be implicitly derived by sufficiently complex models. Thus, such approaches often provide minimal benefits. We propose a SHAP (Shapley Additive exPlanations) based supervised deep learning framework FORCE which relies on two-stage usage of SHAP values in the neural network architecture, (i) an additional latent feature to guide model training, based on clustering SHAP values, and (ii) initiating an attention mechanism within the architecture using latent information. This approach gives a neural network an indication about the effect of unobserved values that modify feature importance for an observation. The proposed framework is evaluated on three real life datasets. Our results demonstrate that FORCE led to dramatic improvements in overall performance as compared to networks that did not incorporate the latent feature and attention framework (e.g., F1 score for presence of heart disease 0.80 vs 0.72). Using cluster assignments and attention based on SHAP values guides deep learning, enhancing latent pattern learning and overall discriminative capability.', 'abstract_zh': '使用SHAP值基于监督的深度学习框架FORCE挖掘潜在模式以提高预测模型的准确性', 'title_zh': 'FORCE:面向特征的表示与聚类解释'}
{'arxiv_id': 'arXiv:2504.05455', 'title': 'Large-Scale Classification of Shortwave Communication Signals with Machine Learning', 'authors': 'Stefan Scholl', 'link': 'https://arxiv.org/abs/2504.05455', 'abstract': 'This paper presents a deep learning approach to the classification of 160 shortwave radio signals. It addresses the typical challenges of the shortwave spectrum, which are the large number of different signal types, the presence of various analog modulations and ionospheric propagation. As a classifier a deep convolutional neural network is used, that is trained to recognize 160 typical shortwave signal classes. The approach is blind and therefore does not require preknowledge or special preprocessing of the signal and no manual design of discriminative features for each signal class. The network is trained on a large number of synthetically generated signals and high quality recordings. Finally, the network is evaluated on real-world radio signals obtained from globally deployed receiver hardware and achieves up to 90% accuracy for an observation time of only 1 second.', 'abstract_zh': '一种用于160个短波无线电信号分类的深度学习方法', 'title_zh': '基于机器学习的短波通信信号大规模分类'}
{'arxiv_id': 'arXiv:2504.05454', 'title': 'GraphPINE: Graph Importance Propagation for Interpretable Drug Response Prediction', 'authors': 'Yoshitaka Inoue, Tianfan Fu, Augustin Luna', 'link': 'https://arxiv.org/abs/2504.05454', 'abstract': 'Explainability is necessary for many tasks in biomedical research. Recent explainability methods have focused on attention, gradient, and Shapley value. These do not handle data with strong associated prior knowledge and fail to constrain explainability results based on known relationships between predictive features.\nWe propose GraphPINE, a graph neural network (GNN) architecture leveraging domain-specific prior knowledge to initialize node importance optimized during training for drug response prediction. Typically, a manual post-prediction step examines literature (i.e., prior knowledge) to understand returned predictive features. While node importance can be obtained for gradient and attention after prediction, node importance from these methods lacks complementary prior knowledge; GraphPINE seeks to overcome this limitation. GraphPINE differs from other GNN gating methods by utilizing an LSTM-like sequential format. We introduce an importance propagation layer that unifies 1) updates for feature matrix and node importance and 2) uses GNN-based graph propagation of feature values. This initialization and updating mechanism allows for informed feature learning and improved graph representation.\nWe apply GraphPINE to cancer drug response prediction using drug screening and gene data collected for over 5,000 gene nodes included in a gene-gene graph with a drug-target interaction (DTI) graph for initial importance. The gene-gene graph and DTIs were obtained from curated sources and weighted by article count discussing relationships between drugs and genes. GraphPINE achieves a PR-AUC of 0.894 and ROC-AUC of 0.796 across 952 drugs. Code is available at this https URL.', 'abstract_zh': 'GraphPINE：利用领域特定先验知识的图神经网络架构用于药物响应预测', 'title_zh': 'GraphPINE: 图结构的重要性传播药物响应可解释预测'}
{'arxiv_id': 'arXiv:2504.05425', 'title': "A Behavior-Based Knowledge Representation Improves Prediction of Players' Moves in Chess by 25%", 'authors': 'Benny Skidanov, Daniel Erbesfeld, Gera Weiss, Achiya Elyasaf', 'link': 'https://arxiv.org/abs/2504.05425', 'abstract': "Predicting player behavior in strategic games, especially complex ones like chess, presents a significant challenge. The difficulty arises from several factors. First, the sheer number of potential outcomes stemming from even a single position, starting from the initial setup, makes forecasting a player's next move incredibly complex. Second, and perhaps even more challenging, is the inherent unpredictability of human behavior. Unlike the optimized play of engines, humans introduce a layer of variability due to differing playing styles and decision-making processes. Each player approaches the game with a unique blend of strategic thinking, tactical awareness, and psychological tendencies, leading to diverse and often unexpected actions. This stylistic variation, combined with the capacity for creativity and even irrational moves, makes predicting human play difficult. Chess, a longstanding benchmark of artificial intelligence research, has seen significant advancements in tools and automation. Engines like Deep Blue, AlphaZero, and Stockfish can defeat even the most skilled human players. However, despite their exceptional ability to outplay top-level grandmasters, predicting the moves of non-grandmaster players, who comprise most of the global chess community -- remains complicated for these engines. This paper proposes a novel approach combining expert knowledge with machine learning techniques to predict human players' next moves. By applying feature engineering grounded in domain expertise, we seek to uncover the patterns in the moves of intermediate-level chess players, particularly during the opening phase of the game. Our methodology offers a promising framework for anticipating human behavior, advancing both the fields of AI and human-computer interaction.", 'abstract_zh': '基于专家知识与机器学习技术预测棋手在战略游戏中，尤其是象棋中，下一步行为', 'title_zh': '行为为基础的知识表示在棋盘游戏中提高玩家棋局预测准确率25%'}
{'arxiv_id': 'arXiv:2504.05424', 'title': 'Safe Automated Refactoring for Efficient Migration of Imperative Deep Learning Programs to Graph Execution', 'authors': 'Raffi Khatchadourian, Tatiana Castro Vélez, Mehdi Bagherzadeh, Nan Jia, Anita Raja', 'link': 'https://arxiv.org/abs/2504.05424', 'abstract': 'Efficiency is essential to support responsiveness w.r.t. ever-growing datasets, especially for Deep Learning (DL) systems. DL frameworks have traditionally embraced deferred execution-style DL code -- supporting symbolic, graph-based Deep Neural Network (DNN) computation. While scalable, such development is error-prone, non-intuitive, and difficult to debug. Consequently, more natural, imperative DL frameworks encouraging eager execution have emerged at the expense of run-time performance. Though hybrid approaches aim for the "best of both worlds," using them effectively requires subtle considerations to make code amenable to safe, accurate, and efficient graph execution. We present an automated refactoring approach that assists developers in specifying whether their otherwise eagerly-executed imperative DL code could be reliably and efficiently executed as graphs while preserving semantics. The approach, based on a novel imperative tensor analysis, automatically determines when it is safe and potentially advantageous to migrate imperative DL code to graph execution. The approach is implemented as a PyDev Eclipse IDE plug-in that integrates the WALA Ariadne analysis framework and evaluated on 19 Python projects consisting of 132.05 KLOC. We found that 326 of 766 candidate functions (42.56%) were refactorable, and an average speedup of 2.16 on performance tests was observed. The results indicate that the approach is useful in optimizing imperative DL code to its full potential.', 'abstract_zh': '效率对于支持不断增长的数据集的响应性至关重要，特别是在深度学习系统中。传统的深度学习框架采用了延迟执行风格的代码，支持基于符号和图形的深层神经网络（DNN）计算。尽管这种开发具有可扩展性，但它容易出错、不直观且难以调试。因此，新兴的鼓励即时执行的自然化指令式深度学习框架以牺牲运行时性能为代价出现。虽然混合方法旨在兼得两者之所长，但有效地使用它们需要细致的考虑，以使代码适合安全、准确且高效的图执行。我们提出了一种自动化重构方法，帮助开发者指定其否则即时执行的指令式深度学习代码是否可以可靠且高效地作为图形执行，同时保留语义。该方法基于一种新颖的指令式张量分析，自动确定何时将指令式深度学习代码迁移到图形执行既安全又有利。该方法以PyDev Eclipse IDE插件的形式实现，结合使用WALA Ariadne分析框架，并在包含132,050行代码的19个Python项目上进行了评估。结果显示，766个候选函数中有326个（42.56%）可以重构，并且性能测试观察到了平均2.16倍的加速。结果表明，该方法有助于充分利用指令式深度学习代码的潜力。', 'title_zh': '安全自动重构以提高 imperative 深度学习程序向图执行迁移的效率'}
{'arxiv_id': 'arXiv:2504.05420', 'title': 'PreSumm: Predicting Summarization Performance Without Summarizing', 'authors': 'Steven Koniaev, Ori Ernst, Jackie Chi Kit Cheung', 'link': 'https://arxiv.org/abs/2504.05420', 'abstract': "Despite recent advancements in automatic summarization, state-of-the-art models do not summarize all documents equally well, raising the question: why? While prior research has extensively analyzed summarization models, little attention has been given to the role of document characteristics in influencing summarization performance. In this work, we explore two key research questions. First, do documents exhibit consistent summarization quality across multiple systems? If so, can we predict a document's summarization performance without generating a summary? We answer both questions affirmatively and introduce PreSumm, a novel task in which a system predicts summarization performance based solely on the source document. Our analysis sheds light on common properties of documents with low PreSumm scores, revealing that they often suffer from coherence issues, complex content, or a lack of a clear main theme. In addition, we demonstrate PreSumm's practical utility in two key applications: improving hybrid summarization workflows by identifying documents that require manual summarization and enhancing dataset quality by filtering outliers and noisy documents. Overall, our findings highlight the critical role of document properties in summarization performance and offer insights into the limitations of current systems that could serve as the basis for future improvements.", 'abstract_zh': '尽管自动摘要领域取得了 recent advancements，最先进的模型在摘要不同文档的效果上并不一致，这引发了一个问题：原因何在？尽管先前的研究广泛分析了摘要模型，但很少有研究关注文档特性如何影响摘要性能。在本文中，我们探讨了两个关键的研究问题。首先，文档在多个系统中的摘要质量是否具有一致性？如果一致，我们能否在生成摘要之前预测文档的摘要性能？我们对这两个问题的答案均为肯定，并引入了一个新颖的任务 PreSumm：系统仅基于源文档预测摘要性能。我们的分析揭示了 PreSumm 得分较低的文档的共同特性，表明这些文档通常存在连贯性问题、复杂内容或缺乏清晰的主题。此外，我们展示了 PreSumm 在两个关键应用中的实际价值：通过识别需要人工摘要的文档以改进混合摘要流程，并通过过滤异常值和噪声文档来提高数据集质量。总体而言，我们的发现强调了文档特性在摘要性能中的关键作用，并提供了有关当前系统限制的见解，这些见解可以作为未来改进的基础。', 'title_zh': '预总结：无需总结预测摘要性能'}
{'arxiv_id': 'arXiv:2504.05408', 'title': "SoK: Frontier AI's Impact on the Cybersecurity Landscape", 'authors': 'Wenbo Guo, Yujin Potter, Tianneng Shi, Zhun Wang, Andy Zhang, Dawn Song', 'link': 'https://arxiv.org/abs/2504.05408', 'abstract': "As frontier AI advances rapidly, understanding its impact on cybersecurity and inherent risks is essential to ensuring safe AI evolution (e.g., guiding risk mitigation and informing policymakers). While some studies review AI applications in cybersecurity, none of them comprehensively discuss AI's future impacts or provide concrete recommendations for navigating its safe and secure usage. This paper presents an in-depth analysis of frontier AI's impact on cybersecurity and establishes a systematic framework for risk assessment and mitigation. To this end, we first define and categorize the marginal risks of frontier AI in cybersecurity and then systemically analyze the current and future impacts of frontier AI in cybersecurity, qualitatively and quantitatively. We also discuss why frontier AI likely benefits attackers more than defenders in the short term from equivalence classes, asymmetry, and economic impact. Next, we explore frontier AI's impact on future software system development, including enabling complex hybrid systems while introducing new risks. Based on our findings, we provide security recommendations, including constructing fine-grained benchmarks for risk assessment, designing AI agents for defenses, building security mechanisms and provable defenses for hybrid systems, enhancing pre-deployment security testing and transparency, and strengthening defenses for users. Finally, we present long-term research questions essential for understanding AI's future impacts and unleashing its defensive capabilities.", 'abstract_zh': '随着前沿人工智能的迅速发展，理解其对网络安全的影响和固有风险对于确保安全的人工智能演变（例如，指导风险缓解并为决策者提供信息）至关重要。虽然有一些研究回顾了人工智能在 cybersecurity 中的应用，但没有一项研究全面讨论人工智能未来的影响或提供具体建议以确保其安全和安全使用。本文深入分析了前沿人工智能对网络安全的影响，并建立了一个系统性的风险评估与缓解框架。首先，我们定义并分类了前沿人工智能在网络安全中的边际风险，然后系统地分析了前沿人工智能当前和未来对网络安全的影响，从定性和定量的角度进行分析。我们还讨论了在短期内等价类、不对称性和经济影响使得前沿人工智能更可能对攻击者而不是防御者有利。接下来，我们探索前沿人工智能对未来软件系统开发的影响，包括启用复杂混合系统的同时引入新风险。基于我们的发现，我们提供了安全建议，包括构建细粒度基准以进行风险评估、设计用于防御的人工智能代理、构建适用于混合系统的安全机制和可验证防御、增强部署前的安全测试和透明度、以及加强用户防御。最后，我们提出了对于理解人工智能未来影响以及释放其防御能力至关重要的长期研究问题。', 'title_zh': 'SoK: 前沿人工智能对网络安全 landscape 的影响'}
{'arxiv_id': 'arXiv:2504.05365', 'title': 'A Nature-Inspired Colony of Artificial Intelligence System with Fast, Detailed, and Organized Learner Agents for Enhancing Diversity and Quality', 'authors': 'Shan Suthaharan', 'link': 'https://arxiv.org/abs/2504.05365', 'abstract': 'The concepts of convolutional neural networks (CNNs) and multi-agent systems are two important areas of research in artificial intelligence (AI). In this paper, we present an approach that builds a CNN-based colony of AI agents to serve as a single system and perform multiple tasks (e.g., predictions or classifications) in an environment. The proposed system impersonates the natural environment of a biological system, like an ant colony or a human colony. The proposed colony of AI that is defined as a role-based system uniquely contributes to accomplish tasks in an environment by incorporating AI agents that are fast learners, detailed learners, and organized learners. These learners can enhance their localized learning and their collective decisions as a single system of colony of AI agents. This approach also enhances the diversity and quality of the colony of AI with the help of Genetic Algorithms and their crossover and mutation mechanisms. The evolution of fast, detailed, and organized learners in the colony of AI is achieved by introducing a unique one-to-one mapping between these learners and the pretrained VGG16, VGG19, and ResNet50 models, respectively. This role-based approach creates two parent-AI agents using the AI models through the processes, called the intra- and inter-marriage of AI, so that they can share their learned knowledge (weights and biases) based on a probabilistic rule and produce diversified child-AI agents to perform new tasks. This process will form a colony of AI that consists of families of multi-model and mixture-model AI agents to improve diversity and quality. Simulations show that the colony of AI, built using the VGG16, VGG19, and ResNet50 models, can provide a single system that generates child-AI agents of excellent predictive performance, ranging between 82% and 95% of F1-scores, to make diversified collective and quality decisions on a task.', 'abstract_zh': '基于卷积神经网络的多agent系统在人工智能中的应用：一种角色化的AI蚁群模型', 'title_zh': '受自然界启发的人工智能 colony 体系，配备快速、详细且组织化的学习代理，以增强多样性和质量'}
{'arxiv_id': 'arXiv:2504.05364', 'title': 'Of All StrIPEs: Investigating Structure-informed Positional Encoding for Efficient Music Generation', 'authors': 'Manvi Agarwal, Changhong Wang, Gael Richard', 'link': 'https://arxiv.org/abs/2504.05364', 'abstract': 'While music remains a challenging domain for generative models like Transformers, a two-pronged approach has recently proved successful: inserting musically-relevant structural information into the positional encoding (PE) module and using kernel approximation techniques based on Random Fourier Features (RFF) to lower the computational cost from quadratic to linear. Yet, it is not clear how such RFF-based efficient PEs compare with those based on rotation matrices, such as Rotary Positional Encoding (RoPE). In this paper, we present a unified framework based on kernel methods to analyze both families of efficient PEs. We use this framework to develop a novel PE method called RoPEPool, capable of extracting causal relationships from temporal sequences. Using RFF-based PEs and rotation-based PEs, we demonstrate how seemingly disparate PEs can be jointly studied by considering the content-context interactions they induce. For empirical validation, we use a symbolic music generation task, namely, melody harmonization. We show that RoPEPool, combined with highly-informative structural priors, outperforms all methods.', 'abstract_zh': '虽然对生成模型如变换器来说音乐仍是一个具有挑战性的领域，但最近一种两步方法已被证明非常成功：将音乐相关的结构信息插入到位置编码（PE）模块中，并利用随机傅里叶特征（RFF）为基础的核近似技术将计算成本从二次降低到线性。然而，基于RFF的高效位置编码与基于旋转矩阵的方法，如旋转位置编码（RoPE），之间的性能对比尚不明确。在本文中，我们提出了一种基于核方法的统一框架来分析这两种高效的PE类型。我们利用该框架开发了一种新的PE方法RoPEPool，它可以从中提取时间序列中的因果关系。通过基于RFF的PE和基于旋转的PE，我们展示了如何通过考虑它们引起的内容-上下文交互来联合研究看似不同的PE。为了进行实证验证，我们使用了符号音乐生成任务，即旋律和声。我们表明，结合高度信息的结构先验，RoPEPool优于所有方法。', 'title_zh': '全方位探究结构导向的位置编码以实现高效的音乐生成'}
{'arxiv_id': 'arXiv:2504.05357', 'title': 'Find A Winning Sign: Sign Is All We Need to Win the Lottery', 'authors': 'Junghun Oh, Sungyong Baik, Kyoung Mu Lee', 'link': 'https://arxiv.org/abs/2504.05357', 'abstract': 'The Lottery Ticket Hypothesis (LTH) posits the existence of a sparse subnetwork (a.k.a. winning ticket) that can generalize comparably to its over-parameterized counterpart when trained from scratch. The common approach to finding a winning ticket is to preserve the original strong generalization through Iterative Pruning (IP) and transfer information useful for achieving the learned generalization by applying the resulting sparse mask to an untrained network. However, existing IP methods still struggle to generalize their observations beyond ad-hoc initialization and small-scale architectures or datasets, or they bypass these challenges by applying their mask to trained weights instead of initialized ones. In this paper, we demonstrate that the parameter sign configuration plays a crucial role in conveying useful information for generalization to any randomly initialized network. Through linear mode connectivity analysis, we observe that a sparse network trained by an existing IP method can retain its basin of attraction if its parameter signs and normalization layer parameters are preserved. To take a step closer to finding a winning ticket, we alleviate the reliance on normalization layer parameters by preventing high error barriers along the linear path between the sparse network trained by our method and its counterpart with initialized normalization layer parameters. Interestingly, across various architectures and datasets, we observe that any randomly initialized network can be optimized to exhibit low error barriers along the linear path to the sparse network trained by our method by inheriting its sparsity and parameter sign information, potentially achieving performance comparable to the original. The code is available at this https URL\\this http URL', 'abstract_zh': 'lottery票假设（LTH）认为，在特定条件下，存在一个稀疏子网络（即 winning ticket），该子网络可以从头开始训练，其泛化能力与参数丰富网络相当。通常找到 winning ticket 的方法是通过迭代剪枝（IP）保留原始网络的强泛化能力，并通过将得到的稀疏掩码应用于未训练网络来传递实现学习泛化的有用信息。然而，现有的 IP 方法仍然难以将观察结果扩展到非随意初始化和小型架构或数据集之外，或者通过将掩码应用于训练后权重而非初始化权重来绕过这些挑战。本文我们证明了参数符号配置在向任意随机初始化网络传递泛化有用信息中起着关键作用。通过线性模式连通性分析，我们发现，使用现有 IP 方法训练的稀疏网络可以保留其吸引子盆地，前提是其参数符号和归一化层参数保持不变。为了更接近找到 winning ticket，我们通过防止从使用我们方法训练的稀疏网络到具有初始化归一化层参数的对应网络的线性路径上的高误差障碍来减轻依赖归一化层参数的限制。有趣的是，在各种架构和数据集上，我们观察到任何随机初始化的网络都可以被优化，以便在其到使用我们方法训练的稀疏网络的线性路径上表现出低误差障碍，潜在地达到与原始网络相当的性能。代码可在以下网址获得：this https URL this http URL', 'title_zh': '寻找获胜的征兆：只需符号即可赢得彩票'}
{'arxiv_id': 'arXiv:2504.05350', 'title': 'Non-linear Phillips Curve for India: Evidence from Explainable Machine Learning', 'authors': 'Shovon Sengupta, Bhanu Pratap, Amit Pawar', 'link': 'https://arxiv.org/abs/2504.05350', 'abstract': 'The conventional linear Phillips curve model, while widely used in policymaking, often struggles to deliver accurate forecasts in the presence of structural breaks and inherent nonlinearities. This paper addresses these limitations by leveraging machine learning methods within a New Keynesian Phillips Curve framework to forecast and explain headline inflation in India, a major emerging economy. Our analysis demonstrates that machine learning-based approaches significantly outperform standard linear models in forecasting accuracy. Moreover, by employing explainable machine learning techniques, we reveal that the Phillips curve relationship in India is highly nonlinear, characterized by thresholds and interaction effects among key variables. Headline inflation is primarily driven by inflation expectations, followed by past inflation and the output gap, while supply shocks, except rainfall, exert only a marginal influence. These findings highlight the ability of machine learning models to improve forecast accuracy and uncover complex, nonlinear dynamics in inflation data, offering valuable insights for policymakers.', 'abstract_zh': '基于机器学习的新凯恩斯菲利普斯曲线框架下的印度核心通胀预测与解释：超越传统的线性模型', 'title_zh': '印度的非线性菲利普斯曲线：可解释机器学习的证据'}
{'arxiv_id': 'arXiv:2504.05349', 'title': 'Hyperflows: Pruning Reveals the Importance of Weights', 'authors': 'Eugen Barbulescu, Antonio Alexoaie', 'link': 'https://arxiv.org/abs/2504.05349', 'abstract': "Network pruning is used to reduce inference latency and power consumption in large neural networks. However, most existing methods struggle to accurately assess the importance of individual weights due to their inherent interrelatedness, leading to poor performance, especially at extreme sparsity levels. We introduce Hyperflows, a dynamic pruning approach that estimates each weight's importance by observing the network's gradient response to the weight's removal. A global pressure term continuously drives all weights toward pruning, with those critical for accuracy being automatically regrown based on their flow, the aggregated gradient signal when they are absent. We explore the relationship between final sparsity and pressure, deriving power-law equations similar to those found in neural scaling laws. Empirically, we demonstrate state-of-the-art results with ResNet-50 and VGG-19 on CIFAR-10 and CIFAR-100.", 'abstract_zh': 'Hyperflows：一种基于梯度响应的动态剪枝方法及其在稀疏性与压力关系上的探索', 'title_zh': '超流：剪枝揭示了权重的重要性'}
{'arxiv_id': 'arXiv:2504.05342', 'title': 'MASS: MoErging through Adaptive Subspace Selection', 'authors': 'Donato Crisostomi, Alessandro Zirilli, Antonio Andrea Gargiulo, Maria Sofia Bucarelli, Simone Scardapane, Fabrizio Silvestri, Iacopo Masi, Emanuele Rodolà', 'link': 'https://arxiv.org/abs/2504.05342', 'abstract': "Model merging has recently emerged as a lightweight alternative to ensembling, combining multiple fine-tuned models into a single set of parameters with no additional training overhead. Yet, existing merging methods fall short of matching the full accuracy of separately fine-tuned endpoints. We present MASS (MoErging through Adaptive Subspace Selection), a new approach that closes this gap by unifying multiple fine-tuned models while retaining near state-of-the-art performance across tasks. Building on the low-rank decomposition of per-task updates, MASS stores only the most salient singular components for each task and merges them into a shared model. At inference time, a non-parametric, data-free router identifies which subspace (or combination thereof) best explains an input's intermediate features and activates the corresponding task-specific block. This procedure is fully training-free and introduces only a two-pass inference overhead plus a ~2 storage factor compared to a single pretrained model, irrespective of the number of tasks. We evaluate MASS on CLIP-based image classification using ViT-B-16, ViT-B-32 and ViT-L-14 for benchmarks of 8, 14 and 20 tasks respectively, establishing a new state-of-the-art. Most notably, MASS recovers up to ~98% of the average accuracy of individual fine-tuned models, making it a practical alternative to ensembling at a fraction of the storage cost.", 'abstract_zh': 'MASS：通过自适应子空间选择的MoErging', 'title_zh': 'MASS: 通过自适应子空间选择的MoE化'}
{'arxiv_id': 'arXiv:2504.05341', 'title': 'Three-Factor Learning in Spiking Neural Networks: An Overview of Methods and Trends from a Machine Learning Perspective', 'authors': 'Szymon Mazurek, Jakub Caputa, Jan K. Argasiński, Maciej Wielgosz', 'link': 'https://arxiv.org/abs/2504.05341', 'abstract': 'Three-factor learning rules in Spiking Neural Networks (SNNs) have emerged as a crucial extension to traditional Hebbian learning and Spike-Timing-Dependent Plasticity (STDP), incorporating neuromodulatory signals to improve adaptation and learning efficiency. These mechanisms enhance biological plausibility and facilitate improved credit assignment in artificial neural systems. This paper takes a view on this topic from a machine learning perspective, providing an overview of recent advances in three-factor learning, discusses theoretical foundations, algorithmic implementations, and their relevance to reinforcement learning and neuromorphic computing. In addition, we explore interdisciplinary approaches, scalability challenges, and potential applications in robotics, cognitive modeling, and AI systems. Finally, we highlight key research gaps and propose future directions for bridging the gap between neuroscience and artificial intelligence.', 'abstract_zh': '三因子学习规则在神经脉冲网络（SNNs）中的新兴作用：作为传统 Hebbsian 学习和时序依赖可塑性（STDP）的关键扩展，通过引入神经调制信号以提高适应性和学习效率。这些机制增强了生物可行性，并促进了人工神经系统中的改进责任指派。从机器学习的角度出发，本文概述了三因子学习的最近进展，讨论了其理论基础、算法实现及其与强化学习和神经形态计算的相关性。此外，我们探讨了跨学科方法、可扩展性挑战及其在机器人、认知建模和AI系统中的潜在应用。最后，我们强调了关键研究空白，并提议了将神经科学与人工智能联系起来的未来研究方向。', 'title_zh': '从机器学习视角出发的.spi神经网络三因素学习综述：方法与趋势'}
{'arxiv_id': 'arXiv:2504.05338', 'title': 'Improving Early Prediction of Type 2 Diabetes Mellitus with ECG-DiaNet: A Multimodal Neural Network Leveraging Electrocardiogram and Clinical Risk Factors', 'authors': 'Farida Mohsen, Zubair Shah', 'link': 'https://arxiv.org/abs/2504.05338', 'abstract': "Type 2 Diabetes Mellitus (T2DM) remains a global health challenge, underscoring the need for early and accurate risk prediction. This study presents ECG-DiaNet, a multimodal deep learning model that integrates electrocardiogram (ECG) features with clinical risk factors (CRFs) to enhance T2DM onset prediction. Using data from Qatar Biobank (QBB), we trained and validated models on a development cohort (n=2043) and evaluated performance on a longitudinal test set (n=395) with five-year follow-up. ECG-DiaNet outperformed unimodal ECG-only and CRF-only models, achieving a higher AUROC (0.845 vs 0.8217) than the CRF-only model, with statistical significance (DeLong p<0.001). Reclassification metrics further confirmed improvements: Net Reclassification Improvement (NRI=0.0153) and Integrated Discrimination Improvement (IDI=0.0482). Risk stratification into low-, medium-, and high-risk groups showed ECG-DiaNet achieved superior positive predictive value (PPV) in high-risk individuals. The model's reliance on non-invasive and widely available ECG signals supports its feasibility in clinical and community health settings. By combining cardiac electrophysiology and systemic risk profiles, ECG-DiaNet addresses the multifactorial nature of T2DM and supports precision prevention. These findings highlight the value of multimodal AI in advancing early detection and prevention strategies for T2DM, particularly in underrepresented Middle Eastern populations.", 'abstract_zh': 'Type 2糖尿病 Mellitus (T2DM) 继续是全球健康挑战，凸显了早期和准确风险预测的必要性。本研究提出了一种多模态深度学习模型ECG-DiaNet，该模型将心电图 (ECG) 特征与临床风险因素 (CRFs) 结合起来，以提高T2DM发病预测。使用卡塔尔生物银行 (QBB) 数据，我们在开发队列 (n=2043) 上训练和验证了模型，并在包含五年人随访的数据队列 (n=395) 上进行了评估。ECG-DiaNet 在 AUCROC (0.845 vs 0.8217) 方面优于仅使用ECG 和仅使用CRF 的模型，具有统计学意义 (DeLong p<0.001)。重新分类指标进一步证实了这些改善：Net Reclassification Improvement (NRI=0.0153) 和 Integrated Discrimination Improvement (IDI=0.0482)。风险分层结果显示ECG-DiaNet 在高风险个体中的阳性预测值 (PPV) 更高。该模型依赖于非侵入性且广泛可用的心电图信号，支持其在临床和社区健康设置中的可行性。通过结合心脏电生理学和系统风险特征，ECG-DiaNet 解决了T2DM 的多因素特征，并支持精准预防。这些发现强调了多模态AI 在促进T2DM 早期检测和预防策略方面的价值，特别是在未被充分代表的中东人群中。', 'title_zh': '基于心电图和临床危险因素的多模态神经网络ECG-DiaNet早期预测2型糖尿病 Mellitus 的改进研究'}
{'arxiv_id': 'arXiv:2504.05334', 'title': 'Level Generation with Constrained Expressive Range', 'authors': 'Mahsa Bazzaz, Seth Cooper', 'link': 'https://arxiv.org/abs/2504.05334', 'abstract': "Expressive range analysis is a visualization-based technique used to evaluate the performance of generative models, particularly in game level generation. It typically employs two quantifiable metrics to position generated artifacts on a 2D plot, offering insight into how content is distributed within a defined metric space. In this work, we use the expressive range of a generator as the conceptual space of possible creations. Inspired by the quality diversity paradigm, we explore this space to generate levels. To do so, we use a constraint-based generator that systematically traverses and generates levels in this space. To train the constraint-based generator we use different tile patterns to learn from the initial example levels. We analyze how different patterns influence the exploration of the expressive range. Specifically, we compare the exploration process based on time, the number of successful and failed sample generations, and the overall interestingness of the generated levels. Unlike typical quality diversity approaches that rely on random generation and hope to get good coverage of the expressive range, this approach systematically traverses the grid ensuring more coverage. This helps create unique and interesting game levels while also improving our understanding of the generator's strengths and limitations.", 'abstract_zh': '表达范围分析是一种基于可视化的技术，用于评估生成模型的表现，特别是在游戏关卡生成中的应用。通常，它使用两个可量化指标在2D图上定位生成的元素，提供内容在定义的度量空间内的分布洞察。在本文中，我们将生成器的表达范围作为可能创作的概念空间。受质量多样性范式的启发，我们在该空间中探索以生成关卡。为此，我们使用基于约束的生成器，系统地遍历并生成该空间中的关卡。为了训练基于约束的生成器，我们使用不同的瓷砖图案从初始示例关卡中学习。我们分析不同模式如何影响表达范围的探索过程。具体而言，我们根据时间、成功和失败样本生成的数量以及生成关卡的总体趣味性来比较探索过程。与依赖随机生成以期望获得表达范围良好覆盖的典型质量多样性方法不同，这种方法系统地遍历网格，确保更好的覆盖。这有助于创建独特的有趣游戏关卡，同时也有助于我们更好地理解生成器的优势和局限性。', 'title_zh': '带有受限表达范围的关卡生成'}
{'arxiv_id': 'arXiv:2504.05333', 'title': 'When is using AI the rational choice? The importance of counterfactuals in AI deployment decisions', 'authors': 'Paul Lehner, Elinor Yeo', 'link': 'https://arxiv.org/abs/2504.05333', 'abstract': 'Decisions to deploy AI capabilities are often driven by counterfactuals - a comparison of decisions made using AI to decisions that would have been made if the AI were not used. Counterfactual misses, which are poor decisions that are attributable to using AI, may have disproportionate disutility to AI deployment decision makers. Counterfactual hits, which are good decisions attributable to AI usage, may provide little benefit beyond the benefit of better decisions. This paper explores how to include counterfactual outcomes into usage decision expected utility assessments. Several properties emerge when counterfactuals are explicitly included. First, there are many contexts where the expected utility of AI usage is positive for intended beneficiaries and strongly negative for stakeholders and deployment decision makers. Second, high levels of complementarity, where differing AI and user assessments are merged beneficially, often leads to substantial disutility for stakeholders. Third, apparently small changes in how users interact with an AI capability can substantially impact stakeholder utility. Fourth, cognitive biases such as expert overconfidence and hindsight bias exacerbate the perceived frequency of costly counterfactual misses. The expected utility assessment approach presented here is intended to help AI developers and deployment decision makers to navigate the subtle but substantial impact of counterfactuals so as to better ensure that beneficial AI capabilities are used.', 'abstract_zh': '基于反事实结果的AI能力部署决策预期效用评估', 'title_zh': '何时使用AI是理性选择？反事实推理在AI部署决策中的重要性'}
{'arxiv_id': 'arXiv:2504.05331', 'title': 'Not someone, but something: Rethinking trust in the age of medical AI', 'authors': 'Jan Beger', 'link': 'https://arxiv.org/abs/2504.05331', 'abstract': "As artificial intelligence (AI) becomes embedded in healthcare, trust in medical decision-making is changing fast. This opinion paper argues that trust in AI isn't a simple transfer from humans to machines -- it's a dynamic, evolving relationship that must be built and maintained. Rather than debating whether AI belongs in medicine, this paper asks: what kind of trust must AI earn, and how? Drawing from philosophy, bioethics, and system design, it explores the key differences between human trust and machine reliability -- emphasizing transparency, accountability, and alignment with the values of care. It argues that trust in AI shouldn't rely on mimicking empathy or intuition, but on thoughtful design, responsible deployment, and clear moral responsibility. The goal is a balanced view -- one that avoids blind optimism and reflexive fear. Trust in AI must be treated not as a given, but as something to be earned over time.", 'abstract_zh': '随着人工智能（AI）在医疗领域的嵌入，对医疗决策的信任正在迅速变化。本文认为，对AI的信任不是从人类简单转移到机器上的单一过程——而是一种动态且不断演变的关系，需要通过建设和维护来形成。本文不讨论AI是否属于医疗领域，而是探讨AI应当获得何种信任以及如何获得。结合哲学、生物伦理学和系统设计的视角，本文探讨了人类信任与机器可靠性之间的关键差异，强调透明度、可问责性和与关爱价值观的一致性。本文 argue，对AI的信任不应依赖于模仿同情或直觉，而应依赖于深思熟虑的设计、负责任的应用和明确的道德责任。目标是形成一个平衡的观点——既避免盲目的乐观主义，也避免无根据的恐惧。对AI的信任不应被视为理所当然，而应视为需要通过时间来赢得的。', 'title_zh': '不仅仅是某个人，而是一种东西：再思考医疗AI时代的信任问题'}
{'arxiv_id': 'arXiv:2504.05323', 'title': 'Multi-Perspective Attention Mechanism for Bias-Aware Sequential Recommendation', 'authors': 'Mingjian Fu, Hengsheng Chen, Dongchun Jiang, Yanchao Tan', 'link': 'https://arxiv.org/abs/2504.05323', 'abstract': "In the era of advancing information technology, recommender systems have emerged as crucial tools for dealing with information overload. However, traditional recommender systems still have limitations in capturing the dynamic evolution of user behavior. To better understand and predict user behavior, especially taking into account the complexity of temporal evolution, sequential recommender systems have gradually become the focus of research. Currently, many sequential recommendation algorithms ignore the amplification effects of prevalent biases, which leads to recommendation results being susceptible to the Matthew Effect. Additionally, it will impose limitations on the recommender system's ability to deeply perceive and capture the dynamic shifts in user preferences, thereby diminishing the extent of its recommendation reach. To address this issue effectively, we propose a recommendation system based on sequential information and attention mechanism called Multi-Perspective Attention Bias Sequential Recommendation (MABSRec). Firstly, we reconstruct user sequences into three short types and utilize graph neural networks for item weighting. Subsequently, an adaptive multi-bias perspective attention module is proposed to enhance the accuracy of recommendations. Experimental results show that the MABSRec model exhibits significant advantages in all evaluation metrics, demonstrating its excellent performance in the sequence recommendation task.", 'abstract_zh': '电子商务环境下基于多视角注意力偏好修正的序列推荐系统（MABSRec）', 'title_zh': '面向偏差感知的多视角注意力机制序列推荐'}
{'arxiv_id': 'arXiv:2504.05319', 'title': 'Predictive Modeling: BIM Command Recommendation Based on Large-scale Usage Logs', 'authors': 'Changyu Du, Zihan Deng, Stavros Nousias, André Borrmann', 'link': 'https://arxiv.org/abs/2504.05319', 'abstract': "The adoption of Building Information Modeling (BIM) and model-based design within the Architecture, Engineering, and Construction (AEC) industry has been hindered by the perception that using BIM authoring tools demands more effort than conventional 2D drafting. To enhance design efficiency, this paper proposes a BIM command recommendation framework that predicts the optimal next actions in real-time based on users' historical interactions. We propose a comprehensive filtering and enhancement method for large-scale raw BIM log data and introduce a novel command recommendation model. Our model builds upon the state-of-the-art Transformer backbones originally developed for large language models (LLMs), incorporating a custom feature fusion module, dedicated loss function, and targeted learning strategy. In a case study, the proposed method is applied to over 32 billion rows of real-world log data collected globally from the BIM authoring software Vectorworks. Experimental results demonstrate that our method can learn universal and generalizable modeling patterns from anonymous user interaction sequences across different countries, disciplines, and projects. When generating recommendations for the next command, our approach achieves a Recall@10 of approximately 84%.", 'abstract_zh': '建筑信息建模（BIM）和基于模型的设计在建筑、工程和施工（AEC）行业的采用受到使用BIM创作工具比传统2D制图需要更多努力的认知阻碍。为了提高设计效率，本文提出了一种BIM命令推荐框架，该框架基于用户的历史交互实时预测最优化的下一步操作。我们提出了一种针对大规模原始BIM日志数据的全面过滤和增强方法，并引入了一种新型命令推荐模型。该模型基于专门为大型语言模型（LLMs）开发的最先进的Transformer骨干架构，结合了自定义特征融合模块、专用损失函数和目标化学习策略。在一项案例研究中，该方法应用于全球从Vectorworks BIM创作软件收集的超过320亿条真实日志数据。实验结果表明，我们的方法可以从不同国家、学科和项目中的匿名用户交互序列中学习到通用且可泛化的建模模式。在生成下一个命令的推荐时，我们的方法实现了约84%的Recall@10。', 'title_zh': '基于大规模使用日志的BIM命令预测模型'}
{'arxiv_id': 'arXiv:2504.05318', 'title': 'Efficient Multi-Task Learning via Generalist Recommender', 'authors': 'Luyang Wang, Cangcheng Tang, Chongyang Zhang, Jun Ruan, Kai Huang, Jason Dai', 'link': 'https://arxiv.org/abs/2504.05318', 'abstract': 'Multi-task learning (MTL) is a common machine learning technique that allows the model to share information across different tasks and improve the accuracy of recommendations for all of them. Many existing MTL implementations suffer from scalability issues as the training and inference performance can degrade with the increasing number of tasks, which can limit production use case scenarios for MTL-based recommender systems. Inspired by the recent advances of large language models, we developed an end-to-end efficient and scalable Generalist Recommender (GRec). GRec takes comprehensive data signals by utilizing NLP heads, parallel Transformers, as well as a wide and deep structure to process multi-modal inputs. These inputs are then combined and fed through a newly proposed task-sentence level routing mechanism to scale the model capabilities on multiple tasks without compromising performance. Offline evaluations and online experiments show that GRec significantly outperforms our previous recommender solutions. GRec has been successfully deployed on one of the largest telecom websites and apps, effectively managing high volumes of online traffic every day.', 'abstract_zh': '多任务学习（MTL）是一种常见的机器学习技术，允许模型在不同任务之间共享信息并提高所有任务的推荐准确性。许多现有的MTL实现由于训练和推理性能会随着任务数量的增加而恶化，从而限制了基于MTL的推荐系统的生产应用场景。受大型语言模型近期进展的启发，我们开发了一个端到端高效且可扩展的通用推荐系统（GRec）。GRec通过利用NLP头部、并行Transformer以及宽深结构来处理多模态输入，并采取了一种新的任务-句子级别路由机制，使其能够在多个任务上扩展模型能力而不牺牲性能。离线评估和在线实验表明，GRec显著优于我们之前的推荐解决方案。GRec已成功部署在最大的电信网站和应用程序之一上，每天有效管理大量的在线流量。', 'title_zh': '高效多任务学习通过通用推荐器'}
{'arxiv_id': 'arXiv:2504.05312', 'title': 'Towards Adaptive Memory-Based Optimization for Enhanced Retrieval-Augmented Generation', 'authors': 'Qitao Qin, Yucong Luo, Yihang Lu, Zhibo Chu, Xianwei Meng', 'link': 'https://arxiv.org/abs/2504.05312', 'abstract': "Retrieval-Augmented Generation (RAG), by integrating non-parametric knowledge from external knowledge bases into models, has emerged as a promising approach to enhancing response accuracy while mitigating factual errors and hallucinations. This method has been widely applied in tasks such as Question Answering (QA). However, existing RAG methods struggle with open-domain QA tasks because they perform independent retrieval operations and directly incorporate the retrieved information into generation without maintaining a summarizing memory or using adaptive retrieval strategies, leading to noise from redundant information and insufficient information integration. To address these challenges, we propose Adaptive memory-based optimization for enhanced RAG (Amber) for open-domain QA tasks, which comprises an Agent-based Memory Updater, an Adaptive Information Collector, and a Multi-granular Content Filter, working together within an iterative memory updating paradigm. Specifically, Amber integrates and optimizes the language model's memory through a multi-agent collaborative approach, ensuring comprehensive knowledge integration from previous retrieval steps. It dynamically adjusts retrieval queries and decides when to stop retrieval based on the accumulated knowledge, enhancing retrieval efficiency and effectiveness. Additionally, it reduces noise by filtering irrelevant content at multiple levels, retaining essential information to improve overall model performance. We conduct extensive experiments on several open-domain QA datasets, and the results demonstrate the superiority and effectiveness of our method and its components. The source code is available \\footnote{this https URL}.", 'abstract_zh': '检索增强生成（RAG）通过将外部知识库中的非参数知识集成到模型中，已发展成为一种增强响应准确性、减轻事实错误和幻觉的有效方法。该方法在问答（QA）等任务中得到了广泛应用。然而，现有的RAG方法在处理开放域QA任务时存在问题，因为它们独立执行检索操作，并直接将检索到的信息融入生成过程中，而不保留总结性记忆或使用适应性检索策略，导致冗余信息的噪音和信息整合不足。为了解决这些挑战，我们提出了适用于开放域QA任务的自适应记忆优化以增强RAG（Amber），该方法包括基于代理的记忆更新器、自适应信息收集器和多粒度内容过滤器，共同在迭代的记忆更新范式中工作。具体而言，Amber通过多代理合作方法整合和优化语言模型的记忆，确保从之前的检索步骤中进行全面的知识整合。它会根据累积的知识动态调整检索查询，并决定何时停止检索，从而提高检索效率和效果。此外，它通过在多个级别上过滤无关内容来减少噪音，保留关键信息以提高整体模型性能。我们在多个开放域QA数据集上进行了广泛的实验，结果表明我们方法及其组件的优越性和有效性。源代码可通过以下链接获取。', 'title_zh': '面向自适应记忆优化的增强检索辅助生成'}
{'arxiv_id': 'arXiv:2504.05307', 'title': 'Toward Total Recall: Enhancing FAIRness through AI-Driven Metadata Standardization', 'authors': 'Sowmya S Sundaram, Mark A Musen', 'link': 'https://arxiv.org/abs/2504.05307', 'abstract': 'Current metadata often suffer from incompleteness, inconsistency, and incorrect formatting, hindering effective data reuse and discovery. Using GPT-4 and a metadata knowledge base (CEDAR), we devised a method that standardizes metadata in scientific data sets, ensuring the adherence to community standards. The standardization process involves correcting and refining metadata entries to conform to established guidelines, significantly improving search performance and recall metrics. The investigation uses BioSample and GEO repositories to demonstrate the impact of these enhancements, showcasing how standardized metadata lead to better retrieval outcomes. The average recall improves significantly, rising from 17.65\\% with the baseline raw datasets of BioSample and GEO to 62.87\\% with our proposed metadata standardization pipeline. This finding highlights the transformative impact of integrating advanced AI models with structured metadata curation tools in achieving more effective and reliable data retrieval.', 'abstract_zh': '当前元数据往往存在不完整、不一致和格式错误的问题，阻碍了数据的有效重用和发现。利用GPT-4和元数据知识库（CEDAR），我们提出了一种方法，对科学数据集中的元数据进行标准化，确保其符合社区标准。标准化过程涉及修正和完善元数据条目的格式，以符合既定指南，显著提高搜索性能和召回率指标。该研究使用BioSample和GEO存储库来展示这些改进的影响，展示了标准化元数据如何提高检索效果。平均召回率显著提升，从基础原始BioSample和GEO数据集的17.65%提升到我们提出的元数据标准化流程的62.87%。这一发现强调了将先进AI模型与结构化元数据管理工具结合使用在实现更有效和可靠的数据检索方面的变革性影响。', 'title_zh': '向着全面回忆：通过AI驱动的元数据标准化提升FAIR性'}
