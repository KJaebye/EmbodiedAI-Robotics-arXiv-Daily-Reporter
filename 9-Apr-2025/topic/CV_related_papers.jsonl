{'arxiv_id': 'arXiv:2504.06105', 'title': 'Uncertainty-Aware Hybrid Machine Learning in Virtual Sensors for Vehicle Sideslip Angle Estimation', 'authors': 'Abinav Kalyanasundaram, Karthikeyan Chandra Sekaran, Philipp Stauber, Michael Lange, Wolfgang Utschick, Michael Botsch', 'link': 'https://arxiv.org/abs/2504.06105', 'abstract': 'Precise vehicle state estimation is crucial for safe and reliable autonomous driving. The number of measurable states and their precision offered by the onboard vehicle sensor system are often constrained by cost. For instance, measuring critical quantities such as the Vehicle Sideslip Angle (VSA) poses significant commercial challenges using current optical sensors. This paper addresses these limitations by focusing on the development of high-performance virtual sensors to enhance vehicle state estimation for active safety. The proposed Uncertainty-Aware Hybrid Learning (UAHL) architecture integrates a machine learning model with vehicle motion models to estimate VSA directly from onboard sensor data. A key aspect of the UAHL architecture is its focus on uncertainty quantification for individual model estimates and hybrid fusion. These mechanisms enable the dynamic weighting of uncertainty-aware predictions from machine learning and vehicle motion models to produce accurate and reliable hybrid VSA estimates. This work also presents a novel dataset named Real-world Vehicle State Estimation Dataset (ReV-StED), comprising synchronized measurements from advanced vehicle dynamic sensors. The experimental results demonstrate the superior performance of the proposed method for VSA estimation, highlighting UAHL as a promising architecture for advancing virtual sensors and enhancing active safety in autonomous vehicles.', 'abstract_zh': '精确的车辆状态估计对于安全可靠的自动驾驶至关重要。车载车辆传感器系统可测量的状态数量及其精度往往受限于成本。例如，使用当前的光学传感器测量关键参数（如汽车侧滑角）存在显著的商业挑战。本文通过专注于高性能虚拟传感器的开发来克服这些限制，以提高车辆状态估计算法中的主动安全性。所提出的一种结合机器学习模型和车辆运动模型的不确定性意识混合学习（UAHL）架构能够直接从车载传感器数据中估计汽车侧滑角。UAHL架构的关键方面在于其对单个模型估计值的不确定性量化和混合融合机制。这些机制使不确定性意识预测能够动态加权结合，从而产生准确可靠的混合侧滑角估计。此外，本文还提出了一种名为Real-world Vehicle State Estimation Dataset (ReV-StED)的新数据集，包含高级车辆动态传感器的同步测量数据。实验结果表明，所提出的方法在侧滑角估计方面表现出优越的性能，突出了UAHL架构在推进虚拟传感器发展和提高自动驾驶车辆主动安全性方面的前景。', 'title_zh': '面向车辆侧滑角估计的不确定性感知混合机器学习在虚拟传感器中的应用'}
{'arxiv_id': 'arXiv:2504.05983', 'title': 'Modular Soft Wearable Glove for Real-Time Gesture Recognition and Dynamic 3D Shape Reconstruction', 'authors': 'Huazhi Dong, Chunpeng Wang, Mingyuan Jiang, Francesco Giorgio-Serchi, Yunjie Yang', 'link': 'https://arxiv.org/abs/2504.05983', 'abstract': "With the increasing demand for human-computer interaction (HCI), flexible wearable gloves have emerged as a promising solution in virtual reality, medical rehabilitation, and industrial automation. However, the current technology still has problems like insufficient sensitivity and limited durability, which hinder its wide application. This paper presents a highly sensitive, modular, and flexible capacitive sensor based on line-shaped electrodes and liquid metal (EGaIn), integrated into a sensor module tailored to the human hand's anatomy. The proposed system independently captures bending information from each finger joint, while additional measurements between adjacent fingers enable the recording of subtle variations in inter-finger spacing. This design enables accurate gesture recognition and dynamic hand morphological reconstruction of complex movements using point clouds. Experimental results demonstrate that our classifier based on Convolution Neural Network (CNN) and Multilayer Perceptron (MLP) achieves an accuracy of 99.15% across 30 gestures. Meanwhile, a transformer-based Deep Neural Network (DNN) accurately reconstructs dynamic hand shapes with an Average Distance (AD) of 2.076\\pm3.231 mm, with the reconstruction accuracy at individual key points surpassing SOTA benchmarks by 9.7% to 64.9%. The proposed glove shows excellent accuracy, robustness and scalability in gesture recognition and hand reconstruction, making it a promising solution for next-generation HCI systems.", 'abstract_zh': '基于线形电极和液态金属的高灵敏度模块化柔性电容传感器在手套中的应用：面向复杂手部动作识别与动态手型重建', 'title_zh': '模块化柔韌可穿戴手套：實現-real-time 手勢識別和動態3D形狀重建'}
{'arxiv_id': 'arXiv:2504.05422', 'title': 'EP-Diffuser: An Efficient Diffusion Model for Traffic Scene Generation and Prediction via Polynomial Representations', 'authors': 'Yue Yao, Mohamed-Khalil Bouzidi, Daniel Goehring, Joerg Reichardt', 'link': 'https://arxiv.org/abs/2504.05422', 'abstract': 'As the prediction horizon increases, predicting the future evolution of traffic scenes becomes increasingly difficult due to the multi-modal nature of agent motion. Most state-of-the-art (SotA) prediction models primarily focus on forecasting the most likely future. However, for the safe operation of autonomous vehicles, it is equally important to cover the distribution for plausible motion alternatives. To address this, we introduce EP-Diffuser, a novel parameter-efficient diffusion-based generative model designed to capture the distribution of possible traffic scene evolutions. Conditioned on road layout and agent history, our model acts as a predictor and generates diverse, plausible scene continuations. We benchmark EP-Diffuser against two SotA models in terms of accuracy and plausibility of predictions on the Argoverse 2 dataset. Despite its significantly smaller model size, our approach achieves both highly accurate and plausible traffic scene predictions. We further evaluate model generalization ability in an out-of-distribution (OoD) test setting using Waymo Open dataset and show superior robustness of our approach. The code and model checkpoints can be found here: this https URL.', 'abstract_zh': '随着预测 horizon 的增加，由于代理运动的多模态性质，预测交通场景的未来演变变得越来越困难。大多数最先进的（SotA）预测模型主要专注于预测最可能的未来。然而，为了确保自动驾驶车辆的安全操作，覆盖可能运动的分布同样重要。为了解决这个问题，我们引入了 EP-Diffuser，这是一种新颖的参数高效扩散生成模型，旨在捕捉可能的交通场景演变分布。在道路布局和代理历史的条件下，我们的模型充当预测器并生成多样化且合理的场景延续。我们基于 Argoverse 2 数据集的预测准确性和合理性，将 EP-Diffuser 与两种 SotA 模型进行了对比测试。尽管模型规模显著较小，我们的方法仍能够实现高度准确且合理的交通场景预测。我们进一步在 Waymo Open 数据集的分布外（OoD）测试设置中评估模型的泛化能力，并展示了该方法的优越鲁棒性。代码和模型检查点可以在这里找到：this https URL。', 'title_zh': 'EP-Diffuser: 一种通过多项式表示的高效扩散模型用于交通场景生成与预测'}
{'arxiv_id': 'arXiv:2504.06185', 'title': 'WoundAmbit: Bridging State-of-the-Art Semantic Segmentation and Real-World Wound Care', 'authors': 'Vanessa Borst, Timo Dittus, Tassilo Dege, Astrid Schmieder, Samuel Kounev', 'link': 'https://arxiv.org/abs/2504.06185', 'abstract': 'Chronic wounds affect a large population, particularly the elderly and diabetic patients, who often exhibit limited mobility and co-existing health conditions. Automated wound monitoring via mobile image capture can reduce in-person physician visits by enabling remote tracking of wound size. Semantic segmentation is key to this process, yet wound segmentation remains underrepresented in medical imaging research. To address this, we benchmark state-of-the-art deep learning models from general-purpose vision, medical imaging, and top methods from public wound challenges. For fair comparison, we standardize training, data augmentation, and evaluation, conducting cross-validationto minimize partitioning bias. We also assess real-world deployment aspects, including generalization to an out-of-distribution wound dataset, computational efficiency, and interpretability. Additionally, we propose a reference object-based approach to convert AI-generated masks into clinically relevant wound size estimates, and evaluate this, along with mask quality, for the best models based on physician assessments. Overall, the transformer-based TransNeXt showed the highest levels of generalizability. Despite variations in inference times, all models processed at least one image per second on the CPU, which is deemed adequate for the intended application. Interpretability analysis typically revealed prominent activations in wound regions, emphasizing focus on clinically relevant features. Expert evaluation showed high mask approval for all analyzed models, with VWFormer and ConvNeXtS backbone performing the best. Size retrieval accuracy was similar across models, and predictions closely matched expert annotations. Finally, we demonstrate how our AI-driven wound size estimation framework, WoundAmbit, can be integrated into a custom telehealth system. Our code will be made available on GitHub upon publication.', 'abstract_zh': '慢性伤口影响大量人群，特别是老年人和糖尿病患者，他们常常表现为活动受限和共存的健康问题。通过移动图像捕获进行的自动化伤口监测可以减少亲自就医的次数，通过实现伤口尺寸的远程跟踪。语义分割是这个过程中的关键步骤，但伤口分割在医学成像研究中仍然相对不足。为了解决这一问题，我们对比了通用视觉、医学影像以及公开伤口挑战的顶级方法中的先进深度学习模型。为确保公平比较，我们统一了训练、数据增强和评估的标准，并通过交叉验证来最小化分层偏差。我们还评估了实际部署方面的因素，包括模型对未见伤口数据集的泛化能力、计算效率和可解释性。此外，我们提出了一种基于参考对象的方法，将AI生成的掩码转换为临床相关的伤口大小估计值，并基于医生评估来评估这些模型的性能，包括掩码质量。总体而言，基于变压器的TransNeXt模型在泛化能力方面表现出最高水平。尽管推断时间存在差异，所有模型在CPU上每秒至少处理一幅图像，这被认为足够满足应用需求。可解释性分析通常显示显著激活集中在伤口区域，强调对临床相关特征的关注。专家评估表明，所有分析的模型都获得了高掩码批准率，VWFormer和ConvNeXtS骨干表现最佳。伤口大小检索的准确性在不同模型间相似，预测值与专家标注高度一致。最后，我们展示了如何将我们的AI驱动的伤口大小估计框架WoundAmbit整合到自定义的远程医疗系统中。代码将在发表后发布于GitHub上。', 'title_zh': 'WoundAmbit: 连接最先进的语义分割与实际伤口护理'}
{'arxiv_id': 'arXiv:2504.06165', 'title': 'Real-Time Pitch/F0 Detection Using Spectrogram Images and Convolutional Neural Networks', 'authors': 'Xufang Zhao, Omer Tsimhoni', 'link': 'https://arxiv.org/abs/2504.06165', 'abstract': 'This paper presents a novel approach to detect F0 through Convolutional Neural Networks and image processing techniques to directly estimate pitch from spectrogram images. Our new approach demonstrates a very good detection accuracy; a total of 92% of predicted pitch contours have strong or moderate correlations to the true pitch contours. Furthermore, the experimental comparison between our new approach and other state-of-the-art CNN methods reveals that our approach can enhance the detection rate by approximately 5% across various Signal-to-Noise Ratio conditions.', 'abstract_zh': '本文提出了一种通过卷积神经网络和图像处理技术从spectrogram图像直接估计音高的新颖方法。我们的新方法展示了非常良好的检测准确性；预测的音调轮廓中有92%与真实音调轮廓具有强烈或中等的相关性。此外，与现有的最佳CNN方法的实验比较表明，我们的方法可以在各种信噪比条件下将检测率提高约5%。', 'title_zh': '基于谱图图像和卷积神经网络的实时音调/F0检测'}
{'arxiv_id': 'arXiv:2504.06099', 'title': 'Towards Varroa destructor mite detection using a narrow spectra illumination', 'authors': 'Samuel Bielik, Simon Bilik', 'link': 'https://arxiv.org/abs/2504.06099', 'abstract': 'This paper focuses on the development and modification of a beehive monitoring device and Varroa destructor detection on the bees with the help of hyperspectral imagery while utilizing a U-net, semantic segmentation architecture, and conventional computer vision methods. The main objectives were to collect a dataset of bees and mites, and propose the computer vision model which can achieve the detection between bees and mites.', 'abstract_zh': '本文专注于利用高光谱影像技术开发和修改养蜂监测设备，并通过U-net语义分割架构和传统计算机视觉方法检测蜜蜂上的Varroa destructor。主要目标是收集蜜蜂和螨虫的 datasets，并提出一种计算机视觉模型以实现蜜蜂与螨虫之间的检测。', 'title_zh': '使用窄谱illumination用于检测Varroa destructor寄生虫的研究'}
{'arxiv_id': 'arXiv:2504.06088', 'title': 'MCAT: Visual Query-Based Localization of Standard Anatomical Clips in Fetal Ultrasound Videos Using Multi-Tier Class-Aware Token Transformer', 'authors': 'Divyanshu Mishra, Pramit Saha, He Zhao, Netzahualcoyotl Hernandez-Cruz, Olga Patey, Aris Papageorghiou, J. Alison Noble', 'link': 'https://arxiv.org/abs/2504.06088', 'abstract': "Accurate standard plane acquisition in fetal ultrasound (US) videos is crucial for fetal growth assessment, anomaly detection, and adherence to clinical guidelines. However, manually selecting standard frames is time-consuming and prone to intra- and inter-sonographer variability. Existing methods primarily rely on image-based approaches that capture standard frames and then classify the input frames across different anatomies. This ignores the dynamic nature of video acquisition and its interpretation. To address these challenges, we introduce Multi-Tier Class-Aware Token Transformer (MCAT), a visual query-based video clip localization (VQ-VCL) method, to assist sonographers by enabling them to capture a quick US sweep. By then providing a visual query of the anatomy they wish to analyze, MCAT returns the video clip containing the standard frames for that anatomy, facilitating thorough screening for potential anomalies. We evaluate MCAT on two ultrasound video datasets and a natural image VQ-VCL dataset based on Ego4D. Our model outperforms state-of-the-art methods by 10% and 13% mIoU on the ultrasound datasets and by 5.35% mIoU on the Ego4D dataset, using 96% fewer tokens. MCAT's efficiency and accuracy have significant potential implications for public health, especially in low- and middle-income countries (LMICs), where it may enhance prenatal care by streamlining standard plane acquisition, simplifying US-based screening, diagnosis and allowing sonographers to examine more patients.", 'abstract_zh': '基于视觉查询的多层级类意识 Tokens 转换器在胎儿超声视频中的标准平面 Acquisition', 'title_zh': 'MCAT: 基于视觉查询的胎儿超声视频中标准解剖剪辑的多层类意识标记转换器定位'}
{'arxiv_id': 'arXiv:2504.05956', 'title': 'Temporal Alignment-Free Video Matching for Few-shot Action Recognition', 'authors': 'SuBeen Lee, WonJun Moon, Hyun Seok Seong, Jae-Pil Heo', 'link': 'https://arxiv.org/abs/2504.05956', 'abstract': 'Few-Shot Action Recognition (FSAR) aims to train a model with only a few labeled video instances. A key challenge in FSAR is handling divergent narrative trajectories for precise video matching. While the frame- and tuple-level alignment approaches have been promising, their methods heavily rely on pre-defined and length-dependent alignment units (e.g., frames or tuples), which limits flexibility for actions of varying lengths and speeds. In this work, we introduce a novel TEmporal Alignment-free Matching (TEAM) approach, which eliminates the need for temporal units in action representation and brute-force alignment during matching. Specifically, TEAM represents each video with a fixed set of pattern tokens that capture globally discriminative clues within the video instance regardless of action length or speed, ensuring its flexibility. Furthermore, TEAM is inherently efficient, using token-wise comparisons to measure similarity between videos, unlike existing methods that rely on pairwise comparisons for temporal alignment. Additionally, we propose an adaptation process that identifies and removes common information across classes, establishing clear boundaries even between novel categories. Extensive experiments demonstrate the effectiveness of TEAM. Codes are available at this http URL.', 'abstract_zh': '基于少样本动作识别的时空单元自由匹配（TEMPOREAL ALIGNMENT-FREE MATCHING FOR FEW-SHOT ACTION RECOGNITION）', 'title_zh': '面向少量样本动作识别的时序对齐自由视频匹配'}
{'arxiv_id': 'arXiv:2504.05882', 'title': 'Turin3D: Evaluating Adaptation Strategies under Label Scarcity in Urban LiDAR Segmentation with Semi-Supervised Techniques', 'authors': 'Luca Barco, Giacomo Blanco, Gaetano Chiriaco, Alessia Intini, Luigi La Riccia, Vittorio Scolamiero, Piero Boccardo, Paolo Garza, Fabrizio Dominici', 'link': 'https://arxiv.org/abs/2504.05882', 'abstract': '3D semantic segmentation plays a critical role in urban modelling, enabling detailed understanding and mapping of city environments. In this paper, we introduce Turin3D: a new aerial LiDAR dataset for point cloud semantic segmentation covering an area of around 1.43 km2 in the city centre of Turin with almost 70M points. We describe the data collection process and compare Turin3D with others previously proposed in the literature. We did not fully annotate the dataset due to the complexity and time-consuming nature of the process; however, a manual annotation process was performed on the validation and test sets, to enable a reliable evaluation of the proposed techniques. We first benchmark the performances of several point cloud semantic segmentation models, trained on the existing datasets, when tested on Turin3D, and then improve their performances by applying a semi-supervised learning technique leveraging the unlabelled training set. The dataset will be publicly available to support research in outdoor point cloud segmentation, with particular relevance for self-supervised and semi-supervised learning approaches given the absence of ground truth annotations for the training set.', 'abstract_zh': '3D语义分割在城市建模中发挥着关键作用， enables 对城市环境的详细理解与建模。在本文中，我们介绍 Turin3D：一个新的覆盖意大利都灵市中心约1.43 km²区域的机载LiDAR数据集，包含几乎7000万个点的点云语义分割数据集。我们描述了数据收集过程，并将Turin3D与文献中之前提出的数据集进行比较。由于注释过程的复杂性和耗时性，我们未对整个数据集进行注释；然而，我们在验证集和测试集上进行了手动注释，以确保对所提方法进行可靠评估。我们首先对标记在现有数据集上训练的几种点云语义分割模型在Turin3D上的性能进行基准测试，然后通过利用未标记训练集的半监督学习技术来提高其性能。该数据集将公开发布以支持室外点云分割研究，特别是对于自监督和半监督学习方法具有重要意义，因为训练集缺乏地面真实标注。', 'title_zh': 'Turin3D：在半监督技术下的稀疏标签条件下城市LiDAR分割适应策略评估'}
{'arxiv_id': 'arXiv:2504.05857', 'title': 'Towards an AI-Driven Video-Based American Sign Language Dictionary: Exploring Design and Usage Experience with Learners', 'authors': 'Saad Hassan, Matyas Bohacek, Chaelin Kim, Denise Crochet', 'link': 'https://arxiv.org/abs/2504.05857', 'abstract': 'Searching for unfamiliar American Sign Language (ASL) signs is challenging for learners because, unlike spoken languages, they cannot type a text-based query to look up an unfamiliar sign. Advances in isolated sign recognition have enabled the creation of video-based dictionaries, allowing users to submit a video and receive a list of the closest matching signs. Previous HCI research using Wizard-of-Oz prototypes has explored interface designs for ASL dictionaries. Building on these studies, we incorporate their design recommendations and leverage state-of-the-art sign-recognition technology to develop an automated video-based dictionary. We also present findings from an observational study with twelve novice ASL learners who used this dictionary during video-comprehension and question-answering tasks. Our results address human-AI interaction challenges not covered in previous WoZ research, including recording and resubmitting signs, unpredictable outputs, system latency, and privacy concerns. These insights offer guidance for designing and deploying video-based ASL dictionary systems.', 'abstract_zh': '探索不熟悉的美国手语（ASL）手势对学习者来说具有挑战性，因为他们无法像使用书面查询那样通过键盘查询不熟悉的手势。手语孤立手势识别的进步使得基于视频的手语词典得以创建，用户可以提交视频并收到最接近的手势列表。基于此，我们融合以往人机交互（HCI）研究中的界面设计建议，并利用最先进的手语识别技术开发了一个自动化的基于视频的手语词典。我们还介绍了十二位初级ASL学习者在视频理解和问答任务中使用该词典的观察研究结果。我们的研究结果解决了之前Wizard-of-Oz（巫师学徒）原型研究未涵盖的人机交互挑战，包括手势的录制与重新提交、不可预测的输出、系统延迟和隐私问题。这些见解为设计和部署基于视频的手语词典系统提供了指导。', 'title_zh': '面向AI驱动的基于视频的手语词典：探索使用者学习体验的设计与使用'}
{'arxiv_id': 'arXiv:2504.05852', 'title': 'Physics-aware generative models for turbulent fluid flows through energy-consistent stochastic interpolants', 'authors': 'Nikolaj T. Mücke, Benjamin Sanderse', 'link': 'https://arxiv.org/abs/2504.05852', 'abstract': 'Generative models have demonstrated remarkable success in domains such as text, image, and video synthesis. In this work, we explore the application of generative models to fluid dynamics, specifically for turbulence simulation, where classical numerical solvers are computationally expensive. We propose a novel stochastic generative model based on stochastic interpolants, which enables probabilistic forecasting while incorporating physical constraints such as energy stability and divergence-freeness. Unlike conventional stochastic generative models, which are often agnostic to underlying physical laws, our approach embeds energy consistency by making the parameters of the stochastic interpolant learnable coefficients. We evaluate our method on a benchmark turbulence problem - Kolmogorov flow - demonstrating superior accuracy and stability over state-of-the-art alternatives such as autoregressive conditional diffusion models (ACDMs) and PDE-Refiner. Furthermore, we achieve stable results for significantly longer roll-outs than standard stochastic interpolants. Our results highlight the potential of physics-aware generative models in accelerating and enhancing turbulence simulations while preserving fundamental conservation properties.', 'abstract_zh': '生成模型在文本、图像和视频合成等领域的应用已经显示出显著的成功。本文探索将生成模型应用于流体力学，特别是涡流模拟，其中经典数值求解器计算成本高。我们提出了一种基于随机插值的新型随机生成模型，该模型能够进行概率预测并纳入如能量稳定性与散度为零等物理约束。与通常对底层物理定律视而不见的常规随机生成模型不同，我们的方法通过使随机插值的参数成为可学习的系数，嵌入能量一致性。我们通过Kolmogorov流这一基准涡流问题评估了该方法，结果显示其在与自回归条件扩散模型（ACDMs）及PDE-Refiner等最先进的替代方法相比时具有更高的准确性和稳定性。此外，我们在远超标准随机插值的较长运行时间下获得了稳定的结果。我们的研究结果突显了物理感知生成模型在保持基本守恒性质的同时加速和增强涡流模拟的潜在价值。', 'title_zh': '物理感知的涡流流体生成模型通过能量一致的随机插值'}
{'arxiv_id': 'arXiv:2504.05783', 'title': 'Video Flow as Time Series: Discovering Temporal Consistency and Variability for VideoQA', 'authors': 'Zijie Song, Zhenzhen Hu, Yixiao Ma, Jia Li, Richang Hong', 'link': 'https://arxiv.org/abs/2504.05783', 'abstract': 'Video Question Answering (VideoQA) is a complex video-language task that demands a sophisticated understanding of both visual content and temporal dynamics. Traditional Transformer-style architectures, while effective in integrating multimodal data, often simplify temporal dynamics through positional encoding and fail to capture non-linear interactions within video sequences. In this paper, we introduce the Temporal Trio Transformer (T3T), a novel architecture that models time consistency and time variability. The T3T integrates three key components: Temporal Smoothing (TS), Temporal Difference (TD), and Temporal Fusion (TF). The TS module employs Brownian Bridge for capturing smooth, continuous temporal transitions, while the TD module identifies and encodes significant temporal variations and abrupt changes within the video content. Subsequently, the TF module synthesizes these temporal features with textual cues, facilitating a deeper contextual understanding and response accuracy. The efficacy of the T3T is demonstrated through extensive testing on multiple VideoQA benchmark datasets. Our results underscore the importance of a nuanced approach to temporal modeling in improving the accuracy and depth of video-based question answering.', 'abstract_zh': '视频问答（VideoQA）是一种复杂的时间-语言任务，要求对视觉内容和时间动态有精深的理解。传统的基于Transformer的架构虽然在整合多模态数据方面很有效，但往往通过位置编码简化了时间动态，无法捕获视频序列内的非线性交互。本文提出了一种新颖的时间三重Transformer（T3T）架构，用于建模时间和时间变异性。T3T集成了三个关键模块：时间平滑（TS）、时间差分（TD）和时间融合（TF）。TS模块采用布朗桥来捕捉平滑连续的时间过渡，TD模块识别并编码视频内容中的重要时间变化和突然变化，TF模块将这些时间特征与文本线索合成，促进更深入的语境理解和答案准确性。T3T的有效性通过在多个视频问答基准数据集上的广泛测试得到验证。我们的结果强调了在提高基于视频的问题回答准确性和深度方面精细化时间建模的重要性。', 'title_zh': '视频流作为时间序列：发现视频问答中的 temporal consistency 和 variability'}
{'arxiv_id': 'arXiv:2504.05774', 'title': 'Transferable Mask Transformer: Cross-domain Semantic Segmentation with Region-adaptive Transferability Estimation', 'authors': 'Enming Zhang, Zhengyu Li, Yanru Wu, Jingge Wang, Yang Tan, Ruizhe Zhao, Guan Wang, Yang Li', 'link': 'https://arxiv.org/abs/2504.05774', 'abstract': "Recent advances in Vision Transformers (ViTs) have set new benchmarks in semantic segmentation. However, when adapting pretrained ViTs to new target domains, significant performance degradation often occurs due to distribution shifts, resulting in suboptimal global attention. Since self-attention mechanisms are inherently data-driven, they may fail to effectively attend to key objects when source and target domains exhibit differences in texture, scale, or object co-occurrence patterns. While global and patch-level domain adaptation methods provide partial solutions, region-level adaptation with dynamically shaped regions is crucial due to spatial heterogeneity in transferability across different image areas. We present Transferable Mask Transformer (TMT), a novel region-level adaptation framework for semantic segmentation that aligns cross-domain representations through spatial transferability analysis. TMT consists of two key components: (1) An Adaptive Cluster-based Transferability Estimator (ACTE) that dynamically segments images into structurally and semantically coherent regions for localized transferability assessment, and (2) A Transferable Masked Attention (TMA) module that integrates region-specific transferability maps into ViTs' attention mechanisms, prioritizing adaptation in regions with low transferability and high semantic uncertainty. Comprehensive evaluations across 20 cross-domain pairs demonstrate TMT's superiority, achieving an average 2% MIoU improvement over vanilla fine-tuning and a 1.28% increase compared to state-of-the-art baselines. The source code will be publicly available.", 'abstract_zh': 'Recent Advances in Vision Transformers (ViTs)已在语义分割任务中设立了新基准。然而，在将预训练的ViTs应用于新的目标领域时，由于分布偏移，通常会出现显著的性能下降，导致全局注意效果欠佳。由于自注意力机制本质上是数据驱动的，当源领域和目标领域在纹理、尺度或物体共现模式上存在差异时，它们可能无法有效关注关键对象。虽然全局和 patch 级别领域适应方法提供了一部分解决方案，但在不同图像区域间转移性具有空间异质性的背景下，基于区域的动态形状区域适应至关重要。我们提出了可转移掩码变压器（TMT），这是一种新颖的基于区域的适应框架，通过空间转移性分析对跨域表示进行对齐。TMT 包含两个关键组件：（1）自适应集群基转移性估计器（ACTE），它可以动态地将图像分割成结构上和语义上一致的区域，以进行局部转移性评估；（2）可转移遮罩注意力（TMA）模块，该模块将区域特定的转移性图集成到 ViTs 的注意力机制中，在转移性低且语义不确定性高的区域优先进行适应。在20个跨域配对的全面评估中，TMT 展示了其优越性，平均MIoU提高2%，相比标准微调提高了1.28%，相比最新基线提高了1.28%。源代码将公开。', 'title_zh': '可转移掩码变换器：基于区域自适应转移性估计的跨域语义分割'}
{'arxiv_id': 'arXiv:2504.05770', 'title': 'A Lightweight Multi-Module Fusion Approach for Korean Character Recognition', 'authors': 'Inho Jake Park, Jaehoon Jay Jeong, Ho-Sang Jo', 'link': 'https://arxiv.org/abs/2504.05770', 'abstract': 'Optical Character Recognition (OCR) is essential in applications such as document processing, license plate recognition, and intelligent surveillance. However, existing OCR models often underperform in real-world scenarios due to irregular text layouts, poor image quality, character variability, and high computational costs.\nThis paper introduces SDA-Net (Stroke-Sensitive Attention and Dynamic Context Encoding Network), a lightweight and efficient architecture designed for robust single-character recognition. SDA-Net incorporates: (1) a Dual Attention Mechanism to enhance stroke-level and spatial feature extraction; (2) a Dynamic Context Encoding module that adaptively refines semantic information using a learnable gating mechanism; (3) a U-Net-inspired Feature Fusion Strategy for combining low-level and high-level features; and (4) a highly optimized lightweight backbone that reduces memory and computational demands.\nExperimental results show that SDA-Net achieves state-of-the-art accuracy on challenging OCR benchmarks, with significantly faster inference, making it well-suited for deployment in real-time and edge-based OCR systems.', 'abstract_zh': '基于笔画敏感注意力和动态上下文编码的轻量化OCR网络（SDA-Net）：面向实时和边缘端OCR系统的高效单字符识别', 'title_zh': '一种轻量级多模块融合方法用于韩文字识别'}
{'arxiv_id': 'arXiv:2504.05686', 'title': 'kNN-SVC: Robust Zero-Shot Singing Voice Conversion with Additive Synthesis and Concatenation Smoothness Optimization', 'authors': 'Keren Shao, Ke Chen, Matthew Baas, Shlomo Dubnov', 'link': 'https://arxiv.org/abs/2504.05686', 'abstract': "Robustness is critical in zero-shot singing voice conversion (SVC). This paper introduces two novel methods to strengthen the robustness of the kNN-VC framework for SVC. First, kNN-VC's core representation, WavLM, lacks harmonic emphasis, resulting in dull sounds and ringing artifacts. To address this, we leverage the bijection between WavLM, pitch contours, and spectrograms to perform additive synthesis, integrating the resulting waveform into the model to mitigate these issues. Second, kNN-VC overlooks concatenative smoothness, a key perceptual factor in SVC. To enhance smoothness, we propose a new distance metric that filters out unsuitable kNN candidates and optimize the summing weights of the candidates during inference. Although our techniques are built on the kNN-VC framework for implementation convenience, they are broadly applicable to general concatenative neural synthesis models. Experimental results validate the effectiveness of these modifications in achieving robust SVC. Demo: this http URL Code: this https URL", 'abstract_zh': '零样本唱歌语音转换的鲁棒性至关重要。本文介绍了两种新的方法以增强kNN-VC框架在唱歌语音转换（SVC）中的鲁棒性。首先，kNN-VC的核心表示WavLM缺乏谐波强调，导致声音单调且出现振铃效应。为解决此问题，我们利用WavLM、音高轮廓和频谱图之间的双射关系进行加法合成，将生成的波形整合到模型中以减轻这些问题。其次，kNN-VC忽视了连接平滑性，这是SVC中的关键感知因素。为了提高平滑性，我们提出了一种新的距离度量，该度量过滤出不合适的kNN候选者，并在推理过程中优化候选者的加权求和。尽管我们的技术基于kNN-VC框架以便于实现，但它们广泛适用于一般的连接合成神经网络模型。实验结果验证了这些修改在实现鲁棒SVC方面的有效性。Demo: this http URL Code: this https URL', 'title_zh': 'kNN-SVC：基于加性合成和拼接平滑优化的鲁棒零样本歌唱语音转换'}
{'arxiv_id': 'arXiv:2504.05537', 'title': 'Towards Efficient Real-Time Video Motion Transfer via Generative Time Series Modeling', 'authors': 'Tasmiah Haque, Md. Asif Bin Syed, Byungheon Jeong, Xue Bai, Sumit Mohan, Somdyuti Paul, Imtiaz Ahmed, Srinjoy Das', 'link': 'https://arxiv.org/abs/2504.05537', 'abstract': 'We propose a deep learning framework designed to significantly optimize bandwidth for motion-transfer-enabled video applications, including video conferencing, virtual reality interactions, health monitoring systems, and vision-based real-time anomaly detection. To capture complex motion effectively, we utilize the First Order Motion Model (FOMM), which encodes dynamic objects by detecting keypoints and their associated local affine transformations. These keypoints are identified using a self-supervised keypoint detector and arranged into a time series corresponding to the successive frames. Forecasting is performed on these keypoints by integrating two advanced generative time series models into the motion transfer pipeline, namely the Variational Recurrent Neural Network (VRNN) and the Gated Recurrent Unit with Normalizing Flow (GRU-NF). The predicted keypoints are subsequently synthesized into realistic video frames using an optical flow estimator paired with a generator network, thereby facilitating accurate video forecasting and enabling efficient, low-frame-rate video transmission. We validate our results across three datasets for video animation and reconstruction using the following metrics: Mean Absolute Error, Joint Embedding Predictive Architecture Embedding Distance, Structural Similarity Index, and Average Pair-wise Displacement. Our results confirm that by utilizing the superior reconstruction property of the Variational Autoencoder, the VRNN integrated FOMM excels in applications involving multi-step ahead forecasts such as video conferencing. On the other hand, by leveraging the Normalizing Flow architecture for exact likelihood estimation, and enabling efficient latent space sampling, the GRU-NF based FOMM exhibits superior capabilities for producing diverse future samples while maintaining high visual quality for tasks like real-time video-based anomaly detection.', 'abstract_zh': '我们提出了一种深度学习框架，旨在显著优化支持运动转移的视频应用中的带宽，包括视频会议、虚拟现实交互、健康监测系统和基于视觉的实时异常检测。该框架利用First Order Motion Model (FOMM) 捕捉复杂运动，通过检测关键点及其相关的局部仿射变换来编码动态对象。关键点通过自我监督的关键点检测器识别，并按顺序帧对应的时间序列进行排列。运动转移管道中整合了两种先进的生成时间序列模型——变分递归神经网络 (VRNN) 和带有归一化流的门控递归单元 (GRU-NF)——以预测关键点。预测的关键点随后通过光学流估计器与生成器网络相结合，合成出逼真的视频帧，从而实现准确的视频预测和高效的低帧率视频传输。我们使用以下指标在三个视频动画和重建数据集上验证了我们的结果：绝对平均误差、联合嵌入预测架构嵌入距离、结构相似性指数和平均成对位移。我们的结果证实，通过利用变分自编码器的优越重建特性，FOMM与VRNN整合在涉及多步预测的应用场景（如视频会议）中表现出色。另一方面，通过利用归一化流架构进行精确似然估计，并允许高效的潜在空间采样，基于GRU-NF的FOMM在生成多样化未来样本的同时保持高质量视觉效果，这在基于视频的实时异常检测任务中表现出色。', 'title_zh': '基于生成时间序列建模的高效实时视频运动转移'}
{'arxiv_id': 'arXiv:2504.05400', 'title': 'GARF: Learning Generalizable 3D Reassembly for Real-World Fractures', 'authors': 'Sihang Li, Zeyu Jiang, Grace Chen, Chenyang Xu, Siqi Tan, Xue Wang, Irving Fang, Kristof Zyskowski, Shannon P. McPherron, Radu Iovita, Chen Feng, Jing Zhang', 'link': 'https://arxiv.org/abs/2504.05400', 'abstract': '3D reassembly is a challenging spatial intelligence task with broad applications across scientific domains. While large-scale synthetic datasets have fueled promising learning-based approaches, their generalizability to different domains is limited. Critically, it remains uncertain whether models trained on synthetic datasets can generalize to real-world fractures where breakage patterns are more complex. To bridge this gap, we propose GARF, a generalizable 3D reassembly framework for real-world fractures. GARF leverages fracture-aware pretraining to learn fracture features from individual fragments, with flow matching enabling precise 6-DoF alignments. At inference time, we introduce one-step preassembly, improving robustness to unseen objects and varying numbers of fractures. In collaboration with archaeologists, paleoanthropologists, and ornithologists, we curate Fractura, a diverse dataset for vision and learning communities, featuring real-world fracture types across ceramics, bones, eggshells, and lithics. Comprehensive experiments have shown our approach consistently outperforms state-of-the-art methods on both synthetic and real-world datasets, achieving 82.87\\% lower rotation error and 25.15\\% higher part accuracy. This sheds light on training on synthetic data to advance real-world 3D puzzle solving, demonstrating its strong generalization across unseen object shapes and diverse fracture types.', 'abstract_zh': '可泛化的3D重构框架：面向真实世界裂痕的 GARF', 'title_zh': 'GARF: 学习可泛化的三维复原以应对真实世界的断裂'}
{'arxiv_id': 'arXiv:2504.05356', 'title': 'DyTTP: Trajectory Prediction with Normalization-Free Transformers', 'authors': 'Yunxiang Liu, Hongkuo Niu', 'link': 'https://arxiv.org/abs/2504.05356', 'abstract': 'Accurate trajectory prediction is a cornerstone for the safe operation of autonomous driving systems, where understanding the dynamic behavior of surrounding agents is crucial. Transformer-based architectures have demonstrated significant promise in capturing complex spatio-temporality dependencies. However, their reliance on normalization layers can lead to computation overhead and training instabilities. In this work, we present a two-fold approach to address these challenges. First, we integrate DynamicTanh (DyT), which is the latest method to promote transformers, into the backbone, replacing traditional layer normalization. This modification simplifies the network architecture and improves the stability of the inference. We are the first work to deploy the DyT to the trajectory prediction task. Complementing this, we employ a snapshot ensemble strategy to further boost trajectory prediction performance. Using cyclical learning rate scheduling, multiple model snapshots are captured during a single training run. These snapshots are then aggregated via simple averaging at inference time, allowing the model to benefit from diverse hypotheses without incurring substantial additional computational cost. Extensive experiments on Argoverse datasets demonstrate that our combined approach significantly improves prediction accuracy, inference speed and robustness in diverse driving scenarios. This work underscores the potential of normalization-free transformer designs augmented with lightweight ensemble techniques in advancing trajectory forecasting for autonomous vehicles.', 'abstract_zh': '基于DynamicTanh和轻量级集成技术的无归一化层变换器在自动驾驶轨迹预测中的应用', 'title_zh': 'DyTTP：无需归一化变换器的轨迹预测'}
{'arxiv_id': 'arXiv:2504.05344', 'title': 'Divergent Paths: Separating Homophilic and Heterophilic Learning for Enhanced Graph-level Representations', 'authors': 'Han Lei, Jiaxing Xu, Xia Dong, Yiping Ke', 'link': 'https://arxiv.org/abs/2504.05344', 'abstract': "Graph Convolutional Networks (GCNs) are predominantly tailored for graphs displaying homophily, where similar nodes connect, but often fail on heterophilic graphs. The strategy of adopting distinct approaches to learn from homophilic and heterophilic components in node-level tasks has been widely discussed and proven effective both theoretically and experimentally. However, in graph-level tasks, research on this topic remains notably scarce. Addressing this gap, our research conducts an analysis on graphs with nodes' category ID available, distinguishing intra-category and inter-category components as embodiment of homophily and heterophily, respectively. We find while GCNs excel at extracting information within categories, they frequently capture noise from inter-category components. Consequently, it is crucial to employ distinct learning strategies for intra- and inter-category elements. To alleviate this problem, we separately learn the intra- and inter-category parts by a combination of an intra-category convolution (IntraNet) and an inter-category high-pass graph convolution (InterNet). Our IntraNet is supported by sophisticated graph preprocessing steps and a novel category-based graph readout function. For the InterNet, we utilize a high-pass filter to amplify the node disparities, enhancing the recognition of details in the high-frequency components. The proposed approach, DivGNN, combines the IntraNet and InterNet with a gated mechanism and substantially improves classification performance on graph-level tasks, surpassing traditional GNN baselines in effectiveness.", 'abstract_zh': '基于区分内类间和跨类别的图卷积网络（DivGNN）研究', 'title_zh': '分歧的道路：分离同ophilic和异ophilic学习以增强图级表示'}
{'arxiv_id': 'arXiv:2504.05316', 'title': 'Scale Up Composed Image Retrieval Learning via Modification Text Generation', 'authors': 'Yinan Zhou, Yaxiong Wang, Haokun Lin, Chen Ma, Li Zhu, Zhedong Zheng', 'link': 'https://arxiv.org/abs/2504.05316', 'abstract': 'Composed Image Retrieval (CIR) aims to search an image of interest using a combination of a reference image and modification text as the query. Despite recent advancements, this task remains challenging due to limited training data and laborious triplet annotation processes. To address this issue, this paper proposes to synthesize the training triplets to augment the training resource for the CIR problem. Specifically, we commence by training a modification text generator exploiting large-scale multimodal models and scale up the CIR learning throughout both the pretraining and fine-tuning stages. During pretraining, we leverage the trained generator to directly create Modification Text-oriented Synthetic Triplets(MTST) conditioned on pairs of images. For fine-tuning, we first synthesize reverse modification text to connect the target image back to the reference image. Subsequently, we devise a two-hop alignment strategy to incrementally close the semantic gap between the multimodal pair and the target image. We initially learn an implicit prototype utilizing both the original triplet and its reversed version in a cycle manner, followed by combining the implicit prototype feature with the modification text to facilitate accurate alignment with the target image. Extensive experiments validate the efficacy of the generated triplets and confirm that our proposed methodology attains competitive recall on both the CIRR and FashionIQ benchmarks.', 'abstract_zh': '组成图像检索（CIR）旨在使用参考图像和修改文本的组合作为查询来搜索感兴趣的图像。尽管近期取得了进展，但由于训练数据有限和三元组注释过程繁琐，这一任务仍然具有挑战性。为了解决这一问题，本文提出合成训练三元组以增强CIR问题的训练资源。具体而言，我们首先利用大型多模态模型训练一个修改文本生成器，并在整个预训练和微调阶段扩展CIR学习。在预训练阶段，我们利用训练好的生成器直接根据图像对生成面向修改文本的合成三元组（MTST）。在微调阶段，我们首先合成逆向修改文本将目标图像连接回参考图像。随后，我们设计了一种两阶段对齐策略逐步缩小多模态对和目标图像之间的语义差距。我们首先以循环方式利用原三元组及其逆向版本学习一个隐式原型，随后将隐式原型特征与修改文本结合以促进与目标图像的准确对齐。广泛的实验验证了生成三元组的有效性，并证实我们提出的方法在CIRR和FashionIQ基准上取得了竞争力的召回率。', 'title_zh': '通过修改文本生成扩展组合图像检索学习'}
