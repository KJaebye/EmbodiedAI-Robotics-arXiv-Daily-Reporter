{'arxiv_id': 'arXiv:2508.19236', 'title': 'MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation', 'authors': 'Hao Shi, Bin Xie, Yingfei Liu, Lin Sun, Fengrong Liu, Tiancai Wang, Erjin Zhou, Haoqiang Fan, Xiangyu Zhang, Gao Huang', 'link': 'https://arxiv.org/abs/2508.19236', 'abstract': 'Temporal context is essential for robotic manipulation because such tasks are inherently non-Markovian, yet mainstream VLA models typically overlook it and struggle with long-horizon, temporally dependent tasks. Cognitive science suggests that humans rely on working memory to buffer short-lived representations for immediate control, while the hippocampal system preserves verbatim episodic details and semantic gist of past experience for long-term memory. Inspired by these mechanisms, we propose MemoryVLA, a Cognition-Memory-Action framework for long-horizon robotic manipulation. A pretrained VLM encodes the observation into perceptual and cognitive tokens that form working memory, while a Perceptual-Cognitive Memory Bank stores low-level details and high-level semantics consolidated from it. Working memory retrieves decision-relevant entries from the bank, adaptively fuses them with current tokens, and updates the bank by merging redundancies. Using these tokens, a memory-conditioned diffusion action expert yields temporally aware action sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasks across three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, it achieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperforming state-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain on Bridge. On 12 real-world tasks spanning general skills and long-horizon temporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizon tasks showing a +26 improvement over state-of-the-art baseline. Project Page: this https URL', 'abstract_zh': '基于记忆的认知-记忆-行动框架：长期_horizon类机器manipulation方法', 'title_zh': 'MemoryVLA：视觉-语言-动作模型中的感知认知记忆及其在机器人操作中的应用'}
{'arxiv_id': 'arXiv:2508.19199', 'title': 'Planning-Query-Guided Model Generation for Model-Based Deformable Object Manipulation', 'authors': 'Alex LaGrassa, Zixuan Huang, Dmitry Berenson, Oliver Kroemer', 'link': 'https://arxiv.org/abs/2508.19199', 'abstract': 'Efficient planning in high-dimensional spaces, such as those involving deformable objects, requires computationally tractable yet sufficiently expressive dynamics models. This paper introduces a method that automatically generates task-specific, spatially adaptive dynamics models by learning which regions of the object require high-resolution modeling to achieve good task performance for a given planning query. Task performance depends on the complex interplay between the dynamics model, world dynamics, control, and task requirements. Our proposed diffusion-based model generator predicts per-region model resolutions based on start and goal pointclouds that define the planning query. To efficiently collect the data for learning this mapping, a two-stage process optimizes resolution using predictive dynamics as a prior before directly optimizing using closed-loop performance. On a tree-manipulation task, our method doubles planning speed with only a small decrease in task performance over using a full-resolution model. This approach informs a path towards using previous planning and control data to generate computationally efficient yet sufficiently expressive dynamics models for new tasks.', 'abstract_zh': '高效规划在高维空间中，特别是在涉及变形对象的空间中，需要具备在计算上可处理且足够表达力的动力学模型。本文介绍了一种方法，该方法通过学习哪些对象区域需要高分辨率建模来自动生成任务特定的空间自适应动力学模型，以实现给定规划查询的良好任务性能。任务性能取决于动力学模型、世界动力学、控制和任务需求之间的复杂相互作用。我们提出的基于扩散的模型生成器根据定义规划查询的起始和目标点云来预测每个区域模型的分辨率。为了高效地收集用于学习这种映射的数据，一个两阶段过程使用预测动力学作为先验来优化分辨率，然后再直接使用闭环性能进行优化。在树操作任务中，该方法在任务性能略有下降的情况下将规划速度提高了一倍。该方法为利用先前的规划和控制数据生成适用于新任务的计算上高效且足够表达的动力学模型指出了可能的路径。', 'title_zh': '基于规划-查询引导的模型生成方法在变形对象操作中的应用'}
{'arxiv_id': 'arXiv:2508.19191', 'title': 'AutoRing: Imitation Learning--based Autonomous Intraocular Foreign Body Removal Manipulation with Eye Surgical Robot', 'authors': 'Yue Wang, Wenjie Deng, Haotian Xue, Di Cui, Yiqi Chen, Mingchuan Zhou, Haochao Ying, Jian Wu', 'link': 'https://arxiv.org/abs/2508.19191', 'abstract': 'Intraocular foreign body removal demands millimeter-level precision in confined intraocular spaces, yet existing robotic systems predominantly rely on manual teleoperation with steep learning curves. To address the challenges of autonomous manipulation (particularly kinematic uncertainties from variable motion scaling and variation of the Remote Center of Motion (RCM) point), we propose AutoRing, an imitation learning framework for autonomous intraocular foreign body ring manipulation. Our approach integrates dynamic RCM calibration to resolve coordinate-system inconsistencies caused by intraocular instrument variation and introduces the RCM-ACT architecture, which combines action-chunking transformers with real-time kinematic realignment. Trained solely on stereo visual data and instrument kinematics from expert demonstrations in a biomimetic eye model, AutoRing successfully completes ring grasping and positioning tasks without explicit depth sensing. Experimental validation demonstrates end-to-end autonomy under uncalibrated microscopy conditions. The results provide a viable framework for developing intelligent eye-surgical systems capable of complex intraocular procedures.', 'abstract_zh': '基于模仿学习的自主眼内异物环 manip 框架', 'title_zh': 'AutoRing: 基于 imitation learning 的自主眼球内foreign body移除操作的眼科手术机器人'}
{'arxiv_id': 'arXiv:2508.19186', 'title': 'Real-Time Model Checking for Closed-Loop Robot Reactive Planning', 'authors': 'Christopher Chandler, Bernd Porr, Giulia Lafratta, Alice Miller', 'link': 'https://arxiv.org/abs/2508.19186', 'abstract': 'We present a new application of model checking which achieves real-time multi-step planning and obstacle avoidance on a real autonomous robot. We have developed a small, purpose-built model checking algorithm which generates plans in situ based on "core" knowledge and attention as found in biological agents. This is achieved in real-time using no pre-computed data on a low-powered device. Our approach is based on chaining temporary control systems which are spawned to counteract disturbances in the local environment that disrupt an autonomous agent from its preferred action (or resting state). A novel discretization of 2D LiDAR data sensitive to bounded variations in the local environment is used. Multi-step planning using model checking by forward depth-first search is applied to cul-de-sac and playground scenarios. Both empirical results and informal proofs of two fundamental properties of our approach demonstrate that model checking can be used to create efficient multi-step plans for local obstacle avoidance, improving on the performance of a reactive agent which can only plan one step. Our approach is an instructional case study for the development of safe, reliable and explainable planning in the context of autonomous vehicles.', 'abstract_zh': '我们提出了一种模型检查的新应用，实现了实时多步规划和障碍避免于一台真实的自主机器人上。我们开发了一种小型的专业模型检查算法，基于生物体的知识和注意力动态生成计划。该过程在低功耗设备上实时进行，无需预先计算数据。我们的方法基于链接临时控制系统，这些系统孵化以抵消扰乱自主体正常行动（或休息状态）的局部环境干扰。我们使用对局部环境有界变化敏感的2D LiDAR数据的新型离散化方式。通过模型检查结合前向深度优先搜索进行多步规划，应用于死端和游乐场场景。我们方法的实证结果和对两个基本性质的非形式证明表明，模型检查可用于创建有效的多步本地障碍避免计划，超越了仅能进行单步规划的反应式代理的表现。我们的方法是自主车辆规划领域开发安全、可靠和可解释规划的一个教学案例。', 'title_zh': '闭环机器人反应式规划的实时模型检查'}
{'arxiv_id': 'arXiv:2508.19172', 'title': 'From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity', 'authors': 'Luca Grillotti, Lisa Coiffard, Oscar Pang, Maxence Faldor, Antoine Cully', 'link': 'https://arxiv.org/abs/2508.19172', 'abstract': 'Autonomous skill discovery aims to enable robots to acquire diverse behaviors without explicit supervision. Learning such behaviors directly on physical hardware remains challenging due to safety and data efficiency constraints. Existing methods, including Quality-Diversity Actor-Critic (QDAC), require manually defined skill spaces and carefully tuned heuristics, limiting real-world applicability. We propose Unsupervised Real-world Skill Acquisition (URSA), an extension of QDAC that enables robots to autonomously discover and master diverse, high-performing skills directly in the real world. We demonstrate that URSA successfully discovers diverse locomotion skills on a Unitree A1 quadruped in both simulation and the real world. Our approach supports both heuristic-driven skill discovery and fully unsupervised settings. We also show that the learned skill repertoire can be reused for downstream tasks such as real-world damage adaptation, where URSA outperforms all baselines in 5 out of 9 simulated and 3 out of 5 real-world damage scenarios. Our results establish a new framework for real-world robot learning that enables continuous skill discovery with limited human intervention, representing a significant step toward more autonomous and adaptable robotic systems. Demonstration videos are available at this http URL .', 'abstract_zh': '自主技能发现旨在使机器人能够在无显式监督的情况下获得多样化的行为。直接在物理硬件上学习此类行为由于安全性和数据效率的限制仍然具有挑战性。现有方法，包括质量-多样性演员-评论家（QDAC），需要手动定义技能空间和仔细调谐的启发式方法，限制其实用性。我们提出无监督的现实世界技能获取（URSA），这是QDAC的一个扩展，使机器人能够在现实世界中自主发现和掌握多样化的高性能技能。我们在模拟和现实世界中均证明URSA成功发现了Unitree A1四足机器人的多样运动技能。我们的方法支持启发式驱动的技能发现和完全无监督的设置。我们还展示了所学技能集合可以重用于下游任务，例如现实世界中的损伤适应，在9个模拟和5个实际损伤场景中有5个场景中，URSA均优于所有基线。我们的结果建立了一种新的现实世界机器人学习框架，使有限的人工干预下能够实现持续技能发现，这代表了向更加自主和适应性强的机器人系统迈出的重要一步。演示视频可在以下网址查看。', 'title_zh': '从白板到涌现能力：通过真实世界无监督品质多样性发现机器人技能'}
{'arxiv_id': 'arXiv:2508.19168', 'title': 'Direction Informed Trees (DIT*): Optimal Path Planning via Direction Filter and Direction Cost Heuristic', 'authors': 'Liding Zhang, Kejia Chen, Kuanqi Cai, Yu Zhang, Yixuan Dang, Yansong Wu, Zhenshan Bing, Fan Wu, Sami Haddadin, Alois Knoll', 'link': 'https://arxiv.org/abs/2508.19168', 'abstract': 'Optimal path planning requires finding a series of feasible states from the starting point to the goal to optimize objectives. Popular path planning algorithms, such as Effort Informed Trees (EIT*), employ effort heuristics to guide the search. Effective heuristics are accurate and computationally efficient, but achieving both can be challenging due to their conflicting nature. This paper proposes Direction Informed Trees (DIT*), a sampling-based planner that focuses on optimizing the search direction for each edge, resulting in goal bias during exploration. We define edges as generalized vectors and integrate similarity indexes to establish a directional filter that selects the nearest neighbors and estimates direction costs. The estimated direction cost heuristics are utilized in edge evaluation. This strategy allows the exploration to share directional information efficiently. DIT* convergence faster than existing single-query, sampling-based planners on tested problems in R^4 to R^16 and has been demonstrated in real-world environments with various planning tasks. A video showcasing our experimental results is available at: this https URL', 'abstract_zh': '基于方向指导树的最优路径规划：一种面向搜索方向优化的采样基于规划算法', 'title_zh': '方向导向树(DIT*): 基于方向过滤和方向成本启发式的最优路径规划'}
{'arxiv_id': 'arXiv:2508.19164', 'title': 'Real-time Testing of Satellite Attitude Control With a Reaction Wheel Hardware-In-the-Loop Platform', 'authors': 'Morokot Sakal, George Nehma, Camilo Riano-Rios, Madhur Tiwari', 'link': 'https://arxiv.org/abs/2508.19164', 'abstract': 'We propose the Hardware-in-the-Loop (HIL) test of an adaptive satellite attitude control system with reaction wheel health estimation capabilities. Previous simulations and Software-in-the-Loop testing have prompted further experiments to explore the validity of the controller with real momentum exchange devices in the loop. This work is a step toward a comprehensive testing framework for validation of spacecraft attitude control algorithms. The proposed HIL testbed includes brushless DC motors and drivers that communicate using a CAN bus, an embedded computer that executes control and adaptation laws, and a satellite simulator that produces simulated sensor data, estimated attitude states, and responds to actions of the external actuators. We propose methods to artificially induce failures on the reaction wheels, and present related issues and lessons learned.', 'abstract_zh': '硬件在环测试环境下自适应卫星姿态控制系统及飞轮健康估计能力的研究', 'title_zh': '基于反应轮硬件在环平台的实时卫星姿态控制测试'}
{'arxiv_id': 'arXiv:2508.19153', 'title': 'QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning', 'authors': 'Allen Wang, Gavin Tao', 'link': 'https://arxiv.org/abs/2508.19153', 'abstract': 'We address vision-guided quadruped motion control with reinforcement learning (RL) and highlight the necessity of combining proprioception with vision for robust control. We propose QuadKAN, a spline-parameterized cross-modal policy instantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates a spline encoder for proprioception and a spline fusion head for proprioception-vision inputs. This structured function class aligns the state-to-action mapping with the piecewise-smooth nature of gait, improving sample efficiency, reducing action jitter and energy consumption, and providing interpretable posture-action sensitivities. We adopt Multi-Modal Delay Randomization (MMDR) and perform end-to-end training with Proximal Policy Optimization (PPO). Evaluations across diverse terrains, including both even and uneven surfaces and scenarios with static or dynamic obstacles, demonstrate that QuadKAN achieves consistently higher returns, greater distances, and fewer collisions than state-of-the-art (SOTA) baselines. These results show that spline-parameterized policies offer a simple, effective, and interpretable alternative for robust vision-guided locomotion. A repository will be made available upon acceptance.', 'abstract_zh': '基于视觉的 quadruped 运动控制with强化学习：结合本体感觉和视觉的重要性及 QuadKAN 方法', 'title_zh': 'QuadKAN: 通过端到端强化学习增强的四足运动控制'}
{'arxiv_id': 'arXiv:2508.19150', 'title': 'Uncertainty-Resilient Active Intention Recognition for Robotic Assistants', 'authors': 'Juan Carlos Saborío, Marc Vinci, Oscar Lima, Sebastian Stock, Lennart Niecksch, Martin Günther, Alexander Sung, Joachim Hertzberg, Martin Atzmüller', 'link': 'https://arxiv.org/abs/2508.19150', 'abstract': 'Purposeful behavior in robotic assistants requires the integration of multiple components and technological advances. Often, the problem is reduced to recognizing explicit prompts, which limits autonomy, or is oversimplified through assumptions such as near-perfect information. We argue that a critical gap remains unaddressed -- specifically, the challenge of reasoning about the uncertain outcomes and perception errors inherent to human intention recognition. In response, we present a framework designed to be resilient to uncertainty and sensor noise, integrating real-time sensor data with a combination of planners. Centered around an intention-recognition POMDP, our approach addresses cooperative planning and acting under uncertainty. Our integrated framework has been successfully tested on a physical robot with promising results.', 'abstract_zh': '具有目的性的机器人助手行为需要多个组件和技术的进步集成。通常，问题被简化为识别显式提示，这限制了自主性，或者通过近乎完美的信息假设来简化。我们认为，仍然存在一个关键的未解决问题，即人类意图识别中固有的不确定性结果和感知错误的推理挑战。为此，我们提出了一种框架，该框架能够抵抗不确定性并整合传感器噪声，将实时传感器数据与多种规划者相结合。围绕意图识别部分可观测马尔可夫决策过程（POMDP），我们的方法解决了在不确定性下的协同规划和执行问题。我们集成的框架在物理机器人上进行了成功的测试，取得了令人鼓舞的结果。', 'title_zh': '具有抗不确定性的主动意图识别技术在机器人助手中的应用'}
{'arxiv_id': 'arXiv:2508.19131', 'title': 'ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown Environments', 'authors': 'Shreya Gummadi, Mateus V. Gasparino, Gianluca Capezzuto, Marcelo Becker, Girish Chowdhary', 'link': 'https://arxiv.org/abs/2508.19131', 'abstract': 'The advancement of robotics and autonomous navigation systems hinges on the ability to accurately predict terrain traversability. Traditional methods for generating datasets to train these prediction models often involve putting robots into potentially hazardous environments, posing risks to equipment and safety. To solve this problem, we present ZeST, a novel approach leveraging visual reasoning capabilities of Large Language Models (LLMs) to create a traversability map in real-time without exposing robots to danger. Our approach not only performs zero-shot traversability and mitigates the risks associated with real-world data collection but also accelerates the development of advanced navigation systems, offering a cost-effective and scalable solution. To support our findings, we present navigation results, in both controlled indoor and unstructured outdoor environments. As shown in the experiments, our method provides safer navigation when compared to other state-of-the-art methods, constantly reaching the final goal.', 'abstract_zh': '基于大规模语言模型的视觉推理实现实时无风险地形可通行性预测', 'title_zh': 'ZeST：一种基于大规模语言模型的零样本未知环境可通行性导航'}
{'arxiv_id': 'arXiv:2508.19114', 'title': 'DELIVER: A System for LLM-Guided Coordinated Multi-Robot Pickup and Delivery using Voronoi-Based Relay Planning', 'authors': 'Alkesh K. Srivastava, Jared Michael Levin, Alexander Derrico, Philip Dames', 'link': 'https://arxiv.org/abs/2508.19114', 'abstract': "We present DELIVER (Directed Execution of Language-instructed Item Via Engineered Relay), a fully integrated framework for cooperative multi-robot pickup and delivery driven by natural language commands. DELIVER unifies natural language understanding, spatial decomposition, relay planning, and motion execution to enable scalable, collision-free coordination in real-world settings. Given a spoken or written instruction, a lightweight instance of LLaMA3 interprets the command to extract pickup and delivery locations. The environment is partitioned using a Voronoi tessellation to define robot-specific operating regions. Robots then compute optimal relay points along shared boundaries and coordinate handoffs. A finite-state machine governs each robot's behavior, enabling robust execution. We implement DELIVER on the MultiTRAIL simulation platform and validate it in both ROS2-based Gazebo simulations and real-world hardware using TurtleBot3 robots. Empirical results show that DELIVER maintains consistent mission cost across varying team sizes while reducing per-agent workload by up to 55% compared to a single-agent system. Moreover, the number of active relay agents remains low even as team size increases, demonstrating the system's scalability and efficient agent utilization. These findings underscore DELIVER's modular and extensible architecture for language-guided multi-robot coordination, advancing the frontiers of cyber-physical system integration.", 'abstract_zh': '基于工程化中继的定向执行语言指示项的框架：DELIVER多机器人拣取与配送协作系统', 'title_zh': 'DELIVER：一种基于 Voronoi 辅助接力规划的大型语言模型引导协同多机器人拣取与配送系统'}
{'arxiv_id': 'arXiv:2508.19074', 'title': 'An LLM-powered Natural-to-Robotic Language Translation Framework with Correctness Guarantees', 'authors': 'ZhenDong Chen, ZhanShang Nie, ShiXing Wan, JunYi Li, YongTian Cheng, Shuai Zhao', 'link': 'https://arxiv.org/abs/2508.19074', 'abstract': 'The Large Language Models (LLM) are increasingly being deployed in robotics to generate robot control programs for specific user tasks, enabling embodied intelligence. Existing methods primarily focus on LLM training and prompt design that utilize LLMs to generate executable programs directly from user tasks in natural language. However, due to the inconsistency of the LLMs and the high complexity of the tasks, such best-effort approaches often lead to tremendous programming errors in the generated code, which significantly undermines the effectiveness especially when the light-weight LLMs are applied. This paper introduces a natural-robotic language translation framework that (i) provides correctness verification for generated control programs and (ii) enhances the performance of LLMs in program generation via feedback-based fine-tuning for the programs. To achieve this, a Robot Skill Language (RSL) is proposed to abstract away from the intricate details of the control programs, bridging the natural language tasks with the underlying robot skills. Then, the RSL compiler and debugger are constructed to verify RSL programs generated by the LLM and provide error feedback to the LLM for refining the outputs until being verified by the compiler. This provides correctness guarantees for the LLM-generated programs before being offloaded to the robots for execution, significantly enhancing the effectiveness of LLM-powered robotic applications. Experiments demonstrate NRTrans outperforms the existing method under a range of LLMs and tasks, and achieves a high success rate for light-weight LLMs.', 'abstract_zh': '大规模语言模型（LLM）在机器人领域的应用：一种自然-机器人语言翻译框架', 'title_zh': '基于LLM的具有正确性保证的自然语言到机器人语言转换框架'}
{'arxiv_id': 'arXiv:2508.19002', 'title': 'HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots', 'authors': 'Shipeng Lyu, Fangyuan Wang, Weiwei Lin, Luhao Zhu, David Navarro-Alarcon, Guodong Guo', 'link': 'https://arxiv.org/abs/2508.19002', 'abstract': 'Achieving both behavioral similarity and appropriateness in human-like motion generation for humanoid robot remains an open challenge, further compounded by the lack of cross-embodiment adaptability. To address this problem, we propose HuBE, a bi-level closed-loop framework that integrates robot state, goal poses, and contextual situations to generate human-like behaviors, ensuring both behavioral similarity and appropriateness, and eliminating structural mismatches between motion generation and execution. To support this framework, we construct HPose, a context-enriched dataset featuring fine-grained situational annotations. Furthermore, we introduce a bone scaling-based data augmentation strategy that ensures millimeter-level compatibility across heterogeneous humanoid robots. Comprehensive evaluations on multiple commercial platforms demonstrate that HuBE significantly improves motion similarity, behavioral appropriateness, and computational efficiency over state-of-the-art baselines, establishing a solid foundation for transferable and human-like behavior execution across diverse humanoid robots.', 'abstract_zh': '实现类人机器人动作生成中行为相似性与适宜性的兼得仍是一项开放性挑战，进一步受到跨实体适应性不足的制约。为应对这一问题，我们提出了一种两层闭环框架HuBE，该框架整合了机器人状态、目标姿态和情境信息，以生成类人行为，确保行为相似性和适宜性，并消除动作生成与执行之间的结构不匹配。为支持该框架，我们构建了HPose，这是一个富含细粒度情境注释的数据集。此外，我们引入了一种基于骨骼比例的数据增强策略，确保不同种类类人机器人在毫米级上的兼容性。在多个商业平台上的全面评估表明，HuBE在动作相似性、行为适宜性和计算效率方面显著优于最新基线方法，为跨多种类人机器人的可迁移和类人行为执行奠定了坚实基础。', 'title_zh': 'HuBE: 具有类人行为执行的人形机器人跨形态人类行为处理'}
{'arxiv_id': 'arXiv:2508.18967', 'title': 'Enhanced UAV Path Planning Using the Tangent Intersection Guidance (TIG) Algorithm', 'authors': 'Hichem Cheriet, Khellat Kihel Badra, Chouraqui Samira', 'link': 'https://arxiv.org/abs/2508.18967', 'abstract': 'Efficient and safe navigation of Unmanned Aerial Vehicles (UAVs) is critical for various applications, including combat support, package delivery and Search and Rescue Operations. This paper introduces the Tangent Intersection Guidance (TIG) algorithm, an advanced approach for UAV path planning in both static and dynamic environments. The algorithm uses the elliptic tangent intersection method to generate feasible paths. It generates two sub-paths for each threat, selects the optimal route based on a heuristic rule, and iteratively refines the path until the target is reached. Considering the UAV kinematic and dynamic constraints, a modified smoothing technique based on quadratic Bézier curves is adopted to generate a smooth and efficient route. Experimental results show that the TIG algorithm can generate the shortest path in less time, starting from 0.01 seconds, with fewer turning angles compared to A*, PRM, RRT*, Tangent Graph, and Static APPATT algorithms in static environments. Furthermore, in completely unknown and partially known environments, TIG demonstrates efficient real-time path planning capabilities for collision avoidance, outperforming APF and Dynamic APPATT algorithms.', 'abstract_zh': '无人驾驶航空器（UAV）的高效和安全导航对于多种应用至关重要，包括战斗支持、包裹递送和搜救作业。本文介绍了切线交点引导（TIG）算法，这是一种适用于静态和动态环境的UAV路径规划的先进方法。该算法采用椭圆切线交点法生成可行路径，为每个威胁生成两条子路径，基于启发式规则选择最优路径，并迭代细化路径直至目标点。考虑到UAV运动和动力学约束，采用基于二次Bézier曲线的修改平滑技术生成平滑高效的路径。实验结果显示，TIG算法能够在更短的时间内生成更短路径，从0.01秒开始，并且与A*、PRM、RRT*、切线图和静态APPATT算法相比，转弯角度更少。此外，在完全未知和部分未知环境中，TIG展现了高效的实时路径规划能力，用于碰撞避免，优于APF和动态APPATT算法。', 'title_zh': '使用切线交点引导（TIG）算法增强的无人机路径规划'}
{'arxiv_id': 'arXiv:2508.18937', 'title': 'VisionSafeEnhanced VPC: Cautious Predictive Control with Visibility Constraints under Uncertainty for Autonomous Robotic Surgery', 'authors': 'Wang Jiayin, Wei Yanran, Jiang Lei, Guo Xiaoyu, Zheng Ayong, Zhao Weidong, Li Zhongkui', 'link': 'https://arxiv.org/abs/2508.18937', 'abstract': "Autonomous control of the laparoscope in robot-assisted Minimally Invasive Surgery (MIS) has received considerable research interest due to its potential to improve surgical safety. Despite progress in pixel-level Image-Based Visual Servoing (IBVS) control, the requirement of continuous visibility and the existence of complex disturbances, such as parameterization error, measurement noise, and uncertainties of payloads, could degrade the surgeon's visual experience and compromise procedural safety. To address these limitations, this paper proposes VisionSafeEnhanced Visual Predictive Control (VPC), a robust and uncertainty-adaptive framework for autonomous laparoscope control that guarantees Field of View (FoV) safety under uncertainty. Firstly, Gaussian Process Regression (GPR) is utilized to perform hybrid (deterministic + stochastic) quantification of operational uncertainties including residual model uncertainties, stochastic uncertainties, and external disturbances. Based on uncertainty quantification, a novel safety aware trajectory optimization framework with probabilistic guarantees is proposed, where a uncertainty-adaptive safety Control Barrier Function (CBF) condition is given based on uncertainty propagation, and chance constraints are simultaneously formulated based on probabilistic approximation. This uncertainty aware formulation enables adaptive control effort allocation, minimizing unnecessary camera motion while maintaining robustness. The proposed method is validated through comparative simulations and experiments on a commercial surgical robot platform (MicroPort MedBot Toumai) performing a sequential multi-target lymph node dissection. Compared with baseline methods, the framework maintains near-perfect target visibility (>99.9%), reduces tracking e", 'abstract_zh': '自主辅助内窥镜控制在机器人辅助微创手术中的视觉增强预测控制（VisionSafeEnhanced Visual Predictive Control）：一种稳健且适应不确定性的框架，保证视角安全', 'title_zh': 'VisionSafe增强型VPC：在不确定性条件下具有可见性约束的谨慎预测控制在自主机器人手术中的应用'}
{'arxiv_id': 'arXiv:2508.18820', 'title': 'AS2FM: Enabling Statistical Model Checking of ROS 2 Systems for Robust Autonomy', 'authors': 'Christian Henkel, Marco Lampacrescia, Michaela Klauck, Matteo Morelli', 'link': 'https://arxiv.org/abs/2508.18820', 'abstract': 'Designing robotic systems to act autonomously in unforeseen environments is a challenging task. This work presents a novel approach to use formal verification, specifically Statistical Model Checking (SMC), to verify system properties of autonomous robots at design-time. We introduce an extension of the SCXML format, designed to model system components including both Robot Operating System 2 (ROS 2) and Behavior Tree (BT) features. Further, we contribute Autonomous Systems to Formal Models (AS2FM), a tool to translate the full system model into JANI. The use of JANI, a standard format for quantitative model checking, enables verification of system properties with off-the-shelf SMC tools. We demonstrate the practical usability of AS2FM both in terms of applicability to real-world autonomous robotic control systems, and in terms of verification runtime scaling. We provide a case study, where we successfully identify problems in a ROS 2-based robotic manipulation use case that is verifiable in less than one second using consumer hardware. Additionally, we compare to the state of the art and demonstrate that our method is more comprehensive in system feature support, and that the verification runtime scales linearly with the size of the model, instead of exponentially.', 'abstract_zh': '设计能够在未预见环境中自主行动的机器人系统是一个具有挑战性的任务。本工作提出了一种利用形式验证，特别是统计模型检查（SMC），在设计阶段验证自主机器人系统属性的新方法。我们引入了SCXML格式的扩展，用于建模包括Robot Operating System 2（ROS 2）和行为树（BT）特征在内的系统组件。此外，我们贡献了自主系统到形式模型（AS2FM）工具，用于将整个系统模型转换为JANI格式。JANI是定量模型检查的标准格式，能够使用现成的SMC工具验证系统属性。我们从实际应用和验证运行时扩展性两方面展示了AS2FM的实用价值。我们提供了一个案例研究，使用消费级硬件在不到一秒的时间内成功识别出基于ROS 2的机器人操作使用案例中的问题，并且验证运行时随模型大小线性扩展。', 'title_zh': 'AS2FM: 为鲁棒自主性启用ROS 2系统的统计模型检查'}
{'arxiv_id': 'arXiv:2508.18817', 'title': 'Learning Real-World Acrobatic Flight from Human Preferences', 'authors': 'Colin Merk, Ismail Geles, Jiaxu Xing, Angel Romero, Giorgia Ramponi, Davide Scaramuzza', 'link': 'https://arxiv.org/abs/2508.18817', 'abstract': 'Preference-based reinforcement learning (PbRL) enables agents to learn control policies without requiring manually designed reward functions, making it well-suited for tasks where objectives are difficult to formalize or inherently subjective. Acrobatic flight poses a particularly challenging problem due to its complex dynamics, rapid movements, and the importance of precise execution. In this work, we explore the use of PbRL for agile drone control, focusing on the execution of dynamic maneuvers such as powerloops. Building on Preference-based Proximal Policy Optimization (Preference PPO), we propose Reward Ensemble under Confidence (REC), an extension to the reward learning objective that improves preference modeling and learning stability. Our method achieves 88.4% of the shaped reward performance, compared to 55.2% with standard Preference PPO. We train policies in simulation and successfully transfer them to real-world drones, demonstrating multiple acrobatic maneuvers where human preferences emphasize stylistic qualities of motion. Furthermore, we demonstrate the applicability of our probabilistic reward model in a representative MuJoCo environment for continuous control. Finally, we highlight the limitations of manually designed rewards, observing only 60.7% agreement with human preferences. These results underscore the effectiveness of PbRL in capturing complex, human-centered objectives across both physical and simulated domains.', 'abstract_zh': '基于偏好的强化学习在敏捷无人机控制中的应用：从动力滚转等动态机动动作探索', 'title_zh': '基于人类偏好的真实世界杂技飞行学习'}
{'arxiv_id': 'arXiv:2508.18802', 'title': 'HyperTASR: Hypernetwork-Driven Task-Aware Scene Representations for Robust Manipulation', 'authors': 'Li Sun, Jiefeng Wu, Feng Chen, Ruizhe Liu, Yanchao Yang', 'link': 'https://arxiv.org/abs/2508.18802', 'abstract': 'Effective policy learning for robotic manipulation requires scene representations that selectively capture task-relevant environmental features. Current approaches typically employ task-agnostic representation extraction, failing to emulate the dynamic perceptual adaptation observed in human cognition. We present HyperTASR, a hypernetwork-driven framework that modulates scene representations based on both task objectives and the execution phase. Our architecture dynamically generates representation transformation parameters conditioned on task specifications and progression state, enabling representations to evolve contextually throughout task execution. This approach maintains architectural compatibility with existing policy learning frameworks while fundamentally reconfiguring how visual features are processed. Unlike methods that simply concatenate or fuse task embeddings with task-agnostic representations, HyperTASR establishes computational separation between task-contextual and state-dependent processing paths, enhancing learning efficiency and representational quality. Comprehensive evaluations in both simulation and real-world environments demonstrate substantial performance improvements across different representation paradigms. Through ablation studies and attention visualization, we confirm that our approach selectively prioritizes task-relevant scene information, closely mirroring human adaptive perception during manipulation tasks. The project website is at \\href{this https URL}{this http URL\\_projectpage}.', 'abstract_zh': '基于任务和执行阶段驱动的场景表示调节框架HyperTASR：有效实现机器人操作策略学习需要选择性捕捉与任务相关环境特征的场景表示。当前方法通常采用任务无关的表示提取，无法模拟人类认知中观察到的动力血压觉适应。', 'title_zh': 'HyperTASR: 基于超网络的任务感知场景表示以实现稳健操作'}
{'arxiv_id': 'arXiv:2508.18705', 'title': 'Enhancing Video-Based Robot Failure Detection Using Task Knowledge', 'authors': 'Santosh Thoduka, Sebastian Houben, Juergen Gall, Paul G. Plöger', 'link': 'https://arxiv.org/abs/2508.18705', 'abstract': 'Robust robotic task execution hinges on the reliable detection of execution failures in order to trigger safe operation modes, recovery strategies, or task replanning. However, many failure detection methods struggle to provide meaningful performance when applied to a variety of real-world scenarios. In this paper, we propose a video-based failure detection approach that uses spatio-temporal knowledge in the form of the actions the robot performs and task-relevant objects within the field of view. Both pieces of information are available in most robotic scenarios and can thus be readily obtained. We demonstrate the effectiveness of our approach on three datasets that we amend, in part, with additional annotations of the aforementioned task-relevant knowledge. In light of the results, we also propose a data augmentation method that improves performance by applying variable frame rates to different parts of the video. We observe an improvement from 77.9 to 80.0 in F1 score on the ARMBench dataset without additional computational expense and an additional increase to 81.4 with test-time augmentation. The results emphasize the importance of spatio-temporal information during failure detection and suggest further investigation of suitable heuristics in future implementations. Code and annotations are available.', 'abstract_zh': '基于视频的时空信息在机器人任务执行故障检测中的应用：一种稳健的机器人任务执行依赖于故障的可靠检测以触发安全操作模式、恢复策略或任务重新规划。然而，许多故障检测方法在应用于各种现实场景时难以提供有意义的性能。本文提出了一种基于视频的故障检测方法，利用机器人执行的动作和视野内的任务相关物体的时空知识。这两类信息在大多数机器人场景中均可获得，因此可以轻松获取。我们通过添加对上述任务相关知识的额外标注，在三个数据集上展示了该方法的有效性。基于实验结果，我们还提出了一种数据增强方法，通过不同的视频部分应用可变帧率以提高性能。在不增加额外计算成本的情况下，ARMBench数据集的F1分数从77.9提高到80.0，进一步通过测试时数据增强提高到81.4。这些结果强调了在故障检测过程中时空信息的重要性，并建议在未来实现中进一步研究合适的启发式方法。代码和标注已公开。', 'title_zh': '基于任务知识增强视频引导的机器人故障检测'}
{'arxiv_id': 'arXiv:2508.18694', 'title': 'AgriChrono: A Multi-modal Dataset Capturing Crop Growth and Lighting Variability with a Field Robot', 'authors': 'Jaehwan Jeong, Tuan-Anh Vu, Mohammad Jony, Shahab Ahmad, Md. Mukhlesur Rahman, Sangpil Kim, M. Khalid Jawed', 'link': 'https://arxiv.org/abs/2508.18694', 'abstract': 'Existing datasets for precision agriculture have primarily been collected in static or controlled environments such as indoor labs or greenhouses, often with limited sensor diversity and restricted temporal span. These conditions fail to reflect the dynamic nature of real farmland, including illumination changes, crop growth variation, and natural disturbances. As a result, models trained on such data often lack robustness and generalization when applied to real-world field scenarios. In this paper, we present AgriChrono, a novel robotic data collection platform and multi-modal dataset designed to capture the dynamic conditions of real-world agricultural environments. Our platform integrates multiple sensors and enables remote, time-synchronized acquisition of RGB, Depth, LiDAR, and IMU data, supporting efficient and repeatable long-term data collection across varying illumination and crop growth stages. We benchmark a range of state-of-the-art 3D reconstruction models on the AgriChrono dataset, highlighting the difficulty of reconstruction in real-world field environments and demonstrating its value as a research asset for advancing model generalization under dynamic conditions. The code and dataset are publicly available at: this https URL', 'abstract_zh': '现有的农业数据集主要在静态或受控环境中收集，如室内实验室或温室，通常传感器多样性有限且时间跨度受限。这些条件无法反映实际农田的动态特性，包括光照变化、作物生长差异和自然干扰。因此，基于此类数据训练的模型在应用于实际农田场景时往往缺乏鲁棒性和通用性。本文介绍了AgriChrono，一种新型的 robotic 数据采集平台和多模态数据集，旨在捕捉实际农业环境中动态条件。该平台集成了多种传感器，实现了远程、同步的 RGB、Depth、LiDAR 和 IMU 数据采集，支持在不同光照条件和作物生长阶段下高效、可重复的长期数据收集。我们在 AgriChrono 数据集上对一系列前沿的3D重建模型进行了基准测试，突显了在实际农田环境中进行重建的难度，并展示了其作为研究资源的价值，有助于在动态条件下提升模型的通用性。代码和数据集已公开：this https URL', 'title_zh': '农时序：一种基于田间机器人的多模态数据集，捕捉作物生长和光照变化'}
{'arxiv_id': 'arXiv:2508.18691', 'title': 'Deep Sensorimotor Control by Imitating Predictive Models of Human Motion', 'authors': 'Himanshu Gaurav Singh, Pieter Abbeel, Jitendra Malik, Antonio Loquercio', 'link': 'https://arxiv.org/abs/2508.18691', 'abstract': 'As the embodiment gap between a robot and a human narrows, new opportunities arise to leverage datasets of humans interacting with their surroundings for robot learning. We propose a novel technique for training sensorimotor policies with reinforcement learning by imitating predictive models of human motions. Our key insight is that the motion of keypoints on human-inspired robot end-effectors closely mirrors the motion of corresponding human body keypoints. This enables us to use a model trained to predict future motion on human data \\emph{zero-shot} on robot data. We train sensorimotor policies to track the predictions of such a model, conditioned on a history of past robot states, while optimizing a relatively sparse task reward. This approach entirely bypasses gradient-based kinematic retargeting and adversarial losses, which limit existing methods from fully leveraging the scale and diversity of modern human-scene interaction datasets. Empirically, we find that our approach can work across robots and tasks, outperforming existing baselines by a large margin. In addition, we find that tracking a human motion model can substitute for carefully designed dense rewards and curricula in manipulation tasks. Code, data and qualitative results available at this https URL.', 'abstract_zh': '随着机器人与人类之间差距的缩小，利用人类与环境交互的数据集训练机器人学习的新机遇应运而生。我们提出了一种通过模仿人类运动预测模型来训练传感器运动策略的新技术。我们的核心洞察是，受人类启发的机器人末端执行器的关键点运动与对应的人体关键点运动高度相似，这使得我们可以在不进行微调的情况下，利用训练好的预测未来运动的人类数据模型直接应用于机器人数据中。我们训练传感器运动策略，使其在历史机器人状态的条件下追踪该模型的预测，同时优化相对稀疏的任务奖励。这种方法完全绕过了基于梯度的动力学重新定位和对抗损失，从而使得现有方法能够充分利用现代人类场景交互数据集的规模和多样性。实验结果显示，我们的方法在不同机器人和任务中表现出色，显著优于现有基线。此外，我们发现，跟踪人类运动模型可以在操纵任务中替代精心设计的密集奖励和课程设置。相关代码、数据和定性结果可在以下链接获取。', 'title_zh': '深度传感器运动控制：人类运动预测模型的模仿'}
{'arxiv_id': 'arXiv:2508.18662', 'title': 'Engineering Automotive Digital Twins on Standardized Architectures: A Case Study', 'authors': 'Stefan Ramdhan, Winnie Trandinh, Istvan David, Vera Pantelic, Mark Lawford', 'link': 'https://arxiv.org/abs/2508.18662', 'abstract': 'Digital twin (DT) technology has become of interest in the automotive industry. There is a growing need for smarter services that utilize the unique capabilities of DTs, ranging from computer-aided remote control to cloud-based fleet coordination. Developing such services starts with the software architecture. However, the scarcity of DT architectural guidelines poses a challenge for engineering automotive DTs. Currently, the only DT architectural standard is the one defined in ISO 23247. Though not developed for automotive systems, it is one of the few feasible starting points for automotive DTs. In this work, we investigate the suitability of the ISO 23247 reference architecture for developing automotive DTs. Through the case study of developing an Adaptive Cruise Control DT for a 1/10\\textsuperscript{th}-scale autonomous vehicle, we identify some strengths and limitations of the reference architecture and begin distilling future directions for researchers, practitioners, and standard developers.', 'abstract_zh': '数字孪生(DT)技术在汽车行业的应用引起了广泛关注。智能服务的需求日益增加，这些服务利用了DT的独特功能，从计算机辅助远程控制到基于云的车队协调。开发such服务始于软件架构。然而，缺乏专门针对DT的工程指南构成了一个挑战。目前，唯一存在的DT架构标准是ISO 23247定义的标准。尽管该标准并非专为汽车系统而设计，但它是开发汽车DT的少数可行起点之一。在这项工作中，我们探讨了ISO 23247参考架构在开发汽车DT方面的适用性。通过针对1/10尺度自主车辆开发自适应巡航控制(DT)案例研究，我们识别了参考架构的一些优势和局限性，并开始为研究者、从业者和标准开发者明确未来的研究方向。', 'title_zh': '基于标准化架构的汽车数字孪生工程：一个案例研究'}
{'arxiv_id': 'arXiv:2508.18627', 'title': 'Integration of Robot and Scene Kinematics for Sequential Mobile Manipulation Planning', 'authors': 'Ziyuan Jiao, Yida Niu, Zeyu Zhang, Yangyang Wu, Yao Su, Yixin Zhu, Hangxin Liu, Song-Chun Zhu', 'link': 'https://arxiv.org/abs/2508.18627', 'abstract': "We present a Sequential Mobile Manipulation Planning (SMMP) framework that can solve long-horizon multi-step mobile manipulation tasks with coordinated whole-body motion, even when interacting with articulated objects. By abstracting environmental structures as kinematic models and integrating them with the robot's kinematics, we construct an Augmented Configuration Apace (A-Space) that unifies the previously separate task constraints for navigation and manipulation, while accounting for the joint reachability of the robot base, arm, and manipulated objects. This integration facilitates efficient planning within a tri-level framework: a task planner generates symbolic action sequences to model the evolution of A-Space, an optimization-based motion planner computes continuous trajectories within A-Space to achieve desired configurations for both the robot and scene elements, and an intermediate plan refinement stage selects action goals that ensure long-horizon feasibility. Our simulation studies first confirm that planning in A-Space achieves an 84.6\\% higher task success rate compared to baseline methods. Validation on real robotic systems demonstrates fluid mobile manipulation involving (i) seven types of rigid and articulated objects across 17 distinct contexts, and (ii) long-horizon tasks of up to 14 sequential steps. Our results highlight the significance of modeling scene kinematics into planning entities, rather than encoding task-specific constraints, offering a scalable and generalizable approach to complex robotic manipulation.", 'abstract_zh': '基于协调全身运动的顺序移动操控规划框架', 'title_zh': '机器人与场景运动学的集成在序列移动操作规划中的应用'}
{'arxiv_id': 'arXiv:2508.18606', 'title': 'SignLoc: Robust Localization using Navigation Signs and Public Maps', 'authors': 'Nicky Zimmerman, Joel Loo, Ayush Agrawal, David Hsu', 'link': 'https://arxiv.org/abs/2508.18606', 'abstract': 'Navigation signs and maps, such as floor plans and street maps, are widely available and serve as ubiquitous aids for way-finding in human environments. Yet, they are rarely used by robot systems. This paper presents SignLoc, a global localization method that leverages navigation signs to localize the robot on publicly available maps -- specifically floor plans and OpenStreetMap (OSM) graphs--without prior sensor-based mapping. SignLoc first extracts a navigation graph from the input map. It then employs a probabilistic observation model to match directional and locational cues from the detected signs to the graph, enabling robust topo-semantic localization within a Monte Carlo framework. We evaluated SignLoc in diverse large-scale environments: part of a university campus, a shopping mall, and a hospital complex. Experimental results show that SignLoc reliably localizes the robot after observing only one to two signs.', 'abstract_zh': '基于导航标志的全球定位方法SignLoc：在公开地图上利用导航标志进行无需先验传感器建图的机器人全局定位', 'title_zh': 'SignLoc: 基于导航标志和公共地图的鲁棒定位方法'}
{'arxiv_id': 'arXiv:2508.18460', 'title': 'Mimicking associative learning of rats via a neuromorphic robot in open field maze using spatial cell models', 'authors': 'Tianze Liu, Md Abu Bakr Siddique, Hongyu An', 'link': 'https://arxiv.org/abs/2508.18460', 'abstract': 'Data-driven Artificial Intelligence (AI) approaches have exhibited remarkable prowess across various cognitive tasks using extensive training data. However, the reliance on large datasets and neural networks presents challenges such as highpower consumption and limited adaptability, particularly in SWaP-constrained applications like planetary exploration. To address these issues, we propose enhancing the autonomous capabilities of intelligent robots by emulating the associative learning observed in animals. Associative learning enables animals to adapt to their environment by memorizing concurrent events. By replicating this mechanism, neuromorphic robots can navigate dynamic environments autonomously, learning from interactions to optimize performance. This paper explores the emulation of associative learning in rodents using neuromorphic robots within open-field maze environments, leveraging insights from spatial cells such as place and grid cells. By integrating these models, we aim to enable online associative learning for spatial tasks in real-time scenarios, bridging the gap between biological spatial cognition and robotics for advancements in autonomous systems.', 'abstract_zh': '基于数据驱动的人工智能方法在认知任务中表现出色，尤其是在大量训练数据的支持下。然而，对大数据集和神经网络的依赖带来了高能耗和适应性有限的问题，特别是在受SWaP约束的应用如行星探索中。为了解决这些问题，我们提出通过模拟动物观察到的关联学习机制来增强智能机器人的自主能力。关联学习使动物能够通过记忆并发事件来适应环境。通过复制这一机制，神经形态机器人可以在动态环境中自主导航，并从互动中学习以优化性能。本文探讨了在开放场迷宫环境中使用神经形态机器人模拟啮齿类动物的关联学习，借鉴了空间细胞如地点细胞和网格细胞的见解。通过整合这些模型，旨在实现实时场景中的空间任务的在线关联学习，从而在生物空间认知和机器人技术之间架起桥梁，促进自主系统的发展。', 'title_zh': '基于空间细胞模型的神经形态机器人在开放场迷宫中模仿rats的关联学习'}
{'arxiv_id': 'arXiv:2508.18443', 'title': 'PneuGelSight: Soft Robotic Vision-Based Proprioception and Tactile Sensing', 'authors': 'Ruohan Zhang, Uksang Yoo, Yichen Li, Arpit Argawal, Wenzhen Yuan', 'link': 'https://arxiv.org/abs/2508.18443', 'abstract': "Soft pneumatic robot manipulators are popular in industrial and human-interactive applications due to their compliance and flexibility. However, deploying them in real-world scenarios requires advanced sensing for tactile feedback and proprioception. Our work presents a novel vision-based approach for sensorizing soft robots. We demonstrate our approach on PneuGelSight, a pioneering pneumatic manipulator featuring high-resolution proprioception and tactile sensing via an embedded camera. To optimize the sensor's performance, we introduce a comprehensive pipeline that accurately simulates its optical and dynamic properties, facilitating a zero-shot knowledge transition from simulation to real-world applications. PneuGelSight and our sim-to-real pipeline provide a novel, easily implementable, and robust sensing methodology for soft robots, paving the way for the development of more advanced soft robots with enhanced sensory capabilities.", 'abstract_zh': '基于视觉的软气动机器人传感器化方法', 'title_zh': 'PneuGelSight：基于软体机器人视觉的姿态感知与触觉传感'}
{'arxiv_id': 'arXiv:2508.18400', 'title': 'Efficient task and path planning for maintenance automation using a robot system', 'authors': 'Christian Friedrich, Akos Csiszar, Armin Lechler, Alexander Verl', 'link': 'https://arxiv.org/abs/2508.18400', 'abstract': 'The research and development of intelligent automation solutions is a ground-breaking point for the factory of the future. A promising and challenging mission is the use of autonomous robot systems to automate tasks in the field of maintenance. For this purpose, the robot system must be able to plan autonomously the different manipulation tasks and the corresponding paths. Basic requirements are the development of algorithms with a low computational complexity and the possibility to deal with environmental uncertainties. In this work, an approach is presented, which is especially suited to solve the problem of maintenance automation. For this purpose, offline data from CAD is combined with online data from an RGBD vision system via a probabilistic filter, to compensate uncertainties from offline data. For planning the different tasks, a method is explained, which use a symbolic description, founded on a novel sampling-based method to compute the disassembly space. For path planning we use global state-of-the art algorithms with a method that allows the adaption of the exploration stepsize in order to reduce the planning time. Every method is experimentally validated and discussed.', 'abstract_zh': '未来工厂中智能自动化解决方案的研究与开发：基于自主机器人系统的维护任务自动化及其实现方法', 'title_zh': '基于机器人系统的维护自动化任务与路径规划高效方法'}
{'arxiv_id': 'arXiv:2508.18399', 'title': 'Maintenance automation: methods for robotics manipulation planning and execution', 'authors': 'Christian Friedrich, Ralf Gulde, Armin Lechler, Alexander Verl', 'link': 'https://arxiv.org/abs/2508.18399', 'abstract': 'Automating complex tasks using robotic systems requires skills for planning, control and execution. This paper proposes a complete robotic system for maintenance automation, which can automate disassembly and assembly operations under environmental uncertainties (e.g. deviations between prior plan information). The cognition of the robotic system is based on a planning approach (using CAD and RGBD data) and includes a method to interpret a symbolic plan and transform it to a set of executable robot instructions. The complete system is experimentally evaluated using real-world applications. This work shows the first step to transfer these theoretical results into a practical robotic solution.', 'abstract_zh': '使用机器人系统自动化复杂任务需要规划、控制和执行技能。本文提出了一种完整的机器人系统，可以在环境不确定性（如先验计划信息与实际情况之间的偏差）下自动化拆装操作的规划、控制和执行。该机器人的认知基于规划方法（使用CAD和RGBD数据），包括一种解释符号计划并将之转换为可执行机器人指令的方法。该完整系统通过实际应用进行了实验评估。本工作展示了将这些理论结果转化为实际机器人解决方案的第一步。', 'title_zh': '机器人操作中的维护自动化：机器人操作规划与执行的方法'}
{'arxiv_id': 'arXiv:2508.18397', 'title': 'Mining the Long Tail: A Comparative Study of Data-Centric Criticality Metrics for Robust Offline Reinforcement Learning in Autonomous Motion Planning', 'authors': 'Antonio Guillen-Perez', 'link': 'https://arxiv.org/abs/2508.18397', 'abstract': 'Offline Reinforcement Learning (RL) presents a promising paradigm for training autonomous vehicle (AV) planning policies from large-scale, real-world driving logs. However, the extreme data imbalance in these logs, where mundane scenarios vastly outnumber rare "long-tail" events, leads to brittle and unsafe policies when using standard uniform data sampling. In this work, we address this challenge through a systematic, large-scale comparative study of data curation strategies designed to focus the learning process on information-rich samples. We investigate six distinct criticality weighting schemes which are categorized into three families: heuristic-based, uncertainty-based, and behavior-based. These are evaluated at two temporal scales, the individual timestep and the complete scenario. We train seven goal-conditioned Conservative Q-Learning (CQL) agents with a state-of-the-art, attention-based architecture and evaluate them in the high-fidelity Waymax simulator. Our results demonstrate that all data curation methods significantly outperform the baseline. Notably, data-driven curation using model uncertainty as a signal achieves the most significant safety improvements, reducing the collision rate by nearly three-fold (from 16.0% to 5.5%). Furthermore, we identify a clear trade-off where timestep-level weighting excels at reactive safety while scenario-level weighting improves long-horizon planning. Our work provides a comprehensive framework for data curation in Offline RL and underscores that intelligent, non-uniform sampling is a critical component for building safe and reliable autonomous agents.', 'abstract_zh': '基于离线强化学习的数据curate策略在自主车辆规划中的应用：一种有希望的范式及其面临的挑战与解决方案', 'title_zh': '长尾效应的挖掘：自主运动规划中稳健离线强化学习的数据为中心的关键性度量比较研究'}
{'arxiv_id': 'arXiv:2508.19159', 'title': 'Safe Navigation under State Uncertainty: Online Adaptation for Robust Control Barrier Functions', 'authors': 'Ersin Das, Rahal Nanayakkara, Xiao Tan, Ryan M. Bena, Joel W. Burdick, Paulo Tabuada, Aaron D. Ames', 'link': 'https://arxiv.org/abs/2508.19159', 'abstract': "Measurements and state estimates are often imperfect in control practice, posing challenges for safety-critical applications, where safety guarantees rely on accurate state information. In the presence of estimation errors, several prior robust control barrier function (R-CBF) formulations have imposed strict conditions on the input. These methods can be overly conservative and can introduce issues such as infeasibility, high control effort, etc. This work proposes a systematic method to improve R-CBFs, and demonstrates its advantages on a tracked vehicle that navigates among multiple obstacles. A primary contribution is a new optimization-based online parameter adaptation scheme that reduces the conservativeness of existing R-CBFs. In order to reduce the complexity of the parameter optimization, we merge several safety constraints into one unified numerical CBF via Poisson's equation. We further address the dual relative degree issue that typically causes difficulty in vehicle tracking. Experimental trials demonstrate the overall performance improvement of our approach over existing formulations.", 'abstract_zh': '基于测量和状态估计的 Imperfections 在控制实践中对安全关键应用构成挑战：鲁棒控制障碍函数在估计误差下的系统化改进及其在多障碍导航履带车辆上的优势', 'title_zh': '在状态不确定性下的安全导航：鲁棒控制屏障函数的在线适应'}
{'arxiv_id': 'arXiv:2508.19094', 'title': 'VibES: Induced Vibration for Persistent Event-Based Sensing', 'authors': 'Vincenzo Polizzi, Stephen Yang, Quentin Clark, Jonathan Kelly, Igor Gilitschenski, David B. Lindell', 'link': 'https://arxiv.org/abs/2508.19094', 'abstract': 'Event cameras are a bio-inspired class of sensors that asynchronously measure per-pixel intensity changes. Under fixed illumination conditions in static or low-motion scenes, rigidly mounted event cameras are unable to generate any events, becoming unsuitable for most computer vision tasks. To address this limitation, recent work has investigated motion-induced event stimulation that often requires complex hardware or additional optical components. In contrast, we introduce a lightweight approach to sustain persistent event generation by employing a simple rotating unbalanced mass to induce periodic vibrational motion. This is combined with a motion-compensation pipeline that removes the injected motion and yields clean, motion-corrected events for downstream perception tasks. We demonstrate our approach with a hardware prototype and evaluate it on real-world captured datasets. Our method reliably recovers motion parameters and improves both image reconstruction and edge detection over event-based sensing without motion induction.', 'abstract_zh': '基于运动诱导的事件生成轻量级方法及其实验验证：改善事件感知中的运动参数恢复、图像重建和边缘检测', 'title_zh': 'VibES: 引起振动以实现持久的事件驱动传感'}
{'arxiv_id': 'arXiv:2508.18898', 'title': 'Interpretable Decision-Making for End-to-End Autonomous Driving', 'authors': 'Mona Mirzaie, Bodo Rosenhahn', 'link': 'https://arxiv.org/abs/2508.18898', 'abstract': 'Trustworthy AI is mandatory for the broad deployment of autonomous vehicles. Although end-to-end approaches derive control commands directly from raw data, interpreting these decisions remains challenging, especially in complex urban scenarios. This is mainly attributed to very deep neural networks with non-linear decision boundaries, making it challenging to grasp the logic behind AI-driven decisions. This paper presents a method to enhance interpretability while optimizing control commands in autonomous driving. To address this, we propose loss functions that promote the interpretability of our model by generating sparse and localized feature maps. The feature activations allow us to explain which image regions contribute to the predicted control command. We conduct comprehensive ablation studies on the feature extraction step and validate our method on the CARLA benchmarks. We also demonstrate that our approach improves interpretability, which correlates with reducing infractions, yielding a safer, high-performance driving model. Notably, our monocular, non-ensemble model surpasses the top-performing approaches from the CARLA Leaderboard by achieving lower infraction scores and the highest route completion rate, all while ensuring interpretability.', 'abstract_zh': '可信的人工智能对于自动驾驶的广泛部署是必要的。尽管端到端的方法可以直接从原始数据中推导出控制命令，但在复杂的城市场景中解释这些决策仍然极具挑战性。这主要归因于非常深的非线性神经网络，使得难以把握基于人工智能决策背后的逻辑。本文提出了一种方法，在优化控制命令的同时提高可解释性。为此，我们提出了能够通过生成稀疏和局部特征图促进模型可解释性的损失函数。特征激活使得我们可以解释哪些图像区域对预测的控制命令做出了贡献。我们在特征提取步骤上进行了全面的消融研究，并在CARLA基准上验证了我们的方法。我们还展示了我们的方法提高了可解释性，这与减少违规行为、获得更安全和高性能的驾驶模型相关。值得注意的是，我们的单目、非集成模型在违规分数较低和路线完成率最高方面超越了CARLA排行榜上表现最好的方法，同时保证了可解释性。', 'title_zh': '端到端可解释的自动驾驶决策'}
{'arxiv_id': 'arXiv:2508.18788', 'title': 'PseudoMapTrainer: Learning Online Mapping without HD Maps', 'authors': 'Christian Löwens, Thorben Funke, Jingchao Xie, Alexandru Paul Condurache', 'link': 'https://arxiv.org/abs/2508.18788', 'abstract': 'Online mapping models show remarkable results in predicting vectorized maps from multi-view camera images only. However, all existing approaches still rely on ground-truth high-definition maps during training, which are expensive to obtain and often not geographically diverse enough for reliable generalization. In this work, we propose PseudoMapTrainer, a novel approach to online mapping that uses pseudo-labels generated from unlabeled sensor data. We derive those pseudo-labels by reconstructing the road surface from multi-camera imagery using Gaussian splatting and semantics of a pre-trained 2D segmentation network. In addition, we introduce a mask-aware assignment algorithm and loss function to handle partially masked pseudo-labels, allowing for the first time the training of online mapping models without any ground-truth maps. Furthermore, our pseudo-labels can be effectively used to pre-train an online model in a semi-supervised manner to leverage large-scale unlabeled crowdsourced data. The code is available at this http URL.', 'abstract_zh': '基于伪标签的在线地图训练：无需地理标记高清地图的多视图相机图像矢量化地图预测', 'title_zh': '伪地图训练器：基于在线映射的高精度地图学习'}
{'arxiv_id': 'arXiv:2508.18729', 'title': 'Are All Marine Species Created Equal? Performance Disparities in Underwater Object Detection', 'authors': 'Melanie Wille, Tobias Fischer, Scarlett Raine', 'link': 'https://arxiv.org/abs/2508.18729', 'abstract': 'Underwater object detection is critical for monitoring marine ecosystems but poses unique challenges, including degraded image quality, imbalanced class distribution, and distinct visual characteristics. Not every species is detected equally well, yet underlying causes remain unclear. We address two key research questions: 1) What factors beyond data quantity drive class-specific performance disparities? 2) How can we systematically improve detection of under-performing marine species? We manipulate the DUO dataset to separate the object detection task into localization and classification and investigate the under-performance of the scallop class. Localization analysis using YOLO11 and TIDE finds that foreground-background discrimination is the most problematic stage regardless of data quantity. Classification experiments reveal persistent precision gaps even with balanced data, indicating intrinsic feature-based challenges beyond data scarcity and inter-class dependencies. We recommend imbalanced distributions when prioritizing precision, and balanced distributions when prioritizing recall. Improving under-performing classes should focus on algorithmic advances, especially within localization modules. We publicly release our code and datasets.', 'abstract_zh': '水下目标检测对于监测海洋生态系统至关重要，但面临着独特挑战，包括图像质量退化、类别分布失衡和独特的视觉特征。并非每种物种都能同等检测，背后的原因尚不清楚。我们探讨了两个关键研究问题：1) 除了数据量外，哪些因素导致类别的性能差异？2) 我们如何系统性地提高性能不佳的海洋物种的检测能力？我们将DUO数据集拆分为定位和分类任务，研究扇贝类别的性能不佳问题。使用YOLO11和TIDE的定位分析发现，无论数据量如何，前景与背景的区分是最具问题的阶段。分类实验表明，在数据平衡的情况下依然存在持续的精度差距，这表明除了数据稀缺性和类别间依赖性之外，还存在固有的特征挑战。我们建议在优先考虑精度时关注不平衡分布，在优先考虑召回率时关注平衡分布。提高性能不佳的类别应集中在算法的进步上，尤其是在定位模块中。我们公开发布我们的代码和数据集。', 'title_zh': '水下目标检测中 marine 种类的表现差异：并非所有海洋物种都平等？'}
{'arxiv_id': 'arXiv:2508.18293', 'title': 'Towards Training-Free Underwater 3D Object Detection from Sonar Point Clouds: A Comparison of Traditional and Deep Learning Approaches', 'authors': 'M. Salman Shaukat, Yannik Käckenmeister, Sebastian Bader, Thomas Kirste', 'link': 'https://arxiv.org/abs/2508.18293', 'abstract': 'Underwater 3D object detection remains one of the most challenging frontiers in computer vision, where traditional approaches struggle with the harsh acoustic environment and scarcity of training data. While deep learning has revolutionized terrestrial 3D detection, its application underwater faces a critical bottleneck: obtaining sufficient annotated sonar data is prohibitively expensive and logistically complex, often requiring specialized vessels, expert surveyors, and favorable weather conditions. This work addresses a fundamental question: Can we achieve reliable underwater 3D object detection without real-world training data? We tackle this challenge by developing and comparing two paradigms for training-free detection of artificial structures in multibeam echo-sounder point clouds. Our dual approach combines a physics-based sonar simulation pipeline that generates synthetic training data for state-of-the-art neural networks, with a robust model-based template matching system that leverages geometric priors of target objects. Evaluation on real bathymetry surveys from the Baltic Sea reveals surprising insights: while neural networks trained on synthetic data achieve 98% mean Average Precision (mAP) on simulated scenes, they drop to 40% mAP on real sonar data due to domain shift. Conversely, our template matching approach maintains 83% mAP on real data without requiring any training, demonstrating remarkable robustness to acoustic noise and environmental variations. Our findings challenge conventional wisdom about data-hungry deep learning in underwater domains and establish the first large-scale benchmark for training-free underwater 3D detection. This work opens new possibilities for autonomous underwater vehicle navigation, marine archaeology, and offshore infrastructure monitoring in data-scarce environments where traditional machine learning approaches fail.', 'abstract_zh': '水下3D物体检测仍然是计算机视觉中最具挑战性的前沿领域之一，其中传统方法难以应对恶劣的声学环境和训练数据匮乏的问题。尽管深度学习已经革新了陆地3D检测，但其在水下应用面临着一个关键瓶颈：收集足够的标注声纳数据成本高昂且 logistically 复杂，通常需要专门的船只、专家调查员和有利的天气条件。本文探讨了一个基本问题：我们能否在没有真实世界训练数据的情况下实现可靠的水下3D物体检测？通过开发和比较两种无需训练的数据驱动检测范式来应对这一挑战，对多波束回声声纳点云中人工结构进行检测。我们的双管齐下方法结合了基于物理的声纳仿真管道以生成最先进的神经网络所需的合成训练数据，以及利用目标物体几何先验信息的稳健模型导向模板匹配系统。对波罗的海实际声底测量的评估揭示了一些令人惊讶的见解：虽然采用合成数据训练的神经网络在模拟场景中的mean Average Precision (mAP) 达到98%，但在实际声纳数据上仅维持40%的mAP，这是因为领域变化的原因。相反，我们的模板匹配方法在没有进行任何训练的情况下，对实际数据保持83%的mAP，显示出对声学噪声和环境变化的显著鲁棒性。本文的研究挑战了水下领域需要大量数据的深度学习传统智慧，并确立了首个大规模的无需训练的水下3D检测基准。本工作为数据稀缺环境中自主水下航行器导航、海洋考古学和海工结构监测开启了新的可能性。', 'title_zh': '无需训练的水下3D目标检测从声纳点云：传统方法与深度学习方法的比较'}
