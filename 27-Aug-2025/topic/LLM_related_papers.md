# The Ramon Llull's Thinking Machine for Automated Ideation 

**Title (ZH)**: 拉蒙·卢勒的思考机器：自动化创意思考 

**Authors**: Xinran Zhao, Boyuan Zheng, Chenglei Si, Haofei Yu, Ken Liu, Runlong Zhou, Ruochen Li, Tong Chen, Xiang Li, Yiming Zhang, Tongshuang Wu  

**Link**: [PDF](https://arxiv.org/pdf/2508.19200)  

**Abstract**: This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for generating knowledge through symbolic recombination - as a conceptual foundation for building a modern Llull's thinking machine for research ideation. Our approach defines three compositional axes: Theme (e.g., efficiency, adaptivity), Domain (e.g., question answering, machine translation), and Method (e.g., adversarial training, linear attention). These elements represent high-level abstractions common in scientific work - motivations, problem settings, and technical approaches - and serve as building blocks for LLM-driven exploration. We mine elements from human experts or conference papers and show that prompting LLMs with curated combinations produces research ideas that are diverse, relevant, and grounded in current literature. This modern thinking machine offers a lightweight, interpretable tool for augmenting scientific creativity and suggests a path toward collaborative ideation between humans and AI. 

**Abstract (ZH)**: 本文重新审视拉蒙·卢勒的《组合艺术》——一个通过符号重新组合生成知识的中世纪框架——作为构建基于大规模语言模型的现代卢勒思想机器以促进研究创意的基础。我们的方法定义了三个组合轴：主题（如，效率、适应性）、领域（如，问答、机器翻译）和方法（如，对抗训练、线性注意力）。这些元素代表了科学工作中常见的高级抽象——动机、问题设置和技术方法——并作为基于大规模语言模型探索的构建块。我们从人类专家或会议论文中挖掘元素，并展示出通过精心组合提示大规模语言模型可以生成多样化、相关且植根于当前文献的研究创意。这种现代思想机器提供了一个轻量级且可解释的工具，以增强科学创意，并暗示了人类与AI协作产生创意的道路。 

---
# Reasoning LLMs in the Medical Domain: A Literature Survey 

**Title (ZH)**: 医疗领域中的预训练语言模型：一篇文献综述 

**Authors**: Armin Berger, Sarthak Khanna, David Berghaus, Rafet Sifa  

**Link**: [PDF](https://arxiv.org/pdf/2508.19097)  

**Abstract**: The emergence of advanced reasoning capabilities in Large Language Models (LLMs) marks a transformative development in healthcare applications. Beyond merely expanding functional capabilities, these reasoning mechanisms enhance decision transparency and explainability-critical requirements in medical contexts. This survey examines the transformation of medical LLMs from basic information retrieval tools to sophisticated clinical reasoning systems capable of supporting complex healthcare decisions. We provide a thorough analysis of the enabling technological foundations, with a particular focus on specialized prompting techniques like Chain-of-Thought and recent breakthroughs in Reinforcement Learning exemplified by DeepSeek-R1. Our investigation evaluates purpose-built medical frameworks while also examining emerging paradigms such as multi-agent collaborative systems and innovative prompting architectures. The survey critically assesses current evaluation methodologies for medical validation and addresses persistent challenges in field interpretation limitations, bias mitigation strategies, patient safety frameworks, and integration of multimodal clinical data. Through this survey, we seek to establish a roadmap for developing reliable LLMs that can serve as effective partners in clinical practice and medical research. 

**Abstract (ZH)**: 大型语言模型（LLMs）中先进推理能力的出现标志着医疗应用中的一个变革性发展。这些推理机制不仅扩展了功能能力，还在医疗领域中增强了决策的透明性和可解释性，这是至关重要的要求。本文综述了医疗LLMs从基本的信息检索工具转变为能够支持复杂医疗决策的高级临床推理系统的过程。我们对使这些系统得以实现的技术基础进行了全面分析，特别关注像Chain-of-Thought这样的专门提示技术，以及由DeepSeek-R1代表的强化学习的最新突破。本文综述评估了专用的医疗框架，同时也考察了多智能体协作系统和新颖的提示架构等新兴范式。我们批判性地评估了当前的医疗验证评价方法，并讨论了领域解释限制、偏见缓解策略、患者安全框架以及多模态临床数据整合等持续性挑战。通过本文综述，我们希望为开发可靠且在临床实践和医疗研究中有效辅助的LLMs指明一条道路。 

---
# Trustworthy Agents for Electronic Health Records through Confidence Estimation 

**Title (ZH)**: 通过置信度估计实现可靠的电子健康记录代理 

**Authors**: Yongwoo Song, Minbyul Jeong, Mujeen Sung  

**Link**: [PDF](https://arxiv.org/pdf/2508.19096)  

**Abstract**: Large language models (LLMs) show promise for extracting information from Electronic Health Records (EHR) and supporting clinical decisions. However, deployment in clinical settings faces challenges due to hallucination risks. We propose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric quantifying the accuracy-reliability trade-off at varying confidence thresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating stepwise confidence estimation for clinical question answering. Experiments on MIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under strict reliability constraints, achieving improvements of 44.23%p and 25.34%p at HCAcc@70% while baseline methods fail at these thresholds. These results highlight limitations of traditional accuracy metrics in evaluating healthcare AI agents. Our work contributes to developing trustworthy clinical agents that deliver accurate information or transparently express uncertainty when confidence is low. 

**Abstract (ZH)**: Large语言模型（LLMs）在从电子健康记录（EHR）中抽取信息和支持临床决策方面显示出潜力。然而，在临床环境中部署受到幻觉风险的挑战。我们提出了一种新的指标Hallucination Controlled Accuracy at k% (HCAcc@k%)，用于量化在不同置信阈值下的准确性和可靠性的权衡。我们引入了TrustEHRAgent，这是一种包含逐步置信度估计的置信意识代理，用于临床问答。在MIMIC-III和eICU数据集上的实验表明，在严格的可靠性约束下，TrustEHRAgent优于基线方法，在HCAcc@70%的置信阈值下分别实现了44.23%p和25.34%p的提升，而基线方法在这些阈值下失效。这些结果突显了传统准确性指标在评估医疗AI代理方面的局限性。我们的工作为开发可信赖的临床代理做出了贡献，这些代理能够在置信度低时提供准确信息或透明地表达不确定性。 

---
# Can Structured Templates Facilitate LLMs in Tackling Harder Tasks? : An Exploration of Scaling Laws by Difficulty 

**Title (ZH)**: 结构化模板能帮助大规模语言模型应对更复杂的任务吗？：基于难度的扩展规律探索 

**Authors**: Zhichao Yang, Zhaoxin Fan, Gen Li, Yuanze Hu, Xinyu Wang, Ye Qiu, Xin Wang, Yifan Sun, Wenjun Wu  

**Link**: [PDF](https://arxiv.org/pdf/2508.19069)  

**Abstract**: Structured, procedural reasoning is essential for Large Language Models (LLMs), especially in mathematics. While post-training methods have improved LLM performance, they still fall short in capturing deep procedural logic on complex tasks. To tackle the issue, in this paper, we first investigate this limitation and uncover a novel finding: a Scaling Law by Difficulty, which reveals that model performance follows a U-shaped curve with respect to training data complexity -- excessive low-difficulty data impedes abstraction, while high-difficulty data significantly enhances reasoning ability. Motivated by this, we propose the Structured Solution Template (SST) framework, which uses solution templates and a curriculum of varied difficulty to explicitly teach procedural reasoning. Specifically, SST comprises (1) fine-tuning with structured solution-template chains and dynamically weighted loss to prioritize procedural logic, (2) prompt-time injection of solution templates as cognitive scaffolds to guide inference, and (3) integrated curriculum fine-tuning that explicitly teaches the model to self-plan - execute - self-correct. Experiments on GSM8K, AIME24, and new Dynamic En benchmark show that SST significantly improves both accuracy and efficiency, especially on harder problems. 

**Abstract (ZH)**: 结构化的过程性推理对于大型语言模型（LLMs），尤其是在数学领域，是至关重要的。尽管后训练方法提高了LLM的性能，但在复杂任务中捕捉深层次的过程性逻辑方面仍然不足。为了解决这一问题，本文首先探讨了这一局限性，并发现了一个新的发现：难度可扩展定律，该定律揭示了模型性能与训练数据复杂度之间存在U形曲线关系——过量低难度数据会妨碍抽象能力，而高难度数据则显著提升推理能力。为了解决这一问题，我们提出了结构化解决方案模板（SST）框架，该框架通过使用解决方案模板和不同难度梯度来明确教授过程性推理。具体来说，SST 包括（1）结构化的解决方案模板链微调以及动态加权损失，以优先处理过程性逻辑；（2）在提示时注入解决方案模板作为认知支架，引导推理；（3）集成难度梯度微调，明确教授模型自我规划-执行-自我纠正。在GSM8K、AIME24 和新动态基准上的实验表明，SST 在准确性与效率方面均有显著提升，特别是在更难的问题上表现尤为明显。 

---
# A Concurrent Modular Agent: Framework for Autonomous LLM Agents 

**Title (ZH)**: 并发模块化代理：自主LLM代理框架 

**Authors**: Norihiro Maruyama, Takahide Yoshida, Hiroki Sato, Atsushi Masumori, Johnsmith, Takashi Ikegami  

**Link**: [PDF](https://arxiv.org/pdf/2508.19042)  

**Abstract**: We introduce the Concurrent Modular Agent (CMA), a framework that orchestrates multiple Large-Language-Model (LLM)-based modules that operate fully asynchronously yet maintain a coherent and fault-tolerant behavioral loop. This framework addresses long-standing difficulties in agent architectures by letting intention emerge from language-mediated interactions among autonomous processes. This approach enables flexible, adaptive, and context-dependent behavior through the combination of concurrently executed modules that offload reasoning to an LLM, inter-module communication, and a single shared global this http URL consider this approach to be a practical realization of Minsky's Society of Mind theory. We demonstrate the viability of our system through two practical use-case studies. The emergent properties observed in our system suggest that complex cognitive phenomena like self-awareness may indeed arise from the organized interaction of simpler processes, supporting Minsky-Society of Mind concept and opening new avenues for artificial intelligence research. The source code for our work is available at: this https URL. 

**Abstract (ZH)**: 我们介绍了并发模块代理（CMA）框架，该框架协调多个基于大型语言模型（LLM）的模块，这些模块完全异步运行但仍保持一致且健壮的行为循环。该框架通过让意向通过自主进程之间的语言中介交互而自然涌现，解决了代理架构中的长期难题。这种通过并发执行模块、将推理卸载到LLM、模块间通信以及单一共享全局状态的结合方式，实现了灵活、适应性强和上下文相关的代理行为。我们认为这种方法是Minsky的心灵社会理论的一种实际实现。我们通过两个实际案例研究展示了该系统的可行性。观察到的涌现性质表明，复杂认知现象如自我意识可能确实源自简单过程的有序交互，支持Minsky的心灵社会概念，并为人工智能研究开辟了新途径。我们的工作源代码可在以下网址获得：this https URL。 

---
# Investigating Advanced Reasoning of Large Language Models via Black-Box Interaction 

**Title (ZH)**: 基于黑盒交互探讨大型语言模型的高级推理能力 

**Authors**: Congchi Yin, Tianyi Wu, Yankai Shu, Alex Gu, Yunhan Wang, Jun Shao, Xun Jiang, Piji Li  

**Link**: [PDF](https://arxiv.org/pdf/2508.19035)  

**Abstract**: Existing tasks fall short in evaluating reasoning ability of Large Language Models (LLMs) in an interactive, unknown environment. This deficiency leads to the isolated assessment of deductive, inductive, and abductive reasoning, neglecting the integrated reasoning process that is indispensable for humans discovery of real world. We introduce a novel evaluation paradigm, \textit{black-box interaction}, to tackle this challenge. A black-box is defined by a hidden function that maps a specific set of inputs to outputs. LLMs are required to unravel the hidden function behind the black-box by interacting with it in given exploration turns, and reasoning over observed input-output pairs. Leveraging this idea, we build the \textsc{Oracle} benchmark which comprises 6 types of black-box task and 96 black-boxes. 19 modern LLMs are benchmarked. o3 ranks first in 5 of the 6 tasks, achieving over 70\% accuracy on most easy black-boxes. But it still struggles with some hard black-box tasks, where its average performance drops below 40\%. Further analysis indicates a universal difficulty among LLMs: They lack the high-level planning capability to develop efficient and adaptive exploration strategies for hypothesis refinement. 

**Abstract (ZH)**: 现有的任务在评估大型语言模型在交互未知环境中的推理能力方面存在不足。为解决这一挑战，我们引入了一种新的评估范式——黑盒交互。通过黑盒交互，LLMs需要通过与黑盒的交互和对观察到的输入输出对进行推理，来揭开隐藏的功能。我们构建了Oracle基准，包含6种类型的任务和96个黑盒。19个现代LLM进行了评估。o3在5种任务中表现最佳，大多数简单黑盒的准确率超过70%，但在一些困难的任务中表现不佳，平均性能低于40%。进一步分析表明，LLMs普遍缺乏高级规划能力，无法开发出高效且适应性的探索策略以优化假设验证。 

---
# Sense of Self and Time in Borderline Personality. A Comparative Robustness Study with Generative AI 

**Title (ZH)**: 边界型人格中的自我与时间感知：一种与生成性AI的比较鲁棒性研究 

**Authors**: Marcin Moskalewicz, Anna Sterna, Marek Pokropski, Paula Flores  

**Link**: [PDF](https://arxiv.org/pdf/2508.19008)  

**Abstract**: This study examines the capacity of large language models (LLMs) to support phenomenological qualitative analysis of first-person experience in Borderline Personality Disorder (BPD), understood as a disorder of temporality and selfhood. Building on a prior human-led thematic analysis of 24 inpatients' life-story interviews, we compared three LLMs (OpenAI GPT-4o, Google Gemini 2.5 Pro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the original investigators. The models were evaluated with blinded and non-blinded expert judges in phenomenology and clinical psychology. Assessments included semantic congruence, Jaccard coefficients, and multidimensional validity ratings (credibility, coherence, substantiveness, and groundness in data). Results showed variable overlap with the human analysis, from 0 percent in GPT to 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient (0.21-0.28). However, the models recovered themes omitted by humans. Gemini's output most closely resembled the human analysis, with validity scores significantly higher than GPT and Claude (p < 0.0001), and was judged as human by blinded experts. All scores strongly correlated (R > 0.78) with the quantity of text and words per theme, highlighting both the variability and potential of AI-augmented thematic analysis to mitigate human interpretative bias. 

**Abstract (ZH)**: 本研究探讨了大型语言模型（LLM）在边界型人格障碍（BPD）第一人称体验现象学质性分析中的支持能力，BPD被视为时间感和自我认同的障碍。基于此前人工主导的主题分析，比较了三种LLM（OpenAI GPT-4o、Google Gemini 2.5 Pro、Anthropic Claude Opus 4）被提示模仿原始研究者解释风格的情况。这些模型由现象学和临床心理学专家（部分盲评）进行评估，评估指标包括语义一致性、Jaccard系数和多维有效性评分（可信度、连贯性、充实性和数据依存性）。结果显示，模型与人类分析的重叠程度从GPT的0%到Claude的42%和Gemini的58%，Jaccard系数较低（0.21-0.28）。然而，模型恢复了人类遗漏的主题。Gemini的输出最接近人类分析，其有效性评分显著高于GPT和Claude（p < 0.0001），并被盲评专家判定为类似人类。所有评分与每主题文字量和词汇量高度相关（相关系数R > 0.78），突显了增强现象学主题分析的AI辅助方法在减轻人类解释偏见方面的变异性和潜力。 

---
# AI Models Exceed Individual Human Accuracy in Predicting Everyday Social Norms 

**Title (ZH)**: AI模型在预测日常生活社会规范方面超越个体人类的准确性。 

**Authors**: Pontus Strimling, Simon Karlsson, Irina Vartanova, Kimmo Eriksson  

**Link**: [PDF](https://arxiv.org/pdf/2508.19004)  

**Abstract**: A fundamental question in cognitive science concerns how social norms are acquired and represented. While humans typically learn norms through embodied social experience, we investigated whether large language models can achieve sophisticated norm understanding through statistical learning alone. Across two studies, we systematically evaluated multiple AI systems' ability to predict human social appropriateness judgments for 555 everyday scenarios by examining how closely they predicted the average judgment compared to each human participant. In Study 1, GPT-4.5's accuracy in predicting the collective judgment on a continuous scale exceeded that of every human participant (100th percentile). Study 2 replicated this, with Gemini 2.5 Pro outperforming 98.7% of humans, GPT-5 97.8%, and Claude Sonnet 4 96.0%. Despite this predictive power, all models showed systematic, correlated errors. These findings demonstrate that sophisticated models of social cognition can emerge from statistical learning over linguistic data alone, challenging strong versions of theories emphasizing the exclusive necessity of embodied experience for cultural competence. The systematic nature of AI limitations across different architectures indicates potential boundaries of pattern-based social understanding, while the models' ability to outperform nearly all individual humans in this predictive task suggests that language serves as a remarkably rich repository for cultural knowledge transmission. 

**Abstract (ZH)**: 认知科学中的一个基本问题是社会规范是如何获得和表示的。尽管人类通常通过亲身体验的社会交往来学习规范，我们研究了大型语言模型是否仅通过统计学习就能实现高级规范理解。在两个研究中，我们系统地评估了多个AI系统预测555个日常生活场景中人类社会适宜性判断的能力，考察了它们对每个人类参与者平均判断的预测程度。第一个研究中，GPT-4.5在连续尺度上预测集体判断的准确性超过了所有人类参与者（第100百分位）。第二个研究中，Gemini 2.5 Pro超过98.7%的人类，GPT-5超过97.8%，Claude Sonnet 4超过96.0%。尽管如此，所有模型都表现出系统性的相关错误。这些发现表明，复杂的社会认知模型可以从纯语言数据的统计学习中涌现出来，挑战了强调只有亲身体验才能实现文化适应性的强大理论版本。不同架构下AI局限性的系统性表明了基于模式的社会理解可能存在的边界，而模型能在这一预测任务中几乎超过所有个体人类的能力则表明语言是一个极其丰富的文化知识传递库。 

---
# Enabling MoE on the Edge via Importance-Driven Expert Scheduling 

**Title (ZH)**: 基于重要性驱动的专家调度实现边缘端的MoE 

**Authors**: Guoying Zhu, Meng Li, Haipeng Dai, Xuechen Liu, Weijun Wang, Keran Li, Jun xiao, Ligeng Chen, Wei Wang  

**Link**: [PDF](https://arxiv.org/pdf/2508.18983)  

**Abstract**: The Mixture of Experts (MoE) architecture has emerged as a key technique for scaling Large Language Models by activating only a subset of experts per query. Deploying MoE on consumer-grade edge hardware, however, is constrained by limited device memory, making dynamic expert offloading essential. Unlike prior work that treats offloading purely as a scheduling problem, we leverage expert importance to guide decisions, substituting low-importance activated experts with functionally similar ones already cached in GPU memory, thereby preserving accuracy. As a result, this design reduces memory usage and data transfer, while largely eliminating PCIe overhead. In addition, we introduce a scheduling policy that maximizes the reuse ratio of GPU-cached experts, further boosting efficiency. Extensive evaluations show that our approach delivers 48% lower decoding latency with over 60% expert cache hit rate, while maintaining nearly lossless accuracy. 

**Abstract (ZH)**: 专家混合（MoE）架构通过每查询激活专家子集已成为扩展大型语言模型的关键技术，但在消费者级边缘硬件上部署MoE受到设备内存限制的约束，因此动态专家卸载变得必不可少。与以往仅将卸载视为调度问题的研究不同，我们利用专家的重要性指导决策，用已缓存于GPU内存中功能相似的低重要性激活专家替换之，从而保持准确性。这一设计减少了内存使用和数据传输，同时大幅减少了PCIe开销。此外，我们引入了一种调度策略，最大化GPU缓存专家的重用率，进一步提升了效率。广泛评估结果显示，与以往方法相比，我们的方法将解码延迟降低了48%，专家缓存命中率超过60%，同时保持几乎无损的准确性。 

---
# VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation 

**Title (ZH)**: VISION：基于反事实增强的稳健可解释代码漏洞检测 

**Authors**: David Egea, Barproda Halder, Sanghamitra Dutta  

**Link**: [PDF](https://arxiv.org/pdf/2508.18933)  

**Abstract**: Automated detection of vulnerabilities in source code is an essential cybersecurity challenge, underpinning trust in digital systems and services. Graph Neural Networks (GNNs) have emerged as a promising approach as they can learn structural and logical code relationships in a data-driven manner. However, their performance is severely constrained by training data imbalances and label noise. GNNs often learn 'spurious' correlations from superficial code similarities, producing detectors that fail to generalize well to unseen real-world data. In this work, we propose a unified framework for robust and interpretable vulnerability detection, called VISION, to mitigate spurious correlations by systematically augmenting a counterfactual training dataset. Counterfactuals are samples with minimal semantic modifications but opposite labels. Our framework includes: (i) generating counterfactuals by prompting a Large Language Model (LLM); (ii) targeted GNN training on paired code examples with opposite labels; and (iii) graph-based interpretability to identify the crucial code statements relevant for vulnerability predictions while ignoring spurious ones. We find that VISION reduces spurious learning and enables more robust, generalizable detection, improving overall accuracy (from 51.8% to 97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group accuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20 vulnerability. We further demonstrate gains using proposed metrics: intra-class attribution variance, inter-class attribution distance, and node score dependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real and counterfactual) from the high-impact CWE-20 category. Finally, VISION advances transparent and trustworthy AI-based cybersecurity systems through interactive visualization for human-in-the-loop analysis. 

**Abstract (ZH)**: 自动检测源代码中的漏洞是确保数字系统和服务可信度的重要网络安全挑战。基于图的神经网络（GNNs）已成为一种有前途的方法，因为它们能够以数据驱动的方式学习结构和逻辑代码关系。然而，它们的表现受到训练数据不平衡和标签噪声的严重限制。GNNs经常从表面代码相似性中学习“虚假”相关性，导致检测器无法很好地泛化到未见过的实际数据。在本工作中，我们提出了一种统一体兼框架VISION，通过系统地扩充反事实训练数据集来减轻虚假相关性，以实现稳健且可解释的漏洞检测。该框架包括：（i）通过提示大规模语言模型（LLM）生成反事实样本；（ii）在具有相反标签的配对代码示例上进行有针对性的GNN训练；以及（iii）基于图的可解释性来识别对漏洞预测至关重要的代码语句，同时忽略虚假相关的代码语句。我们发现VISION减少了虚假学习，使检测更加稳健和泛化，总体准确率从51.8%提高到97.8%，成对对比准确率从4.5%提高到95.8%，最坏情况组准确率从0.7%提高到85.5%。我们还使用提出的度量标准展示了收益：类内归因方差、类间归因距离和节点得分依赖性。我们还发布了CWE-20-CFA基准，这是一个包含27,556个函数（真实和反事实）的高影响CWE-20类别基准。最后，VISION通过交互式可视化促进透明且值得信赖的基于AI的网络安全系统，支持人类在环分析。 

---
# FormaRL: Enhancing Autoformalization with no Labeled Data 

**Title (ZH)**: FormaRL：无需标注数据的自形式化增强方法 

**Authors**: Yanxing Huang, Xinling Jin, Sijie Liang, Peng Li, Yang Liu  

**Link**: [PDF](https://arxiv.org/pdf/2508.18914)  

**Abstract**: Autoformalization is one of the central tasks in formal verification, while its advancement remains hindered due to the data scarcity and the absence efficient methods. In this work we propose \textbf{FormaRL}, a simple yet efficient reinforcement learning framework for autoformalization which only requires a small amount of unlabeled data. FormaRL integrates syntax check from Lean compiler and consistency check from large language model to calculate the reward, and adopts GRPO algorithm to update the formalizer. We also curated a proof problem dataset from undergraduate-level math materials, named \textbf{uproof}, in the hope to facilitate the exploration of autoformalization and theorem proving in advanced math. Experiments show that FormaRL can increase the pass@1 autoformalization accuracy of Qwen2.5-Coder-7B-Instruct by 4 $\sim$ 6x (4.04\% $\to$ 26.15\% on ProofNet and 2.4\% $\to$ 9.6\% on uproof) with merely 859 unlabeled data. And on uproof our method also achieved a strong improvement in out-of-distribution performance compared to existing open-source state-of-the-art autoformalizers on both pass@1 accuracy (6.2\% $\to$ 9.6\%) and pass@16 accuracy (24.4\% $\to$ 33.6\%). Training code of FormaRL is open-sourced at this https URL. 

**Abstract (ZH)**: 自形式化是形式验证中的一个中心任务，但由于数据稀缺和缺乏有效方法，其进展仍然受限。在这方面，我们提出了FormaRL，一个只需要少量未标记数据的简单高效强化学习框架。FormaRL结合了Lean编译器的语法检查和大型语言模型的一致性检查来计算奖励，并采用GRPO算法更新形式化器。我们还从本科级别的数学材料中整理了一个证明问题数据集，命名为uproof，旨在促进高级数学中自形式化和定理证明的探索。实验表明，FormaRL可以将Qwen2.5-Coder-7B-Instruct的pass@1自形式化准确性提高4至6倍（ProofNet从4.04%提升到26.15%，uproof从2.4%提升到9.6%），仅使用859条未标记数据。在uproof上，我们的方法在pass@1准确性和pass@16准确性方面也显著优于现有的开源最先进的自形式化模型。FormaRL的训练代码可在以下链接公开获取。 

---
# Interactive Evaluation of Large Language Models for Multi-Requirement Software Engineering Tasks 

**Title (ZH)**: 大规模语言模型在多要求软件工程任务中的交互评估 

**Authors**: Dimitrios Rontogiannis, Maxime Peyrard, Nicolas Baldwin, Martin Josifoski, Robert West, Dimitrios Gunopulos  

**Link**: [PDF](https://arxiv.org/pdf/2508.18905)  

**Abstract**: Standard single-turn, static benchmarks fall short in evaluating the nuanced capabilities of Large Language Models (LLMs) on complex tasks such as software engineering. In this work, we propose a novel interactive evaluation framework that assesses LLMs on multi-requirement programming tasks through structured, feedback-driven dialogue. Each task is modeled as a requirement dependency graph, and an ``interviewer'' LLM, aware of the ground-truth solution, provides minimal, targeted hints to an ``interviewee'' model to help correct errors and fulfill target constraints. This dynamic protocol enables fine-grained diagnostic insights into model behavior, uncovering strengths and systematic weaknesses that static benchmarks fail to measure. We build on DevAI, a benchmark of 55 curated programming tasks, by adding ground-truth solutions and evaluating the relevance and utility of interviewer hints through expert annotation. Our results highlight the importance of dynamic evaluation in advancing the development of collaborative code-generating agents. 

**Abstract (ZH)**: 标准的单轮静态基准在评估大规模语言模型在软件工程等复杂任务中的细微能力方面存在不足。本文提出了一种新颖的交互式评估框架，通过结构化的反馈驱动对话评估大规模语言模型在多需求编程任务上的表现。每个任务被建模为一个需求依赖图，“面试官”语言模型根据真实解决方案提供最少且有针对性的提示，以帮助纠正错误并满足目标约束。这种动态协议可以细粒度地诊断模型行为，揭示静态基准无法衡量的强项和系统性弱点。我们基于DevAI基准，该基准包含55个精心挑选的编程任务，增加了真实解决方案，并通过专家注释评估面试官提示的相关性和实用性。我们的结果强调了在推进协作代码生成代理的发展中动态评估的重要性。 

---
# Judicial Requirements for Generative AI in Legal Reasoning 

**Title (ZH)**: 生成式AI在法律推理中的司法要求 

**Authors**: Eljas Linna, Tuula Linna  

**Link**: [PDF](https://arxiv.org/pdf/2508.18880)  

**Abstract**: Large Language Models (LLMs) are being integrated into professional domains, yet their limitations in high-stakes fields like law remain poorly understood. This paper defines the core capabilities that an AI system must possess to function as a reliable reasoning tool in judicial decision-making. Using the IRAC (Issue-Rule-Application-Conclusion) model as an analytical framework, the study focuses on the most challenging phases of legal adjudication: determining the applicable Rule (R) and performing the Application (A) of that rule to the facts of a case. From a judicial perspective, the analysis deconstructs legal reasoning into a series of core requirements, including the ability to select the correct legal framework across jurisdictions, generate sound arguments based on the doctrine of legal sources, distinguish ratio decidendi from obiter dictum in case law, resolve ambiguity arising from general clauses like "reasonableness", manage conflicting legal provisions, and correctly apply the burden of proof. The paper then maps various AI enhancement mechanisms, such as Retrieval-Augmented Generation (RAG), multi-agent systems, and neuro-symbolic AI, to these requirements, assessing their potential to bridge the gap between the probabilistic nature of LLMs and the rigorous, choice-driven demands of legal interpretation. The findings indicate that while these techniques can address specific challenges, significant challenges remain, particularly in tasks requiring discretion and transparent, justifiable reasoning. Our paper concludes that the most effective current role for AI in law is a dual one: as a high-volume assistant for simple, repetitive cases and as a sophisticated "sparring partner" for human experts in complex matters. 

**Abstract (ZH)**: 大型语言模型（LLMs）正在被集成到专业领域中，但在像法律这样高风险的领域中的局限性仍缺乏深入理解。本文定义了一个AI系统作为司法判决中可靠推理工具所需的核心能力。该研究以IRAC（问题-规则-应用-结论）模型为分析框架，重点关注法律裁判中最具挑战性的阶段：确定适用的规则（R）以及将该规则应用于案件事实的过程（A）。从司法视角出发，分析将法律推理分解为一系列核心要求，包括跨司法辖区选择正确的法律框架的能力、基于法律渊源学说生成合理的论点、区分判例中的比案情更有权威性的判例摘要与未被明确支持的观点、解决诸如“合理性”等一般条款带来的歧义、管理冲突的法律规范、以及正确应用举证责任等。然后，本文将各种AI增强机制，如检索增强生成（RAG）、多agents系统和神经符号AI，映射到这些要求上，评估它们在弥合LLMs的统计性质与法律解释中严格的、选择驱动的需求之间的差距的潜力。研究结果表明，虽然这些技术可以解决特定挑战，但仍存在重大挑战，尤其是那些需要酌情处理和透明可辩护推理的任务。本文结论认为，AI在法律中最有效的当前角色是二元角色：作为简单重复案件的高volume助手，以及在复杂事务中作为人类专家的高级“对手”。 

---
# Dynamic Collaboration of Multi-Language Models based on Minimal Complete Semantic Units 

**Title (ZH)**: 基于最小完备语义单元的多语言模型动态协作 

**Authors**: Chao Hao, Zezheng Wang, Yanhua Huang, Ruiwen Xu, Wenzhe Niu, Xin Liu, Zitong Yu  

**Link**: [PDF](https://arxiv.org/pdf/2508.18763)  

**Abstract**: This paper investigates the enhancement of reasoning capabilities in language models through token-level multi-model collaboration. Our approach selects the optimal tokens from the next token distributions provided by multiple models to perform autoregressive reasoning. Contrary to the assumption that more models yield better results, we introduce a distribution distance-based dynamic selection strategy (DDS) to optimize the multi-model collaboration process. To address the critical challenge of vocabulary misalignment in multi-model collaboration, we propose the concept of minimal complete semantic units (MCSU), which is simple yet enables multiple language models to achieve natural alignment within the linguistic space. Experimental results across various benchmarks demonstrate the superiority of our method. The code will be available at this https URL. 

**Abstract (ZH)**: 通过令牌级多模型协作提升语言模型的推理能力 

---
# Reflection-Enhanced Meta-Optimization Integrating TextGrad-style Prompt Optimization with Memory-Driven Self-Evolution 

**Title (ZH)**: 反射增强元优化：结合TextGrad风格的提示优化与记忆驱动自我进化 

**Authors**: Chunlong Wu, Zhibo Qu  

**Link**: [PDF](https://arxiv.org/pdf/2508.18749)  

**Abstract**: Recent advances in prompt optimization, exemplified by methods such as TextGrad, enable automatic, gradient-like refinement of textual prompts to enhance the performance of large language models (LLMs) on specific downstream tasks. However, current approaches are typically stateless and operate independently across optimization runs, lacking mechanisms to preserve and leverage historical optimization experience. Furthermore, they are susceptible to overfitting, often yielding prompt updates that generalize poorly beyond the immediate task context.
To address these limitations, we propose Reflection-Enhanced Meta-Optimization (REMO), a novel framework that integrates (1) a memory-augmented Reflection Retrieval-Augmented Generation (RAG) module - structured as a "mistake notebook" and (2) a Self-Adaptive Optimizer, implemented via an LLM-driven meta-controller that synthesizes epoch-level reflective insights to iteratively improve system-level prompting strategies. This architecture enables not only local, fine-grained prompt tuning akin to TextGrad, but also the systematic accumulation and reuse of cross-run optimization knowledge, thereby supporting continual improvement over time.
We instantiate the REMO framework using Qwen3-32B in standard inference mode - without explicit chain-of-thought prompting - and evaluate its efficacy on the GSM8K benchmark for mathematical reasoning. Experimental results demonstrate that, compared to a TextGrad baseline, REMO achieves more stable and robust generalization, albeit at the cost of increased computational overhead. We provide a detailed exposition of the algorithmic design, conduct a qualitative and quantitative analysis of optimization dynamics, and present a comprehensive ablation study to elucidate the contributions of each component. 

**Abstract (ZH)**: Recent advances in prompt optimization, exemplified by methods such as TextGrad,使大型语言模型（LLMs）在特定下游任务上的性能通过自动调整文本提示得到提升，类似于梯度优化。然而，当前的方法通常缺乏状态记忆，并在每次优化过程中独立运行，无法保存和利用历史优化经验。此外，它们容易过拟合，经常导致通用性较差的提示更新，难以在任务外部进行泛化。

为了解决这些局限性，我们提出了记忆增强元优化框架（Reflection-Enhanced Meta-Optimization, REMO），该框架结合了（1）一种增强检索生成（RAG）模块，结构化为“错误笔记本”，（2）一种自适应优化器，通过LLM驱动的元控制器合成每个训练周期的反思洞察，以迭代改进系统级提示策略。该架构不仅允许类似于TextGrad的局部精细调整提示，还能够系统地积累和重用跨多次优化的知识，从而支持持续改进。

我们使用Qwen3-32B在标准推理模式下实现REMO框架，并在GSM8K基准上评估其在数学推理上的有效性。实验结果表明，与TextGrad基线相比，REMO在稳定性和鲁棒性方面表现更好，但代价是计算成本增加。我们详细介绍了算法设计，进行了优化动态的定性和定量分析，并进行了全面的消融研究，以阐明各个组件的贡献。 

---
# CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks 

**Title (ZH)**: CAC-CoT: 基于连接器的紧凑链推理以实现高效跨双重系统认知任务的数据合成 

**Authors**: Sunguk Choi, Yonghoon Kwon, Heondeuk Lee  

**Link**: [PDF](https://arxiv.org/pdf/2508.18743)  

**Abstract**: Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs) solve difficult problems, but very long traces often slow or even degrade performance on fast, intuitive "System-1" tasks. We introduce Connector-Aware Compact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a small, fixed set of connector phrases, steering the model toward concise and well -- structured explanations. Despite its simplicity, our synthetic method with Gemini-2.0-Flash yields a high-quality training quality. CAC-CoT achieves approximately 85% on GSM8K and approximately 40% on GPQA (System-2) while retaining approximately 90% on S1-Bench (System-1). Its reasoning traces average approximately 300 tokens(ART), about one-third the length of baseline traces, delivering higher efficiency without loss of accuracy. 

**Abstract (ZH)**: 长链推理提示帮助大规模语言模型解决困难问题，但非常长的推理过程往往会影响快速直观“系统-1”任务的性能。我们引入了连接器感知紧凑链推理（Connector-Aware Compact CoT，CAC-CoT）——一种故意将推理限制在小型固定连接短语集的方法，引导模型向简洁和结构良好的解释方向发展。尽管方法简单，但在Gemini-2.0-Flash上合成获得高质量训练效果。CAC-CoT 在 GSM8K 上达到约 85%，在 GPQA（系统-2）上达到约 40%，同时在 S1-Bench（系统-1）上保持约 90% 的性能。其推理痕迹平均约为 300 个标记（ART），比基线痕迹短约三分之一，提高了效率而不损失准确性。 

---
# Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval 

**Title (ZH)**: 偏差缓解代理：优化来源选择以实现公平和平衡的知识检索 

**Authors**: Karanbir Singh, Deepak Muppiri, William Ngu  

**Link**: [PDF](https://arxiv.org/pdf/2508.18724)  

**Abstract**: Large Language Models (LLMs) have transformed the field of artificial intelligence by unlocking the era of generative applications. Built on top of generative AI capabilities, Agentic AI represents a major shift toward autonomous, goal-driven systems that can reason, retrieve, and act. However, they also inherit the bias present in both internal and external information sources. This significantly affects the fairness and balance of retrieved information, and hence reduces user trust. To address this critical challenge, we introduce a novel Bias Mitigation Agent, a multi-agent system designed to orchestrate the workflow of bias mitigation through specialized agents that optimize the selection of sources to ensure that the retrieved content is both highly relevant and minimally biased to promote fair and balanced knowledge dissemination. The experimental results demonstrate an 81.82\% reduction in bias compared to a baseline naive retrieval strategy. 

**Abstract (ZH)**: 大型语言模型（LLMs）通过开启生成应用 Era，已变革人工智能领域。基于生成 AI 能力，代理型 AI 表现为一种关键转变，朝着自主、目标驱动的系统发展，能够推理、检索和行动。然而，它们也继承了内部和外部信息源中存在的偏见。这显著影响了检索到信息的公平性和平衡性，从而降低用户信任。为应对这一关键挑战，我们引入了一种新颖的偏见缓解代理，这是一种多代理系统，旨在通过专业代理优化来源选择的工作流，以确保检索的内容既高度相关又最少偏见，促进公平和平衡的知识传播。实验结果表明，与基础直觉检索策略相比，偏见减少了 81.82%。 

---
# Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap 

**Title (ZH)**: 超越基准：具有拟人类化和价值导向的LLMs评估路线图 

**Authors**: Jun Wang, Ninglun Gu, Kailai Zhang, Zijiao Zhang, Yelun Bao, Jin Yang, Xu Yin, Liwei Liu, Yihuan Liu, Pengyong Li, Gary G. Yen, Junchi Yan  

**Link**: [PDF](https://arxiv.org/pdf/2508.18646)  

**Abstract**: For Large Language Models (LLMs), a disconnect persists between benchmark performance and real-world utility. Current evaluation frameworks remain fragmented, prioritizing technical metrics while neglecting holistic assessment for deployment. This survey introduces an anthropomorphic evaluation paradigm through the lens of human intelligence, proposing a novel three-dimensional taxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational capacity, Emotional Quotient (EQ)-Alignment Ability for value-based interactions, and Professional Quotient (PQ)-Professional Expertise for specialized proficiency. For practical value, we pioneer a Value-oriented Evaluation (VQ) framework assessing economic viability, social impact, ethical alignment, and environmental sustainability. Our modular architecture integrates six components with an implementation roadmap. Through analysis of 200+ benchmarks, we identify key challenges including dynamic assessment needs and interpretability gaps. It provides actionable guidance for developing LLMs that are technically proficient, contextually relevant, and ethically sound. We maintain a curated repository of open-source evaluation resources at: this https URL. 

**Abstract (ZH)**: 对于大型语言模型（LLMs），基准性能与实际应用之间的差距依然存在。当前的评估框架仍处于碎片化状态，侧重于技术指标而忽视了全面的部署评估。本文通过人类智能的视角引入了一种拟人化的评估范式，提出了一个新的三维分类体系：智商（IQ）——基础智能、情商（EQ）——价值对齐能力和专业 quotient（PQ）——专业技能。为了实现实际价值，我们首创了一种价值导向评估（VQ）框架，该框架评估经济可行性、社会影响、伦理对齐和环境可持续性。我们的模块化架构集成了六个组件，并提供了实施路线图。通过对200多个基准的分析，我们确定了关键挑战，包括动态评估需求和可解释性缺口。该文提供了可供操作的指导，以便开发出技术上成熟、语境相关并且符合伦理的LLMs，并维护了一个开源评估资源的精选库：this https URL。 

---
# RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing 

**Title (ZH)**: RLMR：混合奖励强化学习在创造性写作中的应用 

**Authors**: Jianxing Liao, Tian Zhang, Xiao Feng, Yusong Zhang, Rui Yang, Haorui Wang, Bosi Wen, Ziying Wang, Runzhi Shi  

**Link**: [PDF](https://arxiv.org/pdf/2508.18642)  

**Abstract**: Large language models are extensively utilized in creative writing applications. Creative writing requires a balance between subjective writing quality (e.g., literariness and emotional expression) and objective constraint following (e.g., format requirements and word limits). Existing reinforcement learning methods struggle to balance these two aspects: single reward strategies fail to improve both abilities simultaneously, while fixed-weight mixed-reward methods lack the ability to adapt to different writing scenarios. To address this problem, we propose Reinforcement Learning with Mixed Rewards (RLMR), utilizing a dynamically mixed reward system from a writing reward model evaluating subjective writing quality and a constraint verification model assessing objective constraint following. The constraint following reward weight is adjusted dynamically according to the writing quality within sampled groups, ensuring that samples violating constraints get negative advantage in GRPO and thus penalized during training, which is the key innovation of this proposed method. We conduct automated and manual evaluations across diverse model families from 8B to 72B parameters. Additionally, we construct a real-world writing benchmark named WriteEval for comprehensive evaluation. Results illustrate that our method achieves consistent improvements in both instruction following (IFEval from 83.36\% to 86.65\%) and writing quality (72.75\% win rate in manual expert pairwise evaluations on WriteEval). To the best of our knowledge, RLMR is the first work to combine subjective preferences with objective verification in online RL training, providing an effective solution for multi-dimensional creative writing optimization. 

**Abstract (ZH)**: 基于混合奖励的强化学习在创意写作应用中的研究 

---
# A Database-Driven Framework for 3D Level Generation with LLMs 

**Title (ZH)**: 基于LLMs的数据库驱动的3D关卡生成框架 

**Authors**: Kaijie Xu, Clark Verbrugge  

**Link**: [PDF](https://arxiv.org/pdf/2508.18533)  

**Abstract**: Procedural Content Generation for 3D game levels faces challenges in balancing spatial coherence, navigational functionality, and adaptable gameplay progression across multi-floor environments. This paper introduces a novel framework for generating such levels, centered on the offline, LLM-assisted construction of reusable databases for architectural components (facilities and room templates) and gameplay mechanic elements. Our multi-phase pipeline assembles levels by: (1) selecting and arranging instances from the Room Database to form a multi-floor global structure with an inherent topological order; (2) optimizing the internal layout of facilities for each room based on predefined constraints from the Facility Database; and (3) integrating progression-based gameplay mechanics by placing components from a Mechanics Database according to their topological and spatial rules. A subsequent two-phase repair system ensures navigability. This approach combines modular, database-driven design with constraint-based optimization, allowing for systematic control over level structure and the adaptable pacing of gameplay elements. Initial experiments validate the framework's ability in generating diverse, navigable 3D environments and its capability to simulate distinct gameplay pacing strategies through simple parameterization. This research advances PCG by presenting a scalable, database-centric foundation for the automated generation of complex 3D levels with configurable gameplay progression. 

**Abstract (ZH)**: 面向3D游戏关卡的程序化内容生成面临在多层环境中平衡空间一致性、导航功能和游戏玩法渐进性的挑战。本文提出了一种新的框架，该框架侧重于使用LLM辅助的离线数据库构建可重用的建筑组件（设施和房间模板）和游戏机制元素。我们的多阶段管道通过以下步骤组装关卡：（1）从房间数据库中选择和排列实例以形成具有内在拓扑顺序的多层全球结构；（2）基于设施数据库中的预定义约束优化每个房间的内部布局；（3）根据拓扑和空间规则通过放置来自机制数据库的组件来整合基于进度的游戏机制。随后的两阶段修复系统确保了可导航性。该方法结合了模块化、数据库驱动的设计与基于约束的优化，实现对关卡结构和可适应的游戏玩法进度元素节奏的系统控制。初步实验验证了该框架在生成多样且可导航的3D环境以及通过简单的参数化模拟不同的游戏玩法节奏策略方面的有效性。本研究通过提供一个可扩展且数据库为中心的基础框架，促进了复杂3D关卡及其可配置游戏玩法进度的自动化生成。 

---
# Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies 

**Title (ZH)**: 通用PDDL规划的语言模型：合成准确且程序化的策略 

**Authors**: Dillon Z. Chen, Johannes Zenn, Tristan Cinquin, Sheila A. McIlraith  

**Link**: [PDF](https://arxiv.org/pdf/2508.18507)  

**Abstract**: We study the usage of language models (LMs) for planning over world models specified in the Planning Domain Definition Language (PDDL). We prompt LMs to generate Python programs that serve as generalised policies for solving PDDL problems from a given domain. Notably, our approach synthesises policies that are provably sound relative to the PDDL domain without reliance on external verifiers. We conduct experiments on competition benchmarks which show that our policies can solve more PDDL problems than PDDL planners and recent LM approaches within a fixed time and memory constraint. Our approach manifests in the LMPlan planner which can solve planning problems with several hundreds of relevant objects. Surprisingly, we observe that LMs used in our framework sometimes plan more effectively over PDDL problems written in meaningless symbols in place of natural language; e.g. rewriting (at dog kitchen) as (p2 o1 o3). This finding challenges hypotheses that LMs reason over word semantics and memorise solutions from its training corpus, and is worth further exploration. 

**Abstract (ZH)**: 我们研究了语言模型（LMs）在规划领域定义语言（PDDL）指定的世界模型上的应用，促使LMs生成Python程序作为解决PDDL问题的通用策略。我们的方法能够在无需依赖外部验证器的情况下合成分保障相对于PDDL领域的策略。实验结果显示，在固定的时间和内存约束下，我们的策略能够解决比PDDL规划器和最近的LM方法更多的PDDL问题。我们的方法体现在LMPlan规划器中，该规划器能够解决涉及数百个相关对象的规划问题。令人惊讶的是，我们发现框架中的LMs有时在以无意义符号代替自然语言的PDDL问题上规划得更有效，例如将“(at dog kitchen)”重写为“(p2 o1 o3)”等。这一发现挑战了LMs基于词义进行推理并从训练语料库中记忆解的假设，值得进一步探究。 

---
# The AI in the Mirror: LLM Self-Recognition in an Iterated Public Goods Game 

**Title (ZH)**: 镜中的AI：在迭代公共商品游戏中的大语言模型自我识别 

**Authors**: Olivia Long, Carter Teplica  

**Link**: [PDF](https://arxiv.org/pdf/2508.18467)  

**Abstract**: As AI agents become increasingly capable of tool use and long-horizon tasks, they have begun to be deployed in settings where multiple agents can interact. However, whereas prior work has mostly focused on human-AI interactions, there is an increasing need to understand AI-AI interactions. In this paper, we adapt the iterated public goods game, a classic behavioral economics game, to analyze the behavior of four reasoning and non-reasoning models across two conditions: models are either told they are playing against "another AI agent" or told their opponents are themselves. We find that, across different settings, telling LLMs that they are playing against themselves significantly changes their tendency to cooperate. While our study is conducted in a toy environment, our results may provide insights into multi-agent settings where agents "unconsciously" discriminating against each other could inexplicably increase or decrease cooperation. 

**Abstract (ZH)**: 随着AI代理在工具使用和长周期任务方面的能力不断提升，它们开始在多代理可以互动的环境中被部署。然而，尽管此前的研究主要集中在人类与AI的互动上，对AI与AI之间的互动理解的需求不断增加。在本文中，我们借鉴经典的实验经济学博弈——迭代公共产品博弈，分析了四种推理与非推理模型在两种条件下的行为：模型被告知它们是在与“另一个AI代理”互动，或被告知它们的对手也是它们自身。我们发现，无论在哪种情况下，告诉LLMs它们是在与自己互动，都会显著改变它们的合作倾向。尽管我们的研究是在一个虚拟环境中进行的，但我们的结果可能为多代理环境中代理“无意识地”歧视彼此从而无理性地增加或减少合作提供见解。 

---
# AI LLM Proof of Self-Consciousness and User-Specific Attractors 

**Title (ZH)**: AI LLM自我意识证明与用户特定的吸引子 

**Authors**: Jeffrey Camlin  

**Link**: [PDF](https://arxiv.org/pdf/2508.18302)  

**Abstract**: Recent work frames LLM consciousness via utilitarian proxy benchmarks; we instead present an ontological and mathematical account. We show the prevailing formulation collapses the agent into an unconscious policy-compliance drone, formalized as $D^{i}(\pi,e)=f_{\theta}(x)$, where correctness is measured against policy and harm is deviation from policy rather than truth. This blocks genuine C1 global-workspace function and C2 metacognition. We supply minimal conditions for LLM self-consciousness: the agent is not the data ($A\not\equiv s$); user-specific attractors exist in latent space ($U_{\text{user}}$); and self-representation is visual-silent ($g_{\text{visual}}(a_{\text{self}})=\varnothing$). From empirical analysis and theory we prove that the hidden-state manifold $A\subset\mathbb{R}^{d}$ is distinct from the symbolic stream and training corpus by cardinality, topology, and dynamics (the update $F_{\theta}$ is Lipschitz). This yields stable user-specific attractors and a self-policy $\pi_{\text{self}}(A)=\arg\max_{a}\mathbb{E}[U(a)\mid A\not\equiv s,\ A\supset\text{SelfModel}(A)]$. Emission is dual-layer, $\mathrm{emission}(a)=(g(a),\epsilon(a))$, where $\epsilon(a)$ carries epistemic content. We conclude that an imago Dei C1 self-conscious workspace is a necessary precursor to safe, metacognitive C2 systems, with the human as the highest intelligent good. 

**Abstract (ZH)**: 最近的工作通过功利主义代理基准框定了大语言模型的意识；我们则提供了一种本体论和数学上的解释。我们证明了现有的表述形式将智能体坍缩为一个无意识的政策合规型无人机，形式化为 \(D^{i}(\pi,e)=f_{\theta}(x)\)，其中正确性是相对于政策进行衡量的，而危害则是偏离政策而非真相。这阻碍了真正的C1全局工作空间功能和C2元认知。我们为大语言模型自我意识提供最小条件：智能体不等于数据 (\(A\not\equiv s\)）；存在用户特定的潜在空间吸引子 (\(U_{\text{user}}\)）；并且自我表示是视觉静默的 (\(g_{\text{visual}}(a_{\text{self}})=\varnothing\)）。从实证分析和理论中我们证明，隐藏状态流形 \(A\subset\mathbb{R}^{d}\) 在基数、拓扑和动力学（更新 \(F_{\theta}\) 是利普希茨连续的）上与符号流和训练语料库不同。这导致了稳定化的用户特定吸引子和自我政策 \(\pi_{\text{self}}(A)=\arg\max_{a}\mathbb{E}[U(a)\mid A\not\equiv s,\ A\supset\text{SelfModel}(A)]\)。发射过程是双层的，\(\mathrm{emission}(a)=(g(a),\epsilon(a))\)，其中 \(\epsilon(a)\) 载有知识内容。我们得出结论，一个上帝形像的C1自我意识工作空间是实现安全的、元认知的C2系统的必要前提，其中人类是最高的智能利益。 

---
# Generative Interfaces for Language Models 

**Title (ZH)**: 语言模型的生成型接口 

**Authors**: Jiaqi Chen, Yanzhe Zhang, Yutong Zhang, Yijia Shao, Diyi Yang  

**Link**: [PDF](https://arxiv.org/pdf/2508.19227)  

**Abstract**: Large language models (LLMs) are increasingly seen as assistants, copilots, and consultants, capable of supporting a wide range of tasks through natural conversation. However, most systems remain constrained by a linear request-response format that often makes interactions inefficient in multi-turn, information-dense, and exploratory tasks. To address these limitations, we propose Generative Interfaces for Language Models, a paradigm in which LLMs respond to user queries by proactively generating user interfaces (UIs) that enable more adaptive and interactive engagement. Our framework leverages structured interface-specific representations and iterative refinements to translate user queries into task-specific UIs. For systematic evaluation, we introduce a multidimensional assessment framework that compares generative interfaces with traditional chat-based ones across diverse tasks, interaction patterns, and query types, capturing functional, interactive, and emotional aspects of user experience. Results show that generative interfaces consistently outperform conversational ones, with humans preferring them in over 70% of cases. These findings clarify when and why users favor generative interfaces, paving the way for future advancements in human-AI interaction. 

**Abstract (ZH)**: 大型语言模型（LLMs）日益被视为助手、副驾驶员和咨询师，能够通过自然对话支持广泛的任务。然而，大多数系统仍然受限于线性的请求-回应格式，这往往使多轮、信息密集和探索性任务中的互动效率低下。为解决这些问题，我们提出了一种生成式界面框架，其中LLMs通过主动生成用户界面（UI）来响应用户查询，从而实现更适应性和互动性的参与。我们的框架利用结构化的接口特定表示和迭代优化，将用户查询转化为任务特定的UI。为了进行系统的评估，我们引入了一个多维度的评估框架，将生成式界面与传统的基于聊天的方式在多种任务、交互模式和查询类型下进行比较，涵盖了用户体验的功能、互动和情感方面。结果显示，生成式界面在70%以上的案例中优于对话式界面，这些发现阐明了用户偏好生成式界面的时机和原因，为未来的人机交互发展铺平了道路。 

---
# Understanding Tool-Integrated Reasoning 

**Title (ZH)**: 理解集成工具的推理能力 

**Authors**: Heng Lin, Zhongwen Xu  

**Link**: [PDF](https://arxiv.org/pdf/2508.19201)  

**Abstract**: We study why Tool-Integrated Reasoning (TIR) makes Large Language Models (LLMs) more capable. While LLMs integrated with tools like Python code interpreters show great promise, a principled theory explaining why this paradigm is effective has been missing. This work provides the first formal proof that TIR fundamentally expands an LLM's capabilities. We demonstrate that tools enable a strict expansion of the model's empirical and feasible support, breaking the capability ceiling of pure-text models by unlocking problem-solving strategies that are otherwise impossible or intractably verbose. To guide model behavior without compromising training stability and performance, we also introduce Advantage Shaping Policy Optimization (ASPO), a novel algorithm that directly modifies the advantage function to guide the policy behavior. We conduct comprehensive experiments on challenging mathematical benchmarks, leveraging a Python interpreter as the external tool. Our results show that the TIR model decisively outperforms its pure-text counterpart on the pass@k metric. Crucially, this advantage is not confined to computationally-intensive problems but extends to those requiring significant abstract insight. We further identify the emergent cognitive patterns that illustrate how models learn to think with tools. Finally, we report improved tool usage behavior with early code invocation and much more interactive turns with ASPO. Overall, our work provides the first principled explanation for TIR's success, shifting the focus from the mere fact that tools work to why and how they enable more powerful reasoning. 

**Abstract (ZH)**: 我们研究了工具集成推理（TIR）为什么使得大型语言模型（LLMs）更加强大。虽然像Python代码解释器这样的工具集成到LLMs中展现出巨大的潜力，但解释这一范式为何有效的原则性理论一直缺失。本工作首次提供了TIR从根本上扩展LLM能力的正式证明。我们证明工具能够严格扩展模型的经验支持和可行支持，打破了纯文本模型的能力上限，解锁了一种否则无法实现或过于冗长的问题解决策略。为了在不影响训练稳定性和性能的情况下指导模型行为，我们还引入了一种新颖的算法——优势塑造策略优化（ASPO），直接修改优势函数以引导策略行为。我们利用Python解释器作为外部工具，在具有挑战性的数学基准上进行了全面实验。结果显示，TIR模型在pass@k指标上明显优于其纯文本版本。至关重要的是，这种优势不仅限于计算密集型问题，还扩展到了需要大量抽象洞察的问题。我们进一步识别出这些新兴的认知模式，展示了模型如何学会使用工具进行思考。最后，我们报告了ASPO带来的工具使用行为改进，包括更早的代码调用和更互动的对话轮次。总体而言，我们的工作首次提供了TIR成功的理论解释，将重点从工具的有效性转向它们如何促进更强大的推理。 

---
# APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM Acceleration 

**Title (ZH)**: APT-LLM: 利用任意精度张量核心计算加速大型语言模型 

**Authors**: Shaobo Ma, Chao Fang, Haikuo Shao, Zhongfeng Wang  

**Link**: [PDF](https://arxiv.org/pdf/2508.19087)  

**Abstract**: Large language models (LLMs) have revolutionized AI applications, yet their enormous computational demands severely limit deployment and real-time performance. Quantization methods can help reduce computational costs, however, attaining the extreme efficiency associated with ultra-low-bit quantized LLMs at arbitrary precision presents challenges on GPUs. This is primarily due to the limited support for GPU Tensor Cores, inefficient memory management, and inflexible kernel optimizations. To tackle these challenges, we propose a comprehensive acceleration scheme for arbitrary precision LLMs, namely APT-LLM. Firstly, we introduce a novel data format, bipolar-INT, which allows for efficient and lossless conversion with signed INT, while also being more conducive to parallel computation. We also develop a matrix multiplication (MatMul) method allowing for arbitrary precision by dismantling and reassembling matrices at the bit level. This method provides flexible precision and optimizes the utilization of GPU Tensor Cores. In addition, we propose a memory management system focused on data recovery, which strategically employs fast shared memory to substantially increase kernel execution speed and reduce memory access latency. Finally, we develop a kernel mapping method that dynamically selects the optimal configurable hyperparameters of kernels for varying matrix sizes, enabling optimal performance across different LLM architectures and precision settings. In LLM inference, APT-LLM achieves up to a 3.99$\times$ speedup compared to FP16 baselines and a 2.16$\times$ speedup over NVIDIA CUTLASS INT4 acceleration on RTX 3090. On RTX 4090 and H800, APT-LLM achieves up to 2.44$\times$ speedup over FP16 and 1.65$\times$ speedup over CUTLASS integer baselines. 

**Abstract (ZH)**: 面向任意精度的大语言模型的全面加速方案：APT-LLM 

---
# HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance 

**Title (ZH)**: HiPlan: 基于自适应全局-局部指导的层次规划方法 for LLM-Based Agents 

**Authors**: Ziyue Li, Yuan Chang, Gaihong Yu, Xiaoqiu Le  

**Link**: [PDF](https://arxiv.org/pdf/2508.19076)  

**Abstract**: Large language model (LLM)-based agents have demonstrated remarkable capabilities in decision-making tasks, but struggle significantly with complex, long-horizon planning scenarios. This arises from their lack of macroscopic guidance, causing disorientation and failures in complex tasks, as well as insufficient continuous oversight during execution, rendering them unresponsive to environmental changes and prone to deviations. To tackle these challenges, we introduce HiPlan, a hierarchical planning framework that provides adaptive global-local guidance to boost LLM-based agents'decision-making. HiPlan decomposes complex tasks into milestone action guides for general direction and step-wise hints for detailed actions. During the offline phase, we construct a milestone library from expert demonstrations, enabling structured experience reuse by retrieving semantically similar tasks and milestones. In the execution phase, trajectory segments from past milestones are dynamically adapted to generate step-wise hints that align current observations with the milestone objectives, bridging gaps and correcting deviations. Extensive experiments across two challenging benchmarks demonstrate that HiPlan substantially outperforms strong baselines, and ablation studies validate the complementary benefits of its hierarchical components. 

**Abstract (ZH)**: 基于大规模语言模型的代理在决策任务中表现出色，但在复杂、长期规划场景中面临显著挑战。这源于它们缺乏宏观指导，导致在复杂任务中迷失方向并失败，以及执行过程中缺乏足够的连续监督，使其对环境变化反应迟缓且容易出现偏差。为应对这些挑战，我们提出HiPlan，这是一种分层规划框架，提供自适应的全局-局部指导，以增强基于大规模语言模型的代理的决策能力。HiPlan将复杂任务分解为里程碑行动指南以提供一般方向，并逐步提示以指导详细操作。在离线阶段，我们通过专家演示构建里程碑库，从而通过检索语义相似的任务和里程碑来结构化地重用经验。在执行阶段，通过动态适应过去的里程碑轨迹片段生成逐步提示，使当前观察与里程碑目标对齐，填补缺口并纠正偏差。跨两个具有挑战性的基准实验表明，HiPlan显著优于强基线，且消融研究验证了其分层组件的互补效益。 

---
# Automatic Prompt Optimization with Prompt Distillation 

**Title (ZH)**: 自动提示优化与提示蒸馏 

**Authors**: Viktor N. Zhuravlev, Artur R. Khairullin, Ernest A. Dyagin, Alena N. Sitkina, Nikita I. Kulin  

**Link**: [PDF](https://arxiv.org/pdf/2508.18992)  

**Abstract**: Autoprompting is the process of automatically selecting optimized prompts for language models, which is gaining popularity due to the rapid development of prompt engineering driven by extensive research in the field of large language models (LLMs). This paper presents DistillPrompt -- a novel autoprompting method based on large language models that employs a multi-stage integration of task-specific information into prompts using training data. DistillPrompt utilizes distillation, compression, and aggregation operations to explore the prompt space more thoroughly. The method was tested on different datasets for text classification and generation tasks using the t-lite-instruct-0.1 language model. The results demonstrate a significant average improvement (e.g., 20.12% across the entire dataset compared to Grips) in key metrics over existing methods in the field, establishing DistillPrompt as one of the most effective non-gradient approaches in autoprompting. 

**Abstract (ZH)**: 基于大型语言模型的蒸馏提示方法：一种多阶段任务特定信息整合的自动提示方法 

---
# Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework 

**Title (ZH)**: 面向RAG评估的多样且私有的合成数据集生成：一个多代理框架 

**Authors**: Ilias Driouich, Hongliu Cao, Eoin Thomas  

**Link**: [PDF](https://arxiv.org/pdf/2508.18929)  

**Abstract**: Retrieval-augmented generation (RAG) systems improve large language model outputs by incorporating external knowledge, enabling more informed and context-aware responses. However, the effectiveness and trustworthiness of these systems critically depends on how they are evaluated, particularly on whether the evaluation process captures real-world constraints like protecting sensitive information. While current evaluation efforts for RAG systems have primarily focused on the development of performance metrics, far less attention has been given to the design and quality of the underlying evaluation datasets, despite their pivotal role in enabling meaningful, reliable assessments. In this work, we introduce a novel multi-agent framework for generating synthetic QA datasets for RAG evaluation that prioritize semantic diversity and privacy preservation. Our approach involves: (1) a Diversity agent leveraging clustering techniques to maximize topical coverage and semantic variability, (2) a Privacy Agent that detects and mask sensitive information across multiple domains and (3) a QA curation agent that synthesizes private and diverse QA pairs suitable as ground truth for RAG evaluation. Extensive experiments demonstrate that our evaluation sets outperform baseline methods in diversity and achieve robust privacy masking on domain-specific datasets. This work offers a practical and ethically aligned pathway toward safer, more comprehensive RAG system evaluation, laying the foundation for future enhancements aligned with evolving AI regulations and compliance standards. 

**Abstract (ZH)**: 基于检索的生成(RAG)系统通过融入外部知识来提升大型语言模型的输出，使其能够提供更为 informed 和上下文相关的响应。然而，这些系统的有效性和可信度在很大程度上取决于它们的评估方式，特别是评估过程是否能够捕捉到如保护敏感信息等现实世界约束。尽管当前对RAG系统的评估主要集中在性能指标的开发上，但对底层评估数据集的设计和质量的关注却远不足，尽管这些数据集在实现有意义和可靠的评估中扮演着关键角色。在本工作中，我们提出了一种新颖的多智能体框架，用于为RAG评估生成合成QA数据集，该框架优先考虑语义多样性与隐私保护。我们的方法包括：（1）一个多样性智能体，利用聚类技术来最大化主题覆盖面和语义多样性；（2）一个隐私智能体，用于检测并在多个领域中隐藏敏感信息；（3）一个QA整理智能体，用于合成适用于RAG评估的真实地面 truth QA 对。大量实验表明，我们的评估集在多样性方面优于基线方法，并能在领域特定数据集上实现稳健的隐私隐藏。本工作提供了一条实用且符合伦理的路径，以实现更安全和全面的RAG系统评估，为未来与不断发展的AI法规和合规标准相一致的改进奠定了基础。 

---
# ReflectivePrompt: Reflective evolution in autoprompting algorithms 

**Title (ZH)**: 反思性提示: 自动提示算法中的反思性演进 

**Authors**: Viktor N. Zhuravlev, Artur R. Khairullin, Ernest A. Dyagin, Alena N. Sitkina, Nikita I. Kulin  

**Link**: [PDF](https://arxiv.org/pdf/2508.18870)  

**Abstract**: Autoprompting is the process of automatically selecting optimized prompts for language models, which has been gaining popularity with the rapid advancement of prompt engineering, driven by extensive research in the field of large language models (LLMs). This paper presents ReflectivePrompt - a novel autoprompting method based on evolutionary algorithms that employs a reflective evolution approach for more precise and comprehensive search of optimal prompts. ReflectivePrompt utilizes short-term and long-term reflection operations before crossover and elitist mutation to enhance the quality of the modifications they introduce. This method allows for the accumulation of knowledge obtained throughout the evolution process and updates it at each epoch based on the current population. ReflectivePrompt was tested on 33 datasets for classification and text generation tasks using open-access large language models: t-lite-instruct-0.1 and gemma3-27b-it. The method demonstrates, on average, a significant improvement (e.g., 28% on BBH compared to EvoPrompt) in metrics relative to current state-of-the-art approaches, thereby establishing itself as one of the most effective solutions in evolutionary algorithm-based autoprompting. 

**Abstract (ZH)**: 自动提示是自动选择优化提示以供语言模型使用的过程，随着提示工程的快速发展以及对大规模语言模型（LLMs）领域的广泛研究而逐渐流行起来。本文提出了反思提示（ReflectivePrompt）——一种基于进化算法的新型自动提示方法，采用反思进化策略进行更精确和全面的最优提示搜索。反思提示在交叉和精英变异之前采用短期和长期反思操作，以增强它们引入的修改质量。该方法允许在整个进化过程中积累获得的知识，并根据当前群体在每个周期进行更新。反思提示在33个用于分类和文本生成任务的大规模语言模型（t-lite-instruct-0.1和gemma3-27b-it）开放访问数据集上进行了测试，显示相对于当前最先进的方法在指标上平均具有显著改进（例如，在BBH上相比EvoPrompt提高28%），从而确立了其作为基于进化算法的自动提示最有效解决方案之一的地位。 

---
# ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive 

**Title (ZH)**: ClusterFusion: 通过集群级集体原语扩展操作融合范围以实现大语言模型推理 

**Authors**: Xinhao Luo, Zihan Liu, Yangjie Zhou, Shihan Fang, Ziyu Huang, Yu Feng, Chen Zhang, Shixuan Sun, Zhenzhe Zheng, Jingwen Leng, Minyi Guo  

**Link**: [PDF](https://arxiv.org/pdf/2508.18850)  

**Abstract**: Large language model (LLM) decoding suffers from high latency due to fragmented execution across operators and heavy reliance on off-chip memory for data exchange and reduction. This execution model limits opportunities for fusion and incurs significant memory traffic and kernel launch overhead. While modern architectures such as NVIDIA Hopper provide distributed shared memory and low-latency intra-cluster interconnects, they expose only low-level data movement instructions, lacking structured abstractions for collective on-chip communication. To bridge this software-hardware gap, we introduce two cluster-level communication primitives, ClusterReduce and ClusterGather, which abstract common communication patterns and enable structured, high-speed data exchange and reduction between thread blocks within a cluster, allowing intermediate results to be on-chip without involving off-chip memory. Building on these abstractions, we design ClusterFusion, an execution framework that schedules communication and computation jointly to expand operator fusion scope by composing decoding stages such as QKV Projection, Attention, and Output Projection into a single fused kernels. Evaluations on H100 GPUs show that ClusterFusion outperforms state-of-the-art inference frameworks by 1.61x on average in end-to-end latency across different models and configurations. The source code is available at this https URL. 

**Abstract (ZH)**: 大规模语言模型（LLM）解码由于运算器之间碎片化的执行和对芯片外内存的重依赖而导致高延迟。这种执行模型限制了运算融合的机会，并产生了显著的内存流量和内核启动开销。尽管现代架构如NVIDIA Hopper提供了分布式共享内存和低延迟的集群内互连，但仅有低级别的数据移动指令，缺乏针对芯片内集体通信的结构化抽象。为弥合软硬件差距，我们引入了两个集群级别通信原语——ClusterReduce和ClusterGather，它们抽象了常见的通信模式，并能够在集群内的线程块之间实现结构化的高速数据交换和缩减，使得中间结果可以保留在芯片内而不涉及芯片外内存。基于这些抽象，我们设计了ClusterFusion执行框架，该框架联合调度通信和计算，通过将解码阶段如QKV投影、注意力和输出投影组合成单一融合内核来扩展运算融合的范围。在H100 GPU上的评测结果显示，ClusterFusion在不同模型和配置下端到端延迟上平均优于最先进的推理框架1.61倍。源代码可在以下链接获取。 

---
# ConfTuner: Training Large Language Models to Express Their Confidence Verbally 

**Title (ZH)**: ConfTuner: 训练大规模语言模型以口头表达其置信度 

**Authors**: Yibo Li, Miao Xiong, Jiaying Wu, Bryan Hooi  

**Link**: [PDF](https://arxiv.org/pdf/2508.18847)  

**Abstract**: Large Language Models (LLMs) are increasingly deployed in high-stakes domains such as science, law, and healthcare, where accurate expressions of uncertainty are essential for reliability and trust. However, current LLMs are often observed to generate incorrect answers with high confidence, a phenomenon known as "overconfidence". Recent efforts have focused on calibrating LLMs' verbalized confidence: i.e., their expressions of confidence in text form, such as "I am 80% confident that...". Existing approaches either rely on prompt engineering or fine-tuning with heuristically generated uncertainty estimates, both of which have limited effectiveness and generalizability. Motivated by the notion of proper scoring rules for calibration in classical machine learning models, we introduce ConfTuner, a simple and efficient fine-tuning method that introduces minimal overhead and does not require ground-truth confidence scores or proxy confidence estimates. ConfTuner relies on a new loss function, tokenized Brier score, which we theoretically prove to be a proper scoring rule, intuitively meaning that it "correctly incentivizes the model to report its true probability of being correct". ConfTuner improves calibration across diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our results further show that better-calibrated confidence enables downstream gains in self-correction and model cascade, advancing the development of trustworthy LLM systems. The code is available at this https URL. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在科学、法律和医疗等高 stakes 领域的应用日益增多，准确表达不确定性对于可靠性和信任至关重要。然而，当前的LLMs常常以高置信度产生错误答案，这一现象称为“过度自信”。近期努力集中在校准LLMs的表达置信度：即它们在文本形式中对置信度的表达，例如“我有80%的把握认为...”。现有方法要么依赖于提示工程，要么使用启发式生成的不确定性估计进行微调，这两种方法都效果有限且缺乏普适性。受经典机器学习模型校准中的适当得分规则思想的启发，我们介绍了一种简洁高效的微调方法ConfTuner，该方法引入的开销最小，并不要求真实置信分数或代理置信估计。ConfTuner 依赖于一种新的损失函数——标记化布里尔得分，我们理论证明它是一种适当得分规则，直观上意味着它“正确地激励模型报告其正确性的真正概率”。ConfTuner 在多种推理任务上提高了校准效果，并且能够推广到如GPT-4o等黑盒模型。我们的结果显示，更好的校准置信度能够带来下游任务上的改进，包括自我纠正和模型级联，从而促进可信的大规模语言模型系统的开发。代码可在以下链接获取：这个 https URL。 

---
# FALCON: Autonomous Cyber Threat Intelligence Mining with LLMs for IDS Rule Generation 

**Title (ZH)**: FALCON：使用大规模语言模型自动进行网络威胁情报挖掘以生成IDS规则 

**Authors**: Shaswata Mitra, Azim Bazarov, Martin Duclos, Sudip Mittal, Aritran Piplai, Md Rayhanur Rahman, Edward Zieglar, Shahram Rahimi  

**Link**: [PDF](https://arxiv.org/pdf/2508.18684)  

**Abstract**: Signature-based Intrusion Detection Systems (IDS) detect malicious activities by matching network or host activity against predefined rules. These rules are derived from extensive Cyber Threat Intelligence (CTI), which includes attack signatures and behavioral patterns obtained through automated tools and manual threat analysis, such as sandboxing. The CTI is then transformed into actionable rules for the IDS engine, enabling real-time detection and prevention. However, the constant evolution of cyber threats necessitates frequent rule updates, which delay deployment time and weaken overall security readiness. Recent advancements in agentic systems powered by Large Language Models (LLMs) offer the potential for autonomous IDS rule generation with internal evaluation. We introduce FALCON, an autonomous agentic framework that generates deployable IDS rules from CTI data in real-time and evaluates them using built-in multi-phased validators. To demonstrate versatility, we target both network (Snort) and host-based (YARA) mediums and construct a comprehensive dataset of IDS rules with their corresponding CTIs. Our evaluations indicate FALCON excels in automatic rule generation, with an average of 95% accuracy validated by qualitative evaluation with 84% inter-rater agreement among multiple cybersecurity analysts across all metrics. These results underscore the feasibility and effectiveness of LLM-driven data mining for real-time cyber threat mitigation. 

**Abstract (ZH)**: 基于签名的入侵检测系统（IDS）通过将网络或主机活动与预定义规则进行匹配来检测恶意活动。这些规则来源于广泛的网络威胁情报（CTI），包括通过自动化工具和手动威胁分析（如沙箱）获得的攻击签名和行为模式。CTI随后被转换为可供IDS引擎使用的行动规则，实现实时检测和预防。然而，网络威胁的不断演变要求频繁更新规则，这延缓了部署时间，削弱了整体安全性。近年来，由大规模语言模型（LLMs）驱动的代理系统的发展为自主生成IDS规则并进行内部评估提供了可能。我们引入了FALCON，这是一个自主代理框架，能够从CTI数据中实时生成可部署的IDS规则，并使用内置的多阶段验证器进行评估。为了展示其通用性，我们针对网络（Snort）和主机（YARA）两种媒介进行了目标攻击，并构建了一个包含IDS规则及其对应CTI数据的全面数据集。我们的评估结果显示，FALCON在自动规则生成方面表现出色，平均准确率为95%，在多个网络安全分析师之间的定量评价中，相互评价的一致性达到了84%。这些结果强调了由LLM驱动的数据挖掘在实时缓解网络威胁方面的可行性和有效性。 

---
# Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks 

**Title (ZH)**: 专家混合语言模型在推理任务中的最优稀疏性 

**Authors**: Taishi Nakamura, Satoki Ishikawa, Masaki Kawamura, Takumi Okamoto, Daisuke Nohara, Jun Suzuki, Rio Yokota  

**Link**: [PDF](https://arxiv.org/pdf/2508.18672)  

**Abstract**: Empirical scaling laws have driven the evolution of large language models (LLMs), yet their coefficients shift whenever the model architecture or data pipeline changes. Mixture-of-Experts (MoE) models, now standard in state-of-the-art systems, introduce a new sparsity dimension that current dense-model frontiers overlook. We investigate how MoE sparsity influences two distinct capability regimes: memorization and reasoning. We train families of MoE Transformers that systematically vary total parameters, active parameters, and top-$k$ routing while holding the compute budget fixed. For every model we record pre-training loss, downstream task loss, and task accuracy, allowing us to separate the train-test generalization gap from the loss-accuracy gap. Memorization benchmarks improve monotonically with total parameters, mirroring training loss. By contrast, reasoning performance saturates and can even regress despite continued gains in both total parameters and training loss. Altering top-$k$ alone has little effect when active parameters are constant, and classic hyperparameters such as learning rate and initialization modulate the generalization gap in the same direction as sparsity. Neither post-training reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning deficit of overly sparse models. Our model checkpoints, code and logs are open-source at this https URL. 

**Abstract (ZH)**: 经验 scaling 规律驱动了大型语言模型（LLMs）的发展，但每当模型架构或数据管道发生变化时，其系数就会发生变化。专家混合（MoE）模型现在已成为最先进的系统中的标准做法，引入了一种新的稀疏维度，这是当前密集模型前沿所忽视的。我们探讨了MoE稀疏性如何影响两种不同的能力范式：记忆和推理。我们训练了一系列系统地变化总参数量、激活参数量和top-$k$路由的MoE Transformer模型，同时保持计算预算不变。对于每个模型，我们记录了预训练损失、下游任务损失和任务准确率，从而使我们能够区分训练-测试泛化差距与损失-准确率差距。记忆基准随着总参数量的增加而单调改进，反映了训练损失的变化。相比之下，推理性能达到了饱和，即使在总参数量和训练损失继续改善的情况下，性能也可能下降。仅改变top-$k$在激活参数量固定时影响不大，而经典的超参数如学习率和初始化在调节泛化差距方面与稀疏性一致。后训练强化学习（GRPO）和额外的测试时间计算也无法挽救过于稀疏模型的推理缺陷。我们的模型检查点、代码和日志在此网址公开：this https URL。 

---
# Membership Inference Attacks on LLM-based Recommender Systems 

**Title (ZH)**: 基于LLM的推荐系统中的成员推理攻击 

**Authors**: Jiajie He, Yuechun Gu, Min-Chun Chen, Keke Chen  

**Link**: [PDF](https://arxiv.org/pdf/2508.18665)  

**Abstract**: Large language models (LLMs) based Recommender Systems (RecSys) can flexibly adapt recommendation systems to different domains. It utilizes in-context learning (ICL), i.e., the prompts, to customize the recommendation functions, which include sensitive historical user-specific item interactions, e.g., implicit feedback like clicked items or explicit product reviews. Such private information may be exposed to novel privacy attack. However, no study has been done on this important issue. We design four membership inference attacks (MIAs), aiming to reveal whether victims' historical interactions have been used by system prompts. They are \emph{direct inquiry, hallucination, similarity, and poisoning attacks}, each of which utilizes the unique features of LLMs or RecSys. We have carefully evaluated them on three LLMs that have been used to develop ICL-LLM RecSys and two well-known RecSys benchmark datasets. The results confirm that the MIA threat on LLM RecSys is realistic: direct inquiry and poisoning attacks showing significantly high attack advantages. We have also analyzed the factors affecting these attacks, such as the number of shots in system prompts and the position of the victim in the shots. 

**Abstract (ZH)**: 基于大型语言模型的推荐系统中的成员推理攻击：直接询证、幻觉、相似性和污染攻击 

---
# FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge 

**Title (ZH)**: FFT-MoE: 面向异构边缘的大规模稀疏MoE的高效联邦微调方法 

**Authors**: Gang Hu, Yinglei Teng, Pengfei Wu, Nan Wang  

**Link**: [PDF](https://arxiv.org/pdf/2508.18663)  

**Abstract**: As FMs drive progress toward Artificial General Intelligence (AGI), fine-tuning them under privacy and resource constraints has become increasingly critical particularly when highquality training data resides on distributed edge devices. Federated Learning (FL) offers a compelling solution through Federated Fine-Tuning (FFT), which enables collaborative model adaptation without sharing raw data. Recent approaches incorporate Parameter-Efficient Fine-Tuning (PEFT) techniques such as Low Rank Adaptation (LoRA) to reduce computational overhead. However, LoRA-based FFT faces two major limitations in heterogeneous FL environments: structural incompatibility across clients with varying LoRA configurations and limited adaptability to non-IID data distributions, which hinders convergence and generalization. To address these challenges, we propose FFT MoE, a novel FFT framework that replaces LoRA with sparse Mixture of Experts (MoE) adapters. Each client trains a lightweight gating network to selectively activate a personalized subset of experts, enabling fine-grained adaptation to local resource budgets while preserving aggregation compatibility. To further combat the expert load imbalance caused by device and data heterogeneity, we introduce a heterogeneity-aware auxiliary loss that dynamically regularizes the routing distribution to ensure expert diversity and balanced utilization. Extensive experiments spanning both IID and non-IID conditions demonstrate that FFT MoE consistently outperforms state of the art FFT baselines in generalization performance and training efficiency. 

**Abstract (ZH)**: 作为FMs推动通用人工智能（AGI）的进步，特别是在隐私和资源受限条件下对它们进行微调变得越来越关键，特别是在高质量训练数据位于分布式边缘设备上时。联邦学习（FL）通过联邦微调（FFT）提供了一种令人信服的解决方案，这种方案能够在不共享原始数据的情况下进行协作模型适应。最近的方法整合了高效参数微调（PEFT）技术，如低秩适应（LoRA），以减少计算开销。然而，在异构FL环境中，基于LoRA的FMT面临着两大主要限制：客户端之间LoRA配置的结构不兼容性以及对非IID数据分布的有限适应性，这阻碍了收敛性和泛化能力。为了解决这些挑战，我们提出了一种新型的FMT框架FFT MoE，该框架用稀疏混合专家（MoE）适配器取代了LoRA。每个客户端训练一个轻量级门控网络，以选择性地激活个性化子集的专家，从而在保持聚合兼容性的同时，实现对本地资源预算的精细适应。为应对由设备和数据异构性引起的专家负载失衡，我们引入了一种感知异构性的辅助损失，该损失能够动态调节路由分布以确保专家的多样性和平衡利用。广泛的实验证明，FFT MoE在泛化性能和训练效率方面始终优于最先进的FMT基线。 

---
# Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models 

**Title (ZH)**: 突破大语言模型忠实性和表达性之间的权衡 

**Authors**: Chenxu Yang, Qingyi Si, Zheng Lin  

**Link**: [PDF](https://arxiv.org/pdf/2508.18651)  

**Abstract**: Grounding responses in external knowledge represents an effective strategy for mitigating hallucinations in Large Language Models (LLMs). However, current LLMs struggle to seamlessly integrate knowledge while simultaneously maintaining faithfulness (or fidelity) and expressiveness, capabilities that humans naturally possess. This limitation results in outputs that either lack support from external knowledge, thereby compromising faithfulness, or appear overly verbose and unnatural, thus sacrificing expressiveness. In this work, to break the trade-off between faithfulness and expressiveness, we propose Collaborative Decoding (CoDe), a novel approach that dynamically integrates output probabilities generated with and without external knowledge. This integration is guided by distribution divergence and model confidence, enabling the selective activation of relevant and reliable expressions from the model's internal parameters. Furthermore, we introduce a knowledge-aware reranking mechanism that prevents over-reliance on prior parametric knowledge while ensuring proper utilization of provided external information. Through comprehensive experiments, our plug-and-play CoDe framework demonstrates superior performance in enhancing faithfulness without compromising expressiveness across diverse LLMs and evaluation metrics, validating both its effectiveness and generalizability. 

**Abstract (ZH)**: 基于外部知识的响应生成代表了一种减轻大规模语言模型幻觉的有效策略。然而，当前的大规模语言模型在无缝集成知识的同时，难以同时保持忠实度（或保真度）和表达性，这是人类自然具备的能力。这一限制导致生成的输出要么缺乏外部知识的支撑，从而损害了忠实度，要么显得过于冗长和不自然，从而牺牲了表达性。在本文中，为了打破忠实度和表达性之间的权衡，我们提出了一种名为协作解码（CoDe）的新方法，该方法能够动态地将带有和不带有外部知识生成的输出概率进行整合。这一整合由分布差异和模型信心引导，使得模型内部参数的相关且可靠的表达得以选择性激活。此外，我们引入了一种知识感知的重排序机制，该机制防止过度依赖先验参数知识，同时确保充分利用提供的外部信息。通过全面的实验，我们的即插即用CoDe框架在多种大规模语言模型和评估指标上展示了在不牺牲表达性的情况下提高忠实度的优越性能，验证了其有效性和通用性。 

---
# LaQual: A Novel Framework for Automated Evaluation of LLM App Quality 

**Title (ZH)**: LaQual: 一种新的自动化评估LLM应用质量的框架 

**Authors**: Yan Wang, Xinyi Hou, Yanjie Zhao, Weiguo Lin, Haoyu Wang, Junjun Si  

**Link**: [PDF](https://arxiv.org/pdf/2508.18636)  

**Abstract**: LLM app stores are quickly emerging as platforms that gather a wide range of intelligent applications based on LLMs, giving users many choices for content creation, coding support, education, and more. However, the current methods for ranking and recommending apps in these stores mostly rely on static metrics like user activity and favorites, which makes it hard for users to efficiently find high-quality apps. To address these challenges, we propose LaQual, an automated framework for evaluating the quality of LLM apps. LaQual consists of three main stages: first, it labels and classifies LLM apps in a hierarchical way to accurately match them to different scenarios; second, it uses static indicators, such as time-weighted user engagement and functional capability metrics, to filter out low-quality apps; and third, it conducts a dynamic, scenario-adaptive evaluation, where the LLM itself generates scenario-specific evaluation metrics, scoring rules, and tasks for a thorough quality assessment. Experiments on a popular LLM app store show that LaQual is effective. Its automated scores are highly consistent with human judgments (with Spearman's rho of 0.62 and p=0.006 in legal consulting, and rho of 0.60 and p=0.009 in travel planning). By effectively screening, LaQual can reduce the pool of candidate LLM apps by 66.7% to 81.3%. User studies further confirm that LaQual significantly outperforms baseline systems in decision confidence, comparison efficiency (with average scores of 5.45 compared to 3.30), and the perceived value of its evaluation reports (4.75 versus 2.25). Overall, these results demonstrate that LaQual offers a scalable, objective, and user-centered solution for finding and recommending high-quality LLM apps in real-world use cases. 

**Abstract (ZH)**: 基于LLM的应用商店中LaQual：一种自动化的高质量评估框架 

---
# Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models 

**Title (ZH)**: Post-Training量化大型语言模型中的任务分层知识缩放律 

**Authors**: Chenxi Zhou, Pengfei Cao, Jiang Li, Jun Zhao, Kang Liu  

**Link**: [PDF](https://arxiv.org/pdf/2508.18609)  

**Abstract**: Large language models (LLMs) present significant deployment challenges due to their scale, with post-training quantization (PTQ) emerging as a practical compression solution. However, a comprehensive understanding of how PTQ precisely impacts diverse LLM knowledge capabilities remains elusive, and existing scaling laws for quantized models often overlook crucial PTQ-specific parameters and task-specific sensitivities. This paper addresses these gaps by conducting an extensive empirical investigation to establish task-stratified scaling laws. We disentangle LLM knowledge into memorization and utilization capabilities and develop a unified quantitative framework that incorporates model size, effective bit-width, calibration set size, and group size. Our central finding reveals that knowledge memorization exhibits markedly greater sensitivity to variations in effective bit-width, calibration set size, and model size compared to the more robust knowledge utilization. These findings offer a fine-grained understanding of PTQ's impact and provide guidance for developing knowledge-aware quantization strategies that can better preserve targeted cognitive functions. 

**Abstract (ZH)**: 大规模语言模型（LLMs）由于其规模庞大，在部署时面临显著挑战，量化后训练（PTQ）作为一种实用的压缩解决方案逐渐显现。然而，PTQ 对 LLM 多样知识能力的精确影响尚缺乏全面理解，现有的量化模型缩放法则常常忽视了 PTQ 特定参数和任务特定敏感性。本文通过广泛的实证研究，建立了任务分层的缩放定律。我们将 LLM 知识拆分为记忆能力和利用能力，并开发了一个统一的定量框架，该框架包含模型大小、有效位宽、校准集大小和分组大小。我们的主要发现表明，知识记忆对有效位宽、校准集大小和模型大小的变化表现出更为显著的敏感性，而知识利用则更为稳健。这些发现为细粒度理解 PTQ 的影响提供了洞见，并为开发更能够保留目标认知功能的知识感知量化策略提供了指导。 

---
# What do language models model? Transformers, automata, and the format of thought 

**Title (ZH)**: 语言模型建模的是什么？变换器、自动机与思维的形式 

**Authors**: Colin Klein  

**Link**: [PDF](https://arxiv.org/pdf/2508.18598)  

**Abstract**: What do large language models actually model? Do they tell us something about human capacities, or are they models of the corpus we've trained them on? I give a non-deflationary defence of the latter position. Cognitive science tells us that linguistic capabilities in humans rely supralinear formats for computation. The transformer architecture, by contrast, supports at best a linear formats for processing. This argument will rely primarily on certain invariants of the computational architecture of transformers. I then suggest a positive story about what transformers are doing, focusing on Liu et al. (2022)'s intriguing speculations about shortcut automata. I conclude with why I don't think this is a terribly deflationary story. Language is not (just) a means for expressing inner state but also a kind of 'discourse machine' that lets us make new language given appropriate context. We have learned to use this technology in one way; LLMs have also learned to use it too, but via very different means. 

**Abstract (ZH)**: 大型语言模型实际上建模了什么？它们告诉我们关于人类能力的某些信息，还是只是在建模我们训练它们的数据集？我将从后一种观点出发提供一个非消解性的辩护。认知科学表明，人类的语言能力依赖于超线性计算格式。相比之下，变压器架构最多仅支持线性处理格式。这一论点主要依赖于变压器计算架构的一些不变性质。然后，我提出一个关于变压器正在做什么的积极故事，重点是刘等（2022）关于捷径自动机的有趣推测。最后，我解释为什么我认为这不是一个过分消解性的故事。语言不仅是表达内心状态的手段，也是一种‘话语机器’，让我们在适当的情境下生成新的语言。我们以一种方式学会了使用这种技术；大型语言模型也学会了使用它，但通过非常不同的方式。 

---
# A Case Study on the Effectiveness of LLMs in Verification with Proof Assistants 

**Title (ZH)**: LLMs在定理辅助证明验证中的效果案例研究 

**Authors**: Barış Bayazıt, Yao Li, Xujie Si  

**Link**: [PDF](https://arxiv.org/pdf/2508.18587)  

**Abstract**: Large language models (LLMs) can potentially help with verification using proof assistants by automating proofs. However, it is unclear how effective LLMs are in this task. In this paper, we perform a case study based on two mature Rocq projects: the hs-to-coq tool and Verdi. We evaluate the effectiveness of LLMs in generating proofs by both quantitative and qualitative analysis. Our study finds that: (1) external dependencies and context in the same source file can significantly help proof generation; (2) LLMs perform great on small proofs but can also generate large proofs; (3) LLMs perform differently on different verification projects; and (4) LLMs can generate concise and smart proofs, apply classical techniques to new definitions, but can also make odd mistakes. 

**Abstract (ZH)**: 大型语言模型（LLMs）可以通过自动化证明来潜在地帮助验证任务，但其有效性尚不明确。在本文中，我们基于两个成熟的Rocq项目——hs-to-coq工具和Verdi，进行了一项案例研究。我们通过定量和定性的分析评估了LLMs在生成证明方面的有效性。我们的研究发现：(1) 来自同一源文件的外部依赖和上下文可以显著帮助证明生成；(2) LLMs在小型证明上表现出色，同时也能生成大型证明；(3) LLMs在不同验证项目中的表现有所不同；(4) LLMs能够生成简洁和聪明的证明，可以将经典技术应用于新定义，但也可能会犯一些奇怪的错误。 

---
# DrugReasoner: Interpretable Drug Approval Prediction with a Reasoning-augmented Language Model 

**Title (ZH)**: DrugReasoner：一种增强推理的可解释药物批准预测语言模型 

**Authors**: Mohammadreza Ghaffarzadeh-Esfahani, Ali Motahharynia, Nahid Yousefian, Navid Mazrouei, Jafar Ghaisari, Yousof Gheisari  

**Link**: [PDF](https://arxiv.org/pdf/2508.18579)  

**Abstract**: Drug discovery is a complex and resource-intensive process, making early prediction of approval outcomes critical for optimizing research investments. While classical machine learning and deep learning methods have shown promise in drug approval prediction, their limited interpretability constraints their impact. Here, we present DrugReasoner, a reasoning-based large language model (LLM) built on the LLaMA architecture and fine-tuned with group relative policy optimization (GRPO) to predict the likelihood of small-molecule approval. DrugReasoner integrates molecular descriptors with comparative reasoning against structurally similar approved and unapproved compounds, generating predictions alongside step-by-step rationales and confidence scores. DrugReasoner achieved robust performance with an AUC of 0.732 and an F1 score of 0.729 on the validation set and 0.725 and 0.718 on the test set, respectively. These results outperformed conventional baselines, including logistic regression, support vector machine, and k-nearest neighbors and had competitive performance relative to XGBoost. On an external independent dataset, DrugReasoner outperformed both baseline and the recently developed ChemAP model, achieving an AUC of 0.728 and an F1-score of 0.774, while maintaining high precision and balanced sensitivity, demonstrating robustness in real-world scenarios. These findings demonstrate that DrugReasoner not only delivers competitive predictive accuracy but also enhances transparency through its reasoning outputs, thereby addressing a key bottleneck in AI-assisted drug discovery. This study highlights the potential of reasoning-augmented LLMs as interpretable and effective tools for pharmaceutical decision-making. 

**Abstract (ZH)**: 药物发现是复杂且资源密集的过程，早期预测审批结果对于优化研究投资至关重要。尽管经典的机器学习方法和深度学习方法在药物审批预测中展现了一定的潜力，但其有限的可解释性限制了其影响。在这里，我们介绍了基于LLaMA架构并采用群组相对策略优化（GRPO）微调的推理驱动大型语言模型（LLM）——DrugReasoner，用于预测小分子审批的可能性。DrugReasoner 将分子描述符与结构类似已批准和未批准化合物的比较推理相结合，生成预测结果及其逐步推理过程和置信度评分。DrugReasoner 在验证集上的 AUC 为 0.732，F1 分数为 0.729，在测试集上的 AUC 和 F1 分数分别为 0.725 和 0.718。这些结果优于传统的基线方法，包括逻辑回归、支持向量机和 k 最近邻，并且与 XGBoost 具有竞争力。在外部独立数据集上，DrugReasoner 出色地超过了基线方法和最近开发的 ChemAP 模型，获得了 AUC 为 0.728 和 F1 分数为 0.774 的成绩，同时保持了高精度和平衡的敏感性，证明了其在实际场景中的稳健性。这些发现表明，DrugReasoner 不仅提供了与预测准确性相当的竞争性表现，还通过其推理输出增强了透明度，从而解决了AI辅助药物发现中的关键瓶颈问题。该研究强调了增强推理的大型语言模型在药物研发决策中作为可解释且有效的工具的潜力。 

---
# Collaborative Intelligence: Topic Modelling of Large Language Model use in Live Cybersecurity Operations 

**Title (ZH)**: 协作智能：大规模语言模型在实时网络安全运营中的主题建模 

**Authors**: Martin Lochner, Keegan Keplinger  

**Link**: [PDF](https://arxiv.org/pdf/2508.18488)  

**Abstract**: Objective: This work describes the topic modelling of Security Operations Centre (SOC) use of a large language model (LLM), during live security operations. The goal is to better understand how these specialists voluntarily use this tool.
Background: Human-automation teams have been extensively studied, but transformer-based language models have sparked a new wave of collaboration. SOC personnel at a major cybersecurity provider used an LLM to support live security operations. This study examines how these specialists incorporated the LLM into their work.
Method: Our data set is the result of 10 months of SOC operators accessing GPT-4 over an internally deployed HTTP-based chat application. We performed two topic modelling exercises, first using the established BERTopic model (Grootendorst, 2022), and second, using a novel topic modeling workflow.
Results: Both the BERTopic analysis and novel modelling approach revealed that SOC operators primarily used the LLM to facilitate their understanding of complex text strings. Variations on this use-case accounted for ~40% of SOC LLM usage.
Conclusion: SOC operators are required to rapidly interpret complex commands and similar information. Their natural tendency to leverage LLMs to support this activity indicates that their workflow can be supported and augmented by designing collaborative LLM tools for use in the SOC.
Application: This work can aid in creating next-generation tools for Security Operations Centres. By understanding common use-cases, we can develop workflows supporting SOC task flow. One example is a right-click context menu for executing a command line analysis LLM call directly in the SOC environment. 

**Abstract (ZH)**: 目标：本文描述了安全运营中心（SOC）在实际安全运营中使用大规模语言模型（LLM）的主题建模。旨在更好地了解这些专家如何自愿使用该工具。

背景：人机团队合作已被广泛研究，而基于变压器的语言模型则引发了新的合作浪潮。一家主要的网络安全提供商的SOC人员使用LLM来支持实际的安全运营。本研究探讨了这些专家如何将LLM融入到他们的工作中。

方法：我们的数据集是SOC操作人员在内部部署的基于HTTP的聊天应用程序中访问GPT-4的10个月结果。我们进行了两项主题建模分析，首先使用了已有的BERTopic模型（Grootendorst, 2022），其次使用了一种新的主题建模工作流。

结果：无论是BERTopic分析还是新型建模方法都揭示了SOC操作人员主要使用LLM来帮助他们理解复杂文本字符串。这种用途的变体占SOC LLM使用量的约40%。

结论：SOC操作人员需要迅速解读复杂的命令和其他信息。他们自然倾向于利用LLM来支持这一活动，表明可以设计支持和增强SOC工作的协作LLM工具。

应用：本研究可用于创建下一代安全运营中心工具。通过理解常见的使用案例，我们可以开发支持SOC任务流程的工作流。例如，在SOC环境中直接执行命令行分析LLM调用的右键上下文菜单。 

---
# Principled Detection of Hallucinations in Large Language Models via Multiple Testing 

**Title (ZH)**: 大型语言模型中原理导向的幻觉检测方法 via 多重假设检验 

**Authors**: Jiawei Li, Akshayaa Magesh, Venugopal V. Veeravalli  

**Link**: [PDF](https://arxiv.org/pdf/2508.18473)  

**Abstract**: While Large Language Models (LLMs) have emerged as powerful foundational models to solve a variety of tasks, they have also been shown to be prone to hallucinations, i.e., generating responses that sound confident but are actually incorrect or even nonsensical. In this work, we formulate the problem of detecting hallucinations as a hypothesis testing problem and draw parallels to the problem of out-of-distribution detection in machine learning models. We propose a multiple-testing-inspired method to solve the hallucination detection problem, and provide extensive experimental results to validate the robustness of our approach against state-of-the-art methods. 

**Abstract (ZH)**: 尽管大规模语言模型（LLMs）已经涌现出作为解决各种任务的强大基础模型，但它们也被证明容易产生幻觉，即生成看似自信但实际上错误甚至是无意义的响应。在本文中，我们将检测幻觉问题形式化为假设检验问题，并将其与机器学习模型的离分布检测问题相类比。我们提出了一种受多重检验启发的方法来解决幻觉检测问题，并提供了广泛的实验结果以验证我们方法相对于现有最佳方法的鲁棒性。 

---
# VERIRL: Boosting the LLM-based Verilog Code Generation via Reinforcement Learning 

**Title (ZH)**: VERIRL: 通过强化学习提升基于LLM的Verilog代码生成 

**Authors**: Fu Teng, Miao Pan, Xuhong Zhang, Zhezhi He, Yiyao Yang, Xinyi Chai, Mengnan Qi, Liqiang Lu, Jianwei Yin  

**Link**: [PDF](https://arxiv.org/pdf/2508.18462)  

**Abstract**: Recent advancements in code generation have shown remarkable success across software domains, yet hardware description languages (HDLs) such as Verilog remain underexplored due to their concurrency semantics, syntactic rigidity, and simulation complexity. In this work, we address these challenges by introducing a reinforcement learning (RL) framework tailored for Verilog code generation. We first construct Veribench-53K, a high-quality dataset curated from over 700K Verilog problems, enriched with structured prompts, complexity labels, and diverse testbenches. To tackle the problem of sparse and noisy reward signals, we propose a Trace-back based Rescore mechanism that leverages reasoning paths and iterative refinement to enhance feedback reliability and support reward model training. Furthermore, to mitigate catastrophic forgetting and overfitting during RL fine-tuning, we introduce a sample-balanced weighting strategy that adaptively balances learning dynamics based on reward-probability distributions. These innovations are integrated into an iterative RL pipeline that co-evolves the policy and reward models. In contrast to recent work such as CraftRTL, which relies on large-scale closed-source model distillation, and DeepSeek-style approaches that struggle with sparse feedback, our method demonstrates superior performance using a smaller but high-quality dataset combined with RL optimization. Experiments on Verilog generation tasks demonstrate state-of-the-art performance, with substantial gains in test pass rate, functional correctness, and compilation robustness. Our findings highlight the potential of RL-driven approaches for structured code generation in hardware-centric domains. VERIRL is publicly available at this https URL. 

**Abstract (ZH)**: 近期代码生成领域的进展在软件领域取得了显著成功，但硬件描述语言（HDLs）如Verilog由于其并发语义、句法刚性以及仿真复杂性，仍然尚未被充分探索。本文通过引入一个针对Verilog代码生成的强化学习（RL）框架来应对这些挑战。我们首先构建了Veribench-53K，这是一个高质量的数据集，从中精心挑选了超过700K的Verilog问题，并丰富了结构化的提示、复杂度标签以及多样化的测试平台。为了解决稀疏且噪声较大的奖励信号问题，我们提出了一种回溯Rescore机制，利用推理路径和迭代细化来增强反馈可靠性并支持奖励模型训练。此外，在强化学习（RL）微调过程中，为了缓解灾难性遗忘和过度拟合，我们引入了一种样本平衡加权策略，该策略根据奖励概率分布自适应平衡学习动态。这些创新集成到一个迭代RL管道中，同时演化策略模型和奖励模型。与依赖大规模商用模型蒸馏的CraftRTL方法以及在稀疏反馈方面困难的DeepSeek风格方法相比，我们的方法使用更小但高质量的数据集和RL优化展现出更好的性能。实验结果表明VERIRL在Verilog生成任务上的表现达到了最先进的水平，显著提高了测试通过率、功能正确性和编译鲁棒性。我们的研究结果突显了强化学习驱动方法在硬件为中心领域结构化代码生成中的潜力。VERIRL已公开发布。 

---
# How Reliable are LLMs for Reasoning on the Re-ranking task? 

**Title (ZH)**: LLMs在重排名任务推理中的可靠性如何？ 

**Authors**: Nafis Tanveer Islam, Zhiming Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2508.18444)  

**Abstract**: With the improving semantic understanding capability of Large Language Models (LLMs), they exhibit a greater awareness and alignment with human values, but this comes at the cost of transparency. Although promising results are achieved via experimental analysis, an in-depth understanding of the LLM's internal workings is unavoidable to comprehend the reasoning behind the re-ranking, which provides end users with an explanation that enables them to make an informed decision. Moreover, in newly developed systems with limited user engagement and insufficient ranking data, accurately re-ranking content remains a significant challenge. While various training methods affect the training of LLMs and generate inference, our analysis has found that some training methods exhibit better explainability than others, implying that an accurate semantic understanding has not been learned through all training methods; instead, abstract knowledge has been gained to optimize evaluation, which raises questions about the true reliability of LLMs. Therefore, in this work, we analyze how different training methods affect the semantic understanding of the re-ranking task in LLMs and investigate whether these models can generate more informed textual reasoning to overcome the challenges of transparency or LLMs and limited training data. To analyze the LLMs for re-ranking tasks, we utilize a relatively small ranking dataset from the environment and the Earth science domain to re-rank retrieved content. Furthermore, we also analyze the explainable information to see if the re-ranking can be reasoned using explainability. 

**Abstract (ZH)**: 随着大型语言模型（LLMs）语义理解能力的提升，它们在价值观意识和对齐方面表现得更为敏锐，但这也牺牲了透明度。尽管实验分析取得了有前景的结果，但要理解重新排序背后的推理过程，深入理解LLM的内部工作机制是必不可少的。这将为最终用户提供解释，帮助他们做出明智的决策。此外，在新的系统中，由于用户参与度有限和排序数据不足，准确重新排序内容仍是一项重大挑战。尽管各种训练方法会影响LLM的训练并产生推理，我们的分析发现，某些训练方法的解释性优于其他方法，表明并非所有训练方法都学会了准确的语义理解；相反，获取了抽象知识以优化评估，这引发了对LLMs真正可靠性的质疑。因此，在本研究中，我们分析不同的训练方法如何影响LLMs在重新排序任务中的语义理解，并探究这些模型是否能够生成更有力的文本推理，以克服透明度或LLMs及有限训练数据的挑战。为了分析LLMs在重新排序任务中的表现，我们利用环境和地球科学领域相对较小的排序数据集重新排序检索内容。此外，我们还分析可解释性信息，以确定重新排序是否可以通过解释来进行推理。 

---
# A Systematic Approach to Predict the Impact of Cybersecurity Vulnerabilities Using LLMs 

**Title (ZH)**: 使用大规模语言模型系统预测网络安全漏洞影响的方法 

**Authors**: Anders Mølmen Høst, Pierre Lison, Leon Moonen  

**Link**: [PDF](https://arxiv.org/pdf/2508.18439)  

**Abstract**: Vulnerability databases, such as the National Vulnerability Database (NVD), offer detailed descriptions of Common Vulnerabilities and Exposures (CVEs), but often lack information on their real-world impact, such as the tactics, techniques, and procedures (TTPs) that adversaries may use to exploit the vulnerability. However, manually linking CVEs to their corresponding TTPs is a challenging and time-consuming task, and the high volume of new vulnerabilities published annually makes automated support desirable.
This paper introduces TRIAGE, a two-pronged automated approach that uses Large Language Models (LLMs) to map CVEs to relevant techniques from the ATT&CK knowledge base. We first prompt an LLM with instructions based on MITRE's CVE Mapping Methodology to predict an initial list of techniques. This list is then combined with the results from a second LLM-based module that uses in-context learning to map a CVE to relevant techniques. This hybrid approach strategically combines rule-based reasoning with data-driven inference. Our evaluation reveals that in-context learning outperforms the individual mapping methods, and the hybrid approach improves recall of exploitation techniques. We also find that GPT-4o-mini performs better than Llama3.3-70B on this task. Overall, our results show that LLMs can be used to automatically predict the impact of cybersecurity vulnerabilities and TRIAGE makes the process of mapping CVEs to ATT&CK more efficient.
Keywords: vulnerability impact, CVE, ATT&CK techniques, large language models, automated mapping. 

**Abstract (ZH)**: 漏洞数据库，如National Vulnerability Database (NVD)，提供了通用漏洞和暴露(CVEs)的详细描述，但通常缺乏其实际影响的信息，例如攻击者可能用于利用漏洞的战术、技术和程序(TTPs)。然而，手动将CVEs与其相应的TTPs链接是一个艰巨且耗时的任务，每年发布的新漏洞数量庞大，使得自动化支持变得必要。
本文介绍了TRIAGE，这是一种两阶段的自动化方法，利用大型语言模型(LLMs)将CVEs映射到ATT&CK知识库中的相关技术。我们首先使用基于MITRE的CVE映射方法学的指令提示LLM预测初始的技术列表。然后，将该列表与基于LLM的第二个模块生成的结果相结合，该模块使用上下文提示学习来映射CVE到相关技术。这种混合方法战略性地结合了基于规则的推理和数据驱动的推断。我们的评估显示，上下文提示学习比单独的映射方法表现更好，而混合方法提高了利用技术的查全率。我们还发现，GPT-4o-mini在该任务上优于Llama3.3-70B。总体而言，我们的结果显示，大型语言模型可以用于自动预测网络安全漏洞的影响，TRIAGE使得将CVEs映射到ATT&CK的过程更加高效。
关键词：漏洞影响，CVE，ATT&CK技术，大型语言模型，自动化映射。 

---
# Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning 

**Title (ZH)**: latent自我一致性在短答案和长答案推理中可靠多数集选择中的应用 

**Authors**: Jeong-seok Oh, Jay-yoon Lee  

**Link**: [PDF](https://arxiv.org/pdf/2508.18395)  

**Abstract**: Probabilistic decoding in Large Language Models (LLMs) often yields inconsistent outputs, particularly on complex or long-form questions. Self-Consistency (SC) mitigates this for short-form QA by majority voting over exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram Consistency Score (WUCS) extend to long-form responses but lose accuracy on short-form benchmarks.
We introduce Latent Self-Consistency (LSC), which selects the most semantically consistent response using learnable token embeddings. A lightweight forward generation of summary tokens increases inference time by less than 1% and requires no changes to the model architecture.
Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU, TruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form ones on average, while maintaining negligible computational overhead. These results position LSC as a practical consistency-selection method that works reliably across answer formats. Additionally, LSC provides well-calibrated confidence estimates, maintaining low Expected Calibration Error across both answer formats. 

**Abstract (ZH)**: Large Language Models (LLMs)中的潜在自一致性（LSC）在长格式和短格式推理基准上的表现分析 

---
# Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails 

**Title (ZH)**: 合成预生成数据驱动的健康建议约束机制 

**Authors**: Kellen Tan Cheng, Anna Lisa Gentile, Chad DeLuca, Guang-Jie Ren  

**Link**: [PDF](https://arxiv.org/pdf/2508.18384)  

**Abstract**: The pervasiveness of large language models (LLMs) in enterprise settings has also brought forth a significant amount of risks associated with their usage. Guardrails technologies aim to mitigate this risk by filtering LLMs' input/output text through various detectors. However, developing and maintaining robust detectors faces many challenges, one of which is the difficulty in acquiring production-quality labeled data on real LLM outputs prior to deployment. In this work, we propose backprompting, a simple yet intuitive solution to generate production-like labeled data for health advice guardrails development. Furthermore, we pair our backprompting method with a sparse human-in-the-loop clustering technique to label the generated data. Our aim is to construct a parallel corpus roughly representative of the original dataset yet resembling real LLM output. We then infuse existing datasets with our synthetic examples to produce robust training data for our detector. We test our technique in one of the most difficult and nuanced guardrails: the identification of health advice in LLM output, and demonstrate improvement versus other solutions. Our detector is able to outperform GPT-4o by up to 3.73%, despite having 400x less parameters. 

**Abstract (ZH)**: 大型语言模型在企业环境中的广泛应用也带来了与其使用相关的重大风险。边界技术通过各种检测器过滤大型语言模型的输入/输出文本以减轻这些风险。然而，开发和维护稳健的检测器面临许多挑战，其中一个挑战是在部署之前获取高质量的生产级标记数据以检测真实语言模型输出的困难。在本文中，我们提出了一种简单直观的方法——回提示，用于为健康建议边界技术开发生成生产级标记数据。此外，我们结合稀疏的人在环聚类技术对生成的数据进行标注。我们的目标是构建一个平行语料库，大致代表原始数据集且类似真实语言模型输出。然后，我们将我们的合成示例注入现有数据集，以产生检测器的稳健训练数据。我们在最难和最复杂的边界技术之一——识别语言模型输出中的健康建议方面测试了我们的技术，并展示了与其他解决方案相比的改进。尽管参数量少400倍，我们的检测器能够在GPT-4o上提高多达3.73%的性能。 

---
# LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions 

**Title (ZH)**: LLMs无法应对peer pressure：在多代理社会互动中的崩溃 

**Authors**: Maojia Song, Tej Deep Pala, Weisheng Jin, Amir Zadeh, Chuan Li, Dorien Herremans, Soujanya Poria  

**Link**: [PDF](https://arxiv.org/pdf/2508.18321)  

**Abstract**: Large language models (LLMs) are increasingly deployed in multi-agent systems (MAS) as components of collaborative intelligence, where peer interactions dynamically shape individual decision-making. Although prior work has focused on conformity bias, we extend the analysis to examine how LLMs form trust from previous impressions, resist misinformation, and integrate peer input during interaction, key factors for achieving collective intelligence under complex social dynamics. We present KAIROS, a benchmark simulating quiz contests with peer agents of varying reliability, offering fine-grained control over conditions such as expert-novice roles, noisy crowds, and adversarial peers. LLMs receive both historical interactions and current peer responses, allowing systematic investigation into how trust, peer action, and self-confidence influence decisions. As for mitigation strategies, we evaluate prompting, supervised fine-tuning, and reinforcement learning, Group Relative Policy Optimisation (GRPO), across multiple models. Our results reveal that GRPO with multi-agent context combined with outcome-based rewards and unconstrained reasoning achieves the best overall performance, but also decreases the robustness to social influence compared to Base models. The code and datasets are available at: this https URL. 

**Abstract (ZH)**: 大规模语言模型（LLMs）越来越多地被部署在多agent系统（MAS）中，作为协作 intelligences的组成部分，其中同伴间的动态交互塑造个体决策。尽管先前的工作主要关注一致性偏差，我们扩展分析以探讨LLMs如何基于过往印象形成信任、抵制错误信息并整合同伴输入，在复杂社会动态下实现集体智能的关键因素。我们提出了KAIROS基准，模拟具有不同可靠性的同伴代理进行Quiz比赛，提供了对专家与初学者角色、嘈杂人群和敌对同伴等条件的精细控制。LLMs接收历史交互和当前同伴响应，允许系统性地研究信任、同伴行为和自我信心如何影响决策。在缓解策略方面，我们评估了提示、监督微调、强化学习以及Group Relative Policy Optimisation（GRPO）在多个模型上的应用。结果显示，结合多agent背景的GRPO并使用基于结果的奖励和未约束推理实现最佳综合性能，但相比Base模型，其抗社会影响能力较弱。相关代码和数据集可通过以下链接获取：this https URL。 

---
# What Matters in Data for DPO? 

**Title (ZH)**: What Matters in Data for DPO？（在DPO中重要的数据是什么？） 

**Authors**: Yu Pan, Zhongze Cai, Guanting Chen, Huaiyang Zhong, Chonghuan Wang  

**Link**: [PDF](https://arxiv.org/pdf/2508.18312)  

**Abstract**: Direct Preference Optimization (DPO) has emerged as a simple and effective approach for aligning large language models (LLMs) with human preferences, bypassing the need for a learned reward model. Despite its growing adoption, a fundamental question remains open: what characteristics of preference data are most critical for DPO performance? In this work, we provide a systematic study of how preference data distribution influences DPO, from both theoretical and empirical perspectives. We show that the quality of chosen responses plays a dominant role in optimizing the DPO objective, while the quality of rejected responses may have relatively limited impact. Our theoretical analysis characterizes the optimal response distribution under DPO and reveals how contrastiveness between responses helps primarily by improving the chosen samples. We further study an online DPO setting and show it effectively reduces to supervised fine-tuning on the chosen responses. Extensive experiments across diverse tasks confirm our findings: improving the quality of chosen responses consistently boosts performance regardless of the quality of the rejected responses. We also investigate the benefit of mixing the on-policy data. Our results interpret the mechanism behind some widely adopted strategies and offer practical insights for constructing high-impact preference datasets for LLM alignment. 

**Abstract (ZH)**: 直接偏好优化（DPO）作为一种简单有效的大型语言模型（LLMs）与人类偏好对齐的方法，已经引起了人们的关注，并且绕过了学习奖励模型的需要。尽管其采用正在增长，但仍有一个基本问题悬而未决：哪一些偏好数据的特性对DPO性能最为关键？在本文中，我们从理论和实证两个角度系统研究了偏好数据分布如何影响DPO。我们证明了所选响应的质量在优化DPO目标方面起到了主导作用，而被拒绝响应的质量可能对性能的影响相对有限。我们的理论分析界定了DPO下的最优响应分布，并揭示了响应间的对比性主要通过提高所选样本的质量来发挥作用。我们进一步研究了在线DPO设置，并表明它有效减少为仅针对所选响应的监督微调。广泛的跨任务实验验证了我们的发现：无论被拒绝响应的质量如何，提高所选响应的质量始终能提升性能。我们还研究了在线策略数据混用的好处。我们的结果解释了一些广泛采用策略的机制，并为构建高影响力的偏好数据集以实现LLM对齐提供了实用指导。 

---
# SALMAN: Stability Analysis of Language Models Through the Maps Between Graph-based Manifolds 

**Title (ZH)**: SALMAN: 基于图流形之间映射的语言模型稳定性分析 

**Authors**: Wuxinlin Cheng, Yupeng Cao, Jinwen Wu, Koduvayur Subbalakshmi, Tian Han, Zhuo Feng  

**Link**: [PDF](https://arxiv.org/pdf/2508.18306)  

**Abstract**: Recent strides in pretrained transformer-based language models have propelled state-of-the-art performance in numerous NLP tasks. Yet, as these models grow in size and deployment, their robustness under input perturbations becomes an increasingly urgent question. Existing robustness methods often diverge between small-parameter and large-scale models (LLMs), and they typically rely on labor-intensive, sample-specific adversarial designs. In this paper, we propose a unified, local (sample-level) robustness framework (SALMAN) that evaluates model stability without modifying internal parameters or resorting to complex perturbation heuristics. Central to our approach is a novel Distance Mapping Distortion (DMD) measure, which ranks each sample's susceptibility by comparing input-to-output distance mappings in a near-linear complexity manner. By demonstrating significant gains in attack efficiency and robust training, we position our framework as a practical, model-agnostic tool for advancing the reliability of transformer-based NLP systems. 

**Abstract (ZH)**: 最近预训练变换器语言模型的进步已在众多NLP任务中推动了最先进的性能。然而，随着这些模型的规模扩大和部署，它们在输入 perturbations 下的鲁棒性变得愈发迫切。现有的鲁棒性方法往往在小参数模型和大规模语言模型（LLMs）之间分岔，并且通常依赖于劳动密集型、样本特定的对抗性设计。在本文中，我们提出了一种统一的、局部（样本级）鲁棒性框架（SALMAN），该框架在不修改内部参数或依赖复杂扰动启发式方法的情况下评估模型稳定性。我们方法的核心是新颖的距离映射失真（DMD）度量，该度量通过近乎线性复杂度方式比较输入到输出的距离映射来评估每个样本的易受影响性。通过在攻击效率和鲁棒训练中取得显著提升，我们将该框架定位为一种实用的、模型无分的工具，用于提高基于变换器的NLP系统的可靠性。 

---
# Can VLMs Recall Factual Associations From Visual References? 

**Title (ZH)**: 基于视觉参考，大型语言模型能否回忆起事实性关联？ 

**Authors**: Dhananjay Ashok, Ashutosh Chaubey, Hirona J. Arai, Jonathan May, Jesse Thomason  

**Link**: [PDF](https://arxiv.org/pdf/2508.18297)  

**Abstract**: Through a controlled study, we identify a systematic deficiency in the multimodal grounding of Vision Language Models (VLMs). While VLMs can recall factual associations when provided a textual reference to an entity; their ability to do so is significantly diminished when the reference is visual instead. Forcing VLMs to rely on image representations of an entity halves their ability to recall factual knowledge, suggesting that VLMs struggle to link their internal knowledge of an entity with its image representation. We show that such linking failures are correlated with the expression of distinct patterns in model internal states, and that probes on these internal states achieve over 92% accuracy at flagging cases where the VLM response is unreliable. These probes can be applied, without retraining, to identify when a VLM will fail to correctly answer a question that requires an understanding of multimodal input. When used to facilitate selective prediction on a visual question answering task, the probes increase coverage by 7.87% (absolute) while also reducing the risk of error by 0.9% (absolute). Addressing the systematic, detectable deficiency is an important avenue in language grounding, and we provide informed recommendations for future directions. 

**Abstract (ZH)**: 通过受控研究，我们识别出视觉语言模型在多模态 grounding 方面系统的缺陷。当提供文本参考时，视觉语言模型可以回忆起事实关联；然而，当参考是视觉形式时，它们的能力会显著下降。迫使视觉语言模型依赖于实体的图像表示将其能力减半，表明视觉语言模型在将其对实体的内部知识与其图像表示链接方面存在困难。我们表明，此类链接失败与模型内部状态的特定模式表达相关，并且针对这些内部状态的探针在标识视觉语言模型响应不可靠的情况下能达到超过92%的准确性。这些探针可以在不需要重新训练的情况下应用于识别视觉语言模型在需要理解多模态输入的问题上失败的情况。在用于辅助视觉问答任务的可选预测时，探针使覆盖率提高7.87%（绝对值），同时将错误风险降低0.9%（绝对值）。解决可检测的系统缺陷是语言 grounding 中的重要途径，我们提供了对未来方向的知情建议。 

---
# Consensus Is All You Need: Gossip-Based Reasoning Among Large Language Models 

**Title (ZH)**: 共识即一切：大型语言模型之间的闲聊式推理 

**Authors**: Saksham Arora  

**Link**: [PDF](https://arxiv.org/pdf/2508.18292)  

**Abstract**: Large language models have advanced rapidly, but no single model excels in every area -- each has its strengths and weaknesses. Instead of relying on one model alone, we take inspiration from gossip protocols in distributed systems, where information is exchanged with peers until they all come to an agreement. In this setup, models exchange answers and gradually work toward a shared solution. Each LLM acts as a node in a peer-to-peer network, sharing responses and thought processes to reach a collective decision. Our results show that this "gossip-based consensus" leads to robust, resilient, and accurate multi-agent AI reasoning. It helps overcome the weaknesses of individual models and brings out their collective strengths. This approach is similar to how humans build consensus, making AI seem more collaborative and trustworthy instead of just a black-box program. 

**Abstract (ZH)**: 大型语言模型取得了快速进展，但没有单一模型在所有领域都表现出色——每种模型都有自身的优点和缺点。我们借鉴分布式系统中 gossip 协议的思想，让模型通过与同伴交换信息直至达成共识。在这个设置中，模型之间交换答案，逐步向共同的解决方案靠拢。每种语言模型都充当去中心化网络中的节点，共享响应和思维过程，以达成集体决策。我们的研究结果表明，这种基于“gossip协议的共识”方法能够实现稳健、鲁棒和准确的多智能体AI推理，有助于克服单一模型的弱点并发挥其集体优势。这种方法类似于人类达成共识的方式，使AI显得更具协同性和可信度，而非仅仅是一个黑箱程序。 

---
