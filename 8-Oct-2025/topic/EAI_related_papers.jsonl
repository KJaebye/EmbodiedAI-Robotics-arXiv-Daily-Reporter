{'arxiv_id': 'arXiv:2510.06207', 'title': 'EmbodiedCoder: Parameterized Embodied Mobile Manipulation via Modern Coding Model', 'authors': 'Zefu Lin, Rongxu Cui, Chen Hanning, Xiangyu Wang, Junjia Xu, Xiaojuan Jin, Chen Wenbo, Hui Zhou, Lue Fan, Wenling Li, Zhaoxiang Zhang', 'link': 'https://arxiv.org/abs/2510.06207', 'abstract': "Recent advances in control robot methods, from end-to-end vision-language-action frameworks to modular systems with predefined primitives, have advanced robots' ability to follow natural language instructions. Nonetheless, many approaches still struggle to scale to diverse environments, as they often rely on large annotated datasets and offer limited this http URL this work, we introduce EmbodiedCoder, a training-free framework for open-world mobile robot manipulation that leverages coding models to directly generate executable robot trajectories. By grounding high-level instructions in code, EmbodiedCoder enables flexible object geometry parameterization and manipulation trajectory synthesis without additional data collection or this http URL coding-based paradigm provides a transparent and generalizable way to connect perception with manipulation. Experiments on real mobile robots show that EmbodiedCoder achieves robust performance across diverse long-term tasks and generalizes effectively to novel objects and this http URL results demonstrate an interpretable approach for bridging high-level reasoning and low-level control, moving beyond fixed primitives toward versatile robot intelligence. See the project page at: this https URL", 'abstract_zh': '近期，从端到端的视觉-语言-动作框架到基于预定义构建块的模块化系统，机器人控制方法的进展提高了机器人遵循自然语言指令的能力。尽管如此，许多方法仍然难以在多变的环境中扩展，因为它们往往依赖于大量标注的数据集，并提供有限的灵活性。在本项工作中，我们提出了EmbodiedCoder，这是一种无需训练的框架，利用编码模型直接生成可执行的机器人轨迹，通过将高级指令嵌入代码，EmbodiedCoder能够灵活地参数化对象几何形状和合成操作轨迹，而无需额外的数据收集或标注。基于编码的范式提供了一种透明且可泛化的连接感知与操作的方式。实验证明，EmbodiedCoder在多样化的长期任务中表现稳健，并能够有效泛化到新型对象和场景。实验结果展示了从高级推理到低级控制的可解释方法，向着具有通用性的机器人智能迈出了一步。项目页面请参见：this https URL。', 'title_zh': 'EmbodiedCoder: 基于现代编码模型的参数化体绘制移动 manipulation'}
{'arxiv_id': 'arXiv:2510.06199', 'title': 'DYMO-Hair: Generalizable Volumetric Dynamics Modeling for Robot Hair Manipulation', 'authors': 'Chengyang Zhao, Uksang Yoo, Arkadeep Narayan Chaudhury, Giljoo Nam, Jonathan Francis, Jeffrey Ichnowski, Jean Oh', 'link': 'https://arxiv.org/abs/2510.06199', 'abstract': "Hair care is an essential daily activity, yet it remains inaccessible to individuals with limited mobility and challenging for autonomous robot systems due to the fine-grained physical structure and complex dynamics of hair. In this work, we present DYMO-Hair, a model-based robot hair care system. We introduce a novel dynamics learning paradigm that is suited for volumetric quantities such as hair, relying on an action-conditioned latent state editing mechanism, coupled with a compact 3D latent space of diverse hairstyles to improve generalizability. This latent space is pre-trained at scale using a novel hair physics simulator, enabling generalization across previously unseen hairstyles. Using the dynamics model with a Model Predictive Path Integral (MPPI) planner, DYMO-Hair is able to perform visual goal-conditioned hair styling. Experiments in simulation demonstrate that DYMO-Hair's dynamics model outperforms baselines on capturing local deformation for diverse, unseen hairstyles. DYMO-Hair further outperforms baselines in closed-loop hair styling tasks on unseen hairstyles, with an average of 22% lower final geometric error and 42% higher success rate than the state-of-the-art system. Real-world experiments exhibit zero-shot transferability of our system to wigs, achieving consistent success on challenging unseen hairstyles where the state-of-the-art system fails. Together, these results introduce a foundation for model-based robot hair care, advancing toward more generalizable, flexible, and accessible robot hair styling in unconstrained physical environments. More details are available on our project page: this https URL.", 'abstract_zh': '基于模型的机器人护发系统DYMO-Hair', 'title_zh': 'DYMO-Hair: 通用的体积动力学建模方法用于机器人头发操控'}
{'arxiv_id': 'arXiv:2510.06160', 'title': 'A Preview of HoloOcean 2.0', 'authors': 'Blake Romrell, Abigail Austin, Braden Meyers, Ryan Anderson, Carter Noh, Joshua G. Mangelson', 'link': 'https://arxiv.org/abs/2510.06160', 'abstract': 'Marine robotics simulators play a fundamental role in the development of marine robotic systems. With increased focus on the marine robotics field in recent years, there has been significant interest in developing higher fidelitysimulation of marine sensors, physics, and visual rendering capabilities to support autonomous marine robot development and validation. HoloOcean 2.0, the next major release of HoloOcean, brings state-of-the-art features under a general marine simulator capable of supporting a variety of tasks. New features in HoloOcean 2.0 include migration to Unreal Engine (UE) 5.3, advanced vehicle dynamics using models from Fossen, and support for ROS2 using a custom bridge. Additional features are currently in development, including significantly more efficient ray tracing-based sidescan, forward-looking, and bathymetric sonar implementations; semantic sensors; environment generation tools; volumetric environmental effects; and realistic waves.', 'abstract_zh': '海洋机器人模拟器在海洋机器人系统发展中的作用至关重要。随着近年来对海洋机器人领域关注的增加，人们日益关注开发更高保真的海洋传感器、物理模拟和视觉渲染能力，以支持自主海洋机器人开发与验证。HoloOcean 2.0 是 HoloOcean 的下一个主要版本，提供了一系列先进的功能，可以在通用海洋模拟器中支持多种任务。HoloOcean 2.0 的新功能包括迁移到 Unreal Engine 5.3、采用 Fossen 的高级车辆动力学模型以及使用自定义桥接支持 ROS2。目前正在开发的其他功能包括基于射线 tracing 的更高效的侧视声纳、前视声纳和测深声纳实现；语义传感器；环境生成工具；体积环境效应；以及逼真的波浪。', 'title_zh': 'HoloOcean 2.0 概览'}
{'arxiv_id': 'arXiv:2510.06146', 'title': 'Vision-Guided Targeted Grasping and Vibration for Robotic Pollination in Controlled Environments', 'authors': 'Jaehwan Jeong, Tuan-Anh Vu, Radha Lahoti, Jiawen Wang, Vivek Alumootil, Sangpil Kim, M. Khalid Jawed', 'link': 'https://arxiv.org/abs/2510.06146', 'abstract': 'Robotic pollination offers a promising alternative to manual labor and bumblebee-assisted methods in controlled agriculture, where wind-driven pollination is absent and regulatory restrictions limit the use of commercial pollinators. In this work, we present and validate a vision-guided robotic framework that uses data from an end-effector mounted RGB-D sensor and combines 3D plant reconstruction, targeted grasp planning, and physics-based vibration modeling to enable precise pollination. First, the plant is reconstructed in 3D and registered to the robot coordinate frame to identify obstacle-free grasp poses along the main stem. Second, a discrete elastic rod model predicts the relationship between actuation parameters and flower dynamics, guiding the selection of optimal pollination strategies. Finally, a manipulator with soft grippers grasps the stem and applies controlled vibrations to induce pollen release. End-to-end experiments demonstrate a 92.5\\% main-stem grasping success rate, and simulation-guided optimization of vibration parameters further validates the feasibility of our approach, ensuring that the robot can safely and effectively perform pollination without damaging the flower. To our knowledge, this is the first robotic system to jointly integrate vision-based grasping and vibration modeling for automated precision pollination.', 'abstract_zh': '机器人授粉为在无风力授粉且监管限制商用授粉昆虫使用的情况下，提供了一种有希望的替代手动劳动和bumblebee辅助方法的受控农业授粉方案。本研究提出并验证了一种基于视觉的机器人框架，该框架利用末端执行器安装的RGB-D传感器数据，结合3D植物重建、目标抓取规划和基于物理的振动建模，实现精确授粉。首先，植物在3D中重建并注册到机器人坐标系中，以识别主要茎轴上的无障碍抓取姿态。其次，离散弹性杆模型预测操作参数与花朵动力学之间的关系，指导选择最优授粉策略。最后，带有软夹爪的操作器抓取茎部并施加受控振动以诱导花粉释放。端到端实验显示92.5%的主要茎抓取成功率，模拟引导的振动参数优化进一步验证了该方法的可行性，确保机器人能够安全有效地进行授粉而不损坏花朵。据我们所知，这是第一个结合基于视觉抓取和振动建模的自动化精准授粉的机器人系统。', 'title_zh': '基于视觉导向的目标抓取与振动在受控环境中的机器人授粉'}
{'arxiv_id': 'arXiv:2510.06068', 'title': 'Cross-Embodiment Dexterous Hand Articulation Generation via Morphology-Aware Learning', 'authors': 'Heng Zhang, Kevin Yuchen Ma, Mike Zheng Shou, Weisi Lin, Yan Wu', 'link': 'https://arxiv.org/abs/2510.06068', 'abstract': "Dexterous grasping with multi-fingered hands remains challenging due to high-dimensional articulations and the cost of optimization-based pipelines. Existing end-to-end methods require training on large-scale datasets for specific hands, limiting their ability to generalize across different embodiments. We propose an eigengrasp-based, end-to-end framework for cross-embodiment grasp generation. From a hand's morphology description, we derive a morphology embedding and an eigengrasp set. Conditioned on these, together with the object point cloud and wrist pose, an amplitude predictor regresses articulation coefficients in a low-dimensional space, which are decoded into full joint articulations. Articulation learning is supervised with a Kinematic-Aware Articulation Loss (KAL) that emphasizes fingertip-relevant motions and injects morphology-specific structure. In simulation on unseen objects across three dexterous hands, our model attains a 91.9% average grasp success rate with less than 0.4 seconds inference per grasp. With few-shot adaptation to an unseen hand, it achieves 85.6% success on unseen objects in simulation, and real-world experiments on this few-shot generalized hand achieve an 87% success rate. The code and additional materials will be made available upon publication on our project website this https URL.", 'abstract_zh': '基于本征抓取的跨体态端到端抓取生成框架', 'title_zh': '基于形态意识学习的跨体态灵巧手关节生成'}
{'arxiv_id': 'arXiv:2510.05985', 'title': 'AI-Enabled Capabilities to Facilitate Next-Generation Rover Surface Operations', 'authors': 'Cristina Luna, Robert Field, Steven Kay', 'link': 'https://arxiv.org/abs/2510.05985', 'abstract': 'Current planetary rovers operate at traverse speeds of approximately 10 cm/s, fundamentally limiting exploration efficiency. This work presents integrated AI systems which significantly improve autonomy through three components: (i) the FASTNAV Far Obstacle Detector (FOD), capable of facilitating sustained 1.0 m/s speeds via computer vision-based obstacle detection; (ii) CISRU, a multi-robot coordination framework enabling human-robot collaboration for in-situ resource utilisation; and (iii) the ViBEKO and AIAXR deep learning-based terrain classification studies. Field validation in Mars analogue environments demonstrated these systems at Technology Readiness Level 4, providing measurable improvements in traverse speed, classification accuracy, and operational safety for next-generation planetary missions.', 'abstract_zh': '当前行星探测车的行进速度约为10 cm/s，从根本上限制了探索效率。本研究提出了一种集成AI系统，通过三种组件显著提高自主性：（i）FASTNAV远障碍检测器（FOD），利用计算机视觉实现持续1.0 m/s的速度；（ii）CISRU，一种多机器人协作框架，旨在实现有人-机器人在现场资源利用中的协作；以及（iii）基于深度学习的地形分类研究ViBEKO和AIAXR。在火星模拟环境中进行的实地验证将这些系统的技术成熟度等级提升至4级，为下一代行星任务提供了可量化的速度提升、分类准确性和操作安全性改进。', 'title_zh': 'AI赋能的下一代探测车表层操作能力'}
{'arxiv_id': 'arXiv:2510.05957', 'title': 'Learning to Crawl: Latent Model-Based Reinforcement Learning for Soft Robotic Adaptive Locomotion', 'authors': 'Vaughn Gzenda, Robin Chhabra', 'link': 'https://arxiv.org/abs/2510.05957', 'abstract': 'Soft robotic crawlers are mobile robots that utilize soft body deformability and compliance to achieve locomotion through surface contact. Designing control strategies for such systems is challenging due to model inaccuracies, sensor noise, and the need to discover locomotor gaits. In this work, we present a model-based reinforcement learning (MB-RL) framework in which latent dynamics inferred from onboard sensors serve as a predictive model that guides an actor-critic algorithm to optimize locomotor policies. We evaluate the framework on a minimal crawler model in simulation using inertial measurement units and time-of-flight sensors as observations. The learned latent dynamics enable short-horizon motion prediction while the actor-critic discovers effective locomotor policies. This approach highlights the potential of latent-dynamics MB-RL for enabling embodied soft robotic adaptive locomotion based solely on noisy sensor feedback.', 'abstract_zh': '基于模型的强化学习在软体爬行机器人中的肢体运动优化方法', 'title_zh': '基于潜在模型的强化学习方法实现软体机器人的适应性运动学习'}
{'arxiv_id': 'arXiv:2510.05827', 'title': 'VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought Reasoning for Language-driven Grasp Generation', 'authors': 'Haoran Zhang, Shuanghao Bai, Wanqi Zhou, Yuedi Zhang, Qi Zhang, Pengxiang Ding, Cheng Chi, Donglin Wang, Badong Chen', 'link': 'https://arxiv.org/abs/2510.05827', 'abstract': 'Robotic grasping is one of the most fundamental tasks in robotic manipulation, and grasp detection/generation has long been the subject of extensive research. Recently, language-driven grasp generation has emerged as a promising direction due to its practical interaction capabilities. However, most existing approaches either lack sufficient reasoning and generalization capabilities or depend on complex modular pipelines. Moreover, current grasp foundation models tend to overemphasize dialog and object semantics, resulting in inferior performance and restriction to single-object grasping. To maintain strong reasoning ability and generalization in cluttered environments, we propose VCoT-Grasp, an end-to-end grasp foundation model that incorporates visual chain-of-thought reasoning to enhance visual understanding for grasp generation. VCoT-Grasp adopts a multi-turn processing paradigm that dynamically focuses on visual inputs while providing interpretable reasoning traces. For training, we refine and introduce a large-scale dataset, VCoT-GraspSet, comprising 167K synthetic images with over 1.36M grasps, as well as 400+ real-world images with more than 1.2K grasps, annotated with intermediate bounding boxes. Extensive experiments on both VCoT-GraspSet and real robot demonstrate that our method significantly improves grasp success rates and generalizes effectively to unseen objects, backgrounds, and distractors. More details can be found at this https URL.', 'abstract_zh': '基于视觉链式推理的端到端抓取基础模型VCoT-抓取', 'title_zh': 'VCoT-Grasp: 基于视觉链式推理的语义驱动抓取生成预训练模型'}
{'arxiv_id': 'arXiv:2510.05780', 'title': 'Human-in-the-loop Optimisation in Robot-assisted Gait Training', 'authors': 'Andreas Christou, Andreas Sochopoulos, Elliot Lister, Sethu Vijayakumar', 'link': 'https://arxiv.org/abs/2510.05780', 'abstract': "Wearable robots offer a promising solution for quantitatively monitoring gait and providing systematic, adaptive assistance to promote patient independence and improve gait. However, due to significant interpersonal and intrapersonal variability in walking patterns, it is important to design robot controllers that can adapt to the unique characteristics of each individual. This paper investigates the potential of human-in-the-loop optimisation (HILO) to deliver personalised assistance in gait training. The Covariance Matrix Adaptation Evolution Strategy (CMA-ES) was employed to continuously optimise an assist-as-needed controller of a lower-limb exoskeleton. Six healthy individuals participated over a two-day experiment. Our results suggest that while the CMA-ES appears to converge to a unique set of stiffnesses for each individual, no measurable impact on the subjects' performance was observed during the validation trials. These findings highlight the impact of human-robot co-adaptation and human behaviour variability, whose effect may be greater than potential benefits of personalising rule-based assistive controllers. Our work contributes to understanding the limitations of current personalisation approaches in exoskeleton-assisted gait rehabilitation and identifies key challenges for effective implementation of human-in-the-loop optimisation in this domain.", 'abstract_zh': '穿戴式机器人在步态监测与个性化辅助训练中的潜在应用：基于人类在环优化的研究', 'title_zh': '有人参与的环路优化在机器人辅助步态训练中的应用'}
{'arxiv_id': 'arXiv:2510.05707', 'title': 'Stable Robot Motions on Manifolds: Learning Lyapunov-Constrained Neural Manifold ODEs', 'authors': 'David Boetius, Abdelrahman Abdelnaby, Ashok Kumar, Stefan Leue, Abdalla Swikir, Fares J. Abu-Dakka', 'link': 'https://arxiv.org/abs/2510.05707', 'abstract': "Learning stable dynamical systems from data is crucial for safe and reliable robot motion planning and control. However, extending stability guarantees to trajectories defined on Riemannian manifolds poses significant challenges due to the manifold's geometric constraints. To address this, we propose a general framework for learning stable dynamical systems on Riemannian manifolds using neural ordinary differential equations. Our method guarantees stability by projecting the neural vector field evolving on the manifold so that it strictly satisfies the Lyapunov stability criterion, ensuring stability at every system state. By leveraging a flexible neural parameterisation for both the base vector field and the Lyapunov function, our framework can accurately represent complex trajectories while respecting manifold constraints by evolving solutions directly on the manifold. We provide an efficient training strategy for applying our framework and demonstrate its utility by solving Riemannian LASA datasets on the unit quaternion (S^3) and symmetric positive-definite matrix manifolds, as well as robotic motions evolving on \\mathbb{R}^3 \\times S^3. We demonstrate the performance, scalability, and practical applicability of our approach through extensive simulations and by learning robot motions in a real-world experiment.", 'abstract_zh': '从数据中学习稳定动力系统对于机器人运动规划和控制的安全性和可靠性至关重要。然而，将稳定性保证扩展到定义在黎曼流形上的轨迹由于流形的几何约束而面临着重大挑战。为此，我们提出了一种使用神经常微分方程在黎曼流形上学习稳定动力系统的通用框架。通过将神经向量场在流形上的演化投影，使其严格满足李雅普un诺夫稳定性准则，我们的方法保证了每个系统状态的稳定性。借助灵活的神经参数化方法来表示基向量场和李雅普un诺夫函数，并直接在流形上演化解，我们的框架可以在遵守流形约束的同时准确地表示复杂的轨迹。我们提供了一种高效的训练策略来应用该框架，并通过在单位四元数（S^3）和对称正定矩阵流形上求解Riemannian LASA数据集，以及在\\(\\mathbb{R}^3 \\times S^3\\)上求解机器人运动，展示了其实用性和有效性。我们通过广泛的仿真实验和现实世界实验中的机器人运动学习，展示了该方法的性能、可扩展性和实际适用性。', 'title_zh': '流形上机器人运动的稳定性：学习Lyapunov约束神经流形ODE方法'}
{'arxiv_id': 'arXiv:2510.05692', 'title': 'Oracle-Guided Masked Contrastive Reinforcement Learning for Visuomotor Policies', 'authors': 'Yuhang Zhang, Jiaping Xiao, Chao Yan, Mir Feroskhan', 'link': 'https://arxiv.org/abs/2510.05692', 'abstract': 'A prevailing approach for learning visuomotor policies is to employ reinforcement learning to map high-dimensional visual observations directly to action commands. However, the combination of high-dimensional visual inputs and agile maneuver outputs leads to long-standing challenges, including low sample efficiency and significant sim-to-real gaps. To address these issues, we propose Oracle-Guided Masked Contrastive Reinforcement Learning (OMC-RL), a novel framework designed to improve the sample efficiency and asymptotic performance of visuomotor policy learning. OMC-RL explicitly decouples the learning process into two stages: an upstream representation learning stage and a downstream policy learning stage. In the upstream stage, a masked Transformer module is trained with temporal modeling and contrastive learning to extract temporally-aware and task-relevant representations from sequential visual inputs. After training, the learned encoder is frozen and used to extract visual representations from consecutive frames, while the Transformer module is discarded. In the downstream stage, an oracle teacher policy with privileged access to global state information supervises the agent during early training to provide informative guidance and accelerate early policy learning. This guidance is gradually reduced to allow independent exploration as training progresses. Extensive experiments in simulated and real-world environments demonstrate that OMC-RL achieves superior sample efficiency and asymptotic policy performance, while also improving generalization across diverse and perceptually complex scenarios.', 'abstract_zh': 'Oracle-Guided Masked Contrastive Reinforcement Learning', 'title_zh': '由Oracle引导的掩蔽对比强化学习用于视觉运动策略'}
{'arxiv_id': 'arXiv:2510.05681', 'title': 'Verifier-free Test-Time Sampling for Vision Language Action Models', 'authors': 'Suhyeok Jang, Dongyoung Kim, Changyeon Kim, Youngsuk Kim, Jinwoo Shin', 'link': 'https://arxiv.org/abs/2510.05681', 'abstract': "Vision-Language-Action models (VLAs) have demonstrated remarkable performance in robot control. However, they remain fundamentally limited in tasks that require high precision due to their single-inference paradigm. While test-time scaling approaches using external verifiers have shown promise, they require additional training and fail to generalize to unseen conditions. We propose Masking Distribution Guided Selection (MG-Select), a novel test-time scaling framework for VLAs that leverages the model's internal properties without requiring additional training or external modules. Our approach utilizes KL divergence from a reference action token distribution as a confidence metric for selecting the optimal action from multiple candidates. We introduce a reference distribution generated by the same VLA but with randomly masked states and language conditions as inputs, ensuring maximum uncertainty while remaining aligned with the target task distribution. Additionally, we propose a joint training strategy that enables the model to learn both conditional and unconditional distributions by applying dropout to state and language conditions, thereby further improving the quality of the reference distribution. Our experiments demonstrate that MG-Select achieves significant performance improvements, including a 28%/35% improvement in real-world in-distribution/out-of-distribution tasks, along with a 168% relative gain on RoboCasa pick-and-place tasks trained with 30 demonstrations.", 'abstract_zh': 'Vision-Language-Action模型（VLAs）在机器人控制任务中展现了显著性能，但在需要高精度的任务中仍受到单次推断范式的限制。虽然利用外部验证器的测试时可扩展方法显示出 promise，但它们需要额外训练且不能向未见条件泛化。我们提出了一种名为 Masking Distribution Guided Selection（MG-Select）的新颖测试时可扩展框架，该框架利用模型的内部特性，无需额外训练或外部模块。我们的方法利用参考动作标记分布的 KL 散度作为选择多个候选动作中最佳动作的信心度量。我们通过使用随机遮蔽状态和语言条件输入生成相同的 VLA 的参考分布，确保最大不确定性同时保持与目标任务分布的对齐。此外，我们提出了一种联合训练策略，通过在状态和语言条件下应用 dropout 来使模型学习条件和非条件分布，从而进一步提高参考分布的质量。实验结果显示，MG-Select 在现实世界同分布/异分布任务中取得了显著性能提升，分别为 28%/35% 的提高，并且在使用 30 次演示训练的 RoboCasa Pick-And-Place 任务中相对增益达到 168%。', 'title_zh': '无需验证的测试时采样方法：面向视觉语言动作模型'}
{'arxiv_id': 'arXiv:2510.05662', 'title': 'DeLTa: Demonstration and Language-Guided Novel Transparent Object Manipulation', 'authors': 'Taeyeop Lee, Gyuree Kang, Bowen Wen, Youngho Kim, Seunghyeok Back, In So Kweon, David Hyunchul Shim, Kuk-Jin Yoon', 'link': 'https://arxiv.org/abs/2510.05662', 'abstract': 'Despite the prevalence of transparent object interactions in human everyday life, transparent robotic manipulation research remains limited to short-horizon tasks and basic grasping this http URL some methods have partially addressed these issues, most of them have limitations in generalizability to novel objects and are insufficient for precise long-horizon robot manipulation. To address this limitation, we propose DeLTa (Demonstration and Language-Guided Novel Transparent Object Manipulation), a novel framework that integrates depth estimation, 6D pose estimation, and vision-language planning for precise long-horizon manipulation of transparent objects guided by natural task instructions. A key advantage of our method is its single-demonstration approach, which generalizes 6D trajectories to novel transparent objects without requiring category-level priors or additional training. Additionally, we present a task planner that refines the VLM-generated plan to account for the constraints of a single-arm, eye-in-hand robot for long-horizon object manipulation tasks. Through comprehensive evaluation, we demonstrate that our method significantly outperforms existing transparent object manipulation approaches, particularly in long-horizon scenarios requiring precise manipulation capabilities. Project page: this https URL', 'abstract_zh': '尽管透明物体交互在人类日常生活中非常普遍，但透明机器人操作研究仍主要集中在短期任务和基本夹取任务中。尽管一些方法部分解决了这些问题，但大多数方法在新颖物体上的泛化能力有限，不足以进行精确的长时间机器人操作。为解决这一限制，我们提出了DeLTa（示范和语言引导的新颖透明物体操作），这是一种新颖的框架，结合了深度估计、6D姿态估计和基于视觉-语言规划的方法，以自然的任务指令为指导进行精确的长时间透明物体操作。我们方法的一个关键优势是其单示范方法，该方法可以在不需要类别级别的先验知识或额外训练的情况下将6D轨迹泛化到新颖的透明物体上。此外，我们还提出了一种任务规划器，用于根据单臂手持式机器人的时间限制对视觉-语言模型生成的计划进行细化。通过全面评估，我们证明了我们的方法在需要精确操作能力的时间较长的任务中显著优于现有透明物体操作方法。项目页面：这个 <https://>。', 'title_zh': 'DeLTa: 示范与语言引导的新颖透明物体操控'}
{'arxiv_id': 'arXiv:2510.05547', 'title': 'ARRC: Advanced Reasoning Robot Control - Knowledge-Driven Autonomous Manipulation Using Retrieval-Augmented Generation', 'authors': 'Eugene Vorobiov, Ammar Jaleel Mahmood, Salim Rezvani, Robin Chhabra', 'link': 'https://arxiv.org/abs/2510.05547', 'abstract': 'We present ARRC (Advanced Reasoning Robot Control), a practical system that connects natural-language instructions to safe local robotic control by combining Retrieval-Augmented Generation (RAG) with RGB-D perception and guarded execution on an affordable robot arm. The system indexes curated robot knowledge (movement patterns, task templates, and safety heuristics) in a vector database, retrieves task-relevant context for each instruction, and conditions a large language model (LLM) to produce JSON-structured action plans. Plans are executed on a UFactory xArm 850 fitted with a Dynamixel-driven parallel gripper and an Intel RealSense D435 camera. Perception uses AprilTag detections fused with depth to produce object-centric metric poses. Execution is enforced via software safety gates: workspace bounds, speed and force caps, timeouts, and bounded retries. We describe the architecture, knowledge design, integration choices, and a reproducible evaluation protocol for tabletop scan, approach, and pick-place tasks. Experimental results demonstrate the efficacy of the proposed approach. Our design shows that RAG-based planning can substantially improve plan validity and adaptability while keeping perception and low-level control local to the robot.', 'abstract_zh': '我们提出ARRC（Advanced Reasoning Robot Control），一种将自然语言指令连接到通过检索增强生成（RAG）结合RGB-D感知和防护执行与可负担得起的机械臂上局部机器人控制的安全系统。该系统在一个向量数据库中索引策划的机器人知识（运动模式、任务模板和安全启发式方法），为每条指令检索任务相关的上下文，并促使大语言模型生成JSON结构化行动方案。方案在配备有Dynamixel驱动并联 gripper 和Intel RealSense D435相机的UFactory xArm 850上执行。感知使用融合深度的AprilTag检测以产生对象中心的度量姿姿。执行通过软件安全门管控：工作空间界限、速度和力限制、超时和有界的重试。我们描述了该系统的架构、知识设计、集成选择以及可重复评估协议。实验结果证明了提出方法的有效性。我们设计表明，基于RAG的规划可以显著提高方案的有效性和适应性，同时保持感知和低级控制的局部性。', 'title_zh': 'ARRC: 高级推理机器人控制 - 基于检索增强生成的知识驱动自主操作'}
{'arxiv_id': 'arXiv:2510.05430', 'title': 'Active Semantic Perception', 'authors': 'Huayi Tang, Pratik Chaudhari', 'link': 'https://arxiv.org/abs/2510.05430', 'abstract': 'We develop an approach for active semantic perception which refers to using the semantics of the scene for tasks such as exploration. We build a compact, hierarchical multi-layer scene graph that can represent large, complex indoor environments at various levels of abstraction, e.g., nodes corresponding to rooms, objects, walls, windows etc. as well as fine-grained details of their geometry. We develop a procedure based on large language models (LLMs) to sample plausible scene graphs of unobserved regions that are consistent with partial observations of the scene. These samples are used to compute an information gain of a potential waypoint for sophisticated spatial reasoning, e.g., the two doors in the living room can lead to either a kitchen or a bedroom. We evaluate this approach in complex, realistic 3D indoor environments in simulation. We show using qualitative and quantitative experiments that our approach can pin down the semantics of the environment quicker and more accurately than baseline approaches.', 'abstract_zh': '基于场景语义的有见证知方法及其在复杂室内环境中的应用', 'title_zh': '主动语义感知'}
{'arxiv_id': 'arXiv:2510.05330', 'title': 'Adaptive Dynamics Planning for Robot Navigation', 'authors': 'Lu Yuanjie, Mao Mingyang, Xu Tong, Wang Linji, Lin Xiaomin, Xiao Xuesu', 'link': 'https://arxiv.org/abs/2510.05330', 'abstract': 'Autonomous robot navigation systems often rely on hierarchical planning, where global planners compute collision-free paths without considering dynamics, and local planners enforce dynamics constraints to produce executable commands. This discontinuity in dynamics often leads to trajectory tracking failure in highly constrained environments. Recent approaches integrate dynamics within the entire planning process by gradually decreasing its fidelity, e.g., increasing integration steps and reducing collision checking resolution, for real-time planning efficiency. However, they assume that the fidelity of the dynamics should decrease according to a manually designed scheme. Such static settings fail to adapt to environmental complexity variations, resulting in computational overhead in simple environments or insufficient dynamics consideration in obstacle-rich scenarios. To overcome this limitation, we propose Adaptive Dynamics Planning (ADP), a learning-augmented paradigm that uses reinforcement learning to dynamically adjust robot dynamics properties, enabling planners to adapt across diverse environments. We integrate ADP into three different planners and further design a standalone ADP-based navigation system, benchmarking them against other baselines. Experiments in both simulation and real-world tests show that ADP consistently improves navigation success, safety, and efficiency.', 'abstract_zh': '自主机器人导航系统常常依赖层次规划，其中全局规划器在不考虑动力学的情况下计算无碰撞路径，而局部规划器则施加动力学约束以生成可执行的命令。这种动力学上的不连续性往往会导致在高度受限环境中轨迹跟踪失败。最近的方法通过逐渐降低动力学的精度（例如增加积分步数和减少碰撞检测的分辨率）来在实时规划中提高效率，但它们假设动力学精度应根据人为设计的方案逐渐降低。这种静态设置无法适应环境复杂性的变化，在简单环境中导致计算开销，而在障碍物丰富的场景中则未能充分考虑动力学。为克服这一局限，我们提出了一种自适应动力学规划（ADP）方法，该方法利用强化学习动态调整机器人动力学属性，使规划器能够在不同环境中灵活适应。我们将ADP整合到三种不同的规划器中，并进一步设计了一个基于ADP的独立导航系统，与其它基线进行比较。在模拟和实际测试中的实验表明，ADP持续提升了导航的成功率、安全性和效率。', 'title_zh': '自适应动力学规划机器人导航'}
{'arxiv_id': 'arXiv:2510.05213', 'title': 'VER: Vision Expert Transformer for Robot Learning via Foundation Distillation and Dynamic Routing', 'authors': 'Yixiao Wang, Mingxiao Huo, Zhixuan Liang, Yushi Du, Lingfeng Sun, Haotian Lin, Jinghuan Shang, Chensheng Peng, Mohit Bansal, Mingyu Ding, Masayoshi Tomizuka', 'link': 'https://arxiv.org/abs/2510.05213', 'abstract': 'Pretrained vision foundation models (VFMs) advance robotic learning via rich visual representations, yet individual VFMs typically excel only in specific domains, limiting generality across tasks. Distilling multiple VFMs into a unified representation for policy can mitigate this limitation but often yields inflexible task-specific feature selection and requires costly full re-training to incorporate robot-domain knowledge. We propose VER, a Vision Expert transformer for Robot learning. During pretraining, VER distills multiple VFMs into a vision expert library. It then fine-tunes only a lightweight routing network (fewer than 0.4% of parameters) to dynamically select task-relevant experts from the pretrained library for downstream robot tasks. We further introduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve both flexibility and precision of dynamic expert selection. Moreover, VER supports parameter-efficient finetuning for scalable expert utilization and adaptive robot-domain knowledge integration. Across 17 diverse robotic tasks and multiple policy heads, VER achieves state-of-the-art performance. We find that VER reduces large-norm outliers in task-irrelevant regions (e.g., background) and concentrates on task-critical regions. Visualizations and codes can be found in this https URL.', 'abstract_zh': 'Vision Expert Transformer for Robot Learning', 'title_zh': 'Vision Expert Transformer for Robot Learning via Foundation Distillation and Dynamic Routing'}
{'arxiv_id': 'arXiv:2510.05996', 'title': 'Information-Theoretic Policy Pre-Training with Empowerment', 'authors': 'Moritz Schneider, Robert Krug, Narunas Vaskevicius, Luigi Palmieri, Michael Volpp, Joschka Boedecker', 'link': 'https://arxiv.org/abs/2510.05996', 'abstract': "Empowerment, an information-theoretic measure of an agent's potential influence on its environment, has emerged as a powerful intrinsic motivation and exploration framework for reinforcement learning (RL). Besides for unsupervised RL and skill learning algorithms, the specific use of empowerment as a pre-training signal has received limited attention in the literature. We show that empowerment can be used as a pre-training signal for data-efficient downstream task adaptation. For this we extend the traditional notion of empowerment by introducing discounted empowerment, which balances the agent's control over the environment across short- and long-term horizons. Leveraging this formulation, we propose a novel pre-training paradigm that initializes policies to maximize discounted empowerment, enabling agents to acquire a robust understanding of environmental dynamics. We analyze empowerment-based pre-training for various existing RL algorithms and empirically demonstrate its potential as a general-purpose initialization strategy: empowerment-maximizing policies with long horizons are data-efficient and effective, leading to improved adaptability in downstream tasks. Our findings pave the way for future research to scale this framework to high-dimensional and complex tasks, further advancing the field of RL.", 'abstract_zh': '基于信息论的赋能，作为一种代理对其环境潜在影响的度量，已成为强化学习（RL）中一种强大的内在动机和探索框架。尽管赋能在无监督RL和技能学习算法中得到了应用，但将其作为预训练信号的具体使用在文献中受到的关注较少。我们展示了赋能可以作为数据高效下游任务适应的预训练信号。为此，我们通过引入折扣赋能扩展了传统赋能概念，以在短期内和长期内平衡代理对环境的控制。基于这种表述，我们提出了一种新颖的预训练范式，通过最大化折扣赋能初始化策略，使代理能够获得对环境动态的稳健理解。我们分析了基于赋能的预训练方法在各种现有RL算法中的应用，并实证证明了其作为一种通用初始化策略的潜力：长期最大化赋能的策略在数据效率和效果上表现出色，从而在下游任务中提高了可适应性。我们的发现为未来研究如何将此框架扩展到高维和复杂任务奠定了基础，进一步推动了RL领域的进步。', 'title_zh': '基于信息论的Empowerment先验策略训练'}
{'arxiv_id': 'arXiv:2510.05865', 'title': 'The Safety Challenge of World Models for Embodied AI Agents: A Review', 'authors': 'Lorenzo Baraldi, Zifan Zeng, Chongzhe Zhang, Aradhana Nayak, Hongbo Zhu, Feng Liu, Qunli Zhang, Peng Wang, Shiming Liu, Zheng Hu, Angelo Cangelosi, Lorenzo Baraldi', 'link': 'https://arxiv.org/abs/2510.05865', 'abstract': "The rapid progress in embodied artificial intelligence has highlighted the necessity for more advanced and integrated models that can perceive, interpret, and predict environmental dynamics. In this context, World Models (WMs) have been introduced to provide embodied agents with the abilities to anticipate future environmental states and fill in knowledge gaps, thereby enhancing agents' ability to plan and execute actions. However, when dealing with embodied agents it is fundamental to ensure that predictions are safe for both the agent and the environment. In this article, we conduct a comprehensive literature review of World Models in the domains of autonomous driving and robotics, with a specific focus on the safety implications of scene and control generation tasks. Our review is complemented by an empirical analysis, wherein we collect and examine predictions from state-of-the-art models, identify and categorize common faults (herein referred to as pathologies), and provide a quantitative evaluation of the results.", 'abstract_zh': '快速发展的具身人工智能突显了需要更加先进和集成的模型以感知、解释和预测环境动态的必要性。在这种背景下，世界模型（WMs）被引入以赋予具身代理预测未来环境状态和填补知识空白的能力，从而增强代理计划和执行动作的能力。然而，在处理具身代理时，确保预测对代理和环境都是安全的至关重要。本文对自主驾驶和机器人领域的世界模型进行了全面的文献综述，特别关注场景和控制生成任务的安全影响。我们的综述通过实证分析予以补充，其中我们收集并分析了最先进的模型的预测，识别并分类常见的病态现象（ herein referred to as pathologies），并提供了结果的定量评估。', 'title_zh': '具身AI代理的世界模型安全挑战：一项综述'}
{'arxiv_id': 'arXiv:2510.05684', 'title': 'D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI', 'authors': 'Suwhan Choi, Jaeyoon Jung, Haebin Seong, Minchan Kim, Minyeong Kim, Yongjun Cho, Yoonshik Kim, Yubeen Park, Youngjae Yu, Yunsung Lee', 'link': 'https://arxiv.org/abs/2510.05684', 'abstract': 'Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environments -- particularly gaming -- offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152x compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at this https URL', 'abstract_zh': '桌面到具身AI：基于桌面环境的具身人工智能预训练框架', 'title_zh': 'D2E: 在桌面数据上扩展视觉-动作预训练以转移至具身AI'}
{'arxiv_id': 'arXiv:2510.05580', 'title': 'MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption', 'authors': 'Chen Li, Zhantao Yang, Han Zhang, Fangyi Chen, Chenchen Zhu, Anudeepsekhar Bolimera, Marios Savvides', 'link': 'https://arxiv.org/abs/2510.05580', 'abstract': 'Vision-Language-Action (VLA) models show promise in embodied reasoning, yet remain far from true generalists-they often require task-specific fine-tuning, and generalize poorly to unseen tasks. We propose MetaVLA, a unified, backbone-agnostic post-training framework for efficient and scalable alignment. MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse target tasks into a single fine-tuning stage while leveraging structurally diverse auxiliary tasks to improve in-domain generalization. Unlike naive multi-task SFT, MetaVLA integrates a lightweight meta-learning mechanism-derived from Attentive Neural Processes-to enable rapid adaptation from diverse contexts with minimal architectural change or inference overhead. On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K, and cuts GPU time by ~76%. These results show that scalable, low-resource post-training is achievable-paving the way toward general-purpose embodied agents. Code will be available.', 'abstract_zh': 'MetaVLA：一种统一的后训练框架，实现高效可扩展的元视角视觉-语言-行动模型对齐', 'title_zh': '元多任务协同训练：统一的元协同训练方法以实现高效的感知适应'}
{'arxiv_id': 'arXiv:2510.06189', 'title': 'Barbarians at the Gate: How AI is Upending Systems Research', 'authors': 'Audrey Cheng, Shu Liu, Melissa Pan, Zhifei Li, Bowen Wang, Alex Krentsel, Tian Xia, Mert Cemri, Jongseok Park, Shuo Yang, Jeff Chen, Aditya Desai, Jiarong Xing, Koushik Sen, Matei Zaharia, Ion Stoica', 'link': 'https://arxiv.org/abs/2510.06189', 'abstract': 'Artificial Intelligence (AI) is starting to transform the research process as we know it by automating the discovery of new solutions. Given a task, the typical AI-driven approach is (i) to generate a set of diverse solutions, and then (ii) to verify these solutions and select one that solves the problem. Crucially, this approach assumes the existence of a reliable verifier, i.e., one that can accurately determine whether a solution solves the given problem. We argue that systems research, long focused on designing and evaluating new performance-oriented algorithms, is particularly well-suited for AI-driven solution discovery. This is because system performance problems naturally admit reliable verifiers: solutions are typically implemented in real systems or simulators, and verification reduces to running these software artifacts against predefined workloads and measuring performance. We term this approach as AI-Driven Research for Systems (ADRS), which iteratively generates, evaluates, and refines solutions. Using penEvolve, an existing open-source ADRS instance, we present case studies across diverse domains, including load balancing for multi-region cloud scheduling, Mixture-of-Experts inference, LLM-based SQL queries, and transaction scheduling. In multiple instances, ADRS discovers algorithms that outperform state-of-the-art human designs (e.g., achieving up to 5.0x runtime improvements or 50% cost reductions). We distill best practices for guiding algorithm evolution, from prompt design to evaluator construction, for existing frameworks. We then discuss the broader implications for the systems community: as AI assumes a central role in algorithm design, we argue that human researchers will increasingly focus on problem formulation and strategic guidance. Our results highlight both the disruptive potential and the urgent need to adapt systems research practices in the age of AI.', 'abstract_zh': '人工智能驱动的系统研究（ADRS）：从发现到验证的迭代过程', 'title_zh': '门外的野蛮人：AI是如何颠覆系统研究的'}
{'arxiv_id': 'arXiv:2510.05596', 'title': 'From Agentification to Self-Evolving Agentic AI for Wireless Networks: Concepts, Approaches, and Future Research Directions', 'authors': 'Changyuan Zhao, Ruichen Zhang, Jiacheng Wang, Dusit Niyato, Geng Sun, Xianbin Wang, Shiwen Mao, Abbas Jamalipour', 'link': 'https://arxiv.org/abs/2510.05596', 'abstract': 'Self-evolving agentic artificial intelligence (AI) offers a new paradigm for future wireless systems by enabling autonomous agents to continually adapt and improve without human intervention. Unlike static AI models, self-evolving agents embed an autonomous evolution cycle that updates models, tools, and workflows in response to environmental dynamics. This paper presents a comprehensive overview of self-evolving agentic AI, highlighting its layered architecture, life cycle, and key techniques, including tool intelligence, workflow optimization, self-reflection, and evolutionary learning. We further propose a multi-agent cooperative self-evolving agentic AI framework, where multiple large language models (LLMs) are assigned role-specialized prompts under the coordination of a supervisor agent. Through structured dialogue, iterative feedback, and systematic validation, the system autonomously executes the entire life cycle without human intervention. A case study on antenna evolution in low-altitude wireless networks (LAWNs) demonstrates how the framework autonomously upgrades fixed antenna optimization into movable antenna optimization. Experimental results show that the proposed self-evolving agentic AI autonomously improves beam gain and restores degraded performance by up to 52.02%, consistently surpassing the fixed baseline with little to no human intervention and validating its adaptability and robustness for next-generation wireless intelligence.', 'abstract_zh': '自主演变代理人工智能：未来无线系统的新型范式及其在低高度无线网络天线优化中的自主升级应用', 'title_zh': '从代理化到自主进化的能动AI无线网络：概念、方法及未来研究方向'}
{'arxiv_id': 'arXiv:2510.06218', 'title': 'EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark', 'authors': 'Deheng Zhang, Yuqian Fu, Runyi Yang, Yang Miao, Tianwen Qian, Xu Zheng, Guolei Sun, Ajad Chhatkuli, Xuanjing Huang, Yu-Gang Jiang, Luc Van Gool, Danda Pani Paudel', 'link': 'https://arxiv.org/abs/2510.06218', 'abstract': 'Most existing benchmarks for egocentric vision understanding focus primarily on daytime scenarios, overlooking the low-light conditions that are inevitable in real-world applications. To investigate this gap, we present EgoNight, the first comprehensive benchmark for nighttime egocentric vision, with visual question answering (VQA) as the core task. A key feature of EgoNight is the introduction of day-night aligned videos, which enhance night annotation quality using the daytime data and reveal clear performance gaps between lighting conditions. To achieve this, we collect both synthetic videos rendered by Blender and real-world recordings, ensuring that scenes and actions are visually and temporally aligned. Leveraging these paired videos, we construct EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and refinement through extensive human verification. Each QA pair is double-checked by annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs across 90 videos, spanning 12 diverse QA types, with more than 300 hours of human work. Evaluations of state-of-the-art multimodal large language models (MLLMs) reveal substantial performance drops when transferring from day to night, underscoring the challenges of reasoning under low-light conditions. Beyond VQA, EgoNight also introduces two auxiliary tasks, day-night correspondence retrieval and egocentric depth estimation at night, that further explore the boundaries of existing models. We believe EgoNight-VQA provides a strong foundation for advancing application-driven egocentric vision research and for developing models that generalize across illumination domains. All the data and code will be made available upon acceptance.', 'abstract_zh': 'EgoNight：面向夜间第一人称视觉理解的综合基准', 'title_zh': 'EgoNight：夜间自视角视觉理解的挑战性基准研究'}
{'arxiv_id': 'arXiv:2510.06203', 'title': 'Reference Grounded Skill Discovery', 'authors': 'Seungeun Rho, Aaron Trinh, Danfei Xu, Sehoon Ha', 'link': 'https://arxiv.org/abs/2510.06203', 'abstract': 'Scaling unsupervised skill discovery algorithms to high-DoF agents remains challenging. As dimensionality increases, the exploration space grows exponentially, while the manifold of meaningful skills remains limited. Therefore, semantic meaningfulness becomes essential to effectively guide exploration in high-dimensional spaces. In this work, we present Reference-Grounded Skill Discovery (RGSD), a novel algorithm that grounds skill discovery in a semantically meaningful latent space using reference data. RGSD first performs contrastive pretraining to embed motions on a unit hypersphere, clustering each reference trajectory into a distinct direction. This grounding enables skill discovery to simultaneously involve both imitation of reference behaviors and the discovery of semantically related diverse behaviors. On a simulated SMPL humanoid with 359-D observations and 69-D actions, RGSD learns structured skills including walking, running, punching, and side stepping, and also discovers related novel behaviors. In downstream control tasks, RGSD outperforms imitation-based skill acquisition baselines. Our results suggest that lightweight reference-guided grounding offers a practical path to discovering semantically rich and structured skills in high-DoF systems.', 'abstract_zh': '高自由度代理无监督技能发现算法的扩展仍具挑战性。RGSD：基于参考的数据 grounding 技能发现算法', 'title_zh': '基于参考的技能发现'}
{'arxiv_id': 'arXiv:2510.06138', 'title': 'Multi-Task Reinforcement Learning with Language-Encoded Gated Policy Networks', 'authors': 'Rushiv Arora', 'link': 'https://arxiv.org/abs/2510.06138', 'abstract': 'Multi-task reinforcement learning often relies on task metadata -- such as brief natural-language descriptions -- to guide behavior across diverse objectives. We present Lexical Policy Networks (LEXPOL), a language-conditioned mixture-of-policies architecture for multi-task RL. LEXPOL encodes task metadata with a text encoder and uses a learned gating module to select or blend among multiple sub-policies, enabling end-to-end training across tasks. On MetaWorld benchmarks, LEXPOL matches or exceeds strong multi-task baselines in success rate and sample efficiency, without task-specific retraining. To analyze the mechanism, we further study settings with fixed expert policies obtained independently of the gate and show that the learned language gate composes these experts to produce behaviors appropriate to novel task descriptions and unseen task combinations. These results indicate that natural-language metadata can effectively index and recombine reusable skills within a single policy.', 'abstract_zh': '基于词法的策略网络：面向多任务 reinforcement learning 的语言调节混合策略架构', 'title_zh': '基于语言编码门控策略网络的多任务强化学习'}
{'arxiv_id': 'arXiv:2510.06067', 'title': 'Reasoning under Vision: Understanding Visual-Spatial Cognition in Vision-Language Models for CAPTCHA', 'authors': 'Python Song, Luke Tenyi Chang, Yun-Yun Tsai, Penghui Li, Junfeng Yang', 'link': 'https://arxiv.org/abs/2510.06067', 'abstract': 'CAPTCHA, originally designed to distinguish humans from robots, has evolved into a real-world benchmark for assessing the spatial reasoning capabilities of vision-language models. In this work, we first show that step-by-step reasoning is crucial for vision-language models (VLMs) to solve CAPTCHAs, which represent high-difficulty spatial reasoning tasks, and that current commercial vision-language models still struggle with such reasoning. In particular, we observe that most commercial VLMs (e.g., Gemini, Claude, GPT, etc.) fail to effectively solve CAPTCHAs and thus achieve low accuracy (around 21.9 percent). However, our findings indicate that requiring the model to perform step-by-step reasoning before generating the final coordinates can significantly enhance its solving accuracy, underscoring the severity of the gap. To systematically study this issue, we introduce CAPTCHA-X, the first real-world CAPTCHA benchmark with reasoning, covering seven categories of CAPTCHAs (such as Gobang, hCaptcha, etc.) with step-by-step action solutions and grounding annotations. We further define five reasoning-oriented metrics that enable a comprehensive evaluation of models reasoning capabilities. To validate the effectiveness of reasoning, we also propose a general agentic VLM-based framework that incorporates the models inherent reasoning abilities. Our method achieves state-of-the-art performance across five high-difficulty CAPTCHA types, with an average solving accuracy of 83.9 percent, substantially surpassing existing baselines. These results reveal the limitations of current models and highlight the importance of reasoning in advancing visual-spatial challenges in the future.', 'abstract_zh': 'CAPTCHA：从区分人类与机器人到评估视觉-语言模型的空间推理能力', 'title_zh': '视觉中的推理：理解视觉-空间认知在验证码视觉语言模型中的作用'}
{'arxiv_id': 'arXiv:2510.06038', 'title': 'From Learning to Mastery: Achieving Safe and Efficient Real-World Autonomous Driving with Human-In-The-Loop Reinforcement Learning', 'authors': 'Li Zeqiao, Wang Yijing, Wang Haoyu, Li Zheng, Li Peng, Liu Wenfei, Zuo Zhiqiang', 'link': 'https://arxiv.org/abs/2510.06038', 'abstract': 'Autonomous driving with reinforcement learning (RL) has significant potential. However, applying RL in real-world settings remains challenging due to the need for safe, efficient, and robust learning. Incorporating human expertise into the learning process can help overcome these challenges by reducing risky exploration and improving sample efficiency. In this work, we propose a reward-free, active human-in-the-loop learning method called Human-Guided Distributional Soft Actor-Critic (H-DSAC). Our method combines Proxy Value Propagation (PVP) and Distributional Soft Actor-Critic (DSAC) to enable efficient and safe training in real-world environments. The key innovation is the construction of a distributed proxy value function within the DSAC framework. This function encodes human intent by assigning higher expected returns to expert demonstrations and penalizing actions that require human intervention. By extrapolating these labels to unlabeled states, the policy is effectively guided toward expert-like behavior. With a well-designed state space, our method achieves real-world driving policy learning within practical training times. Results from both simulation and real-world experiments demonstrate that our framework enables safe, robust, and sample-efficient learning for autonomous driving.', 'abstract_zh': '基于强化学习的自主驾驶有巨大潜力。然而，在实际应用中将其应用仍具挑战性，因为需要确保学习过程的安全、高效和鲁棒性。通过将人类专家的知识融入学习过程，可以降低风险探索并提高样本效率，从而应对这些挑战。在本工作中，我们提出了一种无奖励、主动的人在回路学习方法，名为Human-Guided Distributional Soft Actor-Critic（H-DSAC）。该方法结合了Proxy Value Propagation（PVP）和Distributional Soft Actor-Critic（DSAC），以在现实环境中实现高效和安全的训练。关键创新在于在DSAC框架内构建分布式代理价值函数。该函数通过为专家演示分配更高的预期回报并惩罚需要人工干预的动作，来编码人类意图。通过将这些标签外推到未标记状态下，该策略得到有效指导，朝着专家级行为方向发展。通过合理设计状态空间，我们的方法得以在实际训练时间内实现自主驾驶策略的学习。来自模拟和现实世界实验的结果表明，我们的框架能够实现自主驾驶的安全、鲁棒和样本高效的learning。', 'title_zh': '从学习到精通：通过人类在环强化学习实现安全高效的现实世界自动驾驶'}
{'arxiv_id': 'arXiv:2510.05625', 'title': 'Generative AI-Driven Hierarchical Multi-Agent Framework for Zero-Touch Optical Networks', 'authors': 'Yao Zhang, Yuchen Song, Shengnan Li, Yan Shi, Shikui Shen, Xiongyan Tang, Min Zhang, Danshi Wang', 'link': 'https://arxiv.org/abs/2510.05625', 'abstract': 'The rapid development of Generative Artificial Intelligence (GenAI) has catalyzed a transformative technological revolution across all walks of life. As the backbone of wideband communication, optical networks are expecting high-level autonomous operation and zero-touch management to accommodate their expanding network scales and escalating transmission bandwidth. The integration of GenAI is deemed as the pivotal solution for realizing zero-touch optical networks. However, the lifecycle management of optical networks involves a multitude of tasks and necessitates seamless collaboration across multiple layers, which poses significant challenges to the existing single-agent GenAI systems. In this paper, we propose a GenAI-driven hierarchical multi-agent framework designed to streamline multi-task autonomous execution for zero-touch optical networks. We present the architecture, implementation, and applications of this framework. A field-deployed mesh network is utilized to demonstrate three typical scenarios throughout the lifecycle of optical network: quality of transmission estimation in the planning stage, dynamic channel adding/dropping in the operation stage, and system capacity increase in the upgrade stage. The case studies, illustrate the capabilities of multi-agent framework in multi-task allocation, coordination, execution, evaluation, and summarization. This work provides a promising approach for the future development of intelligent, efficient, and collaborative network management solutions, paving the way for more specialized and adaptive zero-touch optical networks.', 'abstract_zh': '生成式人工智能的快速发展正在推动全方位的技术革命。作为宽带通信的骨干，光网络期望实现高层次的自主运行和零接触管理以应对网络规模的扩大和传输带宽的增加。将生成式人工智能融入其中被视为实现零接触光网络的关键解决方案。然而，光网络的生命周期管理涉及多种任务，并需要多层次的无缝协作，这对现有的单代理生成式人工智能系统提出了重大挑战。本文提出了一种生成式人工智能驱动的分层多代理框架，旨在为零接触光网络简化多任务自主执行。文中介绍了该框架的架构、实现和应用。通过部署在一个典型光网络生命周期中的网状网络，展示了三个典型的场景：规划阶段的传输质量估计、运行阶段的动态通道添加/删除以及升级阶段的系统容量增加。案例研究展示了多代理框架在任务分配、协调、执行、评估和总结方面的能力。本文为智能、高效和协作的网络管理解决方案的发展提供了有前景的方法，并为更具专业性和适应性的零接触光网络铺平了道路。', 'title_zh': '基于生成AI的分层多代理框架以实现零接触光网络'}
{'arxiv_id': 'arXiv:2510.05611', 'title': 'MADIAVE: Multi-Agent Debate for Implicit Attribute Value Extraction', 'authors': 'Wei-Chieh Huang, Cornelia Caragea', 'link': 'https://arxiv.org/abs/2510.05611', 'abstract': "Implicit Attribute Value Extraction (AVE) is essential for accurately representing products in e-commerce, as it infers lantent attributes from multimodal data. Despite advances in multimodal large language models (MLLMs), implicit AVE remains challenging due to the complexity of multidimensional data and gaps in vision-text understanding. In this work, we introduce \\textsc{\\modelname}, a multi-agent debate framework that employs multiple MLLM agents to iteratively refine inferences. Through a series of debate rounds, agents verify and update each other's responses, thereby improving inference performance and robustness. Experiments on the ImplicitAVE dataset demonstrate that even a few rounds of debate significantly boost accuracy, especially for attributes with initially low performance. We systematically evaluate various debate configurations, including identical or different MLLM agents, and analyze how debate rounds affect convergence dynamics. Our findings highlight the potential of multi-agent debate strategies to address the limitations of single-agent approaches and offer a scalable solution for implicit AVE in multimodal e-commerce.", 'abstract_zh': '隐含属性值Extract（AVE）对于准确表示电子商务中的产品至关重要，因为它可以从多模态数据中推断潜在属性。尽管在多模态大型语言模型（MLLMs）方面取得了进展，但由于多维数据的复杂性和视觉-文本理解之间的差距，隐含AVE依然具有挑战性。在这项工作中，我们引入了\\textsc{\\modelname}，这是一种多智能体辩论框架，采用多个MLLM智能体进行迭代优化。通过一系列辩论轮次，智能体相互验证和更新彼此的回答，从而提高推理性能和鲁棒性。在ImplicitAVE数据集上的实验表明，即使是几轮辩论也能显著提高准确性，尤其是对于初始性能较低的属性。我们系统地评估了各种辩论配置，包括相同的或不同的MLLM智能体，并分析了辩论轮次如何影响收敛动态。我们的发现强调了多智能体辩论策略的潜在价值，以解决单智能体方法的局限性，并提供了一种适用于多模态电子商务中隐含AVE的可扩展解决方案。', 'title_zh': 'MADIAVE: 多代理辩论法用于隐含属性值提取'}
{'arxiv_id': 'arXiv:2510.05609', 'title': 'HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection', 'authors': 'Junwen Chen, Peilin Xiong, Keiji Yanai', 'link': 'https://arxiv.org/abs/2510.05609', 'abstract': 'Recent Human-object interaction detection (HOID) methods highly require prior knowledge from VLMs to enhance the interaction recognition capabilities. The training strategies and model architectures for connecting the knowledge from VLMs to the HOI instance representations from the object detector are challenging, and the whole framework is complex for further development or application. On the other hand, the inherent reasoning abilities of MLLMs on human-object interaction detection are under-explored. Inspired by the recent success of training MLLMs with reinforcement learning (RL) methods, we propose HOI-R1 and first explore the potential of the language model on the HOID task without any additional detection modules. We introduce an HOI reasoning process and HOID reward functions to solve the HOID task by pure text. The results on the HICO-DET dataset show that HOI-R1 achieves 2x the accuracy of the baseline with great generalization ability. The source code is available at this https URL.', 'abstract_zh': 'Recent Human-object Interaction Detection Methods Highly Require Prior Knowledge from VLMs to Enhance Interaction Recognition Capabilities: Exploiting Inherent Reasoning Abilities of MLLMs without Additional Detection Modules', 'title_zh': 'HOI-R1：探究多模态大规模语言模型在人类对象交互检测中的潜力'}
{'arxiv_id': 'arXiv:2510.05285', 'title': 'Adjusting the Output of Decision Transformer with Action Gradient', 'authors': 'Rui Lin, Yiwen Zhang, Zhicheng Peng, Minghao Lyu', 'link': 'https://arxiv.org/abs/2510.05285', 'abstract': 'Decision Transformer (DT), which integrates reinforcement learning (RL) with the transformer model, introduces a novel approach to offline RL. Unlike classical algorithms that take maximizing cumulative discounted rewards as objective, DT instead maximizes the likelihood of actions. This paradigm shift, however, presents two key challenges: stitching trajectories and extrapolation of action. Existing methods, such as substituting specific tokens with predictive values and integrating the Policy Gradient (PG) method, address these challenges individually but fail to improve performance stably when combined due to inherent instability. To address this, we propose Action Gradient (AG), an innovative methodology that directly adjusts actions to fulfill a function analogous to that of PG, while also facilitating efficient integration with token prediction techniques. AG utilizes the gradient of the Q-value with respect to the action to optimize the action. The empirical results demonstrate that our method can significantly enhance the performance of DT-based algorithms, with some results achieving state-of-the-art levels.', 'abstract_zh': '决策变换器（DT）：一种将强化学习（RL）与变换器模型相结合的新颖的离线RL方法', 'title_zh': '调整决策变换器的输出动作梯度'}
{'arxiv_id': 'arXiv:2510.05157', 'title': 'Adversarial Reinforcement Learning for Offensive and Defensive Agents in a Simulated Zero-Sum Network Environment', 'authors': 'Abrar Shahid, Ibteeker Mahir Ishum, AKM Tahmidul Haque, M Sohel Rahman, A. B. M. Alim Al Islam', 'link': 'https://arxiv.org/abs/2510.05157', 'abstract': 'This paper presents a controlled study of adversarial reinforcement learning in network security through a custom OpenAI Gym environment that models brute-force attacks and reactive defenses on multi-port services. The environment captures realistic security trade-offs including background traffic noise, progressive exploitation mechanics, IP-based evasion tactics, honeypot traps, and multi-level rate-limiting defenses. Competing attacker and defender agents are trained using Deep Q-Networks (DQN) within a zero-sum reward framework, where successful exploits yield large terminal rewards while incremental actions incur small costs. Through systematic evaluation across multiple configurations (varying trap detection probabilities, exploitation difficulty thresholds, and training regimens), the results demonstrate that defender observability and trap effectiveness create substantial barriers to successful attacks. The experiments reveal that reward shaping and careful training scheduling are critical for learning stability in this adversarial setting. The defender consistently maintains strategic advantage across 50,000+ training episodes, with performance gains amplifying when exposed to complex defensive strategies including adaptive IP blocking and port-specific controls. Complete implementation details, reproducible hyperparameter configurations, and architectural guidelines are provided to support future research in adversarial RL for cybersecurity. The zero-sum formulation and realistic operational constraints make this environment suitable for studying autonomous defense systems, attacker-defender co-evolution, and transfer learning to real-world network security scenarios.', 'abstract_zh': '通过对多端口服务进行暴力攻击和反应式防御建模的自定义OpenAI Gym环境，本文呈现了一项受控研究，探讨在网络安全性中对抗强化学习的问题。', 'title_zh': 'adversarial reinforcement learning在模拟零和网络环境中的进攻与防御智能体中的应用'}
{'arxiv_id': 'arXiv:2510.05123', 'title': 'A Scalable AI Driven, IoT Integrated Cognitive Digital Twin for Multi-Modal Neuro-Oncological Prognostics and Tumor Kinetics Prediction using Enhanced Vision Transformer and XAI', 'authors': 'Saptarshi Banerjee, Himadri Nath Saha, Utsho Banerjee, Rajarshi Karmakar, Jon Turdiev', 'link': 'https://arxiv.org/abs/2510.05123', 'abstract': 'Neuro-oncological prognostics are now vital in modern clinical neuroscience because brain tumors pose significant challenges in detection and management. To tackle this issue, we propose a cognitive digital twin framework that combines real-time EEG signals from a wearable skullcap with structural MRI data for dynamic and personalized tumor monitoring. At the heart of this framework is an Enhanced Vision Transformer (ViT++) that includes innovative components like Patch-Level Attention Regularization (PLAR) and an Adaptive Threshold Mechanism to improve tumor localization and understanding. A Bidirectional LSTM-based neural classifier analyzes EEG patterns over time to classify brain states such as seizure, interictal, and healthy. Grad-CAM-based heatmaps and a this http URL-powered 3D visualization module provide interactive anatomical insights. Furthermore, a tumor kinetics engine predicts volumetric growth by looking at changes in MRI trends and anomalies from EEG data. With impressive accuracy metrics of 94.6% precision, 93.2% recall, and a Dice score of 0.91, this framework sets a new standard for real-time, interpretable neurodiagnostics. It paves the way for future advancements in intelligent brain health monitoring.', 'abstract_zh': '神经 Oncological 预后现在是现代临床神经科学中的重要组成部分，因为脑肿瘤在检测和管理方面提出了重大挑战。为解决这一问题，我们提出了一种结合穿戴式头盔实时 EEG 信号与结构 MRI 数据的认知数字孪生框架，以实现动态和个性化的肿瘤监测。该框架的核心是增强型视觉变换器（ViT++），它包括像Patch-Level Attention Regularization (PLAR) 和自适应阈值机制等创新组件，以提高肿瘤定位和理解。基于双向 LSTM 的神经分类器分析随时间变化的 EEG 模式，以分类脑状态（如癫痫、无发作期和健康状态）。基于 Grad-CAM 的热图和由此链接提供的 3D 可视化模块提供了交互式的解剖学洞察。此外，肿瘤动力学引擎通过观察 MRI 趋势的变化和 EEG 数据中的异常来预测肿瘤体积的增长。凭借 94.6% 的精确度、93.2% 的召回率和 0.91 的 Dice 分数，该框架建立了实时、可解释神经诊断的新标准。它为未来的智能大脑健康监测的进步铺平了道路。', 'title_zh': '基于增强视觉变换器和可解释人工智能的可扩展AI驱动物联网集成认知数字双胞胎多模态神经 Oncology 预后及肿瘤动力学预测'}
{'arxiv_id': 'arXiv:2510.05122', 'title': 'CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation', 'authors': 'Jie Zhu, Yuanchen Zhou, Shuo Jiang, Junhui Li, Lifan Guo, Feng Chen, Chi Zhang, Fang Kong', 'link': 'https://arxiv.org/abs/2510.05122', 'abstract': 'Emotional Support Conversation (ESC) plays a vital role in alleviating psychological stress and providing emotional value through dialogue. While recent studies have largely focused on data augmentation and synthetic corpus construction, they often overlook the deeper cognitive reasoning processes that underpin effective emotional support. To address this gap, we propose \\textbf{CARE}, a novel framework that strengthens reasoning in ESC without relying on large-scale synthetic data. CARE leverages the original ESC training set to guide models in generating logically coherent and supportive responses, thereby explicitly enhancing cognitive reasoning. Building on this foundation, we further employ reinforcement learning to refine and reinforce the reasoning process. Experimental results demonstrate that CARE significantly improves both the logical soundness and supportive quality of responses, advancing the development of empathetic, cognitively robust, and human-like emotional support systems.', 'abstract_zh': '情感支持对话(CARE)在缓解心理压力和通过对话提供情感价值中发挥着重要作用。尽管近期研究主要集中在数据增强和合成语料库构建上，它们往往忽视了有效的的情感支持背后的深层次认知推理过程。为填补这一空白，我们提出了一种名为CARE的新型框架，该框架不依赖大规模合成数据来加强情感支持对话中的推理。CARE利用原始的情感支持对话训练集引导模型生成逻辑连贯和支持性较强的回应，从而明确增强认知推理。在此基础上，我们进一步采用强化学习来完善和强化推理过程。实验结果表明，CARE显著提高了回应的逻辑一致性和支持性质量，推动了共情、认知稳健且类人为的情感支持系统的开发。', 'title_zh': 'CARE: 认知推理增强的强化学习情感支持对话'}
