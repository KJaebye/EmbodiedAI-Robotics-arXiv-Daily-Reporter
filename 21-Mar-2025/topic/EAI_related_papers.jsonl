{'arxiv_id': 'arXiv:2503.16408', 'title': 'RoboFactory: Exploring Embodied Agent Collaboration with Compositional Constraints', 'authors': 'Yiran Qin, Li Kang, Xiufeng Song, Zhenfei Yin, Xiaohong Liu, Xihui Liu, Ruimao Zhang, Lei Bai', 'link': 'https://arxiv.org/abs/2503.16408', 'abstract': 'Designing effective embodied multi-agent systems is critical for solving complex real-world tasks across domains. Due to the complexity of multi-agent embodied systems, existing methods fail to automatically generate safe and efficient training data for such systems. To this end, we propose the concept of compositional constraints for embodied multi-agent systems, addressing the challenges arising from collaboration among embodied agents. We design various interfaces tailored to different types of constraints, enabling seamless interaction with the physical world. Leveraging compositional constraints and specifically designed interfaces, we develop an automated data collection framework for embodied multi-agent systems and introduce the first benchmark for embodied multi-agent manipulation, RoboFactory. Based on RoboFactory benchmark, we adapt and evaluate the method of imitation learning and analyzed its performance in different difficulty agent tasks. Furthermore, we explore the architectures and training strategies for multi-agent imitation learning, aiming to build safe and efficient embodied multi-agent systems.', 'abstract_zh': '设计有效的具身多智能体系统对于解决跨领域复杂现实任务至关重要。由于具身多智能体系统的复杂性，现有方法无法自动生成此类系统的安全高效训练数据。为此，我们提出了具身多智能体系统的组成约束概念，以应对具身智能体协作带来的挑战。我们设计了适用于不同类型约束的各种接口，使系统能够无缝与物理世界交互。借助组成约束和专门设计的接口，我们开发了具身多智能体系统的自动化数据采集框架，并引入了首个具身多智能体操作基准RoboFactory。基于RoboFactory基准，我们调整并评估了模仿学习的方法，并分析了其在不同难度智能体任务中的性能。此外，我们探讨了多智能体模仿学习的架构和训练策略，旨在构建安全高效的具身多智能体系统。', 'title_zh': 'RoboFactory: 探索基于组合约束的实体代理协作'}
{'arxiv_id': 'arXiv:2503.16197', 'title': 'Explosive Jumping with Rigid and Articulated Soft Quadrupeds via Example Guided Reinforcement Learning', 'authors': 'Georgios Apostolides, Wei Pan, Jens Kober, Cosimo Della Santina, Jiatao Ding', 'link': 'https://arxiv.org/abs/2503.16197', 'abstract': 'Achieving controlled jumping behaviour for a quadruped robot is a challenging task, especially when introducing passive compliance in mechanical design. This study addresses this challenge via imitation-based deep reinforcement learning with a progressive training process. To start, we learn the jumping skill by mimicking a coarse jumping example generated by model-based trajectory optimization. Subsequently, we generalize the learned policy to broader situations, including various distances in both forward and lateral directions, and then pursue robust jumping in unknown ground unevenness. In addition, without tuning the reward much, we learn the jumping policy for a quadruped with parallel elasticity. Results show that using the proposed method, i) the robot learns versatile jumps by learning only from a single demonstration, ii) the robot with parallel compliance reduces the landing error by 11.1%, saves energy cost by 15.2% and reduces the peak torque by 15.8%, compared to the rigid robot without parallel elasticity, iii) the robot can perform jumps of variable distances with robustness against ground unevenness (maximal 4cm height perturbations) using only proprioceptive perception.', 'abstract_zh': '实现四足机器人的可控跳跃行为是一项具有挑战的任务，尤其是在机械设计中引入被动顺应性时。本研究通过基于模仿的深度强化学习及渐进式训练过程解决了这一挑战。首先，通过模仿基于模型轨迹优化生成的粗略跳跃示例来学习跳跃技能；随后，将学到的策略推广到不同的情况，包括前后和侧向方向的各种距离，进而追求在未知不平地上具有鲁棒性的跳跃。此外，在不调整奖励函数的情况下，学习具有平行弹性的四足机器人的跳跃策略。结果表明，使用所提出的方法，i) 机器人仅从一个示范中学习，即可掌握多种跳跃技巧；ii) 具有平行顺应性的机器人着陆误差减少11.1%，能耗降低15.2%，峰值扭矩减少15.8%，相比于无平行弹性的刚性机器人；iii) 机器人仅依靠本体感受信息即可在最大4cm高度的不平地上表现出鲁棒性的跳跃，适用于不同距离的跳跃。', 'title_zh': '基于示例引导强化学习的刚性与柔体关节四足跳跃爆炸性起跳'}
{'arxiv_id': 'arXiv:2503.16013', 'title': 'GraspCoT: Integrating Physical Property Reasoning for 6-DoF Grasping under Flexible Language Instructions', 'authors': 'Xiaomeng Chu, Jiajun Deng, Guoliang You, Wei Liu, Xingchen Li, Jianmin Ji, Yanyong Zhang', 'link': 'https://arxiv.org/abs/2503.16013', 'abstract': "Flexible instruction-guided 6-DoF grasping is a significant yet challenging task for real-world robotic systems. Existing methods utilize the contextual understanding capabilities of the large language models (LLMs) to establish mappings between expressions and targets, allowing robots to comprehend users' intentions in the instructions. However, the LLM's knowledge about objects' physical properties remains underexplored despite its tight relevance to grasping. In this work, we propose GraspCoT, a 6-DoF grasp detection framework that integrates a Chain-of-Thought (CoT) reasoning mechanism oriented to physical properties, guided by auxiliary question-answering (QA) tasks. Particularly, we design a set of QA templates to enable hierarchical reasoning that includes three stages: target parsing, physical property analysis, and grasp action selection. Moreover, GraspCoT presents a unified multimodal LLM architecture, which encodes multi-view observations of 3D scenes into 3D-aware visual tokens, and then jointly embeds these visual tokens with CoT-derived textual tokens within LLMs to generate grasp pose predictions. Furthermore, we present IntentGrasp, a large-scale benchmark that fills the gap in public datasets for multi-object grasp detection under diverse and indirect verbal commands. Extensive experiments on IntentGrasp demonstrate the superiority of our method, with additional validation in real-world robotic applications confirming its practicality. Codes and data will be released.", 'abstract_zh': '六自由度指令引导抓取是一种显著且具有挑战性的实际机器人系统任务。现有的方法利用大规模语言模型（LLMs）的上下文理解能力建立表达与目标之间的映射，使机器人能够理解用户的意图。然而，LLM对物体物理属性的知识尚未得到充分利用，尽管这与抓取密切相关。在本文中，我们提出了GraspCoT，这是一种结合了面向物理属性的Chain-of-Thought（CoT）推理机制的六自由度抓取检测框架，该机制由辅助问答（QA）任务引导。特别是，我们设计了一组问答模板以实现分层推理，包括三个阶段：目标解析、物理属性分析和抓取动作选择。此外，GraspCoT展示了一个统一的多模态LLM架构，该架构将多视图3D场景观察编码为3D感知的视觉令牌，并在LLM中联合嵌入由CoT导出的文本令牌以生成抓取姿态预测。此外，我们提出了IntentGrasp，这是一个大规模基准，填补了公共数据集中关于多种间接口头指令下的多物体抓取检测的空白。在IntentGrasp上的广泛实验显示了我们方法的优势，并在现实世界的机器人应用中的进一步验证证实了其实用性。代码和数据将被公开。', 'title_zh': 'GraspCoT：在灵活语言指令下的6-自由度抓取物理属性推理集成'}
{'arxiv_id': 'arXiv:2503.15895', 'title': 'CONTHER: Human-Like Contextual Robot Learning via Hindsight Experience Replay and Transformers without Expert Demonstrations', 'authors': 'Maria Makarova, Qian Liu, Dzmitry Tsetserukou', 'link': 'https://arxiv.org/abs/2503.15895', 'abstract': 'This paper presents CONTHER, a novel reinforcement learning algorithm designed to efficiently and rapidly train robotic agents for goal-oriented manipulation tasks and obstacle avoidance. The algorithm uses a modified replay buffer inspired by the Hindsight Experience Replay (HER) approach to artificially populate experience with successful trajectories, effectively addressing the problem of sparse reward scenarios and eliminating the need to manually collect expert demonstrations.\nThe developed algorithm proposes a Transformer-based architecture to incorporate the context of previous states, allowing the agent to perform a deeper analysis and make decisions in a manner more akin to human learning. The effectiveness of the built-in replay buffer, which acts as an "internal demonstrator", is twofold: it accelerates learning and allows the algorithm to adapt to different tasks. Empirical data confirm the superiority of the algorithm by an average of 38.46% over other considered methods, and the most successful baseline by 28.21%, showing higher success rates and faster convergence in the point-reaching task. Since the control is performed through the robot\'s joints, the algorithm facilitates potential adaptation to a real robot system and construction of an obstacle avoidance task. Therefore, the algorithm has also been tested on tasks requiring following a complex dynamic trajectory and obstacle avoidance. The design of the algorithm ensures its applicability to a wide range of goal-oriented tasks, making it an easily integrated solution for real-world robotics applications.', 'abstract_zh': '这篇论文介绍了CONTHER，一种新型强化学习算法，旨在高效快速地训练具有目标导向操作任务和障碍避障能力的机器人代理。该算法使用了受Hindsight Experience Replay (HER) 方法启发的修改过的重播缓冲区，人工填充成功轨迹的经验，有效地解决了稀疏奖励场景的问题，并消除了手动收集专家示范的需求。开发的算法提出了基于Transformer的架构，以整合先前状态的上下文，使代理能够进行更深入的分析并以更接近人类学习的方式做出决策。内置重播缓冲区作为“内部示范者”的有效性体现在两个方面：加速学习并使算法能够适应不同的任务。实验证据表明，与所考虑的其他方法相比，该算法平均提高38.46%，相对于最成功的基线提高28.21%，在点接触任务中显示出更高的成功率和更快的收敛速度。由于控制是通过机器人关节实现的，该算法便于潜在适应真实的机器人系统，并构建障碍避障任务。因此，该算法还被测试了需要跟随复杂动态轨迹和障碍避障的任务。算法的设计使其适用于广泛的具有目标导向的任务，使其成为一个易于集成的真实世界机器人应用解决方案。', 'title_zh': 'CONTHER: 基于 hindsight experience replay 和 transformers 的类人类背景机器人学习方法，无需专家示范'}
{'arxiv_id': 'arXiv:2503.15781', 'title': 'UAS Visual Navigation in Large and Unseen Environments via a Meta Agent', 'authors': 'Yuci Han, Charles Toth, Alper Yilmaz', 'link': 'https://arxiv.org/abs/2503.15781', 'abstract': 'The aim of this work is to develop an approach that enables Unmanned Aerial System (UAS) to efficiently learn to navigate in large-scale urban environments and transfer their acquired expertise to novel environments. To achieve this, we propose a meta-curriculum training scheme. First, meta-training allows the agent to learn a master policy to generalize across tasks. The resulting model is then fine-tuned on the downstream tasks. We organize the training curriculum in a hierarchical manner such that the agent is guided from coarse to fine towards the target task. In addition, we introduce Incremental Self-Adaptive Reinforcement learning (ISAR), an algorithm that combines the ideas of incremental learning and meta-reinforcement learning (MRL). In contrast to traditional reinforcement learning (RL), which focuses on acquiring a policy for a specific task, MRL aims to learn a policy with fast transfer ability to novel tasks. However, the MRL training process is time consuming, whereas our proposed ISAR algorithm achieves faster convergence than the conventional MRL algorithm. We evaluate the proposed methodologies in simulated environments and demonstrate that using this training philosophy in conjunction with the ISAR algorithm significantly improves the convergence speed for navigation in large-scale cities and the adaptation proficiency in novel environments.', 'abstract_zh': '本研究的目的是开发一种方法，使无人驾驶航空系统（UAS）能够高效地学习在大规模城市环境中导航，并将其获得的专业知识转移到新的环境中。为此，我们提出了一种元课程训练方案。首先，通过元训练，使代理学习一个主策略以在任务间泛化。然后，对该模型进行下游任务的微调。我们将训练课程分层组织，引导代理从粗略到精细地向目标任务过渡。此外，我们引入了增量自适应强化学习（ISAR），这是一种结合增量学习和元强化学习（MRL）思想的算法。与传统的强化学习（RL）专注于特定任务的策略学习不同，MRL旨在学习具有快速迁移能力的策略。然而，MRL的训练过程耗时较长，而我们提出的ISAR算法比传统的MRL算法更快收敛。我们在模拟环境中评估了所提出的方法，并证明这种训练理念与ISAR算法相结合，显著提高了在大规模城市中导航的收敛速度和在新环境中的适应能力。', 'title_zh': '基于元代理的UAS在大型未见环境中的视觉导航'}
{'arxiv_id': 'arXiv:2503.15724', 'title': 'Reward Training Wheels: Adaptive Auxiliary Rewards for Robotics Reinforcement Learning', 'authors': 'Linji Wang, Tong Xu, Yuanjie Lu, Xuesu Xiao', 'link': 'https://arxiv.org/abs/2503.15724', 'abstract': "Robotics Reinforcement Learning (RL) often relies on carefully engineered auxiliary rewards to supplement sparse primary learning objectives to compensate for the lack of large-scale, real-world, trial-and-error data. While these auxiliary rewards accelerate learning, they require significant engineering effort, may introduce human biases, and cannot adapt to the robot's evolving capabilities during training. In this paper, we introduce Reward Training Wheels (RTW), a teacher-student framework that automates auxiliary reward adaptation for robotics RL. To be specific, the RTW teacher dynamically adjusts auxiliary reward weights based on the student's evolving capabilities to determine which auxiliary reward aspects require more or less emphasis to improve the primary objective. We demonstrate RTW on two challenging robot tasks: navigation in highly constrained spaces and off-road vehicle mobility on vertically challenging terrain. In simulation, RTW outperforms expert-designed rewards by 2.35% in navigation success rate and improves off-road mobility performance by 122.62%, while achieving 35% and 3X faster training efficiency, respectively. Physical robot experiments further validate RTW's effectiveness, achieving a perfect success rate (5/5 trials vs. 2/5 for expert-designed rewards) and improving vehicle stability with up to 47.4% reduction in orientation angles.", 'abstract_zh': '基于奖励训练轮的机器人强化学习辅助奖励自动化调整框架', 'title_zh': '奖励训练辅助轮：机器人强化学习的自适应辅助奖励'}
{'arxiv_id': 'arXiv:2503.15715', 'title': 'Experience-based Optimal Motion Planning Algorithm for Solving Difficult Planning Problems Using a Limited Dataset', 'authors': 'Ryota Takamido, Jun Ota', 'link': 'https://arxiv.org/abs/2503.15715', 'abstract': 'This study aims to address the key challenge of obtaining a high-quality solution path within a short calculation time by generalizing a limited dataset. In the informed experience-driven random trees connect star (IERTC*) process, the algorithm flexibly explores the search trees by morphing the micro paths generated from a single experience while reducing the path cost by introducing a re-wiring process and an informed sampling process. The core idea of this algorithm is to apply different strategies depending on the complexity of the local environment; for example, it adopts a more complex curved trajectory if obstacles are densely arranged near the search tree, and it adopts a simpler straight line if the local environment is sparse. The results of experiments using a general motion benchmark test revealed that IERTC* significantly improved the planning success rate in difficult problems in the cluttered environment (an average improvement of 49.3% compared to the state-of-the-art algorithm) while also significantly reducing the solution cost (a reduction of 56.3%) when using one hundred experiences. Furthermore, the results demonstrated outstanding planning performance even when only one experience was available (a 43.8% improvement in success rate and a 57.8% reduction in solution cost).', 'abstract_zh': '本研究旨在通过泛化有限的数据集，在较短的计算时间内获得高质量的解路径。在知情经验驱动随机树连接星（IERTC*）过程中，算法通过形态变化生成的微路径并引入重连线过程和知情采样过程灵活探索搜索树，同时降低路径成本。该算法的核心思想是根据局部环境的复杂性采用不同的策略；例如，如果搜索树附近障碍物密集，则采用复杂的曲线轨迹，而如果局部环境稀疏则采用简单的直线。实验使用通用运动基准测试表明，IERTC*在拥挤环境中难问题的规划成功率显著提高（与最先进的算法相比平均提高49.3%），同时使用一百次经验时显著降低了解路径成本（减少56.3%）。此外，结果显示即使只有一次经验可用，其规划性能也表现出色（成功率提高43.8%，解路径成本减少57.8%）。', 'title_zh': '基于经验的最优运动规划算法：利用有限数据集解决复杂规划问题'}
{'arxiv_id': 'arXiv:2503.15707', 'title': 'Safety Aware Task Planning via Large Language Models in Robotics', 'authors': 'Azal Ahmad Khan, Michael Andrev, Muhammad Ali Murtaza, Sergio Aguilera, Rui Zhang, Jie Ding, Seth Hutchinson, Ali Anwar', 'link': 'https://arxiv.org/abs/2503.15707', 'abstract': "The integration of large language models (LLMs) into robotic task planning has unlocked better reasoning capabilities for complex, long-horizon workflows. However, ensuring safety in LLM-driven plans remains a critical challenge, as these models often prioritize task completion over risk mitigation. This paper introduces SAFER (Safety-Aware Framework for Execution in Robotics), a multi-LLM framework designed to embed safety awareness into robotic task planning. SAFER employs a Safety Agent that operates alongside the primary task planner, providing safety feedback. Additionally, we introduce LLM-as-a-Judge, a novel metric leveraging LLMs as evaluators to quantify safety violations within generated task plans. Our framework integrates safety feedback at multiple stages of execution, enabling real-time risk assessment, proactive error correction, and transparent safety evaluation. We also integrate a control framework using Control Barrier Functions (CBFs) to ensure safety guarantees within SAFER's task planning. We evaluated SAFER against state-of-the-art LLM planners on complex long-horizon tasks involving heterogeneous robotic agents, demonstrating its effectiveness in reducing safety violations while maintaining task efficiency. We also verify the task planner and safety planner through actual hardware experiments involving multiple robots and a human.", 'abstract_zh': '大语言模型在机器人任务规划中的整合增强了复杂长时间流程的推理能力，但确保这些模型驱动计划的安全性仍然是一个关键挑战。本文介绍了SAFER（Safety-Aware Framework for Execution in Robotics），一种多大语言模型框架，旨在将安全意识嵌入到机器人任务规划中。SAFER采用了安全代理与主要任务规划器并行工作，提供安全反馈。此外，我们还引入了LLM-as-a-Judge，这是一种创新的度量标准，利用大语言模型作为评估器来量化生成任务计划中的安全违规行为。我们的框架在执行的多个阶段整合了安全反馈，实现了实时风险评估、主动错误修正和透明的安全评估。我们还使用控制屏障函数（CBFs）控制框架来确保SAFER任务规划中的安全保证。我们在涉及异构机器人代理的复杂长时间任务上对SAFER与最先进的大语言模型规划器进行了评估，证明了其在减少安全违规行为的同时保持任务效率的有效性。我们还通过涉及多个机器人和人类的实际硬件实验验证了任务规划器和安全规划器。', 'title_zh': '基于大型语言模型的机器人安全意识任务规划'}
{'arxiv_id': 'arXiv:2503.15629', 'title': 'Neural Lyapunov Function Approximation with Self-Supervised Reinforcement Learning', 'authors': 'Luc McCutcheon, Bahman Gharesifard, Saber Fallah', 'link': 'https://arxiv.org/abs/2503.15629', 'abstract': 'Control Lyapunov functions are traditionally used to design a controller which ensures convergence to a desired state, yet deriving these functions for nonlinear systems remains a complex challenge. This paper presents a novel, sample-efficient method for neural approximation of nonlinear Lyapunov functions, leveraging self-supervised Reinforcement Learning (RL) to enhance training data generation, particularly for inaccurately represented regions of the state space. The proposed approach employs a data-driven World Model to train Lyapunov functions from off-policy trajectories. The method is validated on both standard and goal-conditioned robotic tasks, demonstrating faster convergence and higher approximation accuracy compared to the state-of-the-art neural Lyapunov approximation baseline. The code is available at: this https URL', 'abstract_zh': '基于自我监督强化学习的非线性Lyapunov函数的样本高效神经近似方法', 'title_zh': '自监督强化学习下的神经Lyapunov函数逼近'}
{'arxiv_id': 'arXiv:2503.16394', 'title': 'Do Visual Imaginations Improve Vision-and-Language Navigation Agents?', 'authors': 'Akhil Perincherry, Jacob Krantz, Stefan Lee', 'link': 'https://arxiv.org/abs/2503.16394', 'abstract': 'Vision-and-Language Navigation (VLN) agents are tasked with navigating an unseen environment using natural language instructions. In this work, we study if visual representations of sub-goals implied by the instructions can serve as navigational cues and lead to increased navigation performance. To synthesize these visual representations or imaginations, we leverage a text-to-image diffusion model on landmark references contained in segmented instructions. These imaginations are provided to VLN agents as an added modality to act as landmark cues and an auxiliary loss is added to explicitly encourage relating these with their corresponding referring expressions. Our findings reveal an increase in success rate (SR) of around 1 point and up to 0.5 points in success scaled by inverse path length (SPL) across agents. These results suggest that the proposed approach reinforces visual understanding compared to relying on language instructions alone. Code and data for our work can be found at this https URL.', 'abstract_zh': '基于视觉-语言导航（VLN）代理在使用自然语言指令导航未知环境时，我们研究了指令中隐含的子目标视觉表示能否作为导航线索，从而提高导航性能。为了合成这些视觉表示或想象，我们利用文本到图像扩散模型，该模型针对分段指令中包含的地标参考进行操作。这些想象作为地标线索提供给VLN代理，并添加了一个辅助损失，以明确鼓励将这些想象与其相应的引用表达式相关联。我们的发现表明，代理的成功率（SR）提高了约1个百分点，成功尺度下的成功率（SPL）提高了多达0.5个百分点。这些结果表明，与仅依赖语言指令相比，所提出的方法增强了视觉理解。我们的代码和数据可以在此处找到：this https URL。', 'title_zh': '视觉想象能提高视觉语言导航代理的能力吗？'}
{'arxiv_id': 'arXiv:2503.16340', 'title': 'Nonlinear action prediction models reveal multi-timescale locomotor control', 'authors': 'Wei-Chen Wang, Antoine De Comite, Monica Daley, Alexandra Voloshina, Nidhi Seethapathi', 'link': 'https://arxiv.org/abs/2503.16340', 'abstract': 'Modeling movement in real-world tasks is a fundamental scientific goal. However, it is unclear whether existing models and their assumptions, overwhelmingly tested in laboratory-constrained settings, generalize to the real world. For example, data-driven models of foot placement control -- a crucial action for stable locomotion -- assume linear and single timescale mappings. We develop nonlinear foot placement prediction models, finding that neural network architectures with flexible input history-dependence like GRU and Transformer perform best across multiple contexts (walking and running, treadmill and overground, varying terrains) and input modalities (multiple body states, gaze), outperforming traditional models. These models reveal context- and modality-dependent timescales: there is more reliance on fast-timescale predictions in complex terrain, gaze predictions precede body state predictions, and full-body state predictions precede center-of-mass-relevant predictions. Thus, nonlinear action prediction models provide quantifiable insights into real-world motor control and can be extended to other actions, contexts, and populations.', 'abstract_zh': '真实世界任务中运动建模是ー个基本的科学目标。然而，现有模型及其假设在实验室受限环境中被广泛测试，其在真实世界中的泛化能力尚不明确。例如，足部放置控制的数据驱动模型——对于稳定运动至关重要的一项操作——假定线性和单一时间尺度映射。我们开发了非线性足部放置预测模型，发现具有灵活输入历史依赖性的神经网络结构如GRU和Transformer在多个上下文（步行和跑步，台上跑动和户外跑动，不同地形）和输入模态（多种身体状态，注视）中表现最优，超越了传统模型。这些模型揭示了上下文和模态依赖的时间尺度：在复杂地形中更多依赖快速时间尺度的预测，注视预测先于身体状态预测，全身状态预测先于质心相关预测。因此，非线性动作预测模型为真实世界运动控制提供了可量化的洞见，并可以扩展到其他动作、上下文和人群。', 'title_zh': '非线性动作预测模型揭示多时标运动控制'}
{'arxiv_id': 'arXiv:2503.15558', 'title': 'Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning', 'authors': 'NVIDIA, Alisson Azzolini, Hannah Brandon, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, Francesco Ferroni, Rama Govindaraju, Jinwei Gu, Siddharth Gururani, Imad El Hanafi, Zekun Hao, Jacob Huffman, Jingyi Jin, Brendan Johnson, Rizwan Khan, George Kurian, Elena Lantz, Nayeon Lee, Zhaoshuo Li, Xuan Li, Tsung-Yi Lin, Yen-Chen Lin, Ming-Yu Liu, Andrew Mathau, Yun Ni, Lindsey Pavao, Wei Ping, David W. Romero, Misha Smelyanskiy, Shuran Song, Lyne Tchapmi, Andrew Z. Wang, Boxin Wang, Haoxiang Wang, Fangyin Wei, Jiashu Xu, Yao Xu, Xiaodong Yang, Zhuolin Yang, Xiaohui Zeng, Zhe Zhang', 'link': 'https://arxiv.org/abs/2503.15558', 'abstract': 'Physical AI systems need to perceive, understand, and perform complex actions in the physical world. In this paper, we present the Cosmos-Reason1 models that can understand the physical world and generate appropriate embodied decisions (e.g., next step action) in natural language through long chain-of-thought reasoning processes. We begin by defining key capabilities for Physical AI reasoning, with a focus on physical common sense and embodied reasoning. To represent physical common sense, we use a hierarchical ontology that captures fundamental knowledge about space, time, and physics. For embodied reasoning, we rely on a two-dimensional ontology that generalizes across different physical embodiments. Building on these capabilities, we develop two multimodal large language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data and train our models in four stages: vision pre-training, general supervised fine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL) as the post-training. To evaluate our models, we build comprehensive benchmarks for physical common sense and embodied reasoning according to our ontologies. Evaluation results show that Physical AI SFT and reinforcement learning bring significant improvements. To facilitate the development of Physical AI, we will make our code and pre-trained models available under the NVIDIA Open Model License at this https URL.', 'abstract_zh': '物理AI系统需要在物理世界中感知、理解和执行复杂的动作。本文介绍了能够理解物理世界并通过长链条思考推理过程生成适当的实体化决策（如下一步行动）的Cosmos-Reason1模型。我们首先定义了物理AI推理的关键能力，重点关注物理常识和实体化推理。为了表示物理常识，我们使用了一个层级ontology来捕捉关于空间、时间和物理的基础知识。对于实体化推理，我们依赖一个二维ontology，在不同的物理实体上进行泛化。基于这些能力，我们开发了两个多模态大型语言模型：Cosmos-Reason1-8B和Cosmos-Reason1-56B。我们按照四个阶段整理数据并训练我们的模型：视觉预训练、通用监督微调（SFT）、物理AI微调和物理AI强化学习（RL）作为后训练。为了评估我们的模型，我们根据我们的ontology构建了全面的物理常识和实体化推理基准。评估结果显示，物理AI微调和强化学习带来了显著改进。为了促进物理AI的发展，我们将按照NVIDIA开源模型许可证在此提供的代码和预训练模型开源。', 'title_zh': 'Cosmos-Reason1: 从物理常识到具身推理'}
{'arxiv_id': 'arXiv:2503.15510', 'title': 'Joint Decision-Making in Robot Teleoperation: When are Two Heads Better Than One?', 'authors': 'Duc-An Nguyen, Raunak Bhattacharyya, Clara Colombatto, Steve Fleming, Ingmar Posner, Nick Hawes', 'link': 'https://arxiv.org/abs/2503.15510', 'abstract': "Operators working with robots in safety-critical domains have to make decisions under uncertainty, which remains a challenging problem for a single human operator. An open question is whether two human operators can make better decisions jointly, as compared to a single operator alone. While prior work has shown that two heads are better than one, such studies have been mostly limited to static and passive tasks. We investigate joint decision-making in a dynamic task involving humans teleoperating robots. We conduct a human-subject experiment with $N=100$ participants where each participant performed a navigation task with two mobiles robots in simulation. We find that joint decision-making through confidence sharing improves dyad performance beyond the better-performing individual (p<0.0001). Further, we find that the extent of this benefit is regulated both by the skill level of each individual, as well as how well-calibrated their confidence estimates are. Finally, we present findings on characterising the human-human dyad's confidence calibration based on the individuals constituting the dyad. Our findings demonstrate for the first time that two heads are better than one, even on a spatiotemporal task which includes active operator control of robots.", 'abstract_zh': '基于任务不确定性的机器人操作者在安全关键领域需要做出决策，这一直是单个人类操作者的挑战性问题。一个开放的问题是两位人类操作者是否可以共同做出优于单个操作者的决策。尽管先前的研究表明两个人比一个人好，但这些研究主要集中在静态和被动任务上。我们研究了人类远程操作机器人在动态任务中的联合决策。我们在包含100名参与者的实验中让每位参与者在模拟环境中使用两个移动机器人完成导航任务。我们发现，通过信心共享进行的联合决策超越了表现较好的个体（p<0.0001）。此外，我们发现这种益处的程度受到每个个体技术水平以及其信心估计准确度的影响。最后，我们基于构成双人的个体介绍了人类-人类双人组信心校准的特征分析。我们的发现首次证明，即使是在包括机器人主动操作控制的时空任务中，两个人也比一个人好。', 'title_zh': '协作决策在机器人遥操作中：两人胜过一人吗？'}
{'arxiv_id': 'arXiv:2503.15504', 'title': 'GRETA: Modular Platform to Create Adaptive Socially Interactive Agents', 'authors': 'Michele Grimaldi, Jieyeon Woo, Fabien Boucaud, Lucie Galland, Nezih Younsi, Liu Yang, Mireille Fares, Sean Graux, Philippe Gauthier, Catherine Pelachaud', 'link': 'https://arxiv.org/abs/2503.15504', 'abstract': "The interaction between humans is very complex to describe since it is composed of different elements from different modalities such as speech, gaze, and gestures influenced by social attitudes and emotions. Furthermore, the interaction can be affected by some features which refer to the interlocutor's state. Actual Socially Interactive Agents SIAs aim to adapt themselves to the state of the interaction partner. In this paper, we discuss this adaptation by describing the architecture of the GRETA platform which considers external features while interacting with humans and/or another ECA and process the dialogue incrementally. We illustrate the new architecture of GRETA which deals with the external features, the adaptation, and the incremental approach for the dialogue processing.", 'abstract_zh': '人类交互非常复杂，因为它由不同模态的元素如语言、注视和手势构成，这些元素受社会态度和情绪的影响。此外，交互还可能受到说话者状态的一些特征影响。实际社会交互代理（SIAs）旨在适应交互伙伴的状态。本文通过描述GRETA平台的架构，讨论这种适应性，该架构在与人类和/或另一个社会型代理互动时考虑外部特征，并采用增量处理对话的方法。我们展示了GRETA的新架构，它处理外部特征、适应性和对话的增量处理方法。', 'title_zh': 'GRETA：模块化平台，用于创建适应性强的社会互动代理'}
{'arxiv_id': 'arXiv:2503.15496', 'title': 'Fast Multi-Party Open-Ended Conversation with a Social Robot', 'authors': 'Giulio Antonio Abbo, Maria Jose Pinto-Bernal, Martijn Catrycke, Tony Belpaeme', 'link': 'https://arxiv.org/abs/2503.15496', 'abstract': "This paper presents the implementation and evaluation of a conversational agent designed for multi-party open-ended interactions. Leveraging state-of-the-art technologies such as voice direction of arrival, voice recognition, face tracking, and large language models, the system aims to facilitate natural and intuitive human-robot conversations. Deployed on the Furhat robot, the system was tested with 30 participants engaging in open-ended group conversations and then in two overlapping discussions. Quantitative metrics, such as latencies and recognition accuracy, along with qualitative measures from user questionnaires, were collected to assess performance. The results highlight the system's effectiveness in managing multi-party interactions, though improvements are needed in response relevance and latency. This study contributes valuable insights for advancing human-robot interaction, particularly in enhancing the naturalness and engagement in group conversations.", 'abstract_zh': '本文介绍了为多人群体开放式交互设计的对话代理的实现与评估。利用语音到达方向、语音识别、面部跟踪以及大规模语言模型等最先进技术，系统旨在促进自然直观的人机对话。该系统部署在Furhat机器人上，并通过30名参与者进行开放式群体对话和两次重叠讨论测试。通过收集量化指标（如延迟和识别准确性）以及用户问卷的定性评估，对系统性能进行了评估。结果显示，该系统在管理多人群体交互方面具有有效性，但需要改进响应相关性和延迟问题。本研究为促进人机交互提供了宝贵的见解，特别是在增强群体对话的自然性和参与度方面。', 'title_zh': '快速多方开放式对话的社会机器人'}
{'arxiv_id': 'arXiv:2503.16348', 'title': 'Palatable Conceptions of Disembodied Being: Terra Incognita in the Space of Possible Minds', 'authors': 'Murray Shanahan', 'link': 'https://arxiv.org/abs/2503.16348', 'abstract': 'Is it possible to articulate a conception of consciousness that is compatible with the exotic characteristics of contemporary, disembodied AI systems, and that can stand up to philosophical scrutiny? How would subjective time and selfhood show up for an entity that conformed to such a conception? Trying to answer these questions, even metaphorically, stretches the language of consciousness to breaking point. Ultimately, the attempt yields something like emptiness, in the Buddhist sense, and helps to undermine our dualistic inclinations towards subjectivity and selfhood.', 'abstract_zh': '当代脱域AI系统之奇特特性与意识观念的兼容性：经哲学审视是否可行？这样一种观念下的主观时间与自我的表现如何？尝试着从这种观念出发，即使是在比喻的意义上，也将意识的语言推向极限。最终，这种尝试呈现出一种类似于佛教意义上的空性，有助于瓦解我们对主观性和自我的二元倾向。', 'title_zh': '可接受的分离存在的观念：未知领域在可能心灵的空间中'}
{'arxiv_id': 'arXiv:2503.16326', 'title': 'OmniGeo: Towards a Multimodal Large Language Models for Geospatial Artificial Intelligence', 'authors': 'Long Yuan, Fengran Mo, Kaiyu Huang, Wenjie Wang, Wangyuxuan Zhai, Xiaoyu Zhu, You Li, Jinan Xu, Jian-Yun Nie', 'link': 'https://arxiv.org/abs/2503.16326', 'abstract': 'The rapid advancement of multimodal large language models (LLMs) has opened new frontiers in artificial intelligence, enabling the integration of diverse large-scale data types such as text, images, and spatial information. In this paper, we explore the potential of multimodal LLMs (MLLM) for geospatial artificial intelligence (GeoAI), a field that leverages spatial data to address challenges in domains including Geospatial Semantics, Health Geography, Urban Geography, Urban Perception, and Remote Sensing. We propose a MLLM (OmniGeo) tailored to geospatial applications, capable of processing and analyzing heterogeneous data sources, including satellite imagery, geospatial metadata, and textual descriptions. By combining the strengths of natural language understanding and spatial reasoning, our model enhances the ability of instruction following and the accuracy of GeoAI systems. Results demonstrate that our model outperforms task-specific models and existing LLMs on diverse geospatial tasks, effectively addressing the multimodality nature while achieving competitive results on the zero-shot geospatial tasks. Our code will be released after publication.', 'abstract_zh': '多模态大型语言模型在地理空间人工智能领域的潜力：OmniGeo模型的研究', 'title_zh': 'OmniGeo：面向地理空间人工智能的多模态大型语言模型'}
{'arxiv_id': 'arXiv:2503.15947', 'title': 'Unreal-MAP: Unreal-Engine-Based General Platform for Multi-Agent Reinforcement Learning', 'authors': 'Tianyi Hu, Qingxu Fu, Zhiqiang Pu, Yuan Wang, Tenghai Qiu', 'link': 'https://arxiv.org/abs/2503.15947', 'abstract': 'In this paper, we propose Unreal Multi-Agent Playground (Unreal-MAP), an MARL general platform based on the Unreal-Engine (UE). Unreal-MAP allows users to freely create multi-agent tasks using the vast visual and physical resources available in the UE community, and deploy state-of-the-art (SOTA) MARL algorithms within them. Unreal-MAP is user-friendly in terms of deployment, modification, and visualization, and all its components are open-source. We also develop an experimental framework compatible with algorithms ranging from rule-based to learning-based provided by third-party frameworks. Lastly, we deploy several SOTA algorithms in example tasks developed via Unreal-MAP, and conduct corresponding experimental analyses. We believe Unreal-MAP can play an important role in the MARL field by closely integrating existing algorithms with user-customized tasks, thus advancing the field of MARL.', 'abstract_zh': '基于Unreal-Engine的Unreal多智能体 playground (Unreal-MAP)：一个多智能体强化学习通用平台', 'title_zh': 'Unreal-MAP：基于UnrealEngine的多 agents 强化学习通用平台'}
{'arxiv_id': 'arXiv:2503.15876', 'title': 'DeepPsy-Agent: A Stage-Aware and Deep-Thinking Emotional Support Agent System', 'authors': 'Kai Chen, Zebing Sun', 'link': 'https://arxiv.org/abs/2503.15876', 'abstract': 'This paper introduces DeepPsy-Agent, an innovative psychological support system that combines the three-stage helping theory in psychology with deep learning techniques. The system consists of two core components: (1) a multi-stage response-capable dialogue model (\\textit{deeppsy-chat}), which enhances reasoning capabilities through stage-awareness and deep-thinking analysis to generate high-quality responses; and (2) a real-time stage transition detection model that identifies contextual shifts to guide the dialogue towards more effective intervention stages. Based on 30,000 real psychological hotline conversations, we employ AI-simulated dialogues and expert re-annotation strategies to construct a high-quality multi-turn dialogue dataset. Experimental results demonstrate that DeepPsy-Agent outperforms general-purpose large language models (LLMs) in key metrics such as problem exposure completeness, cognitive restructuring success rate, and action adoption rate. Ablation studies further validate the effectiveness of stage-awareness and deep-thinking modules, showing that stage information contributes 42.3\\% to performance, while the deep-thinking module increases root-cause identification by 58.3\\% and reduces ineffective suggestions by 72.1\\%. This system addresses critical challenges in AI-based psychological support through dynamic dialogue management and deep reasoning, advancing intelligent mental health services.', 'abstract_zh': 'DeepPsy-Agent：一种结合心理支持三阶段理论与深度学习的心理支持系统', 'title_zh': 'DeepPsy-Agent: 一个阶段意识和深度思考的情感支持代理系统'}
{'arxiv_id': 'arXiv:2503.15807', 'title': 'Video-VoT-R1: An efficient video inference model integrating image packing and AoE architecture', 'authors': 'Cheng Li, Jiexiong Liu, Yixuan Chen, Yanqin Jia', 'link': 'https://arxiv.org/abs/2503.15807', 'abstract': 'In the field of video-language pretraining, existing models face numerous challenges in terms of inference efficiency and multimodal data processing. This paper proposes a KunLunBaize-VoT-R1 video inference model based on a long-sequence image encoder, along with its training and application methods. By integrating image packing technology, the Autonomy-of-Experts (AoE) architecture, and combining the video of Thought (VoT), a large language model (LLM) trained with large-scale reinforcement learning, and multiple training techniques, the efficiency and accuracy of the model in video inference tasks are effectively improved. Experiments show that this model performs outstandingly in multiple tests, providing a new solution for video-language understanding.', 'abstract_zh': '在视频语言预训练领域，现有模型在推理效率和多模态数据处理方面面临诸多挑战。本文提出了一种基于长序列图像编码器的KunLunBaize-VoT-R1视频推理模型及其训练与应用方法。通过整合图像封装技术、Autonomy-of-Experts (AoE) 架构，并结合思辨视频（VoT）、大规模强化学习训练的大语言模型（LLM）以及多种训练技术，有效提升了模型在视频推理任务中的效率和准确性。实验结果表明，该模型在多项测试中表现出色，为视频语言理解提供了一种新的解决方案。', 'title_zh': 'Video-VoT-R1：一种结合图像打包和AoE架构的高效视频推理模型'}
{'arxiv_id': 'arXiv:2503.16365', 'title': 'JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse', 'authors': 'Muyao Li, Zihao Wang, Kaichen He, Xiaojian Ma, Yitao Liang', 'link': 'https://arxiv.org/abs/2503.16365', 'abstract': "Recently, action-based decision-making in open-world environments has gained significant attention. Visual Language Action (VLA) models, pretrained on large-scale web datasets, have shown promise in decision-making tasks. However, previous work has primarily focused on action post-training, often neglecting enhancements to the foundational model itself. In response, we introduce a novel approach, Act from Visual Language Post-Training, which refines Visual Language Models (VLMs) through visual and linguistic guidance in a self-supervised manner. This enhancement improves the models' capabilities in world knowledge, visual recognition, and spatial grounding in open-world environments. Following the above post-training paradigms, we obtain the first VLA models in Minecraft that can follow human instructions on over 1k different atomic tasks, including crafting, smelting, cooking, mining, and killing. Our experiments demonstrate that post-training on non-trajectory tasks leads to a significant 40% improvement over the best agent baseline on a diverse set of atomic tasks. Furthermore, we demonstrate that our approach surpasses traditional imitation learning-based policies in Minecraft, achieving state-of-the-art performance. We have open-sourced the code, models, and datasets to foster further research. The project page can be found in this https URL.", 'abstract_zh': '开放世界环境中基于动作的决策制定最近获得了广泛关注。通过大规模网络数据预训练的视觉语言动作（VLA）模型在决策任务中展现出了巨大潜力。然而，先前的工作主要集中在动作的后训练上，往往忽略了对基础模型本身的改进。为应对这一问题，我们提出了一种新颖的方法——后训练下基于视觉语言的动作制定（Act from Visual Language Post-Training），该方法通过视觉和语言指导对视觉语言模型（VLMs）进行自我监督的完善。这种改进提升了模型在开放世界环境中的世界知识、视觉识别和空间定位能力。按照上述后训练范式，我们在Minecraft中首次获得了能够遵循人类指令完成超过1000种不同原子任务的VLA模型，包括制作、冶炼、烹饪、采矿和杀敌。实验结果表明，针对非轨迹任务的后训练相比最佳代理基线在多种原子任务上取得了显著的40%的提升。此外，我们展示了该方法在Minecraft中超越了传统的模仿学习策略，达到了最先进的性能。我们已开源了代码、模型和数据集以促进进一步的研究。项目页面可在以下链接访问：这个https URL。', 'title_zh': 'JARVIS-VLA：训练后大规模视觉语言模型以通过键盘和鼠标玩视觉游戏'}
{'arxiv_id': 'arXiv:2503.16159', 'title': 'Neural Combinatorial Optimization for Real-World Routing', 'authors': 'Jiwoo Son, Zhikai Zhao, Federico Berto, Chuanbo Hua, Changhyun Kwon, Jinkyoo Park', 'link': 'https://arxiv.org/abs/2503.16159', 'abstract': 'Vehicle Routing Problems (VRPs) are a class of NP-hard problems ubiquitous in several real-world logistics scenarios that pose significant challenges for optimization. Neural Combinatorial Optimization (NCO) has emerged as a promising alternative to classical approaches, as it can learn fast heuristics to solve VRPs. However, most research works in NCO for VRPs focus on simplified settings, which do not account for asymmetric distances and travel durations that cannot be derived by simple Euclidean distances and unrealistic data distributions, hindering real-world deployment. This work introduces RRNCO (Real Routing NCO) to bridge the gap of NCO between synthetic and real-world VRPs in the critical aspects of both data and modeling. First, we introduce a new, openly available dataset with real-world data containing a diverse dataset of locations, distances, and duration matrices from 100 cities, considering realistic settings with actual routing distances and durations obtained from Open Source Routing Machine (OSRM). Second, we propose a novel approach that efficiently processes both node and edge features through contextual gating, enabling the construction of more informed node embedding, and we finally incorporate an Adaptation Attention Free Module (AAFM) with neural adaptive bias mechanisms that effectively integrates not only distance matrices but also angular relationships between nodes, allowing our model to capture rich structural information. RRNCO achieves state-of-the-art results in real-world VRPs among NCO methods. We make our dataset and code publicly available at this https URL.', 'abstract_zh': 'Real Routing Neural Combinatorial Optimization', 'title_zh': '神经组合优化在实际路径规划中的应用'}
{'arxiv_id': 'arXiv:2503.16085', 'title': 'Allostatic Control of Persistent States in Spiking Neural Networks for perception and computation', 'authors': 'Aung Htet, Alejandro Rodriguez Jimenez, Sarah Hamburg, Alessandro Di Nuovo', 'link': 'https://arxiv.org/abs/2503.16085', 'abstract': 'We introduce a novel model for updating perceptual beliefs about the environment by extending the concept of Allostasis to the control of internal representations. Allostasis is a fundamental regulatory mechanism observed in animal physiology that orchestrates responses to maintain a dynamic equilibrium in bodily needs and internal states. In this paper, we focus on an application in numerical cognition, where a bump of activity in an attractor network is used as a spatial numerical representation. While existing neural networks can maintain persistent states, to date, there is no unified framework for dynamically controlling spatial changes in neuronal activity in response to environmental changes. To address this, we couple a well known allostatic microcircuit, the Hammel model, with a ring attractor, resulting in a Spiking Neural Network architecture that can modulate the location of the bump as a function of some reference input. This localized activity in turn is used as a perceptual belief in a simulated subitization task a quick enumeration process without counting. We provide a general procedure to fine-tune the model and demonstrate the successful control of the bump location. We also study the response time in the model with respect to changes in parameters and compare it with biological data. Finally, we analyze the dynamics of the network to understand the selectivity and specificity of different neurons to distinct categories present in the input. The results of this paper, particularly the mechanism for moving persistent states, are not limited to numerical cognition but can be applied to a wide range of tasks involving similar representations.', 'abstract_zh': '我们通过将allostasis的概念扩展到内部表征的控制，引入了一种新的模型来更新对环境的感知信念。本文聚焦于数值认知的应用，在此类应用中，吸引子网络中的活动峰被用作空间数值表示。尽管现有的神经网络可以维持持久状态，但迄今为止，仍缺乏一个统一的框架来动态控制神经活动的空间变化以应对环境变化。为解决这一问题，我们将一个著名的allostasis微回路——Hammel模型，与环形吸引子相结合，从而获得一种尖峰神经网络架构，该架构可以通过某些参考输入的功能来调节活动峰的位置。这种局部活动在模拟瞬时数量感任务中作为感知信念使用，这是一种无需计数的快速枚举过程。我们提供了一种通用的微调程序，并证明了对活动峰位置的有效控制。此外，我们研究了模型的响应时间关于参数变化的响应，并将其与生物数据进行了比较。最后，我们分析了网络的动力学，以理解不同神经元对输入中不同类别的选择性和特异性。本文的结果，特别是在移动持久状态机制方面，不仅适用于数值认知，还适用于涉及类似表示的各种任务。', 'title_zh': '持续状态下的 allostatic 控制在脉冲神经网络中的感知与计算'}
{'arxiv_id': 'arXiv:2503.16021', 'title': 'Autonomous AI imitators increase diversity in homogeneous information ecosystems', 'authors': 'Emil Bakkensen Johansen, Oliver Baumann', 'link': 'https://arxiv.org/abs/2503.16021', 'abstract': "Recent breakthroughs in large language models (LLMs) have facilitated autonomous AI agents capable of imitating human-generated content. This technological advancement raises fundamental questions about AI's potential impact on the diversity and democratic value of information ecosystems. Here, we introduce a large-scale simulation framework to examine AI-based imitation in news, a context critically influential for public discourse. By systematically testing two distinct imitation strategies across a range of information environments varying in initial diversity, we demonstrate that AI-generated articles do not uniformly homogenize content. Instead, AI's influence is strongly context-dependent: AI-generated articles can introduce valuable diversity in originally homogeneous news environments, while potentially diminishing diversity in contexts that initially display high heterogeneity. These results illustrate that the baseline diversity of an information space critically shapes AI's impact, challenging assumptions that AI-driven imitation uniformly threatens information diversity. Instead, when information is initially homogeneous, AI-driven imitation can expand perspectives, styles, and topics. This is especially important in news contexts, where information diversity fosters richer public debate by exposing citizens to alternative viewpoints, challenging biases, and preventing narrative monopolies, which is essential for a resilient democracy.", 'abstract_zh': '近期大规模语言模型的突破促进了能够模仿人类生成内容的自主AI代理。这一技术进步引发了关于AI对信息生态系统多样性和民主价值潜在影响的基本问题。本文介绍了一个大规模模拟框架，以探讨基于AI的新闻模仿现象，这在公共辩论中具有关键影响。通过系统地测试两种不同的模仿策略在多样性初始水平变化的信息环境中，我们表明，AI生成的文章并非均匀同质化内容。相反，AI的影响在很大程度上取决于具体情境：在原本同质的新闻环境中，AI生成的文章可以引入有价值的多样性；而在初始多样性高的环境中，AI可能减少多样性。这些结果表明，信息空间的基线多样性对其影响至关重要，挑战了AI驱动模仿会统一对信息多样性的威胁这一假设。相反，在信息最初同质化的环境中，AI驱动的模仿可以扩展视角、风格和话题。这对于新闻领域尤为重要，因为信息多样性促进了更丰富的公共辩论，使公民接触到不同的观点，挑战偏见，防止单一叙事垄断，这对于建设性的民主是非常关键的。', 'title_zh': '自主AI模仿者增加同质信息生态系统的多样性'}
{'arxiv_id': 'arXiv:2503.15764', 'title': 'Towards Agentic AI Networking in 6G: A Generative Foundation Model-as-Agent Approach', 'authors': 'Yong Xiao, Guangming Shi, Ping Zhang', 'link': 'https://arxiv.org/abs/2503.15764', 'abstract': 'The promising potential of AI and network convergence in improving networking performance and enabling new service capabilities has recently attracted significant interest. Existing network AI solutions, while powerful, are mainly built based on the close-loop and passive learning framework, resulting in major limitations in autonomous solution finding and dynamic environmental adaptation. Agentic AI has recently been introduced as a promising solution to address the above limitations and pave the way for true generally intelligent and beneficial AI systems. The key idea is to create a networking ecosystem to support a diverse range of autonomous and embodied AI agents in fulfilling their goals. In this paper, we focus on the novel challenges and requirements of agentic AI networking. We propose AgentNet, a novel framework for supporting interaction, collaborative learning, and knowledge transfer among AI agents. We introduce a general architectural framework of AgentNet and then propose a generative foundation model (GFM)-based implementation in which multiple GFM-as-agents have been created as an interactive knowledge-base to bootstrap the development of embodied AI agents according to different task requirements and environmental features. We consider two application scenarios, digital-twin-based industrial automation and metaverse-based infotainment system, to describe how to apply AgentNet for supporting efficient task-driven collaboration and interaction among AI agents.', 'abstract_zh': 'AI和网络融合的promise及其在提升网络性能和赋能新服务能力方面的潜力 recently attracted significant interest.现有的网络AI解决方案虽强大，但主要基于闭环和被动学习框架，导致自主解决方案发现和动态环境适应能力受限。近期，代理AI作为一种有前途的解决方案被引入，以解决上述限制并为真正的普遍智能和有益的AI系统铺平道路。核心思想是创建一个网络生态系统，支持各种自主和具身的AI代理实现其目标。在本文中，我们关注代理AI网络的新挑战和要求。我们提出AgentNet，一种支持AI代理之间交互、协作学习和知识转移的新型框架。我们介绍了AgentNet的一般架构，并提出了一种基于生成基础模型（GFM）的实现，其中创建了多个GFM代理作为交互知识库，根据不同的任务需求和环境特征，促进具身AI代理的发展。我们考虑了基于数字孪生的工业自动化和基于元宇宙的交互娱乐系统两个应用场景，以描述如何应用AgentNet支持AI代理之间的高效任务驱动协同和交互。', 'title_zh': '面向6G的能动AI网络构建：生成式基础模型作为代理的方法'}
{'arxiv_id': 'arXiv:2503.15546', 'title': 'Enforcing Cybersecurity Constraints for LLM-driven Robot Agents for Online Transactions', 'authors': 'Shraddha Pradipbhai Shah, Aditya Vilas Deshpande', 'link': 'https://arxiv.org/abs/2503.15546', 'abstract': 'The integration of Large Language Models (LLMs) into autonomous robotic agents for conducting online transactions poses significant cybersecurity challenges. This study aims to enforce robust cybersecurity constraints to mitigate the risks associated with data breaches, transaction fraud, and system manipulation. The background focuses on the rise of LLM-driven robotic systems in e-commerce, finance, and service industries, alongside the vulnerabilities they introduce. A novel security architecture combining blockchain technology with multi-factor authentication (MFA) and real-time anomaly detection was implemented to safeguard transactions. Key performance metrics such as transaction integrity, response time, and breach detection accuracy were evaluated, showing improved security and system performance. The results highlight that the proposed architecture reduced fraudulent transactions by 90%, improved breach detection accuracy to 98%, and ensured secure transaction validation within a latency of 0.05 seconds. These findings emphasize the importance of cybersecurity in the deployment of LLM-driven robotic systems and suggest a framework adaptable to various online platforms.', 'abstract_zh': '大型语言模型（LLMs）在自主机器人代理中进行在线交易的集成带来了重大的网络安全挑战。本研究旨在通过强化网络安全约束来缓解数据泄露、交易欺诈和系统操控相关风险。背景部分侧重于LLM驱动的机器人系统在电子商务、金融和服务业中的兴起及由此带来的一系列脆弱性。本研究实施了一种结合区块链技术和多因素认证（MFA）及实时异常检测的新安全架构，以保障交易的安全。通过关键性能指标如交易完整性、响应时间和入侵检测准确性评估，结果显示该架构提高了安全性并优化了系统性能。研究结果表明，所提出的架构将欺诈性交易减少了90%，将入侵检测准确性提高到98%，并确保在0.05秒的延迟内完成安全交易验证。这些发现强调了在部署LLM驱动的机器人系统中网络安全的重要性，并提出了一种适用于各种在线平台的框架。', 'title_zh': '基于LLM驱动的机器人代理在线交易的网络安全约束强化'}
{'arxiv_id': 'arXiv:2503.15524', 'title': 'KHAIT: K-9 Handler Artificial Intelligence Teaming for Collaborative Sensemaking', 'authors': 'Matthew Wilchek, Linhan Wang, Sally Dickinson, Erica Feuerbacher, Kurt Luther, Feras A. Batarseh', 'link': 'https://arxiv.org/abs/2503.15524', 'abstract': "In urban search and rescue (USAR) operations, communication between handlers and specially trained canines is crucial but often complicated by challenging environments and the specific behaviors canines are trained to exhibit when detecting a person. Since a USAR canine often works out of sight of the handler, the handler lacks awareness of the canine's location and situation, known as the 'sensemaking gap.' In this paper, we propose KHAIT, a novel approach to close the sensemaking gap and enhance USAR effectiveness by integrating object detection-based Artificial Intelligence (AI) and Augmented Reality (AR). Equipped with AI-powered cameras, edge computing, and AR headsets, KHAIT enables precise and rapid object detection from a canine's perspective, improving survivor localization. We evaluate this approach in a real-world USAR environment, demonstrating an average survival allocation time decrease of 22%, enhancing the speed and accuracy of operations.", 'abstract_zh': '在城市搜救（USAR）作业中，搜救人员与特训犬之间的通信至关重要，但常被复杂环境和犬只在搜索人员视线之外时表现出的特定行为所困扰。由于搜救犬往往在搜救人员视线之外工作，搜救人员缺乏对犬只位置和情况的了解，这称为“解释差距”。本文提出了一种名为KHAIT的新方法，通过结合基于物体检测的人工智能（AI）和增强现实（AR），以缩小解释差距并增强USAR的有效性。KHAIT配备有基于AI的摄像头、边缘计算和AR头显，能够从犬只视角进行精确快速的物体检测，提高幸存者定位的准确性和速度。我们在实际的城市搜救环境中评估了该方法，结果显示搜救时间平均减少了22%，提升了作业的速度和准确性。', 'title_zh': 'KHAIT: K-9-handler人工智能协同感知解析 teamwork'}
