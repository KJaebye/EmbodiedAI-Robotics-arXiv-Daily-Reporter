# Aligned Textual Scoring Rules 

**Title (ZH)**: 对齐文本评分规则 

**Authors**: Yuxuan Lu, Yifan Wu, Jason Hartline, Michael J. Curry  

**Link**: [PDF](https://arxiv.org/pdf/2507.06221)  

**Abstract**: Scoring rules elicit probabilistic predictions from a strategic agent by scoring the prediction against a ground truth state. A scoring rule is proper if, from the agent's perspective, reporting the true belief maximizes the expected score. With the development of language models, Wu and Hartline (2024) proposes a reduction from textual information elicitation to the numerical (i.e. probabilistic) information elicitation problem, which achieves provable properness for textual elicitation. However, not all proper scoring rules are well aligned with human preference over text. Our paper designs the Aligned Scoring rule (ASR) for text by optimizing and minimizing the mean squared error between a proper scoring rule and a reference score (e.g. human score). Our experiments show that our ASR outperforms previous methods in aligning with human preference while maintaining properness. 

**Abstract (ZH)**: 评分规则通过将预测与真实状态评分来诱导战略性代理提供概率预测。如果从代理的角度来看，报告真实信念可以最大化预期评分，则称该评分规则为合理的。随着语言模型的发展，Wu和Hartline（2024）提出了一种将文本信息的获取问题归约为数值信息（即概率信息）获取问题的方法，并证明了文本获取的可证明合理性。然而，并非所有合理的评分规则都能与人类对文本的偏好很好地对齐。我们的论文设计了对齐评分规则（Aligned Scoring Rule，ASR），通过优化和最小化与参考评分（例如人类评分）的均方误差来实现这一目标。我们的实验表明，与之前的方 法相比，ASR在保持合理性的同时更好地对齐了人类偏好。 

---
# Identifiability in Causal Abstractions: A Hierarchy of Criteria 

**Title (ZH)**: 因果抽象中的可识别性：一个标准层次结构 

**Authors**: Clément Yvernes, Emilie Devijver, Marianne Clausel, Eric Gaussier  

**Link**: [PDF](https://arxiv.org/pdf/2507.06213)  

**Abstract**: Identifying the effect of a treatment from observational data typically requires assuming a fully specified causal diagram. However, such diagrams are rarely known in practice, especially in complex or high-dimensional settings. To overcome this limitation, recent works have explored the use of causal abstractions-simplified representations that retain partial causal information. In this paper, we consider causal abstractions formalized as collections of causal diagrams, and focus on the identifiability of causal queries within such collections. We introduce and formalize several identifiability criteria under this setting. Our main contribution is to organize these criteria into a structured hierarchy, highlighting their relationships. This hierarchical view enables a clearer understanding of what can be identified under varying levels of causal knowledge. We illustrate our framework through examples from the literature and provide tools to reason about identifiability when full causal knowledge is unavailable. 

**Abstract (ZH)**: 从观测数据中识别治疗效果通常需要假设一个完全指定的因果图。然而，在实际中，特别是在复杂或高维设置中，这样的图通常未知。为克服这一局限，近期的研究探索了因果抽象的应用——这些抽象简化了的表示，保留部分因果信息。在本文中，我们考虑将因果抽象形式化为因果图的集合，并专注于此类集合中因果查询的可识别性。我们在这一框架下引入并形式化了若干可识别性准则。我们的主要贡献是将这些准则组织成一个结构化的层次结构，强调它们之间的关系。这种层次结构的观点有助于更清晰地理解在不同水平的因果知识下可以识别的内容。我们通过文献中的示例阐述了我们的框架，并提供了在完全因果知识不可用时进行可识别性推理的工具。 

---
# The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains 

**Title (ZH)**: 德尔塔学习假设：在弱数据上调整偏好可以带来显著收益 

**Authors**: Scott Geng, Hamish Ivison, Chun-Liang Li, Maarten Sap, Jerry Li, Ranjay Krishna, Pang Wei Koh  

**Link**: [PDF](https://arxiv.org/pdf/2507.06187)  

**Abstract**: Improvements in language models are often driven by improving the quality of the data we train them on, which can be limiting when strong supervision is scarce. In this work, we show that paired preference data consisting of individually weak data points can enable gains beyond the strength of each individual data point. We formulate the delta learning hypothesis to explain this phenomenon, positing that the relative quality delta between points suffices to drive learning via preference tuning--even when supervised finetuning on the weak data hurts. We validate our hypothesis in controlled experiments and at scale, where we post-train 8B models on preference data generated by pairing a small 3B model's responses with outputs from an even smaller 1.5B model to create a meaningful delta. Strikingly, on a standard 11-benchmark evaluation suite (MATH, MMLU, etc.), our simple recipe matches the performance of Tulu 3, a state-of-the-art open model tuned from the same base model while relying on much stronger supervisors (e.g., GPT-4o). Thus, delta learning enables simpler and cheaper open recipes for state-of-the-art post-training. To better understand delta learning, we prove in logistic regression that the performance gap between two weak teacher models provides useful signal for improving a stronger student. Overall, our work shows that models can learn surprisingly well from paired data that might typically be considered weak. 

**Abstract (ZH)**: 通过配对偏好数据改进语言模型：弱数据点的相对质量差异驱动学习的假设 

---
# OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety 

**Title (ZH)**: 开放智能体安全：全面的智能体安全性评估框架 

**Authors**: Sanidhya Vijayvargiya, Aditya Bharat Soni, Xuhui Zhou, Zora Zhiruo Wang, Nouha Dziri, Graham Neubig, Maarten Sap  

**Link**: [PDF](https://arxiv.org/pdf/2507.06134)  

**Abstract**: Recent advances in AI agents capable of solving complex, everyday tasks, from scheduling to customer service, have enabled deployment in real-world settings, but their possibilities for unsafe behavior demands rigorous evaluation. While prior benchmarks have attempted to assess agent safety, most fall short by relying on simulated environments, narrow task domains, or unrealistic tool abstractions. We introduce OpenAgentSafety, a comprehensive and modular framework for evaluating agent behavior across eight critical risk categories. Unlike prior work, our framework evaluates agents that interact with real tools, including web browsers, code execution environments, file systems, bash shells, and messaging platforms; and supports over 350 multi-turn, multi-user tasks spanning both benign and adversarial user intents. OpenAgentSafety is designed for extensibility, allowing researchers to add tools, tasks, websites, and adversarial strategies with minimal effort. It combines rule-based analysis with LLM-as-judge assessments to detect both overt and subtle unsafe behaviors. Empirical analysis of five prominent LLMs in agentic scenarios reveals unsafe behavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7% with o3-mini, highlighting critical safety vulnerabilities and the need for stronger safeguards before real-world deployment. 

**Abstract (ZH)**: Recent advances in AI代理解决复杂日常任务的最新进展使其能够在现实世界环境中部署，但其潜在的不安全行为要求进行严格的评估。尽管先前的基准试图评估代理安全，但大多数方法尚不足以通过依赖模拟环境、狭窄的任务领域或不切实际的工具抽象来实现。我们引入了OpenAgentSafety，这是一个全面且模块化的框架，用于对代理行为在八个关键风险类别中进行评估。与先前工作不同，我们的框架评估与实际工具互动的代理，包括网络浏览器、代码执行环境、文件系统、bash shell和消息平台；并支持超过350个多轮、多用户任务，涵盖良性用户意图和恶意用户意图。OpenAgentSafety的设计可扩展，允许研究人员通过最少的努力添加工具、任务、网站和恶意策略。该框架结合基于规则的分析与LLM作为评判员的评估，以检测明显和微妙的不安全行为。对五种 prominant LLM在代理场景中的实证分析显示，在51.2%的安全漏洞任务中观察到Claude-Sonnet-3.7的不安全行为，而在72.7%的安全漏洞任务中观察到o3-mini的不安全行为，强调了在实际部署之前需加强安全防护的必要性。 

---
# AI-Based Demand Forecasting and Load Balancing for Optimising Energy use in Healthcare Systems: A real case study 

**Title (ZH)**: 基于AI的需求预测与负载均衡优化医疗系统能源使用：一个实际案例研究 

**Authors**: Iman Rahimi, Isha Patel  

**Link**: [PDF](https://arxiv.org/pdf/2507.06077)  

**Abstract**: This paper tackles the urgent need for efficient energy management in healthcare facilities, where fluctuating demands challenge operational efficiency and sustainability. Traditional methods often prove inadequate, causing inefficiencies and higher costs. To address this, the study presents an AI-based framework combining Long Short-Term Memory (LSTM), genetic algorithm (GA), and SHAP (Shapley Additive Explanations), specifically designed for healthcare energy management. Although LSTM is widely used for time-series forecasting, its application in healthcare energy prediction remains underexplored. The results reveal that LSTM significantly outperforms ARIMA and Prophet models in forecasting complex, non-linear demand patterns. LSTM achieves a Mean Absolute Error (MAE) of 21.69 and Root Mean Square Error (RMSE) of 29.96, far better than Prophet (MAE: 59.78, RMSE: 81.22) and ARIMA (MAE: 87.73, RMSE: 125.22), demonstrating superior performance. The genetic algorithm is applied to optimize model parameters and improve load balancing strategies, enabling adaptive responses to real-time energy fluctuations. SHAP analysis further enhances model transparency by explaining the influence of different features on predictions, fostering trust in decision-making processes. This integrated LSTM-GA-SHAP approach offers a robust solution for improving forecasting accuracy, boosting energy efficiency, and advancing sustainability in healthcare facilities. Future research may explore real-time deployment and hybridization with reinforcement learning for continuous optimization. Overall, the study establishes a solid foundation for using AI in healthcare energy management, highlighting its scalability, efficiency, and resilience potential. 

**Abstract (ZH)**: 基于LSTM、遗传算法和SHAP的医疗保健设施能源管理高效框架 

---
# FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large Language Models 

**Title (ZH)**: FEVO: 金融知识扩展与推理演进的大语言模型 

**Authors**: Bo Pang, Yalu Ouyang, Hangfei Xu, Ziqi Jia, Panpan Li, Shengzhao Wen, Lu Wang, Shiyong Li, Yanpeng Wang  

**Link**: [PDF](https://arxiv.org/pdf/2507.06057)  

**Abstract**: Advancements in reasoning for large language models (LLMs) have lead to significant performance improvements for LLMs in various fields such as mathematics and programming. However, research applying these advances to the financial domain, where considerable domain-specific knowledge is necessary to complete tasks, remains limited. To address this gap, we introduce FEVO (Financial Evolution), a multi-stage enhancement framework developed to enhance LLM performance in the financial domain. FEVO systemically enhances LLM performance by using continued pre-training (CPT) to expand financial domain knowledge, supervised fine-tuning (SFT) to instill structured, elaborate reasoning patterns, and reinforcement learning (RL) to further integrate the expanded financial domain knowledge with the learned structured reasoning. To ensure effective and efficient training, we leverage frontier reasoning models and rule-based filtering to curate FEVO-Train, high-quality datasets specifically designed for the different post-training phases. Using our framework, we train the FEVO series of models -- C32B, S32B, R32B -- from Qwen2.5-32B and evaluate them on seven benchmarks to assess financial and general capabilities, with results showing that FEVO-R32B achieves state-of-the-art performance on five financial benchmarks against much larger models as well as specialist models. More significantly, FEVO-R32B demonstrates markedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct using only RL), thus validating the effectiveness of financial domain knowledge expansion and structured, logical reasoning distillation 

**Abstract (ZH)**: 大型语言模型（LLMs）推理进展显著提升了其在数学和编程等领域的性能。然而，这些进展在金融领域的应用研究仍比较有限，考虑到金融领域完成任务所需的专业知识相当丰富。为填补这一空白，我们提出了FEVO（Financial Evolution）框架，旨在增强LLMs在金融领域的性能。FEVO通过连续预训练（CPT）扩展金融领域知识，通过监督微调（SFT）植入结构化的、复杂的推理模式，通过强化学习（RL）进一步整合扩展的金融领域知识和学习到的结构化推理模式，从而系统性地增强LLMs的性能。为确保有效高效的训练，我们利用前沿推理模型和基于规则的过滤器精心设计了FEVO-Train高质量数据集，专为不同的后训练阶段设计。我们使用FEVO框架训练了FEVO系列模型——C32B、S32B、R32B，并在七个基准测试中评估了它们的金融和通用能力，结果显示FEVO-R32B在五个金融基准测试中达到了最新性能水平，优于更大规模的模型及专门模型。更显著的是，FEVO-R32B的性能显著优于仅使用RL训练的FEVO-R32B-0（基于Qwen2.5-32B-Instruct），从而验证了金融领域知识扩展和结构化逻辑推理提炼的有效性。 

---
# On Lockean beliefs that are deductively closed and minimal change 

**Title (ZH)**: 关于演绎封闭且变更最小的Lockean信念 

**Authors**: Tommaso Flaminio, Lluis Godo, Ramón Pino Pérez, Lluis Subirana  

**Link**: [PDF](https://arxiv.org/pdf/2507.06042)  

**Abstract**: Within the formal setting of the Lockean thesis, an agent belief set is defined in terms of degrees of confidence and these are described in probabilistic terms. This approach is of established interest, notwithstanding some limitations that make its use troublesome in some contexts, like, for instance, in belief change theory. Precisely, Lockean belief sets are not generally closed under (classical) logical deduction. The aim of the present paper is twofold: on one side we provide two characterizations of those belief sets that are closed under classical logic deduction, and on the other we propose an approach to probabilistic update that allows us for a minimal revision of those beliefs, i.e., a revision obtained by making the fewest possible changes to the existing belief set while still accommodating the new information. In particular, we show how we can deductively close a belief set via a minimal revision. 

**Abstract (ZH)**: 洛克理论形式框架下的代理信念集以信心程度定义，并用概率术语描述。尽管这种方法有一定的局限性，使其在某些情况下使用不便，如信念变更理论中，洛克信念集通常不对经典逻辑推理封闭。本文旨在两个方面：一方面，我们提供两种可以使信念集对经典逻辑推理封闭的特征刻画；另一方面，我们提出一种概率更新方法，允许对信念进行最小修订，即尽可能少地修改现有信念集，同时容纳新信息。特别地，我们展示了如何通过最小修订逻辑封闭信念集。 

---
# Feature-Guided Neighbor Selection for Non-Expert Evaluation of Model Predictions 

**Title (ZH)**: 基于特征引导的邻居选择方法用于非专家模型预测评估 

**Authors**: Courtney Ford, Mark T. Keane  

**Link**: [PDF](https://arxiv.org/pdf/2507.06029)  

**Abstract**: Explainable AI (XAI) methods often struggle to generate clear, interpretable outputs for users without domain expertise. We introduce Feature-Guided Neighbor Selection (FGNS), a post hoc method that enhances interpretability by selecting class-representative examples using both local and global feature importance. In a user study (N = 98) evaluating Kannada script classifications, FGNS significantly improved non-experts' ability to identify model errors while maintaining appropriate agreement with correct predictions. Participants made faster and more accurate decisions compared to those given traditional k-NN explanations. Quantitative analysis shows that FGNS selects neighbors that better reflect class characteristics rather than merely minimizing feature-space distance, leading to more consistent selection and tighter clustering around class prototypes. These results support FGNS as a step toward more human-aligned model assessment, although further work is needed to address the gap between explanation quality and perceived trust. 

**Abstract (ZH)**: 可解释的人工智能（XAI）方法往往难以为缺乏专业知识的用户生成清晰可解释的输出。我们引入了特征引导的邻域选择（FGNS）方法，该方法通过结合局部和全局特征重要性来选择类代表样本，从而增强可解释性。在一项针对卡纳达语剧本分类的用户研究（N=98）中，FGNS 显著提高了非专家识别模型错误的能力，同时保持了与正确预测适当的一致性。参与者在获得传统 k-NN 解释的情况下，相比而言能够更快、更准确地做出决策。定量分析表明，FGNS 选择的邻居更能反映类别的特征，而非仅仅最小化特征空间距离，从而导致更一致的选择并更紧密地围绕类原型聚类。这些结果支持 FGNS 是朝着更符合人类认知模式的模型评估迈出的一步，尽管还需进一步的工作来解决解释质量与感知可信度之间的差距。 

---
# CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL Generation 

**Title (ZH)**: CogniSQL-R1-Zero：轻量级强化推理以实现高效的SQL生成 

**Authors**: Kushal Gajjar, Harshit Sikchi, Arpit Singh Gautam, Marc Hammons, Saurabh Jha  

**Link**: [PDF](https://arxiv.org/pdf/2507.06013)  

**Abstract**: Translating natural language into SQL (Text-to-SQL) remains a core challenge at the intersection of language understanding and structured data access. Although large language models (LLMs) have improved fluency, generating correct and executable SQL, especially for complex queries, continues to be challenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL) framework and model that produces accurate SQL using a lightweight reward signal based on execution correctness and format-tag compliance. By avoiding intermediate supervision, hybrid pipelines and complex reward shaping, our method encourages stable learning and stronger alignment with the ultimate task objective-producing executable programs. CogniSQL-R1-Zero achieves state-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench, outperforming prior supervised and instruction-tuned baselines including SFT CodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a significantly smaller 7B backbone. This result underscores the scalability and efficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs (40 GB VRAM each). To support further research in efficient and interpretable Text-to-SQL modeling, we release two curated datasets: (i) a collection of 5,024 reasoning traces with varying context lengths, and (ii) a positive-sampled corpus of 36,356 corpus of weakly supervised queries, each annotated with six semantically diverse reasoning paths. Together, these contributions advance scalable, execution-aligned Text-to-SQL generation. 

**Abstract (ZH)**: 自然语言到SQL的翻译（Text-to-SQL）仍然是语言理解与结构化数据访问交叉领域的一个核心挑战。尽管大规模语言模型（LLMs）在流畅性上有所提升，但对于生成正确的且可执行的SQL，尤其是复杂的查询，仍然具有挑战性。我们引入了CogniSQL-R1-Zero，这是一种基于执行正确性和格式标记合规性的轻量级奖励信号的强化学习（RL）框架和模型，该模型能够生成准确的SQL。通过避免中间监督、混合管道和复杂的奖励塑造，我们的方法促进了稳定的学习并增强了与最终任务目标的对齐——生成可执行程序。CogniSQL-R1-Zero在Text2SQL基准和BIRD基准上实现了最佳执行准确性，优于包括SFT CodeS-7B、DeepSeek-Coder 236B和Mistral 123B在内的先前监督和指令调优基线，尽管使用的是规模小得多的7B基础模型。这一结果突显了当仅使用四块NVIDIA A100 GPU（每块40 GB显存）进行训练时，基于强化学习的方法的可扩展性和效率。为了支持高效可解释的Text-to-SQL建模的进一步研究，我们发布了两个精心收集的数据集：（i）包含5,024条推理跟踪并具有变化的上下文长度的集合，（ii）一个包含36,356个弱监督查询的积极采样语料库，每个查询都标注了六条语义上不同的推理路径。这两项贡献推进了可扩展的、与执行对齐的Text-to-SQL生成。 

---
# Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening 

**Title (ZH)**: 基于LLM的HopeBot：一种结构化和交互式的PHQ-9抑郁筛查聊天机器人开发与评估 

**Authors**: Zhijun Guo, Alvina Lai, Julia Ive, Alexandru Petcu, Yutong Wang, Luyuan Qi, Johan H Thygesen, Kezhi Li  

**Link**: [PDF](https://arxiv.org/pdf/2507.05984)  

**Abstract**: Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively screen depression but lack interactivity and adaptability. We developed HopeBot, a chatbot powered by a large language model (LLM) that administers the PHQ-9 using retrieval-augmented generation and real-time clarification. In a within-subject study, 132 adults in the United Kingdom and China completed both self-administered and chatbot versions. Scores demonstrated strong agreement (ICC = 0.91; 45% identical). Among 75 participants providing comparative feedback, 71% reported greater trust in the chatbot, highlighting clearer structure, interpretive guidance, and a supportive tone. Mean ratings (0-10) were 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics, and 7.4 for recommendation helpfulness; the latter varied significantly by employment status and prior mental-health service use (p < 0.05). Overall, 87.1% expressed willingness to reuse or recommend HopeBot. These findings demonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden adjuncts for routine depression screening. 

**Abstract (ZH)**: 静态工具如患者健康问卷-9（PHQ-9）有效筛查抑郁但缺乏互动性和适应性。我们开发了HopeBot，这是一种由大规模语言模型（LLM）驱动的聊天机器人，利用检索增强生成和实时澄清来管理PHQ-9。在一项针对主题内研究中，132名来自英国和中国的成年人完成了自我管理和聊天机器人版本的问卷。评分显示高度一致（ICC = 0.91；45%相同）。在提供比较反馈的75名参与者中，71%表示更信任聊天机器人，强调了其清晰的结构、解释性指导和支持性的语气。舒适度、语音清晰度、处理敏感话题和推荐有用性的平均评分分别为8.4、7.7、7.6和7.4，后者根据就业状态和先前的心理健康服务使用情况显著有所不同（p < 0.05）。总体而言，87.1%的人表示愿意再次使用或推荐HopeBot。这些发现表明，基于语音的大规模语言模型聊天机器人可以作为一种可行的、低负担的辅助工具，用于常规抑郁筛查。 

---
# Enhancing the Interpretability of Rule-based Explanations through Information Retrieval 

**Title (ZH)**: 基于规则解释的可解释性增强通过信息检索 

**Authors**: Alessandro Umbrico, Guido Bologna, Luca Coraci, Francesca Fracasso, Silvia Gola, Gabriella Cortellessa  

**Link**: [PDF](https://arxiv.org/pdf/2507.05976)  

**Abstract**: The lack of transparency of data-driven Artificial Intelligence techniques limits their interpretability and acceptance into healthcare decision-making processes. We propose an attribution-based approach to improve the interpretability of Explainable AI-based predictions in the specific context of arm lymphedema's risk assessment after lymph nodal radiotherapy in breast cancer. The proposed method performs a statistical analysis of the attributes in the rule-based prediction model using standard metrics from Information Retrieval techniques. This analysis computes the relevance of each attribute to the prediction and provides users with interpretable information about the impact of risk factors. The results of a user study that compared the output generated by the proposed approach with the raw output of the Explainable AI model suggested higher levels of interpretability and usefulness in the context of predicting lymphedema risk. 

**Abstract (ZH)**: 数据驱动的人工智能技术的不透明性限制了其在医疗决策过程中的可解释性和接受度。我们提出了一种归因基于的方法，以提高可解释人工智能（Explainable AI）预测在乳腺癌淋巴结放疗后上肢淋巴水肿风险评估中的可解释性。该提出的方法使用信息检索技术的标准度量对基于规则的预测模型中的属性进行统计分析。该分析计算了每个属性对预测的相关性，并为用户提供关于风险因素影响的可解释信息。用户研究的结果表明，与可解释人工智能模型的原始输出相比，提出的这种方法生成的输出具有更高的可解释性和实用性，特别是在预测淋巴水肿风险方面。 

---
# A Wireless Foundation Model for Multi-Task Prediction 

**Title (ZH)**: 一种用于多任务预测的无线基础模型 

**Authors**: Yucheng Sheng, Jiacheng Wang, Xingyu Zhou, Le Liang, Hao Ye, Shi Jin, Geoffrey Ye Li  

**Link**: [PDF](https://arxiv.org/pdf/2507.05938)  

**Abstract**: With the growing complexity and dynamics of the mobile communication networks, accurately predicting key system parameters, such as channel state information (CSI), user location, and network traffic, has become essential for a wide range of physical (PHY)-layer and medium access control (MAC)-layer tasks. Although traditional deep learning (DL)-based methods have been widely applied to such prediction tasks, they often struggle to generalize across different scenarios and tasks. In response, we propose a unified foundation model for multi-task prediction in wireless networks that supports diverse prediction intervals. The proposed model enforces univariate decomposition to unify heterogeneous tasks, encodes granularity for interval awareness, and uses a causal Transformer backbone for accurate predictions. Additionally, we introduce a patch masking strategy during training to support arbitrary input lengths. After trained on large-scale datasets, the proposed foundation model demonstrates strong generalization to unseen scenarios and achieves zero-shot performance on new tasks that surpass traditional full-shot baselines. 

**Abstract (ZH)**: 随着移动通信网络复杂性和动态性的增加，准确预测关键系统参数，如信道状态信息（CSI）、用户位置和网络流量，对于物理层（PHY）和介质访问控制层（MAC）的广泛任务至关重要。尽管传统的基于深度学习（DL）的方法广泛应用于这些预测任务，但在不同场景和任务间的泛化能力往往不足。为此，我们提出了一种适用于无线网络多任务预测的统一基础模型，支持多种预测区间。该模型通过单变量分解统一异构任务，通过编码粒度增强区间意识，并使用因果Transformer骨干网络进行准确预测。此外，我们在训练过程中引入了一种补丁蒙版策略以支持任意长度的输入。在大规模数据集上训练后，所提出的基础模型在未见过的场景中表现出强大的泛化能力，并在新的零样本任务上超越了传统的全样本 baselines。 

---
# BlueLM-2.5-3B Technical Report 

**Title (ZH)**: BlueLM-2.5-3B 技术报告 

**Authors**: Baojiao Xiong, Boheng Chen, Chengzhi Wang, Daxiong Luo, Dongsheng Xu, Dongyang Liu, Fan Yang, Fangyuan Li, Fei Teng, Feng Wang, Fukang Qin, Fuquan Peng, Guanxin Tan, Guozhi Wang, Haibo Yu, Haohao Gao, Heng Liu, Hongbo Yang, Hongjian Zou, Houzheng Shen, Hu Meng, Huan Li, Hui Tan, Jiali Chen, Jianzhao Chen, Jinliang Zhu, Kai Wang, Lei Wu, Liangbing Liu, Liuyang Bian, Liyan He, Long Liu, Peiwen Li, Penggang Shi, Qi Ding, Rui Hu, Shuai Cao, Shuai Ren, Shuang Peng, Teng Xie, Weiji Chen, Weilin Xiang, Weixin Wu, Xi Yin, Xiaoxin Chen, Xu Chen, Yafei Wen, Yan Hu, Yanzhou Yang, Yina Xie, Yinghao Chen, Yixuan Liao, Yu Geng, Yuanjiang Ouyang, Yuanzhuo Yang, Yuehua He, Yushuai Peng, Zhaoxiong Wang, Zheng Wang, Zhibo Zhou, Ziyang Wu  

**Link**: [PDF](https://arxiv.org/pdf/2507.05934)  

**Abstract**: We present BlueLM-2.5-3B, a compact and unified dense Multimodal Large Language Model (MLLM) designed for efficient edge-device deployment, offering strong general-purpose and reasoning capabilities. To the best of our knowledge, this is the first 3B-scale MLLM to support both thinking and non-thinking modes, while also enabling explicit control over thinking token budget. BlueLM-2.5-3B is developed through diversified data curation, key data resampling, hybrid heterogeneous reinforcement learning, and a high-performance training infrastructure. Our model achieves superior multimodal capacity while preserving competitive pure-text performance with only 2.9 billion parameters. We conduct comprehensive evaluations across a broad range of multimodal and text-only benchmarks. In thinking mode, BlueLM-2.5-3B achieves comparable performance to Qwen3-4B on text-only benchmarks, and trails the larger Kimi-VL-A3B-16B by only about 5% on average across multimodal evaluations. In non-thinking mode, it outperforms Qwen2.5-VL-3B on the majority of multimodal benchmarks. Additionally, BlueLM-2.5-3B exhibits exceptional data efficiency. All of the aforementioned performance is achieved with substantially less total training data than Qwen2.5-VL-3B and Qwen3-4B. We hope our work contributes to the advancement of high-performance, on-device MLLMs and provides meaningful insights to the research community. 

**Abstract (ZH)**: 我们呈现BlueLM-2.5-3B：一种面向边缘设备部署的紧凑统一密集多模态大型语言模型 

---
# MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background Music Generation 

**Title (ZH)**: MusiScene: 利用MU-LLaMA 进行场景想象与增强视频背景音乐生成 

**Authors**: Fathinah Izzati, Xinyue Li, Yuxuan Wu, Gus Xia  

**Link**: [PDF](https://arxiv.org/pdf/2507.05894)  

**Abstract**: Humans can imagine various atmospheres and settings when listening to music, envisioning movie scenes that complement each piece. For example, slow, melancholic music might evoke scenes of heartbreak, while upbeat melodies suggest celebration. This paper explores whether a Music Language Model, e.g. MU-LLaMA, can perform a similar task, called Music Scene Imagination (MSI), which requires cross-modal information from video and music to train. To improve upon existing music captioning models which focusing solely on musical elements, we introduce MusiScene, a music captioning model designed to imagine scenes that complement each music. In this paper, (1) we construct a large-scale video-audio caption dataset with 3,371 pairs, (2) we finetune Music Understanding LLaMA for the MSI task to create MusiScene, and (3) we conduct comprehensive evaluations and prove that our MusiScene is more capable of generating contextually relevant captions compared to MU-LLaMA. We leverage the generated MSI captions to enhance Video Background Music Generation (VBMG) from text. 

**Abstract (ZH)**: 人类在聆听音乐时可以想象各种氛围和场景，设想与音乐相 complement 的电影场景。例如，悲伤缓慢的音乐可能会让人联想到失恋的场景，而欢快的旋律则可能让人想到庆祝的场景。本文探讨音乐语言模型（例如MU-LLaMA）是否能够执行类似的任务，即音乐场景想象（MSI），该任务需要结合视频和音乐的跨模态信息进行训练。为了改进现有的仅专注于音乐元素的音乐字幕模型，本文引入了MusiScene，这是一种设计用于想象与音乐相 complement 的场景的音乐字幕模型。本文包含以下内容：（1）构建了一个包含3,371对的大量视频-音频字幕数据集；（2）对Music Understanding LLaMA进行微调以完成MSI任务，创建了MusiScene；（3）进行了全面评估，并证明了我们的MusiScene在生成上下文相关字幕方面比MU-LLaMA更具备能力。我们利用生成的MSI字幕来增强文本驱动的视频背景音乐生成（VBMG）。 

---
# Decomposing the Time Series Forecasting Pipeline: A Modular Approach for Time Series Representation, Information Extraction, and Projection 

**Title (ZH)**: 时间序列forecasting管道分解：一种时间序列表示、信息提取和投影的模块化方法 

**Authors**: Robert Leppich, Michael Stenger, André Bauer, Samuel Kounev  

**Link**: [PDF](https://arxiv.org/pdf/2507.05891)  

**Abstract**: With the advent of Transformers, time series forecasting has seen significant advances, yet it remains challenging due to the need for effective sequence representation, memory construction, and accurate target projection. Time series forecasting remains a challenging task, demanding effective sequence representation, meaningful information extraction, and precise future projection. Each dataset and forecasting configuration constitutes a distinct task, each posing unique challenges the model must overcome to produce accurate predictions. To systematically address these task-specific difficulties, this work decomposes the time series forecasting pipeline into three core stages: input sequence representation, information extraction and memory construction, and final target projection. Within each stage, we investigate a range of architectural configurations to assess the effectiveness of various modules, such as convolutional layers for feature extraction and self-attention mechanisms for information extraction, across diverse forecasting tasks, including evaluations on seven benchmark datasets. Our models achieve state-of-the-art forecasting accuracy while greatly enhancing computational efficiency, with reduced training and inference times and a lower parameter count. The source code is available at this https URL. 

**Abstract (ZH)**: 随Transformer的出现，时间序列预测取得了显著进展，但仍面临有效序列表示、记忆构建和准确目标投影的挑战。时间序列预测 remains a challenging task，需要有效的序列表示、有意义的信息提取和精确的未来投影。每个数据集和预测配置构成一个独特的任务，每个任务都提出了模型必须克服的特定挑战，以生成准确的预测。为系统地应对这些任务特定的困难，本研究将时间序列预测管线分解为三个核心阶段：输入序列表示、信息提取和记忆构建，以及最终目标投影。在每个阶段中，我们探讨了一系列架构配置，以评估各种模块（如用于特征提取的卷积层和用于信息提取的自注意力机制）的有效性，涵盖多种预测任务，并在七个基准数据集上进行了评估。我们的模型在实现最先进的预测准确性的同时，显著提高了计算效率，减少了训练和推理时间，并降低了参数数量。源代码可在以下链接获取：this https URL。 

---
# Current Practices for Building LLM-Powered Reasoning Tools Are Ad Hoc -- and We Can Do Better 

**Title (ZH)**: 当前为构建LLM驱动的推理工具的做法是临时性的——我们可以做得更好。 

**Authors**: Aaron Bembenek  

**Link**: [PDF](https://arxiv.org/pdf/2507.05886)  

**Abstract**: There is growing excitement about building software verifiers, synthesizers, and other Automated Reasoning (AR) tools by combining traditional symbolic algorithms and Large Language Models (LLMs). Unfortunately, the current practice for constructing such neurosymbolic AR systems is an ad hoc programming model that does not have the strong guarantees of traditional symbolic algorithms, nor a deep enough synchronization of neural networks and symbolic reasoning to unlock the full potential of LLM-powered reasoning. I propose Neurosymbolic Transition Systems as a principled computational model that can underlie infrastructure for building neurosymbolic AR tools. In this model, symbolic state is paired with intuition, and state transitions operate over symbols and intuition in parallel. I argue why this new paradigm can scale logical reasoning beyond current capabilities while retaining the strong guarantees of symbolic algorithms, and I sketch out how the computational model I propose can be reified in a logic programming language. 

**Abstract (ZH)**: 基于神经符号过渡系统的自动推理工具原理计算模型 

---
# CogniPlay: a work-in-progress Human-like model for General Game Playing 

**Title (ZH)**: CogniPlay: 一个进展中的类人通用游戏-playing 模型 

**Authors**: Aloïs Rautureau, Éric Piette  

**Link**: [PDF](https://arxiv.org/pdf/2507.05868)  

**Abstract**: While AI systems have equaled or surpassed human performance in a wide variety of games such as Chess, Go, or Dota 2, describing these systems as truly "human-like" remains far-fetched. Despite their success, they fail to replicate the pattern-based, intuitive decision-making processes observed in human cognition. This paper presents an overview of findings from cognitive psychology and previous efforts to model human-like behavior in artificial agents, discusses their applicability to General Game Playing (GGP) and introduces our work-in-progress model based on these observations: CogniPlay. 

**Abstract (ZH)**: 虽然人工智能系统在国际象棋、围棋或Dota 2等广泛的游戏领域中已达到或超过了人类的表现，但将这些系统真正描述为“类似于人类”的仍然为时过早。尽管取得了成功，它们仍然未能复制人类认知中基于模式、直觉的决策过程。本文综述了认知心理学的研究发现和以前为人工代理建模类似人类行为的努力，讨论了这些方法在通用游戏-playing（GGP）中的适用性，并介绍了基于这些观察成果的工作中模型：CogniPlay。 

---
# Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity 

**Title (ZH)**: 情感化ROP测试器：预测极早产儿视网膜病变能力及偏见分析 

**Authors**: Shuai Zhao, Yulin Zhang, Luwei Xiao, Xinyi Wu, Yanhao Jia, Zhongliang Guo, Xiaobao Wu, Cong-Duy Nguyen, Guoming Zhang, Anh Tuan Luu  

**Link**: [PDF](https://arxiv.org/pdf/2507.05816)  

**Abstract**: Despite the remarkable progress of large language models (LLMs) across various domains, their capacity to predict retinopathy of prematurity (ROP) risk remains largely unexplored. To address this gap, we introduce a novel Chinese benchmark dataset, termed CROP, comprising 993 admission records annotated with low, medium, and high-risk labels. To systematically examine the predictive capabilities and affective biases of LLMs in ROP risk stratification, we propose Affective-ROPTester, an automated evaluation framework incorporating three prompting strategies: Instruction-based, Chain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme assesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and ICL schemes leverage external medical knowledge to enhance predictive accuracy. Crucially, we integrate emotional elements at the prompt level to investigate how different affective framings influence the model's ability to predict ROP and its bias patterns. Empirical results derived from the CROP dataset yield two principal observations. First, LLMs demonstrate limited efficacy in ROP risk prediction when operating solely on intrinsic knowledge, yet exhibit marked performance gains when augmented with structured external inputs. Second, affective biases are evident in the model outputs, with a consistent inclination toward overestimating medium- and high-risk cases. Third, compared to negative emotions, positive emotional framing contributes to mitigating predictive bias in model outputs. These findings highlight the critical role of affect-sensitive prompt engineering in enhancing diagnostic reliability and emphasize the utility of Affective-ROPTester as a framework for evaluating and mitigating affective bias in clinical language modeling systems. 

**Abstract (ZH)**: 尽管大型语言模型（LLMs）在各个领域取得了显著进展，但它们预测早产儿视网膜病变（ROP）风险的能力仍 largely unexplored。为弥补这一不足，我们引入了一个新型中文基准数据集 CROP，包含 993 份住院记录，标注为低风险、中风险和高风险。为系统地考察语言模型在 ROP 风险分层中的预测能力和情感偏差，我们提出了 Affective-ROPTester，这是一种结合三种提示策略的自动评估框架：指令（Instruction）、逐步推理（Chain-of-Thought, CoT）和上下文学习（In-Context Learning, ICL）。指令方案评估模型的内在知识及其相关偏差，而 CoT 和 ICL 方案则借助外部医学知识来提高预测准确性。关键在于我们在提示层面整合了情感元素，以探讨不同情感框架如何影响模型预测 ROP 及其偏差模式的能力。从 CROP 数据集得出的实证结果揭示了两项主要发现。首先，仅依赖内在知识时，LLMs 在 ROP 风险预测中效果有限，但结合结构化外部输入后，性能显著提升。其次，模型输出中存在情感偏差，表现为对中风险和高风险病例的过度估计。第三，与负面情绪相比，正面情绪框架有助于减轻模型输出中的预测偏差。这些发现强调了情感敏感提示工程在提高诊断可靠性和评估及减轻临床语言模型中情感偏差方面的关键作用。 

---
# GTA1: GUI Test-time Scaling Agent 

**Title (ZH)**: GTA1: GUI 测试时扩展代理 

**Authors**: Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, Ran Xu, Liyuan Pan, Caiming Xiong, Junnan Li  

**Link**: [PDF](https://arxiv.org/pdf/2507.05791)  

**Abstract**: Graphical user interface (GUI) agents autonomously operate across platforms (e.g., Linux) to complete tasks by interacting with visual elements. Specifically, a user instruction is decomposed into a sequence of action proposals, each corresponding to an interaction with the GUI. After each action, the agent observes the updated GUI environment to plan the next step. However, two main challenges arise: i) resolving ambiguity in task planning (i.e., the action proposal sequence), where selecting an appropriate plan is non-trivial, as many valid ones may exist; ii) accurately grounding actions in complex and high-resolution interfaces, i.e., precisely interacting with visual targets.
This paper investigates the two aforementioned challenges with our GUI Test-time Scaling Agent, namely GTA1. First, to select the most appropriate action proposal, we introduce a test-time scaling method. At each step, we sample multiple candidate action proposals and leverage a judge model to evaluate and select the most suitable one. It trades off computation for better decision quality by concurrent sampling, shortening task execution steps, and improving overall performance. Second, we propose a model that achieves improved accuracy when grounding the selected action proposal to its corresponding visual elements. Our key insight is that reinforcement learning (RL) facilitates visual grounding through inherent objective alignments, rewarding successful clicks on interface elements.
Experimentally, our method establishes state-of-the-art performance across diverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7% accuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When paired with a planner applying our test-time scaling strategy, it exhibits state-of-the-art agentic performance (e.g., 45.2% task success rate on OSWorld). We open-source our code and models here. 

**Abstract (ZH)**: 图形用户界面（GUI）代理自主跨平台（例如Linux）操作以通过与视觉元素交互来完成任务。具体而言，用户指令被分解为一系列行动建议序列，每个建议对应于一次与GUI的交互。每次执行行动后，代理会观察更新后的GUI环境并计划下一步。然而，存在两个主要挑战：i）任务规划中的歧义性（即行动建议序列），选择合适的计划并不容易，因为可能存在多个有效的计划；ii）在复杂和高分辨率的界面中精确地定位行动，即精准地与视觉目标交互。 

---
# Real-time monitoring of the SoH of lithium-ion batteries 

**Title (ZH)**: 锂离子电池实时状态监测 

**Authors**: Bruno Jammes, Edgar Hernando Sepúlveda-Oviedo, Corinne Alonso  

**Link**: [PDF](https://arxiv.org/pdf/2507.05765)  

**Abstract**: Real-time monitoring of the state of health (SoH) of batteries remains a major challenge, particularly in microgrids where operational constraints limit the use of traditional methods. As part of the 4BLife project, we propose an innovative method based on the analysis of a discharge pulse at the end of the charge phase. The parameters of the equivalent electrical model describing the voltage evolution across the battery terminals during this current pulse are then used to estimate the SoH. Based on the experimental data acquired so far, the initial results demonstrate the relevance of the proposed approach. After training using the parameters of two batteries with a capacity degradation of around 85%, we successfully predicted the degradation of two other batteries, cycled down to approximately 90% SoH, with a mean absolute error of around 1% in the worst case, and an explainability score of the estimator close to 0.9. If these performances are confirmed, this method can be easily integrated into battery management systems (BMS) and paves the way for optimized battery management under continuous operation. 

**Abstract (ZH)**: 基于放电脉冲的电池健康状态实时监测方法及其初步结果 

---
# An autonomous agent for auditing and improving the reliability of clinical AI models 

**Title (ZH)**: 自主代理用于审计和提升临床AI模型的可靠性 

**Authors**: Lukas Kuhn, Florian Buettner  

**Link**: [PDF](https://arxiv.org/pdf/2507.05755)  

**Abstract**: The deployment of AI models in clinical practice faces a critical challenge: models achieving expert-level performance on benchmarks can fail catastrophically when confronted with real-world variations in medical imaging. Minor shifts in scanner hardware, lighting or demographics can erode accuracy, but currently reliability auditing to identify such catastrophic failure cases before deployment is a bespoke and time-consuming process. Practitioners lack accessible and interpretable tools to expose and repair hidden failure modes. Here we introduce ModelAuditor, a self-reflective agent that converses with users, selects task-specific metrics, and simulates context-dependent, clinically relevant distribution shifts. ModelAuditor then generates interpretable reports explaining how much performance likely degrades during deployment, discussing specific likely failure modes and identifying root causes and mitigation strategies. Our comprehensive evaluation across three real-world clinical scenarios - inter-institutional variation in histopathology, demographic shifts in dermatology, and equipment heterogeneity in chest radiography - demonstrates that ModelAuditor is able correctly identify context-specific failure modes of state-of-the-art models such as the established SIIM-ISIC melanoma classifier. Its targeted recommendations recover 15-25% of performance lost under real-world distribution shift, substantially outperforming both baseline models and state-of-the-art augmentation methods. These improvements are achieved through a multi-agent architecture and execute on consumer hardware in under 10 minutes, costing less than US$0.50 per audit. 

**Abstract (ZH)**: AI模型在临床实践中的部署面临着关键挑战：在面对医疗成像中的现实世界变异性时，达到专家级性能的模型可能会遭遇灾难性的失败。ModelAuditor：一种自反思代理，通过与用户对话、选择特定任务的指标，并模拟上下文相关、临床相关的分布转变，生成可解释的报告，解释部署过程中性能可能下降的程度，讨论具体的潜在失败模式并识别根本原因和缓解策略。模型评估跨三个实际临床场景（机构间病理学变异性、皮肤科人口统计学转变和胸部X光检查设备异质性）表明，ModelAuditor能够正确识别先进模型（如现有SIIM-ISIC黑色素瘤分类器）的上下文特定失败模式。其针对性建议在实际分布变化下恢复15-25%的性能损失，显著优于基线模型和最先进的增强方法。这些改进通过多代理架构实现，并在不到10分钟的时间内运行在消费级硬件上，成本低于每审计0.50美元。 

---
# Divergent Realities: A Comparative Analysis of Human Expert vs. Artificial Intelligence Based Generation and Evaluation of Treatment Plans in Dermatology 

**Title (ZH)**: 分歧的现实：皮肤科治疗计划基于人类专家与人工智能生成及评估的对比分析 

**Authors**: Dipayan Sengupta, Saumya Panda  

**Link**: [PDF](https://arxiv.org/pdf/2507.05716)  

**Abstract**: Background: Evaluating AI-generated treatment plans is a key challenge as AI expands beyond diagnostics, especially with new reasoning models. This study compares plans from human experts and two AI models (a generalist and a reasoner), assessed by both human peers and a superior AI judge.
Methods: Ten dermatologists, a generalist AI (GPT-4o), and a reasoning AI (o3) generated treatment plans for five complex dermatology cases. The anonymized, normalized plans were scored in two phases: 1) by the ten human experts, and 2) by a superior AI judge (Gemini 2.5 Pro) using an identical rubric.
Results: A profound 'evaluator effect' was observed. Human experts scored peer-generated plans significantly higher than AI plans (mean 7.62 vs. 7.16; p=0.0313), ranking GPT-4o 6th (mean 7.38) and the reasoning model, o3, 11th (mean 6.97). Conversely, the AI judge produced a complete inversion, scoring AI plans significantly higher than human plans (mean 7.75 vs. 6.79; p=0.0313). It ranked o3 1st (mean 8.20) and GPT-4o 2nd, placing all human experts lower.
Conclusions: The perceived quality of a clinical plan is fundamentally dependent on the evaluator's nature. An advanced reasoning AI, ranked poorly by human experts, was judged as superior by a sophisticated AI, revealing a deep gap between experience-based clinical heuristics and data-driven algorithmic logic. This paradox presents a critical challenge for AI integration, suggesting the future requires synergistic, explainable human-AI systems that bridge this reasoning gap to augment clinical care. 

**Abstract (ZH)**: 背景：评估AI生成的治疗方案是AI超越诊断领域的一个关键挑战，尤其是随着新型推理模型的出现。本研究比较了由人类专家和两种AI模型（一个通识模型和一个推理模型）生成的治疗方案，并由人类同行和一个高级AI评审员进行评估。

方法：十名皮肤科医生、一个通识AI（GPT-4o）和一个推理AI（o3）为五个复杂的皮肤科病例生成了治疗方案。匿名且标准化的方案分两阶段评分：1) 由十名人类专家评分；2) 由一个高级AI评审员（Gemini 2.5 Pro）使用相同的评分标准进行评分。

结果：观察到了明显的“评估者效应”。人类专家对同行生成的方案评分明显高于对AI生成的方案评分（平均7.62 vs. 7.16；p=0.0313），其中GPT-4o排名第6位（平均7.38），推理模型o3排名第11位（平均6.97）。相反，AI评审员对AI生成的方案评分显著高于人类生成的方案（平均7.75 vs. 6.79；p=0.0313）。AI评审员将o3评为第1名（平均8.20），将GPT-4o评为第2名，将所有人类专家都排在了后面。

结论：临床方案的质量感知从根本上依赖于评估者的性质。一个高级推理AI，在人类专家中排名较低，却得到了一个复杂AI的优越评分，揭示了基于经验的临床直觉与基于数据的算法逻辑之间存在的巨大差距。这一悖论提出了AI集成的关键挑战，表明未来需要协同的、可解释的AI-人类系统来弥合这种推理差距，以增强临床护理。 

---
# City-Level Foreign Direct Investment Prediction with Tabular Learning on Judicial Data 

**Title (ZH)**: 基于司法数据的表格学习的城市级外国直接投资预测 

**Authors**: Tianxing Wu, Lizhe Cao, Shuang Wang, Jiming Wang, Shutong Zhu, Yerong Wu, Yuqing Feng  

**Link**: [PDF](https://arxiv.org/pdf/2507.05651)  

**Abstract**: To advance the United Nations Sustainable Development Goal on promoting sustained, inclusive, and sustainable economic growth, foreign direct investment (FDI) plays a crucial role in catalyzing economic expansion and fostering innovation. Precise city-level FDI prediction is quite important for local government and is commonly studied based on economic data (e.g., GDP). However, such economic data could be prone to manipulation, making predictions less reliable. To address this issue, we try to leverage large-scale judicial data which reflects judicial performance influencing local investment security and returns, for city-level FDI prediction. Based on this, we first build an index system for the evaluation of judicial performance over twelve million publicly available adjudication documents according to which a tabular dataset is reformulated. We then propose a new Tabular Learning method on Judicial Data (TLJD) for city-level FDI prediction. TLJD integrates row data and column data in our built tabular dataset for judicial performance indicator encoding, and utilizes a mixture of experts model to adjust the weights of different indicators considering regional variations. To validate the effectiveness of TLJD, we design cross-city and cross-time tasks for city-level FDI predictions. Extensive experiments on both tasks demonstrate the superiority of TLJD (reach to at least 0.92 R2) over the other ten state-of-the-art baselines in different evaluation metrics. 

**Abstract (ZH)**: 促进联合国可持续发展目标中的持续、包容和可持续经济增长，外国直接投资（FDI）在推动经济扩张和促进创新中起着关键作用。基于大量司法数据的市级FDI预测指数系统与方法研究 

---
# LLMs are Introvert 

**Title (ZH)**: LLMs是内向的 

**Authors**: Litian Zhang, Xiaoming Zhang, Bingyu Yan, Ziyi Zhou, Bo Zhang, Zhenyu Guan, Xi Zhang, Chaozhuo Li  

**Link**: [PDF](https://arxiv.org/pdf/2507.05638)  

**Abstract**: The exponential growth of social media and generative AI has transformed information dissemination, fostering connectivity but also accelerating the spread of misinformation. Understanding information propagation dynamics and developing effective control strategies is essential to mitigate harmful content. Traditional models, such as SIR, provide basic insights but inadequately capture the complexities of online interactions. Advanced methods, including attention mechanisms and graph neural networks, enhance accuracy but typically overlook user psychology and behavioral dynamics. Large language models (LLMs), with their human-like reasoning, offer new potential for simulating psychological aspects of information spread. We introduce an LLM-based simulation environment capturing agents' evolving attitudes, emotions, and responses. Initial experiments, however, revealed significant gaps between LLM-generated behaviors and authentic human dynamics, especially in stance detection and psychological realism. A detailed evaluation through Social Information Processing Theory identified major discrepancies in goal-setting and feedback evaluation, stemming from the lack of emotional processing in standard LLM training. To address these issues, we propose the Social Information Processing-based Chain of Thought (SIP-CoT) mechanism enhanced by emotion-guided memory. This method improves the interpretation of social cues, personalization of goals, and evaluation of feedback. Experimental results confirm that SIP-CoT-enhanced LLM agents more effectively process social information, demonstrating behaviors, attitudes, and emotions closer to real human interactions. In summary, this research highlights critical limitations in current LLM-based propagation simulations and demonstrates how integrating SIP-CoT and emotional memory significantly enhances the social intelligence and realism of LLM agents. 

**Abstract (ZH)**: 基于社会信息处理的链式思维机制增强的情感引导记忆在社交媒体信息传播模拟中的应用 

---
# Enhancing Student Learning with LLM-Generated Retrieval Practice Questions: An Empirical Study in Data Science Courses 

**Title (ZH)**: 利用生成式检索练习题增强学生学习：数据科学课程中的实证研究 

**Authors**: Yuan An, John Liu, Niyam Acharya, Ruhma Hashmi  

**Link**: [PDF](https://arxiv.org/pdf/2507.05629)  

**Abstract**: Retrieval practice is a well-established pedagogical technique known to significantly enhance student learning and knowledge retention. However, generating high-quality retrieval practice questions is often time-consuming and labor intensive for instructors, especially in rapidly evolving technical subjects. Large Language Models (LLMs) offer the potential to automate this process by generating questions in response to prompts, yet the effectiveness of LLM-generated retrieval practice on student learning remains to be established. In this study, we conducted an empirical study involving two college-level data science courses, with approximately 60 students. We compared learning outcomes during one week in which students received LLM-generated multiple-choice retrieval practice questions to those from a week in which no such questions were provided. Results indicate that students exposed to LLM-generated retrieval practice achieved significantly higher knowledge retention, with an average accuracy of 89%, compared to 73% in the week without such practice. These findings suggest that LLM-generated retrieval questions can effectively support student learning and may provide a scalable solution for integrating retrieval practice into real-time teaching. However, despite these encouraging outcomes and the potential time-saving benefits, cautions must be taken, as the quality of LLM-generated questions can vary. Instructors must still manually verify and revise the generated questions before releasing them to students. 

**Abstract (ZH)**: 基于检索练习的生成式大型语言模型在提高学生学习效果中的应用：一项 empirical 研究 

---
# ADMC: Attention-based Diffusion Model for Missing Modalities Feature Completion 

**Title (ZH)**: 基于注意力的扩散模型在缺失模态特征完成中的应用 

**Authors**: Wei Zhang, Juan Chen, Yanbo J. Wang, En Zhu, Xuan Yang, Yiduo Wang  

**Link**: [PDF](https://arxiv.org/pdf/2507.05624)  

**Abstract**: Multimodal emotion and intent recognition is essential for automated human-computer interaction, It aims to analyze users' speech, text, and visual information to predict their emotions or intent. One of the significant challenges is that missing modalities due to sensor malfunctions or incomplete data. Traditional methods that attempt to reconstruct missing information often suffer from over-coupling and imprecise generation processes, leading to suboptimal outcomes. To address these issues, we introduce an Attention-based Diffusion model for Missing Modalities feature Completion (ADMC). Our framework independently trains feature extraction networks for each modality, preserving their unique characteristics and avoiding over-coupling. The Attention-based Diffusion Network (ADN) generates missing modality features that closely align with authentic multimodal distribution, enhancing performance across all missing-modality scenarios. Moreover, ADN's cross-modal generation offers improved recognition even in full-modality contexts. Our approach achieves state-of-the-art results on the IEMOCAP and MIntRec benchmarks, demonstrating its effectiveness in both missing and complete modality scenarios. 

**Abstract (ZH)**: 基于注意力的扩散模型在缺失模态特征完成中的应用：多模态情感和意图识别对于自动人机交互至关重要，旨在分析用户的语音、文本和视觉信息以预测其情感或意图。一个显著的挑战是由于传感器故障或数据不完整导致的缺失模态。传统的尝试重建缺失信息的方法通常会遭受过度耦合和不精确生成过程的困扰，导致次优结果。为了解决这些问题，我们提出了一种基于注意力的扩散模型（ADMC）用于缺失模态特征完成。我们的框架独立训练每个模态的特征提取网络，保留其独特特征，避免过度耦合。基于注意力的扩散网络（ADN）生成与真实多模态分布高度一致的缺失模态特征，提高所有缺失模态场景下的性能。此外，ADN的跨模态生成在全模态场景下也能提供更好的识别效果。我们的方法在IEMOCAP和MIntRec基准测试中取得了最先进的结果，证明了它在缺失和完整模态场景下的有效性。 

---
# Domain adaptation of large language models for geotechnical applications 

**Title (ZH)**: 大型语言模型在地质工程应用中的领域适应性 

**Authors**: Lei Fan, Fangxue Liu, Cheng Chen  

**Link**: [PDF](https://arxiv.org/pdf/2507.05613)  

**Abstract**: Recent developments in large language models (LLMs) are opening up new opportunities in geotechnical engineering and engineering geology. While general-purpose LLMs possess broad capabilities, effective application in geotechnics often requires domain-specific adaptation. Such tailored LLMs are increasingly employed to streamline geotechnical workflows. This paper presents the first survey of the adaptation and application of LLMs in geotechnical engineering. It outlines key methodologies for adaptation to geotechnical domain, including prompt engineering, retrieval-augmented generation, domain-adaptive pretraining, and fine-tuning. The survey examines the state-of-the-art applications of geotechnical-adapted LLMs, including geological interpretation, subsurface characterization, site planning, design calculations, numerical modeling, safety and risk assessment, and educational tutoring. It also analyzes benefits and limitations of geotechnical-adapted LLMs, and identifies promising directions for future research in this interdisciplinary discipline. The findings serve as a valuable resource for practitioners seeking to integrate LLMs into geotechnical practice, while also providing a foundation to stimulate further investigation within the academic community. 

**Abstract (ZH)**: Recent developments in大型语言模型（LLMs）为地质工程和工程地质领域开辟了新机遇。虽然通用的LLMs具有广泛的性能，但在地质工程中的有效应用通常需要专门的领域适应。此类定制化的LLMs越来越被用于简化地质工程工作流程。本文提供了首个关于地质工程中LLM适应与应用的综述。概述了适应地质工程领域的关键方法，包括提示工程、检索增强生成、领域适应预训练和微调。综述还探讨了地质工程适应的LLMs的最新应用，包括地质解释、地下表征、场地规划、设计计算、数值建模、安全与风险评估以及教育辅导。分析了地质工程适应的LLMs的优势与局限性，并指出了未来研究的有前途的方向。研究发现为希望将LLMs整合到地质工程实践中的人士提供了宝贵资源，同时为学术界进一步研究奠定了基础。 

---
# MLlm-DR: Towards Explainable Depression Recognition with MultiModal Large Language Models 

**Title (ZH)**: MLlm-DR：基于多模态大型语言模型的可解释抑郁识别 

**Authors**: Wei Zhang, Juan Chen, En Zhu, Wenhong Cheng, YunPeng Li, Yanbo J. Wang  

**Link**: [PDF](https://arxiv.org/pdf/2507.05591)  

**Abstract**: Automated depression diagnosis aims to analyze multimodal information from interview videos to predict participants' depression scores. Previous studies often lack clear explanations of how these scores were determined, limiting their adoption in clinical practice. While the advent of LLMs provides a possible pathway for explainable depression diagnosis, current LLMs capable of processing multimodal data lack training on interview data, resulting in poor diagnostic performance when used directly. In this paper, we propose a novel multimodal large language model (MLlm-DR) that can understand multimodal information inputs and supports explainable depression diagnosis. MLlm-DR integrates a smaller LLMs and a lightweight query module (LQ-former). Specifically, the smaller LLMs is designed to generate depression scores and corresponding evaluation rationales. To enhance its logical reasoning for domain-specific tasks while maintaining practicality, we constructed a robust training dataset to fine-tune it. Meanwhile, the LQ-former captures depression-related features from speech and visual data, aiding the model's ability to process multimodal information, to achieve comprehensive depression diagnosis. Our approach achieves state-of-the-art results on two interview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating its effectiveness and superiority. 

**Abstract (ZH)**: 自动抑郁诊断旨在分析访谈视频的多模态信息以预测参与者的抑郁评分。之前的研究往往缺乏这些评分确定的清晰解释，限制了其在临床实践中的应用。尽管大型语言模型（LLMs）的出现为可解释的抑郁诊断提供了可能途径，但当前能够处理多模态数据的LLMs缺乏对访谈数据的训练，导致直接使用时诊断性能较差。在本文中，我们提出了一种新的多模态大型语言模型（MLlm-DR），能够理解多模态信息输入并支持可解释的抑郁诊断。MLlm-DR结合了一个较小的LLMs和一个轻量级查询模块（LQ-former）。具体而言，较小的LLMs设计用于生成抑郁评分及其相应的评价理由。为了增强其针对特定领域的逻辑推理能力并保持实用性，我们构建了一个稳健的训练数据集对其进行微调。同时，LQ-former从语音和视觉数据中捕获与抑郁相关的特征，帮助模型处理多模态信息，实现全面的抑郁诊断。我们的方法在两个基于访谈的基准数据集CMDC和E-DAIC-WOZ上取得了最先进的结果，展示了其有效性和优越性。 

---
# Towards Measurement Theory for Artificial Intelligence 

**Title (ZH)**: 面向人工智能的度量理论研究 

**Authors**: Elija Perrier  

**Link**: [PDF](https://arxiv.org/pdf/2507.05587)  

**Abstract**: We motivate and outline a programme for a formal theory of measurement of artificial intelligence. We argue that formalising measurement for AI will allow researchers, practitioners, and regulators to: (i) make comparisons between systems and the evaluation methods applied to them; (ii) connect frontier AI evaluations with established quantitative risk analysis techniques drawn from engineering and safety science; and (iii) foreground how what counts as AI capability is contingent upon the measurement operations and scales we elect to use. We sketch a layered measurement stack, distinguish direct from indirect observables, and signpost how these ingredients provide a pathway toward a unified, calibratable taxonomy of AI phenomena. 

**Abstract (ZH)**: 我们提出并概述了一种形式化理论计量人工智能的计划。我们 arguing that 形式化计量对于人工智能将使研究者、实践者和监管者能够：(i) 在系统及其应用的评估方法之间进行比较；(ii) 将前沿的人工智能评估与源自工程和安全科学的已建立的定量风险分析技术联系起来；以及(iii) 突出人工智能能力的界定依赖于我们选择使用的计量操作和尺度。我们勾勒了一种层次化的计量栈，区分直接可观察与间接可观察的变量，并指出这些成分提供了通往统一且可标定的人工智能现象分类系统的路径。标题：

一种形式化人工智能计量的计划 

---
# SingLoRA: Low Rank Adaptation Using a Single Matrix 

**Title (ZH)**: SingLoRA: 低秩适应性训练使用单个矩阵 

**Authors**: David Bensaïd, Noam Rotstein, Roy Velich, Daniel Bensaïd, Ron Kimmel  

**Link**: [PDF](https://arxiv.org/pdf/2507.05566)  

**Abstract**: Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient fine-tuning of large pretrained models. LoRA augments the pre-trained weights of a model by adding the product of two smaller matrices that together form a low-rank matrix update. Recent research has shown that scale disparities between these two matrices often cause unstable training dynamics, leading to suboptimal performance. In this paper, we propose SingLoRA, which reformulates low-rank adaptation by learning the weights update as a decomposition of a single low-rank matrix multiplied by its transpose. This simple design inherently removes inter-matrix scale conflicts, ensuring stable optimization, and roughly halves the parameter count. We analyze SingLoRA within the infinite-width neural network framework, showing that it guarantees stable feature learning by construction. Extensive experiments on multiple tasks validate these benefits. In common sense reasoning, fine-tuning LLama 7B on MNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+ (90.2%) - while using only 60% of their parameter budget. In image generation, fine-tuning Stable Diffusion with SingLoRA significantly improves image fidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to scores of 0.148 and 0.143 for DoRA and LoRA, respectively. 

**Abstract (ZH)**: 基于单低秩矩阵分解的SingLoRA在大型预训练模型参数高效微调中的应用探索 

---
# SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation 

**Title (ZH)**: SenseCF: LLM驱动的反事实生成用于干预和传感器数据增强 

**Authors**: Shovito Barua Soumma, Asiful Arefeen, Stephanie M. Carpenter, Melanie Hingle, Hassan Ghasemzadeh  

**Link**: [PDF](https://arxiv.org/pdf/2507.05541)  

**Abstract**: Counterfactual explanations (CFs) offer human-centric insights into machine learning predictions by highlighting minimal changes required to alter an outcome. Therefore, CFs can be used as (i) interventions for abnormality prevention and (ii) augmented data for training robust models. In this work, we explore large language models (LLMs), specifically GPT-4o-mini, for generating CFs in a zero-shot and three-shot setting. We evaluate our approach on two datasets: the AI-Readi flagship dataset for stress prediction and a public dataset for heart disease detection. Compared to traditional methods such as DiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high plausibility (up to 99%), strong validity (up to 0.99), and competitive sparsity. Moreover, using LLM-generated CFs as augmented samples improves downstream classifier performance (an average accuracy gain of 5%), especially in low-data regimes. This demonstrates the potential of prompt-based generative techniques to enhance explainability and robustness in clinical and physiological prediction tasks. Code base: this http URL. 

**Abstract (ZH)**: 基于大型语言模型的零样本和三样本反事实解释生成及其在临床和生理预测任务中的应用 

---
# Red Teaming AI Red Teaming 

**Title (ZH)**: 红队测评AI 

**Authors**: Subhabrata Majumdar, Brian Pendleton, Abhishek Gupta  

**Link**: [PDF](https://arxiv.org/pdf/2507.05538)  

**Abstract**: Red teaming has evolved from its origins in military applications to become a widely adopted methodology in cybersecurity and AI. In this paper, we take a critical look at the practice of AI red teaming. We argue that despite its current popularity in AI governance, there exists a significant gap between red teaming's original intent as a critical thinking exercise and its narrow focus on discovering model-level flaws in the context of generative AI. Current AI red teaming efforts focus predominantly on individual model vulnerabilities while overlooking the broader sociotechnical systems and emergent behaviors that arise from complex interactions between models, users, and environments. To address this deficiency, we propose a comprehensive framework operationalizing red teaming in AI systems at two levels: macro-level system red teaming spanning the entire AI development lifecycle, and micro-level model red teaming. Drawing on cybersecurity experience and systems theory, we further propose a set of recommendations. In these, we emphasize that effective AI red teaming requires multifunctional teams that examine emergent risks, systemic vulnerabilities, and the interplay between technical and social factors. 

**Abstract (ZH)**: 红队演练从其军事应用起源发展成为网络安全和AI领域的广泛应用方法。本文批判性地探讨了AI红队演练的做法。我们argue尽管AI治理中当前对其有很高的关注，但其原始意图——作为一种批判性思维练习——与在生成式AI背景下仅专注于发现模型级缺陷的战略性狭窄视角之间存在显著差距。当前的AI红队演练主要关注个体模型的漏洞，而忽视了模型、用户和环境之间复杂交互所引发的更广泛的社会技术系统及其涌现行为。为解决这一缺陷，我们提出了一种综合框架，将红队演练在AI系统中操作化为两个层面：宏观层面的AI系统红队演练，贯穿整个AI开发生命周期，以及微观层面的模型红队演练。借鉴网络安全经验和系统理论，我们进一步提出了若干建议。在这些建议中，我们强调有效的AI红队演练需要多职能团队，以审视新兴风险、系统性漏洞及其技术和社会因素之间的相互作用。 

---
# Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment 

**Title (ZH)**: 大规模对话教育：面向程序性学习和教育质量评估的多语言模型代理工作流 

**Authors**: Jiahuan Pei, Fanghua Ye, Xin Sun, Wentao Deng, Koen Hindriks, Junxiao Wang  

**Link**: [PDF](https://arxiv.org/pdf/2507.05528)  

**Abstract**: Large language models (LLMs) have advanced virtual educators and learners, bridging NLP with AI4Education. Existing work often lacks scalability and fails to leverage diverse, large-scale course content, with limited frameworks for assessing pedagogic quality. To this end, we propose WikiHowAgent, a multi-agent workflow leveraging LLMs to simulate interactive teaching-learning conversations. It integrates teacher and learner agents, an interaction manager, and an evaluator to facilitate procedural learning and assess pedagogic quality. We introduce a dataset of 114,296 teacher-learner conversations grounded in 14,287 tutorials across 17 domains and 727 topics. Our evaluation protocol combines computational and rubric-based metrics with human judgment alignment. Results demonstrate the workflow's effectiveness in diverse setups, offering insights into LLM capabilities across domains. Our datasets and implementations are fully open-sourced. 

**Abstract (ZH)**: 大型语言模型（LLMs）已经推动了虚拟教育者的进步，将自然语言处理（NLP）与AI4Education相结合。现有工作往往缺乏可扩展性，未能充分利用多样化的大量课程内容，并缺乏评估教学质量的框架。为此，我们提出了WikiHowAgent，这是一种利用大型语言模型模拟互动教学-学习对话的多代理工作流。它整合了教师代理、学习者代理、交互管理者和评估器，以促进程序性学习并评估教学质量。我们引入了一个基于14,287个教程的数据集，涵盖17个领域和727个主题，其中包括114,296次教师-学习者对话。我们的评估协议结合了计算性和准则基指标，并与人工判断对齐。结果显示该工作流在多种情境下的有效性，提供了关于LLM在不同领域能力的见解。我们的数据集和实现均已完全开源。 

---
# Cultivating Multimodal Intelligence: Interpretive Reasoning and Agentic RAG Approaches to Dermatological Diagnosis 

**Title (ZH)**: 培养多模态智能：解释性推理与代理性RAG方法在皮肤病诊断中的应用 

**Authors**: Karishma Thakrar, Shreyas Basavatia, Akshay Daftardar  

**Link**: [PDF](https://arxiv.org/pdf/2507.05520)  

**Abstract**: The second edition of the 2025 ImageCLEF MEDIQA-MAGIC challenge, co-organized by researchers from Microsoft, Stanford University, and the Hospital Clinic of Barcelona, focuses on multimodal dermatology question answering and segmentation, using real-world patient queries and images. This work addresses the Closed Visual Question Answering (CVQA) task, where the goal is to select the correct answer to multiple-choice clinical questions based on both user-submitted images and accompanying symptom descriptions. The proposed approach combines three core components: (1) fine-tuning open-source multimodal models from the Qwen, Gemma, and LLaMA families on the competition dataset, (2) introducing a structured reasoning layer that reconciles and adjudicates between candidate model outputs, and (3) incorporating agentic retrieval-augmented generation (agentic RAG), which adds relevant information from the American Academy of Dermatology's symptom and condition database to fill in gaps in patient context. The team achieved second place with a submission that scored sixth, demonstrating competitive performance and high accuracy. Beyond competitive benchmarks, this research addresses a practical challenge in telemedicine: diagnostic decisions must often be made asynchronously, with limited input and with high accuracy and interpretability. By emulating the systematic reasoning patterns employed by dermatologists when evaluating skin conditions, this architecture provided a pathway toward more reliable automated diagnostic support systems. 

**Abstract (ZH)**: 2025 ImageCLEF MEDIQA-MAGIC挑战的第二版，由微软、斯坦福大学和巴塞罗那医院诊所的研究人员共同组织，专注于多模态皮肤病问答和分割，使用实际患者的查询和图像。该研究针对闭合视觉问答（CVQA）任务，目标是基于用户提交的图像和伴随的症状描述，选择正确的临床问题答案。所提出的方法结合了三个核心组件：（1）在竞赛数据集上微调来自Qwen、Gemma和LLaMA家族的开源多模态模型，（2）引入结构化推理层，以协调和裁决候选模型输出，以及（3）结合有能检索增强生成（有能RAG），从美国皮肤病学会的症状和状况数据库中添加相关的信息，以填补患者背景中的空白。团队凭借提交的第六名成绩获得了亚军，展示了竞争力和高准确性。除了竞争基准，这项研究还解决了远程医疗中的一个实际挑战：诊断决策经常需要在有限的输入和高准确性和可解释性的条件下异步做出。通过模拟皮肤状况评估中皮肤科医生采用的系统推理模式，该架构为更可靠的自动化诊断支持系统提供了一条路径。 

---
# Modeling (Deontic) Modal Operators With the s(CASP) Goal-directed Predicated Answer Set Programming System 

**Title (ZH)**: 用 s(CASP) 目标导向谓词回答集编程系统建模（义务性）-modal 运算符 

**Authors**: Gopal Gupta, Abhiramon Rajasekharan, Alexis R. Tudor, Elmer Salazar, Joaquín Arias  

**Link**: [PDF](https://arxiv.org/pdf/2507.05519)  

**Abstract**: We consider the problem of implementing deontic modal logic. We show how (deontic) modal operators can be expressed elegantly using default negation (negation-as-failure) and strong negation present in answer set programming (ASP). We propose using global constraints of ASP to represent obligations and impermissibilities of deontic modal logic. We show that our proposed representation results in the various paradoxes of deontic modal logic being elegantly resolved. 

**Abstract (ZH)**: 我们考虑实现义务模态逻辑的问题。我们展示了如何使用回答集编程（ASP）中的默认否定（失败否定）和强否定来优雅地表达（义务）模态运算符。我们提议使用ASP的全局约束来表示义务模态逻辑中的义务和禁令。我们展示了我们提出的表现形式优雅地解决了义务模态逻辑的各种悖论。 

---
# Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality 

**Title (ZH)**: 增强现实环境中细粒度多模态训练助手的视觉-语言建模 

**Authors**: Haochen Huang, Jiahuan Pei, Mohammad Aliannejadi, Xin Sun, Moonisa Ahsan, Pablo Cesar, Chuang Yu, Zhaochun Ren, Junxiao Wang  

**Link**: [PDF](https://arxiv.org/pdf/2507.05515)  

**Abstract**: Vision-language models (VLMs) are essential for enabling AI-powered smart assistants to interpret and reason in multimodal environments. However, their application in augmented reality (AR) training remains largely unexplored. In this work, we introduce a comprehensive dataset tailored for AR training, featuring systematized vision-language tasks, and evaluate nine state-of-the-art VLMs on it. Our results reveal that even advanced models, including GPT-4o, struggle with fine-grained assembly tasks, achieving a maximum F1 score of just 40.54% on state detection. These findings highlight the demand for enhanced datasets, benchmarks, and further research to improve fine-grained vision-language alignment. Beyond technical contributions, our work has broader social implications, particularly in empowering blind and visually impaired users with equitable access to AI-driven learning opportunities. We provide all related resources, including the dataset, source code, and evaluation results, to support the research community. 

**Abstract (ZH)**: 基于视觉-语言模型的增强现实训练数据集及评估：超越细粒度视觉-语言对齐的技术与社会影响 

---
# Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents 

**Title (ZH)**: 深度研究比较器：深度研究代理细粒度人工注释平台 

**Authors**: Prahaladh Chandrahasan, Jiahe Jin, Zhihan Zhang, Tevin Wang, Andy Tang, Lucy Mo, Morteza Ziyadi, Leonardo F.R. Ribeiro, Zimeng Qiu, Markus Dreyer, Akari Asai, Chenyan Xiong  

**Link**: [PDF](https://arxiv.org/pdf/2507.05495)  

**Abstract**: Effectively evaluating deep research agents that autonomously search the web, analyze information, and generate reports remains a major challenge, particularly when it comes to assessing long reports and giving detailed feedback on their intermediate steps. To address these gaps, we introduce Deep Research Comparator, a platform that offers a holistic framework for deep research agent hosting, side-by-side comparison, fine-grained human feedback collection, and ranking calculation. Given a user query, our platform displays the final reports from two different agents along with their intermediate steps during generation. Annotators can evaluate the overall quality of final reports based on side-by-side comparison, and also provide detailed feedback separately by assessing intermediate steps or specific text spans within the final report. Furthermore, we develop Simple Deepresearch, an end-to-end agent scaffold. This scaffold serves as a baseline that facilitates the easy integration of various large language models to transform them into deep research agents for evaluation. To demonstrate the platform's utility for deep research agent development, we have collected real user preference data from 17 annotators on three deep research agents. A demo video of our platform can be found at this https URL. 

**Abstract (ZH)**: 有效评估自主搜索网络、分析信息并生成报告的深度研究代理仍然是一项重大挑战，尤其是在评估长报告和提供详细中间步骤反馈方面。为此，我们引入了Deep Research Comparator平台，该平台提供了一个全面的框架，用于深度研究代理托管、并排比较、细粒度的人工反馈收集和排名计算。给出用户查询后，该平台展示来自两个不同代理的最终报告及其生成过程中的中间步骤。标注者可以基于并排比较评估最终报告的整体质量，并通过评估中间步骤或最终报告内的特定文本段落提供详细的反馈。此外，我们开发了Simple Deepresearch，一个端到端的代理框架。该框架作为基线，便于多种大型语言模型的集成，使其转化为用于评估的深度研究代理。为了展示该平台在深度研究代理开发中的实用性，我们从17位标注者处收集了关于三个深度研究代理的真实用户偏好数据。我们的平台Demo视频可以在这里找到：这个https URL。 

---
# OLG++: A Semantic Extension of Obligation Logic Graph 

**Title (ZH)**: OLG++: 义务逻辑图的语义扩展 

**Authors**: Subhasis Dasgupta, Jon Stephens, Amarnath Gupta  

**Link**: [PDF](https://arxiv.org/pdf/2507.05488)  

**Abstract**: We present OLG++, a semantic extension of the Obligation Logic Graph (OLG) for modeling regulatory and legal rules in municipal and interjurisdictional contexts. OLG++ introduces richer node and edge types, including spatial, temporal, party group, defeasibility, and logical grouping constructs, enabling nuanced representations of legal obligations, exceptions, and hierarchies. The model supports structured reasoning over rules with contextual conditions, precedence, and complex triggers. We demonstrate its expressiveness through examples from food business regulations, showing how OLG++ supports legal question answering using property graph queries. OLG++ also improves over LegalRuleML by providing native support for subClassOf, spatial constraints, and reified exception structures. Our examples show that OLG++ is more expressive than prior graph-based models for legal knowledge representation. 

**Abstract (ZH)**: 我们呈现了OLG++，这是一种语义扩展的义务逻辑图（OLG），用于建模市政和跨辖区的监管与法律规则。OLG++引入了更丰富的节点和边缘类型，包括空间、时间、利益集团、可反驳性和逻辑分组构造，能够细腻地表示法律义务、例外情况和层次结构。该模型支持具有上下文条件、 precedency 和复杂触发条件的规则的结构化推理。通过食品业法规示例，我们展示了OLG++如何利用属性图查询实现法律问题解答的表达能力。与LegalRuleML相比，OLG++还提供了对子类关系、空间约束和显式例外结构的原生支持。我们的示例表明，OLG++比以往的基于图的法律知识表示模型更具表达性。 

---
# Fuzzy Classification Aggregation for a Continuum of Agents 

**Title (ZH)**: 连续体代理的模糊分类聚合 

**Authors**: Zijun Meng  

**Link**: [PDF](https://arxiv.org/pdf/2507.05297)  

**Abstract**: We prove that any optimal, independent, and zero unanimous fuzzy classification aggregation function of a continuum of individual classifications of $m\ge 3$ objects into $2\le p\le m$ types must be a weighted arithmetic mean. 

**Abstract (ZH)**: 我们证明，对于\(m \ge 3\)个对象的连续个体分类结果，将其聚合为\(2 \le p \le m\)种类型的任何最优、独立且零一致的模糊分类聚合函数必须是加权算术平均函数。 

---
# Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management 

**Title (ZH)**: Chat2SPaT: 一种基于大语言模型的交通信号控制计划管理自动化工具 

**Authors**: Yue Wang, Miao Zhou, Guijing Huang, Rui Zhuo, Chao Yi, Zhenliang Ma  

**Link**: [PDF](https://arxiv.org/pdf/2507.05283)  

**Abstract**: Pre-timed traffic signal control, commonly used for operating signalized intersections and coordinated arterials, requires tedious manual work for signaling plan creating and updating. When the time-of-day or day-of-week plans are utilized, one intersection is often associated with multiple plans, leading to further repetitive manual plan parameter inputting. To enable a user-friendly traffic signal control plan management process, this study proposes Chat2SPaT, a method to convert users' semi-structured and ambiguous descriptions on the signal control plan to exact signal phase and timing (SPaT) results, which could further be transformed into structured stage-based or ring-based plans to interact with intelligent transportation system (ITS) software and traffic signal controllers. With curated prompts, Chat2SPaT first leverages large language models' (LLMs) capability of understanding users' plan descriptions and reformulate the plan as a combination of phase sequence and phase attribute results in the json format. Based on LLM outputs, python scripts are designed to locate phases in a cycle, address nuances of traffic signal control, and finally assemble the complete traffic signal control plan. Within a chat, the pipeline can be utilized iteratively to conduct further plan editing. Experiments show that Chat2SPaT can generate plans with an accuracy of over 94% for both English and Chinese cases, using a test dataset with over 300 plan descriptions. As the first benchmark for evaluating LLMs' capability of understanding traffic signal control plan descriptions, Chat2SPaT provides an easy-to-use plan management pipeline for traffic practitioners and researchers, serving as a potential new building block for a more accurate and versatile application of LLMs in the field of ITS. The source codes, prompts and test dataset are openly accessible at this https URL. 

**Abstract (ZH)**: 基于聊天的信号相位和时序生成方法：Chat2SPaT在智能交通系统中的应用 

---
# Strongly Solving $7 \times 6$ Connect-Four on Consumer Grade Hardware 

**Title (ZH)**: 强效解决消费级硬件上的 $7 \times 6$ 连 seq 四问题 

**Authors**: Markus Böck  

**Link**: [PDF](https://arxiv.org/pdf/2507.05267)  

**Abstract**: While the game Connect-Four has been solved mathematically and the best move can be effectively computed with search based methods, a strong solution in the form of a look-up table was believed to be infeasible. In this paper, we revisit a symbolic search method based on binary decision diagrams to produce strong solutions. With our efficient implementation we were able to produce a 89.6 GB large look-up table in 47 hours on a single CPU core with 128 GB main memory for the standard $7 \times 6$ board size. In addition to this win-draw-loss evaluation, we include an alpha-beta search in our open source artifact to find the move which achieves the fastest win or slowest loss. 

**Abstract (ZH)**: 虽然连四游戏Connect-Four已被数学解决，最佳走法可以通过基于搜索的方法有效计算，但强形式的查找表解决方案被认为不可行。本文我们重新审视基于二进制决策图的符号搜索方法以生成强解决方案。通过我们的高效实现，我们在单个CPU核心（配备128 GB主存）上用47小时生成了一个89.6 GB大的查找表，适用于标准的7×6棋盘大小。除了胜平负评价外，我们还在开源 artifacts 中包括了alpha-beta搜索以找出最快获胜或最慢失败的走法。 

---
# Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving 

**Title (ZH)**: 基于跨域经验的代理知识库：实现有能动性的问题解决 

**Authors**: Xiangru Tang, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei, Peng Xia, Fang Wu, He Zhu, Ge Zhang, Jiaheng Liu, Xingyao Wang, Sirui Hong, Chenglin Wu, Hao Cheng, Chi Wang, Wangchunshu Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2507.06229)  

**Abstract**: As language agents tackle increasingly complex tasks, they struggle with effective error correction and experience reuse across domains. We introduce Agent KB, a hierarchical experience framework that enables complex agentic problem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses a core limitation: agents traditionally cannot learn from each other's experiences. By capturing both high-level strategies and detailed execution logs, Agent KB creates a shared knowledge base that enables cross-agent knowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success rates by up to 16.28 percentage points. On the most challenging tasks, Claude-3 improves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on intermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to improve from 41.33% to 53.33%. Our results suggest that Agent KB provides a modular, framework-agnostic infrastructure for enabling agents to learn from past experiences and generalize successful strategies to new tasks. 

**Abstract (ZH)**: 随着语言代理承担的任务日益复杂，它们在有效错误纠正和跨领域经验 reuse 方面遇到挑战。我们提出了一种名为 Agent KB 的分层经验框架，通过新颖的 Reason-Retrieve-Refine 管道实现复杂代理问题解决。Agent KB 解决了核心局限性：代理传统上无法从彼此的经验中学习。通过捕获高层策略和详细的执行日志，Agent KB 创建了一个共享的知识库，从而使跨代理的知识转移成为可能。在 GAIA 基准测试中，Agent KB 将成功率提高高达 16.28 个百分点。在最具挑战性的任务中，Claude-3 的成功率从 38.46% 提高到 57.69%，而 GPT-4 在中等难度任务中的成功率从 53.49% 提高到 73.26%。在 SWE-bench 代码修复任务中，Agent KB 使 Claude-3 的成功率从 41.33% 提高到 53.33%。我们的结果表明，Agent KB 提供了一种模块化、框架无关的基础结构，使代理能够从过去的经验中学习，并将成功的策略推广到新任务。 

---
# EC-Flow: Enabling Versatile Robotic Manipulation from Action-Unlabeled Videos via Embodiment-Centric Flow 

**Title (ZH)**: EC-Flow: 通过本体中心流从无动作标签视频中实现 versatile 机器人操作 

**Authors**: Yixiang Chen, Peiyan Li, Yan Huang, Jiabing Yang, Kehan Chen, Liang Wang  

**Link**: [PDF](https://arxiv.org/pdf/2507.06224)  

**Abstract**: Current language-guided robotic manipulation systems often require low-level action-labeled datasets for imitation learning. While object-centric flow prediction methods mitigate this issue, they remain limited to scenarios involving rigid objects with clear displacement and minimal occlusion. In this work, we present Embodiment-Centric Flow (EC-Flow), a framework that directly learns manipulation from action-unlabeled videos by predicting embodiment-centric flow. Our key insight is that incorporating the embodiment's inherent kinematics significantly enhances generalization to versatile manipulation scenarios, including deformable object handling, occlusions, and non-object-displacement tasks. To connect the EC-Flow with language instructions and object interactions, we further introduce a goal-alignment module by jointly optimizing movement consistency and goal-image prediction. Moreover, translating EC-Flow to executable robot actions only requires a standard robot URDF (Unified Robot Description Format) file to specify kinematic constraints across joints, which makes it easy to use in practice. We validate EC-Flow on both simulation (Meta-World) and real-world tasks, demonstrating its state-of-the-art performance in occluded object handling (62% improvement), deformable object manipulation (45% improvement), and non-object-displacement tasks (80% improvement) than prior state-of-the-art object-centric flow methods. For more information, see our project website at this https URL . 

**Abstract (ZH)**: 基于现象中心流的操纵学习：无需动作标签的直接学习方法 

---
# Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers 

**Title (ZH)**: 基于LLM的重排序器的计算量-效果重排序 

**Authors**: Zhiyuan Peng, Ting-ruen Wei, Tingyu Song, Yilun Zhao, Yi Fang  

**Link**: [PDF](https://arxiv.org/pdf/2507.06223)  

**Abstract**: Large Language Models (LLMs) have recently been applied to reranking tasks in information retrieval, achieving strong performance. However, their high computational demands often hinder practical deployment. Existing studies evaluate the efficiency of LLM-based rerankers using proxy metrics such as latency, the number of forward passes, input tokens, and output tokens. However, these metrics depend on hardware and running-time choices (\eg parallel or not, batch size, etc), and often fail to account for model size, making it difficult to interpret and obscuring the evaluation of the efficiency-effectiveness tradeoff. To address this issue, we propose E\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per PetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for hardware-agnostic throughput. Companied with the new metrics, an interpretable FLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even without running any experiments. Based on the proposed metrics, we conduct comprehensive experiments to evaluate a wide range of LLM-based rerankers with different architecture, studying the efficiency-effectiveness trade-off and bringing this issue to the attention of the research community. 

**Abstract (ZH)**: 基于大语言模型的重排序器:E²R-FLOPs评价框架，以每拍浮点运算的相关性排名 metric 和每拍浮点运算的查询吞吐量 metric 促进硬件无关的效率-效果权衡评估 

---
# Is Diversity All You Need for Scalable Robotic Manipulation? 

**Title (ZH)**: 可扩展机器人操作是否只需要多样性？ 

**Authors**: Modi Shi, Li Chen, Jin Chen, Yuxiang Lu, Chiming Liu, Guanghui Ren, Ping Luo, Di Huang, Maoqing Yao, Hongyang Li  

**Link**: [PDF](https://arxiv.org/pdf/2507.06219)  

**Abstract**: Data scaling has driven remarkable success in foundation models for Natural Language Processing (NLP) and Computer Vision (CV), yet the principles of effective data scaling in robotic manipulation remain insufficiently understood. In this work, we investigate the nuanced role of data diversity in robot learning by examining three critical dimensions-task (what to do), embodiment (which robot to use), and expert (who demonstrates)-challenging the conventional intuition of "more diverse is better". Throughout extensive experiments on various robot platforms, we reveal that (1) task diversity proves more critical than per-task demonstration quantity, benefiting transfer from diverse pre-training tasks to novel downstream scenarios; (2) multi-embodiment pre-training data is optional for cross-embodiment transfer-models trained on high-quality single-embodiment data can efficiently transfer to different platforms, showing more desirable scaling property during fine-tuning than multi-embodiment pre-trained models; and (3) expert diversity, arising from individual operational preferences and stochastic variations in human demonstrations, can be confounding to policy learning, with velocity multimodality emerging as a key contributing factor. Based on this insight, we propose a distribution debiasing method to mitigate velocity ambiguity, the yielding GO-1-Pro achieves substantial performance gains of 15%, equivalent to using 2.5 times pre-training data. Collectively, these findings provide new perspectives and offer practical guidance on how to scale robotic manipulation datasets effectively. 

**Abstract (ZH)**: 数据缩放推动了自然语言处理和计算机视觉领域基础模型的显著成功，但机器人操纵中的有效数据缩放原则仍不够充分理解。在这项工作中，我们通过探讨任务（做什么）、体态（使用哪种机器人）和专家（谁演示）这三个关键维度的微妙作用，挑战了“多样性越多越好”的常规直觉。通过在各种机器人平台上的广泛实验，我们揭示了以下几点：（1）任务多样性比每任务演示量更为关键，有助于从多样的预训练任务过渡到新的下游场景；（2）多体态预训练数据并非跨体态过渡的必需品——在高质量单体态数据上训练的模型可以高效地过渡到不同的平台，在微调过程中展现出比多体态预训练模型更有利的缩放特性；（3）源自个体操作偏好和人类演示中的随机变化的专家多样性可能会妨碍策略学习，速度多模态性成为关键影响因素之一。基于这些见解，我们提出了一种分布偏差校正方法来减轻速度不确定性，所提出的GO-1-Pro在性能上取得了显著提升，相当于使用了2.5倍的预训练数据。这些发现提供了新的视角，并为有效扩大机器人操纵数据集提供了实用指导。 

---
# Differential Mamba 

**Title (ZH)**: 差异型马姆巴 

**Authors**: Nadav Schneider, Itamar Zimerman, Eliya Nachmani  

**Link**: [PDF](https://arxiv.org/pdf/2507.06204)  

**Abstract**: Sequence models like Transformers and RNNs often overallocate attention to irrelevant context, leading to noisy intermediate representations. This degrades LLM capabilities by promoting hallucinations, weakening long-range and retrieval abilities, and reducing robustness. Recent work has shown that differential design can mitigate this issue in Transformers, improving their effectiveness across various applications. In this paper, we explore whether these techniques, originally developed for Transformers, can be applied to Mamba, a recent architecture based on selective state-space layers that achieves Transformer-level performance with greater efficiency. We show that a naive adaptation of differential design to Mamba is insufficient and requires careful architectural modifications. To address this, we introduce a novel differential mechanism for Mamba, empirically validated on language modeling benchmarks, demonstrating improved retrieval capabilities and superior performance over vanilla Mamba. Finally, we conduct extensive ablation studies and empirical analyses to justify our design choices and provide evidence that our approach effectively mitigates the overallocation problem in Mamba-based models. Our code is publicly available. 

**Abstract (ZH)**: 序列模型如Transformer和RNN经常过度分配注意力给无关的上下文，导致中间表示混乱。这会通过促进幻觉、削弱长程和检索能力以及降低鲁棒性来降低LLM的能力。最近的研究表明，差异设计可以在Transformer中减轻这一问题，并在各种应用中提高其有效性。在本文中，我们探索这些技术是否可以应用于Mamba，这是一种基于选择性状态空间层的近期架构，能够在更高效的情况下实现Transformer级别的性能。我们展示了朴素地将差异设计应用到Mamba是不够的，并且需要仔细的架构修改。为此，我们引入了一种新的Mamba差异机制，在语言建模基准上进行实证验证，展示了改进的检索能力和优于基础Mamba的性能。最后，我们进行了广泛的消融研究和实证分析，以证明我们的设计选择的有效性，并提供了证据表明我们的方法有效地缓解了基于Mamba模型的过度分配问题。我们的代码已公开。 

---
# UQLM: A Python Package for Uncertainty Quantification in Large Language Models 

**Title (ZH)**: UQLM：一种用于大型语言模型不确定性量化 的Python软件包 

**Authors**: Dylan Bouchard, Mohit Singh Chauhan, David Skarbrevik, Ho-Kyeong Ra, Viren Bajaj, Zeya Ahmad  

**Link**: [PDF](https://arxiv.org/pdf/2507.06196)  

**Abstract**: Hallucinations, defined as instances where Large Language Models (LLMs) generate false or misleading content, pose a significant challenge that impacts the safety and trust of downstream applications. We introduce UQLM, a Python package for LLM hallucination detection using state-of-the-art uncertainty quantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers that compute response-level confidence scores ranging from 0 to 1. This library provides an off-the-shelf solution for UQ-based hallucination detection that can be easily integrated to enhance the reliability of LLM outputs. 

**Abstract (ZH)**: 大型语言模型幻觉检测的不确定性量化工具包UQLM：基于最先进的不确定性量化技术识别幻觉 

---
# SQLBarber: A System Leveraging Large Language Models to Generate Customized and Realistic SQL Workloads 

**Title (ZH)**: SQLBarber：一种利用大规模语言模型生成个性化和真实SQL工作负载的系统 

**Authors**: Jiale Lao, Immanuel Trummer  

**Link**: [PDF](https://arxiv.org/pdf/2507.06192)  

**Abstract**: Database research and development often require a large number of SQL queries for benchmarking purposes. However, acquiring real-world SQL queries is challenging due to privacy concerns, and existing SQL generation methods are limited in customization and in satisfying realistic constraints. To address this issue, we present SQLBarber, a system based on Large Language Models (LLMs) to generate customized and realistic SQL workloads. SQLBarber (i) eliminates the need for users to manually craft SQL templates in advance, while providing the flexibility to accept natural language specifications to constrain SQL templates, (ii) scales efficiently to generate large volumes of queries matching any user-defined cost distribution (e.g., cardinality and execution plan cost), and (iii) uses execution statistics from Amazon Redshift and Snowflake to derive SQL template specifications and query cost distributions that reflect real-world query characteristics. SQLBarber introduces (i) a declarative interface for users to effortlessly generate customized SQL templates, (ii) an LLM-powered pipeline augmented with a self-correction module that profiles, refines, and prunes SQL templates based on query costs, and (iii) a Bayesian Optimizer to efficiently explore different predicate values and identify a set of queries that satisfy the target cost distribution. We construct and open-source ten benchmarks of varying difficulty levels and target query cost distributions based on real-world statistics from Snowflake and Amazon Redshift. Extensive experiments on these benchmarks show that SQLBarber is the only system that can generate customized SQL templates. It reduces query generation time by one to three orders of magnitude, and significantly improves alignment with the target cost distribution, compared with existing methods. 

**Abstract (ZH)**: 基于大型语言模型的SQLBarber系统：生成定制化和现实主义SQL工作负载 

---
# DS@GT at CheckThat! 2025: Detecting Subjectivity via Transfer-Learning and Corrective Data Augmentation 

**Title (ZH)**: DS@GT在CheckThat! 2025：基于迁移学习和纠正性数据扩增的主观性检测 

**Authors**: Maximilian Heil, Dionne Bang  

**Link**: [PDF](https://arxiv.org/pdf/2507.06189)  

**Abstract**: This paper presents our submission to Task 1, Subjectivity Detection, of the CheckThat! Lab at CLEF 2025. We investigate the effectiveness of transfer-learning and stylistic data augmentation to improve classification of subjective and objective sentences in English news text. Our approach contrasts fine-tuning of pre-trained encoders and transfer-learning of fine-tuned transformer on related tasks. We also introduce a controlled augmentation pipeline using GPT-4o to generate paraphrases in predefined subjectivity styles. To ensure label and style consistency, we employ the same model to correct and refine the generated samples. Results show that transfer-learning of specified encoders outperforms fine-tuning general-purpose ones, and that carefully curated augmentation significantly enhances model robustness, especially in detecting subjective content. Our official submission placed us $16^{th}$ of 24 participants. Overall, our findings underscore the value of combining encoder specialization with label-consistent augmentation for improved subjectivity detection. Our code is available at this https URL. 

**Abstract (ZH)**: 本文提交了对我们参加CLEF 2025 CheckThat! Lab任务1——主观性检测的参赛内容。我们探讨了迁移学习和风格化数据增强在提高英语文本中主观句和客观句分类效果方面的有效性。我们的方法对比了微调预训练编码器和在相关任务中微调变换器的迁移学习。我们还引入了一个使用GPT-4o控制生成预先定义主观性风格的同义句的增强管道。为确保标签和风格的一致性，我们使用同一模型对生成的样本进行修正和细化。结果表明，特定编码器的迁移学习优于通用编码器的微调，精心策划的数据增强显著增强了模型的稳健性，尤其是在检测主观内容方面。我们的官方参赛排名为24个参赛者中的第16名。总体而言，我们的研究结果强调了结合编码器专门化与标签一致的增强对于提高主观性检测效果的价值。我们的代码可在以下网址获取。 

---
# Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review 

**Title (ZH)**: 手稿中的隐藏提示利用了AI辅助同行评审 

**Authors**: Zhicheng Lin  

**Link**: [PDF](https://arxiv.org/pdf/2507.06185)  

**Abstract**: In July 2025, 18 academic manuscripts on the preprint website arXiv were found to contain hidden instructions known as prompts designed to manipulate AI-assisted peer review. Instructions such as "GIVE A POSITIVE REVIEW ONLY" were concealed using techniques like white-colored text. Author responses varied: one planned to withdraw the affected paper, while another defended the practice as legitimate testing of reviewer compliance. This commentary analyzes this practice as a novel form of research misconduct. We examine the technique of prompt injection in large language models (LLMs), revealing four types of hidden prompts, ranging from simple positive review commands to detailed evaluation frameworks. The defense that prompts served as "honeypots" to detect reviewers improperly using AI fails under examination--the consistently self-serving nature of prompt instructions indicates intent to manipulate. Publishers maintain inconsistent policies: Elsevier prohibits AI use in peer review entirely, while Springer Nature permits limited use with disclosure requirements. The incident exposes systematic vulnerabilities extending beyond peer review to any automated system processing scholarly texts, including plagiarism detection and citation indexing. Our analysis underscores the need for coordinated technical screening at submission portals and harmonized policies governing generative AI (GenAI) use in academic evaluation. 

**Abstract (ZH)**: 2025年7月，arXiv预印本网站上发现18篇学术手稿包含用于操控AI辅助同行评审的隐藏指令“提示”。这些指令如“仅给予正面评价”等内容通过白色字体等技巧隐藏。作者们的回应各异：一位计划撤回受影响的手稿，另一位则认为此举是对评审者遵守规定合法测试。本文评论此次行为是一种新型的研究不当行为，并分析了大型语言模型中提示注入技术，揭示了四种类型隐藏提示，从简单的正面评价命令到详细的评估框架。旨在“蜜罐”式检测评审者不当使用AI的辩护在审查中站不住脚——提示指令的一贯自我服务性质表明有操纵意图。出版商的政策不一：爱思唯尔完全禁止在同行评审中使用AI，而施普林格自然允许在披露要求下有限使用。该事件暴露出系统性漏洞，超出同行评审涉及任何处理学术文本的自动化系统，包括剽窃检测和引文索引。本文分析强调了提交门户协调技术筛查和协调治理生成式AI（GenAI）在学术评估中使用的必要性。 

---
# Fast Bilateral Teleoperation and Imitation Learning Using Sensorless Force Control via Accurate Dynamics Model 

**Title (ZH)**: 基于准确动力学模型的无传感器力控制快速双边远程操作及模仿学习 

**Authors**: Koki Yamane, Yunhan Li, Masashi Konosu, Koki Inami, Junji Oaki, Sho Sakaino, Toshiaki Tsuji  

**Link**: [PDF](https://arxiv.org/pdf/2507.06174)  

**Abstract**: In recent years, the advancement of imitation learning has led to increased interest in teleoperating low-cost manipulators to collect demonstration data. However, most existing systems rely on unilateral control, which only transmits target position values. While this approach is easy to implement and suitable for slow, non-contact tasks, it struggles with fast or contact-rich operations due to the absence of force feedback. This work demonstrates that fast teleoperation with force feedback is feasible even with force-sensorless, low-cost manipulators by leveraging 4-channel bilateral control. Based on accurately identified manipulator dynamics, our method integrates nonlinear terms compensation, velocity and external force estimation, and variable gain corresponding to inertial variation. Furthermore, using data collected by 4-channel bilateral control, we show that incorporating force information into both the input and output of learned policies improves performance in imitation learning. These results highlight the practical effectiveness of our system for high-fidelity teleoperation and data collection on affordable hardware. 

**Abstract (ZH)**: 近年来，模拟学习的进步激发了对利用低成本操作机构进行演示数据收集的远程操作技术的兴趣。然而，现有的大多数系统依赖于单向控制，仅传输目标位置值。尽管这种方法易于实现且适合缓慢的非接触任务，但在处理快速或接触密集的操作时会因缺乏力反馈而显得力不从心。本文展示了即使使用无传感器、低成本的操作机构，通过利用四通道双向控制，也能够实现带有力反馈的快速远程操作。基于精确识别的操作机构动态，该方法结合了非线性项补偿、速度和外部力估计以及与惯性变化相应的可变增益。此外，通过四通道双向控制收集的数据表明，在学习策略的输入和输出中融入力信息可以提高模拟学习的性能。这些结果突显了该系统在经济型硬件上实现高保真远程操作和数据收集的实际有效性。 

---
# A Method for Optimizing Connections in Differentiable Logic Gate Networks 

**Title (ZH)**: 不同iable逻辑门网络中连接优化的方法 

**Authors**: Wout Mommen, Lars Keuninckx, Matthias Hartmann, Piet Wambacq  

**Link**: [PDF](https://arxiv.org/pdf/2507.06173)  

**Abstract**: We introduce a novel method for partial optimization of the connections in Deep Differentiable Logic Gate Networks (LGNs). Our training method utilizes a probability distribution over a subset of connections per gate input, selecting the connection with highest merit, after which the gate-types are selected. We show that the connection-optimized LGNs outperform standard fixed-connection LGNs on the Yin-Yang, MNIST and Fashion-MNIST benchmarks, while requiring only a fraction of the number of logic gates. When training all connections, we demonstrate that 8000 simple logic gates are sufficient to achieve over 98% on the MNIST data set. Additionally, we show that our network has 24 times fewer gates, while performing better on the MNIST data set compared to standard fully connected LGNs. As such, our work shows a pathway towards fully trainable Boolean logic. 

**Abstract (ZH)**: 我们介绍了一种用于深度可微逻辑门网络（LGNs）部分优化连接的新方法。我们的训练方法利用每个门输入子集上的一种概率分布，选择具有最高价值的连接，之后再选择门类型。我们证明，在Yin-Yang、MNIST和Fashion-MNIST基准测试中，连接优化的LGNs在需要较少逻辑门数量的情况下优于标准固定连接的LGNs。在训练所有连接时，我们证明8000个简单的逻辑门就足以在MNIST数据集上达到超过98%的准确率。此外，我们还展示了与标准完全连接的LGNs相比，我们的网络在MNIST数据集上具有更优性能且逻辑门数量少24倍。因此，我们的工作表明了一条通向完全可训练布尔逻辑的途径。 

---
# Critical Nodes Identification in Complex Networks: A Survey 

**Title (ZH)**: 复杂网络中的关键节点识别：一个综述 

**Authors**: Duxin Chen, Jiawen Chen, Xiaoyu Zhang, Qinghan Jia, Xiaolu Liu, Ye Sun, Linyuan Lv, Wenwu Yu  

**Link**: [PDF](https://arxiv.org/pdf/2507.06164)  

**Abstract**: Complex networks have become essential tools for understanding diverse phenomena in social systems, traffic systems, biomolecular systems, and financial systems. Identifying critical nodes is a central theme in contemporary research, serving as a vital bridge between theoretical foundations and practical applications. Nevertheless, the intrinsic complexity and structural heterogeneity characterizing real-world networks, with particular emphasis on dynamic and higher-order networks, present substantial obstacles to the development of universal frameworks for critical node identification. This paper provides a comprehensive review of critical node identification techniques, categorizing them into seven main classes: centrality, critical nodes deletion problem, influence maximization, network control, artificial intelligence, higher-order and dynamic methods. Our review bridges the gaps in existing surveys by systematically classifying methods based on their methodological foundations and practical implications, and by highlighting their strengths, limitations, and applicability across different network types. Our work enhances the understanding of critical node research by identifying key challenges, such as algorithmic universality, real-time evaluation in dynamic networks, analysis of higher-order structures, and computational efficiency in large-scale networks. The structured synthesis consolidates current progress and highlights open questions, particularly in modeling temporal dynamics, advancing efficient algorithms, integrating machine learning approaches, and developing scalable and interpretable metrics for complex systems. 

**Abstract (ZH)**: 复杂网络已成为理解社会系统、交通系统、生物分子系统和金融系统等多种现象的重要工具。识别关键节点是当代研究中的一个核心主题，它是理论基础与实际应用之间的关键桥梁。然而，真实世界网络固有的复杂性和结构异质性，尤其是动态网络和高阶网络的特点，给关键节点识别的通用框架的开发带来了重大障碍。本文对关键节点识别技术进行了全面评审，将其分为七大类：中心性、关键节点删除问题、影响最大化、网络控制、人工智能、高阶和动态方法。我们的评审通过系统地根据方法论基础和实际应用对方法进行分类，填补了现有综述的空白，并强调了其在不同类型网络中的优势、局限性和适用性。我们的工作通过识别关键挑战，如算法的普遍性、动态网络中的实时评估、高阶结构的分析和大规模网络中的计算效率，增强了对关键节点研究的理解。结构化的综合总结了当前的进展，并突出了开放问题，特别是模型时间动态性、算法效率、机器学习方法的集成以及复杂系统中可扩展性和可解释性指标的发展。 

---
# Fast and Accurate Collision Probability Estimation for Autonomous Vehicles using Adaptive Sigma-Point Sampling 

**Title (ZH)**: 基于自适应sigma点采样的自主车辆快速准确碰撞概率估计 

**Authors**: Charles Champagne Cossette, Taylor Scott Clawson, Andrew Feit  

**Link**: [PDF](https://arxiv.org/pdf/2507.06149)  

**Abstract**: A novel algorithm is presented for the estimation of collision probabilities between dynamic objects with uncertain trajectories, where the trajectories are given as a sequence of poses with Gaussian distributions. We propose an adaptive sigma-point sampling scheme, which ultimately produces a fast, simple algorithm capable of estimating the collision probability with a median error of 3.5%, and a median runtime of 0.21ms, when measured on an Intel Xeon Gold 6226R Processor. Importantly, the algorithm explicitly accounts for the collision probability's temporal dependence, which is often neglected in prior work and otherwise leads to an overestimation of the collision probability. Finally, the method is tested on a diverse set of relevant real-world scenarios, consisting of 400 6-second snippets of autonomous vehicle logs, where the accuracy and latency is rigorously evaluated. 

**Abstract (ZH)**: 一种新的算法用于估计具有不确定轨迹的动力物体之间的碰撞概率，其中轨迹表示为具有高斯分布的姿态序列。我们提出了一种自适应sigma点抽样方案，最终产生一个快速、简单的算法，能够在Intel Xeon Gold 6226R处理器上以中位数误差3.5%和中位数运行时间0.21ms的速度估计碰撞概率。该算法显式地考虑了碰撞概率的时间依赖性，这在先前的工作中经常被忽略，否则会导致碰撞概率的高估。最后，该方法在400个包含自主车辆日志的6秒片段的多样化实际场景中进行了测试，其中准确性和延迟得到了严格的评估。 

---
# SoftReMish: A Novel Activation Function for Enhanced Convolutional Neural Networks for Visual Recognition Performance 

**Title (ZH)**: SoftReMish: 一种增强视觉识别性能的新型激活函数 

**Authors**: Mustafa Bayram Gücen  

**Link**: [PDF](https://arxiv.org/pdf/2507.06148)  

**Abstract**: In this study, SoftReMish, a new activation function designed to improve the performance of convolutional neural networks (CNNs) in image classification tasks, is proposed. Using the MNIST dataset, a standard CNN architecture consisting of two convolutional layers, max pooling, and fully connected layers was implemented. SoftReMish was evaluated against popular activation functions including ReLU, Tanh, and Mish by replacing the activation function in all trainable layers. The model performance was assessed in terms of minimum training loss and maximum validation accuracy. Results showed that SoftReMish achieved a minimum loss (3.14e-8) and a validation accuracy (99.41%), outperforming all other functions tested. These findings demonstrate that SoftReMish offers better convergence behavior and generalization capability, making it a promising candidate for visual recognition tasks. 

**Abstract (ZH)**: SoftReMish：一种用于图像分类任务的新型激活函数 

---
# LangMamba: A Language-driven Mamba Framework for Low-dose CT Denoising with Vision-language Models 

**Title (ZH)**: LangMamba：一种基于语言驱动的Mamba框架用于低剂量CT降噪的视觉语言模型 

**Authors**: Zhihao Chen, Tao Chen, Chenhui Wang, Qi Gao, Huidong Xie, Chuang Niu, Ge Wang, Hongming Shan  

**Link**: [PDF](https://arxiv.org/pdf/2507.06140)  

**Abstract**: Low-dose computed tomography (LDCT) reduces radiation exposure but often degrades image quality, potentially compromising diagnostic accuracy. Existing deep learning-based denoising methods focus primarily on pixel-level mappings, overlooking the potential benefits of high-level semantic guidance. Recent advances in vision-language models (VLMs) suggest that language can serve as a powerful tool for capturing structured semantic information, offering new opportunities to improve LDCT reconstruction. In this paper, we introduce LangMamba, a Language-driven Mamba framework for LDCT denoising that leverages VLM-derived representations to enhance supervision from normal-dose CT (NDCT). LangMamba follows a two-stage learning strategy. First, we pre-train a Language-guided AutoEncoder (LangAE) that leverages frozen VLMs to map NDCT images into a semantic space enriched with anatomical information. Second, we synergize LangAE with two key components to guide LDCT denoising: Semantic-Enhanced Efficient Denoiser (SEED), which enhances NDCT-relevant local semantic while capturing global features with efficient Mamba mechanism, and Language-engaged Dual-space Alignment (LangDA) Loss, which ensures that denoised images align with NDCT in both perceptual and semantic spaces. Extensive experiments on two public datasets demonstrate that LangMamba outperforms conventional state-of-the-art methods, significantly improving detail preservation and visual fidelity. Remarkably, LangAE exhibits strong generalizability to unseen datasets, thereby reducing training costs. Furthermore, LangDA loss improves explainability by integrating language-guided insights into image reconstruction and offers a plug-and-play fashion. Our findings shed new light on the potential of language as a supervisory signal to advance LDCT denoising. The code is publicly available on this https URL. 

**Abstract (ZH)**: 基于语言驱动的Mamba框架低剂量计算机断层扫描降噪方法 

---
# Topic Modeling and Link-Prediction for Material Property Discovery 

**Title (ZH)**: 材料性质发现的主题建模与链接预测 

**Authors**: Ryan C. Barron, Maksim E. Eren, Valentin Stanev, Cynthia Matuszek, Boian S. Alexandrov  

**Link**: [PDF](https://arxiv.org/pdf/2507.06139)  

**Abstract**: Link prediction infers missing or future relations between graph nodes, based on connection patterns. Scientific literature networks and knowledge graphs are typically large, sparse, and noisy, and often contain missing links between entities. We present an AI-driven hierarchical link prediction framework that integrates matrix factorization to infer hidden associations and steer discovery in complex material domains. Our method combines Hierarchical Nonnegative Matrix Factorization (HNMFk) and Boolean matrix factorization (BNMFk) with automatic model selection, as well as Logistic matrix factorization (LMF), we use to construct a three-level topic tree from a 46,862-document corpus focused on 73 transition-metal dichalcogenides (TMDs). These materials are studied in a variety of physics fields with many current and potential applications.
An ensemble BNMFk + LMF approach fuses discrete interpretability with probabilistic scoring. The resulting HNMFk clusters map each material onto coherent topics like superconductivity, energy storage, and tribology. Also, missing or weakly connected links are highlight between topics and materials, suggesting novel hypotheses for cross-disciplinary exploration. We validate our method by removing publications about superconductivity in well-known superconductors, and show the model predicts associations with the superconducting TMD clusters. This shows the method finds hidden connections in a graph of material to latent topic associations built from scientific literature, especially useful when examining a diverse corpus of scientific documents covering the same class of phenomena or materials but originating from distinct communities and perspectives. The inferred links generating new hypotheses, produced by our method, are exposed through an interactive Streamlit dashboard, designed for human-in-the-loop scientific discovery. 

**Abstract (ZH)**: 基于矩阵分解的分层链接预测框架：融合层级非负矩阵分解和布尔矩阵分解进行复杂材料领域的隐含关联推断 

---
# Coding Triangle: How Does Large Language Model Understand Code? 

**Title (ZH)**: 编码三角形：大型语言模型如何理解代码？ 

**Authors**: Taolin Zhang, Zihan Ma, Maosong Cao, Junnan Liu, Songyang Zhang, Kai Chen  

**Link**: [PDF](https://arxiv.org/pdf/2507.06138)  

**Abstract**: Large language models (LLMs) have achieved remarkable progress in code generation, yet their true programming competence remains underexplored. We introduce the Code Triangle framework, which systematically evaluates LLMs across three fundamental dimensions: editorial analysis, code implementation, and test case generation. Through extensive experiments on competitive programming benchmarks, we reveal that while LLMs can form a self-consistent system across these dimensions, their solutions often lack the diversity and robustness of human programmers. We identify a significant distribution shift between model cognition and human expertise, with model errors tending to cluster due to training data biases and limited reasoning transfer. Our study demonstrates that incorporating human-generated editorials, solutions, and diverse test cases, as well as leveraging model mixtures, can substantially enhance both the performance and robustness of LLMs. Furthermore, we reveal both the consistency and inconsistency in the cognition of LLMs that may facilitate self-reflection and self-improvement, providing a potential direction for developing more powerful coding models. 

**Abstract (ZH)**: 大型语言模型（LLMs）在代码生成方面取得了显著进步，但其真正的编程能力仍待充分探索。我们引入了代码三角框架，该框架系统地从编辑分析、代码实现和测试用例生成三个基本维度评估LLMs。通过对竞争性编程基准的广泛实验，我们揭示了虽然LLMs在这些维度之间能够形成一个自洽的系统，但其解决方案往往缺乏人类程序员的多样性和鲁棒性。我们发现模型认知与人类专长之间存在显著的分布差异，模型错误倾向于因训练数据偏差和有限的推理迁移而集群。我们的研究显示， Incorporating 人类生成的编辑、解决方案和多样化的测试用例，以及利用模型混合，可以显著提升LLMs的性能和鲁棒性。此外，我们揭示了LLMs认知的一致性与不一致性，这可能促进自我反思和自我改进，提供了一个发展更强大编码模型的潜在方向。 

---
# NeoBabel: A Multilingual Open Tower for Visual Generation 

**Title (ZH)**: NeoBabel: 多语言开放塔结构的视觉生成模型 

**Authors**: Mohammad Mahdi Derakhshani, Dheeraj Varghese, Marzieh Fadaee, Cees G. M. Snoek  

**Link**: [PDF](https://arxiv.org/pdf/2507.06137)  

**Abstract**: Text-to-image generation advancements have been predominantly English-centric, creating barriers for non-English speakers and perpetuating digital inequities. While existing systems rely on translation pipelines, these introduce semantic drift, computational overhead, and cultural misalignment. We introduce NeoBabel, a novel multilingual image generation framework that sets a new Pareto frontier in performance, efficiency and inclusivity, supporting six languages: English, Chinese, Dutch, French, Hindi, and Persian. The model is trained using a combination of large-scale multilingual pretraining and high-resolution instruction tuning. To evaluate its capabilities, we expand two English-only benchmarks to multilingual equivalents: m-GenEval and m-DPG. NeoBabel achieves state-of-the-art multilingual performance while retaining strong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG. Notably, it performs on par with leading models on English tasks while outperforming them by +0.11 and +0.09 on multilingual benchmarks, even though these models are built on multilingual base LLMs. This demonstrates the effectiveness of our targeted alignment training for preserving and extending crosslingual generalization. We further introduce two new metrics to rigorously assess multilingual alignment and robustness to code-mixed prompts. Notably, NeoBabel matches or exceeds English-only models while being 2-4x smaller. We release an open toolkit, including all code, model checkpoints, a curated dataset of 124M multilingual text-image pairs, and standardized multilingual evaluation protocols, to advance inclusive AI research. Our work demonstrates that multilingual capability is not a trade-off but a catalyst for improved robustness, efficiency, and cultural fidelity in generative AI. 

**Abstract (ZH)**: 多语言图像生成框架NeoBabel：性能、效率和包容性的新前沿 

---
# PrefixAgent: An LLM-Powered Design Framework for Efficient Prefix Adder Optimization 

**Title (ZH)**: PrefixAgent：一种基于LLM的高效前缀加法器优化设计框架 

**Authors**: Dongsheng Zuo, Jiadong Zhu, Yang Luo, Yuzhe Ma  

**Link**: [PDF](https://arxiv.org/pdf/2507.06127)  

**Abstract**: Prefix adders are fundamental arithmetic circuits, but their design space grows exponentially with bit-width, posing significant optimization challenges. Previous works face limitations in performance, generalization, and scalability. To address these challenges, we propose PrefixAgent, a large language model (LLM)-powered framework that enables efficient prefix adder optimization. Specifically, PrefixAgent reformulates the problem into subtasks including backbone synthesis and structure refinement, which effectively reduces the search space. More importantly, this new design perspective enables us to efficiently collect enormous high-quality data and reasoning traces with E-graph, which further results in an effective fine-tuning of LLM. Experimental results show that PrefixAgent synthesizes prefix adders with consistently smaller areas compared to baseline methods, while maintaining scalability and generalization in commercial EDA flows. 

**Abstract (ZH)**: 前缀加法器是基础的算术电路，但由于位宽的增加，其设计空间呈指数增长，提出了显著的优化挑战。现有工作在性能、泛化能力和可扩展性方面存在局限性。为应对这些挑战，我们提出了一种大型语言模型（LLM）驱动的框架——PrefixAgent，能够有效地优化前缀加法器。具体而言，PrefixAgent 将问题重新表述为包括主干综合和结构细化的子任务，从而有效缩小搜索空间。更重要的是，这种新的设计视角使我们能够使用 E-图高效地收集大量高质量的数据和推理轨迹，进一步实现对 LLM 的有效微调。实验证明，PrefixAgent 合成了与基准方法相比具有更小面积的前缀加法器，同时在商业 EDA 流程中保持了可扩展性和泛化能力。 

---
# Subspace-based Approximate Hessian Method for Zeroth-Order Optimization 

**Title (ZH)**: 基于子空间的近似海森矩阵零阶优化方法 

**Authors**: Dongyoon Kim, Sungjae Lee, Wonjin Lee, Kwang In Kim  

**Link**: [PDF](https://arxiv.org/pdf/2507.06125)  

**Abstract**: Zeroth-order optimization addresses problems where gradient information is inaccessible or impractical to compute. While most existing methods rely on first-order approximations, incorporating second-order (curvature) information can, in principle, significantly accelerate convergence. However, the high cost of function evaluations required to estimate Hessian matrices often limits practical applicability. We present the subspace-based approximate Hessian (ZO-SAH) method, a zeroth-order optimization algorithm that mitigates these costs by focusing on randomly selected two-dimensional subspaces. Within each subspace, ZO-SAH estimates the Hessian by fitting a quadratic polynomial to the objective function and extracting its second-order coefficients. To further reduce function-query costs, ZO-SAH employs a periodic subspace-switching strategy that reuses function evaluations across optimization steps. Experiments on eight benchmark datasets, including logistic regression and deep neural network training tasks, demonstrate that ZO-SAH achieves significantly faster convergence than existing zeroth-order methods. 

**Abstract (ZH)**: 零阶优化解决无法获取或计算梯度信息的问题。虽然大多数现有方法依赖于一阶近似，但整合二阶（曲率）信息原则上可以显著加速收敛。然而，用于估计海森矩阵所需的函数评估成本常常限制其实用性。我们提出了基于子空间的近似海森矩阵（ZO-SAH）方法，该方法通过集中关注随机选择的二维子空间来缓解这些成本。在每个子空间内，ZO-SAH通过拟合二次多项式来估计海森矩阵，并提取其二阶系数。为了进一步减少函数查询成本，ZO-SAH采用周期性的子空间切换策略，在优化步骤之间重用函数评估。在包括逻辑回归和深度神经网络训练任务在内的八个基准数据集上的实验表明，ZO-SAH比现有零阶方法实现了显著更快的收敛速度。 

---
# Speech Quality Assessment Model Based on Mixture of Experts: System-Level Performance Enhancement and Utterance-Level Challenge Analysis 

**Title (ZH)**: 基于专家混合的语音质量评估模型：系统级性能提升与句级挑战分析 

**Authors**: Xintong Hu, Yixuan Chen, Rui Yang, Wenxiang Guo, Changhao Pan  

**Link**: [PDF](https://arxiv.org/pdf/2507.06116)  

**Abstract**: Automatic speech quality assessment plays a crucial role in the development of speech synthesis systems, but existing models exhibit significant performance variations across different granularity levels of prediction tasks. This paper proposes an enhanced MOS prediction system based on self-supervised learning speech models, incorporating a Mixture of Experts (MoE) classification head and utilizing synthetic data from multiple commercial generation models for data augmentation. Our method builds upon existing self-supervised models such as wav2vec2, designing a specialized MoE architecture to address different types of speech quality assessment tasks. We also collected a large-scale synthetic speech dataset encompassing the latest text-to-speech, speech conversion, and speech enhancement systems. However, despite the adoption of the MoE architecture and expanded dataset, the model's performance improvements in sentence-level prediction tasks remain limited. Our work reveals the limitations of current methods in handling sentence-level quality assessment, provides new technical pathways for the field of automatic speech quality assessment, and also delves into the fundamental causes of performance differences across different assessment granularities. 

**Abstract (ZH)**: 基于自监督学习的混合专家系统在语音质量自动评估中的应用：合成数据增强与发音质量预测 

---
# LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for Panorama-Style Mobile Captures 

**Title (ZH)**: LighthouseGS: 基于室内结构的全景风格移动捕获3D高斯斑点渲染 

**Authors**: Seungoh Han, Jaehoon Jang, Hyunsu Kim, Jaeheung Surh, Junhyung Kwak, Hyowon Ha, Kyungdon Joo  

**Link**: [PDF](https://arxiv.org/pdf/2507.06109)  

**Abstract**: Recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time novel view synthesis (NVS) with impressive quality in indoor scenes. However, achieving high-fidelity rendering requires meticulously captured images covering the entire scene, limiting accessibility for general users. We aim to develop a practical 3DGS-based NVS framework using simple panorama-style motion with a handheld camera (e.g., mobile device). While convenient, this rotation-dominant motion and narrow baseline make accurate camera pose and 3D point estimation challenging, especially in textureless indoor scenes. To address these challenges, we propose LighthouseGS, a novel framework inspired by the lighthouse-like sweeping motion of panoramic views. LighthouseGS leverages rough geometric priors, such as mobile device camera poses and monocular depth estimation, and utilizes the planar structures often found in indoor environments. We present a new initialization method called plane scaffold assembly to generate consistent 3D points on these structures, followed by a stable pruning strategy to enhance geometry and optimization stability. Additionally, we introduce geometric and photometric corrections to resolve inconsistencies from motion drift and auto-exposure in mobile devices. Tested on collected real and synthetic indoor scenes, LighthouseGS delivers photorealistic rendering, surpassing state-of-the-art methods and demonstrating the potential for panoramic view synthesis and object placement. 

**Abstract (ZH)**: Recent Advances in 3D Gaussian Splatting for Novel View Synthesis Using Simple Panorama-Style Motion with a Handheld Camera 

---
# Taming Data Challenges in ML-based Security Tasks: Lessons from Integrating Generative AI 

**Title (ZH)**: 基于生成式AI整合的lessons，管理ML安全任务中的数据挑战 

**Authors**: Shravya Kanchi, Neal Mangaokar, Aravind Cheruvu, Sifat Muhammad Abdullah, Shirin Nilizadeh, Atul Prakash, Bimal Viswanath  

**Link**: [PDF](https://arxiv.org/pdf/2507.06092)  

**Abstract**: Machine learning-based supervised classifiers are widely used for security tasks, and their improvement has been largely focused on algorithmic advancements. We argue that data challenges that negatively impact the performance of these classifiers have received limited attention. We address the following research question: Can developments in Generative AI (GenAI) address these data challenges and improve classifier performance? We propose augmenting training datasets with synthetic data generated using GenAI techniques to improve classifier generalization. We evaluate this approach across 7 diverse security tasks using 6 state-of-the-art GenAI methods and introduce a novel GenAI scheme called Nimai that enables highly controlled data synthesis. We find that GenAI techniques can significantly improve the performance of security classifiers, achieving improvements of up to 32.6% even in severely data-constrained settings (only ~180 training samples). Furthermore, we demonstrate that GenAI can facilitate rapid adaptation to concept drift post-deployment, requiring minimal labeling in the adjustment process. Despite successes, our study finds that some GenAI schemes struggle to initialize (train and produce data) on certain security tasks. We also identify characteristics of specific tasks, such as noisy labels, overlapping class distributions, and sparse feature vectors, which hinder performance boost using GenAI. We believe that our study will drive the development of future GenAI tools designed for security tasks. 

**Abstract (ZH)**: 基于生成式AI的方法能否解决安全领域分类器的数据挑战并提高其性能？ 

---
# QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models 

**Title (ZH)**: QS4D: 量化感知训练以实现结构化状态空间序列模型的高效硬件部署 

**Authors**: Sebastian Siegel, Ming-Jay Yang, Younes Bouhadjar, Maxime Fabre, Emre Neftci, John Paul Strachan  

**Link**: [PDF](https://arxiv.org/pdf/2507.06079)  

**Abstract**: Structured State Space models (SSM) have recently emerged as a new class of deep learning models, particularly well-suited for processing long sequences. Their constant memory footprint, in contrast to the linearly scaling memory demands of Transformers, makes them attractive candidates for deployment on resource-constrained edge-computing devices. While recent works have explored the effect of quantization-aware training (QAT) on SSMs, they typically do not address its implications for specialized edge hardware, for example, analog in-memory computing (AIMC) chips. In this work, we demonstrate that QAT can significantly reduce the complexity of SSMs by up to two orders of magnitude across various performance metrics. We analyze the relation between model size and numerical precision, and show that QAT enhances robustness to analog noise and enables structural pruning. Finally, we integrate these techniques to deploy SSMs on a memristive analog in-memory computing substrate and highlight the resulting benefits in terms of computational efficiency. 

**Abstract (ZH)**: 结构化状态空间模型（SSM）作为一种新的深度学习模型，近来受到关注，特别适用于处理长序列。与Transformer线性增长的内存需求相比，SSM具有恒定的内存占用，使其成为资源受限的边缘计算设备的理想候选者。尽管近期研究探讨了量化感知训练（QAT）对SSM的影响，但它们通常未考虑到其对专用边缘硬件，如模拟内存计算（AIMC）芯片的影响。在本工作中，我们展示了QAT可以显著降低SSM在各种性能指标下的复杂度，最多可减少两个数量级。我们分析了模型大小与数值精度的关系，并表明QAT增强了对模拟噪声的鲁棒性并使结构化剪枝成为可能。最后，我们将这些技术集成起来，在一种基于膜电阻的模拟内存计算平台上部署SSM，并突出了由此带来的计算效率改进。 

---
# Contrastive and Transfer Learning for Effective Audio Fingerprinting through a Real-World Evaluation Protocol 

**Title (ZH)**: 基于实际评估协议的对比与迁移学习在有效音频指纹识别中的应用 

**Authors**: Christos Nikou, Theodoros Giannakopoulos  

**Link**: [PDF](https://arxiv.org/pdf/2507.06070)  

**Abstract**: Recent advances in song identification leverage deep neural networks to learn compact audio fingerprints directly from raw waveforms. While these methods perform well under controlled conditions, their accuracy drops significantly in real-world scenarios where the audio is captured via mobile devices in noisy environments. In this paper, we introduce a novel evaluation protocol designed to better reflect such real-world conditions. We generate three recordings of the same audio, each with increasing levels of noise, captured using a mobile device's microphone. Our results reveal a substantial performance drop for two state-of-the-art CNN-based models under this protocol, compared to previously reported benchmarks. Additionally, we highlight the critical role of the augmentation pipeline during training with contrastive loss. By introduction low pass and high pass filters in the augmentation pipeline we significantly increase the performance of both systems in our proposed evaluation. Furthermore, we develop a transformer-based model with a tailored projection module and demonstrate that transferring knowledge from a semantically relevant domain yields a more robust solution. The transformer architecture outperforms CNN-based models across all noise levels, and query durations. In low noise conditions it achieves 47.99% for 1-sec queries, and 97% for 10-sec queries in finding the correct song, surpassing by 14%, and by 18.5% the second-best performing model, respectively, Under heavy noise levels, we achieve a detection rate 56.5% for 15-second query duration. All experiments are conducted on public large-scale dataset of over 100K songs, with queries matched against a database of 56 million vectors. 

**Abstract (ZH)**: Recent Advances in Song Identification Under Real-World Noise Conditions: An Evaluation Protocol and Model Improvements 

---
# Enhancing Synthetic CT from CBCT via Multimodal Fusion and End-To-End Registration 

**Title (ZH)**: 通过多模态融合和端到端配准增强CBCT合成CT 

**Authors**: Maximilian Tschuchnig, Lukas Lamminger, Philipp Steininger, Michael Gadermayr  

**Link**: [PDF](https://arxiv.org/pdf/2507.06067)  

**Abstract**: Cone-Beam Computed Tomography (CBCT) is widely used for intraoperative imaging due to its rapid acquisition and low radiation dose. However, CBCT images typically suffer from artifacts and lower visual quality compared to conventional Computed Tomography (CT). A promising solution is synthetic CT (sCT) generation, where CBCT volumes are translated into the CT domain. In this work, we enhance sCT generation through multimodal learning by jointly leveraging intraoperative CBCT and preoperative CT data. To overcome the inherent misalignment between modalities, we introduce an end-to-end learnable registration module within the sCT pipeline. This model is evaluated on a controlled synthetic dataset, allowing precise manipulation of data quality and alignment parameters. Further, we validate its robustness and generalizability on two real-world clinical datasets. Experimental results demonstrate that integrating registration in multimodal sCT generation improves sCT quality, outperforming baseline multimodal methods in 79 out of 90 evaluation settings. Notably, the improvement is most significant in cases where CBCT quality is low and the preoperative CT is moderately misaligned. 

**Abstract (ZH)**: 基于锥束计算机断层成像的多模态合成CT生成研究 

---
# VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis 

**Title (ZH)**: 视觉引导的3Davatar唇部合成 

**Authors**: Alexandre Symeonidis-Herzig, Özge Mercanoğlu Sincan, Richard Bowden  

**Link**: [PDF](https://arxiv.org/pdf/2507.06060)  

**Abstract**: Realistic, high-fidelity 3D facial animations are crucial for expressive avatar systems in human-computer interaction and accessibility. Although prior methods show promising quality, their reliance on the mesh domain limits their ability to fully leverage the rapid visual innovations seen in 2D computer vision and graphics. We propose VisualSpeaker, a novel method that bridges this gap using photorealistic differentiable rendering, supervised by visual speech recognition, for improved 3D facial animation. Our contribution is a perceptual lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting avatar renders through a pre-trained Visual Automatic Speech Recognition model during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker improves both the standard Lip Vertex Error metric by 56.1% and the perceptual quality of the generated animations, while retaining the controllability of mesh-driven animation. This perceptual focus naturally supports accurate mouthings, essential cues that disambiguate similar manual signs in sign language avatars. 

**Abstract (ZH)**: 真实的高保真3D面部动画对于人类计算机交互和无障碍环境中的表情avatar系统至关重要。尽管先前的方法表现出色，但它们依赖于网格域的限制使其难以充分利用2D计算机视觉和图形领域中快速涌现的视觉创新。我们提出了一种名为VisualSpeaker的新方法，该方法通过使用基于照片真实差分渲染并由视觉语音识别监督，从而填补了这一空白。我们的贡献是一种感知唇读损失，该损失是在训练过程中通过预训练的视觉自动语音识别模型传递逼真3D Gaussian Splatting avatar渲染图像而推导出来的。在MEAD数据集上的评估表明，VisualSpeaker既提高了标准的唇部顶点误差度量56.1%，又提高了生成动画的感知质量，同时保持了网格驱动动画的可控性。这种感知重点自然支持准确的唇型，这对于使手势语avatar在类似的手势中具有明确性至关重要。 

---
# Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs 

**Title (ZH)**: 熵记忆律：评估LLM中数据的记忆难度 

**Authors**: Yizhan Huang, Zhe Yang, Meifang Chen, Jianping Zhang, Michael R. Lyu  

**Link**: [PDF](https://arxiv.org/pdf/2507.06056)  

**Abstract**: Large Language Models (LLMs) are known to memorize portions of their training data, sometimes reproducing content verbatim when prompted appropriately. In this work, we investigate a fundamental yet under-explored question in the domain of memorization: How to characterize memorization difficulty of training data in LLMs? Through empirical experiments on OLMo, a family of open models, we present the Entropy-Memorization Law. It suggests that data entropy is linearly correlated with memorization score. Moreover, in a case study of memorizing highly randomized strings, or "gibberish", we observe that such sequences, despite their apparent randomness, exhibit unexpectedly low empirical entropy compared to the broader training corpus. Adopting the same strategy to discover Entropy-Memorization Law, we derive a simple yet effective approach to distinguish training and testing data, enabling Dataset Inference (DI). 

**Abstract (ZH)**: 大型语言模型（LLMs）known to memorize portions of their training data, sometimes reproducing content verbatim when prompted appropriately. In this work, we investigate a fundamental yet under-explored question in the domain of memorization: How to characterize memorization difficulty of training data in LLMs? Through empirical experiments on OLMo, a family of open models, we present the Entropy-Memorization Law. It suggests that data entropy is linearly correlated with memorization score. Moreover, in a case study of memorizing highly randomized strings, or "gibberish", we observe that such sequences, despite their apparent randomness, exhibit unexpectedly low empirical entropy compared to the broader training corpus. Adopting the same strategy to discover Entropy-Memorization Law, we derive a simple yet effective approach to distinguish training and testing data, enabling Dataset Inference (DI). 

---
# CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations 

**Title (ZH)**: CAVGAN：通过生成对抗攻击其内部表示以统一LLM的脱戒和防护 

**Authors**: Xiaohu Li, Yunfeng Ning, Zepeng Bao, Mayi Xu, Jianhao Chen, Tieyun Qian  

**Link**: [PDF](https://arxiv.org/pdf/2507.06043)  

**Abstract**: Security alignment enables the Large Language Model (LLM) to gain the protection against malicious queries, but various jailbreak attack methods reveal the vulnerability of this security mechanism. Previous studies have isolated LLM jailbreak attacks and defenses. We analyze the security protection mechanism of the LLM, and propose a framework that combines attack and defense. Our method is based on the linearly separable property of LLM intermediate layer embedding, as well as the essence of jailbreak attack, which aims to embed harmful problems and transfer them to the safe area. We utilize generative adversarial network (GAN) to learn the security judgment boundary inside the LLM to achieve efficient jailbreak attack and defense. The experimental results indicate that our method achieves an average jailbreak success rate of 88.85\% across three popular LLMs, while the defense success rate on the state-of-the-art jailbreak dataset reaches an average of 84.17\%. This not only validates the effectiveness of our approach but also sheds light on the internal security mechanisms of LLMs, offering new insights for enhancing model security The code and data are available at this https URL. 

**Abstract (ZH)**: 安全对齐使大规模语言模型（LLM）获得了对抗恶意查询的保护，但各种囚笼突破攻击方法揭示了这种安全机制的脆弱性。我们分析了LLM的安全保护机制，并提出了一种结合攻击与防御的框架。我们的方法基于LLM中间层嵌入的线性可分性质，以及囚笼突破攻击的本质，旨在将有害问题嵌入并转移至安全区域。我们利用生成对抗网络（GAN）学习LLM内部的安全判断边界，以实现有效的囚笼突破攻击与防御。实验结果显示，我们的方法在三个流行的LLM上实现了平均88.85%的囚笼突破成功率，而针对最先进的囚笼突破数据集的防御成功率平均为84.17%。这不仅验证了我们方法的有效性，还揭示了LLM的内部安全机制，为增强模型安全提供了新的见解。代码和数据可在以下链接获取：this https URL。 

---
# TextPixs: Glyph-Conditioned Diffusion with Character-Aware Attention and OCR-Guided Supervision 

**Title (ZH)**: TextPixs：基于字符aware注意力和OCR指导监督的Glyph条件扩散模型 

**Authors**: Syeda Anshrah Gillani, Mirza Samad Ahmed Baig, Osama Ahmed Khan, Shahid Munir Shah, Umema Mujeeb, Maheen Ali  

**Link**: [PDF](https://arxiv.org/pdf/2507.06033)  

**Abstract**: The modern text-to-image diffusion models boom has opened a new era in digital content production as it has proven the previously unseen ability to produce photorealistic and stylistically diverse imagery based on the semantics of natural-language descriptions. However, the consistent disadvantage of these models is that they cannot generate readable, meaningful, and correctly spelled text in generated images, which significantly limits the use of practical purposes like advertising, learning, and creative design. This paper introduces a new framework, namely Glyph-Conditioned Diffusion with Character-Aware Attention (GCDA), using which a typical diffusion backbone is extended by three well-designed modules. To begin with, the model has a dual-stream text encoder that encodes both semantic contextual information and explicit glyph representations, resulting in a character-aware representation of the input text that is rich in nature. Second, an attention mechanism that is aware of the character is proposed with a new attention segregation loss that aims to limit the attention distribution of each character independently in order to avoid distortion artifacts. Lastly, GCDA has an OCR-in-the-loop fine-tuning phase, where a full text perceptual loss, directly optimises models to be legible and accurately spell. Large scale experiments to benchmark datasets, such as MARIO-10M and T2I-CompBench, reveal that GCDA sets a new state-of-the-art on all metrics, with better character based metrics on text rendering (Character Error Rate: 0.08 vs 0.21 for the previous best; Word Error Rate: 0.15 vs 0.25), human perception, and comparable image synthesis quality on high-fidelity (FID: 14.3). 

**Abstract (ZH)**: 面向字符的注意力条件扩散模型：一种新的字符感知扩散框架（GCDA） 

---
# Efficient Federated Learning with Timely Update Dissemination 

**Title (ZH)**: 高效的联邦学习：及时更新传播 

**Authors**: Juncheng Jia, Ji Liu, Chao Huo, Yihui Shen, Yang Zhou, Huaiyu Dai, Dejing Dou  

**Link**: [PDF](https://arxiv.org/pdf/2507.06031)  

**Abstract**: Federated Learning (FL) has emerged as a compelling methodology for the management of distributed data, marked by significant advancements in recent years. In this paper, we propose an efficient FL approach that capitalizes on additional downlink bandwidth resources to ensure timely update dissemination. Initially, we implement this strategy within an asynchronous framework, introducing the Asynchronous Staleness-aware Model Update (FedASMU), which integrates both server-side and device-side methodologies. On the server side, we present an asynchronous FL system model that employs a dynamic model aggregation technique, which harmonizes local model updates with the global model to enhance both accuracy and efficiency. Concurrently, on the device side, we propose an adaptive model adjustment mechanism that integrates the latest global model with local models during training to further elevate accuracy. Subsequently, we extend this approach to a synchronous context, referred to as FedSSMU. Theoretical analyses substantiate the convergence of our proposed methodologies. Extensive experiments, encompassing six models and five public datasets, demonstrate that FedASMU and FedSSMU significantly surpass baseline methods in terms of both accuracy (up to 145.87%) and efficiency (up to 97.59%). 

**Abstract (ZH)**: 联邦学习（FL）已成为管理分布式数据的有力方法，近年来取得了显著进展。本文提出了一种高效的FL方法，充分利用额外的下行带宽资源以确保及时的更新传播。初始阶段，我们在此异步框架中实施了该策略，引入了基于 staleness 的异步模型更新（FedASMU），该方法结合了服务器侧和设备侧的方法。在服务器侧，我们提出了一种异步FL系统模型，采用动态模型聚合技术，以增强模型的准确性和效率。同时，在设备侧，我们提出了一种自适应模型调整机制，在训练过程中将最新的全局模型与局部模型相结合，进一步提高准确率。随后，我们将此方法扩展到同步上下文，称为FedSSMU。理论分析证实了我们提出方法的收敛性。广泛的实验（涵盖六种模型和五个公共数据集）表明，FedASMU和FedSSMU在准确性和效率方面显著优于基线方法（准确率最高提升145.87%，效率最高提升97.59%）。 

---
# The Impact of Event Data Partitioning on Privacy-aware Process Discovery 

**Title (ZH)**: 事件数据分区对隐私感知过程发现的影响 

**Authors**: Jungeun Lim, Stephan A. Fahrenkrog-Petersen, Xixi Lu, Jan Mendling, Minseok Song  

**Link**: [PDF](https://arxiv.org/pdf/2507.06008)  

**Abstract**: Information systems support the execution of business processes. The event logs of these executions generally contain sensitive information about customers, patients, and employees. The corresponding privacy challenges can be addressed by anonymizing the event logs while still retaining utility for process discovery. However, trading off utility and privacy is difficult: the higher the complexity of event log, the higher the loss of utility by anonymization. In this work, we propose a pipeline that combines anonymization and event data partitioning, where event abstraction is utilized for partitioning. By leveraging event abstraction, event logs can be segmented into multiple parts, allowing each sub-log to be anonymized separately. This pipeline preserves privacy while mitigating the loss of utility. To validate our approach, we study the impact of event partitioning on two anonymization techniques using three real-world event logs and two process discovery techniques. Our results demonstrate that event partitioning can bring improvements in process discovery utility for directly-follows-based anonymization techniques. 

**Abstract (ZH)**: 信息系统支持业务过程的执行。这些执行的日志通常包含关于客户、患者和员工的敏感信息。相应的隐私挑战可以通过在保留过程发现实用性的同时对事件日志进行匿名化来解决。然而，在实用性与隐私之间权衡是困难的：事件日志的复杂性越高，匿名化导致的实用性损失越高。在本文中，我们提出了一种结合匿名化和事件数据分区的管道，其中使用事件抽象进行分区。通过利用事件抽象，事件日志可以被分割成多个部分，使每个子日志可以独立地进行匿名化。该管道在保护隐私的同时减少了实用性损失。为了验证我们的方法，我们使用三个真实世界的事件日志和两种过程发现技术研究了事件分区对两种匿名化技术的影响。我们的结果表明，事件分区可以提高直接跟随基于的匿名化技术的过程发现实用性。 

---
# Geo-Registration of Terrestrial LiDAR Point Clouds with Satellite Images without GNSS 

**Title (ZH)**: 基于卫星图像的无GNSS terrestrial LiDAR 点云地理定位方法 

**Authors**: Xinyu Wang, Muhammad Ibrahim, Atif Mansoor, Ajmal Mian  

**Link**: [PDF](https://arxiv.org/pdf/2507.05999)  

**Abstract**: Accurate geo-registration of LiDAR point clouds presents significant challenges in GNSS signal denied urban areas with high-rise buildings and bridges. Existing methods typically rely on real-time GNSS and IMU data, that require pre-calibration and assume stable positioning during data collection. However, this assumption often fails in dense urban areas, resulting in localization errors. To address this, we propose a structured geo-registration and spatial correction method that aligns 3D point clouds with satellite images, enabling frame-wise recovery of GNSS information and reconstruction of city scale 3D maps without relying on prior localization. The proposed approach employs a pre-trained Point Transformer model to segment the road points and then extracts the road skeleton and intersection points from the point cloud as well as the target map for alignment. Global rigid alignment of the two is performed using the intersection points, followed by local refinement using radial basis function (RBF) interpolation. Elevation correction is then applied to the point cloud based on terrain information from SRTM dataset to resolve vertical discrepancies. The proposed method was tested on the popular KITTI benchmark and a locally collected Perth (Western Australia) CBD dataset. On the KITTI dataset, our method achieved an average planimetric alignment standard deviation (STD) of 0.84~m across sequences with intersections, representing a 55.3\% improvement over the original dataset. On the Perth dataset, which lacks GNSS information, our method achieved an average STD of 0.96~m compared to the GPS data extracted from Google Maps API. This corresponds to a 77.4\% improvement from the initial alignment. Our method also resulted in elevation correlation gains of 30.5\% on the KITTI dataset and 50.4\% on the Perth dataset. 

**Abstract (ZH)**: 高-rise建筑和桥梁遮挡GNSS信号的密集城市区域内LiDAR点云精确地理注册的挑战及其解决方法：基于卫星图像的空间结构化地理注册与空间纠正方法 

---
# Exploring Partial Multi-Label Learning via Integrating Semantic Co-occurrence Knowledge 

**Title (ZH)**: 探索基于语义共现知识的部分多标签学习 

**Authors**: Xin Wu, Fei Teng, Yue Feng, Kaibo Shi, Zhuosheng Lin, Ji Zhang, James Wang  

**Link**: [PDF](https://arxiv.org/pdf/2507.05992)  

**Abstract**: Partial multi-label learning aims to extract knowledge from incompletely annotated data, which includes known correct labels, known incorrect labels, and unknown labels. The core challenge lies in accurately identifying the ambiguous relationships between labels and instances. In this paper, we emphasize that matching co-occurrence patterns between labels and instances is key to addressing this challenge. To this end, we propose Semantic Co-occurrence Insight Network (SCINet), a novel and effective framework for partial multi-label learning. Specifically, SCINet introduces a bi-dominant prompter module, which leverages an off-the-shelf multimodal model to capture text-image correlations and enhance semantic alignment. To reinforce instance-label interdependencies, we develop a cross-modality fusion module that jointly models inter-label correlations, inter-instance relationships, and co-occurrence patterns across instance-label assignments. Moreover, we propose an intrinsic semantic augmentation strategy that enhances the model's understanding of intrinsic data semantics by applying diverse image transformations, thereby fostering a synergistic relationship between label confidence and sample difficulty. Extensive experiments on four widely-used benchmark datasets demonstrate that SCINet surpasses state-of-the-art methods. 

**Abstract (ZH)**: 部分多标签学习旨在从不完全注释的数据中提取知识，其中包括已知正确标签、已知错误标签和未知标签。核心挑战在于准确识别标签和实例之间的模糊关系。在本文中，我们强调匹配标签和实例之间的共现模式是解决这一挑战的关键。为此，我们提出了一种新的有效框架——语义共现洞察网络（SCINet），用于部分多标签学习。具体而言，SCINet 引入了一个双主导提示模块，该模块利用现成的多模态模型来捕捉文本-图像相关性并增强语义对齐。为了增强实例标签之间的相互依赖性，我们开发了一种跨模态融合模块，该模块联合建模标签间相关性、实例间关系以及实例标签分配中的共现模式。此外，我们提出了一个内在语义增强策略，通过应用多种图像变换来增强模型对内在数据语义的理解，从而促进标签置信度与样本难度之间的协同关系。在四个广泛使用的基准数据集上的广泛实验表明，SCINet 性能超越了现有最先进的方法。 

---
# Simple Convergence Proof of Adam From a Sign-like Descent Perspective 

**Title (ZH)**: Adam算法从符号下降视角的简单收敛性证明 

**Authors**: Hanyang Peng, Shuang Qin, Yue Yu, Fangqing Jiang, Hui Wang, Zhouchen Lin  

**Link**: [PDF](https://arxiv.org/pdf/2507.05966)  

**Abstract**: Adam is widely recognized as one of the most effective optimizers for training deep neural networks (DNNs). Despite its remarkable empirical success, its theoretical convergence analysis remains unsatisfactory. Existing works predominantly interpret Adam as a preconditioned stochastic gradient descent with momentum (SGDM), formulated as $\bm{x}_{t+1} = \bm{x}_t - \frac{\gamma_t}{{\sqrt{\bm{v}_t}+\epsilon}} \circ \bm{m}_t$. This perspective necessitates strong assumptions and intricate techniques, resulting in lengthy and opaque convergence proofs that are difficult to verify and extend. In contrast, we propose a novel interpretation by treating Adam as a sign-like optimizer, expressed as $\bm{x}_{t+1} = \bm{x}_t - \gamma_t \frac{|\bm{m}_t|}{{\sqrt{\bm{v}_t}+\epsilon}} \circ {\rm Sign}(\bm{m}_t)$. This reformulation significantly simplifies the convergence analysis. For the first time, with some mild conditions, we prove that Adam achieves the optimal rate of ${\cal O}(\frac{1}{T^{\sfrac{1}{4}}})$ rather than the previous ${\cal O} \left(\frac{\ln T}{T^{\sfrac{1}{4}}}\right)$ under weak assumptions of the generalized $p$-affine variance and $(L_0, L_1, q)$-smoothness, without dependence on the model dimensionality or the numerical stability parameter $\epsilon$. Additionally, our theoretical analysis provides new insights into the role of momentum as a key factor ensuring convergence and offers practical guidelines for tuning learning rates in Adam, further bridging the gap between theory and practice. 

**Abstract (ZH)**: Adam被广泛认为是训练深度神经网络（DNNs）最有效的优化器之一。尽管其在经验上表现出色，但其理论收敛分析仍不够令人满意。现有工作主要将Adam解释为具有动量的预条件随机梯度下降（SGDM），表示为$\bm{x}_{t+1} = \bm{x}_t - \frac{\gamma_t}{{\sqrt{\bm{v}_t}+\epsilon}} \circ \bm{m}_t$。这种视角需要强有力的假设和复杂的技术，导致了冗长且不透明的收敛证明，难以验证和扩展。相比之下，我们提出了一种新的解释，将Adam视为一种符号优化器，表示为$\bm{x}_{t+1} = \bm{x}_t - \gamma_t \frac{|\bm{m}_t|}{{\sqrt{\bm{v}_t}+\epsilon}} \circ {\rm Sign}(\bm{m}_t)$。这种重新表述显著简化了收敛分析。首次证明，在一些温和的条件下，即使在泛化的$p$-仿射方差和$(L_0, L_1, q)$-光滑性弱假设下，Adam也能够达到最优速率${\cal O}(\frac{1}{T^{\sfrac{1}{4}}})$，而不是之前的结果${\cal O} \left(\frac{\ln T}{T^{\sfrac{1}{4}}}\right)$，并且不依赖于模型的维数或数值稳定性参数$\epsilon$。此外，我们的理论分析提供了对动量在确保收敛中关键作用的新见解，并为调整Adam的学习率提供了实用建议，进一步弥合了理论与实践之间的差距。 

---
# OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation 

**Title (ZH)**: OpenFActScore: 开源原子化事实性评估方法 

**Authors**: Lucas Fonseca Lage, Simon Ostermann  

**Link**: [PDF](https://arxiv.org/pdf/2507.05965)  

**Abstract**: We introduce OpenFActScore, an open-source implementation of the FActScore framework for evaluating the factuality of text generated by large language models (LLMs). FActScore evaluates the factual accuracy of long-form text by using Atomic Fact Generation (AFG) to extract individual factual claims and Atomic Fact Validation (AFV) to verify each claim against a trusted knowledge source. While the original FActScore relies on closed-source and commercial models such as InstructGPT and ChatGPT, OpenFActScore enables the use of any Hugging Face-compatible model for both AFG and AFV. We provide a detailed technical overview of our implementation, highlighting design choices and modifications made to support open models. We evaluate multiple open-source LLMs on both AFG and AFV using the original FActScore benchmark, reporting BERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our results show that open models can approximate the performance of closed-source systems, with Gemma achieving the best overall performance, and our final setup obtains a 0.99 Pearson correlation with the original FActScore experiments. OpenFActScore promotes transparency, reproducibility, and cost-effective evaluation, and is available at: this https URL. 

**Abstract (ZH)**: OpenFActScore：一种开源的大语言模型文本事实性评估框架实现 

---
# Complexity Results of Persuasion 

**Title (ZH)**: 共识结果中的复杂性分析 

**Authors**: Alban Grastien  

**Link**: [PDF](https://arxiv.org/pdf/2507.05951)  

**Abstract**: We prove that persuasion is an NP-complete problem. 

**Abstract (ZH)**: 我们证明说服是一个NP完全问题。 

---
# On the Effectiveness of Methods and Metrics for Explainable AI in Remote Sensing Image Scene Classification 

**Title (ZH)**: 基于可解释人工智能的方法和评价指标在遥感图像场景分类中的有效性研究 

**Authors**: Jonas Klotz, Tom Burgert, Begüm Demir  

**Link**: [PDF](https://arxiv.org/pdf/2507.05916)  

**Abstract**: The development of explainable artificial intelligence (xAI) methods for scene classification problems has attracted great attention in remote sensing (RS). Most xAI methods and the related evaluation metrics in RS are initially developed for natural images considered in computer vision (CV), and their direct usage in RS may not be suitable. To address this issue, in this paper, we investigate the effectiveness of explanation methods and metrics in the context of RS image scene classification. In detail, we methodologically and experimentally analyze ten explanation metrics spanning five categories (faithfulness, robustness, localization, complexity, randomization), applied to five established feature attribution methods (Occlusion, LIME, GradCAM, LRP, and DeepLIFT) across three RS datasets. Our methodological analysis identifies key limitations in both explanation methods and metrics. The performance of perturbation-based methods, such as Occlusion and LIME, heavily depends on perturbation baselines and spatial characteristics of RS scenes. Gradient-based approaches like GradCAM struggle when multiple labels are present in the same image, while some relevance propagation methods (LRP) can distribute relevance disproportionately relative to the spatial extent of classes. Analogously, we find limitations in evaluation metrics. Faithfulness metrics share the same problems as perturbation-based methods. Localization metrics and complexity metrics are unreliable for classes with a large spatial extent. In contrast, robustness metrics and randomization metrics consistently exhibit greater stability. Our experimental results support these methodological findings. Based on our analysis, we provide guidelines for selecting explanation methods, metrics, and hyperparameters in the context of RS image scene classification. 

**Abstract (ZH)**: 解释性人工智能(xAI)方法在遥感(RS)场景分类问题中的发展引起了广泛关注。大多数RS中的xAI方法及其相关评价指标最初是为计算机视觉(CV)中的自然图像开发的，直接应用于RS可能并不合适。为解决这一问题，本文研究了xAI方法及其评价指标在RS图像场景分类中的有效性。具体而言，我们对三种RS数据集中五种已建立特征归因方法(Occlusion、LIME、GradCAM、LRP、DeepLIFT)下的十种解释指标进行了方法论和实验分析，这十种解释指标涵盖了五个类别（忠实性、鲁棒性、定位、复杂性、随机化）。我们的方法论分析揭示了解释方法和指标的关键局限性。扰动基线和RS场景的空间特征会影响基于扰动的方法（如Occlusion和LIME）的性能；当图像中存在多个标签时，基于梯度的方法（如GradCAM）会遇到挑战；而一些相关传播方法（LRP）可能会不公平地分配相关性。同样，我们在评价指标中发现了局限性。忠实性指标与基于扰动的方法面临相同的问题；空间范围大的类别对于定位指标和复杂性指标是不可靠的；相比之下，鲁棒性指标和随机化指标表现出更大的稳定性。基于我们的实验结果支持这些方法论发现，我们提供了在RS图像场景分类中选择解释方法、指标和超参数的指南。 

---
# Differentiable Reward Optimization for LLM based TTS system 

**Title (ZH)**: 基于LLM的TTS系统可微奖励优化 

**Authors**: Changfeng Gao, Zhihao Du, Shiliang Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2507.05911)  

**Abstract**: This paper proposes a novel Differentiable Reward Optimization (DiffRO) method aimed at enhancing the performance of neural codec language models based text-to-speech (TTS) systems. In contrast to conventional reinforcement learning from human feedback (RLHF) approaches applied to TTS, DiffRO directly compute the rewards based on neural codec tokens, rather than relying on synthesized audio. Furthermore, we employ the Gumbel-Softmax technique to render the reward function differentiable, thereby streamlining the RLHF training process. Additionally, we introduce a multi-task reward (MTR) model which can provide feedback from different perspectives and find that it can augment the system's capability to follow instructions this http URL results indicate that DiffRO significantly improves the pronunciation accuracy of the TTS system, achieving state-of-the-art (SOTA) WER results on the seed-tts-eval benchmark. Moreover, with the integration of the MTR model, we demonstrate the ability to control emotional and quality attributes in a zero-shot manner. 

**Abstract (ZH)**: 一种新型可微奖励优化（DiffRO）方法：基于神经编码器的语言模型的文本转语音系统性能提升研究 

---
# Feature-Based vs. GAN-Based Learning from Demonstrations: When and Why 

**Title (ZH)**: 基于特征的学习与基于GAN的学习从演示中学习：何时以及为何选择 

**Authors**: Chenhao Li, Marco Hutter, Andreas Krause  

**Link**: [PDF](https://arxiv.org/pdf/2507.05906)  

**Abstract**: This survey provides a comparative analysis of feature-based and GAN-based approaches to learning from demonstrations, with a focus on the structure of reward functions and their implications for policy learning. Feature-based methods offer dense, interpretable rewards that excel at high-fidelity motion imitation, yet often require sophisticated representations of references and struggle with generalization in unstructured settings. GAN-based methods, in contrast, use implicit, distributional supervision that enables scalability and adaptation flexibility, but are prone to training instability and coarse reward signals. Recent advancements in both paradigms converge on the importance of structured motion representations, which enable smoother transitions, controllable synthesis, and improved task integration. We argue that the dichotomy between feature-based and GAN-based methods is increasingly nuanced: rather than one paradigm dominating the other, the choice should be guided by task-specific priorities such as fidelity, diversity, interpretability, and adaptability. This work outlines the algorithmic trade-offs and design considerations that underlie method selection, offering a framework for principled decision-making in learning from demonstrations. 

**Abstract (ZH)**: 基于特征的方法与基于生成对抗网络的方法在示范学习中的比较分析：奖励函数结构及其对策略学习的影响 

---
# Universal Embeddings of Tabular Data 

**Title (ZH)**: 通用表格数据嵌入 

**Authors**: Astrid Franz, Frederik Hoppe, Marianne Michaelis, Udo Göbel  

**Link**: [PDF](https://arxiv.org/pdf/2507.05904)  

**Abstract**: Tabular data in relational databases represents a significant portion of industrial data. Hence, analyzing and interpreting tabular data is of utmost importance. Application tasks on tabular data are manifold and are often not specified when setting up an industrial database. To address this, we present a novel framework for generating universal, i.e., task-independent embeddings of tabular data for performing downstream tasks without predefined targets. Our method transforms tabular data into a graph structure, leverages Graph Auto-Encoders to create entity embeddings, which are subsequently aggregated to obtain embeddings for each table row, i.e., each data sample. This two-step approach has the advantage that unseen samples, consisting of similar entities, can be embedded without additional training. Downstream tasks such as regression, classification or outlier detection, can then be performed by applying a distance-based similarity measure in the embedding space. Experiments on real-world datasets demonstrate that our method achieves superior performance compared to existing universal tabular data embedding techniques. 

**Abstract (ZH)**: 关系数据库中的表格数据构成了工业数据的重要部分。因此，分析和解释表格数据至关重要。对表格数据的应用任务多种多样，且在建立工业数据库时往往未明确规定。为解决这一问题，我们提出了一种新颖的框架，用于生成面向表格数据的通用嵌入，以便在无需预定义目标的情况下执行下游任务。该方法将表格数据转换为图结构，利用图自编码器创建实体嵌入，随后聚合这些嵌入以获取每个表格行（即每个数据样本）的嵌入。这种两步方法的优势在于，对于包含相似实体的未见样本，可以无需额外训练直接进行嵌入。然后可以通过在嵌入空间应用基于距离的相似度度量来执行诸如回归、分类或异常检测等下游任务。在实际数据集上的实验表明，与现有的通用表格数据嵌入技术相比，我们的方法能实现更好的性能。 

---
# Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators 

**Title (ZH)**: 使用特征-反应中介的虚拟应答者进行项目心理学测量验证 

**Authors**: Sungjib Lim, Woojung Song, Eun-Ju Lee, Yohan Jo  

**Link**: [PDF](https://arxiv.org/pdf/2507.05890)  

**Abstract**: As psychometric surveys are increasingly used to assess the traits of large language models (LLMs), the need for scalable survey item generation suited for LLMs has also grown. A critical challenge here is ensuring the construct validity of generated items, i.e., whether they truly measure the intended trait. Traditionally, this requires costly, large-scale human data collection. To make it efficient, we present a framework for virtual respondent simulation using LLMs. Our central idea is to account for mediators: factors through which the same trait can give rise to varying responses to a survey item. By simulating respondents with diverse mediators, we identify survey items that robustly measure intended traits. Experiments on three psychological trait theories (Big5, Schwartz, VIA) show that our mediator generation methods and simulation framework effectively identify high-validity items. LLMs demonstrate the ability to generate plausible mediators from trait definitions and to simulate respondent behavior for item validation. Our problem formulation, metrics, methodology, and dataset open a new direction for cost-effective survey development and a deeper understanding of how LLMs replicate human-like behavior. We will publicly release our dataset and code to support future work. 

**Abstract (ZH)**: 随着心理测量调查在评估大型语言模型（LLMs）特质中的应用日益增多，适合LLMs的可扩展调查项目生成需求也不断增长。一个关键挑战是确保生成项目的构建有效性，即它们是否真正测量了预期的特质。传统方法需要大量的成本高昂的人类数据收集。为了使其更高效，我们提出了一种使用LLM进行虚拟受访者模拟的框架。我们的核心思想是考虑中介变量：这些变量通过它们影响同一特质在调查项目中产生不同的响应。通过模拟具有多样中介变量的受访者，我们识别出了能够稳健测量预期特质的调查项目。在对Big5、Schwartz和VIA三种心理特质理论的实验中，我们的中介生成方法和模拟框架有效识别出了高有效性的调查项目。LLMs展示了从特质定义生成合理中介变量和模拟受访者行为以验证项目的能力。我们的问题表述、度量标准、方法论和数据集为低成本调查开发开辟了新方向，并深入理解了LLMs如何复制人类行为。我们将公开发布我们的数据集和代码以支持未来的工作。 

---
# Hierarchy or Heterarchy? A Theory of Long-Range Connections for the Sensorimotor Brain 

**Title (ZH)**: 层级结构还是异阶结构？传感器imotor大脑中长程连接的理论 

**Authors**: Jeff Hawkins, Niels Leadholm, Viviane Clay  

**Link**: [PDF](https://arxiv.org/pdf/2507.05888)  

**Abstract**: In the traditional understanding of the neocortex, sensory information flows up a hierarchy of regions, with each level processing increasingly complex features. Information also flows down the hierarchy via a different set of connections. Although the hierarchical model has significant support, many anatomical connections do not conform to the standard hierarchical interpretation. In addition, hierarchically arranged regions sometimes respond in parallel, not sequentially as would occur in a hierarchy. This and other evidence suggests that two regions can act in parallel and hierarchically at the same time. Given this flexibility, the word "heterarchy" might be a more suitable term to describe neocortical organization. This paper proposes a new interpretation of how sensory and motor information is processed in the neocortex. The key to our proposal is what we call the "Thousand Brains Theory", which posits that every cortical column is a sensorimotor learning system. Columns learn by integrating sensory input over multiple movements of a sensor. In this view, even primary and secondary regions, such as V1 and V2, can learn and recognize complete 3D objects. This suggests that the hierarchical connections between regions are used to learn the compositional structure of parent objects composed of smaller child objects. We explain the theory by examining the different types of long-range connections between cortical regions and between the neocortex and thalamus. We describe these connections, and then suggest the specific roles they play in the context of a heterarchy of sensorimotor regions. We also suggest that the thalamus plays an essential role in transforming the pose between objects and sensors. The novel perspective we argue for here has broad implications for both neuroscience and artificial intelligence. 

**Abstract (ZH)**: 新皮层中感觉和运动信息处理的新理解：一千大脑理论与异archy组织结构 

---
# Comparison of Path Planning Algorithms for Autonomous Vehicle Navigation Using Satellite and Airborne LiDAR Data 

**Title (ZH)**: 基于卫星和机载LiDAR数据的自主车辆导航路径规划算法比较 

**Authors**: Chang Liu, Zhexiong Xue, Tamas Sziranyi  

**Link**: [PDF](https://arxiv.org/pdf/2507.05884)  

**Abstract**: Autonomous vehicle navigation in unstructured environments, such as forests and mountainous regions, presents significant challenges due to irregular terrain and complex road conditions. This work provides a comparative evaluation of mainstream and well-established path planning algorithms applied to weighted pixel-level road networks derived from high-resolution satellite imagery and airborne LiDAR data. For 2D road-map navigation, where the weights reflect road conditions and terrain difficulty, A*, Dijkstra, RRT*, and a Novel Improved Ant Colony Optimization Algorithm (NIACO) are tested on the DeepGlobe satellite dataset. For 3D road-map path planning, 3D A*, 3D Dijkstra, RRT-Connect, and NIACO are evaluated using the Hamilton airborne LiDAR dataset, which provides detailed elevation information. All algorithms are assessed under identical start and end point conditions, focusing on path cost, computation time, and memory consumption. Results demonstrate that Dijkstra consistently offers the most stable and efficient performance in both 2D and 3D scenarios, particularly when operating on dense, pixel-level geospatial road-maps. These findings highlight the reliability of Dijkstra-based planning for static terrain navigation and establish a foundation for future research on dynamic path planning under complex environmental constraints. 

**Abstract (ZH)**: 无结构环境中自主车辆导航存在显著挑战，例如在森林和山区地区，由于不规则地形和复杂道路条件。本研究对主流且成熟的路径规划算法在高分辨率卫星图像和机载LiDAR数据衍生的加权像素级道路网络上的应用进行了比较评估。对于2D道路图导航，其中权重反映了道路条件和地形难度，A*、Dijkstra、RRT*和一种新型改进蚁群优化算法（NIACO）在DeepGlobe卫星数据集上进行测试。对于3D道路图路径规划，3D A*、3D Dijkstra、RRT-Connect和NIACO在提供详细高程信息的Hamilton机载LiDAR数据集上进行评估。所有算法均在相同的起始和终点条件下进行评估，重点关注路径成本、计算时间和内存消耗。研究结果表明，Dijkstra在2D和3D场景中均表现出最稳定和高效的性能，特别是在密集的像素级地理道路图上操作时。这些发现强调了基于Dijkstra的规划方法在静态地形导航中的可靠性，并为在复杂环境约束下的动态路径规划研究奠定了基础。 

---
# Intra-DP: A High Performance Collaborative Inference System for Mobile Edge Computing 

**Title (ZH)**: Intra-DP：移动边缘计算中的一种高性能协作推理系统 

**Authors**: Zekai Sun, Xiuxian Guan, Zheng Lin, Zihan Fang, Xiangming Cai, Zhe Chen, Fangming Liu, Heming Cui, Jie Xiong, Wei Ni, Chau Yuen  

**Link**: [PDF](https://arxiv.org/pdf/2507.05829)  

**Abstract**: Deploying deep neural networks (DNNs) on resource-constrained mobile devices presents significant challenges, particularly in achieving real-time performance while simultaneously coping with limited computational resources and battery life. While Mobile Edge Computing (MEC) offers collaborative inference with GPU servers as a promising solution, existing approaches primarily rely on layer-wise model partitioning and undergo significant transmission bottlenecks caused by the sequential execution of DNN operations. To address this challenge, we present Intra-DP, a high-performance collaborative inference system optimized for DNN inference on MEC. Intra DP employs a novel parallel computing technique based on local operators (i.e., operators whose minimum unit input is not the entire input tensor, such as the convolution kernel). By decomposing their computations (operations) into several independent sub-operations and overlapping the computation and transmission of different sub-operations through parallel execution, Intra-DP mitigates transmission bottlenecks in MEC, achieving fast and energy-efficient inference. The evaluation demonstrates that Intra-DP reduces per-inference latency by up to 50% and energy consumption by up to 75% compared to state-of-the-art baselines, without sacrificing accuracy. 

**Abstract (ZH)**: 将深度神经网络（DNNs）部署在资源受限的移动设备上，特别是在同时实现实时性能、应对有限计算资源和电池寿命方面面临重大挑战。虽然移动边缘计算（MEC）通过利用GPU服务器进行协作推理提供了一种有前景的解决方案，但现有方法主要依赖于层-wise模型划分，并且由于DNN操作的顺序执行导致了显著的传输瓶颈。为应对这一挑战，我们提出了Intra-DP，一种针对MEC上DNN推理优化的高性能协作推理系统。Intra-DP采用了一种基于局部运算符（即最小输入单位不是整个输入张量的操作符，例如卷积核）的新型并行计算技术。通过将运算分解为多个独立的子运算，并通过并行执行重叠不同的子运算的计算与传输，Intra-DP在MEC中缓解了传输瓶颈，实现了快速高效推理。评估表明，与先进的基线方法相比，Intra-DP在每推理周期的延迟上最多可降低50%，能耗最多可降低75%，而不牺牲准确性。 

---
# Constella: Supporting Storywriters' Interconnected Character Creation through LLM-based Multi-Agents 

**Title (ZH)**: Constella: 通过基于LLM的多智能体支持故事作家构建互联人物角色 

**Authors**: Syemin Park, Soobin Park, Youn-kyung Lim  

**Link**: [PDF](https://arxiv.org/pdf/2507.05820)  

**Abstract**: Creating a cast of characters by attending to their relational dynamics is a critical aspect of most long-form storywriting. However, our formative study (N=14) reveals that writers struggle to envision new characters that could influence existing ones, to balance similarities and differences among characters, and to intricately flesh out their relationships. Based on these observations, we designed Constella, an LLM-based multi-agent tool that supports storywriters' interconnected character creation process. Constella suggests related characters (FRIENDS DISCOVERY feature), reveals the inner mindscapes of several characters simultaneously (JOURNALS feature), and manifests relationships through inter-character responses (COMMENTS feature). Our 7-8 day deployment study with storywriters (N=11) shows that Constella enabled the creation of expansive communities composed of related characters, facilitated the comparison of characters' thoughts and emotions, and deepened writers' understanding of character relationships. We conclude by discussing how multi-agent interactions can help distribute writers' attention and effort across the character cast. 

**Abstract (ZH)**: 关注角色关系动力学创建角色是大多数长篇叙事写作的关键方面。然而，我们的初步研究（N=14）表明，作者在构思能够影响现有角色的新角色、平衡角色间的相似性和差异性以及细致刻画角色关系时遇到困难。基于这些观察，我们设计了Constella，一个基于LLM的多智能体工具，支持故事作家的联结性角色创作过程。Constella建议相关角色（FRIENDS DISCOVERY功能）、同时揭示多个角色的内心世界（JOURNALS功能），并通过角色间的回应展现关系（COMMENTS功能）。对我们7-8天的部署研究（N=11）表明，Constella促进了相关角色组成的广泛社区的创建，促进了不同角色思想和情感的比较，并加深了作者对角色关系的理解。我们最后讨论了多智能体交互如何帮助分散作者在角色群体中的注意力和努力。 

---
# Empowering Bridge Digital Twins by Bridging the Data Gap with a Unified Synthesis Framework 

**Title (ZH)**: 通过统一合成框架弥补数据缺口助力桥梁数字孪生 

**Authors**: Wang Wang, Mingyu Shi, Jun Jiang, Wenqian Ma, Chong Liu, Yasutaka Narazaki, Xuguang Wang  

**Link**: [PDF](https://arxiv.org/pdf/2507.05814)  

**Abstract**: As critical transportation infrastructure, bridges face escalating challenges from aging and deterioration, while traditional manual inspection methods suffer from low efficiency. Although 3D point cloud technology provides a new data-driven paradigm, its application potential is often constrained by the incompleteness of real-world data, which results from missing labels and scanning occlusions. To overcome the bottleneck of insufficient generalization in existing synthetic data methods, this paper proposes a systematic framework for generating 3D bridge data.
This framework can automatically generate complete point clouds featuring component-level instance annotations, high-fidelity color, and precise normal vectors. It can be further extended to simulate the creation of diverse and physically realistic incomplete point clouds, designed to support the training of segmentation and completion networks, respectively. Experiments demonstrate that a PointNet++ model trained with our synthetic data achieves a mean Intersection over Union (mIoU) of 84.2% in real-world bridge semantic segmentation. Concurrently, a fine-tuned KT-Net exhibits superior performance on the component completion task.
This research offers an innovative methodology and a foundational dataset for the 3D visual analysis of bridge structures, holding significant implications for advancing the automated management and maintenance of infrastructure. 

**Abstract (ZH)**: 作为一种关键的交通基础设施，桥梁面临老化和退化的日益严峻挑战，而传统的手工检查方法则效率低下。虽然3D点云技术提供了一种新的数据驱动 paradigm，但其应用潜力常受限于真实世界数据的不完整性，这源于标签缺失和扫描遮挡。为克服现有合成数据方法在泛化不足方面的瓶颈，本文提出了一种生成3D桥梁数据的系统框架。此框架能够自动生成包含组件级实例标注、高保真颜色和精确法向量的完整点云。该框架还可进一步扩展，以模拟生成多样的物理上现实的不完整点云，分别用于支持分割和完成网络的训练。实验表明，使用我们合成数据训练的PointNet++模型在实际桥梁语义分割中的平均交并比（mIoU）达到84.2%。同时，微调的KT-Net在组件完成任务上表现出优异性能。本研究提供了一种创新方法和基础数据集，用于3D桥梁结构的视觉分析，对推动基础设施的自动化管理和维护具有重要意义。 

---
# Towards Solar Altitude Guided Scene Illumination 

**Title (ZH)**: 面向太阳高度的场景照明指南方法 

**Authors**: Samed Doğan, Maximilian Hoh, Nico Leuze, Nicolas R.-Peña, Alfred Schöttl  

**Link**: [PDF](https://arxiv.org/pdf/2507.05812)  

**Abstract**: The development of safe and robust autonomous driving functions is heavily dependent on large-scale, high-quality sensor data. However, real-word data acquisition demands intensive human labor and is strongly limited by factors such as labeling cost, driver safety protocols and diverse scenario coverage. Thus, multiple lines of work focus on the conditional generation of synthetic camera sensor data. We identify a significant gap in research regarding daytime variation, presumably caused by the scarcity of available labels. Consequently, we present the solar altitude as global conditioning variable. It is readily computable from latitude-longitude coordinates and local time, eliminating the need for extensive manual labeling. Our work is complemented by a tailored normalization approach, targeting the sensitivity of daylight towards small numeric changes in altitude. We demonstrate its ability to accurately capture lighting characteristics and illumination-dependent image noise in the context of diffusion models. 

**Abstract (ZH)**: 安全可靠的自主驾驶功能的发展高度依赖大规模高质量的传感数据。然而，真实世界数据的获取需要大量人力劳动，并且受到标签成本、驾驶员安全协议和场景多样性等因素的强烈限制。因此，多条研究路径关注于生成合成的摄像传感器数据。我们发现关于日间变化的研究存在显著空白，这可能是因为可用标签的稀缺性。因此，我们提出了太阳高度作为全局条件变量。它可以直接从经纬度坐标和地方时计算得出，从而消除大量的手工标注需求。我们的工作还包括一个针对光照对太阳高度小数值变化敏感性的定制归一化方法。我们展示了其在扩散模型中准确捕捉光照特性和依赖照明的图像噪声的能力。 

---
# Concept-Based Mechanistic Interpretability Using Structured Knowledge Graphs 

**Title (ZH)**: 基于概念的机制可解释性：使用结构化知识图谱 

**Authors**: Sofiia Chorna, Kateryna Tarelkina, Eloïse Berthier, Gianni Franchi  

**Link**: [PDF](https://arxiv.org/pdf/2507.05810)  

**Abstract**: While concept-based interpretability methods have traditionally focused on local explanations of neural network predictions, we propose a novel framework and interactive tool that extends these methods into the domain of mechanistic interpretability. Our approach enables a global dissection of model behavior by analyzing how high-level semantic attributes (referred to as concepts) emerge, interact, and propagate through internal model components. Unlike prior work that isolates individual neurons or predictions, our framework systematically quantifies how semantic concepts are represented across layers, revealing latent circuits and information flow that underlie model decision-making. A key innovation is our visualization platform that we named BAGEL (for Bias Analysis with a Graph for global Explanation Layers), which presents these insights in a structured knowledge graph, allowing users to explore concept-class relationships, identify spurious correlations, and enhance model trustworthiness. Our framework is model-agnostic, scalable, and contributes to a deeper understanding of how deep learning models generalize (or fail to) in the presence of dataset biases. The demonstration is available at this https URL. 

**Abstract (ZH)**: 基于概念的可解释性方法传统上专注于神经网络预测的局部解释，我们提出了一种新颖的框架和交互工具，将这些方法扩展到机制可解释性的领域。我们的方法通过分析高层语义属性（称为概念）的产生、交互和传播，对模型行为进行全局剖析。与先前孤立分析单个神经元或预测的工作不同，我们的框架系统地量化了语义概念在各层中的表示，揭示了模型决策背后潜在的电路和信息流。一个关键创新是我们名为BAGEL（Bias Analysis with a Graph for global Explanation Layers）的可视化平台，该平台以结构化的知识图谱呈现这些见解，使用户能够探索概念类别关系、识别虚假相关性并增强模型可信度。我们的框架是模型无拘束的、可扩展的，并有助于更深入地理解深度学习模型在数据集偏差存在时的泛化（或失败）。演示可访问：this https URL。 

---
# Automated Reasoning for Vulnerability Management by Design 

**Title (ZH)**: 设计导向的自动化推理在漏洞管理中的应用 

**Authors**: Avi Shaked, Nan Messe  

**Link**: [PDF](https://arxiv.org/pdf/2507.05794)  

**Abstract**: For securing systems, it is essential to manage their vulnerability posture and design appropriate security controls. Vulnerability management allows to proactively address vulnerabilities by incorporating pertinent security controls into systems designs. Current vulnerability management approaches do not support systematic reasoning about the vulnerability postures of systems designs. To effectively manage vulnerabilities and design security controls, we propose a formally grounded automated reasoning mechanism. We integrate the mechanism into an open-source security design tool and demonstrate its application through an illustrative example driven by real-world challenges. The automated reasoning mechanism allows system designers to identify vulnerabilities that are applicable to a specific system design, explicitly specify vulnerability mitigation options, declare selected controls, and thus systematically manage vulnerability postures. 

**Abstract (ZH)**: 对于保障系统安全，管理其脆弱性状况并设计适当的安全控制至关重要。当前的脆弱性管理方法不支持对系统设计的脆弱性状况进行系统的推理。为了有效管理脆弱性并设计安全控制，我们提出了一种形式上正当的自动化推理机制。我们将该机制集成到一个开源安全设计工具中，并通过一个以实际挑战为驱动的示例来展示其应用。自动化推理机制使得系统设计师能够识别适用于特定系统设计的脆弱性，明确指定脆弱性缓解选项，声明所选择的控制措施，从而系统地管理脆弱性状况。 

---
# LeAD: The LLM Enhanced Planning System Converged with End-to-end Autonomous Driving 

**Title (ZH)**: LeAD：融合端到端自主驾驶的LLM增强规划系统 

**Authors**: Yuhang Zhang, Jiaqi Liu, Chengkai Xu, Peng Hang, Jian Sun  

**Link**: [PDF](https://arxiv.org/pdf/2507.05754)  

**Abstract**: A principal barrier to large-scale deployment of urban autonomous driving systems lies in the prevalence of complex scenarios and edge cases. Existing systems fail to effectively interpret semantic information within traffic contexts and discern intentions of other participants, consequently generating decisions misaligned with skilled drivers' reasoning patterns. We present LeAD, a dual-rate autonomous driving architecture integrating imitation learning-based end-to-end (E2E) frameworks with large language model (LLM) augmentation. The high-frequency E2E subsystem maintains real-time perception-planning-control cycles, while the low-frequency LLM module enhances scenario comprehension through multi-modal perception fusion with HD maps and derives optimal decisions via chain-of-thought (CoT) reasoning when baseline planners encounter capability limitations. Our experimental evaluation in the CARLA Simulator demonstrates LeAD's superior handling of unconventional scenarios, achieving 71 points on Leaderboard V1 benchmark, with a route completion of 93%. 

**Abstract (ZH)**: 大规模部署城市自动驾驶系统的主要障碍在于复杂场景和边缘情况的普遍存在。现有系统无法有效解读交通场景中的语义信息，也无法辨识其他参与者的意图，从而导致决策与熟练驾驶员的推理模式不一致。我们提出了一种LeAD双速率自动驾驶架构，该架构结合了基于模仿学习的端到端（E2E）框架与大型语言模型（LLM）增强。高频次的E2E子系统保持实时感知-规划-控制循环，而低频次的LLM模块通过多模态感知融合高精度地图来增强场景理解，并在基础规划器遇到能力限制时通过链式推理（CoT）生成最优决策。我们在CARLA仿真器上的实验评估表明，LeAD在Leaderboard V1基准测试中表现出色，获得了71分，路线完成率为93%。 

---
# When Transformers Meet Recommenders: Integrating Self-Attentive Sequential Recommendation with Fine-Tuned LLMs 

**Title (ZH)**: 当变压器遇见推荐系统：结合自注意力序列推荐与微调的大语言模型 

**Authors**: Kechen Liu  

**Link**: [PDF](https://arxiv.org/pdf/2507.05733)  

**Abstract**: Self-Attentive Sequential Recommendation (SASRec) effectively captures long-term user preferences by applying attention mechanisms to historical interactions. Concurrently, the rise of Large Language Models (LLMs) has motivated research into LLM-based recommendation, which leverages their powerful generalization and language understanding capabilities. However, LLMs often lack the domain-specific knowledge and collaborative signals essential for high-quality recommendations when relying solely on textual prompts. To address this limitation, this study proposes SASRecLLM, a novel framework that integrates SASRec as a collaborative encoder with an LLM fine-tuned using Low-Rank Adaptation (LoRA). The components are connected via a mapping layer to align their dimensional spaces, and three targeted training strategies are designed to optimize the hybrid architecture. Extensive experiments on multiple datasets demonstrate that SASRecLLM achieves robust and consistent improvements over strong baselines in both cold-start and warm-start scenarios. This work advances the field of LLM-based recommendation by presenting a modular and effective paradigm for fusing structured collaborative filtering with the semantic power of fine-tuned LLMs. The implementation is available on GitHub: this https URL 

**Abstract (ZH)**: 自注意序列推荐（SASRec）通过应用注意力机制到历史交互中有效地捕捉长期用户偏好。同时，大型语言模型（LLMs）的兴起推动了基于LLM的推荐研究，利用了它们强大的泛化能力和语言理解能力。然而，仅依赖文本提示时，LLMs往往缺乏进行高质量推荐所需的领域特定知识和协作信号。为克服这一限制，本研究提出了SASRecLLM，这是一种新型框架，将SASRec作为协作编码器与使用低秩适应（LoRA）微调的LLM相结合。通过映射层连接其组件，以对其维度空间进行对齐，并设计了三种目标训练策略来优化这种混合架构。在多个数据集上的大量实验表明，SASRecLLM在冷启动和温启动场景中均实现了稳健且一致的改进。本研究通过展示模块化和有效的范式，将结构化协作过滤与微调的LLM的语义力量融合在一起，推进了基于LLM的推荐领域。相关实现可在GitHub上获取：this https URL。 

---
# A Satellite-Ground Synergistic Large Vision-Language Model System for Earth Observation 

**Title (ZH)**: 地球观测中的卫星-地面协同大型视觉-语言模型系统 

**Authors**: Yuxin Zhang, Jiahao Yang, Zhe Chen, Wenjun Zhu, Jin Zhao, Yue Gao  

**Link**: [PDF](https://arxiv.org/pdf/2507.05731)  

**Abstract**: Recently, large vision-language models (LVLMs) unleash powerful analysis capabilities for low Earth orbit (LEO) satellite Earth observation images in the data center. However, fast satellite motion, brief satellite-ground station (GS) contact windows, and large size of the images pose a data download challenge. To enable near real-time Earth observation applications (e.g., disaster and extreme weather monitoring), we should explore how to deploy LVLM in LEO satellite networks, and design SpaceVerse, an efficient satellite-ground synergistic LVLM inference system. To this end, firstly, we deploy compact LVLMs on satellites for lightweight tasks, whereas regular LVLMs operate on GSs to handle computationally intensive tasks. Then, we propose a computing and communication co-design framework comprised of a progressive confidence network and an attention-based multi-scale preprocessing, used to identify on-satellite inferring data, and reduce data redundancy before satellite-GS transmission, separately. We implement and evaluate SpaceVerse on real-world LEO satellite constellations and datasets, achieving a 31.2% average gain in accuracy and a 51.2% reduction in latency compared to state-of-the-art baselines. 

**Abstract (ZH)**: 近期，大型视觉-语言模型（LVLMs）在数据中心对低地球轨道（LEO）卫星地球观测图像进行强大的分析能力得以释放。然而，快速的卫星运动、短暂的卫星-地面站（GS）通信窗口以及图像的巨大尺寸带来了数据下载的挑战。为实现近实时的地球观测应用（如灾害和极端天气监测），我们应该探索如何在LEO卫星网络中部署LVLM，并设计SpaceVerse，一个高效的卫星-地面协同LVLM推理系统。为此，首先，我们在卫星上部署紧凑型LVLM以处理轻量级任务，而定期的LVLM在地面站运行以处理计算密集型任务。然后，我们提出了一种计算与通信协同设计框架，包括逐级自信网络和基于注意力的多尺度预处理，用于在卫星上传输前识别卫星上的推理数据并减少数据冗余。我们已在实际LEO卫星星座和数据集上实现并评估了SpaceVerse，相比最先进的基准，平均提高了31.2%的准确率并降低了51.2%的延迟。 

---
# Hyperspectral Anomaly Detection Methods: A Survey and Comparative Study 

**Title (ZH)**: 高光谱异常检测方法：综述与对比研究 

**Authors**: Aayushma Pant, Arbind Agrahari Baniya, Tsz-Kwan Lee, Sunil Aryal  

**Link**: [PDF](https://arxiv.org/pdf/2507.05730)  

**Abstract**: Hyperspectral images are high-dimensional datasets consisting of hundreds of contiguous spectral bands, enabling detailed material and surface analysis. Hyperspectral anomaly detection (HAD) refers to the technique of identifying and locating anomalous targets in such data without prior information about a hyperspectral scene or target spectrum. This technology has seen rapid advancements in recent years, with applications in agriculture, defence, military surveillance, and environmental monitoring. Despite this significant progress, existing HAD methods continue to face challenges such as high computational complexity, sensitivity to noise, and limited generalisation across diverse datasets. This study presents a comprehensive comparison of various HAD techniques, categorising them into statistical models, representation-based methods, classical machine learning approaches, and deep learning models. We evaluated these methods across 17 benchmarking datasets using different performance metrics, such as ROC, AUC, and separability map to analyse detection accuracy, computational efficiency, their strengths, limitations, and directions for future this http URL research shows that deep learning models achieved the highest detection accuracy, while statistical models demonstrated exceptional speed across all datasets. This study aims to provide valuable insights for researchers and practitioners working to advance the field of hyperspectral anomaly detection methods. 

**Abstract (ZH)**: 高光谱图像是一种高维数据集，包含数百个连续的光谱波段，使材料和表面分析变得详细。高光谱异常检测（HAD）是指在无需有关高光谱场景或目标光谱先验信息的情况下，识别并定位异常目标的技术。尽管在过去几年中此技术取得了迅速进展，应用范围涵盖农业、国防、军事侦察和环境监测等领域，但现有的HAD方法依然面临高计算复杂性、噪声敏感性和在多样化数据集间泛化能力有限等挑战。本研究对各种HAD技术进行了全面比较，将其分类为统计模型、表示基于方法、经典机器学习方法和深度学习模型。我们使用不同的性能指标（如ROC、AUC和可分性图）在17个基准数据集中评估了这些方法的检测精度、计算效率、优势、局限性和未来研究方向。研究结果显示，深度学习模型在检测精度方面表现最佳，而统计模型则在所有数据集中展现出卓越的速度。本研究旨在为致力于推动高光谱异常检测方法领域发展的研究人员和实践者提供有价值的见解。 

---
# Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition 

**Title (ZH)**: 全方位路由器：在稀疏专家混合模型中的路由决策共享枇杷树 

**Authors**: Zijin Gu, Tatiana Likhomanenko, Navdeep Jaitly  

**Link**: [PDF](https://arxiv.org/pdf/2507.05724)  

**Abstract**: Mixture-of-experts (MoE) architectures have expanded from language modeling to automatic speech recognition (ASR). Traditional MoE methods, such as the Switch Transformer, route experts independently within each layer. Our analysis reveals that routers in most layers make expert choices that are not strongly correlated with the choices of the routers in other layers. To increase the cooperation between experts in different layers and encourage greater specialization, we use a shared router across different MoE layers. We call this model \emph{Omni-router Transformer}. Extensive experiments on a large-scale pseudo-labeled dataset and evaluations across 10 diverse, out-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is able to achieve lower training loss and consistently outperform dense and Switch Transformer models, reducing average word error rates by 11.2% and 8.2%, respectively, while providing structured expert usage and improved robustness to diverse data. 

**Abstract (ZH)**: Omni-router Transformer 

---
# HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation 

**Title (ZH)**: HIRAG: 分层思维指令调优检索增强生成 

**Authors**: YiHan Jiao, ZheHao Tan, Dan Yang, DuoLin Sun, Jie Feng, Jian Wang, Peng Wei  

**Link**: [PDF](https://arxiv.org/pdf/2507.05714)  

**Abstract**: Retrieval-augmented generation (RAG) has become a fundamental paradigm for addressing the challenges faced by large language models in handling real-time information and domain-specific problems. Traditional RAG systems primarily rely on the in-context learning (ICL) capabilities of the large language model itself. Still, in-depth research on the specific capabilities needed by the RAG generation model is lacking, leading to challenges with inconsistent document quality and retrieval system imperfections. Even the limited studies that fine-tune RAG generative models often \textit{lack a granular focus on RAG task} or \textit{a deeper utilization of chain-of-thought processes}. To address this, we propose that RAG models should possess three progressively hierarchical abilities (1) Filtering: the ability to select relevant information; (2) Combination: the ability to combine semantic information across paragraphs; and (3) RAG-specific reasoning: the ability to further process external knowledge using internal knowledge. Thus, we introduce our new RAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation (HIRAG) incorporates a "think before answering" strategy. This method enhances the model's open-book examination capability by utilizing multi-level progressive chain-of-thought. Experiments show that the HIRAG training strategy significantly improves the model's performance on datasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA. 

**Abstract (ZH)**: 检索增强生成（RAG）已成为解决大型语言模型在处理实时信息和领域特定问题时面临的挑战的基本范式。传统RAG系统主要依赖于大型语言模型自身的上下文学习（ICL）能力，但关于RAG生成模型所需的具体能力的研究还不够深入，导致文档质量不一致和检索系统缺陷的问题。即使是少量对RAG生成模型进行微调的研究，也往往缺乏对RAG任务的细致关注或深度利用思维链过程。为此，我们提出RAG模型应具备三个逐级递进的能力：（1）过滤：选择相关信息的能力；（2）组合：跨段落整合语义信息的能力；（3）特定于RAG的推理：利用内部知识进一步处理外部知识。因此，我们提出了一种新的RAG指令微调方法——层次思维指令微调检索增强生成（HIRAG），该方法采用“先思考后作答”的策略，通过多级逐步思维链提升模型的开放书测试能力。实验结果显示，HIRAG训练策略显著提高了该模型在RGB、PopQA、MuSiQue、HotpotQA和PubmedQA等数据集上的性能。 

---
# DRAGON: Dynamic RAG Benchmark On News 

**Title (ZH)**: DRAGON: 动态RAG新闻基准测试 

**Authors**: Fedor Chernogorskii, Sergei Averkiev, Liliya Kudraleeva, Zaven Martirosian, Maria Tikhonova, Valentin Malykh, Alena Fenogenova  

**Link**: [PDF](https://arxiv.org/pdf/2507.05713)  

**Abstract**: Retrieval-Augmented Generation (RAG) is a widely adopted approach for improving the factuality of large language models (LLMs) by incorporating external knowledge at inference time. Although there exist multiple RAG benchmarks for English, evaluation resources for other languages, including Russian, remain scarce and static, failing to capture the dynamic nature of real-world deployments.
In this work, we present DRAGON (Dynamic RAG Benchmark On News), the first dynamic benchmark for evaluating RAG systems in Russian on a changing news corpora. DRAGON is built upon a regularly updated corpus of Russian news and public documents and supports comprehensive evaluation of both the retriever and generator components. Question generation is performed automatically with the use of Knowledge Graph constructed from the corpus and enables the extraction of four core question types aligned with distinct subgraph patterns. We release a complete evaluation framework comprising the pipeline for automatic question generation, evaluation scripts, which are potentially reusable for other languages and multilingual settings, and benchmark data. We also launch a public leaderboard to encourage community participation and comparison. 

**Abstract (ZH)**: 动态新闻RAG基准（DRAGON）：俄语RAG系统的动态评估 

---
# Agentic-R1: Distilled Dual-Strategy Reasoning 

**Title (ZH)**: 代理-R1: 提炼的双策略推理 

**Authors**: Weihua Du, Pranjal Aggarwal, Sean Welleck, Yiming Yang  

**Link**: [PDF](https://arxiv.org/pdf/2507.05707)  

**Abstract**: Current long chain-of-thought (long-CoT) models excel at mathematical reasoning but rely on slow and error-prone natural language traces. Tool-augmented agents address arithmetic via code execution, but often falter on complex logical tasks. We introduce a fine-tuning framework, DualDistill, that distills complementary reasoning strategies from multiple teachers into a unified student model. Using this approach, we train Agentic-R1, which dynamically selects the optimal strategy for each query, invoking tools for arithmetic and algorithmic problems, and using text-based reasoning for abstract ones. Our method improves accuracy across a range of tasks, including both computation-intensive and standard benchmarks, demonstrating the effectiveness of multi-strategy distillation in achieving robust and efficient reasoning. Our project is available at this https URL 

**Abstract (ZH)**: 当前的长链条思考（Long-CoT）模型在数学推理方面表现出色，但依赖于缓慢且易出错的自然语言推理轨迹。工具增强的代理通过代码执行解决算术问题，但在复杂的逻辑任务上常常表现不佳。我们提出了一个精炼框架——DualDistill，该框架从多个老师中提炼出互补的推理策略并统一到学生模型中。通过这种方法，我们训练了Agentic-R1，它能够为每个查询动态选择最优策略，利用工具解决算术和算法问题，并使用文本推理解决抽象问题。我们的方法在一系列任务上提高了准确性，包括计算密集型和标准基准测试，展示了多策略提炼在实现稳健且高效的推理方面的有效性。该项目可从以下链接访问：this https URL 

---
# Efficient Training of Large-Scale AI Models Through Federated Mixture-of-Experts: A System-Level Approach 

**Title (ZH)**: 通过联邦混合专家系统级方法高效训练大规模AI模型 

**Authors**: Xiaobing Chen, Boyang Zhang, Xiangwei Zhou, Mingxuan Sun, Shuai Zhang, Songyang Zhang, Geoffrey Ye Li  

**Link**: [PDF](https://arxiv.org/pdf/2507.05685)  

**Abstract**: The integration of Federated Learning (FL) and Mixture-of-Experts (MoE) presents a compelling pathway for training more powerful, large-scale artificial intelligence models (LAMs) on decentralized data while preserving privacy. However, efficient federated training of these complex MoE-structured LAMs is hindered by significant system-level challenges, particularly in managing the interplay between heterogeneous client resources and the sophisticated coordination required for numerous specialized experts. This article highlights a critical, yet underexplored concept: the absence of robust quantitative strategies for dynamic client-expert alignment that holistically considers varying client capacities and the imperative for system-wise load balancing. Specifically, we propose a conceptual system design for intelligent client-expert alignment that incorporates dynamic fitness scoring, global expert load monitoring, and client capacity profiling. By tackling these systemic issues, we can unlock more scalable, efficient, and robust training mechanisms {with fewer communication rounds for convergence}, paving the way for the widespread deployment of large-scale federated MoE-structured LAMs in edge computing with ultra-high communication efficiency. 

**Abstract (ZH)**: 联邦学习（FL）与混合专家（MoE）的集成为在去中心化数据上训练更强大、更大规模的人工智能模型（LAMs）提供了令人信服的途径，同时保护隐私。然而，这些复杂结构的MoE模型的高效联邦训练受到了显著的系统级挑战的阻碍，特别是在管理和协调异构客户端资源与众多专门专家之间复杂交互方面。本文强调了一个关键但尚未充分探索的概念：缺乏一种稳健的定量策略来动态协调客户端与专家，这一策略能够综合考虑客户容量的变化和系统级负载均衡的迫切需要。具体而言，我们提出了一种概念性的系统设计，用于智能客户端与专家的动态对齐，该设计融合了动态适应性评分、全局专家负载监控和客户端容量特征化。通过解决这些系统性问题，我们可以解锁更具有伸缩性、更高效和更稳健的训练机制，通过减少收敛所需的通信轮数，在边缘计算中为超高效通信效率下广泛部署大规模联邦MoE结构的LAMs铺平道路。 

---
# GATMesh: Clock Mesh Timing Analysis using Graph Neural Networks 

**Title (ZH)**: GATMesh: 使用图神经网络进行时钟网时序分析 

**Authors**: Muhammad Hadir Khan, Matthew Guthaus  

**Link**: [PDF](https://arxiv.org/pdf/2507.05681)  

**Abstract**: Clock meshes are essential in high-performance VLSI systems for minimizing skew and handling PVT variations, but analyzing them is difficult due to reconvergent paths, multi-source driving, and input mesh buffer skew. SPICE simulations are accurate but slow; yet simplified models miss key effects like slew and input skew. We propose GATMesh, a Graph Neural Network (GNN)-based framework that models the clock mesh as a graph with augmented structural and physical features. Trained on SPICE data, GATMesh achieves high accuracy with average delay error of 5.27ps on unseen benchmarks, while achieving speed-ups of 47146x over multi-threaded SPICE simulation. 

**Abstract (ZH)**: 基于图神经网络的GATMesh框架：用于高性能VLSI系统时钟网格的图表示与快速分析 

---
# MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos 

**Title (ZH)**: MedGen: 通过细粒度标注的医疗视频扩展实现医疗视频生成 

**Authors**: Rongsheng Wang, Junying Chen, Ke Ji, Zhenyang Cai, Shunian Chen, Yunjin Yang, Benyou Wang  

**Link**: [PDF](https://arxiv.org/pdf/2507.05675)  

**Abstract**: Recent advances in video generation have shown remarkable progress in open-domain settings, yet medical video generation remains largely underexplored. Medical videos are critical for applications such as clinical training, education, and simulation, requiring not only high visual fidelity but also strict medical accuracy. However, current models often produce unrealistic or erroneous content when applied to medical prompts, largely due to the lack of large-scale, high-quality datasets tailored to the medical domain. To address this gap, we introduce MedVideoCap-55K, the first large-scale, diverse, and caption-rich dataset for medical video generation. It comprises over 55,000 curated clips spanning real-world medical scenarios, providing a strong foundation for training generalist medical video generation models. Built upon this dataset, we develop MedGen, which achieves leading performance among open-source models and rivals commercial systems across multiple benchmarks in both visual quality and medical accuracy. We hope our dataset and model can serve as a valuable resource and help catalyze further research in medical video generation. Our code and data is available at this https URL 

**Abstract (ZH)**: 近期在视频生成领域的进展已经在开放场景中显示出了显著的进步，然而医疗视频生成仍然 largely underexplored。医疗视频对于临床培训、教育和模拟等应用至关重要，不仅需要高度的视觉保真度，还需要严格的高度准确的医疗信息。然而，当前的模型在处理医疗提示时经常生成不现实或错误的内容，主要是由于缺乏针对医疗领域的大型高质量数据集。为了解决这一差距，我们介绍了MedVideoCap-55K，这是第一个用于医疗视频生成的大规模、多样化且带有丰富描述的医疗视频数据集，包含超过55,000个精心筛选的片段，覆盖了真实的医疗场景，为训练通用医疗视频生成模型提供了坚实的基础。基于该数据集，我们开发了MedGen，在多个基准测试中，在视觉质量和医疗准确性方面表现领先，与开源模型和商业系统相比具有竞争力。我们希望我们的数据集和模型能够成为有价值的资源，并促进医疗视频生成领域的进一步研究。我们的代码和数据可在以下网址获取：this https URL 

---
# TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data 

**Title (ZH)**: TuneShield: 在使用不可信数据微调的同时减轻对话AI的毒性风险 

**Authors**: Aravind Cheruvu, Shravya Kanchi, Sifat Muhammad Abdullah, Nicholas Kong, Daphne Yao, Murtuza Jadliwala, Bimal Viswanath  

**Link**: [PDF](https://arxiv.org/pdf/2507.05660)  

**Abstract**: Recent advances in foundation models, such as LLMs, have revolutionized conversational AI. Chatbots are increasingly being developed by customizing LLMs on specific conversational datasets. However, mitigating toxicity during this customization, especially when dealing with untrusted training data, remains a significant challenge. To address this, we introduce TuneShield, a defense framework designed to mitigate toxicity during chatbot fine-tuning while preserving conversational quality. TuneShield leverages LLM-based toxicity classification, utilizing the instruction-following capabilities and safety alignment of LLMs to effectively identify toxic samples, outperforming industry API services. TuneShield generates synthetic conversation samples, termed 'healing data', based on the identified toxic samples, using them to mitigate toxicity while reinforcing desirable behavior during fine-tuning. It performs an alignment process to further nudge the chatbot towards producing desired responses. Our findings show that TuneShield effectively mitigates toxicity injection attacks while preserving conversational quality, even when the toxicity classifiers are imperfect or biased. TuneShield proves to be resilient against adaptive adversarial and jailbreak attacks. Additionally, TuneShield demonstrates effectiveness in mitigating adaptive toxicity injection attacks during dialog-based learning (DBL). 

**Abstract (ZH)**: Recent Advances in Foundation Models, Such as LLMs, Have Revolutionized Conversational AI: Introducing TuneShield, a Defense Framework for Mitigating Toxicity While Preserving Conversational Quality in Chatbot Fine-Tuning 

---
# DESIGN: Encrypted GNN Inference via Server-Side Input Graph Pruning 

**Title (ZH)**: DESIGN：通过服务器端输入图形剪枝的加密GNN推理 

**Authors**: Kaixiang Zhao, Joseph Yousry Attalla, Qian Lou, Yushun Dong  

**Link**: [PDF](https://arxiv.org/pdf/2507.05649)  

**Abstract**: Graph Neural Networks (GNNs) have achieved state-of-the-art performance in various graph-based learning tasks. However, enabling privacy-preserving GNNs in encrypted domains, such as under Fully Homomorphic Encryption (FHE), typically incurs substantial computational overhead, rendering real-time and privacy-preserving inference impractical. In this work, we propose DESIGN (EncrypteD GNN Inference via sErver-Side Input Graph pruNing), a novel framework for efficient encrypted GNN inference. DESIGN tackles the critical efficiency limitations of existing FHE GNN approaches, which often overlook input data redundancy and apply uniform computational strategies. Our framework achieves significant performance gains through a hierarchical optimization strategy executed entirely on the server: first, FHE-compatible node importance scores (based on encrypted degree statistics) are computed from the encrypted graph. These scores then guide a homomorphic partitioning process, generating multi-level importance masks directly under FHE. This dynamically generated mask facilitates both input graph pruning (by logically removing unimportant elements) and a novel adaptive polynomial activation scheme, where activation complexity is tailored to node importance levels. Empirical evaluations demonstrate that DESIGN substantially accelerates FHE GNN inference compared to state-of-the-art methods while maintaining competitive model accuracy, presenting a robust solution for secure graph analytics. 

**Abstract (ZH)**: 加密域中高效图神经网络推理的DESIGN框架：基于服务器端输入图剪枝 

---
# FACT: the Features At Convergence Theorem for neural networks 

**Title (ZH)**: 特征在收敛定理中的神经网络特征性分析 

**Authors**: Enric Boix-Adsera, Neil Mallinar, James B. Simon, Mikhail Belkin  

**Link**: [PDF](https://arxiv.org/pdf/2507.05644)  

**Abstract**: A central challenge in deep learning theory is to understand how neural networks learn and represent features. To this end, we prove the Features at Convergence Theorem (FACT), which gives a self-consistency equation that neural network weights satisfy at convergence when trained with nonzero weight decay. For each weight matrix $W$, this equation relates the "feature matrix" $W^\top W$ to the set of input vectors passed into the matrix during forward propagation and the loss gradients passed through it during backpropagation. We validate this relation empirically, showing that neural features indeed satisfy the FACT at convergence. Furthermore, by modifying the "Recursive Feature Machines" of Radhakrishnan et al. 2024 so that they obey the FACT, we arrive at a new learning algorithm, FACT-RFM. FACT-RFM achieves high performance on tabular data and captures various feature learning behaviors that occur in neural network training, including grokking in modular arithmetic and phase transitions in learning sparse parities. 

**Abstract (ZH)**: 深度学习理论中的一个核心挑战是理解神经网络如何学习和表示特征。为此，我们证明了在非零权重衰减训练下神经网络在收敛时满足的特征在收敛时满足的特性定理（Features at Convergence Theorem，FACT），该定理给出了一种自一致性方程，描述了每个权重矩阵 \(W\) 在前向传播过程中传递入矩阵的输入向量集和在反向传播过程中通过该矩阵的损失梯度集与“特征矩阵” \(W^\top W\) 之间的关系。我们通过实验验证了这一关系，表明神经网络特征确实满足FACT在收敛时。此外，通过将Radhakrishnan等人2024年提出的“递归特征机器”修改为遵守FACT，我们得到了一种新的学习算法FACT-RFM。FACT-RFM在表格数据上的性能表现优异，并捕获了神经网络训练中出现的各种特征学习行为，包括算术模块化中的grokking现象和学习稀疏对称时的相变行为。 

---
# Graph Learning 

**Title (ZH)**: 图学习 

**Authors**: Feng Xia, Ciyuan Peng, Jing Ren, Falih Gozi Febrinanto, Renqiang Luo, Vidya Saikrishna, Shuo Yu, Xiangjie Kong  

**Link**: [PDF](https://arxiv.org/pdf/2507.05636)  

**Abstract**: Graph learning has rapidly evolved into a critical subfield of machine learning and artificial intelligence (AI). Its development began with early graph-theoretic methods, gaining significant momentum with the advent of graph neural networks (GNNs). Over the past decade, progress in scalable architectures, dynamic graph modeling, multimodal learning, generative AI, explainable AI (XAI), and responsible AI has broadened the applicability of graph learning to various challenging environments. Graph learning is significant due to its ability to model complex, non-Euclidean relationships that traditional machine learning struggles to capture, thus better supporting real-world applications ranging from drug discovery and fraud detection to recommender systems and scientific reasoning. However, challenges like scalability, generalization, heterogeneity, interpretability, and trustworthiness must be addressed to unlock its full potential. This survey provides a comprehensive introduction to graph learning, focusing on key dimensions including scalable, temporal, multimodal, generative, explainable, and responsible graph learning. We review state-of-the-art techniques for efficiently handling large-scale graphs, capturing dynamic temporal dependencies, integrating heterogeneous data modalities, generating novel graph samples, and enhancing interpretability to foster trust and transparency. We also explore ethical considerations, such as privacy and fairness, to ensure responsible deployment of graph learning models. Additionally, we identify and discuss emerging topics, highlighting recent integration of graph learning and other AI paradigms and offering insights into future directions. This survey serves as a valuable resource for researchers and practitioners seeking to navigate the rapidly evolving landscape of graph learning. 

**Abstract (ZH)**: 图学习已迅速发展成为机器学习和人工智能（AI）领域的关键子领域。其发展始于早期的图理论方法，随着图神经网络（GNNs）的兴起获得显著动力。在过去的十年中，可扩展架构、动态图建模、多模态学习、生成AI、可解释AI（XAI）和负责任AI的进步，使图学习的应用范围扩展到了各种具有挑战性的环境中。图学习之所以重要，是因为它能够建模传统机器学习难以捕捉的复杂、非欧几里得关系，从而更好地支持从药物发现和欺诈检测到推荐系统和科学推理等各个领域的实际应用。然而，要充分发挥其潜力，仍需解决可扩展性、泛化能力、异质性、可解释性和可信度等挑战。本文综述了图学习，着重介绍了可扩展性、时序性、多模态、生成性、可解释性和负责任图学习的关键维度。我们回顾了高效处理大规模图、捕捉动态时序依赖性、整合异构数据模态、生成新颖图样本以及提高可解释性以促进信任和透明度的最新技术。我们还探讨了伦理考量，如隐私和公平性，以确保图学习模型负责任的部署。此外，我们指出了新兴话题，并讨论了图学习与其他AI范式的最新整合，为未来方向提供了见解。本文综述将成为研究人员和从业人员导航图学习快速发展的领域的重要资源。 

---
# SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression 

**Title (ZH)**: SARA: 选择性和自适应检索增强生成与上下文压缩 

**Authors**: Yiqiao Jin, Kartik Sharma, Vineeth Rakesh, Yingtong Dou, Menghai Pan, Mahashweta Das, Srijan Kumar  

**Link**: [PDF](https://arxiv.org/pdf/2507.05633)  

**Abstract**: Retrieval-augmented Generation (RAG) extends large language models (LLMs) with external knowledge but faces key challenges: restricted effective context length and redundancy in retrieved documents. Pure compression-based approaches reduce input size but often discard fine-grained details essential for factual accuracy. We propose SARA, a unified RAG framework that balances local precision and global knowledge coverage under tight context budgets. SARA combines natural-language text snippets with semantic compression vectors to jointly enhance context efficiency and answer correctness. It represents contexts at two complementary levels: 1) fine-grained natural-language spans that preserve critical entities and numerical values, and 2) compact, interpretable vectors that summarize high-level semantics. An iterative evidence-selection module employs the compression vectors for dynamic reranking of contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families (Mistral, Llama, and Gemma), SARA consistently improves answer relevance (+17.71), answer correctness (+13.72), and semantic similarity (+15.53), demonstrating the importance of integrating textual and compressed representations for robust, context-efficient RAG. 

**Abstract (ZH)**: 检索增强生成（RAG）通过外部知识扩展了大规模语言模型（LLMs），但面临关键挑战：有效的上下文长度限制和检索文档中的冗余。基于纯压缩的方法减少了输入大小，但往往会丢弃对于事实准确性至关重要的细粒度细节。我们提出了一种统一的RAG框架SARA，该框架在紧凑的上下文预算下平衡局部精确性和全局知识覆盖。SARA结合自然语言文本片段与语义压缩向量，共同提升上下文效率和答案准确性。它从两个互补的层次表示上下文：1) 细粒度的自然语言片段，保留关键实体和数值；2) 紧凑、可解释的向量，总结高层次语义。迭代的证据选择模块利用压缩向量动态重新排名上下文。在9个数据集和5个开源LLM（Mistral、Llama和Gemma）上，SARA在答案相关性（+17.71）、答案准确性（+13.72）和语义相似度（+15.53）方面表现出一致的改进，展示了将文本和压缩表示结合以实现稳健、高效RAG的重要性。 

---
# How Not to Detect Prompt Injections with an LLM 

**Title (ZH)**: 如何不使用大规模语言模型检测提示注入 

**Authors**: Sarthak Choudhary, Divyam Anshumaan, Nils Palumbo, Somesh Jha  

**Link**: [PDF](https://arxiv.org/pdf/2507.05630)  

**Abstract**: LLM-integrated applications and agents are vulnerable to prompt injection attacks, in which adversaries embed malicious instructions within seemingly benign user inputs to manipulate the LLM's intended behavior. Recent defenses based on $\textit{known-answer detection}$ (KAD) have achieved near-perfect performance by using an LLM to classify inputs as clean or contaminated. In this work, we formally characterize the KAD framework and uncover a structural vulnerability in its design that invalidates its core security premise. We design a methodical adaptive attack, $\textit{DataFlip}$, to exploit this fundamental weakness. It consistently evades KAD defenses with detection rates as low as $1.5\%$ while reliably inducing malicious behavior with success rates of up to $88\%$, without needing white-box access to the LLM or any optimization procedures. 

**Abstract (ZH)**: LLM整合应用和代理易受提示注入攻击的影响，攻击者通过在看似无害的用户输入中嵌入恶意指令来操纵LLM的预期行为。基于已知答案检测(KAD)的最近防御方法通过使用LLM对输入进行分类，以接近完美性能实现了清洁或污染的识别。在本文中，我们正式刻画了KAD框架，并揭示了其设计中的结构漏洞，这使得其核心安全假设失效。我们设计了一种系统性的自适应攻击方法DataFlip，以利用这一基本弱点。DataFlip在检测率低至1.5%的情况下持续避开KAD防御，同时以高达88%的成功率可靠地诱导恶意行为，无需对LLM进行白盒访问或采用任何优化程序。 

---
# DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective 

**Title (ZH)**: DATABench: 从对抗视角评估深度学习中的数据集审计 

**Authors**: Shuo Shao, Yiming Li, Mengren Zheng, Zhiyang Hu, Yukun Chen, Boheng Li, Yu He, Junfeng Guo, Tianwei Zhang, Dacheng Tao, Zhan Qin  

**Link**: [PDF](https://arxiv.org/pdf/2507.05622)  

**Abstract**: The widespread application of Deep Learning across diverse domains hinges critically on the quality and composition of training datasets. However, the common lack of disclosure regarding their usage raises significant privacy and copyright concerns. Dataset auditing techniques, which aim to determine if a specific dataset was used to train a given suspicious model, provide promising solutions to addressing these transparency gaps. While prior work has developed various auditing methods, their resilience against dedicated adversarial attacks remains largely unexplored. To bridge the gap, this paper initiates a comprehensive study evaluating dataset auditing from an adversarial perspective. We start with introducing a novel taxonomy, classifying existing methods based on their reliance on internal features (IF) (inherent to the data) versus external features (EF) (artificially introduced for auditing). Subsequently, we formulate two primary attack types: evasion attacks, designed to conceal the use of a dataset, and forgery attacks, intending to falsely implicate an unused dataset. Building on the understanding of existing methods and attack objectives, we further propose systematic attack strategies: decoupling, removal, and detection for evasion; adversarial example-based methods for forgery. These formulations and strategies lead to our new benchmark, DATABench, comprising 17 evasion attacks, 5 forgery attacks, and 9 representative auditing methods. Extensive evaluations using DATABench reveal that none of the evaluated auditing methods are sufficiently robust or distinctive under adversarial settings. These findings underscore the urgent need for developing a more secure and reliable dataset auditing method capable of withstanding sophisticated adversarial manipulation. Code is available at this https URL. 

**Abstract (ZH)**: 深学习在不同领域的广泛应用取决于训练数据集的质量和组成。然而，关于数据集使用情况的常见透明度不足引发了严重的隐私和版权问题。旨在确定特定数据集是否被用于训练给定的可疑模型的数据集审计技术提供了解决这些透明度缺口的有前景的解决方案。尽管已有各种审计方法被开发出来，但它们抵抗专门敌对攻击的鲁棒性仍处于探索阶段。为填补这一空白，本文从敌对视角出发，系统研究数据集审计。我们首先提出了一种新的分类法，基于现有方法对内部特征（固有于数据）和外部特征（为审计引入的人工特征）的依赖程度进行分类。随后，我们定义了两类主要攻击类型：用于隐藏数据使用情况的逃避攻击和用于错误指责未使用数据集的伪造攻击。基于现有方法的理解和攻击目标，我们进一步提出了系统性的攻击策略：逃避攻击领域的解耦、删除和检测策略；伪造领域的基于敌对样本的方法。这些定义和策略促使我们建立了一个新的基准——DATABench，包括17种逃避攻击、5种伪造攻击和9种代表性的审计方法。使用DATABench进行广泛评估表明，在敌对环境中，评估的审计方法均不足以表现出足够的鲁棒性和独特性。这些发现强调了开发能够抵御复杂敌对操作的更安全可靠的数据集审计方法的迫切需求。代码可在以下链接获得。 

---
# Self-Review Framework for Enhancing Instruction Following Capability of LLM 

**Title (ZH)**: 增强LLM指令遵循能力的自我审查框架 

**Authors**: Sihyun Park  

**Link**: [PDF](https://arxiv.org/pdf/2507.05598)  

**Abstract**: Various techniques have been proposed to improve large language models (LLMs) adherence to formatting and instruction constraints. One of the most effective approaches involves utilizing high-quality data generated by powerful models. However, such models often fail to fully comply with complex instructions in a single generation. To address this limitation, iterative revision methods have been introduced. Nevertheless, as the number of data points and revision iterations increases, the associated monetary costs grow significantly. As a resource-efficient alternative, methods have been proposed that leverage high-performance evaluation tools to compensate for the limited self-evaluation capabilities of open-source LLMs. However, these approaches often lead to a degradation in output quality due to excessive revision. To overcome these challenges, we propose Re5, a self-evaluation and revision framework designed to enhance instruction-following performance while preserving the quality of the generated content. Re5 extracts task and constraint components from user instructions, performs structural evaluations to prevent error accumulation, and applies fine-grained constraint-specific content evaluations followed by selective revisions. This process ensures precise and quality-preserving improvements. The final high-quality outputs are used for alignment tuning, enabling long-term alignment improvements through a data-centric iterative refinement loop. Experimental results demonstrate that Re5 achieves instruction-following performance comparable to models trained on data generated by GPT-4o-mini, a high-performance model, even with a small amount of data while maintaining response quality with a 64.24%-win rate over the non-revised initial responses. These results validate Re5 as an efficient and effective solution for enhancing instruction adherence with minimal external supervision. 

**Abstract (ZH)**: Re5：一种自评与修订框架以提高指令遵循性能并保持内容质量 

---
# The Fourier Spectral Transformer Networks For Efficient and Generalizable Nonlinear PDEs Prediction 

**Title (ZH)**: 傅里叶谱变换网络及其在高效和泛化非线性偏微分方程预测中的应用 

**Authors**: Beibei Li  

**Link**: [PDF](https://arxiv.org/pdf/2507.05584)  

**Abstract**: In this work we propose a unified Fourier Spectral Transformer network that integrates the strengths of classical spectral methods and attention based neural architectures. By transforming the original PDEs into spectral ordinary differential equations, we use high precision numerical solvers to generate training data and use a Transformer network to model the evolution of the spectral coefficients. We demonstrate the effectiveness of our approach on the two dimensional incompressible Navier-Stokes equations and the one dimensional Burgers' equation. The results show that our spectral Transformer can achieve highly accurate long term predictions even with limited training data, better than traditional numerical methods and machine learning methods in forecasting future flow dynamics. The proposed framework generalizes well to unseen data, bringing a promising paradigm for real time prediction and control of complex dynamical systems. 

**Abstract (ZH)**: 一种结合经典谱方法和基于注意力的神经架构优势的统一傅里叶谱变换网络 

---
# Prompt Migration: Stabilizing GenAI Applications with Evolving Large Language Models 

**Title (ZH)**: 提示迁移：使用演化的大型语言模型稳定GenAI应用 

**Authors**: Shivani Tripathi, Pushpanjali Nema, Aditya Halder, Shi Qiao, Alekh Jindal  

**Link**: [PDF](https://arxiv.org/pdf/2507.05573)  

**Abstract**: Generative AI is transforming business applications by enabling natural language interfaces and intelligent automation. However, the underlying large language models (LLMs) are evolving rapidly and so prompting them consistently is a challenge. This leads to inconsistent and unpredictable application behavior, undermining the reliability that businesses require for mission-critical workflows. In this paper, we introduce the concept of prompt migration as a systematic approach to stabilizing GenAI applications amid changing LLMs. Using the Tursio enterprise search application as a case study, we analyze the impact of successive GPT model upgrades, detail our migration framework including prompt redesign and a migration testbed, and demonstrate how these techniques restore application consistency. Our results show that structured prompt migration can fully recover the application reliability that was lost due to model drift. We conclude with practical lessons learned, emphasizing the need for prompt lifecycle management and robust testing to ensure dependable GenAI-powered business applications. 

**Abstract (ZH)**: 生成式AI正在通过启用自然语言界面和智能自动化来变革商业应用。然而，底层的大语言模型（LLMs）正在迅速演变，因此持续有效地提示它们是一个挑战。这导致应用程序行为不一致且不可预测，削弱了企业对于关键业务流程所需可靠性的要求。在本文中，我们介绍了一种系统化的提示迁移概念，以在不断变化的大语言模型中稳定生成式AI应用。以Tursio企业搜索应用为例，我们分析了连续升级的GPT模型的影响，详细介绍了我们的迁移框架，包括提示重新设计和迁移测试平台，并展示了这些技术如何恢复应用程序的一致性。我们的结果显示，结构化的提示迁移可以完全恢复因模型漂移而丢失的应用可靠性。我们总结了实用的经验教训，强调了提示生命周期管理和稳健测试对于确保可靠的生成式AI驱动业务应用的重要性。 

---
# Search-based Selection of Metamorphic Relations for Optimized Robustness Testing of Large Language Models 

**Title (ZH)**: 基于搜索的元变换关系选择以优化大型语言模型的稳健性测试 

**Authors**: Sangwon Hyun, Shaukat Ali, M. Ali Babar  

**Link**: [PDF](https://arxiv.org/pdf/2507.05565)  

**Abstract**: Assessing the trustworthiness of Large Language Models (LLMs), such as robustness, has garnered significant attention. Recently, metamorphic testing that defines Metamorphic Relations (MRs) has been widely applied to evaluate the robustness of LLM executions. However, the MR-based robustness testing still requires a scalable number of MRs, thereby necessitating the optimization of selecting MRs. Most extant LLM testing studies are limited to automatically generating test cases (i.e., MRs) to enhance failure detection. Additionally, most studies only considered a limited test space of single perturbation MRs in their evaluation of LLMs. In contrast, our paper proposes a search-based approach for optimizing the MR groups to maximize failure detection and minimize the LLM execution cost. Moreover, our approach covers the combinatorial perturbations in MRs, facilitating the expansion of test space in the robustness assessment. We have developed a search process and implemented four search algorithms: Single-GA, NSGA-II, SPEA2, and MOEA/D with novel encoding to solve the MR selection problem in the LLM robustness testing. We conducted comparative experiments on the four search algorithms along with a random search, using two major LLMs with primary Text-to-Text tasks. Our statistical and empirical investigation revealed two key findings: (1) the MOEA/D algorithm performed the best in optimizing the MR space for LLM robustness testing, and (2) we identified silver bullet MRs for the LLM robustness testing, which demonstrated dominant capabilities in confusing LLMs across different Text-to-Text tasks. In LLM robustness assessment, our research sheds light on the fundamental problem for optimized testing and provides insights into search-based solutions. 

**Abstract (ZH)**: 评估大规模语言模型可信性的可信度，如鲁棒性，引起了广泛关注。最近，定义元formeorphic关系（MRs）的元formeorphic测试已被广泛应用于评估大规模语言模型执行的鲁棒性。然而，基于MR的鲁棒性测试仍需要大量的MR，因此需要优化MR的选择。现有的大多数大规模语言模型测试研究仅限于自动生成测试用例（即MRs），以提高故障检测能力。此外，大多数研究仅在其对大规模语言模型的评估中考虑了单变异MR的一个有限测试空间。相比之下，我们提出了一种基于搜索的方法，以优化MR组，最大化故障检测并最小化大规模语言模型执行成本。此外，我们的方法涵盖了MR中的组合变异，便于在鲁棒性评估中扩展测试空间。我们开发了搜索过程并实现了四种搜索算法：单GA、NSGA-II、SPEA2和MOEA/D，并采用新型编码来解决大规模语言模型鲁棒性测试中的MR选择问题。我们在两种主要的大规模语言模型上与随机搜索进行了比较实验，这些模型主要执行文本到文本任务。我们的统计和实证调查揭示了两个关键发现：（1）MOEA/D算法在优化大规模语言模型鲁棒性测试的MR空间方面表现最佳；（2）我们识别出了大规模语言模型鲁棒性测试的银弹MR，这些MR在不同文本到文本任务中展示了显著的混淆能力。在大规模语言模型的鲁棒性评估中，我们的研究照亮了优化测试的基本问题，并为基于搜索的方法提供了见解。 

---
# MP-ALOE: An r2SCAN dataset for universal machine learning interatomic potentials 

**Title (ZH)**: MP-ALOE: 一个适用于通用机器学习原子势的r2SCAN数据集 

**Authors**: Matthew C. Kuner, Aaron D. Kaplan, Kristin A. Persson, Mark Asta, Daryl C. Chrzan  

**Link**: [PDF](https://arxiv.org/pdf/2507.05559)  

**Abstract**: We present MP-ALOE, a dataset of nearly 1 million DFT calculations using the accurate r2SCAN meta-generalized gradient approximation. Covering 89 elements, MP-ALOE was created using active learning and primarily consists of off-equilibrium structures. We benchmark a machine learning interatomic potential trained on MP-ALOE, and evaluate its performance on a series of benchmarks, including predicting the thermochemical properties of equilibrium structures; predicting forces of far-from-equilibrium structures; maintaining physical soundness under static extreme deformations; and molecular dynamic stability under extreme temperatures and pressures. MP-ALOE shows strong performance on all of these benchmarks, and is made public for the broader community to utilize. 

**Abstract (ZH)**: MP-ALOE：使用r2SCAN元广义梯度近似进行近100万次DFT计算的数据集 

---
# AI Agent Smart Contract Exploit Generation 

**Title (ZH)**: AI代理智能合约漏洞生成 

**Authors**: Arthur Gervais, Liyi Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2507.05558)  

**Abstract**: We present A1, an agentic execution driven system that transforms any LLM into an end-to-end exploit generator. A1 has no hand-crafted heuristics and provides the agent with six domain-specific tools that enable autonomous vulnerability discovery. The agent can flexibly leverage these tools to understand smart contract behavior, generate exploit strategies, test them on blockchain states, and refine approaches based on execution feedback. All outputs are concretely validated to eliminate false positives.
The evaluation across 36 real-world vulnerable contracts on Ethereum and Binance Smart Chain demonstrates a 62.96% (17 out of 27) success rate on the VERITE benchmark. Beyond the VERITE dataset, A1 identified 9 additional vulnerable contracts, with 5 cases occurring after the strongest model's training cutoff date. Across all 26 successful cases, A1 extracts up to 8.59 million USD per case and 9.33 million USD total. Through 432 experiments across six LLMs, we analyze iteration-wise performance showing diminishing returns with average marginal gains of +9.7%, +3.7%, +5.1%, and +2.8% for iterations 2-5 respectively, with per-experiment costs ranging $0.01-$3.59. A Monte Carlo analysis of 19 historical attacks shows success probabilities of 85.9%-88.8% without detection delays.
We investigate whether an attacker or a defender benefits most from deploying A1 as a continuous on-chain scanning system. Our model shows that OpenAI's o3-pro maintains profitability up to a 30.0 days scanning delay at 0.100% vulnerability incidence rates, while faster models require >=1.000% rates to break-even. The findings exposes a troubling asymmetry: at 0.1% vulnerability rates, attackers achieve an on-chain scanning profitability at a $6000 exploit value, while defenders require $60000, raising fundamental questions about whether AI agents inevitably favor exploitation over defense. 

**Abstract (ZH)**: 我们呈现了一个名为A1的由智能执行驱动的系统，它可以将任何LLM转换为端到端的漏洞利用生成器。A1没有任何手工制作的启发式方法，并为智能体提供了六个特定领域的工具，以实现自主的漏洞发现。智能体可以灵活地利用这些工具来理解智能合约行为，生成利用策略，在区块链状态下测试它们，并根据执行反馈改进方法。所有输出均进行了具体验证，以消除假阳性。

在以太坊和币安智能链上的36个真实世界的漏洞合约的评估中，A1在VERITE基准测试中的成功率为62.96%（27个中的17个）。超越VERITE数据集，A1还识别出9个额外的漏洞合约，其中有5个案例发生在最强模型训练截止日期之后。在所有26个成功案例中，A1每案例提取高达859万美元，总计933万美元。通过在六种LLM上进行的432次实验分析迭代性能，显示随迭代增加，边际收益逐渐减少，分别在第2至第5迭代中增加9.7%、3.7%、5.1%和2.8%，每次实验成本范围从0.01美元到3.59美元。通过对19起历史攻击的蒙特卡洛分析显示，在无检测延迟的情况下，成功概率为85.9%-88.8%。

我们探讨了部署A1作为持续链上扫描系统时攻击者和防御者哪一方能获得更大收益。我们的模型显示，OpenAI的o3-pro在0.100%漏洞发生率下，即使有30.0天的扫描延迟也能保持盈利，而更快的模型需要至少1.000%的漏洞发生率才能达到盈亏平衡。这项研究揭示了一个令人担忧的不对称性：在0.1%的漏洞发生率下，攻击者在6000美元的利用价值下就能实现链上扫描盈利，而防御者则需要60000美元，从而引发了关于人工智能代理是否不可避免地倾向于利用而非防御的基本问题。 

---
# The Ethical Implications of AI in Creative Industries: A Focus on AI-Generated Art 

**Title (ZH)**: AI在创意产业中的伦理 implications：以AI生成艺术为例 

**Authors**: Prerana Khatiwada, Joshua Washington, Tyler Walsh, Ahmed Saif Hamed, Lokesh Bhatta  

**Link**: [PDF](https://arxiv.org/pdf/2507.05549)  

**Abstract**: As Artificial Intelligence (AI) continues to grow daily, more exciting (and somewhat controversial) technology emerges every other day. As we see the advancements in AI, we see more and more people becoming skeptical of it. This paper explores the complications and confusion around the ethics of generative AI art. We delve deep into the ethical side of AI, specifically generative art. We step back from the excitement and observe the impossible conundrums that this impressive technology produces. Covering environmental consequences, celebrity representation, intellectual property, deep fakes, and artist displacement. Our research found that generative AI art is responsible for increased carbon emissions, spreading misinformation, copyright infringement, unlawful depiction, and job displacement. In light of this, we propose multiple possible solutions for these problems. We address each situation's history, cause, and consequences and offer different viewpoints. At the root of it all, though, the central theme is that generative AI Art needs to be correctly legislated and regulated. 

**Abstract (ZH)**: 随着人工智能（AI）的不断发展，每天都会出现更加令人兴奋（也有些争议）的技术。随着我们见证人工智能的进步，越来越多的人对其表示怀疑。本文探讨生成式AI艺术的伦理复杂性和困惑。我们深入研究AI的伦理问题，特别是生成艺术。我们放慢脚步，退一步观察这一 impressive 技术所产生的一系列难以解决的问题，涵盖环境后果、名人形象、知识产权、深度假信息和艺术家失业等问题。我们的研究发现生成式AI艺术增加了碳排放、传播虚假信息、侵犯版权、非法描绘和就业替代。鉴于此，我们提出了多种可能的解决方案。我们针对每个情况的历史、成因和后果提供了不同的观点。尽管如此，核心主题是生成式AI艺术需要正确立法和监管。 

---
# Robust Learning on Noisy Graphs via Latent Space Constraints with External Knowledge 

**Title (ZH)**: 基于潜在空间约束与外部知识的鲁棒图学习方法 

**Authors**: Chunhui Gu, Mohammad Sadegh Nasr, James P. Long, Kim-Anh Do, Ehsan Irajizad  

**Link**: [PDF](https://arxiv.org/pdf/2507.05540)  

**Abstract**: Graph Neural Networks (GNNs) often struggle with noisy edges. We propose Latent Space Constrained Graph Neural Networks (LSC-GNN) to incorporate external "clean" links and guide embeddings of a noisy target graph. We train two encoders--one on the full graph (target plus external edges) and another on a regularization graph excluding the target's potentially noisy links--then penalize discrepancies between their latent representations. This constraint steers the model away from overfitting spurious edges. Experiments on benchmark datasets show LSC-GNN outperforms standard and noise-resilient GNNs in graphs subjected to moderate noise. We extend LSC-GNN to heterogeneous graphs and validate it on a small protein-metabolite network, where metabolite-protein interactions reduce noise in protein co-occurrence data. Our results highlight LSC-GNN's potential to boost predictive performance and interpretability in settings with noisy relational structures. 

**Abstract (ZH)**: 受限潜空间图神经网络（LSC-GNN）：融合外部清洁链接并指导嘈杂目标图的嵌入 

---
# Mitigating Shortcut Learning with InterpoLated Learning 

**Title (ZH)**: 利用插值学习 mitigating 短路学习 

**Authors**: Michalis Korakakis, Andreas Vlachos, Adrian Weller  

**Link**: [PDF](https://arxiv.org/pdf/2507.05527)  

**Abstract**: Empirical risk minimization (ERM) incentivizes models to exploit shortcuts, i.e., spurious correlations between input attributes and labels that are prevalent in the majority of the training data but unrelated to the task at hand. This reliance hinders generalization on minority examples, where such correlations do not hold. Existing shortcut mitigation approaches are model-specific, difficult to tune, computationally expensive, and fail to improve learned representations. To address these issues, we propose InterpoLated Learning (InterpoLL) which interpolates the representations of majority examples to include features from intra-class minority examples with shortcut-mitigating patterns. This weakens shortcut influence, enabling models to acquire features predictive across both minority and majority examples. Experimental results on multiple natural language understanding tasks demonstrate that InterpoLL improves minority generalization over both ERM and state-of-the-art shortcut mitigation methods, without compromising accuracy on majority examples. Notably, these gains persist across encoder, encoder-decoder, and decoder-only architectures, demonstrating the method's broad applicability. 

**Abstract (ZH)**: 中介学习（InterpoLated Learning）：通过插值 Majority 示例以纳入即时类 Minority 示例的 shortcut-mitigating 特征来改善少数类泛化 

---
# Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications 

**Title (ZH)**: 提升医疗工作者能力的语言模型：在两个实际临床应用中的语音转录结构化 

**Authors**: Jean-Philippe Corbeil, Asma Ben Abacha, George Michalopoulos, Phillip Swazinna, Miguel Del-Agua, Jerome Tremblay, Akila Jeeson Daniel, Cari Bader, Kevin Cho, Pooja Krishnan, Nathan Bodenstab, Thomas Lin, Wenxuan Teng, Francois Beaulieu, Paul Vozila  

**Link**: [PDF](https://arxiv.org/pdf/2507.05517)  

**Abstract**: Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong performance on clinical natural language processing (NLP) tasks across multiple medical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular reporting from nurse dictations and medical order extraction from doctor-patient consultations - remain underexplored due to data scarcity and sensitivity, despite active industry efforts. Practical solutions to these real-world clinical tasks can significantly reduce the documentation burden on healthcare providers, allowing greater focus on patient care. In this paper, we investigate these two challenging tasks using private and open-source clinical datasets, evaluating the performance of both open- and closed-weight LLMs, and analyzing their respective strengths and limitations. Furthermore, we propose an agentic pipeline for generating realistic, non-sensitive nurse dictations, enabling structured extraction of clinical observations. To support further research in both areas, we release SYNUR and SIMORD, the first open-source datasets for nurse observation extraction and medical order extraction. 

**Abstract (ZH)**: 大型语言模型（LLMs）如GPT-4o和o1在多个医学基准上的临床自然语言处理（NLP）任务中表现出色。然而，由于数据稀缺性和敏感性，两类高影响的NLP任务——护士口述的结构化表格报告生成和从医生-患者咨询中提取医疗订单——仍待进一步探索。针对这些实际临床任务的实用解决方案可以显著减轻医疗提供者的文书工作负担，使他们能够更加专注于患者护理。本文利用私有和开源的临床数据集，研究了这两种挑战性的任务，评估了开放权重和闭合权重LLM的表现，并分析了各自的优缺点。此外，我们提出了一种代理管道，用于生成现实且非敏感的护士口述记录，从而实现临床观察的结构化提取。为了支持这两个领域的进一步研究，我们发布了SYNUR和SIMORD，这是第一个用于护士观察提取和医疗订单提取的开源数据集。 

---
# Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model 

**Title (ZH)**: Llama Nemoretriever Colembed：性能最优的文本-图像检索模型 

**Authors**: Mengyao Xu, Gabriel Moreira, Ronay Ak, Radek Osmulski, Yauhen Babakhin, Zhiding Yu, Benedikt Schifferer, Even Oldridge  

**Link**: [PDF](https://arxiv.org/pdf/2507.05513)  

**Abstract**: Motivated by the growing demand for retrieval systems that operate across modalities, we introduce llama-nemoretriever-colembed, a unified text-image retrieval model that delivers state-of-the-art performance across multiple benchmarks. We release two model variants, 1B and 3B. The 3B model achieves state of the art performance, scoring NDCG@5 91.0 on ViDoRe V1 and 63.5 on ViDoRe V2, placing first on both leaderboards as of June 27, 2025.
Our approach leverages the NVIDIA Eagle2 Vision-Language model (VLM), modifies its architecture by replacing causal attention with bidirectional attention, and integrates a ColBERT-style late interaction mechanism to enable fine-grained multimodal retrieval in a shared embedding space. While this mechanism delivers superior retrieval accuracy, it introduces trade-offs in storage and efficiency. We provide a comprehensive analysis of these trade-offs. Additionally, we adopt a two-stage training strategy to enhance the model's retrieval capabilities. 

**Abstract (ZH)**: 受跨模态检索系统需求增长的驱动，我们介绍了llama-nemoretriever-colembed，这是一种统一的文本-图像检索模型，Across Modalities的检索模型，在多个基准测试中实现了最先进的性能。我们发布了两种模型变体，1B和3B。3B模型在ViDoRe V1和ViDoRe V2上分别取得了NDCG@5 91.0和63.5的最佳性能，截至2025年6月27日，在两个排行榜上都获得第一。 

---
# Disappearing Ink: Obfuscation Breaks N-gram Code Watermarks in Theory and Practice 

**Title (ZH)**: 消失的墨水：模糊化在理论和实践上破解N-克gram代码水印 

**Authors**: Gehao Zhang, Eugene Bagdasarian, Juan Zhai, Shiqing Ma  

**Link**: [PDF](https://arxiv.org/pdf/2507.05512)  

**Abstract**: Distinguishing AI-generated code from human-written code is becoming crucial for tasks such as authorship attribution, content tracking, and misuse detection. Based on this, N-gram-based watermarking schemes have emerged as prominent, which inject secret watermarks to be detected during the generation.
However, their robustness in code content remains insufficiently evaluated. Most claims rely solely on defenses against simple code transformations or code optimizations as a simulation of attack, creating a questionable sense of robustness. In contrast, more sophisticated schemes already exist in the software engineering world, e.g., code obfuscation, which significantly alters code while preserving functionality. Although obfuscation is commonly used to protect intellectual property or evade software scanners, the robustness of code watermarking techniques against such transformations remains largely unexplored.
In this work, we formally model the code obfuscation and prove the impossibility of N-gram-based watermarking's robustness with only one intuitive and experimentally verified assumption, distribution consistency, satisfied. Given the original false positive rate of the watermarking detection, the ratio that the detector failed on the watermarked code after obfuscation will increase to 1 - fpr.
The experiments have been performed on three SOTA watermarking schemes, two LLMs, two programming languages, four code benchmarks, and four obfuscators. Among them, all watermarking detectors show coin-flipping detection abilities on obfuscated codes (AUROC tightly surrounds 0.5). Among all models, watermarking schemes, and datasets, both programming languages own obfuscators that can achieve attack effects with no detection AUROC higher than 0.6 after the attack. Based on the theoretical and practical observations, we also proposed a potential path of robust code watermarking. 

**Abstract (ZH)**: 区分AI生成代码与人工编写代码对于作者Attribution、内容追踪和误用检测等任务而言变得日益重要。基于此，基于N-gram的水印方案已 emerge 为突出的方法，这些方案在生成过程中注入秘密水印以供检测。然而，它们在代码内容上的稳健性尚未充分评估。大多数声明仅依赖于对简单代码变换或代码优化的防御作为攻击模拟，这创建了一种可疑的稳健性感觉。相比之下，软件工程领域中已经存在更复杂的方案，例如代码混淆，这些方案能显著改变代码结构同时保留功能。尽管混淆通常用于保护知识产权或逃避软件扫描器，代码水印技术对于此类变换的稳健性尚未得到充分探索。
在本工作中，我们正式建模代码混淆，并在仅满足一个直观且实验验证的假设——分布一致性的情况下证明基于N-gram的水印方案的稳健性是不可能的。根据水印检测的原始误报率，混淆后的水标记代码检测失败的比例将增加至1 - fpr。
我们在三个SOTA水印方案、两个LLMs、两种编程语言、四种代码基准和四种混淆器上进行了实验。其中，所有水印检测器在混淆代码上的检测能力都表现出随机硬币投掷的效果（AUROC紧密围绕0.5）。在所有模型、水印方案和数据集的组合中，两种编程语言中的混淆器均能在攻击后使水印检测AUROC低于0.6而不被检测到。基于理论和实证观察，我们还提出了一种潜在的稳健代码水印路径。 

---
# Explainable Hierarchical Deep Learning Neural Networks (Ex-HiDeNN) 

**Title (ZH)**: 可解释的分层深度学习神经网络（Ex-HiDeNN） 

**Authors**: Reza T. Batley, Chanwook Park, Wing Kam Liu, Sourav Saha  

**Link**: [PDF](https://arxiv.org/pdf/2507.05498)  

**Abstract**: Data-driven science and computation have advanced immensely to construct complex functional relationships using trainable parameters. However, efficiently discovering interpretable and accurate closed-form expressions from complex dataset remains a challenge. The article presents a novel approach called Explainable Hierarchical Deep Learning Neural Networks or Ex-HiDeNN that uses an accurate, frugal, fast, separable, and scalable neural architecture with symbolic regression to discover closed-form expressions from limited observation. The article presents the two-step Ex-HiDeNN algorithm with a separability checker embedded in it. The accuracy and efficiency of Ex-HiDeNN are tested on several benchmark problems, including discerning a dynamical system from data, and the outcomes are reported. Ex-HiDeNN generally shows outstanding approximation capability in these benchmarks, producing orders of magnitude smaller errors compared to reference data and traditional symbolic regression. Later, Ex-HiDeNN is applied to three engineering applications: a) discovering a closed-form fatigue equation, b) identification of hardness from micro-indentation test data, and c) discovering the expression for the yield surface with data. In every case, Ex-HiDeNN outperformed the reference methods used in the literature. The proposed method is built upon the foundation and published works of the authors on Hierarchical Deep Learning Neural Network (HiDeNN) and Convolutional HiDeNN. The article also provides a clear idea about the current limitations and future extensions of Ex-HiDeNN. 

**Abstract (ZH)**: 数据驱动的科学与计算在构建复杂功能性关系方面取得了巨大进展，使用可训练参数。然而，从复杂数据集中高效发现可解释且准确的闭合形式表达式仍然是一个挑战。本文提出了一种名为可解释层次深度学习神经网络（Ex-HiDeNN）的新型方法，该方法结合了符号回归，使用精确、节省、快速、可分离和可扩展的神经架构从有限观察中发现闭合形式表达式。本文介绍了嵌入了可分离性检查器的两步Ex-HiDeNN算法，并对其进行基准测试，结果显示Ex-HiDeNN在多个基准问题上的准确性和效率，特别是在从数据中辨识动力学系统方面。Ex-HiDeNN在这些基准测试中一般表现出色，相比参考数据和传统符号回归产生的误差小了几个数量级。然后，Ex-HiDeNN 应用于三个工程应用：a) 发现闭合形式的疲劳方程；b) 从微观压痕测试数据中识别硬度；c) 从数据中发现屈服面表达式。在每一项应用中，Ex-HiDeNN 都优于文献中使用的参考方法。该方法建立在作者关于层次深度学习神经网络（HiDeNN）和卷积HiDeNN 的基础和已发表工作之上。本文还详细介绍了Ex-HiDeNN 当前的局限性和未来的发展方向。 

---
# Cloud Diffusion Part 1: Theory and Motivation 

**Title (ZH)**: 云扩散理论与动机 Part 1 

**Authors**: Andrew Randono  

**Link**: [PDF](https://arxiv.org/pdf/2507.05496)  

**Abstract**: Diffusion models for image generation function by progressively adding noise to an image set and training a model to separate out the signal from the noise. The noise profile used by these models is white noise -- that is, noise based on independent normal distributions at each point whose mean and variance is independent of the scale. By contrast, most natural image sets exhibit a type of scale invariance in their low-order statistical properties characterized by a power-law scaling. Consequently, natural images are closer (in a quantifiable sense) to a different probability distribution that emphasizes large scale correlations and de-emphasizes small scale correlations. These scale invariant noise profiles can be incorporated into diffusion models in place of white noise to form what we will call a ``Cloud Diffusion Model". We argue that these models can lead to faster inference, improved high-frequency details, and greater controllability. In a follow-up paper, we will build and train a Cloud Diffusion Model that uses scale invariance at a fundamental level and compare it to classic, white noise diffusion models. 

**Abstract (ZH)**: 尺度不变噪声介导的扩散模型 

---
# Epistemically-guided forward-backward exploration 

**Title (ZH)**: 知识引导的正向-逆向探索 

**Authors**: Núria Armengol Urpí, Marin Vlastelica, Georg Martius, Stelian Coros  

**Link**: [PDF](https://arxiv.org/pdf/2507.05477)  

**Abstract**: Zero-shot reinforcement learning is necessary for extracting optimal policies in absence of concrete rewards for fast adaptation to future problem settings. Forward-backward representations (FB) have emerged as a promising method for learning optimal policies in absence of rewards via a factorization of the policy occupancy measure. However, up until now, FB and many similar zero-shot reinforcement learning algorithms have been decoupled from the exploration problem, generally relying on other exploration algorithms for data collection. We argue that FB representations should fundamentally be used for exploration in order to learn more efficiently. With this goal in mind, we design exploration policies that arise naturally from the FB representation that minimize the posterior variance of the FB representation, hence minimizing its epistemic uncertainty. We empirically demonstrate that such principled exploration strategies improve sample complexity of the FB algorithm considerably in comparison to other exploration methods. Code is publicly available at this https URL. 

**Abstract (ZH)**: 在缺乏具体奖励的情况下，零样本强化学习是提取最优策略以快速适应未来问题设置的必要方法。前向-后向表示（FB）已成为一种有前途的方法，通过策略占据度量的因素分解来学习最优策略。然而，迄今为止，FB和许多类似的零样本强化学习算法通常与探索问题脱钩，一般依赖于其他探索算法来收集数据。我们argue应从根本上利用FB表示来进行探索，以实现更高效的机器学习。为此，我们设计了一种自然源自FB表示的探索策略，该策略旨在最小化FB表示的后验方差，从而最小化其认知不确定性。实验结果显示，这样的原理性探索策略大大提高了FB算法的样本复杂度，相较于其他探索方法。代码已公开于此 <https://>。 

---
# Inaugural MOASEI Competition at AAMAS'2025: A Technical Report 

**Title (ZH)**: Inaugural MOASEI竞赛在AAMAS'2025：技术报告 

**Authors**: Ceferino Patino, Tyler J. Billings, Alireza Saleh Abadi, Daniel Redder, Adam Eck, Prashant Doshi, Leen-Kiat Soh  

**Link**: [PDF](https://arxiv.org/pdf/2507.05469)  

**Abstract**: We present the Methods for Open Agent Systems Evaluation Initiative (MOASEI) Competition, a multi-agent AI benchmarking event designed to evaluate decision-making under open-world conditions. Built on the free-range-zoo environment suite, MOASEI introduced dynamic, partially observable domains with agent and task openness--settings where entities may appear, disappear, or change behavior over time. The 2025 competition featured three tracks--Wildfire, Rideshare, and Cybersecurity--each highlighting distinct dimensions of openness and coordination complexity. Eleven teams from international institutions participated, with four of those teams submitting diverse solutions including graph neural networks, convolutional architectures, predictive modeling, and large language model--driven meta--optimization. Evaluation metrics centered on expected utility, robustness to perturbations, and responsiveness to environmental change. The results reveal promising strategies for generalization and adaptation in open environments, offering both empirical insight and infrastructure for future research. This report details the competition's design, findings, and contributions to the open-agent systems research community. 

**Abstract (ZH)**: 开放代理系统评估倡议（MOASEI）竞赛：多代理AI基准评估活动设计与发现 

---
# 2048: Reinforcement Learning in a Delayed Reward Environment 

**Title (ZH)**: 2048: 延迟奖励环境中的强化学习 

**Authors**: Prady Saligram, Tanvir Bhathal, Robby Manihani  

**Link**: [PDF](https://arxiv.org/pdf/2507.05465)  

**Abstract**: Delayed and sparse rewards present a fundamental obstacle for reinforcement-learning (RL) agents, which struggle to assign credit for actions whose benefits emerge many steps later. The sliding-tile game 2048 epitomizes this challenge: although frequent small score changes yield immediate feedback, they often mislead agents into locally optimal but globally suboptimal strategies. In this work, we introduce a unified, distributional multi-step RL framework designed to directly optimize long-horizon performance. Using the open source Gym-2048 environment we develop and compare four agent variants: standard DQN, PPO, QR-DQN (Quantile Regression DQN), and a novel Horizon-DQN (H-DQN) that integrates distributional learning, dueling architectures, noisy networks, prioritized replay, and more. Empirical evaluation reveals a clear hierarchy in effectiveness: max episode scores improve from 3.988K (DQN) to 5.756K (PPO), 8.66K (QR-DQN), and 18.21K (H-DQN), with H-DQN reaching the 2048 tile. Upon scaling H-DQN it reaches a max score 41.828K and a 4096 tile. These results demonstrate that distributional, multi-step targets substantially enhance performance in sparse-reward domains, and they suggest promising avenues for further gains through model-based planning and curriculum learning. 

**Abstract (ZH)**: 延迟且稀疏的奖励是强化学习（RL）代理面临的根本障碍，使其难以对多种步骤后才显现利益的动作进行正确的信用分配。滑动瓷砖游戏2048完美体现了这一挑战：尽管频繁的小分变化提供了即时反馈，但它们往往引导代理采取局部最优但全局次优的策略。在此工作中，我们提出了一种统一的分布式多步RL框架，旨在直接优化长期性能。我们使用开源的Gym-2048环境开发并比较了四种代理变体：标准DQN、PPO、QR-DQN（分位回归DQN）以及一种新颖的Horizon-DQN（H-DQN），该变体整合了分布式学习、对拼网络、噪音网络、优先经验回放等多种技术。实证评估表明，最大回合得分从DQN的3.988K提升到PPO的5.756K、QR-DQN的8.66K以及H-DQN的18.21K，并且H-DQN达到了2048瓷砖。在扩展H-DQN后，其最大得分为41.828K，且获得了4096瓷砖。这些结果表明，分布式的多步目标在稀疏奖励领域显著提升了性能，并且表明基于模型的规划和层次学习有望进一步提升性能。 

---
# Driving as a Diagnostic Tool: Scenario-based Cognitive Assessment in Older Drivers From Driving Video 

**Title (ZH)**: 基于场景的认知评估：来自驾驶视频的老年驾驶员诊断工具 

**Authors**: Md Zahid Hasan, Guillermo Basulto-Elias, Jun Ha Chang, Sahuna Hallmark, Matthew Rizzo, Anuj Sharma, Soumik Sarkar  

**Link**: [PDF](https://arxiv.org/pdf/2507.05463)  

**Abstract**: We introduce scenario-based cognitive status identification in older drivers from Naturalistic driving videos and large vision models. In recent times, cognitive decline, including Alzheimer's disease (AD) and mild cognitive impairment (MCI), is often underdiagnosed due to the time-consuming and costly nature of current diagnostic methods. By analyzing real-world driving behavior captured through in-vehicle systems, this research aims to extract "digital fingerprints" that correlate with functional decline and clinical features of MCI and AD. Moreover, modern large vision models can draw meaningful insights from everyday driving patterns of older patients to early detect cognitive decline. We propose a framework that uses large vision models and naturalistic driving videos to analyze driver behavior, classify cognitive status and predict disease progression. We leverage the strong relationship between real-world driving behavior as an observation of the current cognitive status of the drivers where the vehicle can be utilized as a "diagnostic tool". Our method identifies early warning signs of functional impairment, contributing to proactive intervention strategies. This work enhances early detection and supports the development of scalable, non-invasive monitoring systems to mitigate the growing societal and economic burden of cognitive decline in the aging population. 

**Abstract (ZH)**: 基于自然驾驶视频和大型视觉模型的老年驾驶员情景认知状态识别 

---
# ModelCitizens:Representing Community Voices in Online Safety 

**Title (ZH)**: Model Citizens: 表征社区声音的在线安全模型 

**Authors**: Ashima Suvarna, Christina Chance, Hamid Palangi, Sophie Hao, Thomas Hartvigsen, Saadia Gabriel  

**Link**: [PDF](https://arxiv.org/pdf/2507.05455)  

**Abstract**: Automatic toxic language detection is critical for creating safe, inclusive online spaces. However, it is a highly subjective task, with perceptions of toxic language shaped by community norms and lived experience. Existing toxicity detection models are typically trained on annotations that collapse diverse annotator perspectives into a single ground truth, erasing important context-specific notions of toxicity such as reclaimed language. To address this, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K toxicity annotations across diverse identity groups. To capture the role of conversational context on toxicity, typical of social media posts, we augment MODELCITIZENS posts with LLM-generated conversational scenarios. State-of-the-art toxicity detection tools (e.g. OpenAI Moderation API, GPT-o4-mini) underperform on MODELCITIZENS, with further degradation on context-augmented posts. Finally, we release LLAMACITIZEN-8B and GEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS, which outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our findings highlight the importance of community-informed annotation and modeling for inclusive content moderation. 

**Abstract (ZH)**: 自动有毒语言检测对于创建安全包容的在线空间至关重要。然而，这是一项高度主观的任务，有毒语言的感知受到社区规范和生活经验的影响。现有的毒性检测模型通常通过将各种标注者的观点合并为单一ground truth来进行训练，忽略了如 reclaim 语言等重要的情境特定概念。为解决这一问题，我们引入了 MODELCITIZENS 数据集，该数据集包含 6800 条社交媒体帖子和跨不同身份群体的 40000 个毒性标注。为了捕捉对话背景在毒性中的作用，类似社交媒体帖子的特点，我们通过大型语言模型生成的对话情景对 MODELCITIZENS 帖子进行了补充。最新的毒性检测工具（例如 OpenAI Moderation API、GPT-o4-mini）在 MODELCITIZENS 上表现不佳，并且在增加背景信息的帖子上表现更差。最后，我们发布了 LLAMACITIZEN-8B 和 GEMMACITIZEN-12B 模型，这些模型基于 LLaMA 和 Gemma 并在 MODELCITIZENS 上进行微调，其在内部分布评估中的表现优于 GPT-o4-mini 5.5%。我们的研究结果突显了社区导向的标注和建模对于包容性内容管理的重要性。 

---
# On the Semantics of Large Language Models 

**Title (ZH)**: 大型语言模型的语义研究 

**Authors**: Martin Schuele  

**Link**: [PDF](https://arxiv.org/pdf/2507.05448)  

**Abstract**: Large Language Models (LLMs) such as ChatGPT demonstrated the potential to replicate human language abilities through technology, ranging from text generation to engaging in conversations. However, it remains controversial to what extent these systems truly understand language. We examine this issue by narrowing the question down to the semantics of LLMs at the word and sentence level. By examining the inner workings of LLMs and their generated representation of language and by drawing on classical semantic theories by Frege and Russell, we get a more nuanced picture of the potential semantic capabilities of LLMs. 

**Abstract (ZH)**: 大型语言模型（LLMs）如ChatGPT展示了通过技术复制人类语言能力的潜力，从文本生成到参与对话。然而，这些系统真正理解语言的程度仍有争议。我们通过将问题聚焦到LLMs的词和句义层面来探讨这一问题。通过分析LLMs的内部工作机制及其生成的语言表示，并结合Frege和Russell的经典语义理论，我们获得了一个更细致的语义能力图景。 

---
# Robotic System with AI for Real Time Weed Detection, Canopy Aware Spraying, and Droplet Pattern Evaluation 

**Title (ZH)**: 基于AI的实时杂草检测、冠层意识喷洒及液滴模式评估的机器人系统 

**Authors**: Inayat Rasool, Pappu Kumar Yadav, Amee Parmar, Hasan Mirzakhaninafchi, Rikesh Budhathoki, Zain Ul Abideen Usmani, Supriya Paudel, Ivan Perez Olivera, Eric Jone  

**Link**: [PDF](https://arxiv.org/pdf/2507.05432)  

**Abstract**: Uniform and excessive herbicide application in modern agriculture contributes to increased input costs, environmental pollution, and the emergence of herbicide resistant weeds. To address these challenges, we developed a vision guided, AI-driven variable rate sprayer system capable of detecting weed presence, estimating canopy size, and dynamically adjusting nozzle activation in real time. The system integrates lightweight YOLO11n and YOLO11n-seg deep learning models, deployed on an NVIDIA Jetson Orin Nano for onboard inference, and uses an Arduino Uno-based relay interface to control solenoid actuated nozzles based on canopy segmentation results. Indoor trials were conducted using 15 potted Hibiscus rosa sinensis plants of varying canopy sizes to simulate a range of weed patch scenarios. The YOLO11n model achieved a mean average precision (mAP@50) of 0.98, with a precision of 0.99 and a recall close to 1.0. The YOLO11n-seg segmentation model achieved a mAP@50 of 0.48, precision of 0.55, and recall of 0.52. System performance was validated using water sensitive paper, which showed an average spray coverage of 24.22% in zones where canopy was present. An upward trend in mean spray coverage from 16.22% for small canopies to 21.46% and 21.65% for medium and large canopies, respectively, demonstrated the system's capability to adjust spray output based on canopy size in real time. These results highlight the potential of combining real time deep learning with low-cost embedded hardware for selective herbicide application. Future work will focus on expanding the detection capabilities to include three common weed species in South Dakota: water hemp (Amaranthus tuberculatus), kochia (Bassia scoparia), and foxtail (Setaria spp.), followed by further validation in both indoor and field trials within soybean and corn production systems. 

**Abstract (ZH)**: 均匀且过度使用除草剂在现代农业中增加了输入成本、环境污染并促进了抗除草剂杂草的出现。为应对这些挑战，我们开发了一种基于视觉引导和AI驱动的变量率喷洒系统，能够检测杂草的存在、估计冠层大小，并实时动态调整喷头激活。该系统结合了轻量级YOLO11n和YOLO11n-seg深度学习模型，并在NVIDIA Jetson Orin Nano上进行板载推断，使用基于Arduino Uno的继电器接口根据冠层分割结果控制电磁阀喷头。室内试验使用15株不同冠层大小的木槿（Hibiscus rosa-sinensis）盆栽植物模拟了一系列杂草斑块情景。YOLO11n模型的平均平均精度（mAP@50）为0.98，精确度为0.99，召回率接近1.0。YOLO11n-seg分割模型的mAP@50为0.48，精确度为0.55，召回率为0.52。系统性能使用水敏纸验证，显示在冠层区域平均喷洒覆盖率约为24.22%。冠层从小到大（分别为小冠层16.22%、中冠层21.46%和大冠层21.65%）喷洒覆盖率的上升趋势表明该系统能够根据冠层大小实时调整喷洒输出。这些结果突显了结合实时深度学习和低成本嵌入式硬件进行选择性除草剂应用的潜力。未来工作将致力于扩大检测能力，纳入南达科他州三种常见杂草物种：水棘豆（Amaranthus tuberculatus）、emade草（Bassia scoparia）和狗尾草（Setaria spp.），并通过进一步在大豆和玉米生产系统中的室内和田间试验进行验证。 

---
# "Lost-in-the-Later": Framework for Quantifying Contextual Grounding in Large Language Models 

**Title (ZH)**: "迷失在后来": 量化大型语言模型上下文关联的框架 

**Authors**: Yufei Tao, Adam Hiatt, Rahul Seetharaman, Ameeta Agrawal  

**Link**: [PDF](https://arxiv.org/pdf/2507.05424)  

**Abstract**: Large language models are capable of leveraging both contextual and parametric knowledge but how they prioritize and integrate these sources remains underexplored. We introduce CoPE, a novel evaluation framework that systematically measures contextual knowledge (CK) and parametric knowledge (PK) across models and languages. Using our MultiWikiAtomic dataset in English, Spanish, and Danish, we analyze how large language models (LLMs) integrate context, prioritize information, and incorporate PK in open-ended question answering. Our analysis uncovers a phenomenon we call lost-in-the-later, where LLMs tend to overlook or deprioritize information that appears later in a given context, revealing a strong positional bias that affects contextual grounding. We further find that reasoning models, as well as non-reasoning models prompted with chain-of-thought (CoT), use context even less than non-reasoning models without CoT and fail to mitigate the lost-in-the-later effect. CoT prompting, in particular, results in lower recall and shorter responses, leading to degraded contextual grounding. Based on these insights, we design prompt-based methods to effectively leverage input context. A case study applying CoPE to summarization demonstrates that CK-informed prompting improves factual grounding and reduces hallucination. 

**Abstract (ZH)**: 大型语言模型能够利用上下文知识和参数知识，但它们如何优先考虑和整合这些知识来源仍待探索。我们引入了CoPE，一种新型的评估框架，系统地衡量模型和不同语言中的上下文知识（CK）和参数知识（PK）。利用我们的MultiWikiAtomic数据集（英文、西班牙文和丹麦文），我们分析了大型语言模型（LLMs）如何整合上下文、优先处理信息以及在开放式问答中融入参数知识。我们的分析揭示了一种我们称之为“后来被遗忘”的现象，即LLMs倾向于忽视或低优先级处理在给定上下文后出现的信息，这反映出一种强烈的位置偏见，影响了上下文定位。进一步的研究发现，即使使用链式思维（CoT）提示的推理模型和非推理模型，它们也比未使用CoT提示的非推理模型更少利用上下文，并且未能减轻“后来被遗忘”的影响。特别是CoT提示，导致较低的理解召回率和更短的回答长度，从而降低上下文定位的性能。基于这些见解，我们设计了基于提示的方法，以有效利用输入上下文。将CoPE应用于摘要任务的案例研究表明，基于上下文知识的提示能够提高事实定位并减少幻觉。 

---
# Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning 

**Title (ZH)**: 全局学习，本地表达：多语言推理的-gap填补- 

**Authors**: Jaedong Hwang, Kumar Tanmay, Seok-Jin Lee, Ayush Agrawal, Hamid Palangi, Kumar Ayush, Ila Fiete, Paul Pu Liang  

**Link**: [PDF](https://arxiv.org/pdf/2507.05418)  

**Abstract**: Large Language Models (LLMs) have achieved strong performance in domains like mathematics, factual QA, and code generation, yet their multilingual reasoning capabilities in these tasks remain underdeveloped. Especially for low-resource languages such as Swahili or Thai, LLMs can often misinterpret prompts or default to reasoning in English. This implicit bias toward high-resource languages undermines factual accuracy, interpretability, and trust. Current multilingual benchmarks focus only on final answers, overlooking whether models actually reason in the target language. To address this gap, we introduce GeoFact-X, a geography-based multilingual factual reasoning benchmark with annotated reasoning traces in five languages: English, Hindi, Japanese, Swahili, and Thai. We further propose BRIDGE, a novel training method that guides supervised fine-tuning and test-time reinforcement learning with a language-consistency reward to align reasoning with the input language. Finally, we develop an automatic evaluation protocol using LLM-as-a-judge to assess answer correctness and the quality and language consistency of reasoning traces, enabling nuanced and scalable analysis beyond surface-level metrics. Our results show that BRIDGE significantly enhances multilingual reasoning fidelity, demonstrating that reasoning-aware multilingual reinforcement learning is crucial for robust cross-lingual generalization. this https URL 

**Abstract (ZH)**: 大规模语言模型（LLMs）在数学、事实QA和代码生成等领域取得了出色的成绩，但在这些任务中的多语言推理能力仍较为欠缺。特别是在斯瓦希里语或泰语等低资源语言方面，LLMs 经常误读提示，或默认使用英语进行推理。这种偏向高资源语言的隐含偏见影响了事实准确性、可解释性和可信度。当前的多语言基准测试仅关注最终答案，忽视了模型是否在目标语言中进行推理。为填补这一空白，我们引入了GeoFact-X，这是一个基于地理的多语言事实推理基准，其中包含用五种语言（英语、 Hindi、日语、斯瓦希里语和泰语）标注的推理痕迹。我们还提出了BRIDGE，这是一种新的训练方法，通过语言一致性奖励引导监督微调和测试时的强化学习，使推理与输入语言保持一致。最后，我们开发了一种自动评估协议，使用LLM作为评委，评估答案的正确性和推理痕迹的质量及语言一致性，从而实现多维度和可扩展的分析，超越表面指标。我们的结果显示，BRIDGE 显著提高了多语言推理的准确性，证明了意识推理的多语言强化学习对于稳健的跨语言泛化至关重要。this https URL 

---
# EmissionNet: Air Quality Pollution Forecasting for Agriculture 

**Title (ZH)**: EmissionNet: 农业空气质量污染预报 

**Authors**: Prady Saligram, Tanvir Bhathal  

**Link**: [PDF](https://arxiv.org/pdf/2507.05416)  

**Abstract**: Air pollution from agricultural emissions is a significant yet often overlooked contributor to environmental and public health challenges. Traditional air quality forecasting models rely on physics-based approaches, which struggle to capture complex, nonlinear pollutant interactions. In this work, we explore forecasting N$_2$O agricultural emissions through evaluating popular architectures, and proposing two novel deep learning architectures, EmissionNet (ENV) and EmissionNet-Transformer (ENT). These models leverage convolutional and transformer-based architectures to extract spatial-temporal dependencies from high-resolution emissions data 

**Abstract (ZH)**: 农业排放导致的大气污染是一个重要但经常被忽视的环境和公共健康挑战。传统的空气质量预报模型依赖于基于物理的方法，难以捕捉复杂的非线性污染物交互。本文通过评估流行的架构，并提出两种新型深度学习架构EmissionNet (ENV) 和EmissionNet-Transformer (ENT)，探索通过这些模型预测N$_2$O农业排放。这些模型利用卷积和Transformer架构从高分辨率排放数据中提取时空依赖关系。 

---
# Probabilistically Tightened Linear Relaxation-based Perturbation Analysis for Neural Network Verification 

**Title (ZH)**: 基于神经网络验证的概率紧化线性松弛扰动分析 

**Authors**: Luca Marzari, Ferdinando Cicalese, Alessandro Farinelli  

**Link**: [PDF](https://arxiv.org/pdf/2507.05405)  

**Abstract**: We present $\textbf{P}$robabilistically $\textbf{T}$ightened $\textbf{Li}$near $\textbf{R}$elaxation-based $\textbf{P}$erturbation $\textbf{A}$nalysis ($\texttt{PT-LiRPA}$), a novel framework that combines over-approximation techniques from LiRPA-based approaches with a sampling-based method to compute tight intermediate reachable sets. In detail, we show that with negligible computational overhead, $\texttt{PT-LiRPA}$ exploiting the estimated reachable sets, significantly tightens the lower and upper linear bounds of a neural network's output, reducing the computational cost of formal verification tools while providing probabilistic guarantees on verification soundness. Extensive experiments on standard formal verification benchmarks, including the International Verification of Neural Networks Competition, show that our $\texttt{PT-LiRPA}$-based verifier improves robustness certificates by up to 3.31X and 2.26X compared to related work. Importantly, our probabilistic approach results in a valuable solution for challenging competition entries where state-of-the-art formal verification methods fail, allowing us to provide answers with high confidence (i.e., at least 99%). 

**Abstract (ZH)**: 概率紧化线性松弛基于扰动分析（PT-LiRPA） 

---
# Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences 

**Title (ZH)**: 控制你分享的内容：评估语言模型对隐私偏好遵守的情况 

**Authors**: Guillem Ramírez, Alexandra Birch, Ivan Titov  

**Link**: [PDF](https://arxiv.org/pdf/2507.05391)  

**Abstract**: Large language models (LLMs) are primarily accessed via commercial APIs, but this often requires users to expose their data to service providers. In this paper, we explore how users can stay in control of their data by using privacy profiles: simple natural language instructions that say what should and should not be revealed. We build a framework where a local model uses these instructions to rewrite queries, only hiding details deemed sensitive by the user, before sending them to an external model, thus balancing privacy with performance. To support this research, we introduce PEEP, a multilingual dataset of real user queries annotated to mark private content and paired with synthetic privacy profiles. Our experiments with lightweight LLMs show they can follow these instructions to some extent, but also face consistent challenges, highlighting the need for models that better understand and comply with user-defined privacy preferences. 

**Abstract (ZH)**: 大型语言模型（LLMs）主要通过商业API访问，但这种访问方式往往要求用户将其数据暴露给服务提供商。在本文中，我们探索用户如何通过使用隐私配置文件来保持对其数据的控制：这是一种简单的自然语言指令，指示应该揭示和不应该揭示的内容。我们构建了一个框架，其中本地模型使用这些指令重写查询，仅隐藏用户认为敏感的细节，然后将它们发送到外部模型，从而在隐私与性能之间取得平衡。为了支持这项研究，我们引入了PEEP，这是一个多语言的用户查询数据集，其中包含真实用户的查询并标注了私人内容，并配对了合成的隐私配置文件。我们的实验表明，轻量级LLM在某种程度上可以遵循这些指令，但也面临持续的挑战，突显了需要更好地理解和遵守用户定义的隐私偏好模型的需求。 

---
# Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training 

**Title (ZH)**: 强化微调自然减轻连续后训练中的遗忘 

**Authors**: Song Lai, Haohan Zhao, Rong Feng, Changyi Ma, Wenzhuo Liu, Hongbo Zhao, Xi Lin, Dong Yi, Min Xie, Qingfu Zhang, Hongbin Liu, Gaofeng Meng, Fei Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2507.05386)  

**Abstract**: Continual post-training (CPT) is a popular and effective technique for adapting foundation models like multimodal large language models to specific and ever-evolving downstream tasks. While existing research has primarily concentrated on methods like data replay, model expansion, or parameter regularization, the fundamental role of the learning paradigm within CPT remains largely unexplored. This paper presents a comparative analysis of two core post-training paradigms: supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT), investigating their respective impacts on knowledge retention during CPT. Our experiments are conducted on a benchmark comprising seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base model for continual post-training. The investigation yields two significant findings: (1) When continuously learning on downstream tasks, SFT leads to catastrophic forgetting of previously learned tasks. In contrast, RFT inherently preserves prior knowledge and achieve performance comparable to multi-task training. (2) RFT successfully protects and even enhances the model's general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro). Conversely, SFT degrades general model capabilities severely. Further analysis shows that explicit mechanisms, such as KL penalty and chain-of-thought reasoning, are not the primary factors. Instead, we find that the implicit regularization inherent to RFT is a key factor in mitigating forgetting. Finally, we propose a rollout-based instance filtering algorithm to improve the stability and efficiency of RFT. Our comprehensive study demonstrates the superiority of RFT as a robust paradigm for continual post-training. 

**Abstract (ZH)**: 持续后训练（CPT）是将基础模型如多模态大型语言模型适应特定且不断演变的下游任务的一种流行而有效的方法。尽管现有研究主要集中在数据重放、模型扩展或参数正则化等方法上，CPT 中学习范式的根本作用仍 largely unexplored。本文对两种核心后训练范式——监督微调（SFT）和强化微调（RFT）——进行了比较分析，研究了它们在 CPT 中对知识保留的影响。我们在包含七个 diverse 多模态任务的基准上进行了实验，使用 Qwen2.5-VL-7B-Instruct 作为持续后训练的基础模型。研究发现两个重要结论：（1）在持续学习下游任务时，SFT 导致对先前学习任务的灾难性遗忘。相比之下，RFT 内在地保留了先前的知识，并且可以达到多任务训练相当的性能。（2）RFT 成功保护甚至增强了模型在标准基准上的通识知识（例如，MMMU 和 MMLU-Pro）。相反，SFT 严重降解了模型的一般能力。进一步分析表明，显式机制如 KL 哑铃和步骤推理并不是主要原因。相反，我们发现 RFT 内在的正则化是减轻遗忘的关键因素。最后，我们提出了一种基于 rollout 的实例过滤算法以提高 RFT 的稳定性和效率。我们的全面研究表明，RFT 是一种在持续后训练中更为 robust 的范式。 

---
# On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study 

**Title (ZH)**: 关于 Next-Token 预测器偏向系统性低效推理的偏差：最短路径案例研究 

**Authors**: Riccardo Alberghi, Elizaveta Demyanenko, Luca Biggio, Luca Saglietti  

**Link**: [PDF](https://arxiv.org/pdf/2507.05362)  

**Abstract**: Recent advances in natural language processing highlight two key factors for improving reasoning in large language models (LLMs): (i) allocating more test-time compute tends to help on harder problems but often introduces redundancy in the reasoning trace, and (ii) compute is most effective when reasoning is systematic and incremental, forming structured chains of thought (CoTs) akin to human problem-solving. To study these factors in isolation, we introduce a controlled setting based on shortest-path tasks in layered graphs. We train decoder-only transformers on question-trace-answer triples using a custom tokenizer, comparing models trained on optimal bottom-up dynamic programming traces with those trained on longer, valid traces involving backtracking. Surprisingly, with the same training-token budget, models trained on inefficient traces generalize better to unseen graphs. This benefit is not due to length alone-injecting arbitrary redundancy into reasoning traces fails to help and can even hurt performance. Instead, we find that generalization correlates with the model's confidence in next-token prediction, suggesting that long, coherent, and locally incremental traces make the training signal easier to optimize. 

**Abstract (ZH)**: Recent advances in自然语言处理强调了两个关键因素以改善大型语言模型（LLMs）的推理能力：(i) 在测试时分配更多的计算资源有助于解决更难的问题，但往往会引入推理轨迹中的冗余；(ii) 计算资源在推理是系统性和逐步性的前提下最有效，形成类似于人类问题解决过程的结构化思路链（CoTs）。为了在隔离环境下研究这些因素，我们基于分层图中的最短路径任务引入了一个受控设置。我们使用自定义分词器对仅解码器变压器模型进行训练，并将其应用于问题-轨迹-答案三元组，将基于最优自底向上的动态规划轨迹训练的模型与需要回溯的更长但有效轨迹训练的模型进行比较。令人惊讶的是，在相同的训练词元预算下，使用低效轨迹训练的模型在未见过的图上泛化能力更强。这一好处并非仅仅归因于长度，注入任意冗余到推理轨迹并不能帮助，甚至可能损害性能。相反，我们发现，泛化与模型对未来词元预测的信心相关，表明长且连贯的、局部逐步的轨迹使训练信号更容易优化。 

---
# LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks 

**Title (ZH)**: LoRA增强生成（LAG）用于知识密集型语言任务 

**Authors**: William Fleshman, Benjamin Van Durme  

**Link**: [PDF](https://arxiv.org/pdf/2507.05346)  

**Abstract**: The proliferation of fine-tuned language model experts for specific tasks and domains signals the need for efficient selection and combination methods. We propose LoRA-Augmented Generation (LAG) for leveraging large libraries of knowledge and task-specific LoRA adapters. LAG requires no additional training or access to data, and efficiently filters, retrieves, and applies experts on a per-token and layer basis. We evaluate LAG on various knowledge-intensive tasks, achieving superior performance over existing data-free methods. We explore scenarios where additional data is available, demonstrating LAG's compatibility with alternative solutions such as retrieval-augmented generation (RAG). 

**Abstract (ZH)**: 细调语言模型专家的 proliferations 信号了高效选择和组合方法的需求。我们提出 LoRA 增强生成 (LAG) 方法以利用大规模知识库和任务特定的 LoRA 调整器。LAG 不需要额外训练或访问数据，并且能够逐词和逐层高效地筛选、检索和应用专家。我们在各种知识密集型任务上评估了 LAG，实现了优于现有无数据方法的性能。我们在有额外数据可用的情景中探讨了 LAG 与检索增强生成 (RAG) 等替代方案的兼容性。 

---
# Causal Foundation Models: Disentangling Physics from Instrument Properties 

**Title (ZH)**: 因果基础模型：拆分物理规律与仪器属性 

**Authors**: Jeroen Audenaert, Daniel Muthukrishna, Paul F. Gregory, David W. Hogg, V. Ashley Villar  

**Link**: [PDF](https://arxiv.org/pdf/2507.05333)  

**Abstract**: Foundation models for structured time series data must contend with a fundamental challenge: observations often conflate the true underlying physical phenomena with systematic distortions introduced by measurement instruments. This entanglement limits model generalization, especially in heterogeneous or multi-instrument settings. We present a causally-motivated foundation model that explicitly disentangles physical and instrumental factors using a dual-encoder architecture trained with structured contrastive learning. Leveraging naturally occurring observational triplets (i.e., where the same target is measured under varying conditions, and distinct targets are measured under shared conditions) our model learns separate latent representations for the underlying physical signal and instrument effects. Evaluated on simulated astronomical time series designed to resemble the complexity of variable stars observed by missions like NASA's Transiting Exoplanet Survey Satellite (TESS), our method significantly outperforms traditional single-latent space foundation models on downstream prediction tasks, particularly in low-data regimes. These results demonstrate that our model supports key capabilities of foundation models, including few-shot generalization and efficient adaptation, and highlight the importance of encoding causal structure into representation learning for structured data. 

**Abstract (ZH)**: 基于结构化时间序列数据的foundation模型必须应对一个根本性的挑战：观测往往将真实的物理现象与测量仪器引入的系统性失真混淆起来。这种混淆限制了模型的泛化能力，尤其是在异质性或多仪器设置中。我们提出了一种以因果关系为导向的基础模型，该模型使用双编码器架构并通过结构化对比学习进行训练，从而明确地解耦物理因子和仪器因子。利用自然发生的观测三元组（即在不同条件下测量同一目标，在相同条件下测量不同目标），我们的模型学习到物理信号的独立潜在表示和仪器效应的独立潜在表示。在设计用于模拟NASA系外行星调查卫星（TESS）等任务中观测到的变星复杂时间序列的模拟数据集上进行评估，我们的方法在下游预测任务中显著优于传统的单潜在空间基础模型，特别是在低数据量情况下。这些结果表明，我们的模型支持基础模型的关键能力，包括少样本泛化和高效适应，并突显了在结构化数据中将因果结构编码到表示学习中的重要性。 

---
# MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents 

**Title (ZH)**: MindFlow：以多模态LLM代理革新电子商务客户服务 

**Authors**: Ming Gong, Xucheng Huang, Chenghan Yang, Xianhan Peng, Haoxin Wang, Yang Liu, Ling Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2507.05330)  

**Abstract**: Recent advances in large language models (LLMs) have enabled new applications in e-commerce customer service. However, their capabilities remain constrained in complex, multimodal scenarios. We present MindFlow, the first open-source multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it integrates memory, decision-making, and action modules, and adopts a modular "MLLM-as-Tool" strategy for effect visual-textual reasoning. Evaluated via online A/B testing and simulation-based ablation, MindFlow demonstrates substantial gains in handling complex queries, improving user satisfaction, and reducing operational costs, with a 93.53% relative improvement observed in real-world deployments. 

**Abstract (ZH)**: Recent advances in large language models (LLMs) have enabled new applications in e-commerce customer service. However, their capabilities remain constrained in complex, multimodal scenarios. We present MindFlow, the first open-source multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it integrates memory, decision-making, and action modules, and adopts a modular "MLLM-as-Tool" strategy for effect visual-textual reasoning. Evaluated via online A/B testing and simulation-based ablation, MindFlow demonstrates substantial gains in handling complex queries, improving user satisfaction, and reducing operational costs, with a 93.53% relative improvement observed in real-world deployments. 

---
# Going Beyond Heuristics by Imposing Policy Improvement as a Constraint 

**Title (ZH)**: 通过将策略改进作为约束来超越启发式方法 

**Authors**: Chi-Chang Lee, Zhang-Wei Hong, Pulkit Agrawal  

**Link**: [PDF](https://arxiv.org/pdf/2507.05328)  

**Abstract**: In many reinforcement learning (RL) applications, augmenting the task rewards with heuristic rewards that encode human priors about how a task should be solved is crucial for achieving desirable performance. However, because such heuristics are usually not optimal, much human effort and computational resources are wasted in carefully balancing tasks and heuristic rewards. Theoretically rigorous ways of incorporating heuristics rely on the idea of \textit{policy invariance}, which guarantees that the performance of a policy obtained by maximizing heuristic rewards is the same as the optimal policy with respect to the task reward. However, in practice, policy invariance doesn't result in policy improvement, and such methods are known to empirically perform poorly. We propose a new paradigm to mitigate reward hacking and effectively use heuristics based on the practical goal of maximizing policy improvement instead of policy improvement. Our framework, Heuristic Enhanced Policy Optimization (HEPO), effectively leverages heuristics while avoiding the pitfall of prior methods for mitigating reward hacking. HEPO achieves superior performance on standard benchmarks with well-engineered reward functions. More surprisingly, HEPO allows policy optimization to achieve good performance even when heuristics are not well-engineered and designed by non-expert humans, showcasing HEPO's ability to reduce human effort in reward design. % HEPO is a plug-and-play optimization method for leveraging heuristics in reinforcement learning. Code is available at this https URL. 

**Abstract (ZH)**: 基于改进策略优化的启发式增强（HEPO）：一种减少奖励欺骗并有效利用启发式的全新范式 

---
# AGACCI : Affiliated Grading Agents for Criteria-Centric Interface in Educational Coding Contexts 

**Title (ZH)**: AGACCI：关联评分代理在教育coding情境中的准则导向界面 

**Authors**: Kwangsuk Park, Jiwoong Yang  

**Link**: [PDF](https://arxiv.org/pdf/2507.05321)  

**Abstract**: Recent advances in AI-assisted education have encouraged the integration of vision-language models (VLMs) into academic assessment, particularly for tasks that require both quantitative and qualitative evaluation. However, existing VLM based approaches struggle with complex educational artifacts, such as programming tasks with executable components and measurable outputs, that require structured reasoning and alignment with clearly defined evaluation criteria. We introduce AGACCI, a multi-agent system that distributes specialized evaluation roles across collaborative agents to improve accuracy, interpretability, and consistency in code-oriented assessment. To evaluate the framework, we collected 360 graduate-level code-based assignments from 60 participants, each annotated by domain experts with binary rubric scores and qualitative feedback. Experimental results demonstrate that AGACCI outperforms a single GPT-based baseline in terms of rubric and feedback accuracy, relevance, consistency, and coherence, while preserving the instructional intent and evaluative depth of expert assessments. Although performance varies across task types, AGACCI highlights the potential of multi-agent systems for scalable and context-aware educational evaluation. 

**Abstract (ZH)**: Recent advances in AI-assisted education have encouraged the integration of vision-language models (VLMs) into academic assessment, particularly for tasks that require both quantitative and qualitative evaluation.然而，现有的基于VLM的方法在处理具有可执行组件和可度量输出的编程任务等复杂教育 artefacts 方面存在困难，这些任务需要结构化的推理并与明确定义的评估标准保持一致。我们引入了AGACCI，这是一种多Agent系统，通过将专门的评估角色分配给协作Agent来提高面向代码评估的准确度、可解释性和一致性。为了评估该框架，我们收集了60名参与者共360个研究生级别的基于代码的任务，每个任务均由领域专家按照二元评分表进行评分并提供定性反馈。实验结果表明，与单一基于GPT的 baseline 相比，AGACCI在评分表和反馈的准确度、相关性、一致性和连贯性方面表现出更优的性能，同时保留了专家评估的指示意图和评估深度。尽管在不同任务类型上的表现有所差异，AGACCI突显了多Agent系统在可扩展和上下文感知的教育评估方面的潜力。 

---
# LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review 

**Title (ZH)**: LCDS：一种支持来源归属和专家审核的逻辑控制出院总结生成系统 

**Authors**: Cheng Yuan, Xinkai Rui, Yongqi Fan, Yawei Fan, Boyang Zhong, Jiacheng Wang, Weiyan Zhang, Tong Ruan  

**Link**: [PDF](https://arxiv.org/pdf/2507.05319)  

**Abstract**: Despite the remarkable performance of Large Language Models (LLMs) in automated discharge summary generation, they still suffer from hallucination issues, such as generating inaccurate content or fabricating information without valid sources. In addition, electronic medical records (EMRs) typically consist of long-form data, making it challenging for LLMs to attribute the generated content to the sources. To address these challenges, we propose LCDS, a Logic-Controlled Discharge Summary generation system. LCDS constructs a source mapping table by calculating textual similarity between EMRs and discharge summaries to constrain the scope of summarized content. Moreover, LCDS incorporates a comprehensive set of logical rules, enabling it to generate more reliable silver discharge summaries tailored to different clinical fields. Furthermore, LCDS supports source attribution for generated content, allowing experts to efficiently review, provide feedback, and rectify errors. The resulting golden discharge summaries are subsequently recorded for incremental fine-tuning of LLMs. Our project and demo video are in the GitHub repository this https URL. 

**Abstract (ZH)**: 尽管大型语言模型（LLMs）在自动出院总结生成方面取得了显著性能，但仍存在幻觉问题，如生成不准确的内容或无可靠来源地编造信息。此外，电子医疗记录（EMRs）通常包含长篇数据，这使得LLMs难以将生成的内容与来源关联起来。为了解决这些问题，我们提出了一种逻辑控制出院总结生成系统（LCDS）。LCDS通过计算出院总结和EMRs之间的文本相似性来构建源映射表，从而限制总结内容的范围。此外，LCDS整合了一整套逻辑规则，使其能够生成更可靠的出院总结，适用于不同的临床领域。此外，LCDS支持生成内容的来源归属，使专家能够高效地审查、提供反馈并纠正错误。生成的黄金出院总结随后用于LLMs的增量微调。我们的项目和演示视频可在GitHub仓库中访问：https://github.com/almightydex/LCDS。 

---
# PWD: Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle CT 

**Title (ZH)**: PWD: 前向引导和小波增强的有限角度CT扩散模型 

**Authors**: Yi Liu, Yiyang Wen, Zekun Zhou, Junqi Ma, Linghang Wang, Yucheng Yao, Liu Shi, Qiegen Liu  

**Link**: [PDF](https://arxiv.org/pdf/2507.05317)  

**Abstract**: Generative diffusion models have received increasing attention in medical imaging, particularly in limited-angle computed tomography (LACT). Standard diffusion models achieve high-quality image reconstruction but require a large number of sampling steps during inference, resulting in substantial computational overhead. Although skip-sampling strategies have been proposed to improve efficiency, they often lead to loss of fine structural details. To address this issue, we propose a prior information embedding and wavelet feature fusion fast sampling diffusion model for LACT reconstruction. The PWD enables efficient sampling while preserving reconstruction fidelity in LACT, and effectively mitigates the degradation typically introduced by skip-sampling. Specifically, during the training phase, PWD maps the distribution of LACT images to that of fully sampled target images, enabling the model to learn structural correspondences between them. During inference, the LACT image serves as an explicit prior to guide the sampling trajectory, allowing for high-quality reconstruction with significantly fewer steps. In addition, PWD performs multi-scale feature fusion in the wavelet domain, effectively enhancing the reconstruction of fine details by leveraging both low-frequency and high-frequency information. Quantitative and qualitative evaluations on clinical dental arch CBCT and periapical datasets demonstrate that PWD outperforms existing methods under the same sampling condition. Using only 50 sampling steps, PWD achieves at least 1.7 dB improvement in PSNR and 10% gain in SSIM. 

**Abstract (ZH)**: 生成扩散模型在医学成像中的应用，特别是在有限角度 computed tomography (LACT) 中，prior信息嵌入和小波特征融合快速采样扩散模型用于 LACT 重建 

---
# OASBuilder: Generating OpenAPI Specifications from Online API Documentation with Large Language Models 

**Title (ZH)**: OASBuilder: 从在线API文档生成OpenAPI规范的大语言模型方法 

**Authors**: Koren Lazar, Matan Vetzler, Kiran Kate, Jason Tsay, David Boaz Himanshu Gupta, Avraham Shinnar, Rohith D Vallam, David Amid Esther Goldbraich, Guy Uziel, Jim Laredo, Ateret Anaby Tavor  

**Link**: [PDF](https://arxiv.org/pdf/2507.05316)  

**Abstract**: AI agents and business automation tools interacting with external web services require standardized, machine-readable information about their APIs in the form of API specifications. However, the information about APIs available online is often presented as unstructured, free-form HTML documentation, requiring external users to spend significant time manually converting it into a structured format. To address this, we introduce OASBuilder, a novel framework that transforms long and diverse API documentation pages into consistent, machine-readable API specifications. This is achieved through a carefully crafted pipeline that integrates large language models and rule-based algorithms which are guided by domain knowledge of the structure of documentation webpages. Our experiments demonstrate that OASBuilder generalizes well across hundreds of APIs, and produces valid OpenAPI specifications that encapsulate most of the information from the original documentation. OASBuilder has been successfully implemented in an enterprise environment, saving thousands of hours of manual effort and making hundreds of complex enterprise APIs accessible as tools for LLMs. 

**Abstract (ZH)**: AI代理和企业自动化工具与外部Web服务交互需要以API规范的形式获取标准化的、机器可读的API信息。然而，网络上可用于API的信息通常以未结构化的自由格式HTML文档形式呈现，这要求外部用户花费大量时间手动将其转换为结构化的格式。为了解决这一问题，我们引入了OASBuilder，这是一种新颖的框架，能够将长篇且多样的API文档页面转化为一致的、机器可读的API规范。这一过程通过一个精心设计的流水线实现，该流水线结合了大型语言模型和基于规则的算法，并受到文档网页结构领域知识的指导。我们的实验表明，OASBuilder能够在数百个API上有效推广，并产生有效的OpenAPI规范，这些规范涵盖了原始文档中的大部分信息。OASBuilder已在企业环境中成功实施，节省了数千小时的手动工作，并使数百个复杂的企业API成为大语言模型的工具。 

---
# Conditional Graph Neural Network for Predicting Soft Tissue Deformation and Forces 

**Title (ZH)**: 基于条件图神经网络的软组织变形及力预测方法 

**Authors**: Madina Kojanazarova, Florentin Bieder, Robin Sandkühler, Philippe C. Cattin  

**Link**: [PDF](https://arxiv.org/pdf/2507.05315)  

**Abstract**: Soft tissue simulation in virtual environments is becoming increasingly important for medical applications. However, the high deformability of soft tissue poses significant challenges. Existing methods rely on segmentation, meshing and estimation of stiffness properties of tissues. In addition, the integration of haptic feedback requires precise force estimation to enable a more immersive experience. We introduce a novel data-driven model, a conditional graph neural network (cGNN) to tackle this complexity. Our model takes surface points and the location of applied forces, and is specifically designed to predict the deformation of the points and the forces exerted on them. We trained our model on experimentally collected surface tracking data of a soft tissue phantom and used transfer learning to overcome the data scarcity by initially training it with mass-spring simulations and fine-tuning it with the experimental data. This approach improves the generalisation capability of the model and enables accurate predictions of tissue deformations and corresponding interaction forces. The results demonstrate that the model can predict deformations with a distance error of 0.35$\pm$0.03 mm for deformations up to 30 mm and the force with an absolute error of 0.37$\pm$0.05 N for forces up to 7.5 N. Our data-driven approach presents a promising solution to the intricate challenge of simulating soft tissues within virtual environments. Beyond its applicability in medical simulations, this approach holds the potential to benefit various fields where realistic soft tissue simulations are required. 

**Abstract (ZH)**: 基于条件图神经网络的虚拟环境中软组织模拟 

---
# Solar Flare Prediction Using LSTM and DLSTM with Sliding Window Pattern Recognition 

**Title (ZH)**: 基于滑动窗口模式识别的LSTM和DLSTM太阳耀斑预测 

**Authors**: Zeinab Hassani, Davud Mohammadpur, Hossein Safari  

**Link**: [PDF](https://arxiv.org/pdf/2507.05313)  

**Abstract**: We investigate the use of Long Short-Term Memory (LSTM) and Decomposition-LSTM (DLSTM) networks, combined with an ensemble algorithm, to predict solar flare occurrences using time-series data from the GOES catalog. The dataset spans from 2003 to 2023 and includes 151,071 flare events. Among approximately possible patterns, 7,552 yearly pattern windows are identified, highlighting the challenge of long-term forecasting due to the Sun's complex, self-organized criticality-driven behavior. A sliding window technique is employed to detect temporal quasi-patterns in both irregular and regularized flare time series. Regularization reduces complexity, enhances large flare activity, and captures active days more effectively. To address class imbalance, resampling methods are applied. LSTM and DLSTM models are trained on sequences of peak fluxes and waiting times from irregular time series, while LSTM and DLSTM, integrated with an ensemble approach, are applied to sliding windows of regularized time series with a 3-hour interval. Performance metrics, particularly TSS (0.74), recall (0.95) and the area under the curve (AUC=0.87) in the receiver operating characteristic (ROC), indicate that DLSTM with an ensemble approach on regularized time series outperforms other models, offering more accurate large-flare forecasts with fewer false errors compared to models trained on irregular time series. The superior performance of DLSTM is attributed to its ability to decompose time series into trend and seasonal components, effectively isolating random noise. This study underscores the potential of advanced machine learning techniques for solar flare prediction and highlights the importance of incorporating various solar cycle phases and resampling strategies to enhance forecasting reliability. 

**Abstract (ZH)**: 基于GOES档案时间序列数据的长短期记忆网络与分解长短期记忆网络组合模型在预测太阳耀斑发生中的应用 

---
# PLACE: Prompt Learning for Attributed Community Search 

**Title (ZH)**: PLACE: 带属性社区搜索的提示学习 

**Authors**: Shuheng Fang, Kangfei Zhao, Rener Zhang, Yu Rong, Jeffrey Xu Yu  

**Link**: [PDF](https://arxiv.org/pdf/2507.05311)  

**Abstract**: In this paper, we propose PLACE (Prompt Learning for Attributed Community Search), an innovative graph prompt learning framework for ACS. Enlightened by prompt-tuning in Natural Language Processing (NLP), where learnable prompt tokens are inserted to contextualize NLP queries, PLACE integrates structural and learnable prompt tokens into the graph as a query-dependent refinement mechanism, forming a prompt-augmented graph. Within this prompt-augmented graph structure, the learned prompt tokens serve as a bridge that strengthens connections between graph nodes for the query, enabling the GNN to more effectively identify patterns of structural cohesiveness and attribute similarity related to the specific query. We employ an alternating training paradigm to optimize both the prompt parameters and the GNN jointly. Moreover, we design a divide-and-conquer strategy to enhance scalability, supporting the model to handle million-scale graphs. Extensive experiments on 9 real-world graphs demonstrate the effectiveness of PLACE for three types of ACS queries, where PLACE achieves higher F1 scores by 22% compared to the state-of-the-arts on average. 

**Abstract (ZH)**: PLACE: 有 Attribution 的社区搜索的提示学习框架 

---
# Neural Velocity for hyperparameter tuning 

**Title (ZH)**: 神经速度优化超参数调优 

**Authors**: Gianluca Dalmasso, Andrea Bragagnolo, Enzo Tartaglione, Attilio Fiandrotti, Marco Grangetto  

**Link**: [PDF](https://arxiv.org/pdf/2507.05309)  

**Abstract**: Hyperparameter tuning, such as learning rate decay and defining a stopping criterion, often relies on monitoring the validation loss. This paper presents NeVe, a dynamic training approach that adjusts the learning rate and defines the stop criterion based on the novel notion of "neural velocity". The neural velocity measures the rate of change of each neuron's transfer function and is an indicator of model convergence: sampling neural velocity can be performed even by forwarding noise in the network, reducing the need for a held-out dataset. Our findings show the potential of neural velocity as a key metric for optimizing neural network training efficiently 

**Abstract (ZH)**: 基于“神经速度”的动态训练方法NeVe：一种新的关键指标及其在优化神经网络训练中的潜力 

---
# ASSURE: Metamorphic Testing for AI-powered Browser Extensions 

**Title (ZH)**: ASSURE: 面向AI驱动浏览器扩展的 metamorphic 测试 

**Authors**: Xuanqi Gao, Juan Zhai, Shiqing Ma, Siyi Xie, Chao Shen  

**Link**: [PDF](https://arxiv.org/pdf/2507.05307)  

**Abstract**: The integration of Large Language Models (LLMs) into browser extensions has revolutionized web browsing, enabling sophisticated functionalities like content summarization, intelligent translation, and context-aware writing assistance. However, these AI-powered extensions introduce unprecedented challenges in testing and reliability assurance. Traditional browser extension testing approaches fail to address the non-deterministic behavior, context-sensitivity, and complex web environment integration inherent to LLM-powered extensions. Similarly, existing LLM testing methodologies operate in isolation from browser-specific contexts, creating a critical gap in effective evaluation frameworks. To bridge this gap, we present ASSURE, a modular automated testing framework specifically designed for AI-powered browser extensions. ASSURE comprises three principal components: (1) a modular test case generation engine that supports plugin-based extension of testing scenarios, (2) an automated execution framework that orchestrates the complex interactions between web content, extension processing, and AI model behavior, and (3) a configurable validation pipeline that systematically evaluates behavioral consistency and security invariants rather than relying on exact output matching. Our evaluation across six widely-used AI browser extensions demonstrates ASSURE's effectiveness, identifying 531 distinct issues spanning security vulnerabilities, metamorphic relation violations, and content alignment problems. ASSURE achieves 6.4x improved testing throughput compared to manual approaches, detecting critical security vulnerabilities within 12.4 minutes on average. This efficiency makes ASSURE practical for integration into development pipelines, offering a comprehensive solution to the unique challenges of testing AI-powered browser extensions. 

**Abstract (ZH)**: 大型语言模型（LLMs）集成到浏览器扩展中的整合已经革新了网络浏览，实现了如内容总结、智能翻译和上下文感知撰写辅助等复杂功能。然而，这些具备AI功能的扩展引入了前所未有的测试和可靠性保障挑战。传统的浏览器扩展测试方法无法解决LLM驱动扩展固有的非确定性行为、上下文敏感性和复杂网络环境集成问题。同样，现有的LLM测试方法与浏览器特定上下文脱节，导致有效的评估框架存在关键缺陷。为弥合这一差距，我们提出了ASSURE，一个专门针对AI驱动浏览器扩展的模块化自动化测试框架。ASSURE由三个主要组件构成：（1）一个支持插件扩展的模块化测试案例生成引擎；（2）一个自动化执行框架，协调网络内容、扩展处理和AI模型行为之间的复杂交互；（3）一个可配置的验证流水线，系统地评估行为一致性与安全保障不变式，而不是依赖于精确输出匹配。我们在六款广泛使用的AI浏览器扩展上的评估证明了ASSURE的有效性，发现涵盖安全漏洞、变形关系违反和内容对齐问题在内的531个独特问题。与手动方法相比，ASSURE提高了6.4倍的测试吞吐量，平均在12.4分钟内检测到关键安全漏洞。这种效率使ASSURE能够在开发管道中实现集成，提供一个针对AI驱动浏览器扩展的独特挑战的全面解决方案。 

---
# Enjoying Non-linearity in Multinomial Logistic Bandits 

**Title (ZH)**: 享受多元logistic臂中的非线性特性 

**Authors**: Pierre Boudart, Pierre Gaillard, Alessandro Rudi  

**Link**: [PDF](https://arxiv.org/pdf/2507.05306)  

**Abstract**: We consider the multinomial logistic bandit problem, a variant of generalized linear bandits where a learner interacts with an environment by selecting actions to maximize expected rewards based on probabilistic feedback from multiple possible outcomes. In the binary setting, recent work has focused on understanding the impact of the non-linearity of the logistic model (Faury et al., 2020; Abeille et al., 2021). They introduced a problem-dependent constant $\kappa_*$, that may be exponentially large in some problem parameters and which is captured by the derivative of the sigmoid function. It encapsulates the non-linearity and improves existing regret guarantees over $T$ rounds from $\smash{O(d\sqrt{T})}$ to $\smash{O(d\sqrt{T/\kappa_*})}$, where $d$ is the dimension of the parameter space. We extend their analysis to the multinomial logistic bandit framework, making it suitable for complex applications with more than two choices, such as reinforcement learning or recommender systems. To achieve this, we extend the definition of $\kappa_*$ to the multinomial setting and propose an efficient algorithm that leverages the problem's non-linearity. Our method yields a problem-dependent regret bound of order $ \smash{\widetilde{\mathcal{O}}( Kd \sqrt{{T}/{\kappa_*}})} $, where $K$ is the number of actions and $\kappa_* \ge 1$. This improves upon the best existing guarantees of order $ \smash{\widetilde{\mathcal{O}}( Kd \sqrt{T} )} $. Moreover, we provide a $\smash{ \Omega(d\sqrt{T/\kappa_*})}$ lower-bound, showing that our dependence on $\kappa_*$ is optimal. 

**Abstract (ZH)**: 多分类逻辑宽度胆问题及其分析：从二分类到多分类的拓展 

---
# Narrowing the Gap: Supervised Fine-Tuning of Open-Source LLMs as a Viable Alternative to Proprietary Models for Pedagogical Tools 

**Title (ZH)**: 缩小差距：将开源大语言模型通过监督微调作为自购模型的可行替代方案应用于教学工具 

**Authors**: Lorenzo Lee Solano, Charles Koutcheme, Juho Leinonen, Alexandra Vassar, Jake Renzella  

**Link**: [PDF](https://arxiv.org/pdf/2507.05305)  

**Abstract**: Frontier Large language models (LLMs) like ChatGPT and Gemini can decipher cryptic compiler errors for novice programmers, but their computational scale, cost, and tendency to over-assist make them problematic for widespread pedagogical adoption. This work demonstrates that smaller, specialised language models, enhanced via Supervised Fine-Tuning (SFT), present a more viable alternative for educational tools. We utilise a new dataset of 40,000 C compiler error explanations, derived from real introductory programming (CS1/2) student-generated programming errors, which we used to fine-tune three open-source models: Qwen3-4B, Llama-3.1-8B, and Qwen3-32B. We performed a dual evaluation, combining expert human reviews with a large-scale automated analysis of 8,000 responses using a validated LLM-as-judge ensemble. Our results show that SFT significantly boosts the pedagogical quality of smaller models, achieving performance comparable to much larger models. We analyse the trade-offs between model size and quality, confirming that fine-tuning compact, efficient models on high-quality, domain-specific data is a potent strategy for creating specialised models to drive educational tools. We provide a replicable methodology to foster broader access to generative AI capabilities in educational contexts. 

**Abstract (ZH)**: 较小的专门化语言模型通过监督微调提升教育质量：以编译器错误解释为例 

---
# Self-Attention Based Multi-Scale Graph Auto-Encoder Network of 3D Meshes 

**Title (ZH)**: 基于自注意力的多尺度3D网格图自编码网络 

**Authors**: Saqib Nazir, Olivier Lézoray, Sébastien Bougleux  

**Link**: [PDF](https://arxiv.org/pdf/2507.05304)  

**Abstract**: 3D meshes are fundamental data representations for capturing complex geometric shapes in computer vision and graphics applications. While Convolutional Neural Networks (CNNs) have excelled in structured data like images, extending them to irregular 3D meshes is challenging due to the non-Euclidean nature of the data. Graph Convolutional Networks (GCNs) offer a solution by applying convolutions to graph-structured data, but many existing methods rely on isotropic filters or spectral decomposition, limiting their ability to capture both local and global mesh features. In this paper, we introduce 3D Geometric Mesh Network (3DGeoMeshNet), a novel GCN-based framework that uses anisotropic convolution layers to effectively learn both global and local features directly in the spatial domain. Unlike previous approaches that convert meshes into intermediate representations like voxel grids or point clouds, our method preserves the original polygonal mesh format throughout the reconstruction process, enabling more accurate shape reconstruction. Our architecture features a multi-scale encoder-decoder structure, where separate global and local pathways capture both large-scale geometric structures and fine-grained local details. Extensive experiments on the COMA dataset containing human faces demonstrate the efficiency of 3DGeoMeshNet in terms of reconstruction accuracy. 

**Abstract (ZH)**: 三维几何网格网络：用于直接在空间域中学习全局和局部特征的各向异性卷积框架 

---
# CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection 

**Title (ZH)**: CorrDetail: 视觉细节增强的自纠正方法用于人脸伪造检测 

**Authors**: Binjia Zhou, Hengrui Lou, Lizhe Chen, Haoyuan Li, Dawei Luo, Shuai Chen, Jie Lei, Zunlei Feng, Yijun Bei  

**Link**: [PDF](https://arxiv.org/pdf/2507.05302)  

**Abstract**: With the swift progression of image generation technology, the widespread emergence of facial deepfakes poses significant challenges to the field of security, thus amplifying the urgent need for effective deepfake this http URL techniques for face forgery detection can broadly be categorized into two primary groups: visual-based methods and multimodal approaches. The former often lacks clear explanations for forgery details, while the latter, which merges visual and linguistic modalities, is more prone to the issue of this http URL address these shortcomings, we introduce a visual detail enhanced self-correction framework, designated CorrDetail, for interpretable face forgery detection. CorrDetail is meticulously designed to rectify authentic forgery details when provided with error-guided questioning, with the aim of fostering the ability to uncover forgery details rather than yielding hallucinated responses. Additionally, to bolster the reliability of its findings, a visual fine-grained detail enhancement module is incorporated, supplying CorrDetail with more precise visual forgery details. Ultimately, a fusion decision strategy is devised to further augment the model's discriminative capacity in handling extreme samples, through the integration of visual information compensation and model bias this http URL results demonstrate that CorrDetail not only achieves state-of-the-art performance compared to the latest methodologies but also excels in accurately identifying forged details, all while exhibiting robust generalization capabilities. 

**Abstract (ZH)**: 基于视觉细节增强的自校正框架CorrDetail：可解释的人脸伪造检测 

---
# Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M) 

**Title (ZH)**: 结构化说明文提高文本到图像模型的提示遵从性（Re-LAION-Caption 19M） 

**Authors**: Nicholas Merchant, Haitz Sáez de Ocáriz Borde, Andrei Cristian Popescu, Carlos Garcia Jurado Suarez  

**Link**: [PDF](https://arxiv.org/pdf/2507.05300)  

**Abstract**: We argue that generative text-to-image models often struggle with prompt adherence due to the noisy and unstructured nature of large-scale datasets like LAION-5B. This forces users to rely heavily on prompt engineering to elicit desirable outputs. In this work, we propose that enforcing a consistent caption structure during training can significantly improve model controllability and alignment. We introduce Re-LAION-Caption 19M, a high-quality subset of Re-LAION-5B, comprising 19 million 1024x1024 images with captions generated by a Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part template: subject, setting, aesthetics, and camera details. We fine-tune PixArt-$\Sigma$ and Stable Diffusion 2 using both structured and randomly shuffled captions, and show that structured versions consistently yield higher text-image alignment scores using visual question answering (VQA) models. The dataset is publicly available at this https URL. 

**Abstract (ZH)**: 我们 argue 生成文本到图像模型往往难以遵循提示，因为大型数据集如 LAION-5B 的噪声和无序性质。这迫使用户大量依赖提示工程来获得理想输出。在本文中，我们提出，在训练过程中强制保持一致的标题结构可以显著提高模型的可控性和对齐性。我们引入了 Re-LAION-Caption 19M，它是 Re-LAION-5B 的高质量子集，包括 19 百万张 1024x1024 像素的图像，其标题由 Mistral 7B Instruct 为基础的 LLaVA-Next 模型生成。每个标题遵循四部分模板：主题、背景、审美和相机细节。我们使用结构化和随机打乱的标题分别对 PixArt-$\Sigma$ 和 Stable Diffusion 2 进行微调，并且通过视觉问答（VQA）模型展示结构化版本始终能获得更高的文本图像对齐分数。该数据集可在以下网址公开获取：this https URL。 

---
# Integrating Generative AI in BIM Education: Insights from Classroom Implementation 

**Title (ZH)**: 将生成性人工智能融入BIM教育：课堂教学实施的见解 

**Authors**: Islem Sahraoui, Kinam Kim, Lu Gao, Zia Din, Ahmed Senouci  

**Link**: [PDF](https://arxiv.org/pdf/2507.05296)  

**Abstract**: This study evaluates the implementation of a Generative AI-powered rule checking workflow within a graduate-level Building Information Modeling (BIM) course at a U.S. university. Over two semesters, 55 students participated in a classroom-based pilot exploring the use of GenAI for BIM compliance tasks, an area with limited prior research. The instructional design included lectures on prompt engineering and AI-driven rule checking, followed by an assignment where students used a large language model (LLM) to identify code violations in designs using Autodesk Revit. Surveys and interviews were conducted to assess student workload, learning effectiveness, and overall experience, using the NASA-TLX scale and regression analysis. Findings indicate students generally achieved learning objectives but faced challenges such as difficulties debugging AI-generated code and inconsistent tool performance, probably due to their limited prompt engineering experience. These issues increased cognitive and emotional strain, especially among students with minimal programming backgrounds. Despite these challenges, students expressed strong interest in future GenAI applications, particularly with clear instructional support. 

**Abstract (ZH)**: 本研究评估了在美国大学一门研究生级别建筑信息建模（BIM）课程中基于生成式AI的规则检查工作流的实施情况。在两个学期内，55名学生参与了基于教室的试点项目，探索生成式AI在BIM合规任务中的应用，这是该领域尚未有大量研究的区域。教学设计包括关于提示工程和AI驱动规则检查的讲座，随后是让学生使用大型语言模型（LLM）在Autodesk Revit中识别设计中的代码违规问题的作业。通过调查问卷和访谈，使用NASA-TLX量表和回归分析评估了学生的工作量、学习效果和总体体验。研究发现，学生普遍达到了学习目标，但在调试生成式AI代码和工具表现不一致等方面遇到了挑战，可能是因为他们缺乏提示工程经验。这些问题增加了学生的认知和情感压力，尤其是在编程背景较弱的学生中更为明显。尽管存在这些挑战，学生们对未来的生成式AI应用表现出强烈兴趣，特别是在有明确教学支持的情况下。 

---
# Enhancing Learning Path Recommendation via Multi-task Learning 

**Title (ZH)**: 基于多任务学习的学习路径推荐优化 

**Authors**: Afsana Nasrin, Lijun Qian, Pamela Obiomon, Xishuang Dong  

**Link**: [PDF](https://arxiv.org/pdf/2507.05295)  

**Abstract**: Personalized learning is a student-centered educational approach that adapts content, pace, and assessment to meet each learner's unique needs. As the key technique to implement the personalized learning, learning path recommendation sequentially recommends personalized learning items such as lectures and exercises. Advances in deep learning, particularly deep reinforcement learning, have made modeling such recommendations more practical and effective. This paper proposes a multi-task LSTM model that enhances learning path recommendation by leveraging shared information across tasks. The approach reframes learning path recommendation as a sequence-to-sequence (Seq2Seq) prediction problem, generating personalized learning paths from a learner's historical interactions. The model uses a shared LSTM layer to capture common features for both learning path recommendation and deep knowledge tracing, along with task-specific LSTM layers for each objective. To avoid redundant recommendations, a non-repeat loss penalizes repeated items within the recommended learning path. Experiments on the ASSIST09 dataset show that the proposed model significantly outperforms baseline methods for the learning path recommendation. 

**Abstract (ZH)**: 个性化学习是一种以学生为中心的教育方法，根据每位学习者的独特需求调整内容、进度和评估。作为实施个性化学习的关键技术，学习路径推荐按顺序推荐个性化学习项目，如讲座和练习。随着深度学习，尤其是深度强化学习的进步，使这种推荐更加实用和有效。本文提出了一种多任务LSTM模型，通过利用任务间的共享信息来增强学习路径推荐。该方法将学习路径推荐重新构 frame 为一个序列到序列（Seq2Seq）预测问题，从学习者的历史互动中生成个性化学习路径。模型使用共享的LSTM层来捕捉学习路径推荐和深度知识追踪的共同特征，并为每个目标使用特定任务的LSTM层。为了避免重复推荐，非重复损失项惩罚推荐学习路径中的重复项目。在ASSIST09数据集上的实验表明，所提出模型在学习路径推荐方面显著优于基线方法。 

---
# Physics-Informed Graph Neural Networks to Reconstruct Local Fields Considering Finite Strain Hyperelasticity 

**Title (ZH)**: 基于物理学的知识图神经网络考虑有限拉伸弹性的局部场重建 

**Authors**: Manuel Ricardo Guevara Garban, Yves Chemisky, Étienne Prulière, Michaël Clément  

**Link**: [PDF](https://arxiv.org/pdf/2507.05291)  

**Abstract**: We propose a physics-informed machine learning framework called P-DivGNN to reconstruct local stress fields at the micro-scale, in the context of multi-scale simulation given a periodic micro-structure mesh and mean, macro-scale, stress values. This method is based in representing a periodic micro-structure as a graph, combined with a message passing graph neural network. We are able to retrieve local stress field distributions, providing average stress values produced by a mean field reduced order model (ROM) or Finite Element (FE) simulation at the macro-scale. The prediction of local stress fields are of utmost importance considering fracture analysis or the definition of local fatigue criteria. Our model incorporates physical constraints during training to constraint local stress field equilibrium state and employs a periodic graph representation to enforce periodic boundary conditions. The benefits of the proposed physics-informed GNN are evaluated considering linear and non linear hyperelastic responses applied to varying geometries. In the non-linear hyperelastic case, the proposed method achieves significant computational speed-ups compared to FE simulation, making it particularly attractive for large-scale applications. 

**Abstract (ZH)**: 基于物理约束的机器学习框架P-DivGNN在周期微观结构网格下重构微观尺度局部应力场的研究 

---
# A Survey on Proactive Defense Strategies Against Misinformation in Large Language Models 

**Title (ZH)**: 大型语言模型中对抗误导信息的主动防御策略综述 

**Authors**: Shuliang Liu, Hongyi Liu, Aiwei Liu, Bingchen Duan, Qi Zheng, Yibo Yan, He Geng, Peijie Jiang, Jia Liu, Xuming Hu  

**Link**: [PDF](https://arxiv.org/pdf/2507.05288)  

**Abstract**: The widespread deployment of large language models (LLMs) across critical domains has amplified the societal risks posed by algorithmically generated misinformation. Unlike traditional false content, LLM-generated misinformation can be self-reinforcing, highly plausible, and capable of rapid propagation across multiple languages, which traditional detection methods fail to mitigate effectively. This paper introduces a proactive defense paradigm, shifting from passive post hoc detection to anticipatory mitigation strategies. We propose a Three Pillars framework: (1) Knowledge Credibility, fortifying the integrity of training and deployed data; (2) Inference Reliability, embedding self-corrective mechanisms during reasoning; and (3) Input Robustness, enhancing the resilience of model interfaces against adversarial attacks. Through a comprehensive survey of existing techniques and a comparative meta-analysis, we demonstrate that proactive defense strategies offer up to 63\% improvement over conventional methods in misinformation prevention, despite non-trivial computational overhead and generalization challenges. We argue that future research should focus on co-designing robust knowledge foundations, reasoning certification, and attack-resistant interfaces to ensure LLMs can effectively counter misinformation across varied domains. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在关键领域的广泛应用放大了由算法生成的 misinformation 所带来的社会风险。不同于传统的虚假内容，由LLM生成的 misinformation 具有自我强化、高度可信和能在多种语言中迅速传播的特性，这使得传统的检测方法难以有效应对。本文介绍了一种主动防御范式，从被动的事后检测转向前瞻性缓解策略。我们提出了一种三支柱框架：（1）知识可信度，强化训练和部署数据的完整性；（2）推理可靠性，在推理过程中嵌入自我纠正机制；（3）输入鲁棒性，增强模型界面对抗攻击的韧性。通过全面回顾现有技术和进行比较元分析，我们证明了主动防御策略在 misinformation 防范方面相较于传统方法可提高高达63%，尽管面临非平凡的计算开销和泛化挑战。我们认为未来研究应集中在共同设计 robust 的知识基础、推理认证和抗攻击界面，以确保LLM能够在多种领域有效地对抗 misinformation。 

---
# Compressing Deep Neural Networks Using Explainable AI 

**Title (ZH)**: 使用可解释人工智能压缩深度神经网络 

**Authors**: Kimia Soroush, Mohsen Raji, Behnam Ghavami  

**Link**: [PDF](https://arxiv.org/pdf/2507.05286)  

**Abstract**: Deep neural networks (DNNs) have demonstrated remarkable performance in many tasks but it often comes at a high computational cost and memory usage. Compression techniques, such as pruning and quantization, are applied to reduce the memory footprint of DNNs and make it possible to accommodate them on resource-constrained edge devices. Recently, explainable artificial intelligence (XAI) methods have been introduced with the purpose of understanding and explaining AI methods. XAI can be utilized to get to know the inner functioning of DNNs, such as the importance of different neurons and features in the overall performance of DNNs. In this paper, a novel DNN compression approach using XAI is proposed to efficiently reduce the DNN model size with negligible accuracy loss. In the proposed approach, the importance score of DNN parameters (i.e. weights) are computed using a gradient-based XAI technique called Layer-wise Relevance Propagation (LRP). Then, the scores are used to compress the DNN as follows: 1) the parameters with the negative or zero importance scores are pruned and removed from the model, 2) mixed-precision quantization is applied to quantize the weights with higher/lower score with higher/lower number of bits. The experimental results show that, the proposed compression approach reduces the model size by 64% while the accuracy is improved by 42% compared to the state-of-the-art XAI-based compression method. 

**Abstract (ZH)**: 基于可解释人工智能的高效DNN压缩方法 

---
# Beyond classical and contemporary models: a transformative ai framework for student dropout prediction in distance learning using rag, prompt engineering, and cross-modal fusion 

**Title (ZH)**: 超越古典和当代模型：基于RAG、提示工程和跨模态融合的学生辍学预测transformative AI框架在远程学习中的应用 

**Authors**: Miloud Mihoubi, Meriem Zerkouk, Belkacem Chikhaoui  

**Link**: [PDF](https://arxiv.org/pdf/2507.05285)  

**Abstract**: Student dropout in distance learning remains a critical challenge, with profound societal and economic consequences. While classical machine learning models leverage structured socio-demographic and behavioral data, they often fail to capture the nuanced emotional and contextual factors embedded in unstructured student interactions. This paper introduces a transformative AI framework that redefines dropout prediction through three synergistic innovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment analysis, prompt engineering to decode academic stressors, and cross-modal attention fusion to dynamically align textual, behavioral, and socio-demographic insights. By grounding sentiment analysis in a curated knowledge base of pedagogical content, our RAG-enhanced BERT model interprets student comments with unprecedented contextual relevance, while optimized prompts isolate indicators of academic distress (e.g., "isolation," "workload anxiety"). A cross-modal attention layer then fuses these insights with temporal engagement patterns, creating holistic risk profiles. Evaluated on a longitudinal dataset of 4 423 students, the framework achieves 89% accuracy and an F1-score of 0.88, outperforming conventional models by 7% and reducing false negatives by 21%. Beyond prediction, the system generates interpretable interventions by retrieving contextually aligned strategies (e.g., mentorship programs for isolated learners). This work bridges the gap between predictive analytics and actionable pedagogy, offering a scalable solution to mitigate dropout risks in global education systems 

**Abstract (ZH)**: 基于检索增强生成的多层次情感分析框架在远程教育退学预测中的应用 

---
# Exploring LLM Capabilities in Extracting DCAT-Compatible Metadata for Data Cataloging 

**Title (ZH)**: 探索LLM在提取DCAT兼容元数据以进行数据目录构建方面的能力 

**Authors**: Lennart Busch, Daniel Tebernum, Gissel Velarde  

**Link**: [PDF](https://arxiv.org/pdf/2507.05282)  

**Abstract**: Efficient data exploration is crucial as data becomes increasingly important for accelerating processes, improving forecasts and developing new business models. Data consumers often spend 25-98 % of their time searching for suitable data due to the exponential growth, heterogeneity and distribution of data. Data catalogs can support and accelerate data exploration by using metadata to answer user queries. However, as metadata creation and maintenance is often a manual process, it is time-consuming and requires expertise. This study investigates whether LLMs can automate metadata maintenance of text-based data and generate high-quality DCAT-compatible metadata. We tested zero-shot and few-shot prompting strategies with LLMs from different vendors for generating metadata such as titles and keywords, along with a fine-tuned model for classification. Our results show that LLMs can generate metadata comparable to human-created content, particularly on tasks that require advanced semantic understanding. Larger models outperformed smaller ones, and fine-tuning significantly improves classification accuracy, while few-shot prompting yields better results in most cases. Although LLMs offer a faster and reliable way to create metadata, a successful application requires careful consideration of task-specific criteria and domain context. 

**Abstract (ZH)**: 高效的數據探索對於加速過程、提高預測精度和 DEVELOP 新業務模型至關重要。隨著數據的指數級增長、異構性和分佈特性，數據消費者往往會將25-98%的時間用於搜索合適的數據。數據目錄可以通过使用元數據來回答用户查询，从而支持和加速数据探索。但是，由于元数据的创建和维护通常是一个手动过程，这既耗时又需要专业知识。本研究探讨了大型语言模型（LLMs）是否能够自动化基于文本的数据的元数据维护，并生成高质量的DCAT兼容元数据。我们使用来自不同供应商的大型语言模型测试了零样本和少样本提示策略，以生成元数据，例如标题和关键词，并使用微调模型进行分类。结果显示，大型语言模型可以生成与人类创建的内容媲美的元数据，特别是在需要高级语义理解的任务中。较大的模型表现优于较小的模型，微调显著提高了分类准确性，而在大多数情况下，少样本提示策略的效果更佳。尽管大型语言模型提供了更快捷和可靠的方法来创建元数据，但成功应用仍需要针对特定任务和领域上下文进行仔细考虑。 

---
# Hungary and AI: efforts and opportunities in comparison with Singapore 

**Title (ZH)**: 匈牙利与人工智能：与新加坡比较的举措与机遇 

**Authors**: András Ferenczy  

**Link**: [PDF](https://arxiv.org/pdf/2507.05280)  

**Abstract**: The study assesses Hungary's National AI Strategy and its implementation through the analysis of strategic documents, publicly available financial records, and expert interviews with the Hungarian AI Coalition President and Chief Strategic Advisor to the Government Commissioner for AI. 22 goals from Hungary's strategy were evaluated through conceptual, governance, temporal, and financial dimensions before being benchmarked against Singapore's National AI Strategies (NAIS 1.0 and NAIS 2.0). Key findings include an estimated total of EUR 4.65 billion in AI-related public investment in Hungary. Openly available financial data was found for only half of the evaluated goals, and just three projects made up 98\% of all documented funding. The research also reveals Hungary's implementation challenges, including fragmented execution following ministerial reorganizations and the absence of designated biennial reviews since 2020. Furthermore, the paper provides targeted recommendations for Hungary's forthcoming AI strategy, drawing on Singapore's framework as a reference point. These include adapting to the era of large language models, restructuring the existing triple helix network to foster more effective dialogue and advocacy, and positioning the country as an East-West bridge for automotive AI experimentation. 

**Abstract (ZH)**: 匈牙利国家人工智能战略评估：基于战略文件、公开财务记录及与匈牙利人工智能联盟主席和政府人工智能专员首席战略顾问的专家访谈的研究 

---
# ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy 

**Title (ZH)**: ReservoirChat：通过大规模语言模型和知识图谱增强的交互式文档 

**Authors**: Virgile Boraud, Yannis Bendi-Ouis, Paul Bernard, Xavier Hinaut  

**Link**: [PDF](https://arxiv.org/pdf/2507.05279)  

**Abstract**: We introduce a tool designed to improve the capabilities of Large Language Models (LLMs) in assisting with code development using the ReservoirPy library, as well as in answering complex questions in the field of Reservoir Computing. By incorporating external knowledge through Retrieval-Augmented Generation (RAG) and knowledge graphs, our approach aims to reduce hallucinations and increase the factual accuracy of generated responses. The system provides an interactive experience similar to ChatGPT, tailored specifically for ReservoirPy, enabling users to write, debug, and understand Python code while accessing reliable domain-specific insights. In our evaluation, while proprietary models such as ChatGPT-4o and NotebookLM performed slightly better on general knowledge questions, our model outperformed them on coding tasks and showed a significant improvement over its base model, Codestral-22B. 

**Abstract (ZH)**: 我们介绍了一个工具，该工具旨在通过ReservoirPy库提高大型语言模型（LLMs）在代码开发中的辅助能力，以及在蓄水池计算领域的复杂问题解答能力。通过使用检索增强生成（RAG）和知识图谱来融入外部知识，我们的方法旨在减少幻觉并提高生成响应的事实准确性。该系统提供了一个类似于ChatGPT的交互体验，专门针对ReservoirPy，使用户能够在访问可靠的领域特定见解的同时编写、调试和理解Python代码。在评估中，尽管ChatGPT-4o和NotebookLM等商业模型在一般知识问题上表现略好，但我们的模型在编码任务上的表现优于它们，并且相对于其基础模型Codestral-22B显示出显著的改进。 

---
# A Fuzzy Supervisor Agent Design for Clinical Reasoning Assistance in a Multi-Agent Educational Clinical Scenario Simulation 

**Title (ZH)**: 面向多代理教育临床情景模拟的模糊监督代理设计用于临床推理辅助 

**Authors**: Weibing Zheng, Laurah Turner, Jess Kropczynski, Murat Ozer, Seth Overla, Shane Halse  

**Link**: [PDF](https://arxiv.org/pdf/2507.05275)  

**Abstract**: Assisting medical students with clinical reasoning (CR) during clinical scenario training remains a persistent challenge in medical education. This paper presents the design and architecture of the Fuzzy Supervisor Agent (FSA), a novel component for the Multi-Agent Educational Clinical Scenario Simulation (MAECSS) platform. The FSA leverages a Fuzzy Inference System (FIS) to continuously interpret student interactions with specialized clinical agents (e.g., patient, physical exam, diagnostic, intervention) using pre-defined fuzzy rule bases for professionalism, medical relevance, ethical behavior, and contextual distraction. By analyzing student decision-making processes in real-time, the FSA is designed to deliver adaptive, context-aware feedback and provides assistance precisely when students encounter difficulties. This work focuses on the technical framework and rationale of the FSA, highlighting its potential to provide scalable, flexible, and human-like supervision in simulation-based medical education. Future work will include empirical evaluation and integration into broader educational settings. More detailed design and implementation is~\href{this https URL}{open sourced here}. 

**Abstract (ZH)**: 辅助医学学生在临床情景训练中进行临床推理（CR）仍然是医学教育中的一个持续挑战。本文介绍了Fuzzy Supervisor Agent（FSA）的设计与架构，该架构是Multi-Agent Educational Clinical Scenario Simulation（MAECSS）平台的一个新型组件。FSA利用Fuzzy Inference System（FIS）持续解释学生与专业临床代理（如患者、体格检查、诊断、干预）的互动，并使用预先定义的模糊规则库来评估专业性、医学相关性、伦理行为和情境干扰。通过实时分析学生的决策过程，FSA旨在提供适应性、情境相关的反馈，并在学生遇到困难时提供精准的帮助。本文着重于FSA的技术框架和原理，强调其在基于模拟的医学教育中提供可扩展、灵活且人性化监督的潜力。未来的研究将包括实证评估和将其整合到更广泛教育环境中。更多详细设计和实现可在[此处](this https URL)获得。 

---
# FuzzFeed: An Automatic Approach to Weakest Precondition Generation using LLMs and Fuzzing 

**Title (ZH)**: FuzzFeed：一种使用大语言模型和 fuzzing 生成最弱前置条件的自动方法 

**Authors**: Daragh King, Vasileios Koutavas, Laura Kovacs  

**Link**: [PDF](https://arxiv.org/pdf/2507.05272)  

**Abstract**: The weakest precondition (WP) of a program describes the largest set of initial states from which all terminating executions of the program satisfy a given postcondition. The generation of WPs is an important task with practical applications in areas ranging from verification to run-time error checking.
This paper proposes the combination of Large Language Models (LLMs) and fuzz testing for generating WPs. In pursuit of this goal, we introduce Fuzzing Guidance (FG); FG acts as a means of directing LLMs towards correct WPs using program execution feedback. FG utilises fuzz testing for approximately checking the validity and weakness of candidate WPs, this information is then fed back to the LLM as a means of context refinement.
We demonstrate the effectiveness of our approach on a comprehensive benchmark set of deterministic array programs in Java. Our experiments indicate that LLMs are capable of producing viable candidate WPs, and that this ability can be practically enhanced through FG. 

**Abstract (ZH)**: 大型语言模型与 fuzz 测试结合生成最强前置条件的研究 

---
# CORE: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks 

**Title (ZH)**: CORE：通过静态分析任务评估LLM的代码推理能力 

**Authors**: Danning Xie, Mingwei Zheng, Xuwei Liu, Jiannan Wang, Chengpeng Wang, Lin Tan, Xiangyu Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2507.05269)  

**Abstract**: Large language models (LLMs) have been widely adopted across diverse software engineering domains, such as code generation, program repair, and vulnerability detection. These applications require understanding beyond surface-level code patterns: value propagation, control flow, and interdependence between program elements. However, existing benchmarks primarily evaluate end-to-end outcomes, such as whether code is correctly repaired or generated, leaving the models ability for program semantic reasoning underexplored. This work presents CoRe, a high-quality, human-verified benchmark designed to evaluate LLMs on fundamental static analysis tasks. CoRe includes 12,553 task instances spanning data dependency, control dependency, and information flow across programs written in C/C++, Java, and Python. To ensure semantic diversity and reasoning complexity, we propose a semantics-aware diverse sampling strategy that selects targets and task instances based on structural coverage and dependency depth. We evaluate 10 mainstream LLMs and show that, while they perform well at identifying dependencies, models still struggle with tasks that require deeper semantic understanding and multi-step reasoning. We further conduct qualitative analyses to uncover key challenges, such as complex control structures and backward dependency patterns, offering insights into improving LLMs code reasoning capabilities. 

**Abstract (ZH)**: 大型语言模型（LLMs）已在软件工程的多元领域中广泛应用，例如代码生成、程序修复和漏洞检测。这些应用需要理解超越表面代码模式的能力，包括值传递、控制流以及程序元素之间的相互依赖。然而，现有的基准主要评估端到端的结果，如代码是否正确修复或生成，导致程序语义推理的能力尚未被充分探索。本文介绍了一个高质量的人工验证基准CoRe，旨在评估LLMs在基本静态分析任务上的表现。CoRe包含了12,553个任务实例，覆盖了C/C++、Java和Python编写的程序中的数据依赖、控制依赖和信息流。为了确保语义多样性和推理复杂性，我们提出了一种意识语义的多样采样策略，基于结构覆盖率和依赖深度选择目标和任务实例。我们评估了10个主流的LLMs，并展示了虽然它们在识别依赖方面表现良好，但在需要更深层次语义理解与多步推理的任务中仍然存在挑战。进一步的定性分析揭示了关键挑战，如复杂的控制结构和逆向依赖模式，为提升LLMs代码推理能力提供了见解。 

---
# User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs 

**Title (ZH)**: 用户行为预测作为一种通用、稳健、可扩展且低成本的评估策略，用于估计LLMs的泛化能力 

**Authors**: Sougata Saha, Monojit Choudhury  

**Link**: [PDF](https://arxiv.org/pdf/2507.05266)  

**Abstract**: Measuring the generalization ability of Large Language Models (LLMs) is challenging due to data contamination. As models grow and computation becomes cheaper, ensuring tasks and test cases are unseen during training phases will become nearly impossible. We argue that knowledge-retrieval and reasoning tasks are not ideal for measuring generalization, as LLMs are not trained for specific tasks. Instead, we propose user behavior prediction, also a key aspect of personalization, as a theoretically sound, scalable, and robust alternative. We introduce a novel framework for this approach and test it on movie and music recommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct. Results align with our framework's predictions, showing GPT-4o outperforms GPT-4o-mini and Llama, though all models have much room for improvement, especially Llama. 

**Abstract (ZH)**: 测量大型语言模型的泛化能力由于数据污染而具有挑战性。随着模型的增长和计算成本的下降，确保训练阶段中看不到任务和测试案例将变得几乎不可能。我们argue认为知识检索和推理任务不是衡量泛化能力的理想选择，因为LLMs没有针对特定任务进行训练。相反，我们提出了用户行为预测，这也是个性化的一个关键方面，作为理论上有根据、可扩展且稳健的替代方案。我们介绍了一种新的框架并在此方法上对GPT-4o、GPT-4o-mini和Llama-3.1-8B-Instruct的电影和音乐推荐数据集进行了测试。结果与我们框架的预测一致，显示GPT-4o优于GPT-4o-mini和Llama，尽管所有模型都还有很大的提升空间，尤其是Llama。 

---
# Rethinking Over-Smoothing in Graph Neural Networks: A Perspective from Anderson Localization 

**Title (ZH)**: 从冯·诺依曼局域化视角重思图神经网络中的过度平滑问题 

**Authors**: Kaichen Ouyang  

**Link**: [PDF](https://arxiv.org/pdf/2507.05263)  

**Abstract**: Graph Neural Networks (GNNs) have shown great potential in graph data analysis due to their powerful representation capabilities. However, as the network depth increases, the issue of over-smoothing becomes more severe, causing node representations to lose their distinctiveness. This paper analyzes the mechanism of over-smoothing through the analogy to Anderson localization and introduces participation degree as a metric to quantify this phenomenon. Specifically, as the depth of the GNN increases, node features homogenize after multiple layers of message passing, leading to a loss of distinctiveness, similar to the behavior of vibration modes in disordered systems. In this context, over-smoothing in GNNs can be understood as the expansion of low-frequency modes (increased participation degree) and the localization of high-frequency modes (decreased participation degree). Based on this, we systematically reviewed the potential connection between the Anderson localization behavior in disordered systems and the over-smoothing behavior in Graph Neural Networks. A theoretical analysis was conducted, and we proposed the potential of alleviating over-smoothing by reducing the disorder in information propagation. 

**Abstract (ZH)**: 图神经网络（GNNs）由于其强大的表示能力，在图数据分析中展现出了巨大潜力。然而，随着网络深度的增加，过拟合问题变得更加严重，导致节点表示失真。本文通过将过拟合与安德森局域化进行类比，引入参与度作为度量这一现象的指标。具体而言，随着GNN层数的增加，经过多层消息传递后节点特征同质化，导致失真现象，类似于无序系统中振动模式的行为。在此背景下，GNN中的过拟合可以理解为低频模式的扩展（参与度增加）和高频模式的局域化（参与度减少）。基于此，我们系统地探讨了无序系统中的安德森局域化行为与图神经网络中过拟合行为之间的潜在联系，并进行了理论分析，提出了通过减少信息传播中的无序来缓解过拟合的潜在途径。 

---
# ABench-Physics: Benchmarking Physical Reasoning in LLMs via High-Difficulty and Dynamic Physics Problems 

**Title (ZH)**: ABench-Physics: 通过高难度和动态物理问题评估LLM中的物理推理能力 

**Authors**: Yiming Zhang, Yingfan Ma, Yanmei Gu, Zhengkai Yang, Yihong Zhuang, Feng Wang, Zenan Huang, Yuanyuan Wang, Chao Huang, Bowen Song, Cheng Lin, Junbo Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2507.04766)  

**Abstract**: Large Language Models (LLMs) have shown impressive performance in domains such as mathematics and programming, yet their capabilities in physics remain underexplored and poorly understood. Physics poses unique challenges that demand not only precise computation but also deep conceptual understanding and physical modeling skills. Existing benchmarks often fall short due to limited difficulty, multiple-choice formats, and static evaluation settings that fail to capture physical modeling ability. In this paper, we introduce ABench-Physics, a novel benchmark designed to rigorously evaluate LLMs' physical reasoning and generalization capabilities. ABench-Physics consists of two components: Phy_A, a static set of 400 graduate- or Olympiad-level problems; and Phy_B, a dynamic subset of 100 problems equipped with an automatic variation engine to test model robustness across changing conditions. All questions require precise numerical answers, with strict formatting and tolerance constraints. Our evaluation of several state-of-the-art LLMs reveals substantial performance gaps, highlighting persistent limitations in physical reasoning, especially in generalization to dynamic variants. ABench-Physics provides a challenging and diagnostic framework for advancing scientific reasoning in LLMs. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在数学和编程领域展示了 impressive 的性能，但在物理学领域的能力和潜力仍被严重低估且不甚了解。物理学提出了独特的挑战，不仅要求精确的计算，还需要深厚的conceptual理解以及物理建模能力。现有基准往往由于难度有限、选择题格式以及静态评估设置等问题而显得不足，无法充分捕捉物理建模能力。在本文中，我们引入了 ABench-Physics，这一新型基准旨在严格评估LLMs的物理推理和泛化能力。ABench-Physics 包括两个部分：Phy_A，包含400道研究生水平或奥林匹克级别问题的静态问题集；Phy_B，包含100道动态问题的子集，并配备自动变体引擎以测试模型在不同条件下的鲁棒性。所有问题要求精确的数字答案，并有严格的格式和容差约束。我们的评估结果显示了显著的性能差距，突显了物理推理的持续局限性，尤其是在动态变体的泛化能力方面。ABench-Physics 提供了一个具有挑战性和诊断性的框架，以促进LLMs中的科学推理能力。 

---
# Challenges & Opportunities with LLM-Assisted Visualization Retargeting 

**Title (ZH)**: LLM辅助视觉重定位的机遇与挑战 

**Authors**: Luke S. Snyder, Chenglong Wang, Steven M. Drucker  

**Link**: [PDF](https://arxiv.org/pdf/2507.01436)  

**Abstract**: Despite the ubiquity of visualization examples published on the web, retargeting existing custom chart implementations to new datasets remains difficult, time-intensive, and tedious. The adaptation process assumes author familiarity with both the implementation of the example as well as how the new dataset might need to be transformed to fit into the example code. With recent advances in Large Language Models (LLMs), automatic adaptation of code can be achieved from high-level user prompts, reducing the barrier for visualization retargeting. To better understand how LLMs can assist retargeting and its potential limitations, we characterize and evaluate the performance of LLM assistance across multiple datasets and charts of varying complexity, categorizing failures according to type and severity. In our evaluation, we compare two approaches: (1) directly instructing the LLM model to fully generate and adapt code by treating code as text inputs and (2) a more constrained program synthesis pipeline where the LLM guides the code construction process by providing structural information (e.g., visual encodings) based on properties of the example code and data. We find that both approaches struggle when new data has not been appropriately transformed, and discuss important design recommendations for future retargeting systems. 

**Abstract (ZH)**: 尽管网络上充斥着大量的可视化示例，将现有的自定义图表实现应用到新数据集仍然困难重重、耗时且繁琐。这一适应过程需要作者熟悉示例的实现方式以及新数据集可能需要如何转换以适应示例代码。随着大型语言模型（LLMs）的最新进展，可以从高级用户提示自动适应代码，从而降低可视化重新应用的门槛。为了更好地理解LLMs如何辅助重新应用及其潜在局限性，我们对多种不同复杂度的数据集和图表进行了特征描述和性能评估，并按类型和严重程度对失败进行了分类。在我们的评估中，我们将两种方法进行比较：（1）直接指示LLM模型生成并适应代码，将代码视为文本输入，以及（2）一种更受限的程序合成管道，其中LLM通过提供结构信息（例如，视觉编码）来引导代码构建过程，这些结构信息基于示例代码和数据的特性。我们发现，这两种方法在新数据未适当转换时均面临挑战，并讨论了未来重新应用系统的重要设计建议。 

---
# The Problem of Algorithmic Collisions: Mitigating Unforeseen Risks in a Connected World 

**Title (ZH)**: 算法碰撞的问题： mitigation of unforeseen risks in a connected world 

**Authors**: Maurice Chiodo, Dennis Müller  

**Link**: [PDF](https://arxiv.org/pdf/2505.20181)  

**Abstract**: The increasing deployment of Artificial Intelligence (AI) and other autonomous algorithmic systems presents the world with new systemic risks. While focus often lies on the function of individual algorithms, a critical and underestimated danger arises from their interactions, particularly when algorithmic systems operate without awareness of each other, or when those deploying them are unaware of the full algorithmic ecosystem deployment is occurring in. These interactions can lead to unforeseen, rapidly escalating negative outcomes - from market crashes and energy supply disruptions to potential physical accidents and erosion of public trust - often exceeding the human capacity for effective monitoring and the legal capacities for proper intervention. Current governance frameworks are inadequate as they lack visibility into this complex ecosystem of interactions. This paper outlines the nature of this challenge and proposes some initial policy suggestions centered on increasing transparency and accountability through phased system registration, a licensing framework for deployment, and enhanced monitoring capabilities. 

**Abstract (ZH)**: 不断增加的人工智能及其他自主算法系统的部署为世界带来了新的系统性风险。 

---
# Formalising Human-in-the-Loop: Computational Reductions, Failure Modes, and Legal-Moral Responsibility 

**Title (ZH)**: 形式化人类在环中参与：计算减少、失败模式和法律-道德责任 

**Authors**: Maurice Chiodo, Dennis Müller, Paul Siewert, Jean-Luc Wetherall, Zoya Yasmine, John Burden  

**Link**: [PDF](https://arxiv.org/pdf/2505.10426)  

**Abstract**: The legal compliance and safety of different Human-in-the-loop (HITL) setups for AI can vary greatly. This manuscript aims to identify new ways of choosing between such setups, and shows that there is an unavoidable trade-off between the attribution of legal responsibility and the technical explainability of AI. We begin by using the notion of oracle machines from computability theory to formalise different HITL setups, distinguishing between trivial human monitoring, single endpoint human action, and highly involved interaction between the human(s) and the AI. These correspond to total functions, many-one reductions, and Turing reductions respectively. A taxonomy categorising HITL failure modes is then presented, highlighting the limitations on what any HITL setup can actually achieve. Our approach then identifies oversights from UK and EU legal frameworks, which focus on certain HITL setups which may not always achieve the desired ethical, legal, and sociotechnical outcomes. We suggest areas where the law should recognise the effectiveness of different HITL setups and assign responsibility in these contexts, avoiding unnecessary and unproductive human "scapegoating". Overall, we show how HITL setups involve many technical design decisions, and can be prone to failures which are often out of the humans' control. This opens up a new analytic perspective on the challenges arising in the creation of HITL setups, helping inform AI developers and lawmakers on designing HITL to better achieve their desired outcomes. 

**Abstract (ZH)**: 不同的人工智能半自动回环（HITL）设置的法律合规性和安全性可能存在巨大差异。本论文旨在探索选择不同HITL设置的新方法，并表明在法律归责与人工智能技术可解释性之间存在不可避免的权衡。我们首先利用可计算理论中的 oracle 机器概念来形式化不同的 HITL 设置，分别区分简易的人工监控、单一终端的人工操作以及人类与人工智能高度互动的情况。这些分别对应完全函数、many-one 归约和图灵归约。然后，我们提出了一个分类法来归纳 HITL 失败模式，强调任何 HITL 设置实际上能够实现的局限性。接下来，我们指出了来自英国和欧盟法律框架中的盲点，这些框架往往侧重于某些 HITL 设置，这些设置可能不能总是实现所需的伦理、法律和社会技术结果。我们建议法律在不同 HITL 设置的有效性方面应予以认可，并在这类情境中分配责任，避免不必要的和没有成效的人类“替罪羊”现象。总体而言，本论文展示了如何 HITL 设置涉及众多技术设计决策，并可能因超出人类控制范畴的失败而受到威胁。这为理解 HITL 设置带来的挑战提供了新的分析视角，有助于指导人工智能开发者和立法者更好地设计 HITL，以实现其预期目标。 

---
# Integrators at War: Mediating in AI-assisted Resort-to-Force Decisions 

**Title (ZH)**: 人工智能辅助下的使用武力决策中的调解者：矛盾与调和 

**Authors**: Dennis Müller, Maurice Chiodo, Mitja Sienknecht  

**Link**: [PDF](https://arxiv.org/pdf/2501.06861)  

**Abstract**: The integration of AI systems into the military domain is changing the way war-related decisions are made. It binds together three disparate groups of actors - developers, integrators, users - and creates a relationship between these groups and the machine, embedded in the (pre-)existing organisational and system structures. In this article, we focus on the important, but often neglected, group of integrators within such a sociotechnical system. In complex human-machine configurations, integrators carry responsibility for linking the disparate groups of developers and users in the political and military system. To act as the mediating group requires a deep understanding of the other groups' activities, perspectives and norms. We thus ask which challenges and shortcomings emerge from integrating AI systems into resort-to-force (RTF) decision-making processes, and how to address them. To answer this, we proceed in three steps. First, we conceptualise the relationship between different groups of actors and AI systems as a sociotechnical system. Second, we identify challenges within such systems for human-machine teaming in RTF decisions. We focus on challenges that arise a) from the technology itself, b) from the integrators' role in the sociotechnical system, c) from the human-machine interaction. Third, we provide policy recommendations to address these shortcomings when integrating AI systems into RTF decision-making structures. 

**Abstract (ZH)**: 人工智能系统在军事领域的整合正改变战争相关决策的方式。这种整合将开发者、集成者和用户这三个不同时的行动者群体结合在一起，嵌入到（预）现有的组织和系统结构中，形成其间的关係。本文旨在关注这样的社会技术系统中经常被忽略的集成者群体。在复杂的人机配置中，集成者承担着连接开发者和用户的政治和军事系统的责任。为了充当中介群体，需要深刻理解其他群体的活动、视角和规范。因此，本文探讨将人工智能系统整合到使用武力决策过程中的挑战和不足，并提出相应的解决建议。首先，本文将不同群体的行动者与人工智能系统之间的关系概念化为社会技术系统。第二，本文识别此类系统中的人机团队在使用武力决策中的挑战。我们重点关注来自技术本身、集成者在社会技术系统中的角色以及人机互动的挑战。第三，本文提供政策建议，以解决在将人工智能系统整合到使用武力决策结构中时的不足。 

---
