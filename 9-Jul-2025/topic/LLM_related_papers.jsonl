{'arxiv_id': 'arXiv:2507.05754', 'title': 'LeAD: The LLM Enhanced Planning System Converged with End-to-end Autonomous Driving', 'authors': 'Yuhang Zhang, Jiaqi Liu, Chengkai Xu, Peng Hang, Jian Sun', 'link': 'https://arxiv.org/abs/2507.05754', 'abstract': "A principal barrier to large-scale deployment of urban autonomous driving systems lies in the prevalence of complex scenarios and edge cases. Existing systems fail to effectively interpret semantic information within traffic contexts and discern intentions of other participants, consequently generating decisions misaligned with skilled drivers' reasoning patterns. We present LeAD, a dual-rate autonomous driving architecture integrating imitation learning-based end-to-end (E2E) frameworks with large language model (LLM) augmentation. The high-frequency E2E subsystem maintains real-time perception-planning-control cycles, while the low-frequency LLM module enhances scenario comprehension through multi-modal perception fusion with HD maps and derives optimal decisions via chain-of-thought (CoT) reasoning when baseline planners encounter capability limitations. Our experimental evaluation in the CARLA Simulator demonstrates LeAD's superior handling of unconventional scenarios, achieving 71 points on Leaderboard V1 benchmark, with a route completion of 93%.", 'abstract_zh': '大规模部署城市自动驾驶系统的主要障碍在于复杂场景和边缘情况的普遍存在。现有系统无法有效解读交通场景中的语义信息，也无法辨识其他参与者的意图，从而导致决策与熟练驾驶员的推理模式不一致。我们提出了一种LeAD双速率自动驾驶架构，该架构结合了基于模仿学习的端到端（E2E）框架与大型语言模型（LLM）增强。高频次的E2E子系统保持实时感知-规划-控制循环，而低频次的LLM模块通过多模态感知融合高精度地图来增强场景理解，并在基础规划器遇到能力限制时通过链式推理（CoT）生成最优决策。我们在CARLA仿真器上的实验评估表明，LeAD在Leaderboard V1基准测试中表现出色，获得了71分，路线完成率为93%。', 'title_zh': 'LeAD：融合端到端自主驾驶的LLM增强规划系统'}
{'arxiv_id': 'arXiv:2507.06187', 'title': 'The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains', 'authors': 'Scott Geng, Hamish Ivison, Chun-Liang Li, Maarten Sap, Jerry Li, Ranjay Krishna, Pang Wei Koh', 'link': 'https://arxiv.org/abs/2507.06187', 'abstract': "Improvements in language models are often driven by improving the quality of the data we train them on, which can be limiting when strong supervision is scarce. In this work, we show that paired preference data consisting of individually weak data points can enable gains beyond the strength of each individual data point. We formulate the delta learning hypothesis to explain this phenomenon, positing that the relative quality delta between points suffices to drive learning via preference tuning--even when supervised finetuning on the weak data hurts. We validate our hypothesis in controlled experiments and at scale, where we post-train 8B models on preference data generated by pairing a small 3B model's responses with outputs from an even smaller 1.5B model to create a meaningful delta. Strikingly, on a standard 11-benchmark evaluation suite (MATH, MMLU, etc.), our simple recipe matches the performance of Tulu 3, a state-of-the-art open model tuned from the same base model while relying on much stronger supervisors (e.g., GPT-4o). Thus, delta learning enables simpler and cheaper open recipes for state-of-the-art post-training. To better understand delta learning, we prove in logistic regression that the performance gap between two weak teacher models provides useful signal for improving a stronger student. Overall, our work shows that models can learn surprisingly well from paired data that might typically be considered weak.", 'abstract_zh': '通过配对偏好数据改进语言模型：弱数据点的相对质量差异驱动学习的假设', 'title_zh': '德尔塔学习假设：在弱数据上调整偏好可以带来显著收益'}
{'arxiv_id': 'arXiv:2507.06057', 'title': 'FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large Language Models', 'authors': 'Bo Pang, Yalu Ouyang, Hangfei Xu, Ziqi Jia, Panpan Li, Shengzhao Wen, Lu Wang, Shiyong Li, Yanpeng Wang', 'link': 'https://arxiv.org/abs/2507.06057', 'abstract': 'Advancements in reasoning for large language models (LLMs) have lead to significant performance improvements for LLMs in various fields such as mathematics and programming. However, research applying these advances to the financial domain, where considerable domain-specific knowledge is necessary to complete tasks, remains limited. To address this gap, we introduce FEVO (Financial Evolution), a multi-stage enhancement framework developed to enhance LLM performance in the financial domain. FEVO systemically enhances LLM performance by using continued pre-training (CPT) to expand financial domain knowledge, supervised fine-tuning (SFT) to instill structured, elaborate reasoning patterns, and reinforcement learning (RL) to further integrate the expanded financial domain knowledge with the learned structured reasoning. To ensure effective and efficient training, we leverage frontier reasoning models and rule-based filtering to curate FEVO-Train, high-quality datasets specifically designed for the different post-training phases. Using our framework, we train the FEVO series of models -- C32B, S32B, R32B -- from Qwen2.5-32B and evaluate them on seven benchmarks to assess financial and general capabilities, with results showing that FEVO-R32B achieves state-of-the-art performance on five financial benchmarks against much larger models as well as specialist models. More significantly, FEVO-R32B demonstrates markedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct using only RL), thus validating the effectiveness of financial domain knowledge expansion and structured, logical reasoning distillation', 'abstract_zh': '大型语言模型（LLMs）推理进展显著提升了其在数学和编程等领域的性能。然而，这些进展在金融领域的应用研究仍比较有限，考虑到金融领域完成任务所需的专业知识相当丰富。为填补这一空白，我们提出了FEVO（Financial Evolution）框架，旨在增强LLMs在金融领域的性能。FEVO通过连续预训练（CPT）扩展金融领域知识，通过监督微调（SFT）植入结构化的、复杂的推理模式，通过强化学习（RL）进一步整合扩展的金融领域知识和学习到的结构化推理模式，从而系统性地增强LLMs的性能。为确保有效高效的训练，我们利用前沿推理模型和基于规则的过滤器精心设计了FEVO-Train高质量数据集，专为不同的后训练阶段设计。我们使用FEVO框架训练了FEVO系列模型——C32B、S32B、R32B，并在七个基准测试中评估了它们的金融和通用能力，结果显示FEVO-R32B在五个金融基准测试中达到了最新性能水平，优于更大规模的模型及专门模型。更显著的是，FEVO-R32B的性能显著优于仅使用RL训练的FEVO-R32B-0（基于Qwen2.5-32B-Instruct），从而验证了金融领域知识扩展和结构化逻辑推理提炼的有效性。', 'title_zh': 'FEVO: 金融知识扩展与推理演进的大语言模型'}
{'arxiv_id': 'arXiv:2507.06013', 'title': 'CogniSQL-R1-Zero: Lightweight Reinforced Reasoning for Efficient SQL Generation', 'authors': 'Kushal Gajjar, Harshit Sikchi, Arpit Singh Gautam, Marc Hammons, Saurabh Jha', 'link': 'https://arxiv.org/abs/2507.06013', 'abstract': 'Translating natural language into SQL (Text-to-SQL) remains a core challenge at the intersection of language understanding and structured data access. Although large language models (LLMs) have improved fluency, generating correct and executable SQL, especially for complex queries, continues to be challenging. We introduce CogniSQL-R1-Zero, a reinforcement learning (RL) framework and model that produces accurate SQL using a lightweight reward signal based on execution correctness and format-tag compliance. By avoiding intermediate supervision, hybrid pipelines and complex reward shaping, our method encourages stable learning and stronger alignment with the ultimate task objective-producing executable programs. CogniSQL-R1-Zero achieves state-of-the-art execution accuracy on Text2SQL benchmark; BIRD bench, outperforming prior supervised and instruction-tuned baselines including SFT CodeS-7B, DeepSeek-Coder 236B, and Mistral 123B-despite being trained on a significantly smaller 7B backbone. This result underscores the scalability and efficiency of our RL-based approach when trained on just four NVIDIA A100 GPUs (40 GB VRAM each). To support further research in efficient and interpretable Text-to-SQL modeling, we release two curated datasets: (i) a collection of 5,024 reasoning traces with varying context lengths, and (ii) a positive-sampled corpus of 36,356 corpus of weakly supervised queries, each annotated with six semantically diverse reasoning paths. Together, these contributions advance scalable, execution-aligned Text-to-SQL generation.', 'abstract_zh': '自然语言到SQL的翻译（Text-to-SQL）仍然是语言理解与结构化数据访问交叉领域的一个核心挑战。尽管大规模语言模型（LLMs）在流畅性上有所提升，但对于生成正确的且可执行的SQL，尤其是复杂的查询，仍然具有挑战性。我们引入了CogniSQL-R1-Zero，这是一种基于执行正确性和格式标记合规性的轻量级奖励信号的强化学习（RL）框架和模型，该模型能够生成准确的SQL。通过避免中间监督、混合管道和复杂的奖励塑造，我们的方法促进了稳定的学习并增强了与最终任务目标的对齐——生成可执行程序。CogniSQL-R1-Zero在Text2SQL基准和BIRD基准上实现了最佳执行准确性，优于包括SFT CodeS-7B、DeepSeek-Coder 236B和Mistral 123B在内的先前监督和指令调优基线，尽管使用的是规模小得多的7B基础模型。这一结果突显了当仅使用四块NVIDIA A100 GPU（每块40 GB显存）进行训练时，基于强化学习的方法的可扩展性和效率。为了支持高效可解释的Text-to-SQL建模的进一步研究，我们发布了两个精心收集的数据集：（i）包含5,024条推理跟踪并具有变化的上下文长度的集合，（ii）一个包含36,356个弱监督查询的积极采样语料库，每个查询都标注了六条语义上不同的推理路径。这两项贡献推进了可扩展的、与执行对齐的Text-to-SQL生成。', 'title_zh': 'CogniSQL-R1-Zero：轻量级强化推理以实现高效的SQL生成'}
{'arxiv_id': 'arXiv:2507.05984', 'title': 'Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening', 'authors': 'Zhijun Guo, Alvina Lai, Julia Ive, Alexandru Petcu, Yutong Wang, Luyuan Qi, Johan H Thygesen, Kezhi Li', 'link': 'https://arxiv.org/abs/2507.05984', 'abstract': 'Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively screen depression but lack interactivity and adaptability. We developed HopeBot, a chatbot powered by a large language model (LLM) that administers the PHQ-9 using retrieval-augmented generation and real-time clarification. In a within-subject study, 132 adults in the United Kingdom and China completed both self-administered and chatbot versions. Scores demonstrated strong agreement (ICC = 0.91; 45% identical). Among 75 participants providing comparative feedback, 71% reported greater trust in the chatbot, highlighting clearer structure, interpretive guidance, and a supportive tone. Mean ratings (0-10) were 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics, and 7.4 for recommendation helpfulness; the latter varied significantly by employment status and prior mental-health service use (p < 0.05). Overall, 87.1% expressed willingness to reuse or recommend HopeBot. These findings demonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden adjuncts for routine depression screening.', 'abstract_zh': '静态工具如患者健康问卷-9（PHQ-9）有效筛查抑郁但缺乏互动性和适应性。我们开发了HopeBot，这是一种由大规模语言模型（LLM）驱动的聊天机器人，利用检索增强生成和实时澄清来管理PHQ-9。在一项针对主题内研究中，132名来自英国和中国的成年人完成了自我管理和聊天机器人版本的问卷。评分显示高度一致（ICC = 0.91；45%相同）。在提供比较反馈的75名参与者中，71%表示更信任聊天机器人，强调了其清晰的结构、解释性指导和支持性的语气。舒适度、语音清晰度、处理敏感话题和推荐有用性的平均评分分别为8.4、7.7、7.6和7.4，后者根据就业状态和先前的心理健康服务使用情况显著有所不同（p < 0.05）。总体而言，87.1%的人表示愿意再次使用或推荐HopeBot。这些发现表明，基于语音的大规模语言模型聊天机器人可以作为一种可行的、低负担的辅助工具，用于常规抑郁筛查。', 'title_zh': '基于LLM的HopeBot：一种结构化和交互式的PHQ-9抑郁筛查聊天机器人开发与评估'}
{'arxiv_id': 'arXiv:2507.05934', 'title': 'BlueLM-2.5-3B Technical Report', 'authors': 'Baojiao Xiong, Boheng Chen, Chengzhi Wang, Daxiong Luo, Dongsheng Xu, Dongyang Liu, Fan Yang, Fangyuan Li, Fei Teng, Feng Wang, Fukang Qin, Fuquan Peng, Guanxin Tan, Guozhi Wang, Haibo Yu, Haohao Gao, Heng Liu, Hongbo Yang, Hongjian Zou, Houzheng Shen, Hu Meng, Huan Li, Hui Tan, Jiali Chen, Jianzhao Chen, Jinliang Zhu, Kai Wang, Lei Wu, Liangbing Liu, Liuyang Bian, Liyan He, Long Liu, Peiwen Li, Penggang Shi, Qi Ding, Rui Hu, Shuai Cao, Shuai Ren, Shuang Peng, Teng Xie, Weiji Chen, Weilin Xiang, Weixin Wu, Xi Yin, Xiaoxin Chen, Xu Chen, Yafei Wen, Yan Hu, Yanzhou Yang, Yina Xie, Yinghao Chen, Yixuan Liao, Yu Geng, Yuanjiang Ouyang, Yuanzhuo Yang, Yuehua He, Yushuai Peng, Zhaoxiong Wang, Zheng Wang, Zhibo Zhou, Ziyang Wu', 'link': 'https://arxiv.org/abs/2507.05934', 'abstract': 'We present BlueLM-2.5-3B, a compact and unified dense Multimodal Large Language Model (MLLM) designed for efficient edge-device deployment, offering strong general-purpose and reasoning capabilities. To the best of our knowledge, this is the first 3B-scale MLLM to support both thinking and non-thinking modes, while also enabling explicit control over thinking token budget. BlueLM-2.5-3B is developed through diversified data curation, key data resampling, hybrid heterogeneous reinforcement learning, and a high-performance training infrastructure. Our model achieves superior multimodal capacity while preserving competitive pure-text performance with only 2.9 billion parameters. We conduct comprehensive evaluations across a broad range of multimodal and text-only benchmarks. In thinking mode, BlueLM-2.5-3B achieves comparable performance to Qwen3-4B on text-only benchmarks, and trails the larger Kimi-VL-A3B-16B by only about 5% on average across multimodal evaluations. In non-thinking mode, it outperforms Qwen2.5-VL-3B on the majority of multimodal benchmarks. Additionally, BlueLM-2.5-3B exhibits exceptional data efficiency. All of the aforementioned performance is achieved with substantially less total training data than Qwen2.5-VL-3B and Qwen3-4B. We hope our work contributes to the advancement of high-performance, on-device MLLMs and provides meaningful insights to the research community.', 'abstract_zh': '我们呈现BlueLM-2.5-3B：一种面向边缘设备部署的紧凑统一密集多模态大型语言模型', 'title_zh': 'BlueLM-2.5-3B 技术报告'}
{'arxiv_id': 'arXiv:2507.05886', 'title': 'Current Practices for Building LLM-Powered Reasoning Tools Are Ad Hoc -- and We Can Do Better', 'authors': 'Aaron Bembenek', 'link': 'https://arxiv.org/abs/2507.05886', 'abstract': 'There is growing excitement about building software verifiers, synthesizers, and other Automated Reasoning (AR) tools by combining traditional symbolic algorithms and Large Language Models (LLMs). Unfortunately, the current practice for constructing such neurosymbolic AR systems is an ad hoc programming model that does not have the strong guarantees of traditional symbolic algorithms, nor a deep enough synchronization of neural networks and symbolic reasoning to unlock the full potential of LLM-powered reasoning. I propose Neurosymbolic Transition Systems as a principled computational model that can underlie infrastructure for building neurosymbolic AR tools. In this model, symbolic state is paired with intuition, and state transitions operate over symbols and intuition in parallel. I argue why this new paradigm can scale logical reasoning beyond current capabilities while retaining the strong guarantees of symbolic algorithms, and I sketch out how the computational model I propose can be reified in a logic programming language.', 'abstract_zh': '基于神经符号过渡系统的自动推理工具原理计算模型', 'title_zh': '当前为构建LLM驱动的推理工具的做法是临时性的——我们可以做得更好。'}
{'arxiv_id': 'arXiv:2507.05816', 'title': 'Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity', 'authors': 'Shuai Zhao, Yulin Zhang, Luwei Xiao, Xinyi Wu, Yanhao Jia, Zhongliang Guo, Xiaobao Wu, Cong-Duy Nguyen, Guoming Zhang, Anh Tuan Luu', 'link': 'https://arxiv.org/abs/2507.05816', 'abstract': "Despite the remarkable progress of large language models (LLMs) across various domains, their capacity to predict retinopathy of prematurity (ROP) risk remains largely unexplored. To address this gap, we introduce a novel Chinese benchmark dataset, termed CROP, comprising 993 admission records annotated with low, medium, and high-risk labels. To systematically examine the predictive capabilities and affective biases of LLMs in ROP risk stratification, we propose Affective-ROPTester, an automated evaluation framework incorporating three prompting strategies: Instruction-based, Chain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme assesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and ICL schemes leverage external medical knowledge to enhance predictive accuracy. Crucially, we integrate emotional elements at the prompt level to investigate how different affective framings influence the model's ability to predict ROP and its bias patterns. Empirical results derived from the CROP dataset yield two principal observations. First, LLMs demonstrate limited efficacy in ROP risk prediction when operating solely on intrinsic knowledge, yet exhibit marked performance gains when augmented with structured external inputs. Second, affective biases are evident in the model outputs, with a consistent inclination toward overestimating medium- and high-risk cases. Third, compared to negative emotions, positive emotional framing contributes to mitigating predictive bias in model outputs. These findings highlight the critical role of affect-sensitive prompt engineering in enhancing diagnostic reliability and emphasize the utility of Affective-ROPTester as a framework for evaluating and mitigating affective bias in clinical language modeling systems.", 'abstract_zh': '尽管大型语言模型（LLMs）在各个领域取得了显著进展，但它们预测早产儿视网膜病变（ROP）风险的能力仍 largely unexplored。为弥补这一不足，我们引入了一个新型中文基准数据集 CROP，包含 993 份住院记录，标注为低风险、中风险和高风险。为系统地考察语言模型在 ROP 风险分层中的预测能力和情感偏差，我们提出了 Affective-ROPTester，这是一种结合三种提示策略的自动评估框架：指令（Instruction）、逐步推理（Chain-of-Thought, CoT）和上下文学习（In-Context Learning, ICL）。指令方案评估模型的内在知识及其相关偏差，而 CoT 和 ICL 方案则借助外部医学知识来提高预测准确性。关键在于我们在提示层面整合了情感元素，以探讨不同情感框架如何影响模型预测 ROP 及其偏差模式的能力。从 CROP 数据集得出的实证结果揭示了两项主要发现。首先，仅依赖内在知识时，LLMs 在 ROP 风险预测中效果有限，但结合结构化外部输入后，性能显著提升。其次，模型输出中存在情感偏差，表现为对中风险和高风险病例的过度估计。第三，与负面情绪相比，正面情绪框架有助于减轻模型输出中的预测偏差。这些发现强调了情感敏感提示工程在提高诊断可靠性和评估及减轻临床语言模型中情感偏差方面的关键作用。', 'title_zh': '情感化ROP测试器：预测极早产儿视网膜病变能力及偏见分析'}
{'arxiv_id': 'arXiv:2507.05638', 'title': 'LLMs are Introvert', 'authors': 'Litian Zhang, Xiaoming Zhang, Bingyu Yan, Ziyi Zhou, Bo Zhang, Zhenyu Guan, Xi Zhang, Chaozhuo Li', 'link': 'https://arxiv.org/abs/2507.05638', 'abstract': "The exponential growth of social media and generative AI has transformed information dissemination, fostering connectivity but also accelerating the spread of misinformation. Understanding information propagation dynamics and developing effective control strategies is essential to mitigate harmful content. Traditional models, such as SIR, provide basic insights but inadequately capture the complexities of online interactions. Advanced methods, including attention mechanisms and graph neural networks, enhance accuracy but typically overlook user psychology and behavioral dynamics. Large language models (LLMs), with their human-like reasoning, offer new potential for simulating psychological aspects of information spread. We introduce an LLM-based simulation environment capturing agents' evolving attitudes, emotions, and responses. Initial experiments, however, revealed significant gaps between LLM-generated behaviors and authentic human dynamics, especially in stance detection and psychological realism. A detailed evaluation through Social Information Processing Theory identified major discrepancies in goal-setting and feedback evaluation, stemming from the lack of emotional processing in standard LLM training. To address these issues, we propose the Social Information Processing-based Chain of Thought (SIP-CoT) mechanism enhanced by emotion-guided memory. This method improves the interpretation of social cues, personalization of goals, and evaluation of feedback. Experimental results confirm that SIP-CoT-enhanced LLM agents more effectively process social information, demonstrating behaviors, attitudes, and emotions closer to real human interactions. In summary, this research highlights critical limitations in current LLM-based propagation simulations and demonstrates how integrating SIP-CoT and emotional memory significantly enhances the social intelligence and realism of LLM agents.", 'abstract_zh': '基于社会信息处理的链式思维机制增强的情感引导记忆在社交媒体信息传播模拟中的应用', 'title_zh': 'LLMs是内向的'}
{'arxiv_id': 'arXiv:2507.05629', 'title': 'Enhancing Student Learning with LLM-Generated Retrieval Practice Questions: An Empirical Study in Data Science Courses', 'authors': 'Yuan An, John Liu, Niyam Acharya, Ruhma Hashmi', 'link': 'https://arxiv.org/abs/2507.05629', 'abstract': 'Retrieval practice is a well-established pedagogical technique known to significantly enhance student learning and knowledge retention. However, generating high-quality retrieval practice questions is often time-consuming and labor intensive for instructors, especially in rapidly evolving technical subjects. Large Language Models (LLMs) offer the potential to automate this process by generating questions in response to prompts, yet the effectiveness of LLM-generated retrieval practice on student learning remains to be established. In this study, we conducted an empirical study involving two college-level data science courses, with approximately 60 students. We compared learning outcomes during one week in which students received LLM-generated multiple-choice retrieval practice questions to those from a week in which no such questions were provided. Results indicate that students exposed to LLM-generated retrieval practice achieved significantly higher knowledge retention, with an average accuracy of 89%, compared to 73% in the week without such practice. These findings suggest that LLM-generated retrieval questions can effectively support student learning and may provide a scalable solution for integrating retrieval practice into real-time teaching. However, despite these encouraging outcomes and the potential time-saving benefits, cautions must be taken, as the quality of LLM-generated questions can vary. Instructors must still manually verify and revise the generated questions before releasing them to students.', 'abstract_zh': '基于检索练习的生成式大型语言模型在提高学生学习效果中的应用：一项 empirical 研究', 'title_zh': '利用生成式检索练习题增强学生学习：数据科学课程中的实证研究'}
{'arxiv_id': 'arXiv:2507.05613', 'title': 'Domain adaptation of large language models for geotechnical applications', 'authors': 'Lei Fan, Fangxue Liu, Cheng Chen', 'link': 'https://arxiv.org/abs/2507.05613', 'abstract': 'Recent developments in large language models (LLMs) are opening up new opportunities in geotechnical engineering and engineering geology. While general-purpose LLMs possess broad capabilities, effective application in geotechnics often requires domain-specific adaptation. Such tailored LLMs are increasingly employed to streamline geotechnical workflows. This paper presents the first survey of the adaptation and application of LLMs in geotechnical engineering. It outlines key methodologies for adaptation to geotechnical domain, including prompt engineering, retrieval-augmented generation, domain-adaptive pretraining, and fine-tuning. The survey examines the state-of-the-art applications of geotechnical-adapted LLMs, including geological interpretation, subsurface characterization, site planning, design calculations, numerical modeling, safety and risk assessment, and educational tutoring. It also analyzes benefits and limitations of geotechnical-adapted LLMs, and identifies promising directions for future research in this interdisciplinary discipline. The findings serve as a valuable resource for practitioners seeking to integrate LLMs into geotechnical practice, while also providing a foundation to stimulate further investigation within the academic community.', 'abstract_zh': 'Recent developments in大型语言模型（LLMs）为地质工程和工程地质领域开辟了新机遇。虽然通用的LLMs具有广泛的性能，但在地质工程中的有效应用通常需要专门的领域适应。此类定制化的LLMs越来越被用于简化地质工程工作流程。本文提供了首个关于地质工程中LLM适应与应用的综述。概述了适应地质工程领域的关键方法，包括提示工程、检索增强生成、领域适应预训练和微调。综述还探讨了地质工程适应的LLMs的最新应用，包括地质解释、地下表征、场地规划、设计计算、数值建模、安全与风险评估以及教育辅导。分析了地质工程适应的LLMs的优势与局限性，并指出了未来研究的有前途的方向。研究发现为希望将LLMs整合到地质工程实践中的人士提供了宝贵资源，同时为学术界进一步研究奠定了基础。', 'title_zh': '大型语言模型在地质工程应用中的领域适应性'}
{'arxiv_id': 'arXiv:2507.05591', 'title': 'MLlm-DR: Towards Explainable Depression Recognition with MultiModal Large Language Models', 'authors': 'Wei Zhang, Juan Chen, En Zhu, Wenhong Cheng, YunPeng Li, Yanbo J. Wang', 'link': 'https://arxiv.org/abs/2507.05591', 'abstract': "Automated depression diagnosis aims to analyze multimodal information from interview videos to predict participants' depression scores. Previous studies often lack clear explanations of how these scores were determined, limiting their adoption in clinical practice. While the advent of LLMs provides a possible pathway for explainable depression diagnosis, current LLMs capable of processing multimodal data lack training on interview data, resulting in poor diagnostic performance when used directly. In this paper, we propose a novel multimodal large language model (MLlm-DR) that can understand multimodal information inputs and supports explainable depression diagnosis. MLlm-DR integrates a smaller LLMs and a lightweight query module (LQ-former). Specifically, the smaller LLMs is designed to generate depression scores and corresponding evaluation rationales. To enhance its logical reasoning for domain-specific tasks while maintaining practicality, we constructed a robust training dataset to fine-tune it. Meanwhile, the LQ-former captures depression-related features from speech and visual data, aiding the model's ability to process multimodal information, to achieve comprehensive depression diagnosis. Our approach achieves state-of-the-art results on two interview-based benchmark datasets, CMDC and E-DAIC-WOZ, demonstrating its effectiveness and superiority.", 'abstract_zh': '自动抑郁诊断旨在分析访谈视频的多模态信息以预测参与者的抑郁评分。之前的研究往往缺乏这些评分确定的清晰解释，限制了其在临床实践中的应用。尽管大型语言模型（LLMs）的出现为可解释的抑郁诊断提供了可能途径，但当前能够处理多模态数据的LLMs缺乏对访谈数据的训练，导致直接使用时诊断性能较差。在本文中，我们提出了一种新的多模态大型语言模型（MLlm-DR），能够理解多模态信息输入并支持可解释的抑郁诊断。MLlm-DR结合了一个较小的LLMs和一个轻量级查询模块（LQ-former）。具体而言，较小的LLMs设计用于生成抑郁评分及其相应的评价理由。为了增强其针对特定领域的逻辑推理能力并保持实用性，我们构建了一个稳健的训练数据集对其进行微调。同时，LQ-former从语音和视觉数据中捕获与抑郁相关的特征，帮助模型处理多模态信息，实现全面的抑郁诊断。我们的方法在两个基于访谈的基准数据集CMDC和E-DAIC-WOZ上取得了最先进的结果，展示了其有效性和优越性。', 'title_zh': 'MLlm-DR：基于多模态大型语言模型的可解释抑郁识别'}
{'arxiv_id': 'arXiv:2507.05541', 'title': 'SenseCF: LLM-Prompted Counterfactuals for Intervention and Sensor Data Augmentation', 'authors': 'Shovito Barua Soumma, Asiful Arefeen, Stephanie M. Carpenter, Melanie Hingle, Hassan Ghasemzadeh', 'link': 'https://arxiv.org/abs/2507.05541', 'abstract': 'Counterfactual explanations (CFs) offer human-centric insights into machine learning predictions by highlighting minimal changes required to alter an outcome. Therefore, CFs can be used as (i) interventions for abnormality prevention and (ii) augmented data for training robust models. In this work, we explore large language models (LLMs), specifically GPT-4o-mini, for generating CFs in a zero-shot and three-shot setting. We evaluate our approach on two datasets: the AI-Readi flagship dataset for stress prediction and a public dataset for heart disease detection. Compared to traditional methods such as DiCE, CFNOW, and NICE, our few-shot LLM-based approach achieves high plausibility (up to 99%), strong validity (up to 0.99), and competitive sparsity. Moreover, using LLM-generated CFs as augmented samples improves downstream classifier performance (an average accuracy gain of 5%), especially in low-data regimes. This demonstrates the potential of prompt-based generative techniques to enhance explainability and robustness in clinical and physiological prediction tasks. Code base: this http URL.', 'abstract_zh': '基于大型语言模型的零样本和三样本反事实解释生成及其在临床和生理预测任务中的应用', 'title_zh': 'SenseCF: LLM驱动的反事实生成用于干预和传感器数据增强'}
{'arxiv_id': 'arXiv:2507.05528', 'title': 'Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment', 'authors': 'Jiahuan Pei, Fanghua Ye, Xin Sun, Wentao Deng, Koen Hindriks, Junxiao Wang', 'link': 'https://arxiv.org/abs/2507.05528', 'abstract': "Large language models (LLMs) have advanced virtual educators and learners, bridging NLP with AI4Education. Existing work often lacks scalability and fails to leverage diverse, large-scale course content, with limited frameworks for assessing pedagogic quality. To this end, we propose WikiHowAgent, a multi-agent workflow leveraging LLMs to simulate interactive teaching-learning conversations. It integrates teacher and learner agents, an interaction manager, and an evaluator to facilitate procedural learning and assess pedagogic quality. We introduce a dataset of 114,296 teacher-learner conversations grounded in 14,287 tutorials across 17 domains and 727 topics. Our evaluation protocol combines computational and rubric-based metrics with human judgment alignment. Results demonstrate the workflow's effectiveness in diverse setups, offering insights into LLM capabilities across domains. Our datasets and implementations are fully open-sourced.", 'abstract_zh': '大型语言模型（LLMs）已经推动了虚拟教育者的进步，将自然语言处理（NLP）与AI4Education相结合。现有工作往往缺乏可扩展性，未能充分利用多样化的大量课程内容，并缺乏评估教学质量的框架。为此，我们提出了WikiHowAgent，这是一种利用大型语言模型模拟互动教学-学习对话的多代理工作流。它整合了教师代理、学习者代理、交互管理者和评估器，以促进程序性学习并评估教学质量。我们引入了一个基于14,287个教程的数据集，涵盖17个领域和727个主题，其中包括114,296次教师-学习者对话。我们的评估协议结合了计算性和准则基指标，并与人工判断对齐。结果显示该工作流在多种情境下的有效性，提供了关于LLM在不同领域能力的见解。我们的数据集和实现均已完全开源。', 'title_zh': '大规模对话教育：面向程序性学习和教育质量评估的多语言模型代理工作流'}
{'arxiv_id': 'arXiv:2507.05283', 'title': 'Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management', 'authors': 'Yue Wang, Miao Zhou, Guijing Huang, Rui Zhuo, Chao Yi, Zhenliang Ma', 'link': 'https://arxiv.org/abs/2507.05283', 'abstract': "Pre-timed traffic signal control, commonly used for operating signalized intersections and coordinated arterials, requires tedious manual work for signaling plan creating and updating. When the time-of-day or day-of-week plans are utilized, one intersection is often associated with multiple plans, leading to further repetitive manual plan parameter inputting. To enable a user-friendly traffic signal control plan management process, this study proposes Chat2SPaT, a method to convert users' semi-structured and ambiguous descriptions on the signal control plan to exact signal phase and timing (SPaT) results, which could further be transformed into structured stage-based or ring-based plans to interact with intelligent transportation system (ITS) software and traffic signal controllers. With curated prompts, Chat2SPaT first leverages large language models' (LLMs) capability of understanding users' plan descriptions and reformulate the plan as a combination of phase sequence and phase attribute results in the json format. Based on LLM outputs, python scripts are designed to locate phases in a cycle, address nuances of traffic signal control, and finally assemble the complete traffic signal control plan. Within a chat, the pipeline can be utilized iteratively to conduct further plan editing. Experiments show that Chat2SPaT can generate plans with an accuracy of over 94% for both English and Chinese cases, using a test dataset with over 300 plan descriptions. As the first benchmark for evaluating LLMs' capability of understanding traffic signal control plan descriptions, Chat2SPaT provides an easy-to-use plan management pipeline for traffic practitioners and researchers, serving as a potential new building block for a more accurate and versatile application of LLMs in the field of ITS. The source codes, prompts and test dataset are openly accessible at this https URL.", 'abstract_zh': '基于聊天的信号相位和时序生成方法：Chat2SPaT在智能交通系统中的应用', 'title_zh': 'Chat2SPaT: 一种基于大语言模型的交通信号控制计划管理自动化工具'}
{'arxiv_id': 'arXiv:2507.06223', 'title': 'Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers', 'authors': 'Zhiyuan Peng, Ting-ruen Wei, Tingyu Song, Yilun Zhao, Yi Fang', 'link': 'https://arxiv.org/abs/2507.06223', 'abstract': 'Large Language Models (LLMs) have recently been applied to reranking tasks in information retrieval, achieving strong performance. However, their high computational demands often hinder practical deployment. Existing studies evaluate the efficiency of LLM-based rerankers using proxy metrics such as latency, the number of forward passes, input tokens, and output tokens. However, these metrics depend on hardware and running-time choices (\\eg parallel or not, batch size, etc), and often fail to account for model size, making it difficult to interpret and obscuring the evaluation of the efficiency-effectiveness tradeoff. To address this issue, we propose E\\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per PetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for hardware-agnostic throughput. Companied with the new metrics, an interpretable FLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even without running any experiments. Based on the proposed metrics, we conduct comprehensive experiments to evaluate a wide range of LLM-based rerankers with different architecture, studying the efficiency-effectiveness trade-off and bringing this issue to the attention of the research community.', 'abstract_zh': '基于大语言模型的重排序器:E²R-FLOPs评价框架，以每拍浮点运算的相关性排名 metric 和每拍浮点运算的查询吞吐量 metric 促进硬件无关的效率-效果权衡评估', 'title_zh': '基于LLM的重排序器的计算量-效果重排序'}
{'arxiv_id': 'arXiv:2507.06204', 'title': 'Differential Mamba', 'authors': 'Nadav Schneider, Itamar Zimerman, Eliya Nachmani', 'link': 'https://arxiv.org/abs/2507.06204', 'abstract': 'Sequence models like Transformers and RNNs often overallocate attention to irrelevant context, leading to noisy intermediate representations. This degrades LLM capabilities by promoting hallucinations, weakening long-range and retrieval abilities, and reducing robustness. Recent work has shown that differential design can mitigate this issue in Transformers, improving their effectiveness across various applications. In this paper, we explore whether these techniques, originally developed for Transformers, can be applied to Mamba, a recent architecture based on selective state-space layers that achieves Transformer-level performance with greater efficiency. We show that a naive adaptation of differential design to Mamba is insufficient and requires careful architectural modifications. To address this, we introduce a novel differential mechanism for Mamba, empirically validated on language modeling benchmarks, demonstrating improved retrieval capabilities and superior performance over vanilla Mamba. Finally, we conduct extensive ablation studies and empirical analyses to justify our design choices and provide evidence that our approach effectively mitigates the overallocation problem in Mamba-based models. Our code is publicly available.', 'abstract_zh': '序列模型如Transformer和RNN经常过度分配注意力给无关的上下文，导致中间表示混乱。这会通过促进幻觉、削弱长程和检索能力以及降低鲁棒性来降低LLM的能力。最近的研究表明，差异设计可以在Transformer中减轻这一问题，并在各种应用中提高其有效性。在本文中，我们探索这些技术是否可以应用于Mamba，这是一种基于选择性状态空间层的近期架构，能够在更高效的情况下实现Transformer级别的性能。我们展示了朴素地将差异设计应用到Mamba是不够的，并且需要仔细的架构修改。为此，我们引入了一种新的Mamba差异机制，在语言建模基准上进行实证验证，展示了改进的检索能力和优于基础Mamba的性能。最后，我们进行了广泛的消融研究和实证分析，以证明我们的设计选择的有效性，并提供了证据表明我们的方法有效地缓解了基于Mamba模型的过度分配问题。我们的代码已公开。', 'title_zh': '差异型马姆巴'}
{'arxiv_id': 'arXiv:2507.06196', 'title': 'UQLM: A Python Package for Uncertainty Quantification in Large Language Models', 'authors': 'Dylan Bouchard, Mohit Singh Chauhan, David Skarbrevik, Ho-Kyeong Ra, Viren Bajaj, Zeya Ahmad', 'link': 'https://arxiv.org/abs/2507.06196', 'abstract': 'Hallucinations, defined as instances where Large Language Models (LLMs) generate false or misleading content, pose a significant challenge that impacts the safety and trust of downstream applications. We introduce UQLM, a Python package for LLM hallucination detection using state-of-the-art uncertainty quantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers that compute response-level confidence scores ranging from 0 to 1. This library provides an off-the-shelf solution for UQ-based hallucination detection that can be easily integrated to enhance the reliability of LLM outputs.', 'abstract_zh': '大型语言模型幻觉检测的不确定性量化工具包UQLM：基于最先进的不确定性量化技术识别幻觉', 'title_zh': 'UQLM：一种用于大型语言模型不确定性量化 的Python软件包'}
{'arxiv_id': 'arXiv:2507.06192', 'title': 'SQLBarber: A System Leveraging Large Language Models to Generate Customized and Realistic SQL Workloads', 'authors': 'Jiale Lao, Immanuel Trummer', 'link': 'https://arxiv.org/abs/2507.06192', 'abstract': 'Database research and development often require a large number of SQL queries for benchmarking purposes. However, acquiring real-world SQL queries is challenging due to privacy concerns, and existing SQL generation methods are limited in customization and in satisfying realistic constraints. To address this issue, we present SQLBarber, a system based on Large Language Models (LLMs) to generate customized and realistic SQL workloads. SQLBarber (i) eliminates the need for users to manually craft SQL templates in advance, while providing the flexibility to accept natural language specifications to constrain SQL templates, (ii) scales efficiently to generate large volumes of queries matching any user-defined cost distribution (e.g., cardinality and execution plan cost), and (iii) uses execution statistics from Amazon Redshift and Snowflake to derive SQL template specifications and query cost distributions that reflect real-world query characteristics. SQLBarber introduces (i) a declarative interface for users to effortlessly generate customized SQL templates, (ii) an LLM-powered pipeline augmented with a self-correction module that profiles, refines, and prunes SQL templates based on query costs, and (iii) a Bayesian Optimizer to efficiently explore different predicate values and identify a set of queries that satisfy the target cost distribution. We construct and open-source ten benchmarks of varying difficulty levels and target query cost distributions based on real-world statistics from Snowflake and Amazon Redshift. Extensive experiments on these benchmarks show that SQLBarber is the only system that can generate customized SQL templates. It reduces query generation time by one to three orders of magnitude, and significantly improves alignment with the target cost distribution, compared with existing methods.', 'abstract_zh': '基于大型语言模型的SQLBarber系统：生成定制化和现实主义SQL工作负载', 'title_zh': 'SQLBarber：一种利用大规模语言模型生成个性化和真实SQL工作负载的系统'}
{'arxiv_id': 'arXiv:2507.06138', 'title': 'Coding Triangle: How Does Large Language Model Understand Code?', 'authors': 'Taolin Zhang, Zihan Ma, Maosong Cao, Junnan Liu, Songyang Zhang, Kai Chen', 'link': 'https://arxiv.org/abs/2507.06138', 'abstract': 'Large language models (LLMs) have achieved remarkable progress in code generation, yet their true programming competence remains underexplored. We introduce the Code Triangle framework, which systematically evaluates LLMs across three fundamental dimensions: editorial analysis, code implementation, and test case generation. Through extensive experiments on competitive programming benchmarks, we reveal that while LLMs can form a self-consistent system across these dimensions, their solutions often lack the diversity and robustness of human programmers. We identify a significant distribution shift between model cognition and human expertise, with model errors tending to cluster due to training data biases and limited reasoning transfer. Our study demonstrates that incorporating human-generated editorials, solutions, and diverse test cases, as well as leveraging model mixtures, can substantially enhance both the performance and robustness of LLMs. Furthermore, we reveal both the consistency and inconsistency in the cognition of LLMs that may facilitate self-reflection and self-improvement, providing a potential direction for developing more powerful coding models.', 'abstract_zh': '大型语言模型（LLMs）在代码生成方面取得了显著进步，但其真正的编程能力仍待充分探索。我们引入了代码三角框架，该框架系统地从编辑分析、代码实现和测试用例生成三个基本维度评估LLMs。通过对竞争性编程基准的广泛实验，我们揭示了虽然LLMs在这些维度之间能够形成一个自洽的系统，但其解决方案往往缺乏人类程序员的多样性和鲁棒性。我们发现模型认知与人类专长之间存在显著的分布差异，模型错误倾向于因训练数据偏差和有限的推理迁移而集群。我们的研究显示， Incorporating 人类生成的编辑、解决方案和多样化的测试用例，以及利用模型混合，可以显著提升LLMs的性能和鲁棒性。此外，我们揭示了LLMs认知的一致性与不一致性，这可能促进自我反思和自我改进，提供了一个发展更强大编码模型的潜在方向。', 'title_zh': '编码三角形：大型语言模型如何理解代码？'}
{'arxiv_id': 'arXiv:2507.06127', 'title': 'PrefixAgent: An LLM-Powered Design Framework for Efficient Prefix Adder Optimization', 'authors': 'Dongsheng Zuo, Jiadong Zhu, Yang Luo, Yuzhe Ma', 'link': 'https://arxiv.org/abs/2507.06127', 'abstract': 'Prefix adders are fundamental arithmetic circuits, but their design space grows exponentially with bit-width, posing significant optimization challenges. Previous works face limitations in performance, generalization, and scalability. To address these challenges, we propose PrefixAgent, a large language model (LLM)-powered framework that enables efficient prefix adder optimization. Specifically, PrefixAgent reformulates the problem into subtasks including backbone synthesis and structure refinement, which effectively reduces the search space. More importantly, this new design perspective enables us to efficiently collect enormous high-quality data and reasoning traces with E-graph, which further results in an effective fine-tuning of LLM. Experimental results show that PrefixAgent synthesizes prefix adders with consistently smaller areas compared to baseline methods, while maintaining scalability and generalization in commercial EDA flows.', 'abstract_zh': '前缀加法器是基础的算术电路，但由于位宽的增加，其设计空间呈指数增长，提出了显著的优化挑战。现有工作在性能、泛化能力和可扩展性方面存在局限性。为应对这些挑战，我们提出了一种大型语言模型（LLM）驱动的框架——PrefixAgent，能够有效地优化前缀加法器。具体而言，PrefixAgent 将问题重新表述为包括主干综合和结构细化的子任务，从而有效缩小搜索空间。更重要的是，这种新的设计视角使我们能够使用 E-图高效地收集大量高质量的数据和推理轨迹，进一步实现对 LLM 的有效微调。实验证明，PrefixAgent 合成了与基准方法相比具有更小面积的前缀加法器，同时在商业 EDA 流程中保持了可扩展性和泛化能力。', 'title_zh': 'PrefixAgent：一种基于LLM的高效前缀加法器优化设计框架'}
{'arxiv_id': 'arXiv:2507.06056', 'title': 'Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs', 'authors': 'Yizhan Huang, Zhe Yang, Meifang Chen, Jianping Zhang, Michael R. Lyu', 'link': 'https://arxiv.org/abs/2507.06056', 'abstract': 'Large Language Models (LLMs) are known to memorize portions of their training data, sometimes reproducing content verbatim when prompted appropriately. In this work, we investigate a fundamental yet under-explored question in the domain of memorization: How to characterize memorization difficulty of training data in LLMs? Through empirical experiments on OLMo, a family of open models, we present the Entropy-Memorization Law. It suggests that data entropy is linearly correlated with memorization score. Moreover, in a case study of memorizing highly randomized strings, or "gibberish", we observe that such sequences, despite their apparent randomness, exhibit unexpectedly low empirical entropy compared to the broader training corpus. Adopting the same strategy to discover Entropy-Memorization Law, we derive a simple yet effective approach to distinguish training and testing data, enabling Dataset Inference (DI).', 'abstract_zh': '大型语言模型（LLMs）known to memorize portions of their training data, sometimes reproducing content verbatim when prompted appropriately. In this work, we investigate a fundamental yet under-explored question in the domain of memorization: How to characterize memorization difficulty of training data in LLMs? Through empirical experiments on OLMo, a family of open models, we present the Entropy-Memorization Law. It suggests that data entropy is linearly correlated with memorization score. Moreover, in a case study of memorizing highly randomized strings, or "gibberish", we observe that such sequences, despite their apparent randomness, exhibit unexpectedly low empirical entropy compared to the broader training corpus. Adopting the same strategy to discover Entropy-Memorization Law, we derive a simple yet effective approach to distinguish training and testing data, enabling Dataset Inference (DI).', 'title_zh': '熵记忆律：评估LLM中数据的记忆难度'}
{'arxiv_id': 'arXiv:2507.06043', 'title': 'CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations', 'authors': 'Xiaohu Li, Yunfeng Ning, Zepeng Bao, Mayi Xu, Jianhao Chen, Tieyun Qian', 'link': 'https://arxiv.org/abs/2507.06043', 'abstract': 'Security alignment enables the Large Language Model (LLM) to gain the protection against malicious queries, but various jailbreak attack methods reveal the vulnerability of this security mechanism. Previous studies have isolated LLM jailbreak attacks and defenses. We analyze the security protection mechanism of the LLM, and propose a framework that combines attack and defense. Our method is based on the linearly separable property of LLM intermediate layer embedding, as well as the essence of jailbreak attack, which aims to embed harmful problems and transfer them to the safe area. We utilize generative adversarial network (GAN) to learn the security judgment boundary inside the LLM to achieve efficient jailbreak attack and defense. The experimental results indicate that our method achieves an average jailbreak success rate of 88.85\\% across three popular LLMs, while the defense success rate on the state-of-the-art jailbreak dataset reaches an average of 84.17\\%. This not only validates the effectiveness of our approach but also sheds light on the internal security mechanisms of LLMs, offering new insights for enhancing model security The code and data are available at this https URL.', 'abstract_zh': '安全对齐使大规模语言模型（LLM）获得了对抗恶意查询的保护，但各种囚笼突破攻击方法揭示了这种安全机制的脆弱性。我们分析了LLM的安全保护机制，并提出了一种结合攻击与防御的框架。我们的方法基于LLM中间层嵌入的线性可分性质，以及囚笼突破攻击的本质，旨在将有害问题嵌入并转移至安全区域。我们利用生成对抗网络（GAN）学习LLM内部的安全判断边界，以实现有效的囚笼突破攻击与防御。实验结果显示，我们的方法在三个流行的LLM上实现了平均88.85%的囚笼突破成功率，而针对最先进的囚笼突破数据集的防御成功率平均为84.17%。这不仅验证了我们方法的有效性，还揭示了LLM的内部安全机制，为增强模型安全提供了新的见解。代码和数据可在以下链接获取：this https URL。', 'title_zh': 'CAVGAN：通过生成对抗攻击其内部表示以统一LLM的脱戒和防护'}
{'arxiv_id': 'arXiv:2507.05965', 'title': 'OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation', 'authors': 'Lucas Fonseca Lage, Simon Ostermann', 'link': 'https://arxiv.org/abs/2507.05965', 'abstract': 'We introduce OpenFActScore, an open-source implementation of the FActScore framework for evaluating the factuality of text generated by large language models (LLMs). FActScore evaluates the factual accuracy of long-form text by using Atomic Fact Generation (AFG) to extract individual factual claims and Atomic Fact Validation (AFV) to verify each claim against a trusted knowledge source. While the original FActScore relies on closed-source and commercial models such as InstructGPT and ChatGPT, OpenFActScore enables the use of any Hugging Face-compatible model for both AFG and AFV. We provide a detailed technical overview of our implementation, highlighting design choices and modifications made to support open models. We evaluate multiple open-source LLMs on both AFG and AFV using the original FActScore benchmark, reporting BERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our results show that open models can approximate the performance of closed-source systems, with Gemma achieving the best overall performance, and our final setup obtains a 0.99 Pearson correlation with the original FActScore experiments. OpenFActScore promotes transparency, reproducibility, and cost-effective evaluation, and is available at: this https URL.', 'abstract_zh': 'OpenFActScore：一种开源的大语言模型文本事实性评估框架实现', 'title_zh': 'OpenFActScore: 开源原子化事实性评估方法'}
{'arxiv_id': 'arXiv:2507.05911', 'title': 'Differentiable Reward Optimization for LLM based TTS system', 'authors': 'Changfeng Gao, Zhihao Du, Shiliang Zhang', 'link': 'https://arxiv.org/abs/2507.05911', 'abstract': "This paper proposes a novel Differentiable Reward Optimization (DiffRO) method aimed at enhancing the performance of neural codec language models based text-to-speech (TTS) systems. In contrast to conventional reinforcement learning from human feedback (RLHF) approaches applied to TTS, DiffRO directly compute the rewards based on neural codec tokens, rather than relying on synthesized audio. Furthermore, we employ the Gumbel-Softmax technique to render the reward function differentiable, thereby streamlining the RLHF training process. Additionally, we introduce a multi-task reward (MTR) model which can provide feedback from different perspectives and find that it can augment the system's capability to follow instructions this http URL results indicate that DiffRO significantly improves the pronunciation accuracy of the TTS system, achieving state-of-the-art (SOTA) WER results on the seed-tts-eval benchmark. Moreover, with the integration of the MTR model, we demonstrate the ability to control emotional and quality attributes in a zero-shot manner.", 'abstract_zh': '一种新型可微奖励优化（DiffRO）方法：基于神经编码器的语言模型的文本转语音系统性能提升研究', 'title_zh': '基于LLM的TTS系统可微奖励优化'}
{'arxiv_id': 'arXiv:2507.05890', 'title': 'Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators', 'authors': 'Sungjib Lim, Woojung Song, Eun-Ju Lee, Yohan Jo', 'link': 'https://arxiv.org/abs/2507.05890', 'abstract': 'As psychometric surveys are increasingly used to assess the traits of large language models (LLMs), the need for scalable survey item generation suited for LLMs has also grown. A critical challenge here is ensuring the construct validity of generated items, i.e., whether they truly measure the intended trait. Traditionally, this requires costly, large-scale human data collection. To make it efficient, we present a framework for virtual respondent simulation using LLMs. Our central idea is to account for mediators: factors through which the same trait can give rise to varying responses to a survey item. By simulating respondents with diverse mediators, we identify survey items that robustly measure intended traits. Experiments on three psychological trait theories (Big5, Schwartz, VIA) show that our mediator generation methods and simulation framework effectively identify high-validity items. LLMs demonstrate the ability to generate plausible mediators from trait definitions and to simulate respondent behavior for item validation. Our problem formulation, metrics, methodology, and dataset open a new direction for cost-effective survey development and a deeper understanding of how LLMs replicate human-like behavior. We will publicly release our dataset and code to support future work.', 'abstract_zh': '随着心理测量调查在评估大型语言模型（LLMs）特质中的应用日益增多，适合LLMs的可扩展调查项目生成需求也不断增长。一个关键挑战是确保生成项目的构建有效性，即它们是否真正测量了预期的特质。传统方法需要大量的成本高昂的人类数据收集。为了使其更高效，我们提出了一种使用LLM进行虚拟受访者模拟的框架。我们的核心思想是考虑中介变量：这些变量通过它们影响同一特质在调查项目中产生不同的响应。通过模拟具有多样中介变量的受访者，我们识别出了能够稳健测量预期特质的调查项目。在对Big5、Schwartz和VIA三种心理特质理论的实验中，我们的中介生成方法和模拟框架有效识别出了高有效性的调查项目。LLMs展示了从特质定义生成合理中介变量和模拟受访者行为以验证项目的能力。我们的问题表述、度量标准、方法论和数据集为低成本调查开发开辟了新方向，并深入理解了LLMs如何复制人类行为。我们将公开发布我们的数据集和代码以支持未来的工作。', 'title_zh': '使用特征-反应中介的虚拟应答者进行项目心理学测量验证'}
{'arxiv_id': 'arXiv:2507.05820', 'title': "Constella: Supporting Storywriters' Interconnected Character Creation through LLM-based Multi-Agents", 'authors': 'Syemin Park, Soobin Park, Youn-kyung Lim', 'link': 'https://arxiv.org/abs/2507.05820', 'abstract': "Creating a cast of characters by attending to their relational dynamics is a critical aspect of most long-form storywriting. However, our formative study (N=14) reveals that writers struggle to envision new characters that could influence existing ones, to balance similarities and differences among characters, and to intricately flesh out their relationships. Based on these observations, we designed Constella, an LLM-based multi-agent tool that supports storywriters' interconnected character creation process. Constella suggests related characters (FRIENDS DISCOVERY feature), reveals the inner mindscapes of several characters simultaneously (JOURNALS feature), and manifests relationships through inter-character responses (COMMENTS feature). Our 7-8 day deployment study with storywriters (N=11) shows that Constella enabled the creation of expansive communities composed of related characters, facilitated the comparison of characters' thoughts and emotions, and deepened writers' understanding of character relationships. We conclude by discussing how multi-agent interactions can help distribute writers' attention and effort across the character cast.", 'abstract_zh': '关注角色关系动力学创建角色是大多数长篇叙事写作的关键方面。然而，我们的初步研究（N=14）表明，作者在构思能够影响现有角色的新角色、平衡角色间的相似性和差异性以及细致刻画角色关系时遇到困难。基于这些观察，我们设计了Constella，一个基于LLM的多智能体工具，支持故事作家的联结性角色创作过程。Constella建议相关角色（FRIENDS DISCOVERY功能）、同时揭示多个角色的内心世界（JOURNALS功能），并通过角色间的回应展现关系（COMMENTS功能）。对我们7-8天的部署研究（N=11）表明，Constella促进了相关角色组成的广泛社区的创建，促进了不同角色思想和情感的比较，并加深了作者对角色关系的理解。我们最后讨论了多智能体交互如何帮助分散作者在角色群体中的注意力和努力。', 'title_zh': 'Constella: 通过基于LLM的多智能体支持故事作家构建互联人物角色'}
{'arxiv_id': 'arXiv:2507.05733', 'title': 'When Transformers Meet Recommenders: Integrating Self-Attentive Sequential Recommendation with Fine-Tuned LLMs', 'authors': 'Kechen Liu', 'link': 'https://arxiv.org/abs/2507.05733', 'abstract': 'Self-Attentive Sequential Recommendation (SASRec) effectively captures long-term user preferences by applying attention mechanisms to historical interactions. Concurrently, the rise of Large Language Models (LLMs) has motivated research into LLM-based recommendation, which leverages their powerful generalization and language understanding capabilities. However, LLMs often lack the domain-specific knowledge and collaborative signals essential for high-quality recommendations when relying solely on textual prompts. To address this limitation, this study proposes SASRecLLM, a novel framework that integrates SASRec as a collaborative encoder with an LLM fine-tuned using Low-Rank Adaptation (LoRA). The components are connected via a mapping layer to align their dimensional spaces, and three targeted training strategies are designed to optimize the hybrid architecture. Extensive experiments on multiple datasets demonstrate that SASRecLLM achieves robust and consistent improvements over strong baselines in both cold-start and warm-start scenarios. This work advances the field of LLM-based recommendation by presenting a modular and effective paradigm for fusing structured collaborative filtering with the semantic power of fine-tuned LLMs. The implementation is available on GitHub: this https URL', 'abstract_zh': '自注意序列推荐（SASRec）通过应用注意力机制到历史交互中有效地捕捉长期用户偏好。同时，大型语言模型（LLMs）的兴起推动了基于LLM的推荐研究，利用了它们强大的泛化能力和语言理解能力。然而，仅依赖文本提示时，LLMs往往缺乏进行高质量推荐所需的领域特定知识和协作信号。为克服这一限制，本研究提出了SASRecLLM，这是一种新型框架，将SASRec作为协作编码器与使用低秩适应（LoRA）微调的LLM相结合。通过映射层连接其组件，以对其维度空间进行对齐，并设计了三种目标训练策略来优化这种混合架构。在多个数据集上的大量实验表明，SASRecLLM在冷启动和温启动场景中均实现了稳健且一致的改进。本研究通过展示模块化和有效的范式，将结构化协作过滤与微调的LLM的语义力量融合在一起，推进了基于LLM的推荐领域。相关实现可在GitHub上获取：this https URL。', 'title_zh': '当变压器遇见推荐系统：结合自注意力序列推荐与微调的大语言模型'}
{'arxiv_id': 'arXiv:2507.05713', 'title': 'DRAGON: Dynamic RAG Benchmark On News', 'authors': 'Fedor Chernogorskii, Sergei Averkiev, Liliya Kudraleeva, Zaven Martirosian, Maria Tikhonova, Valentin Malykh, Alena Fenogenova', 'link': 'https://arxiv.org/abs/2507.05713', 'abstract': 'Retrieval-Augmented Generation (RAG) is a widely adopted approach for improving the factuality of large language models (LLMs) by incorporating external knowledge at inference time. Although there exist multiple RAG benchmarks for English, evaluation resources for other languages, including Russian, remain scarce and static, failing to capture the dynamic nature of real-world deployments.\nIn this work, we present DRAGON (Dynamic RAG Benchmark On News), the first dynamic benchmark for evaluating RAG systems in Russian on a changing news corpora. DRAGON is built upon a regularly updated corpus of Russian news and public documents and supports comprehensive evaluation of both the retriever and generator components. Question generation is performed automatically with the use of Knowledge Graph constructed from the corpus and enables the extraction of four core question types aligned with distinct subgraph patterns. We release a complete evaluation framework comprising the pipeline for automatic question generation, evaluation scripts, which are potentially reusable for other languages and multilingual settings, and benchmark data. We also launch a public leaderboard to encourage community participation and comparison.', 'abstract_zh': '动态新闻RAG基准（DRAGON）：俄语RAG系统的动态评估', 'title_zh': 'DRAGON: 动态RAG新闻基准测试'}
{'arxiv_id': 'arXiv:2507.05660', 'title': 'TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data', 'authors': 'Aravind Cheruvu, Shravya Kanchi, Sifat Muhammad Abdullah, Nicholas Kong, Daphne Yao, Murtuza Jadliwala, Bimal Viswanath', 'link': 'https://arxiv.org/abs/2507.05660', 'abstract': "Recent advances in foundation models, such as LLMs, have revolutionized conversational AI. Chatbots are increasingly being developed by customizing LLMs on specific conversational datasets. However, mitigating toxicity during this customization, especially when dealing with untrusted training data, remains a significant challenge. To address this, we introduce TuneShield, a defense framework designed to mitigate toxicity during chatbot fine-tuning while preserving conversational quality. TuneShield leverages LLM-based toxicity classification, utilizing the instruction-following capabilities and safety alignment of LLMs to effectively identify toxic samples, outperforming industry API services. TuneShield generates synthetic conversation samples, termed 'healing data', based on the identified toxic samples, using them to mitigate toxicity while reinforcing desirable behavior during fine-tuning. It performs an alignment process to further nudge the chatbot towards producing desired responses. Our findings show that TuneShield effectively mitigates toxicity injection attacks while preserving conversational quality, even when the toxicity classifiers are imperfect or biased. TuneShield proves to be resilient against adaptive adversarial and jailbreak attacks. Additionally, TuneShield demonstrates effectiveness in mitigating adaptive toxicity injection attacks during dialog-based learning (DBL).", 'abstract_zh': 'Recent Advances in Foundation Models, Such as LLMs, Have Revolutionized Conversational AI: Introducing TuneShield, a Defense Framework for Mitigating Toxicity While Preserving Conversational Quality in Chatbot Fine-Tuning', 'title_zh': 'TuneShield: 在使用不可信数据微调的同时减轻对话AI的毒性风险'}
{'arxiv_id': 'arXiv:2507.05633', 'title': 'SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression', 'authors': 'Yiqiao Jin, Kartik Sharma, Vineeth Rakesh, Yingtong Dou, Menghai Pan, Mahashweta Das, Srijan Kumar', 'link': 'https://arxiv.org/abs/2507.05633', 'abstract': 'Retrieval-augmented Generation (RAG) extends large language models (LLMs) with external knowledge but faces key challenges: restricted effective context length and redundancy in retrieved documents. Pure compression-based approaches reduce input size but often discard fine-grained details essential for factual accuracy. We propose SARA, a unified RAG framework that balances local precision and global knowledge coverage under tight context budgets. SARA combines natural-language text snippets with semantic compression vectors to jointly enhance context efficiency and answer correctness. It represents contexts at two complementary levels: 1) fine-grained natural-language spans that preserve critical entities and numerical values, and 2) compact, interpretable vectors that summarize high-level semantics. An iterative evidence-selection module employs the compression vectors for dynamic reranking of contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families (Mistral, Llama, and Gemma), SARA consistently improves answer relevance (+17.71), answer correctness (+13.72), and semantic similarity (+15.53), demonstrating the importance of integrating textual and compressed representations for robust, context-efficient RAG.', 'abstract_zh': '检索增强生成（RAG）通过外部知识扩展了大规模语言模型（LLMs），但面临关键挑战：有效的上下文长度限制和检索文档中的冗余。基于纯压缩的方法减少了输入大小，但往往会丢弃对于事实准确性至关重要的细粒度细节。我们提出了一种统一的RAG框架SARA，该框架在紧凑的上下文预算下平衡局部精确性和全局知识覆盖。SARA结合自然语言文本片段与语义压缩向量，共同提升上下文效率和答案准确性。它从两个互补的层次表示上下文：1) 细粒度的自然语言片段，保留关键实体和数值；2) 紧凑、可解释的向量，总结高层次语义。迭代的证据选择模块利用压缩向量动态重新排名上下文。在9个数据集和5个开源LLM（Mistral、Llama和Gemma）上，SARA在答案相关性（+17.71）、答案准确性（+13.72）和语义相似度（+15.53）方面表现出一致的改进，展示了将文本和压缩表示结合以实现稳健、高效RAG的重要性。', 'title_zh': 'SARA: 选择性和自适应检索增强生成与上下文压缩'}
{'arxiv_id': 'arXiv:2507.05630', 'title': 'How Not to Detect Prompt Injections with an LLM', 'authors': 'Sarthak Choudhary, Divyam Anshumaan, Nils Palumbo, Somesh Jha', 'link': 'https://arxiv.org/abs/2507.05630', 'abstract': "LLM-integrated applications and agents are vulnerable to prompt injection attacks, in which adversaries embed malicious instructions within seemingly benign user inputs to manipulate the LLM's intended behavior. Recent defenses based on $\\textit{known-answer detection}$ (KAD) have achieved near-perfect performance by using an LLM to classify inputs as clean or contaminated. In this work, we formally characterize the KAD framework and uncover a structural vulnerability in its design that invalidates its core security premise. We design a methodical adaptive attack, $\\textit{DataFlip}$, to exploit this fundamental weakness. It consistently evades KAD defenses with detection rates as low as $1.5\\%$ while reliably inducing malicious behavior with success rates of up to $88\\%$, without needing white-box access to the LLM or any optimization procedures.", 'abstract_zh': 'LLM整合应用和代理易受提示注入攻击的影响，攻击者通过在看似无害的用户输入中嵌入恶意指令来操纵LLM的预期行为。基于已知答案检测(KAD)的最近防御方法通过使用LLM对输入进行分类，以接近完美性能实现了清洁或污染的识别。在本文中，我们正式刻画了KAD框架，并揭示了其设计中的结构漏洞，这使得其核心安全假设失效。我们设计了一种系统性的自适应攻击方法DataFlip，以利用这一基本弱点。DataFlip在检测率低至1.5%的情况下持续避开KAD防御，同时以高达88%的成功率可靠地诱导恶意行为，无需对LLM进行白盒访问或采用任何优化程序。', 'title_zh': '如何不使用大规模语言模型检测提示注入'}
{'arxiv_id': 'arXiv:2507.05598', 'title': 'Self-Review Framework for Enhancing Instruction Following Capability of LLM', 'authors': 'Sihyun Park', 'link': 'https://arxiv.org/abs/2507.05598', 'abstract': 'Various techniques have been proposed to improve large language models (LLMs) adherence to formatting and instruction constraints. One of the most effective approaches involves utilizing high-quality data generated by powerful models. However, such models often fail to fully comply with complex instructions in a single generation. To address this limitation, iterative revision methods have been introduced. Nevertheless, as the number of data points and revision iterations increases, the associated monetary costs grow significantly. As a resource-efficient alternative, methods have been proposed that leverage high-performance evaluation tools to compensate for the limited self-evaluation capabilities of open-source LLMs. However, these approaches often lead to a degradation in output quality due to excessive revision. To overcome these challenges, we propose Re5, a self-evaluation and revision framework designed to enhance instruction-following performance while preserving the quality of the generated content. Re5 extracts task and constraint components from user instructions, performs structural evaluations to prevent error accumulation, and applies fine-grained constraint-specific content evaluations followed by selective revisions. This process ensures precise and quality-preserving improvements. The final high-quality outputs are used for alignment tuning, enabling long-term alignment improvements through a data-centric iterative refinement loop. Experimental results demonstrate that Re5 achieves instruction-following performance comparable to models trained on data generated by GPT-4o-mini, a high-performance model, even with a small amount of data while maintaining response quality with a 64.24%-win rate over the non-revised initial responses. These results validate Re5 as an efficient and effective solution for enhancing instruction adherence with minimal external supervision.', 'abstract_zh': 'Re5：一种自评与修订框架以提高指令遵循性能并保持内容质量', 'title_zh': '增强LLM指令遵循能力的自我审查框架'}
{'arxiv_id': 'arXiv:2507.05573', 'title': 'Prompt Migration: Stabilizing GenAI Applications with Evolving Large Language Models', 'authors': 'Shivani Tripathi, Pushpanjali Nema, Aditya Halder, Shi Qiao, Alekh Jindal', 'link': 'https://arxiv.org/abs/2507.05573', 'abstract': 'Generative AI is transforming business applications by enabling natural language interfaces and intelligent automation. However, the underlying large language models (LLMs) are evolving rapidly and so prompting them consistently is a challenge. This leads to inconsistent and unpredictable application behavior, undermining the reliability that businesses require for mission-critical workflows. In this paper, we introduce the concept of prompt migration as a systematic approach to stabilizing GenAI applications amid changing LLMs. Using the Tursio enterprise search application as a case study, we analyze the impact of successive GPT model upgrades, detail our migration framework including prompt redesign and a migration testbed, and demonstrate how these techniques restore application consistency. Our results show that structured prompt migration can fully recover the application reliability that was lost due to model drift. We conclude with practical lessons learned, emphasizing the need for prompt lifecycle management and robust testing to ensure dependable GenAI-powered business applications.', 'abstract_zh': '生成式AI正在通过启用自然语言界面和智能自动化来变革商业应用。然而，底层的大语言模型（LLMs）正在迅速演变，因此持续有效地提示它们是一个挑战。这导致应用程序行为不一致且不可预测，削弱了企业对于关键业务流程所需可靠性的要求。在本文中，我们介绍了一种系统化的提示迁移概念，以在不断变化的大语言模型中稳定生成式AI应用。以Tursio企业搜索应用为例，我们分析了连续升级的GPT模型的影响，详细介绍了我们的迁移框架，包括提示重新设计和迁移测试平台，并展示了这些技术如何恢复应用程序的一致性。我们的结果显示，结构化的提示迁移可以完全恢复因模型漂移而丢失的应用可靠性。我们总结了实用的经验教训，强调了提示生命周期管理和稳健测试对于确保可靠的生成式AI驱动业务应用的重要性。', 'title_zh': '提示迁移：使用演化的大型语言模型稳定GenAI应用'}
{'arxiv_id': 'arXiv:2507.05565', 'title': 'Search-based Selection of Metamorphic Relations for Optimized Robustness Testing of Large Language Models', 'authors': 'Sangwon Hyun, Shaukat Ali, M. Ali Babar', 'link': 'https://arxiv.org/abs/2507.05565', 'abstract': 'Assessing the trustworthiness of Large Language Models (LLMs), such as robustness, has garnered significant attention. Recently, metamorphic testing that defines Metamorphic Relations (MRs) has been widely applied to evaluate the robustness of LLM executions. However, the MR-based robustness testing still requires a scalable number of MRs, thereby necessitating the optimization of selecting MRs. Most extant LLM testing studies are limited to automatically generating test cases (i.e., MRs) to enhance failure detection. Additionally, most studies only considered a limited test space of single perturbation MRs in their evaluation of LLMs. In contrast, our paper proposes a search-based approach for optimizing the MR groups to maximize failure detection and minimize the LLM execution cost. Moreover, our approach covers the combinatorial perturbations in MRs, facilitating the expansion of test space in the robustness assessment. We have developed a search process and implemented four search algorithms: Single-GA, NSGA-II, SPEA2, and MOEA/D with novel encoding to solve the MR selection problem in the LLM robustness testing. We conducted comparative experiments on the four search algorithms along with a random search, using two major LLMs with primary Text-to-Text tasks. Our statistical and empirical investigation revealed two key findings: (1) the MOEA/D algorithm performed the best in optimizing the MR space for LLM robustness testing, and (2) we identified silver bullet MRs for the LLM robustness testing, which demonstrated dominant capabilities in confusing LLMs across different Text-to-Text tasks. In LLM robustness assessment, our research sheds light on the fundamental problem for optimized testing and provides insights into search-based solutions.', 'abstract_zh': '评估大规模语言模型可信性的可信度，如鲁棒性，引起了广泛关注。最近，定义元formeorphic关系（MRs）的元formeorphic测试已被广泛应用于评估大规模语言模型执行的鲁棒性。然而，基于MR的鲁棒性测试仍需要大量的MR，因此需要优化MR的选择。现有的大多数大规模语言模型测试研究仅限于自动生成测试用例（即MRs），以提高故障检测能力。此外，大多数研究仅在其对大规模语言模型的评估中考虑了单变异MR的一个有限测试空间。相比之下，我们提出了一种基于搜索的方法，以优化MR组，最大化故障检测并最小化大规模语言模型执行成本。此外，我们的方法涵盖了MR中的组合变异，便于在鲁棒性评估中扩展测试空间。我们开发了搜索过程并实现了四种搜索算法：单GA、NSGA-II、SPEA2和MOEA/D，并采用新型编码来解决大规模语言模型鲁棒性测试中的MR选择问题。我们在两种主要的大规模语言模型上与随机搜索进行了比较实验，这些模型主要执行文本到文本任务。我们的统计和实证调查揭示了两个关键发现：（1）MOEA/D算法在优化大规模语言模型鲁棒性测试的MR空间方面表现最佳；（2）我们识别出了大规模语言模型鲁棒性测试的银弹MR，这些MR在不同文本到文本任务中展示了显著的混淆能力。在大规模语言模型的鲁棒性评估中，我们的研究照亮了优化测试的基本问题，并为基于搜索的方法提供了见解。', 'title_zh': '基于搜索的元变换关系选择以优化大型语言模型的稳健性测试'}
{'arxiv_id': 'arXiv:2507.05558', 'title': 'AI Agent Smart Contract Exploit Generation', 'authors': 'Arthur Gervais, Liyi Zhou', 'link': 'https://arxiv.org/abs/2507.05558', 'abstract': "We present A1, an agentic execution driven system that transforms any LLM into an end-to-end exploit generator. A1 has no hand-crafted heuristics and provides the agent with six domain-specific tools that enable autonomous vulnerability discovery. The agent can flexibly leverage these tools to understand smart contract behavior, generate exploit strategies, test them on blockchain states, and refine approaches based on execution feedback. All outputs are concretely validated to eliminate false positives.\nThe evaluation across 36 real-world vulnerable contracts on Ethereum and Binance Smart Chain demonstrates a 62.96% (17 out of 27) success rate on the VERITE benchmark. Beyond the VERITE dataset, A1 identified 9 additional vulnerable contracts, with 5 cases occurring after the strongest model's training cutoff date. Across all 26 successful cases, A1 extracts up to 8.59 million USD per case and 9.33 million USD total. Through 432 experiments across six LLMs, we analyze iteration-wise performance showing diminishing returns with average marginal gains of +9.7%, +3.7%, +5.1%, and +2.8% for iterations 2-5 respectively, with per-experiment costs ranging $0.01-$3.59. A Monte Carlo analysis of 19 historical attacks shows success probabilities of 85.9%-88.8% without detection delays.\nWe investigate whether an attacker or a defender benefits most from deploying A1 as a continuous on-chain scanning system. Our model shows that OpenAI's o3-pro maintains profitability up to a 30.0 days scanning delay at 0.100% vulnerability incidence rates, while faster models require >=1.000% rates to break-even. The findings exposes a troubling asymmetry: at 0.1% vulnerability rates, attackers achieve an on-chain scanning profitability at a $6000 exploit value, while defenders require $60000, raising fundamental questions about whether AI agents inevitably favor exploitation over defense.", 'abstract_zh': '我们呈现了一个名为A1的由智能执行驱动的系统，它可以将任何LLM转换为端到端的漏洞利用生成器。A1没有任何手工制作的启发式方法，并为智能体提供了六个特定领域的工具，以实现自主的漏洞发现。智能体可以灵活地利用这些工具来理解智能合约行为，生成利用策略，在区块链状态下测试它们，并根据执行反馈改进方法。所有输出均进行了具体验证，以消除假阳性。\n\n在以太坊和币安智能链上的36个真实世界的漏洞合约的评估中，A1在VERITE基准测试中的成功率为62.96%（27个中的17个）。超越VERITE数据集，A1还识别出9个额外的漏洞合约，其中有5个案例发生在最强模型训练截止日期之后。在所有26个成功案例中，A1每案例提取高达859万美元，总计933万美元。通过在六种LLM上进行的432次实验分析迭代性能，显示随迭代增加，边际收益逐渐减少，分别在第2至第5迭代中增加9.7%、3.7%、5.1%和2.8%，每次实验成本范围从0.01美元到3.59美元。通过对19起历史攻击的蒙特卡洛分析显示，在无检测延迟的情况下，成功概率为85.9%-88.8%。\n\n我们探讨了部署A1作为持续链上扫描系统时攻击者和防御者哪一方能获得更大收益。我们的模型显示，OpenAI的o3-pro在0.100%漏洞发生率下，即使有30.0天的扫描延迟也能保持盈利，而更快的模型需要至少1.000%的漏洞发生率才能达到盈亏平衡。这项研究揭示了一个令人担忧的不对称性：在0.1%的漏洞发生率下，攻击者在6000美元的利用价值下就能实现链上扫描盈利，而防御者则需要60000美元，从而引发了关于人工智能代理是否不可避免地倾向于利用而非防御的基本问题。', 'title_zh': 'AI代理智能合约漏洞生成'}
{'arxiv_id': 'arXiv:2507.05517', 'title': 'Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications', 'authors': 'Jean-Philippe Corbeil, Asma Ben Abacha, George Michalopoulos, Phillip Swazinna, Miguel Del-Agua, Jerome Tremblay, Akila Jeeson Daniel, Cari Bader, Kevin Cho, Pooja Krishnan, Nathan Bodenstab, Thomas Lin, Wenxuan Teng, Francois Beaulieu, Paul Vozila', 'link': 'https://arxiv.org/abs/2507.05517', 'abstract': 'Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong performance on clinical natural language processing (NLP) tasks across multiple medical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular reporting from nurse dictations and medical order extraction from doctor-patient consultations - remain underexplored due to data scarcity and sensitivity, despite active industry efforts. Practical solutions to these real-world clinical tasks can significantly reduce the documentation burden on healthcare providers, allowing greater focus on patient care. In this paper, we investigate these two challenging tasks using private and open-source clinical datasets, evaluating the performance of both open- and closed-weight LLMs, and analyzing their respective strengths and limitations. Furthermore, we propose an agentic pipeline for generating realistic, non-sensitive nurse dictations, enabling structured extraction of clinical observations. To support further research in both areas, we release SYNUR and SIMORD, the first open-source datasets for nurse observation extraction and medical order extraction.', 'abstract_zh': '大型语言模型（LLMs）如GPT-4o和o1在多个医学基准上的临床自然语言处理（NLP）任务中表现出色。然而，由于数据稀缺性和敏感性，两类高影响的NLP任务——护士口述的结构化表格报告生成和从医生-患者咨询中提取医疗订单——仍待进一步探索。针对这些实际临床任务的实用解决方案可以显著减轻医疗提供者的文书工作负担，使他们能够更加专注于患者护理。本文利用私有和开源的临床数据集，研究了这两种挑战性的任务，评估了开放权重和闭合权重LLM的表现，并分析了各自的优缺点。此外，我们提出了一种代理管道，用于生成现实且非敏感的护士口述记录，从而实现临床观察的结构化提取。为了支持这两个领域的进一步研究，我们发布了SYNUR和SIMORD，这是第一个用于护士观察提取和医疗订单提取的开源数据集。', 'title_zh': '提升医疗工作者能力的语言模型：在两个实际临床应用中的语音转录结构化'}
{'arxiv_id': 'arXiv:2507.05465', 'title': '2048: Reinforcement Learning in a Delayed Reward Environment', 'authors': 'Prady Saligram, Tanvir Bhathal, Robby Manihani', 'link': 'https://arxiv.org/abs/2507.05465', 'abstract': 'Delayed and sparse rewards present a fundamental obstacle for reinforcement-learning (RL) agents, which struggle to assign credit for actions whose benefits emerge many steps later. The sliding-tile game 2048 epitomizes this challenge: although frequent small score changes yield immediate feedback, they often mislead agents into locally optimal but globally suboptimal strategies. In this work, we introduce a unified, distributional multi-step RL framework designed to directly optimize long-horizon performance. Using the open source Gym-2048 environment we develop and compare four agent variants: standard DQN, PPO, QR-DQN (Quantile Regression DQN), and a novel Horizon-DQN (H-DQN) that integrates distributional learning, dueling architectures, noisy networks, prioritized replay, and more. Empirical evaluation reveals a clear hierarchy in effectiveness: max episode scores improve from 3.988K (DQN) to 5.756K (PPO), 8.66K (QR-DQN), and 18.21K (H-DQN), with H-DQN reaching the 2048 tile. Upon scaling H-DQN it reaches a max score 41.828K and a 4096 tile. These results demonstrate that distributional, multi-step targets substantially enhance performance in sparse-reward domains, and they suggest promising avenues for further gains through model-based planning and curriculum learning.', 'abstract_zh': '延迟且稀疏的奖励是强化学习（RL）代理面临的根本障碍，使其难以对多种步骤后才显现利益的动作进行正确的信用分配。滑动瓷砖游戏2048完美体现了这一挑战：尽管频繁的小分变化提供了即时反馈，但它们往往引导代理采取局部最优但全局次优的策略。在此工作中，我们提出了一种统一的分布式多步RL框架，旨在直接优化长期性能。我们使用开源的Gym-2048环境开发并比较了四种代理变体：标准DQN、PPO、QR-DQN（分位回归DQN）以及一种新颖的Horizon-DQN（H-DQN），该变体整合了分布式学习、对拼网络、噪音网络、优先经验回放等多种技术。实证评估表明，最大回合得分从DQN的3.988K提升到PPO的5.756K、QR-DQN的8.66K以及H-DQN的18.21K，并且H-DQN达到了2048瓷砖。在扩展H-DQN后，其最大得分为41.828K，且获得了4096瓷砖。这些结果表明，分布式的多步目标在稀疏奖励领域显著提升了性能，并且表明基于模型的规划和层次学习有望进一步提升性能。', 'title_zh': '2048: 延迟奖励环境中的强化学习'}
{'arxiv_id': 'arXiv:2507.05448', 'title': 'On the Semantics of Large Language Models', 'authors': 'Martin Schuele', 'link': 'https://arxiv.org/abs/2507.05448', 'abstract': 'Large Language Models (LLMs) such as ChatGPT demonstrated the potential to replicate human language abilities through technology, ranging from text generation to engaging in conversations. However, it remains controversial to what extent these systems truly understand language. We examine this issue by narrowing the question down to the semantics of LLMs at the word and sentence level. By examining the inner workings of LLMs and their generated representation of language and by drawing on classical semantic theories by Frege and Russell, we get a more nuanced picture of the potential semantic capabilities of LLMs.', 'abstract_zh': '大型语言模型（LLMs）如ChatGPT展示了通过技术复制人类语言能力的潜力，从文本生成到参与对话。然而，这些系统真正理解语言的程度仍有争议。我们通过将问题聚焦到LLMs的词和句义层面来探讨这一问题。通过分析LLMs的内部工作机制及其生成的语言表示，并结合Frege和Russell的经典语义理论，我们获得了一个更细致的语义能力图景。', 'title_zh': '大型语言模型的语义研究'}
{'arxiv_id': 'arXiv:2507.05424', 'title': '"Lost-in-the-Later": Framework for Quantifying Contextual Grounding in Large Language Models', 'authors': 'Yufei Tao, Adam Hiatt, Rahul Seetharaman, Ameeta Agrawal', 'link': 'https://arxiv.org/abs/2507.05424', 'abstract': 'Large language models are capable of leveraging both contextual and parametric knowledge but how they prioritize and integrate these sources remains underexplored. We introduce CoPE, a novel evaluation framework that systematically measures contextual knowledge (CK) and parametric knowledge (PK) across models and languages. Using our MultiWikiAtomic dataset in English, Spanish, and Danish, we analyze how large language models (LLMs) integrate context, prioritize information, and incorporate PK in open-ended question answering. Our analysis uncovers a phenomenon we call lost-in-the-later, where LLMs tend to overlook or deprioritize information that appears later in a given context, revealing a strong positional bias that affects contextual grounding. We further find that reasoning models, as well as non-reasoning models prompted with chain-of-thought (CoT), use context even less than non-reasoning models without CoT and fail to mitigate the lost-in-the-later effect. CoT prompting, in particular, results in lower recall and shorter responses, leading to degraded contextual grounding. Based on these insights, we design prompt-based methods to effectively leverage input context. A case study applying CoPE to summarization demonstrates that CK-informed prompting improves factual grounding and reduces hallucination.', 'abstract_zh': '大型语言模型能够利用上下文知识和参数知识，但它们如何优先考虑和整合这些知识来源仍待探索。我们引入了CoPE，一种新型的评估框架，系统地衡量模型和不同语言中的上下文知识（CK）和参数知识（PK）。利用我们的MultiWikiAtomic数据集（英文、西班牙文和丹麦文），我们分析了大型语言模型（LLMs）如何整合上下文、优先处理信息以及在开放式问答中融入参数知识。我们的分析揭示了一种我们称之为“后来被遗忘”的现象，即LLMs倾向于忽视或低优先级处理在给定上下文后出现的信息，这反映出一种强烈的位置偏见，影响了上下文定位。进一步的研究发现，即使使用链式思维（CoT）提示的推理模型和非推理模型，它们也比未使用CoT提示的非推理模型更少利用上下文，并且未能减轻“后来被遗忘”的影响。特别是CoT提示，导致较低的理解召回率和更短的回答长度，从而降低上下文定位的性能。基于这些见解，我们设计了基于提示的方法，以有效利用输入上下文。将CoPE应用于摘要任务的案例研究表明，基于上下文知识的提示能够提高事实定位并减少幻觉。', 'title_zh': '"迷失在后来": 量化大型语言模型上下文关联的框架'}
{'arxiv_id': 'arXiv:2507.05418', 'title': 'Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning', 'authors': 'Jaedong Hwang, Kumar Tanmay, Seok-Jin Lee, Ayush Agrawal, Hamid Palangi, Kumar Ayush, Ila Fiete, Paul Pu Liang', 'link': 'https://arxiv.org/abs/2507.05418', 'abstract': 'Large Language Models (LLMs) have achieved strong performance in domains like mathematics, factual QA, and code generation, yet their multilingual reasoning capabilities in these tasks remain underdeveloped. Especially for low-resource languages such as Swahili or Thai, LLMs can often misinterpret prompts or default to reasoning in English. This implicit bias toward high-resource languages undermines factual accuracy, interpretability, and trust. Current multilingual benchmarks focus only on final answers, overlooking whether models actually reason in the target language. To address this gap, we introduce GeoFact-X, a geography-based multilingual factual reasoning benchmark with annotated reasoning traces in five languages: English, Hindi, Japanese, Swahili, and Thai. We further propose BRIDGE, a novel training method that guides supervised fine-tuning and test-time reinforcement learning with a language-consistency reward to align reasoning with the input language. Finally, we develop an automatic evaluation protocol using LLM-as-a-judge to assess answer correctness and the quality and language consistency of reasoning traces, enabling nuanced and scalable analysis beyond surface-level metrics. Our results show that BRIDGE significantly enhances multilingual reasoning fidelity, demonstrating that reasoning-aware multilingual reinforcement learning is crucial for robust cross-lingual generalization. this https URL', 'abstract_zh': '大规模语言模型（LLMs）在数学、事实QA和代码生成等领域取得了出色的成绩，但在这些任务中的多语言推理能力仍较为欠缺。特别是在斯瓦希里语或泰语等低资源语言方面，LLMs 经常误读提示，或默认使用英语进行推理。这种偏向高资源语言的隐含偏见影响了事实准确性、可解释性和可信度。当前的多语言基准测试仅关注最终答案，忽视了模型是否在目标语言中进行推理。为填补这一空白，我们引入了GeoFact-X，这是一个基于地理的多语言事实推理基准，其中包含用五种语言（英语、 Hindi、日语、斯瓦希里语和泰语）标注的推理痕迹。我们还提出了BRIDGE，这是一种新的训练方法，通过语言一致性奖励引导监督微调和测试时的强化学习，使推理与输入语言保持一致。最后，我们开发了一种自动评估协议，使用LLM作为评委，评估答案的正确性和推理痕迹的质量及语言一致性，从而实现多维度和可扩展的分析，超越表面指标。我们的结果显示，BRIDGE 显著提高了多语言推理的准确性，证明了意识推理的多语言强化学习对于稳健的跨语言泛化至关重要。this https URL', 'title_zh': '全局学习，本地表达：多语言推理的-gap填补-'}
{'arxiv_id': 'arXiv:2507.05391', 'title': 'Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences', 'authors': 'Guillem Ramírez, Alexandra Birch, Ivan Titov', 'link': 'https://arxiv.org/abs/2507.05391', 'abstract': 'Large language models (LLMs) are primarily accessed via commercial APIs, but this often requires users to expose their data to service providers. In this paper, we explore how users can stay in control of their data by using privacy profiles: simple natural language instructions that say what should and should not be revealed. We build a framework where a local model uses these instructions to rewrite queries, only hiding details deemed sensitive by the user, before sending them to an external model, thus balancing privacy with performance. To support this research, we introduce PEEP, a multilingual dataset of real user queries annotated to mark private content and paired with synthetic privacy profiles. Our experiments with lightweight LLMs show they can follow these instructions to some extent, but also face consistent challenges, highlighting the need for models that better understand and comply with user-defined privacy preferences.', 'abstract_zh': '大型语言模型（LLMs）主要通过商业API访问，但这种访问方式往往要求用户将其数据暴露给服务提供商。在本文中，我们探索用户如何通过使用隐私配置文件来保持对其数据的控制：这是一种简单的自然语言指令，指示应该揭示和不应该揭示的内容。我们构建了一个框架，其中本地模型使用这些指令重写查询，仅隐藏用户认为敏感的细节，然后将它们发送到外部模型，从而在隐私与性能之间取得平衡。为了支持这项研究，我们引入了PEEP，这是一个多语言的用户查询数据集，其中包含真实用户的查询并标注了私人内容，并配对了合成的隐私配置文件。我们的实验表明，轻量级LLM在某种程度上可以遵循这些指令，但也面临持续的挑战，突显了需要更好地理解和遵守用户定义的隐私偏好模型的需求。', 'title_zh': '控制你分享的内容：评估语言模型对隐私偏好遵守的情况'}
{'arxiv_id': 'arXiv:2507.05386', 'title': 'Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training', 'authors': 'Song Lai, Haohan Zhao, Rong Feng, Changyi Ma, Wenzhuo Liu, Hongbo Zhao, Xi Lin, Dong Yi, Min Xie, Qingfu Zhang, Hongbin Liu, Gaofeng Meng, Fei Zhu', 'link': 'https://arxiv.org/abs/2507.05386', 'abstract': "Continual post-training (CPT) is a popular and effective technique for adapting foundation models like multimodal large language models to specific and ever-evolving downstream tasks. While existing research has primarily concentrated on methods like data replay, model expansion, or parameter regularization, the fundamental role of the learning paradigm within CPT remains largely unexplored. This paper presents a comparative analysis of two core post-training paradigms: supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT), investigating their respective impacts on knowledge retention during CPT. Our experiments are conducted on a benchmark comprising seven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base model for continual post-training. The investigation yields two significant findings: (1) When continuously learning on downstream tasks, SFT leads to catastrophic forgetting of previously learned tasks. In contrast, RFT inherently preserves prior knowledge and achieve performance comparable to multi-task training. (2) RFT successfully protects and even enhances the model's general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro). Conversely, SFT degrades general model capabilities severely. Further analysis shows that explicit mechanisms, such as KL penalty and chain-of-thought reasoning, are not the primary factors. Instead, we find that the implicit regularization inherent to RFT is a key factor in mitigating forgetting. Finally, we propose a rollout-based instance filtering algorithm to improve the stability and efficiency of RFT. Our comprehensive study demonstrates the superiority of RFT as a robust paradigm for continual post-training.", 'abstract_zh': '持续后训练（CPT）是将基础模型如多模态大型语言模型适应特定且不断演变的下游任务的一种流行而有效的方法。尽管现有研究主要集中在数据重放、模型扩展或参数正则化等方法上，CPT 中学习范式的根本作用仍 largely unexplored。本文对两种核心后训练范式——监督微调（SFT）和强化微调（RFT）——进行了比较分析，研究了它们在 CPT 中对知识保留的影响。我们在包含七个 diverse 多模态任务的基准上进行了实验，使用 Qwen2.5-VL-7B-Instruct 作为持续后训练的基础模型。研究发现两个重要结论：（1）在持续学习下游任务时，SFT 导致对先前学习任务的灾难性遗忘。相比之下，RFT 内在地保留了先前的知识，并且可以达到多任务训练相当的性能。（2）RFT 成功保护甚至增强了模型在标准基准上的通识知识（例如，MMMU 和 MMLU-Pro）。相反，SFT 严重降解了模型的一般能力。进一步分析表明，显式机制如 KL 哑铃和步骤推理并不是主要原因。相反，我们发现 RFT 内在的正则化是减轻遗忘的关键因素。最后，我们提出了一种基于 rollout 的实例过滤算法以提高 RFT 的稳定性和效率。我们的全面研究表明，RFT 是一种在持续后训练中更为 robust 的范式。', 'title_zh': '强化微调自然减轻连续后训练中的遗忘'}
{'arxiv_id': 'arXiv:2507.05362', 'title': 'On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study', 'authors': 'Riccardo Alberghi, Elizaveta Demyanenko, Luca Biggio, Luca Saglietti', 'link': 'https://arxiv.org/abs/2507.05362', 'abstract': "Recent advances in natural language processing highlight two key factors for improving reasoning in large language models (LLMs): (i) allocating more test-time compute tends to help on harder problems but often introduces redundancy in the reasoning trace, and (ii) compute is most effective when reasoning is systematic and incremental, forming structured chains of thought (CoTs) akin to human problem-solving. To study these factors in isolation, we introduce a controlled setting based on shortest-path tasks in layered graphs. We train decoder-only transformers on question-trace-answer triples using a custom tokenizer, comparing models trained on optimal bottom-up dynamic programming traces with those trained on longer, valid traces involving backtracking. Surprisingly, with the same training-token budget, models trained on inefficient traces generalize better to unseen graphs. This benefit is not due to length alone-injecting arbitrary redundancy into reasoning traces fails to help and can even hurt performance. Instead, we find that generalization correlates with the model's confidence in next-token prediction, suggesting that long, coherent, and locally incremental traces make the training signal easier to optimize.", 'abstract_zh': 'Recent advances in自然语言处理强调了两个关键因素以改善大型语言模型（LLMs）的推理能力：(i) 在测试时分配更多的计算资源有助于解决更难的问题，但往往会引入推理轨迹中的冗余；(ii) 计算资源在推理是系统性和逐步性的前提下最有效，形成类似于人类问题解决过程的结构化思路链（CoTs）。为了在隔离环境下研究这些因素，我们基于分层图中的最短路径任务引入了一个受控设置。我们使用自定义分词器对仅解码器变压器模型进行训练，并将其应用于问题-轨迹-答案三元组，将基于最优自底向上的动态规划轨迹训练的模型与需要回溯的更长但有效轨迹训练的模型进行比较。令人惊讶的是，在相同的训练词元预算下，使用低效轨迹训练的模型在未见过的图上泛化能力更强。这一好处并非仅仅归因于长度，注入任意冗余到推理轨迹并不能帮助，甚至可能损害性能。相反，我们发现，泛化与模型对未来词元预测的信心相关，表明长且连贯的、局部逐步的轨迹使训练信号更容易优化。', 'title_zh': '关于 Next-Token 预测器偏向系统性低效推理的偏差：最短路径案例研究'}
{'arxiv_id': 'arXiv:2507.05346', 'title': 'LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks', 'authors': 'William Fleshman, Benjamin Van Durme', 'link': 'https://arxiv.org/abs/2507.05346', 'abstract': "The proliferation of fine-tuned language model experts for specific tasks and domains signals the need for efficient selection and combination methods. We propose LoRA-Augmented Generation (LAG) for leveraging large libraries of knowledge and task-specific LoRA adapters. LAG requires no additional training or access to data, and efficiently filters, retrieves, and applies experts on a per-token and layer basis. We evaluate LAG on various knowledge-intensive tasks, achieving superior performance over existing data-free methods. We explore scenarios where additional data is available, demonstrating LAG's compatibility with alternative solutions such as retrieval-augmented generation (RAG).", 'abstract_zh': '细调语言模型专家的 proliferations 信号了高效选择和组合方法的需求。我们提出 LoRA 增强生成 (LAG) 方法以利用大规模知识库和任务特定的 LoRA 调整器。LAG 不需要额外训练或访问数据，并且能够逐词和逐层高效地筛选、检索和应用专家。我们在各种知识密集型任务上评估了 LAG，实现了优于现有无数据方法的性能。我们在有额外数据可用的情景中探讨了 LAG 与检索增强生成 (RAG) 等替代方案的兼容性。', 'title_zh': 'LoRA增强生成（LAG）用于知识密集型语言任务'}
{'arxiv_id': 'arXiv:2507.05330', 'title': 'MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents', 'authors': 'Ming Gong, Xucheng Huang, Chenghan Yang, Xianhan Peng, Haoxin Wang, Yang Liu, Ling Jiang', 'link': 'https://arxiv.org/abs/2507.05330', 'abstract': 'Recent advances in large language models (LLMs) have enabled new applications in e-commerce customer service. However, their capabilities remain constrained in complex, multimodal scenarios. We present MindFlow, the first open-source multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it integrates memory, decision-making, and action modules, and adopts a modular "MLLM-as-Tool" strategy for effect visual-textual reasoning. Evaluated via online A/B testing and simulation-based ablation, MindFlow demonstrates substantial gains in handling complex queries, improving user satisfaction, and reducing operational costs, with a 93.53% relative improvement observed in real-world deployments.', 'abstract_zh': 'Recent advances in large language models (LLMs) have enabled new applications in e-commerce customer service. However, their capabilities remain constrained in complex, multimodal scenarios. We present MindFlow, the first open-source multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it integrates memory, decision-making, and action modules, and adopts a modular "MLLM-as-Tool" strategy for effect visual-textual reasoning. Evaluated via online A/B testing and simulation-based ablation, MindFlow demonstrates substantial gains in handling complex queries, improving user satisfaction, and reducing operational costs, with a 93.53% relative improvement observed in real-world deployments.', 'title_zh': 'MindFlow：以多模态LLM代理革新电子商务客户服务'}
{'arxiv_id': 'arXiv:2507.05319', 'title': 'LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review', 'authors': 'Cheng Yuan, Xinkai Rui, Yongqi Fan, Yawei Fan, Boyang Zhong, Jiacheng Wang, Weiyan Zhang, Tong Ruan', 'link': 'https://arxiv.org/abs/2507.05319', 'abstract': 'Despite the remarkable performance of Large Language Models (LLMs) in automated discharge summary generation, they still suffer from hallucination issues, such as generating inaccurate content or fabricating information without valid sources. In addition, electronic medical records (EMRs) typically consist of long-form data, making it challenging for LLMs to attribute the generated content to the sources. To address these challenges, we propose LCDS, a Logic-Controlled Discharge Summary generation system. LCDS constructs a source mapping table by calculating textual similarity between EMRs and discharge summaries to constrain the scope of summarized content. Moreover, LCDS incorporates a comprehensive set of logical rules, enabling it to generate more reliable silver discharge summaries tailored to different clinical fields. Furthermore, LCDS supports source attribution for generated content, allowing experts to efficiently review, provide feedback, and rectify errors. The resulting golden discharge summaries are subsequently recorded for incremental fine-tuning of LLMs. Our project and demo video are in the GitHub repository this https URL.', 'abstract_zh': '尽管大型语言模型（LLMs）在自动出院总结生成方面取得了显著性能，但仍存在幻觉问题，如生成不准确的内容或无可靠来源地编造信息。此外，电子医疗记录（EMRs）通常包含长篇数据，这使得LLMs难以将生成的内容与来源关联起来。为了解决这些问题，我们提出了一种逻辑控制出院总结生成系统（LCDS）。LCDS通过计算出院总结和EMRs之间的文本相似性来构建源映射表，从而限制总结内容的范围。此外，LCDS整合了一整套逻辑规则，使其能够生成更可靠的出院总结，适用于不同的临床领域。此外，LCDS支持生成内容的来源归属，使专家能够高效地审查、提供反馈并纠正错误。生成的黄金出院总结随后用于LLMs的增量微调。我们的项目和演示视频可在GitHub仓库中访问：https://github.com/almightydex/LCDS。', 'title_zh': 'LCDS：一种支持来源归属和专家审核的逻辑控制出院总结生成系统'}
{'arxiv_id': 'arXiv:2507.05307', 'title': 'ASSURE: Metamorphic Testing for AI-powered Browser Extensions', 'authors': 'Xuanqi Gao, Juan Zhai, Shiqing Ma, Siyi Xie, Chao Shen', 'link': 'https://arxiv.org/abs/2507.05307', 'abstract': "The integration of Large Language Models (LLMs) into browser extensions has revolutionized web browsing, enabling sophisticated functionalities like content summarization, intelligent translation, and context-aware writing assistance. However, these AI-powered extensions introduce unprecedented challenges in testing and reliability assurance. Traditional browser extension testing approaches fail to address the non-deterministic behavior, context-sensitivity, and complex web environment integration inherent to LLM-powered extensions. Similarly, existing LLM testing methodologies operate in isolation from browser-specific contexts, creating a critical gap in effective evaluation frameworks. To bridge this gap, we present ASSURE, a modular automated testing framework specifically designed for AI-powered browser extensions. ASSURE comprises three principal components: (1) a modular test case generation engine that supports plugin-based extension of testing scenarios, (2) an automated execution framework that orchestrates the complex interactions between web content, extension processing, and AI model behavior, and (3) a configurable validation pipeline that systematically evaluates behavioral consistency and security invariants rather than relying on exact output matching. Our evaluation across six widely-used AI browser extensions demonstrates ASSURE's effectiveness, identifying 531 distinct issues spanning security vulnerabilities, metamorphic relation violations, and content alignment problems. ASSURE achieves 6.4x improved testing throughput compared to manual approaches, detecting critical security vulnerabilities within 12.4 minutes on average. This efficiency makes ASSURE practical for integration into development pipelines, offering a comprehensive solution to the unique challenges of testing AI-powered browser extensions.", 'abstract_zh': '大型语言模型（LLMs）集成到浏览器扩展中的整合已经革新了网络浏览，实现了如内容总结、智能翻译和上下文感知撰写辅助等复杂功能。然而，这些具备AI功能的扩展引入了前所未有的测试和可靠性保障挑战。传统的浏览器扩展测试方法无法解决LLM驱动扩展固有的非确定性行为、上下文敏感性和复杂网络环境集成问题。同样，现有的LLM测试方法与浏览器特定上下文脱节，导致有效的评估框架存在关键缺陷。为弥合这一差距，我们提出了ASSURE，一个专门针对AI驱动浏览器扩展的模块化自动化测试框架。ASSURE由三个主要组件构成：（1）一个支持插件扩展的模块化测试案例生成引擎；（2）一个自动化执行框架，协调网络内容、扩展处理和AI模型行为之间的复杂交互；（3）一个可配置的验证流水线，系统地评估行为一致性与安全保障不变式，而不是依赖于精确输出匹配。我们在六款广泛使用的AI浏览器扩展上的评估证明了ASSURE的有效性，发现涵盖安全漏洞、变形关系违反和内容对齐问题在内的531个独特问题。与手动方法相比，ASSURE提高了6.4倍的测试吞吐量，平均在12.4分钟内检测到关键安全漏洞。这种效率使ASSURE能够在开发管道中实现集成，提供一个针对AI驱动浏览器扩展的独特挑战的全面解决方案。', 'title_zh': 'ASSURE: 面向AI驱动浏览器扩展的 metamorphic 测试'}
{'arxiv_id': 'arXiv:2507.05305', 'title': 'Narrowing the Gap: Supervised Fine-Tuning of Open-Source LLMs as a Viable Alternative to Proprietary Models for Pedagogical Tools', 'authors': 'Lorenzo Lee Solano, Charles Koutcheme, Juho Leinonen, Alexandra Vassar, Jake Renzella', 'link': 'https://arxiv.org/abs/2507.05305', 'abstract': 'Frontier Large language models (LLMs) like ChatGPT and Gemini can decipher cryptic compiler errors for novice programmers, but their computational scale, cost, and tendency to over-assist make them problematic for widespread pedagogical adoption. This work demonstrates that smaller, specialised language models, enhanced via Supervised Fine-Tuning (SFT), present a more viable alternative for educational tools. We utilise a new dataset of 40,000 C compiler error explanations, derived from real introductory programming (CS1/2) student-generated programming errors, which we used to fine-tune three open-source models: Qwen3-4B, Llama-3.1-8B, and Qwen3-32B. We performed a dual evaluation, combining expert human reviews with a large-scale automated analysis of 8,000 responses using a validated LLM-as-judge ensemble. Our results show that SFT significantly boosts the pedagogical quality of smaller models, achieving performance comparable to much larger models. We analyse the trade-offs between model size and quality, confirming that fine-tuning compact, efficient models on high-quality, domain-specific data is a potent strategy for creating specialised models to drive educational tools. We provide a replicable methodology to foster broader access to generative AI capabilities in educational contexts.', 'abstract_zh': '较小的专门化语言模型通过监督微调提升教育质量：以编译器错误解释为例', 'title_zh': '缩小差距：将开源大语言模型通过监督微调作为自购模型的可行替代方案应用于教学工具'}
{'arxiv_id': 'arXiv:2507.05288', 'title': 'A Survey on Proactive Defense Strategies Against Misinformation in Large Language Models', 'authors': 'Shuliang Liu, Hongyi Liu, Aiwei Liu, Bingchen Duan, Qi Zheng, Yibo Yan, He Geng, Peijie Jiang, Jia Liu, Xuming Hu', 'link': 'https://arxiv.org/abs/2507.05288', 'abstract': 'The widespread deployment of large language models (LLMs) across critical domains has amplified the societal risks posed by algorithmically generated misinformation. Unlike traditional false content, LLM-generated misinformation can be self-reinforcing, highly plausible, and capable of rapid propagation across multiple languages, which traditional detection methods fail to mitigate effectively. This paper introduces a proactive defense paradigm, shifting from passive post hoc detection to anticipatory mitigation strategies. We propose a Three Pillars framework: (1) Knowledge Credibility, fortifying the integrity of training and deployed data; (2) Inference Reliability, embedding self-corrective mechanisms during reasoning; and (3) Input Robustness, enhancing the resilience of model interfaces against adversarial attacks. Through a comprehensive survey of existing techniques and a comparative meta-analysis, we demonstrate that proactive defense strategies offer up to 63\\% improvement over conventional methods in misinformation prevention, despite non-trivial computational overhead and generalization challenges. We argue that future research should focus on co-designing robust knowledge foundations, reasoning certification, and attack-resistant interfaces to ensure LLMs can effectively counter misinformation across varied domains.', 'abstract_zh': '大规模语言模型（LLMs）在关键领域的广泛应用放大了由算法生成的 misinformation 所带来的社会风险。不同于传统的虚假内容，由LLM生成的 misinformation 具有自我强化、高度可信和能在多种语言中迅速传播的特性，这使得传统的检测方法难以有效应对。本文介绍了一种主动防御范式，从被动的事后检测转向前瞻性缓解策略。我们提出了一种三支柱框架：（1）知识可信度，强化训练和部署数据的完整性；（2）推理可靠性，在推理过程中嵌入自我纠正机制；（3）输入鲁棒性，增强模型界面对抗攻击的韧性。通过全面回顾现有技术和进行比较元分析，我们证明了主动防御策略在 misinformation 防范方面相较于传统方法可提高高达63%，尽管面临非平凡的计算开销和泛化挑战。我们认为未来研究应集中在共同设计 robust 的知识基础、推理认证和抗攻击界面，以确保LLM能够在多种领域有效地对抗 misinformation。', 'title_zh': '大型语言模型中对抗误导信息的主动防御策略综述'}
{'arxiv_id': 'arXiv:2507.05282', 'title': 'Exploring LLM Capabilities in Extracting DCAT-Compatible Metadata for Data Cataloging', 'authors': 'Lennart Busch, Daniel Tebernum, Gissel Velarde', 'link': 'https://arxiv.org/abs/2507.05282', 'abstract': 'Efficient data exploration is crucial as data becomes increasingly important for accelerating processes, improving forecasts and developing new business models. Data consumers often spend 25-98 % of their time searching for suitable data due to the exponential growth, heterogeneity and distribution of data. Data catalogs can support and accelerate data exploration by using metadata to answer user queries. However, as metadata creation and maintenance is often a manual process, it is time-consuming and requires expertise. This study investigates whether LLMs can automate metadata maintenance of text-based data and generate high-quality DCAT-compatible metadata. We tested zero-shot and few-shot prompting strategies with LLMs from different vendors for generating metadata such as titles and keywords, along with a fine-tuned model for classification. Our results show that LLMs can generate metadata comparable to human-created content, particularly on tasks that require advanced semantic understanding. Larger models outperformed smaller ones, and fine-tuning significantly improves classification accuracy, while few-shot prompting yields better results in most cases. Although LLMs offer a faster and reliable way to create metadata, a successful application requires careful consideration of task-specific criteria and domain context.', 'abstract_zh': '高效的數據探索對於加速過程、提高預測精度和 DEVELOP 新業務模型至關重要。隨著數據的指數級增長、異構性和分佈特性，數據消費者往往會將25-98%的時間用於搜索合適的數據。數據目錄可以通过使用元數據來回答用户查询，从而支持和加速数据探索。但是，由于元数据的创建和维护通常是一个手动过程，这既耗时又需要专业知识。本研究探讨了大型语言模型（LLMs）是否能够自动化基于文本的数据的元数据维护，并生成高质量的DCAT兼容元数据。我们使用来自不同供应商的大型语言模型测试了零样本和少样本提示策略，以生成元数据，例如标题和关键词，并使用微调模型进行分类。结果显示，大型语言模型可以生成与人类创建的内容媲美的元数据，特别是在需要高级语义理解的任务中。较大的模型表现优于较小的模型，微调显著提高了分类准确性，而在大多数情况下，少样本提示策略的效果更佳。尽管大型语言模型提供了更快捷和可靠的方法来创建元数据，但成功应用仍需要针对特定任务和领域上下文进行仔细考虑。', 'title_zh': '探索LLM在提取DCAT兼容元数据以进行数据目录构建方面的能力'}
{'arxiv_id': 'arXiv:2507.05279', 'title': 'ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy', 'authors': 'Virgile Boraud, Yannis Bendi-Ouis, Paul Bernard, Xavier Hinaut', 'link': 'https://arxiv.org/abs/2507.05279', 'abstract': 'We introduce a tool designed to improve the capabilities of Large Language Models (LLMs) in assisting with code development using the ReservoirPy library, as well as in answering complex questions in the field of Reservoir Computing. By incorporating external knowledge through Retrieval-Augmented Generation (RAG) and knowledge graphs, our approach aims to reduce hallucinations and increase the factual accuracy of generated responses. The system provides an interactive experience similar to ChatGPT, tailored specifically for ReservoirPy, enabling users to write, debug, and understand Python code while accessing reliable domain-specific insights. In our evaluation, while proprietary models such as ChatGPT-4o and NotebookLM performed slightly better on general knowledge questions, our model outperformed them on coding tasks and showed a significant improvement over its base model, Codestral-22B.', 'abstract_zh': '我们介绍了一个工具，该工具旨在通过ReservoirPy库提高大型语言模型（LLMs）在代码开发中的辅助能力，以及在蓄水池计算领域的复杂问题解答能力。通过使用检索增强生成（RAG）和知识图谱来融入外部知识，我们的方法旨在减少幻觉并提高生成响应的事实准确性。该系统提供了一个类似于ChatGPT的交互体验，专门针对ReservoirPy，使用户能够在访问可靠的领域特定见解的同时编写、调试和理解Python代码。在评估中，尽管ChatGPT-4o和NotebookLM等商业模型在一般知识问题上表现略好，但我们的模型在编码任务上的表现优于它们，并且相对于其基础模型Codestral-22B显示出显著的改进。', 'title_zh': 'ReservoirChat：通过大规模语言模型和知识图谱增强的交互式文档'}
{'arxiv_id': 'arXiv:2507.05272', 'title': 'FuzzFeed: An Automatic Approach to Weakest Precondition Generation using LLMs and Fuzzing', 'authors': 'Daragh King, Vasileios Koutavas, Laura Kovacs', 'link': 'https://arxiv.org/abs/2507.05272', 'abstract': 'The weakest precondition (WP) of a program describes the largest set of initial states from which all terminating executions of the program satisfy a given postcondition. The generation of WPs is an important task with practical applications in areas ranging from verification to run-time error checking.\nThis paper proposes the combination of Large Language Models (LLMs) and fuzz testing for generating WPs. In pursuit of this goal, we introduce Fuzzing Guidance (FG); FG acts as a means of directing LLMs towards correct WPs using program execution feedback. FG utilises fuzz testing for approximately checking the validity and weakness of candidate WPs, this information is then fed back to the LLM as a means of context refinement.\nWe demonstrate the effectiveness of our approach on a comprehensive benchmark set of deterministic array programs in Java. Our experiments indicate that LLMs are capable of producing viable candidate WPs, and that this ability can be practically enhanced through FG.', 'abstract_zh': '大型语言模型与 fuzz 测试结合生成最强前置条件的研究', 'title_zh': 'FuzzFeed：一种使用大语言模型和 fuzzing 生成最弱前置条件的自动方法'}
{'arxiv_id': 'arXiv:2507.05269', 'title': 'CORE: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks', 'authors': 'Danning Xie, Mingwei Zheng, Xuwei Liu, Jiannan Wang, Chengpeng Wang, Lin Tan, Xiangyu Zhang', 'link': 'https://arxiv.org/abs/2507.05269', 'abstract': 'Large language models (LLMs) have been widely adopted across diverse software engineering domains, such as code generation, program repair, and vulnerability detection. These applications require understanding beyond surface-level code patterns: value propagation, control flow, and interdependence between program elements. However, existing benchmarks primarily evaluate end-to-end outcomes, such as whether code is correctly repaired or generated, leaving the models ability for program semantic reasoning underexplored. This work presents CoRe, a high-quality, human-verified benchmark designed to evaluate LLMs on fundamental static analysis tasks. CoRe includes 12,553 task instances spanning data dependency, control dependency, and information flow across programs written in C/C++, Java, and Python. To ensure semantic diversity and reasoning complexity, we propose a semantics-aware diverse sampling strategy that selects targets and task instances based on structural coverage and dependency depth. We evaluate 10 mainstream LLMs and show that, while they perform well at identifying dependencies, models still struggle with tasks that require deeper semantic understanding and multi-step reasoning. We further conduct qualitative analyses to uncover key challenges, such as complex control structures and backward dependency patterns, offering insights into improving LLMs code reasoning capabilities.', 'abstract_zh': '大型语言模型（LLMs）已在软件工程的多元领域中广泛应用，例如代码生成、程序修复和漏洞检测。这些应用需要理解超越表面代码模式的能力，包括值传递、控制流以及程序元素之间的相互依赖。然而，现有的基准主要评估端到端的结果，如代码是否正确修复或生成，导致程序语义推理的能力尚未被充分探索。本文介绍了一个高质量的人工验证基准CoRe，旨在评估LLMs在基本静态分析任务上的表现。CoRe包含了12,553个任务实例，覆盖了C/C++、Java和Python编写的程序中的数据依赖、控制依赖和信息流。为了确保语义多样性和推理复杂性，我们提出了一种意识语义的多样采样策略，基于结构覆盖率和依赖深度选择目标和任务实例。我们评估了10个主流的LLMs，并展示了虽然它们在识别依赖方面表现良好，但在需要更深层次语义理解与多步推理的任务中仍然存在挑战。进一步的定性分析揭示了关键挑战，如复杂的控制结构和逆向依赖模式，为提升LLMs代码推理能力提供了见解。', 'title_zh': 'CORE：通过静态分析任务评估LLM的代码推理能力'}
{'arxiv_id': 'arXiv:2507.05266', 'title': 'User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs', 'authors': 'Sougata Saha, Monojit Choudhury', 'link': 'https://arxiv.org/abs/2507.05266', 'abstract': "Measuring the generalization ability of Large Language Models (LLMs) is challenging due to data contamination. As models grow and computation becomes cheaper, ensuring tasks and test cases are unseen during training phases will become nearly impossible. We argue that knowledge-retrieval and reasoning tasks are not ideal for measuring generalization, as LLMs are not trained for specific tasks. Instead, we propose user behavior prediction, also a key aspect of personalization, as a theoretically sound, scalable, and robust alternative. We introduce a novel framework for this approach and test it on movie and music recommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct. Results align with our framework's predictions, showing GPT-4o outperforms GPT-4o-mini and Llama, though all models have much room for improvement, especially Llama.", 'abstract_zh': '测量大型语言模型的泛化能力由于数据污染而具有挑战性。随着模型的增长和计算成本的下降，确保训练阶段中看不到任务和测试案例将变得几乎不可能。我们argue认为知识检索和推理任务不是衡量泛化能力的理想选择，因为LLMs没有针对特定任务进行训练。相反，我们提出了用户行为预测，这也是个性化的一个关键方面，作为理论上有根据、可扩展且稳健的替代方案。我们介绍了一种新的框架并在此方法上对GPT-4o、GPT-4o-mini和Llama-3.1-8B-Instruct的电影和音乐推荐数据集进行了测试。结果与我们框架的预测一致，显示GPT-4o优于GPT-4o-mini和Llama，尽管所有模型都还有很大的提升空间，尤其是Llama。', 'title_zh': '用户行为预测作为一种通用、稳健、可扩展且低成本的评估策略，用于估计LLMs的泛化能力'}
{'arxiv_id': 'arXiv:2507.01436', 'title': 'Challenges & Opportunities with LLM-Assisted Visualization Retargeting', 'authors': 'Luke S. Snyder, Chenglong Wang, Steven M. Drucker', 'link': 'https://arxiv.org/abs/2507.01436', 'abstract': 'Despite the ubiquity of visualization examples published on the web, retargeting existing custom chart implementations to new datasets remains difficult, time-intensive, and tedious. The adaptation process assumes author familiarity with both the implementation of the example as well as how the new dataset might need to be transformed to fit into the example code. With recent advances in Large Language Models (LLMs), automatic adaptation of code can be achieved from high-level user prompts, reducing the barrier for visualization retargeting. To better understand how LLMs can assist retargeting and its potential limitations, we characterize and evaluate the performance of LLM assistance across multiple datasets and charts of varying complexity, categorizing failures according to type and severity. In our evaluation, we compare two approaches: (1) directly instructing the LLM model to fully generate and adapt code by treating code as text inputs and (2) a more constrained program synthesis pipeline where the LLM guides the code construction process by providing structural information (e.g., visual encodings) based on properties of the example code and data. We find that both approaches struggle when new data has not been appropriately transformed, and discuss important design recommendations for future retargeting systems.', 'abstract_zh': '尽管网络上充斥着大量的可视化示例，将现有的自定义图表实现应用到新数据集仍然困难重重、耗时且繁琐。这一适应过程需要作者熟悉示例的实现方式以及新数据集可能需要如何转换以适应示例代码。随着大型语言模型（LLMs）的最新进展，可以从高级用户提示自动适应代码，从而降低可视化重新应用的门槛。为了更好地理解LLMs如何辅助重新应用及其潜在局限性，我们对多种不同复杂度的数据集和图表进行了特征描述和性能评估，并按类型和严重程度对失败进行了分类。在我们的评估中，我们将两种方法进行比较：（1）直接指示LLM模型生成并适应代码，将代码视为文本输入，以及（2）一种更受限的程序合成管道，其中LLM通过提供结构信息（例如，视觉编码）来引导代码构建过程，这些结构信息基于示例代码和数据的特性。我们发现，这两种方法在新数据未适当转换时均面临挑战，并讨论了未来重新应用系统的重要设计建议。', 'title_zh': 'LLM辅助视觉重定位的机遇与挑战'}
