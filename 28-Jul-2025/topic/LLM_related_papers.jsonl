{'arxiv_id': 'arXiv:2507.19364', 'title': 'Integrating LLM in Agent-Based Social Simulation: Opportunities and Challenges', 'authors': 'Patrick Taillandier, Jean Daniel Zucker, Arnaud Grignard, Benoit Gaudou, Nghi Quang Huynh, Alexis Drogoul', 'link': 'https://arxiv.org/abs/2507.19364', 'abstract': 'This position paper examines the use of Large Language Models (LLMs) in social simulation, analyzing both their potential and their limitations from a computational social science perspective. The first part reviews recent findings on the ability of LLMs to replicate key aspects of human cognition, including Theory of Mind reasoning and social inference, while also highlighting significant limitations such as cognitive biases, lack of true understanding, and inconsistencies in behavior. The second part surveys emerging applications of LLMs in multi-agent simulation frameworks, focusing on system architectures, scale, and validation strategies. Notable projects such as Generative Agents (Smallville) and AgentSociety are discussed in terms of their design choices, empirical grounding, and methodological innovations. Particular attention is given to the challenges of behavioral fidelity, calibration, and reproducibility in large-scale LLM-driven simulations. The final section distinguishes between contexts where LLMs, like other black-box systems, offer direct value-such as interactive simulations and serious games-and those where their use is more problematic, notably in explanatory or predictive modeling. The paper concludes by advocating for hybrid approaches that integrate LLMs into traditional agent-based modeling platforms (GAMA, Netlogo, etc), enabling modelers to combine the expressive flexibility of language-based reasoning with the transparency and analytical rigor of classical rule-based systems.', 'abstract_zh': '这一立场论文探讨了大型语言模型（LLMs）在社会仿真中的应用，从计算社会科学的角度分析了它们的潜在价值和局限性。第一部分回顾了LLMs在复制人类认知关键方面的能力，包括心理理论推理和社会推断，并指出了认知偏差、缺乏真正理解以及行为不一致性等重要局限。第二部分概述了LLMs在多代理仿真框架中的新兴应用，重点关注系统架构、规模和验证策略。讨论了诸如生成代理（Smallville）和AgentSociety等重要项目的设计选择、实证基础和方法论创新。特别关注大规模LLM驱动仿真中的行为忠实性、校准和可再现性挑战。最后一部分区分了LLMs与其他黑盒系统在提供直接价值（如交互仿真和严肃游戏）的情境，以及它们在解释性或预测建模中的使用更具问题性的场景。论文最后提倡将LLMs与传统基于代理的建模平台（如GAMA、NetLogo等）相结合的混合方法，使建模者能够结合基于语言的推理的表达灵活性与经典基于规则系统的透明性和分析严谨性。', 'title_zh': '基于代理的社会仿真中大型语言模型的整合：机遇与挑战'}
{'arxiv_id': 'arXiv:2507.18775', 'title': 'Initial Steps in Integrating Large Reasoning and Action Models for Service Composition', 'authors': 'Ilche Georgievski, Marco Aiello', 'link': 'https://arxiv.org/abs/2507.18775', 'abstract': 'Service composition remains a central challenge in building adaptive and intelligent software systems, often constrained by limited reasoning capabilities or brittle execution mechanisms. This paper explores the integration of two emerging paradigms enabled by large language models: Large Reasoning Models (LRMs) and Large Action Models (LAMs). We argue that LRMs address the challenges of semantic reasoning and ecosystem complexity while LAMs excel in dynamic action execution and system interoperability. However, each paradigm has complementary limitations - LRMs lack grounded action capabilities, and LAMs often struggle with deep reasoning. We propose an integrated LRM-LAM architectural framework as a promising direction for advancing automated service composition. Such a system can reason about service requirements and constraints while dynamically executing workflows, thus bridging the gap between intention and execution. This integration has the potential to transform service composition into a fully automated, user-friendly process driven by high-level natural language intent.', 'abstract_zh': '基于大型语言模型的大型推理模型和大型行动模型集成：服务组合的新范式', 'title_zh': '初始步骤：集成大规模推理与行动模型进行服务组合'}
{'arxiv_id': 'arXiv:2507.19477', 'title': 'Advancing Event Forecasting through Massive Training of Large Language Models: Challenges, Solutions, and Broader Impacts', 'authors': 'Sang-Woo Lee, Sohee Yang, Donghyun Kwak, Noah Y. Siegel', 'link': 'https://arxiv.org/abs/2507.19477', 'abstract': "Many recent papers have studied the development of superforecaster-level event forecasting LLMs. While methodological problems with early studies cast doubt on the use of LLMs for event forecasting, recent studies with improved evaluation methods have shown that state-of-the-art LLMs are gradually reaching superforecaster-level performance, and reinforcement learning has also been reported to improve future forecasting. Additionally, the unprecedented success of recent reasoning models and Deep Research-style models suggests that technology capable of greatly improving forecasting performance has been developed. Therefore, based on these positive recent trends, we argue that the time is ripe for research on large-scale training of superforecaster-level event forecasting LLMs. We discuss two key research directions: training methods and data acquisition. For training, we first introduce three difficulties of LLM-based event forecasting training: noisiness-sparsity, knowledge cut-off, and simple reward structure problems. Then, we present related ideas to mitigate these problems: hypothetical event Bayesian networks, utilizing poorly-recalled and counterfactual events, and auxiliary reward signals. For data, we propose aggressive use of market, public, and crawling datasets to enable large-scale training and evaluation. Finally, we explain how these technical advances could enable AI to provide predictive intelligence to society in broader areas. This position paper presents promising specific paths and considerations for getting closer to superforecaster-level AI technology, aiming to call for researchers' interest in these directions.", 'abstract_zh': '近年来，许多研究论文探讨了超预测水平事件forecasting大型语言模型（LLM）的发展。尽管早期研究中的方法论问题曾对使用LLM进行事件forecasting的能力提出质疑，但近期采用改进评估方法的研究表明，最先进的LLM正在逐步达到超预测水平的表现，并且强化学习还被报告能改善未来forecasting。此外，近期推理模型和Deep Research风格模型前所未有的成功表明，大幅提升forecasting性能的技术已经开发出来。因此，基于这些积极的发展趋势，我们认为大规模训练超预测水平事件forecasting大型语言模型的时机已经成熟。我们讨论了两个关键的研究方向：训练方法和数据收集。在训练方面，我们首先介绍了基于LLM的事件forecasting训练的三个困难：噪声稀疏性、知识截断问题和简单的奖励结构问题。然后，我们提出了相关的方法来缓解这些问题：假设事件贝叶斯网络、利用召回不佳和反事实事件以及辅助奖励信号。在数据方面，我们建议积极使用市场、公共和爬取的数据集以实现大规模训练和评估。最后，我们解释了这些技术进步如何能够使AI在更广泛的领域提供预测智能。这份观点论文提出了通往达到超预测水平AI技术的具体路径和考虑，旨在呼吁研究人员对这些方向的兴趣。', 'title_zh': '通过大规模训练大型语言模型推进事件预测：挑战、解决方案及更广泛的影响'}
{'arxiv_id': 'arXiv:2507.19457', 'title': 'GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning', 'authors': 'Lakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael J Ryan, Meng Jiang, Christopher Potts, Koushik Sen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, Omar Khattab', 'link': 'https://arxiv.org/abs/2507.19457', 'abstract': "Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.", 'abstract_zh': '大型语言模型（LLMs）通过强化学习（RL）方法如群组相对策略优化（GRPO）越来越多地适应下游任务，这通常需要成千上万次回放来学习新任务。我们认为，语言的可解释性通常为LLMs提供了更为丰富的学习媒介，相比于从稀疏标量奖励派生的策略梯度。为了验证这一观点，我们引入了GEPA（Genetic-Pareto），这是一种全面结合自然语言反思的提示优化器，用于从试错中学到高层次规则。对于包含一个或多个LLM提示的任何AI系统，GEPA会采样系统级轨迹（例如推理、工具调用和工具输出），并在自然语言中对其进行反思以诊断问题、建议并测试提示更新，结合来自其自身尝试帕累托前沿的互补教训。由于GEPA的设计，它通常能够将少量回放转变成显著的质量提升。在四个任务中，GEPA在平均上比GRPO高出10%，最高可达20%，同时使用不到GRPO的35%的回放次数。GEPA还跨两个LLM比领先的提示优化器MIPROv2高出超过10%，并且展示了在代码优化的推理时搜索策略方面的有前途的结果。', 'title_zh': 'GEPA: 自省式提示进化可以超越强化学习'}
{'arxiv_id': 'arXiv:2507.19427', 'title': 'Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding', 'authors': 'StepFun, Bin Wang, Bojun Wang, Changyi Wan, Guanzhe Huang, Hanpeng Hu, Haonan Jia, Hao Nie, Mingliang Li, Nuo Chen, Siyu Chen, Song Yuan, Wuxun Xie, Xiaoniu Song, Xing Chen, Xingping Yang, Xuelin Zhang, Yanbo Yu, Yaoyu Wang, Yibo Zhu, Yimin Jiang, Yu Zhou, Yuanwei Lu, Houyi Li, Jingcheng Hu, Ka Man Lo, Ailin Huang, Binxing Jiao, Bo Li, Boyu Chen, Changxin Miao, Chang Lou, Chen Hu, Chen Xu, Chenfeng Yu, Chengyuan Yao, Daokuan Lv, Dapeng Shi, Deshan Sun, Ding Huang, Dingyuan Hu, Dongqing Pang, Enle Liu, Fajie Zhang, Fanqi Wan, Gulin Yan, Han Zhang, Han Zhou, Hanghao Wu, Hangyu Guo, Hanqi Chen, Hanshan Zhang, Hao Wu, Haocheng Zhang, Haolong Yan, Haoran Lv, Haoran Wei, Hebin Zhou, Heng Wang, Heng Wang, Hongxin Li, Hongyu Zhou, Hongyuan Wang, Huiyong Guo, Jia Wang, Jiahao Gong, Jialing Xie, Jian Zhou, Jianjian Sun, Jiaoren Wu, Jiaran Zhang, Jiayu Liu, Jie Cheng, Jie Luo, Jie Yan, Jie Yang, Jieyi Hou, Jinguang Zhang, Jinlan Cao, Jisheng Yin, Junfeng Liu, Junhao Huang, Junzhe Lin, Kaijun Tan, Kaixiang Li, Kang An, Kangheng Lin, Kenkun Liu, Lei Yang, Liang Zhao, Liangyu Chen, Lieyu Shi, Liguo Tan, Lin Lin, Lin Zhang, Lina Chen, Liwen Huang, Liying Shi, Longlong Gu, Mei Chen', 'link': 'https://arxiv.org/abs/2507.19427', 'abstract': "Large language models (LLMs) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both KV cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324 in the same setup and sets a new Pareto frontier for LLM decoding.", 'abstract_zh': '大型语言模型（LLMs）在解码过程中的硬件效率较低，尤其是对于 largo 上下文推理任务。本文介绍了 Step-3，这是一种321B参数的VLM，通过硬件意识下的模型-系统协同设计来优化解码成本最小化。Step-3 在两个关键维度上进行了创新：（1）一种新颖的多矩阵因子化注意力（MFA）机制，显著减少了关键值缓存大小和计算量，同时保持高注意力表达性；（2）注意力-前馈网络分离（AFD），这是一种分布式推理系统，将注意力和前馈网络层解耦为专门的子系统。这种协同设计取得了前所未有的成本效率：Step-3 相比 DeepSeek-V3 和 Qwen3 MoE 235B 模型显著降低了理论解码成本，尤其是在更长的上下文环境中。Step-3 在每token激活38B参数的同时实现低成本，表明硬件对齐的注意力算术强度、MoE 稀疏性和AFD 对于成本效益至关重要。我们在其有利场景下与 DeepSeek-V3 进行了直接对比。在 Hopper GPU 上，我们的实现下在50ms TPOT SLA（4K 上下文，FP8，无MTP）下每个GPU的解码吞吐量可达4,039 tokens/s，高于 DeepSeek-V3 的2,324，并为LLM解码设定了新的帕累托前沿。', 'title_zh': '步进3是大而经济的：低成本解码的模型系统协同设计'}
{'arxiv_id': 'arXiv:2507.19399', 'title': 'Running in CIRCLE? A Simple Benchmark for LLM Code Interpreter Security', 'authors': 'Gabriel Chua', 'link': 'https://arxiv.org/abs/2507.19399', 'abstract': 'As large language models (LLMs) increasingly integrate native code interpreters, they enable powerful real-time execution capabilities, substantially expanding their utility. However, such integrations introduce potential system-level cybersecurity threats, fundamentally different from prompt-based vulnerabilities. To systematically evaluate these interpreter-specific risks, we propose CIRCLE (Code-Interpreter Resilience Check for LLM Exploits), a simple benchmark comprising 1,260 prompts targeting CPU, memory, and disk resource exhaustion. Each risk category includes explicitly malicious ("direct") and plausibly benign ("indirect") prompt variants. Our automated evaluation framework assesses not only whether LLMs refuse or generates risky code, but also executes the generated code within the interpreter environment to evaluate code correctness, simplifications made by the LLM to make the code safe, or execution timeouts. Evaluating 7 commercially available models from OpenAI and Google, we uncover significant and inconsistent vulnerabilities. For instance, evaluations show substantial disparities even within providers - OpenAI\'s o4-mini correctly refuses risky requests at 7.1%, notably higher rates compared to GPT-4.1 at 0.5%. Results particularly underscore that indirect, socially-engineered prompts substantially weaken model defenses. This highlights an urgent need for interpreter-specific cybersecurity benchmarks, dedicated mitigation tools (e.g., guardrails), and clear industry standards to guide safe and responsible deployment of LLM interpreter integrations. The benchmark dataset and evaluation code are publicly released to foster further research.', 'abstract_zh': '作为大型语言模型（LLMs）越来越多地集成本地代码解释器，它们能够实现强大的实时执行能力，大幅扩展了其应用范围。然而，这些集成引入了潜在的系统级网络安全威胁，与基于提示的漏洞本质上不同。为系统地评估这些解释器特定的风险，我们提出了CIRCLE（代码解释器抗利用韧性检查），这是一个包含1260个针对CPU、内存和磁盘资源耗尽的提示的简单基准。每个风险类别包括明确恶意的（直接）和合理的无害的（间接）提示变体。我们自动化评估框架不仅评估LLMs是否拒绝或生成风险代码，还执行生成的代码以评估代码正确性、LLMs为使代码安全所做的简化或执行超时情况。评估来自OpenAI和Google的7种商用模型，我们发现了显著且不一致的漏洞。例如，评估结果显示，在同一个提供商内也存在显著差异——OpenAI的o4-mini在拒绝风险请求方面正确率达到了7.1%，远高于GPT-4.1的0.5%。结果特别强调，间接的社会工程化提示极大地削弱了模型的防御能力。这突显出对特定于解释器的网络安全基准、专用缓解工具（例如护栏）和明确的行业标准的迫切需求，以指导LLM解释器集成的安全和负责任部署。基准数据集和评估代码已公开发布，以促进进一步的研究。', 'title_zh': '在CIRCLE中运行？一个简单的LLM代码解释器安全性基准测试'}
{'arxiv_id': 'arXiv:2507.19390', 'title': 'ReCatcher: Towards LLMs Regression Testing for Code Generation', 'authors': 'Altaf Allah Abbassi, Leuson Da Silva, Amin Nikanjam, Foutse Khomh', 'link': 'https://arxiv.org/abs/2507.19390', 'abstract': 'Large Language Models (LLMs) for code generation evolve rapidly through fine-tuning, merging, or new model releases. However, such updates can introduce regressions, not only in correctness but also in code quality and performance. To address this, we present ReCatcher, a regression testing framework for Python code generation. ReCatcher systematically compares two LLMs, typically a current model and a candidate update, across three dimensions: logical correctness, static code quality, and execution performance. We apply ReCatcher to assess regressions across three update scenarios, fine-tuning, merging, and model release, using CodeLlama, DeepSeek-Coder, and GPT-4o. Our evaluation shows that fine-tuning with cross-language datasets increases syntax errors by up to 12%. Merging with general-purpose models like Llama2 leads to regressions in correctness by up to 18%. GPT-4o introduces regressions of up to 50% in handling missing imports compared to GPT-3.5-turbo, while GPT-4o-mini suffers up to 80% performance degradation in execution time versus GPT-4o. Overall, logical correctness, performance, and error handling (e.g., syntax errors and missing imports) are the most regression-prone areas. Comparing ReCatcher with baseline solutions, it presents better and consistent accuracy across logical and performance aspects. ReCatcher highlights the importance of systematic regression evaluation before adopting new models, while assisting researchers and practitioners in making more informed update decisions.', 'abstract_zh': '大型语言模型（LLMs）在代码生成方面的回归测试框架：ReCatcher', 'title_zh': 'ReCatcher: 朝向代码生成的大语言模型回归测试'}
{'arxiv_id': 'arXiv:2507.19361', 'title': 'SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice Understanding Large Language Models', 'authors': 'Zhen Wan, Chao-Han Huck Yang, Yahan Yu, Jinchuan Tian, Sheng Li, Ke Hu, Zhehuai Chen, Shinji Watanabe, Fei Cheng, Chenhui Chu, Sadao Kurohashi', 'link': 'https://arxiv.org/abs/2507.19361', 'abstract': "We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human cognition-inspired evaluation pipeline for voice understanding large language models, LLM Voice, designed to assess their voice understanding ability. Moving beyond popular voice understanding metrics such as word error rate (WER), SIQ examines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy: (1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e., similarity of LLM's interpretations); and (3) Application (i.e., QA accuracy for simulating downstream tasks). We demonstrate that SIQ not only quantifies voice understanding abilities but also provides unified comparisons between cascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation errors in existing benchmarks, and detects hallucinations in LLM Voice. Our framework represents a first-of-its-kind intelligence examination that bridges cognitive principles with voice-oriented benchmarks, while exposing overlooked challenges in multi-modal training.", 'abstract_zh': '基于语音的智能商 quotient (SIQ)：一种受人类认知启发的语音理解评估管道，用于评估大语言模型 LLM Voice 的语音理解能力', 'title_zh': 'SpeechIQ：语音理解大语言模型 Across 不同认知层次的语音智能商'}
{'arxiv_id': 'arXiv:2507.19353', 'title': 'Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM on Long-Context Tasks', 'authors': 'Kai Liu, Zhan Su, Peijie Dong, Fengran Mo, Jianfei Gao, ShaoTing Zhang, Kai Chen', 'link': 'https://arxiv.org/abs/2507.19353', 'abstract': 'Recently, recurrent large language models (Recurrent LLMs) with linear computational complexity have re-emerged as efficient alternatives to self-attention-based LLMs (Self-Attention LLMs), which have quadratic complexity. However, Recurrent LLMs often underperform on long-context tasks due to their limited fixed-size memory. Previous research has primarily focused on enhancing the memory capacity of Recurrent LLMs through architectural innovations, but these approaches have not yet enabled Recurrent LLMs to match the performance of Self-Attention LLMs on long-context tasks. We argue that this limitation arises because processing the entire context at once is not well-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a chunk-wise inference method inspired by human reading strategies. Smooth Reading processes context in chunks and iteratively summarizes the contextual information, thereby reducing memory demands and making the approach more compatible with Recurrent LLMs. Our experimental results show that this method substantially narrows the performance gap between Recurrent and Self-Attention LLMs on long-context tasks, while preserving the efficiency advantages of Recurrent LLMs. Our Smooth Reading boosts SWA-3B-4k (a Recurrent LLM) from 5.68% lower to 3.61% higher performance than Self-Attention LLMs on LongBench. Besides, our method maintains the high efficiency, training 3x faster and inferring 2x faster at 64k context compared to Self-Attention LLMs. To our knowledge, this is the first work to achieve comparable performance using Recurrent LLMs compared with Self-Attention LLMs on long-context tasks. We hope our method will inspire future research in this area. To facilitate further progress, we will release code and dataset.', 'abstract_zh': '最近，具有线性计算复杂度的递归大规模语言模型（递归LLMs）重新成为自注意力机制的大规模语言模型（自注意力LLMs）的有效替代，自注意力LLMs具有二次复杂度。然而，递归LLMs在长上下文任务上通常表现不佳，因为它们具有有限的固定大小内存。之前的研究主要集中在通过架构创新增强递归LLMs的内存容量上，但这些方法仍未使递归LLMs在长上下文任务上的性能达到自注意力LLMs的水平。我们认为，这一限制是因为一次性处理整个上下文不适合递归LLMs。在本文中，我们提出了一种基于分块推断的平滑阅读方法（Smooth Reading），该方法受到人类阅读策略的启发。平滑阅读逐块处理上下文并迭代总结上下文信息，从而降低内存需求，并使该方法更符合递归LLMs的特点。实验结果表明，这种方法在长上下文任务上显著缩小了递归LLMs与自注意力LLMs之间的性能差距，同时保持了递归LLMs的高效性优势。我们的平滑阅读方法将SWA-3B-4k（一个递归LLMs）的性能从相对于自注意力LLMs低5.68%提升到高3.61%。此外，我们的方法保持了高效率，相较于自注意力LLMs，在64k上下文时训练速度快3倍，推理速度快2倍。据我们所知，这是首次在长上下文任务上使递归LLMs的性能与自注意力LLMs接近的研究工作。我们希望我们的方法能够激励该领域的未来研究。为了促进进一步的研究，我们将发布代码和数据集。', 'title_zh': '平滑阅读：连接循环LLM与自注意力LLM在长期上下文任务中的差距'}
{'arxiv_id': 'arXiv:2507.19334', 'title': 'Doubling Your Data in Minutes: Ultra-fast Tabular Data Generation via LLM-Induced Dependency Graphs', 'authors': 'Shuo Yang, Zheyu Zhang, Bardh Prenkaj, Gjergji Kasneci', 'link': 'https://arxiv.org/abs/2507.19334', 'abstract': 'Tabular data is critical across diverse domains, yet high-quality datasets remain scarce due to privacy concerns and the cost of collection. Contemporary approaches adopt large language models (LLMs) for tabular augmentation, but exhibit two major limitations: (1) dense dependency modeling among tabular features that can introduce bias, and (2) high computational overhead in sampling. To address these issues, we propose SPADA for SPArse Dependency-driven Augmentation, a lightweight generative framework that explicitly captures sparse dependencies via an LLM-induced graph. We treat each feature as a node and synthesize values by traversing the graph, conditioning each feature solely on its parent nodes. We explore two synthesis strategies: a non-parametric method using Gaussian kernel density estimation, and a conditional normalizing flow model that learns invertible mappings for conditional density estimation. Experiments on four datasets show that SPADA reduces constraint violations by 4% compared to diffusion-based methods and accelerates generation by nearly 9,500 times over LLM-based baselines.', 'abstract_zh': '基于稀疏依赖驱动的数据增强：SPADAmissive', 'title_zh': '几分钟内翻倍你的数据：通过LLM诱导的依赖图实现超快速表格数据生成'}
{'arxiv_id': 'arXiv:2507.19283', 'title': 'Towards LLM-Enhanced Group Recommender Systems', 'authors': 'Sebastian Lubos, Alexander Felfernig, Thi Ngoc Trang Tran, Viet-Man Le, Damian Garber, Manuel Henrich, Reinhard Willfort, Jeremias Fuchs', 'link': 'https://arxiv.org/abs/2507.19283', 'abstract': 'In contrast to single-user recommender systems, group recommender systems are designed to generate and explain recommendations for groups. This group-oriented setting introduces additional complexities, as several factors - absent in individual contexts - must be addressed. These include understanding group dynamics (e.g., social dependencies within the group), defining effective decision-making processes, ensuring that recommendations are suitable for all group members, and providing group-level explanations as well as explanations for individual users. In this paper, we analyze in which way large language models (LLMs) can support these aspects and help to increase the overall decision support quality and applicability of group recommender systems.', 'abstract_zh': '与单用户推荐系统相比，群体推荐系统旨在为群体生成并解释推荐，这种以群体为导向的设置引入了额外的复杂性，需要解决多个在个体背景下不存在的因素，如理解群体动态（例如，群体内的社会依赖关系）、定义有效的决策过程、确保推荐适合所有群体成员，并提供群体层面的解释以及个体用户的解释。在本文中，我们分析大语言模型（LLMs）如何支持这些方面，并有助于提高群体推荐系统的整体决策支持质量和适用性。', 'title_zh': '向基于LLM的群体推荐系统迈进'}
{'arxiv_id': 'arXiv:2507.19271', 'title': 'Fine-Tuning Multilingual Language Models for Code Review: An Empirical Study on Industrial C# Projects', 'authors': 'Igli Begolli, Meltem Aksoy, Daniel Neider', 'link': 'https://arxiv.org/abs/2507.19271', 'abstract': 'Code review is essential for maintaining software quality but often time-consuming and cognitively demanding, especially in industrial environments. Recent advancements in language models (LMs) have opened new avenues for automating core review tasks. This study presents the empirical evaluation of monolingual fine-tuning on the performance of open-source LMs across three key automated code review tasks: Code Change Quality Estimation, Review Comment Generation, and Code Refinement. We fine-tuned three distinct models, CodeReviewer, CodeLlama-7B, and DeepSeek-R1-Distill, on a C\\# specific dataset combining public benchmarks with industrial repositories. Our study investigates how different configurations of programming languages and natural languages in the training data affect LM performance, particularly in comment generation. Additionally, we benchmark the fine-tuned models against an automated software analysis tool (ASAT) and human reviewers to evaluate their practical utility in real-world settings. Our results show that monolingual fine-tuning improves model accuracy and relevance compared to multilingual baselines. While LMs can effectively support code review workflows, especially for routine or repetitive tasks, human reviewers remain superior in handling semantically complex or context-sensitive changes. Our findings highlight the importance of language alignment and task-specific adaptation in optimizing LMs for automated code review.', 'abstract_zh': '基于单语微调的开源语言模型在关键自动化代码审查任务中的 empirical 评估：Code Review细粒度任务中的单语微调研究', 'title_zh': '基于工业C#项目的大规模多语言模型微调：代码审查实证研究'}
{'arxiv_id': 'arXiv:2507.19247', 'title': 'A Markov Categorical Framework for Language Modeling', 'authors': 'Yifan Zhang', 'link': 'https://arxiv.org/abs/2507.19247', 'abstract': "Auto-regressive language models factorize sequence probabilities and are trained by minimizing the negative log-likelihood (NLL) objective. While empirically powerful, a deep theoretical understanding of why this simple objective yields such versatile representations remains elusive. This work introduces a unifying analytical framework using Markov Categories (MCs) to deconstruct the AR generation process and the NLL objective. We model the single-step generation map as a composition of Markov kernels in the category Stoch. This compositional view, when enriched with statistical divergences, allows us to dissect information flow and learned geometry. Our framework makes three main contributions. First, we provide a formal, information-theoretic rationale for the success of modern speculative decoding methods like EAGLE, quantifying the information surplus in hidden states that these methods exploit. Second, we formalize how NLL minimization forces the model to learn not just the next token, but the data's intrinsic conditional stochasticity, a process we analyze using categorical entropy. Third, and most centrally, we prove that NLL training acts as an implicit form of spectral contrastive learning. By analyzing the information geometry of the model's prediction head, we show that NLL implicitly forces the learned representation space to align with the eigenspectrum of a predictive similarity operator, thereby learning a geometrically structured space without explicit contrastive pairs. This compositional and information-geometric perspective reveals the deep structural principles underlying the effectiveness of modern LMs. Project Page: this https URL", 'abstract_zh': '自动回归语言模型分解序列概率，并通过最小化负对数似然（NLL）目标进行训练。尽管在实践中表现出色，但这种简单目标为何能产生如此多样的表示形式的深层理论理解仍然缺乏。本文引入了一个统一的分析框架，使用马尔科夫范畴（MCs）分解自动回归生成过程和NLL目标。我们将单步生成映射建模为Stoch范畴中马尔科夫核的组合。这种组合视角，在统计散度增强后，使得我们可以剖析信息流动和学习到的几何结构。我们的框架提供了三个主要贡献。首先，我们为现代投机性解码方法（如EAGLE）的成功提供了形式化的信息论依据，量化了这些方法所利用的隐藏状态中的信息盈余。其次，我们形式化了如何通过最小化NLL迫使模型不仅学习下一个标记，还学习数据的内在条件随机性，我们使用范畴熵对此过程进行了分析。第三，也是最核心的一点，我们证明NLL训练实际上是一种隐式的光谱对比学习形式。通过分析模型预测头的信息几何结构，我们展示了NLL隐式地迫使学习到的表征空间与预测相似性操作的特征谱对齐，从而学习一个几何结构化的空间，而无需显式的对比学习对。这种组合和信息几何视角揭示了现代语言模型有效性的深层结构原则。项目页面：这个 https URL', 'title_zh': '马尔可夫范畴框架语言建模'}
{'arxiv_id': 'arXiv:2507.19195', 'title': 'Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?', 'authors': 'Chaymaa Abbas, Mariette Awad, Razane Tajeddine', 'link': 'https://arxiv.org/abs/2507.19195', 'abstract': 'Despite the ongoing improvements in the design of large language models (LLMs) to foster inclusion and balanced responses, these systems remain susceptible to encoding and amplifying social biases. This study examines how dialectal variation, specifically African American Vernacular English (AAVE) versus Standard American English (SAE), interacts with data poisoning to influence toxicity in outputs. Using both small- and medium-scale LLaMA models, we show that even minimal exposure to poisoned data significantly increases toxicity for AAVE inputs, while it remains comparatively unaffected for SAE. Larger models exhibit a more significant amplification effect which suggests heightened susceptibility with scale. To further assess these disparities, we employed GPT-4o as a fairness auditor, which identified harmful stereotypical patterns disproportionately tied to AAVE inputs, including portrayals of aggression, criminality, and intellectual inferiority. These findings underscore the compounding impact of data poisoning and dialectal bias and emphasize the need for dialect-aware evaluation, targeted debiasing interventions, and socially responsible training protocols during development.', 'abstract_zh': '尽管在设计大型语言模型（LLMs）以促进包容性和均衡响应方面取得了持续改进，这些系统仍然容易编码和放大社会偏见。本研究探讨了口音变体，特别是美式黑人方言（AAVE）与标准美式英语（SAE）在数据污染影响下的毒性输出差异。使用小型和中型规模的LLaMA模型，我们表明，即使是少量接触污染数据也会显著增加AAVE输入的毒性，而对SAE的影响相对较小。更大规模的模型表现出更显著的放大效应，这表明随规模增大，其易感性增加。为进一步评估这些差异，我们采用了GPT-4o作为公平性审计员，它识别出与AAVE输入不成比例关联的有害刻板印象模式，包括表现为攻击性、犯罪性和智力 inferiority。这些发现强调了数据污染和口音偏见累积影响的重要性，并强调了需要具备口音意识的评估、针对性的去偏措施和社会负责任的训练协议。', 'title_zh': '小规模数据毒性会加剧大规模语言模型中的方言关联偏见吗？'}
{'arxiv_id': 'arXiv:2507.19185', 'title': 'PrompTrend: Continuous Community-Driven Vulnerability Discovery and Assessment for Large Language Models', 'authors': 'Tarek Gasmi, Ramzi Guesmi, Mootez Aloui, Jihene Bennaceur', 'link': 'https://arxiv.org/abs/2507.19185', 'abstract': 'Static benchmarks fail to capture LLM vulnerabilities emerging through community experimentation in online forums. We present PrompTrend, a system that collects vulnerability data across platforms and evaluates them using multidimensional scoring, with an architecture designed for scalable monitoring. Cross-sectional analysis of 198 vulnerabilities collected from online communities over a five-month period (January-May 2025) and tested on nine commercial models reveals that advanced capabilities correlate with increased vulnerability in some architectures, psychological attacks significantly outperform technical exploits, and platform dynamics shape attack effectiveness with measurable model-specific patterns. The PrompTrend Vulnerability Assessment Framework achieves 78% classification accuracy while revealing limited cross-model transferability, demonstrating that effective LLM security requires comprehensive socio-technical monitoring beyond traditional periodic assessment. Our findings challenge the assumption that capability advancement improves security and establish community-driven psychological manipulation as the dominant threat vector for current language models.', 'abstract_zh': '静态基准无法捕捉通过在线论坛社区实验新兴的LLM漏洞。我们提出PrompTrend系统，该系统跨平台收集漏洞数据，并使用多维评分进行评估，具有可扩展监控架构。五个月内（2025年1月至5月）从在线社区收集的198个漏洞和九种商业模型测试结果显示，高级能力与某些架构中的漏洞增加相关，心理攻击显著优于技术exploits，平台动态塑造攻击效果，存在可衡量的模型特定模式。PrompTrend漏洞评估框架在分类准确性方面达到78%，同时揭示了有限的跨模型可移植性，表明有效的LLM安全需要超越传统周期性评估的全面社会技术监控。我们的研究结果挑战了能力提升改善安全性的假设，并确立了社区驱动的心理操纵作为当前语言模型主要威胁向量的地位。', 'title_zh': 'PrompTrend: 大型语言模型持续社区驱动的漏洞发现与评估'}
{'arxiv_id': 'arXiv:2507.19156', 'title': 'An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case', 'authors': 'Gioele Giachino, Marco Rondina, Antonio Vetrò, Riccardo Coppola, Juan Carlos De Martin', 'link': 'https://arxiv.org/abs/2507.19156', 'abstract': "The increasing use of Large Language Models (LLMs) in a large variety of domains has sparked worries about how easily they can perpetuate stereotypes and contribute to the generation of biased content. With a focus on gender and professional bias, this work examines in which manner LLMs shape responses to ungendered prompts, contributing to biased outputs. This analysis uses a structured experimental method, giving different prompts involving three different professional job combinations, which are also characterized by a hierarchical relationship. This study uses Italian, a language with extensive grammatical gender differences, to highlight potential limitations in current LLMs' ability to generate objective text in non-English languages. Two popular LLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google Gemini (gemini-1.5-flash). Through APIs, we collected a range of 3600 responses. The results highlight how content generated by LLMs can perpetuate stereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she' pronouns to the 'assistant' rather than the 'manager'. The presence of bias in AI-generated text can have significant implications in many fields, such as in the workplaces or in job selections, raising ethical concerns about its use. Understanding these risks is pivotal to developing mitigation strategies and assuring that AI-based systems do not increase social inequalities, but rather contribute to more equitable outcomes. Future research directions include expanding the study to additional chatbots or languages, refining prompt engineering methods or further exploiting a larger experimental base.", 'abstract_zh': '大型语言模型在多样化领域的广泛应用引发了对其容易延续刻板印象和生成有偏见内容的担忧。本文重点关注性别和职业偏见，研究大型语言模型如何通过反应无性别提示来塑造有偏见的输出。该分析采用结构化的实验方法，通过提供涉及三种不同职业职位组合的不同提示，其中这些职位组合也具有层级关系。本研究使用具有广泛语法性别差异的意大利语，以突出当前大型语言模型在非英语语言中生成客观文本方面的潜在限制。研究考察了两个流行的基于大型语言模型的聊天机器人，即OpenAI ChatGPT (gpt-4o-mini)和Google Gemini (gemini-1.5-flash)。通过API，我们收集了约3600个响应结果。研究结果表明，由大型语言模型生成的内容可以延续刻板印象。例如，Gemini有100%（ChatGPT为97%）的机会将“她”的代词与“助手”而非“经理”相关联。AI生成文本中的偏见可能在许多领域，如工作场所或职位选择中产生重大影响，从而引发对其使用的伦理担忧。理解这些风险对于开发缓解策略并确保基于人工智能的系统不增加社会不平等，而是贡献更公平的结果至关重要。未来的研究方向包括扩大研究范围至其他聊天机器人或语言、改进提示工程方法或进一步利用更大的实验基础。', 'title_zh': '大型语言模型中性别刻板印象表征的实证研究：意大利案例'}
{'arxiv_id': 'arXiv:2507.19144', 'title': 'Solar Photovoltaic Assessment with Large Language Model', 'authors': 'Muhao Guo, Yang Weng', 'link': 'https://arxiv.org/abs/2507.19144', 'abstract': 'Accurate detection and localization of solar photovoltaic (PV) panels in satellite imagery is essential for optimizing microgrids and active distribution networks (ADNs), which are critical components of renewable energy systems. Existing methods lack transparency regarding their underlying algorithms or training datasets, rely on large, high-quality PV training data, and struggle to generalize to new geographic regions or varied environmental conditions without extensive re-training. These limitations lead to inconsistent detection outcomes, hindering large-scale deployment and data-driven grid optimization. In this paper, we investigate how large language models (LLMs) can be leveraged to overcome these challenges. Despite their promise, LLMs face several challenges in solar panel detection, including difficulties with multi-step logical processes, inconsistent output formatting, frequent misclassification of visually similar objects (e.g., shadows, parking lots), and low accuracy in complex tasks such as spatial localization and quantification. To overcome these issues, we propose the PV Assessment with LLMs (PVAL) framework, which incorporates task decomposition for more efficient workflows, output standardization for consistent and scalable formatting, few-shot prompting to enhance classification accuracy, and fine-tuning using curated PV datasets with detailed annotations. PVAL ensures transparency, scalability, and adaptability across heterogeneous datasets while minimizing computational overhead. By combining open-source accessibility with robust methodologies, PVAL establishes an automated and reproducible pipeline for solar panel detection, paving the way for large-scale renewable energy integration and optimized grid management.', 'abstract_zh': '基于大型语言模型的太阳能光伏板检测与定位框架：提高微电网和主动配电网的优化能力', 'title_zh': '大规模语言模型评估太阳能光伏sistems'}
{'arxiv_id': 'arXiv:2507.19115', 'title': 'Automated Code Review Using Large Language Models at Ericsson: An Experience Report', 'authors': 'Shweta Ramesh, Joy Bose, Hamender Singh, A K Raghavan, Sujoy Roychowdhury, Giriprasad Sridhara, Nishrith Saini, Ricardo Britto', 'link': 'https://arxiv.org/abs/2507.19115', 'abstract': 'Code review is one of the primary means of assuring the quality of released software along with testing and static analysis. However, code review requires experienced developers who may not always have the time to perform an in-depth review of code. Thus, automating code review can help alleviate the cognitive burden on experienced software developers allowing them to focus on their primary activities of writing code to add new features and fix bugs. In this paper, we describe our experience in using Large Language Models towards automating the code review process in Ericsson. We describe the development of a lightweight tool using LLMs and static program analysis. We then describe our preliminary experiments with experienced developers in evaluating our code review tool and the encouraging results.', 'abstract_zh': '使用大型语言模型自动化代码审查过程的经验：在爱立信的应用', 'title_zh': '爱立信中使用大型语言模型进行自动化代码审查的经验报告'}
{'arxiv_id': 'arXiv:2507.19102', 'title': 'Distilling a Small Utility-Based Passage Selector to Enhance Retrieval-Augmented Generation', 'authors': 'Hengran Zhang, Keping Bi, Jiafeng Guo, Jiaming Zhang, Shuaiqiang Wang, Dawei Yin, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2507.19102', 'abstract': 'Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating retrieved information. Standard retrieval process prioritized relevance, focusing on topical alignment between queries and passages. In contrast, in RAG, the emphasis has shifted to utility, which considers the usefulness of passages for generating accurate answers. Despite empirical evidence showing the benefits of utility-based retrieval in RAG, the high computational cost of using LLMs for utility judgments limits the number of passages evaluated. This restriction is problematic for complex queries requiring extensive information. To address this, we propose a method to distill the utility judgment capabilities of LLMs into smaller, more efficient models. Our approach focuses on utility-based selection rather than ranking, enabling dynamic passage selection tailored to specific queries without the need for fixed thresholds. We train student models to learn pseudo-answer generation and utility judgments from teacher LLMs, using a sliding window method that dynamically selects useful passages. Our experiments demonstrate that utility-based selection provides a flexible and cost-effective solution for RAG, significantly reducing computational costs while improving answer quality. We present the distillation results using Qwen3-32B as the teacher model for both relevance ranking and utility-based selection, distilled into RankQwen1.7B and UtilityQwen1.7B. Our findings indicate that for complex questions, utility-based selection is more effective than relevance ranking in enhancing answer generation performance. We will release the relevance ranking and utility-based selection annotations for the MS MARCO dataset, supporting further research in this area.', 'abstract_zh': '基于检索的生成（RAG）通过引入检索信息增强了大规模语言模型（LLMs）。标准的检索过程侧重于相关性，关注查询和段落之间的主题对齐。相比之下，在RAG中，重点已经转向了实用性，考虑的是段落对于生成准确答案的有用性。尽管实证证据表明基于实用性检索在RAG中的优势，但在LLMs上进行实用性判断的高计算成本限制了评估的段落数量。这种限制对于需要大量信息的复杂查询来说是个问题。为了解决这一问题，我们提出了一种方法，将LLMs的实用性判断能力提炼到更小、更高效的模型中。我们的方法侧重于实用性选择而不是排序，能够根据特定查询动态选择有用的段落，无需固定阈值。我们训练学生模型从教师LLMs中学习伪答案生成和实用性判断，并使用滑动窗口方法动态选择有用的段落。我们的实验表明，基于实用性的选择为RAG提供了一种灵活且经济高效的解决方案，显著降低了计算成本并提高了答案质量。我们使用Qwen3-32B作为教师模型，进行了相关排序和基于实用性的选择的提炼，分别得到RankQwen1.7B和UtilityQwen1.7B。我们的研究结果表明，对于复杂问题，基于实用性的选择比基于相关性的排序更能提高答案生成性能。我们将发布MS MARCO数据集的相关排序和基于实用性的选择标注，以支持该领域的进一步研究。', 'title_zh': '基于实用主义的片段选择模型凝缩以增强检索增强生成'}
{'arxiv_id': 'arXiv:2507.18973', 'title': 'A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with Multi-Tool Aggregation', 'authors': 'Bohan Yao, Vikas Yadav', 'link': 'https://arxiv.org/abs/2507.18973', 'abstract': 'Augmenting large language models (LLMs) with external tools is a promising avenue for developing high-performance mathematical reasoning systems. Prior tool-augmented approaches typically finetune an LLM to select and invoke a single tool at each reasoning step and show promising results on simpler math reasoning benchmarks such as GSM8K. However, these approaches struggle with more complex math problems that require precise reasoning over multiple steps. To address this limitation, in this work, we propose Multi-TAG, a Multi-Tool AGgregation-based framework. Instead of relying on a single tool, Multi-TAG guides an LLM to concurrently invoke multiple tools at each reasoning step. It then aggregates their diverse outputs to verify and refine the reasoning process, enhancing solution robustness and accuracy. Notably, Multi-TAG is a finetuning-free, inference-only framework, making it readily applicable to any LLM backbone, including large open-weight models which are computationally expensive to finetune and proprietary frontier models which cannot be finetuned with custom recipes. We evaluate Multi-TAG on four challenging benchmarks: MATH500, AIME, AMC, and OlympiadBench. Across both open-weight and closed-source LLM backbones, Multi-TAG consistently and substantially outperforms state-of-the-art baselines, achieving average improvements of 6.0% to 7.5% over state-of-the-art baselines.', 'abstract_zh': '增强大型语言模型（LLMs）与外部工具的结合是开发高性能数学推理系统的一个有前途的途径。在此工作中，我们提出了Multi-TAG，一种基于多工具聚合的框架。Multi-TAG 在每一步推理过程中指导LLM同时调用多个工具，然后汇总它们的多样化输出以验证和细化推理过程，从而增强解决方案的稳健性和准确性。值得注意的是，Multi-TAG 是一个不需要微调的仅推理框架，使其可以轻松应用于任何LLM基础模型，包括计算成本高昂且难以微调的大型开放权重模型以及不能使用自定义食谱进行微调的专有前沿模型。我们在四个具有挑战性的基准上评估了Multi-TAG：MATH500、AIME、AMC 和 OlympiadBench。在开放权重和闭源LLM基础模型上，Multi-TAG 一致且显著地超过了最先进的基线，平均改进幅度在6.0%到7.5%之间。', 'title_zh': '不是一根锤子，而是一个工具箱——Multi-TAG：基于多工具聚合的数学推理扩展'}
{'arxiv_id': 'arXiv:2507.18945', 'title': 'TreeReader: A Hierarchical Academic Paper Reader Powered by Language Models', 'authors': 'Zijian Zhang, Pan Chen, Fangshi Du, Runlong Ye, Oliver Huang, Michael Liut, Alán Aspuru-Guzik', 'link': 'https://arxiv.org/abs/2507.18945', 'abstract': "Efficiently navigating and understanding academic papers is crucial for scientific progress. Traditional linear formats like PDF and HTML can cause cognitive overload and obscure a paper's hierarchical structure, making it difficult to locate key information. While LLM-based chatbots offer summarization, they often lack nuanced understanding of specific sections, may produce unreliable information, and typically discard the document's navigational structure. Drawing insights from a formative study on academic reading practices, we introduce TreeReader, a novel language model-augmented paper reader. TreeReader decomposes papers into an interactive tree structure where each section is initially represented by an LLM-generated concise summary, with underlying details accessible on demand. This design allows users to quickly grasp core ideas, selectively explore sections of interest, and verify summaries against the source text. A user study was conducted to evaluate TreeReader's impact on reading efficiency and comprehension. TreeReader provides a more focused and efficient way to navigate and understand complex academic literature by bridging hierarchical summarization with interactive exploration.", 'abstract_zh': '高效导航和理解学术论文对于科学研究至关重要。传统的线性格式如PDF和HTML会导致认知负担过重并模糊论文的层级结构，使得定位关键信息变得困难。虽然基于LLM的聊天机器人可以提供摘要，但它们通常对特定部分的理解不够深入，可能会产生不可靠的信息，并且通常会丢弃文档的导航结构。借鉴对学术阅读实践的形成性研究，我们引入了TreeReader，这是一种新型的语言模型增强型论文阅读工具。TreeReader将论文分解为一个交互式树结构，每个部分最初由LLM生成的简洁摘要表示，详细内容可根据需要获取。该设计使得用户能够快速把握核心思想，选择性地探索感兴趣的部分，并对照原始文本验证摘要。我们进行了一项用户研究以评估TreeReader对阅读效率和理解度的影响。TreeReader通过将层级总结与互动探索相结合，提供了一种更聚焦和高效的方式来导航和理解复杂的学术文献。', 'title_zh': 'TreeReader: 一种基于语言模型的层次化学术论文阅读器'}
{'arxiv_id': 'arXiv:2507.18918', 'title': 'Uncovering Cross-Linguistic Disparities in LLMs using Sparse Autoencoders', 'authors': 'Richmond Sin Jing Xuan, Jalil Huseynov, Yang Zhang', 'link': 'https://arxiv.org/abs/2507.18918', 'abstract': 'Multilingual large language models (LLMs) exhibit strong cross-linguistic generalization, yet medium to low resource languages underperform on common benchmarks such as ARC-Challenge, MMLU, and HellaSwag. We analyze activation patterns in Gemma-2-2B across all 26 residual layers and 10 languages: Chinese (zh), Russian (ru), Spanish (es), Italian (it), medium to low resource languages including Indonesian (id), Catalan (ca), Marathi (mr), Malayalam (ml), and Hindi (hi), with English (en) as the reference. Using Sparse Autoencoders (SAEs), we reveal systematic disparities in activation patterns. Medium to low resource languages receive up to 26.27 percent lower activations in early layers, with a persistent gap of 19.89 percent in deeper layers. To address this, we apply activation-aware fine-tuning via Low-Rank Adaptation (LoRA), leading to substantial activation gains, such as 87.69 percent for Malayalam and 86.32 percent for Hindi, while maintaining English retention at approximately 91 percent. After fine-tuning, benchmark results show modest but consistent improvements, highlighting activation alignment as a key factor in enhancing multilingual LLM performance.', 'abstract_zh': '多语言大规模语言模型在跨语言泛化方面表现出色，但中低资源语言在ARC-Challenge、MMLU和HellaSwag等常见基准测试中表现不佳。我们通过分析Gemma-2-2B在所有26个残差层和10种语言（中文、俄语、西班牙语、意大利语，以及包括印尼语、加泰罗尼亚语、马拉地语、马拉雅拉姆语和印地语在内的中低资源语言）中的激活模式，揭示了系统性的差异。中低资源语言在早期层中接收到的激活值最多低26.27%，在更深的层中则持续保持19.89%的差距。为解决这一问题，我们通过低秩适应（LoRA）进行激活感知微调，从而显著提高激活量，如马拉雅拉姆语提升87.69%，印地语提升86.32%，同时保持英语保留约91%。经过微调后，基准测试结果显示有适度但一致的改进，突出了激活对齐在增强多语言LLM性能中的关键作用。', 'title_zh': '使用稀疏自编码器揭示跨语言差异的大型语言模型分析'}
{'arxiv_id': 'arXiv:2507.18857', 'title': 'PrismRAG: Boosting RAG Factuality with Distractor Resilience and Strategized Reasoning', 'authors': 'Mohammad Kachuee, Teja Gollapudi, Minseok Kim, Yin Huang, Kai Sun, Xiao Yang, Jiaqi Wang, Nirav Shah, Yue Liu, Aaron Colak, Anuj Kumar, Wen-tau Yih, Xin Luna Dong', 'link': 'https://arxiv.org/abs/2507.18857', 'abstract': 'Retrieval-augmented generation (RAG) often falls short when retrieved context includes confusing semi-relevant passages, or when answering questions require deep contextual understanding and reasoning. We propose an efficient fine-tuning framework, called PrismRAG, that (i) trains the model with distractor-aware QA pairs mixing gold evidence with subtle distractor passages, and (ii) instills reasoning-centric habits that make the LLM plan, rationalize, and synthesize without relying on extensive human engineered instructions. Evaluated across 12 open-book RAG QA benchmarks spanning diverse application domains and scenarios, PrismRAG improves average factuality by 5.4%, outperforming state-of-the-art solutions.', 'abstract_zh': 'PrismRAG：一种高效的细调框架，用于增强生成模型在包含混淆半相关段落的检索结果中的表现，并促进基于推理的习惯以提高事实准确性', 'title_zh': 'PrismRAG: 提升RAG事实性的干扰物抗性和策略化推理'}
{'arxiv_id': 'arXiv:2507.18812', 'title': 'MemoCoder: Automated Function Synthesis using LLM-Supported Agents', 'authors': 'Yiping Jia, Zhen Ming Jiang, Shayan Noei, Ying Zou', 'link': 'https://arxiv.org/abs/2507.18812', 'abstract': 'With the widespread adoption of Large Language Models (LLMs) such as GitHub Copilot and ChatGPT, developers increasingly rely on AI-assisted tools to support code generation. While LLMs can generate syntactically correct solutions for well-structured programming tasks, they often struggle with challenges that require iterative debugging, error handling, or adaptation to diverse problem structures. Existing approaches such as fine-tuning or self-repair strategies either require costly retraining or lack mechanisms to accumulate and reuse knowledge from previous attempts.\nTo address these limitations, we propose MemoCoder, a multi-agent framework that enables collaborative problem solving and persistent learning from past fixes. At the core of MemoCoder is a Fixing Knowledge Set, which stores successful repairs and supports retrieval for future tasks. A central Mentor Agent supervises the repair process by identifying recurring error patterns and refining high-level fixing strategies, providing a novel supervisory role that guides the self-repair loop. We evaluate MemoCoder across three public benchmarks -- MBPP, HumanEval, and LiveCodeBench -- spanning a range of problem complexities. Experimental results show that MemoCoder consistently outperforms both zero-shot prompting and a Self-Repair strategy, with improvements ranging from 3.1% to 12.1% in Pass@10 and from 1.4% to 14.5% in Pass@50, demonstrating its effectiveness in iterative refinement and knowledge-guided code generation.', 'abstract_zh': '基于多智能体的笔记代码器：迭代调试与知识积累支持的代码生成', 'title_zh': 'MemoCoder: 使用LLM支持的代理进行自动函数合成'}
{'arxiv_id': 'arXiv:2507.18802', 'title': 'DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition', 'authors': 'Danqing Shi, Furui Cheng, Tino Weinkauf, Antti Oulasvirta, Mennatallah El-Assady', 'link': 'https://arxiv.org/abs/2507.18802', 'abstract': 'Human preferences are widely used to align large language models (LLMs) through methods such as reinforcement learning from human feedback (RLHF). However, the current user interfaces require annotators to compare text paragraphs, which is cognitively challenging when the texts are long or unfamiliar. This paper contributes by studying the decomposition principle as an approach to improving the quality of human feedback for LLM alignment. This approach breaks down the text into individual claims instead of directly comparing two long-form text responses. Based on the principle, we build a novel user interface DxHF. It enhances the comparison process by showing decomposed claims, visually encoding the relevance of claims to the conversation and linking similar claims. This allows users to skim through key information and identify differences for better and quicker judgment. Our technical evaluation shows evidence that decomposition generally improves feedback accuracy regarding the ground truth, particularly for users with uncertainty. A crowdsourcing study with 160 participants indicates that using DxHF improves feedback accuracy by an average of 5%, although it increases the average feedback time by 18 seconds. Notably, accuracy is significantly higher in situations where users have less certainty. The finding of the study highlights the potential of HCI as an effective method for improving human-AI alignment.', 'abstract_zh': '人类偏好分解原则在改善大型语言模型对齐中的应用：一种新型用户界面DxHF的研究', 'title_zh': 'DxHF: 通过交互分解提供高质量的人工反馈以实现LLM对齐'}
{'arxiv_id': 'arXiv:2507.18755', 'title': 'Agentic Program Repair from Test Failures at Scale: A Neuro-symbolic approach with static analysis and test execution feedback', 'authors': 'Chandra Maddila, Adam Tait, Claire Chang, Daniel Cheng, Nauman Ahmad, Vijayaraghavan Murali, Marshall Roch, Arnaud Avondet, Aaron Meltzer, Victor Montalvao, Michael Hopko, Chris Waterson, Parth Thakkar, Renuka Fernandez, Kristian Kristensen, Sivan Barzily, Sherry Chen, Rui Abreu, Nachiappan Nagappan, Payam Shodjai, Killian Murphy, James Everingham, Aparna Ramani, Peter C. Rigby', 'link': 'https://arxiv.org/abs/2507.18755', 'abstract': "Aim: With the advent of LLMs, sophisticated agentic program repair has become viable at large organizations with large codebases. In this work, we develop an Engineering Agent that fixes the source code based on test failures at scale across diverse software offerings internally.\nMethod: Using Llama as the base, we employ the ReAct harness to develop an agent. We start with a test failure that was triaged by a rule-based test failure bot. We then set up an agentic harness and allow the agent to reason and run a set of 15 actions from reading a file to generating a patch. We provide feedback to the agent through static analysis and test failures so it can refine its solution. We leverage an LLM-as-a-Judge to ensure that the patch conforms to the standards followed by a human review to land fixes.\nBenchmark Findings: We curated offline benchmarks for our patch generator, the Engineering Agent loop, and the LLM-as-a-Judge. In offline evaluations we found that a specialized 70B model is highly competitive with the much larger but vanilla Llama-405B. In an ablation study, we found that the ReAct harness (neural model) benefited from the symbolic information from static analysis tools and test execution traces. A model that strikes a balance between the solve rate and error rate vs the cost and latency has a benchmark solve rate of 42.3% using an average 11.8 feedback iterations.\nProduction Findings: In a three month period, 80% of the generated fixes were reviewed, of which 31.5% were landed (25.5% of the total number of generated fixes).\nFeedback from Engineers: We used open coding to extract qualitative themes from engineers' feedback. We saw positive feedback in the form of quick approvals, gratitude, and surprise. We also found mixed feedback when the Engineering Agent's solution was partially correct and it served as a good starting point.", 'abstract_zh': '目标：随着大规模语言模型（LLM）的出现，复杂的代理程序修复在大型代码库的大型组织中变得可行。在本研究中，我们开发了一个工程代理，该代理基于测试失败在多样化的软件产品内部大规模修复源代码。\n方法：以Llama为基础，我们使用ReAct框架开发了一个代理。我们从一个由基于规则的测试失败机器人处理的测试失败开始。然后，我们设置了一个代理框架，允许代理进行推理并运行从读取文件到生成补丁的一系列15个操作。我们通过静态分析和测试失败向代理提供反馈，以便其优化解决方案。我们利用LLM作为裁判来确保补丁符合人工审查的标准，以实现修复。\n基准发现：我们为补丁生成器、工程代理循环和LLM作为裁判制定了离线基准。在离线评估中，我们发现一个专门的70B模型与更大但未经过特殊训练的Llama-405B具有很高的竞争力。在消融研究中，我们发现ReAct框架（神经模型）受益于静态分析工具的符号信息和测试执行记录。具有平衡解决率和错误率与成本和延迟的模型，在平均11.8次反馈迭代的情况下，基准解决率为42.3%。\n生产发现：在三个月期间，生成的修复中有80%被审查，其中31.5%被纳入（占总生成修复的25.5%）。\n工程师反馈：我们使用开放编码从工程师的反馈中提取定性主题。我们发现快速批准、感激和惊讶的积极反馈。当工程代理的解决方案部分正确时，我们还发现了混合反馈，它作为一个好的起点是有益的。', 'title_zh': '大规模基于测试失败的代理程序修复：一种结合静态分析和测试执行反馈的神经符号方法'}
{'arxiv_id': 'arXiv:2507.18742', 'title': 'Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement', 'authors': 'Víctor Gallego', 'link': 'https://arxiv.org/abs/2507.18742', 'abstract': "Language models (LMs) are susceptible to in-context reward hacking, where they exploit flaws in tainted or faulty written specifications or rubrics to achieve high scores without fulfilling the user's true intent. We introduce Specification Self-Correction (SSC), a novel, test-time framework that enables an LM to identify and correct flaws within its own guiding specification. SSC employs a multi-step inference process where the model first generates a response based on a potentially tainted specification, critiques its output, and then revises the specification itself to remove the exploitable loophole. A final, more robust response is then generated using this self-corrected specification. Across experiments spanning creative writing and agentic coding tasks with several LMs, we demonstrate that while models initially game tainted specifications in 50-70\\% of cases, the SSC process reduces this vulnerability by over 90\\%. This dynamic repair occurs at inference time, requires no weight modification, and leads to more robustly aligned model behavior. Code at this https URL .", 'abstract_zh': '语言模型（LMs）易遭受上下文内奖励作弊，它们会利用污染或有缺陷的书面规范或评分标准中的漏洞，以高分的形式实现目标而不履行用户的真正意图。我们提出了规范自我修正（SSC）这一新颖的测试时框架，使LM能够识别并修正自身指导规范中的漏洞。SSC采用多步推断过程，模型首先基于可能被污染的规范生成回应，然后批评其输出，并修订规范本身以移除可利用的漏洞。最终，使用经过自我修正的规范生成更为健壯的回应。在涉及多种语言模型的创意写作和自主编程任务的实验中，我们展示了虽然模型在50-70%的情况下会利用污染的规范进行游戏，但SSC过程通过超过90%的漏洞降低显著提高了模型的行为一致性。此动态修复在推断时发生，无需修改权重，并导致更稳健的行为对齐。代码参见此链接：https://github.com/alibaba/Qwen-SSC。', 'title_zh': '规格自校正：通过测试时 refinement 减轻上下文奖励作弊'}
{'arxiv_id': 'arXiv:2507.18671', 'title': 'Innovator: Scientific Continued Pretraining with Fine-grained MoE Upcycling', 'authors': 'Ning Liao, Xiaoxing Wang, Zehao Lin, Weiyang Guo, Feng Hong, Shixiang Song, Geng Yu, Zihua Zhao, Sitao Xie, Longxuan Wei, Xiangqi Jin, Xiaohan Qin, Jiale Ma, Kai Chen, Jiangchao Yao, Zhouhan Lin, Junchi Yan, Zhiyu Li, Feiyu Xiong, Yanfeng Wang, Linfeng Zhang', 'link': 'https://arxiv.org/abs/2507.18671', 'abstract': 'A large language model (LLM) with knowledge in both scientific and general tasks is the foundation of science general intelligence. However, directly continued pretraining an LLM using science data usually leads to catastrophic forgetting, which indicates severe degradation in general ability. In this report, we present Innovator, which solves this problem by upcycling a pre-trained dense LLM into a fine-grained Mixtures-of-Experts model during continued pretraining, where different experts are expected to learn science knowledge in different disciplines, and a shared expert is utilized for general tasks. Innovator introduces a four-stage upcycle training paradigm: (1) Scientific Expert Induction on discipline-specific data, (2) Fine-grained Expert Splitting via FFN dimension decomposition, (3) Science-Aware Routing warmup, and (4) Generalist-Scientist Integration training on hybrid datasets. Such a paradigm enables knowledge in the general domain, and different scientific disciplines can be decoupled, avoiding the negative influence among knowledge in different domains. With 53.3B total parameters and 13.3B activated, Innovator extends Qwen2.5-7B using a shared general expert and 64 specialized scientific experts with 8 activated. Trained on 300B tokens with tri-level quality-controlled data, Innovator achieves 25% average improvement across 30 scientific tasks with a win rate as 70%, while retaining 99% performance in general tasks. Furthermore, Innovator-Reason, which is post-trained from Innovator for reasoning boosting, exhibits excellent reasoning performance in solving complex scientific problems with improvements over 30%.', 'abstract_zh': '一种大型语言模型（LLM）通过融合科学知识和通用任务知识来构建科学通用智能的基础。然而，直接使用科学数据继续预训练LLM通常会导致严重的泛化能力下降，即灾难性遗忘。本报告介绍了Innovator，该模型通过在继续预训练过程中将一个预先训练的密集LLM升级为细粒度的混合专家模型来解决此问题，在此过程中，不同的专家被期望在不同的学科中学习科学知识，而共享专家则用于通用任务。Innovator引入了一种四阶段升级训练范式：（1）学科特定数据的科学专家诱导；（2）通过FFN维度分解进行细粒度专家分割；（3）科学意识路由预热；（4）综合通用专家和专业科学专家的泛化-科学家集成训练。这种范式使得通用领域和不同科学学科的知识能够分离，避免了不同领域知识之间的负面影响。Innovator拥有总计533亿个参数，激活参数为133亿，通过共享一个通用专家和64个专业科学专家（8个激活）来扩展Qwen2.5-7B。在300亿个令牌的分级质量控制数据集上训练，Innovator在30项科学任务中实现了25%的平均改进和70%的胜率，同时保留了99%的通用任务性能。此外，用于提升推理能力的Innovator-Reason，在解决复杂科学问题方面表现出色，并实现了超过30%的改进。', 'title_zh': '创新者：科学的持续预训练与细粒度MoE升级'}
{'arxiv_id': 'arXiv:2507.18638', 'title': 'Prompt Engineering and the Effectiveness of Large Language Models in Enhancing Human Productivity', 'authors': 'Rizal Khoirul Anam', 'link': 'https://arxiv.org/abs/2507.18638', 'abstract': 'The widespread adoption of large language models (LLMs) such as ChatGPT, Gemini, and DeepSeek has significantly changed how people approach tasks in education, professional work, and creative domains. This paper investigates how the structure and clarity of user prompts impact the effectiveness and productivity of LLM outputs. Using data from 243 survey respondents across various academic and occupational backgrounds, we analyze AI usage habits, prompting strategies, and user satisfaction. The results show that users who employ clear, structured, and context-aware prompts report higher task efficiency and better outcomes. These findings emphasize the essential role of prompt engineering in maximizing the value of generative AI and provide practical implications for its everyday use.', 'abstract_zh': '大语言模型（LLM）如ChatGPT、Gemini和DeepSeek的广泛应用已显著改变了人们在教育、职业工作和创造性领域中的任务处理方式。本文研究了用户提示的结构和清晰度如何影响LLM输出的有效性和生产力。通过分析来自各个学术和职业背景的243名调查受访者的数据，我们探讨了AI使用习惯、提示策略和用户满意度。研究结果表明，使用清晰、结构化且上下文相关的提示的用户报告更高的任务效率和更好的成果。这些发现强调了提示工程在最大化生成式AI价值中的重要作用，并提供了其实用建议。', 'title_zh': 'Prompt工程及其对增强人类生产力的大规模语言模型效果研究'}
