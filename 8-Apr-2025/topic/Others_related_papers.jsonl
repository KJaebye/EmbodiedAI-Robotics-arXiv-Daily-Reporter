{'arxiv_id': 'arXiv:2504.04991', 'title': 'Wavelet Policy: Imitation Policy Learning in Frequency Domain with Wavelet Transforms', 'authors': 'Changchuan Yang, Yuhang Dong, Guanzhong Tian, Haizhou Ge, Hongrui Zhu', 'link': 'https://arxiv.org/abs/2504.04991', 'abstract': 'Recent imitation learning policies, often framed as time series prediction tasks, directly map robotic observations-such as high-dimensional visual data and proprioception-into the action space. While time series prediction primarily relies on spatial domain modeling, the underutilization of frequency domain analysis in robotic manipulation trajectory prediction may lead to neglecting the inherent temporal information embedded within action sequences. To address this, we reframe imitation learning policies through the lens of the frequency domain and introduce the Wavelet Policy. This novel approach employs wavelet transforms (WT) for feature preprocessing and extracts multi-scale features from the frequency domain using the SE2MD (Single Encoder to Multiple Decoder) architecture. Furthermore, to enhance feature mapping in the frequency domain and increase model capacity, we introduce a Learnable Frequency-Domain Filter (LFDF) after each frequency decoder, improving adaptability under different visual conditions. Our results show that the Wavelet Policy outperforms state-of-the-art (SOTA) end-to-end methods by over 10% on four challenging robotic arm tasks, while maintaining a comparable parameter count. In long-range settings, its performance declines more slowly as task volume increases. The code will be publicly available.', 'abstract_zh': '基于频域的imitation学习策略：Wavelet Policy及其应用', 'title_zh': '小波策略：基于小波变换的频域imitation策略学习'}
{'arxiv_id': 'arXiv:2504.04939', 'title': 'A Taxonomy of Self-Handover', 'authors': 'Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi', 'link': 'https://arxiv.org/abs/2504.04939', 'abstract': "Self-handover, transferring an object between one's own hands, is a common but understudied bimanual action. While it facilitates seamless transitions in complex tasks, the strategies underlying its execution remain largely unexplored. Here, we introduce the first systematic taxonomy of self-handover, derived from manual annotation of over 12 hours of cooking activity performed by 21 participants. Our analysis reveals that self-handover is not merely a passive transition, but a highly coordinated action involving anticipatory adjustments by both hands. As a step toward automated analysis of human manipulation, we further demonstrate the feasibility of classifying self-handover types using a state-of-the-art vision-language model. These findings offer fresh insights into bimanual coordination, underscoring the role of self-handover in enabling smooth task transitions-an ability essential for adaptive dual-arm robotics.", 'abstract_zh': '自我转换：在双手之间转移物体是一项常见但研究不足的双手法动作。通过系统分类和自动化分析的研究，揭示了自我转换不仅是被动的过渡，更是双手之间高度协调的动作，涉及预见性的调整。这些发现为理解和分类自我转换类型提供了新的视角，强调了自我转换在实现顺畅任务过渡中的作用，这对于适应型双臂机器人至关重要。', 'title_zh': '自我切换的分类学'}
{'arxiv_id': 'arXiv:2504.04868', 'title': 'On Scenario Formalisms for Automated Driving', 'authors': 'Christian Neurohr, Lukas Westhofen, Tjark Koopmann, Eike Möhlmann, Eckard Böde, Axel Hahn', 'link': 'https://arxiv.org/abs/2504.04868', 'abstract': 'The concept of scenario and its many qualifications -- specifically logical and abstract scenarios -- have emerged as a foundational element in safeguarding automated driving systems. However, the original linguistic definitions of the different scenario qualifications were often applied ambiguously, leading to a divergence between scenario description languages proposed or standardized in practice and their terminological foundation. This resulted in confusion about the unique features as well as strengths and weaknesses of logical and abstract scenarios. To alleviate this, we give clear linguistic definitions for the scenario qualifications concrete, logical, and abstract scenario and propose generic, unifying formalisms using curves, mappings to sets of curves, and temporal logics, respectively. We demonstrate that these formalisms allow pinpointing strengths and weaknesses precisely by comparing expressiveness, specification complexity, sampling, and monitoring of logical and abstract scenarios. Our work hence enables the practitioner to comprehend the different scenario qualifications and identify a suitable formalism.', 'abstract_zh': '场景及其多种限定概念——尤其是逻辑和抽象场景——已成为保障自动驾驶系统安全的基础元素。然而，不同场景限定概念的原初语言定义常常被模糊应用，导致实践中提出的或标准化的场景描述语言与其术语基础之间出现分歧。这造成了对逻辑和抽象场景的独特特征及其优势和劣势的混淆。为了解决这一问题，我们明确了具体的、逻辑的和抽象的场景的限定概念，并提出了分别基于曲线、曲线集合的映射和时序逻辑的一般统一形式化方法。我们展示了这些形式化方法可以通过比较表达能力、规范复杂性、抽样和监控逻辑和抽象场景的能力，精确指出其优势和劣势。因此，我们的工作使实践者能够理解不同的场景限定概念，并识别出合适的形式化方法。', 'title_zh': '面向自动驾驶的场景形式化方法研究'}
{'arxiv_id': 'arXiv:2504.04603', 'title': 'Diffusion-Based Approximate MPC: Fast and Consistent Imitation of Multi-Modal Action Distributions', 'authors': 'Pau Marquez Julbe, Julian Nubert, Henrik Hose, Sebastian Trimpe, Katherine J. Kuchenbecker', 'link': 'https://arxiv.org/abs/2504.04603', 'abstract': 'Approximating model predictive control (MPC) using imitation learning (IL) allows for fast control without solving expensive optimization problems online. However, methods that use neural networks in a simple L2-regression setup fail to approximate multi-modal (set-valued) solution distributions caused by local optima found by the numerical solver or non-convex constraints, such as obstacles, significantly limiting the applicability of approximate MPC in practice. We solve this issue by using diffusion models to accurately represent the complete solution distribution (i.e., all modes) at high control rates (more than 1000 Hz). This work shows that diffusion based AMPC significantly outperforms L2-regression-based approximate MPC for multi-modal action distributions. In contrast to most earlier work on IL, we also focus on running the diffusion-based controller at a higher rate and in joint space instead of end-effector space. Additionally, we propose the use of gradient guidance during the denoising process to consistently pick the same mode in closed loop to prevent switching between solutions. We propose using the cost and constraint satisfaction of the original MPC problem during parallel sampling of solutions from the diffusion model to pick a better mode online. We evaluate our method on the fast and accurate control of a 7-DoF robot manipulator both in simulation and on hardware deployed at 250 Hz, achieving a speedup of more than 70 times compared to solving the MPC problem online and also outperforming the numerical optimization (used for training) in success ratio.', 'abstract_zh': '使用扩散模型进行多模式近似模型预测控制', 'title_zh': '基于扩散的近似MPC：快速且一致地模仿多模态动作分布'}
{'arxiv_id': 'arXiv:2504.04421', 'title': 'Deliberate Planning of 3D Bin Packing on Packing Configuration Trees', 'authors': 'Hang Zhao, Juzhan Xu, Kexiong Yu, Ruizhen Hu, Chenyang Zhu, Kai Xu', 'link': 'https://arxiv.org/abs/2504.04421', 'abstract': 'Online 3D Bin Packing Problem (3D-BPP) has widespread applications in industrial automation. Existing methods usually solve the problem with limited resolution of spatial discretization, and/or cannot deal with complex practical constraints well. We propose to enhance the practical applicability of online 3D-BPP via learning on a novel hierarchical representation, packing configuration tree (PCT). PCT is a full-fledged description of the state and action space of bin packing which can support packing policy learning based on deep reinforcement learning (DRL). The size of the packing action space is proportional to the number of leaf nodes, making the DRL model easy to train and well-performing even with continuous solution space. We further discover the potential of PCT as tree-based planners in deliberately solving packing problems of industrial significance, including large-scale packing and different variations of BPP setting. A recursive packing method is proposed to decompose large-scale packing into smaller sub-trees while a spatial ensemble mechanism integrates local solutions into global. For different BPP variations with additional decision variables, such as lookahead, buffering, and offline packing, we propose a unified planning framework enabling out-of-the-box problem solving. Extensive evaluations demonstrate that our method outperforms existing online BPP baselines and is versatile in incorporating various practical constraints. The planning process excels across large-scale problems and diverse problem variations. We develop a real-world packing robot for industrial warehousing, with careful designs accounting for constrained placement and transportation stability. Our packing robot operates reliably and efficiently on unprotected pallets at 10 seconds per box. It achieves averagely 19 boxes per pallet with 57.4% space utilization for relatively large-size boxes.', 'abstract_zh': '在线3D容器打包问题（3D-BPP）在工业自动化中具有广泛的应用。现有的方法通常在有限的空间离散化分辨率下解决问题，或者无法很好地处理复杂的实际约束。我们提出了一种通过学习新提出的分层表示——打包配置树（PCT）来增强在线3D-BPP的实际适用性。PCT是对容器打包状态和动作空间的全面描述，可以支持基于深度强化学习（DRL）的打包策略学习。打包动作空间的大小与叶节点数量成正比，使DRL模型易于训练，即使在连续的解空间中也能表现出色。我们进一步发现PCT作为基于树的规划器的潜在能力，专门解决具有工业意义的打包问题，包括大规模打包和不同时效的BPP设置变体。提出了一种递归打包方法来将大规模打包分解为较小的子树，而空间ensemble机制将局部解集成到全局。对于不同的具有额外决策变量的BPP变体，如前瞻、缓存和离线打包，我们提出了一种统一的规划框架，使问题解决更加方便。广泛评估表明，我们的方法优于现有的在线BPP基线，并且能够灵活地整合各种实际约束。规划过程在大规模问题和多变的问题类型中表现出色。我们为工业仓储开发了一台实际应用的打包机器人，设计考虑了受限放置和运输稳定性。该打包机器人在无保护托盘上以每箱10秒的速度可靠且高效地运行，平均每托盘放置19箱，空间利用率为57.4%。', 'title_zh': '3D货物配置树上的故意规划装箱ترتيب التخطيط المقصود ل burial فيrees ثلاثي الأبعاد على نCarouselءات الترتيبثلاثي الأبعاد'}
{'arxiv_id': 'arXiv:2504.03989', 'title': 'CORTEX-AVD: CORner Case Testing & EXploration for Autonomous Vehicles Development', 'authors': 'Gabriel Shimanuki, Alexandre Nascimento, Lucio Vismari, Joao Camargo Jr, Jorge Almeida Jr, Paulo Cugnasca', 'link': 'https://arxiv.org/abs/2504.03989', 'abstract': "Autonomous Vehicles (AVs) aim to improve traffic safety and efficiency by reducing human error. However, ensuring AVs reliability and safety is a challenging task when rare, high-risk traffic scenarios are considered. These 'Corner Cases' (CC) scenarios, such as unexpected vehicle maneuvers or sudden pedestrian crossings, must be safely and reliable dealt by AVs during their operations. But they arehard to be efficiently generated. Traditional CC generation relies on costly and risky real-world data acquisition, limiting scalability, and slowing research and development progress. Simulation-based techniques also face challenges, as modeling diverse scenarios and capturing all possible CCs is complex and time-consuming. To address these limitations in CC generation, this research introduces CORTEX-AVD, CORner Case Testing & EXploration for Autonomous Vehicles Development, an open-source framework that integrates the CARLA Simulator and Scenic to automatically generate CC from textual descriptions, increasing the diversity and automation of scenario modeling. Genetic Algorithms (GA) are used to optimize the scenario parameters in six case study scenarios, increasing the occurrence of high-risk events. Unlike previous methods, CORTEX-AVD incorporates a multi-factor fitness function that considers variables such as distance, time, speed, and collision likelihood. Additionally, the study provides a benchmark for comparing GA-based CC generation methods, contributing to a more standardized evaluation of synthetic data generation and scenario assessment. Experimental results demonstrate that the CORTEX-AVD framework significantly increases CC incidence while reducing the proportion of wasted simulations.", 'abstract_zh': 'corners-case 测试与探索框架 CORTEX-AVD：自主车辆开发中的异常场景生成', 'title_zh': 'CORTEX-AVD: 角 case 测试与探索在自动驾驶车辆开发中的应用'}
{'arxiv_id': 'arXiv:2504.05223', 'title': 'Reducing the Communication of Distributed Model Predictive Control: Autoencoders and Formation Control', 'authors': 'Torben Schiz, Henrik Ebel', 'link': 'https://arxiv.org/abs/2504.05223', 'abstract': "Communication remains a key factor limiting the applicability of distributed model predictive control (DMPC) in realistic settings, despite advances in wireless communication. DMPC schemes can require an overwhelming amount of information exchange between agents as the amount of data depends on the length of the predication horizon, for which some applications require a significant length to formally guarantee nominal asymptotic stability. This work aims to provide an approach to reduce the communication effort of DMPC by reducing the size of the communicated data between agents. Using an autoencoder, the communicated data is reduced by the encoder part of the autoencoder prior to communication and reconstructed by the decoder part upon reception within the distributed optimization algorithm that constitutes the DMPC scheme. The choice of a learning-based reduction method is motivated by structure inherent to the data, which results from the data's connection to solutions of optimal control problems. The approach is implemented and tested at the example of formation control of differential-drive robots, which is challenging for optimization-based control due to the robots' nonholonomic constraints, and which is interesting due to the practical importance of mobile robotics. The applicability of the proposed approach is presented first in form of a simulative analysis showing that the resulting control performance yields a satisfactory accuracy. In particular, the proposed approach outperforms the canonical naive way to reduce communication by reducing the length of the prediction horizon. Moreover, it is shown that numerical experiments conducted on embedded computation hardware, with real distributed computation and wireless communication, work well with the proposed way of reducing communication even in practical scenarios in which full communication fails.", 'abstract_zh': '基于自编码器的数据压缩方法在分布式模型预测控制中的通信效率提升', 'title_zh': '减少分布式模型预测控制中的通信开销：自编码器与 formation 控制'}
{'arxiv_id': 'arXiv:2504.04638', 'title': 'Modeling, Translation, and Analysis of Different examples using Simulink, Stateflow, SpaceEx, and FlowStar', 'authors': 'Yogesh Gajula, Ravi Varma Lingala', 'link': 'https://arxiv.org/abs/2504.04638', 'abstract': 'This report details the translation and testing of multiple benchmarks, including the Six Vehicle Platoon, Two Bouncing Ball, Three Tank System, and Four-Dimensional Linear Switching, which represent continuous and hybrid systems. These benchmarks were gathered from past instances involving diverse verification tools such as SpaceEx, Flow*, HyST, MATLAB-Simulink, Stateflow, etc. They cover a range of systems modeled as hybrid automata, providing a comprehensive set for analysis and evaluation. Initially, we created models for all four systems using various suitable tools. Subsequently, these models were converted to the SpaceEx format and then translated into different formats compatible with various verification tools. Adapting our approach to the dynamic characteristics of each system, we performed reachability analysis using the respective verification tools.', 'abstract_zh': '本报告详细介绍了对六辆车队、两颗滚动球、三个坦克系统和四维线性切换等多个基准的翻译和测试，这些基准代表了连续和混合系统。这些基准来源于以往涉及多种验证工具（如SpaceEx、Flow*、HyST、MATLAB-Simulink、Stateflow等）的应用实例，涵盖了作为混合自动机建模的各类系统，提供了一套全面的分析和评估集。首先，我们使用各种合适的工具为所有四个系统创建了模型。随后，将这些模型转换为SpaceEx格式，并翻译成各种验证工具兼容的不同格式。根据每个系统的动态特性，我们使用相应的验证工具进行了可达性分析。', 'title_zh': 'Simulink、Stateflow、SpaceEx和FlowStar的不同示例建模、翻译与分析'}
{'arxiv_id': 'arXiv:2504.04605', 'title': 'Nonlinear Robust Optimization for Planning and Control', 'authors': 'Arshiya Taj Abdul, Augustinos D. Saravanos, Evangelos A. Theodorou', 'link': 'https://arxiv.org/abs/2504.04605', 'abstract': 'This paper presents a novel robust trajectory optimization method for constrained nonlinear dynamical systems subject to unknown bounded disturbances. In particular, we seek optimal control policies that remain robustly feasible with respect to all possible realizations of the disturbances within prescribed uncertainty sets. To address this problem, we introduce a bi-level optimization algorithm. The outer level employs a trust-region successive convexification approach which relies on linearizing the nonlinear dynamics and robust constraints. The inner level involves solving the resulting linearized robust optimization problems, for which we derive tractable convex reformulations and present an Augmented Lagrangian method for efficiently solving them. To further enhance the robustness of our methodology on nonlinear systems, we also illustrate that potential linearization errors can be effectively modeled as unknown disturbances as well. Simulation results verify the applicability of our approach in controlling nonlinear systems in a robust manner under unknown disturbances. The promise of effectively handling approximation errors in such successive linearization schemes from a robust optimization perspective is also highlighted.', 'abstract_zh': '一种用于受未知有界干扰约束非线性动力学系统的新型鲁棒轨迹优化方法', 'title_zh': '非线性鲁棒优化在规划与控制中的应用'}
{'arxiv_id': 'arXiv:2504.04508', 'title': 'The Mediating Effects of Emotions on Trust through Risk Perception and System Performance in Automated Driving', 'authors': 'Lilit Avetisyan, Emmanuel Abolarin, Vanik Zakarian, X. Jessie Yang, Feng Zhou', 'link': 'https://arxiv.org/abs/2504.04508', 'abstract': 'Trust in automated vehicles (AVs) has traditionally been explored through a cognitive lens, but growing evidence highlights the significant role emotions play in shaping trust. This study investigates how risk perception and AV performance (error vs. no error) influence emotional responses and trust in AVs, using mediation analysis to examine the indirect effects of emotions. In this study, 70 participants (42 male, 28 female) watched real-life recorded videos of AVs operating with or without errors, coupled with varying levels of risk information (high, low, or none). They reported their anticipated emotional responses using 19 discrete emotion items, and trust was assessed through dispositional, learned, and situational trust measures. Factor analysis identified four key emotional components, namely hostility, confidence, anxiety, and loneliness, that were influenced by risk perception and AV performance. The linear mixed model showed that risk perception was not a significant predictor of trust, while performance and individual differences were. Mediation analysis revealed that confidence was a strong positive mediator, while hostile and anxious emotions negatively impacted trust. However, lonely emotions did not significantly mediate the relationship between AV performance and trust. The results show that real-time AV behavior is more influential on trust than pre-existing risk perceptions, indicating trust in AVs might be more experience-based than shaped by prior beliefs. Our findings also underscore the importance of fostering positive emotional responses for trust calibration, which has important implications for user experience design in automated driving.', 'abstract_zh': '自动化车辆中信任的情感作用：风险感知和车辆性能的影响', 'title_zh': '情绪通过风险感知和系统性能在自动驾驶中对信任的中介效应'}
{'arxiv_id': 'arXiv:2504.04239', 'title': 'Nonlinear Observer Design for Landmark-Inertial Simultaneous Localization and Mapping', 'authors': 'Mouaad Boughellaba, Soulaimane Berkane, Abdelhamid Tayebi', 'link': 'https://arxiv.org/abs/2504.04239', 'abstract': 'This paper addresses the problem of Simultaneous Localization and Mapping (SLAM) for rigid body systems in three-dimensional space. We introduce a new matrix Lie group SE_{3+n}(3), whose elements are composed of the pose, gravity, linear velocity and landmark positions, and propose an almost globally asymptotically stable nonlinear geometric observer that integrates Inertial Measurement Unit (IMU) data with landmark measurements. The proposed observer estimates the pose and map up to a constant position and a constant rotation about the gravity direction. Numerical simulations are provided to validate the performance and effectiveness of the proposed observer, demonstrating its potential for robust SLAM applications.', 'abstract_zh': '这篇论文解决了三维空间中刚体系统的同时定位与建图（SLAM）问题。我们引入了一个新的矩阵李群SE_{3+n}(3)，其元素由姿态、重力、线性速度和地标位置组成，并提出了一种几乎全局渐近稳定的非线性几何观测器，该观测器将惯性测量单元（IMU）数据与地标测量数据进行整合。所提出的观测器估计姿态和地图，相对于重力方向有一个固定的平移和旋转。提供了数值仿真来验证所提观测器的性能和有效性，展示了其在鲁棒SLAM应用中的潜力。', 'title_zh': '地标-惯性同时定位与建图的非线性观测器设计'}
{'arxiv_id': 'arXiv:2504.04170', 'title': 'Learning about the Physical World through Analytic Concepts', 'authors': 'Jianhua Sun, Cewu Lu', 'link': 'https://arxiv.org/abs/2504.04170', 'abstract': 'Reviewing the progress in artificial intelligence over the past decade, various significant advances (e.g. object detection, image generation, large language models) have enabled AI systems to produce more semantically meaningful outputs and achieve widespread adoption in internet scenarios. Nevertheless, AI systems still struggle when it comes to understanding and interacting with the physical world. This reveals an important issue: relying solely on semantic-level concepts learned from internet data (e.g. texts, images) to understand the physical world is far from sufficient -- machine intelligence currently lacks an effective way to learn about the physical world. This research introduces the idea of analytic concept -- representing the concepts related to the physical world through programs of mathematical procedures, providing machine intelligence a portal to perceive, reason about, and interact with the physical world. Except for detailing the design philosophy and providing guidelines for the application of analytic concepts, this research also introduce about the infrastructure that has been built around analytic concepts. I aim for my research to contribute to addressing these questions: What is a proper abstraction of general concepts in the physical world for machine intelligence? How to systematically integrate structured priors with neural networks to constrain AI systems to comply with physical laws?', 'abstract_zh': '过去十年人工智能进展回顾：从语义层面的概念到物理世界的分析概念', 'title_zh': '通过分析概念学习物理世界'}
{'arxiv_id': 'arXiv:2504.03769', 'title': 'Optimal Sensor Placement Using Combinations of Hybrid Measurements for Source Localization', 'authors': 'Kang Tang, Sheng Xu, Yuqi Yang, He Kong, Yongsheng Ma', 'link': 'https://arxiv.org/abs/2504.03769', 'abstract': 'This paper focuses on static source localization employing different combinations of measurements, including time-difference-of-arrival (TDOA), received-signal-strength (RSS), angle-of-arrival (AOA), and time-of-arrival (TOA) measurements. Since sensor-source geometry significantly impacts localization accuracy, the strategies of optimal sensor placement are proposed systematically using combinations of hybrid measurements. Firstly, the relationship between sensor placement and source estimation accuracy is formulated by a derived Cramér-Rao bound (CRB). Secondly, the A-optimality criterion, i.e., minimizing the trace of the CRB, is selected to calculate the smallest reachable estimation mean-squared-error (MSE) in a unified manner. Thirdly, the optimal sensor placement strategies are developed to achieve the optimal estimation bound. Specifically, the specific constraints of the optimal geometries deduced by specific measurement, i.e., TDOA, AOA, RSS, and TOA, are found and discussed theoretically. Finally, the new findings are verified by simulation studies.', 'abstract_zh': '本文聚焦于利用不同组合的测量数据进行静态源定位，包括时间到达差（TDOA）、接收信号强度（RSS）、到达角（AOA）和到达时间（TOA）测量。由于传感器-源几何结构显著影响定位精度，提出了系统性的最优传感器布局策略，利用组合混和测量数据。首先，通过推导出的克拉默-拉奥下界（CRB）制定了传感器布局与源估计精度之间的关系。其次，选择了A-最优标准，即最小化CRB的迹，以此统一计算最小可达到的均方误差（MSE）。第三，发展了最优传感器布局策略以达到最优估计界限。具体而言，探讨了特定测量数据，即TDOA、AOA、RSS和TOA，所推导出的最佳几何结构的具体约束条件。最后，通过仿真研究验证了新发现。', 'title_zh': '基于混合测量组合的源定位传感器优化布设'}
{'arxiv_id': 'arXiv:2410.14996', 'title': 'EDRF: Enhanced Driving Risk Field Based on Multimodal Trajectory Prediction and Its Applications', 'authors': 'Junkai Jiang, Zeyu Han, Yuning Wang, Mengchi Cai, Qingwen Meng, Qing Xu, Jianqiang Wang', 'link': 'https://arxiv.org/abs/2410.14996', 'abstract': "Driving risk assessment is crucial for both autonomous vehicles and human-driven vehicles. The driving risk can be quantified as the product of the probability that an event (such as collision) will occur and the consequence of that event. However, the probability of events occurring is often difficult to predict due to the uncertainty of drivers' or vehicles' behavior. Traditional methods generally employ kinematic-based approaches to predict the future trajectories of entities, which often yield unrealistic prediction results. In this paper, the Enhanced Driving Risk Field (EDRF) model is proposed, integrating deep learning-based multimodal trajectory prediction results with Gaussian distribution models to quantitatively capture the uncertainty of traffic entities' behavior. The applications of the EDRF are also proposed. It is applied across various tasks (traffic risk monitoring, ego-vehicle risk analysis, and motion and trajectory planning) through the defined concept Interaction Risk (IR). Adequate example scenarios are provided for each application to illustrate the effectiveness of the model.", 'abstract_zh': '基于增强驾驶风险场的驾驶风险评估', 'title_zh': '基于多模态轨迹预测的增强驾驶风险场及其应用'}
{'arxiv_id': 'arXiv:2504.05231', 'title': 'Mapping biodiversity at very-high resolution in Europe', 'authors': 'César Leblanc, Lukas Picek, Benjamin Deneu, Pierre Bonnet, Maximilien Servajean, Rémi Palard, Alexis Joly', 'link': 'https://arxiv.org/abs/2504.05231', 'abstract': 'This paper describes a cascading multimodal pipeline for high-resolution biodiversity mapping across Europe, integrating species distribution modeling, biodiversity indicators, and habitat classification. The proposed pipeline first predicts species compositions using a deep-SDM, a multimodal model trained on remote sensing, climate time series, and species occurrence data at 50x50m resolution. These predictions are then used to generate biodiversity indicator maps and classify habitats with Pl@ntBERT, a transformer-based LLM designed for species-to-habitat mapping. With this approach, continental-scale species distribution maps, biodiversity indicator maps, and habitat maps are produced, providing fine-grained ecological insights. Unlike traditional methods, this framework enables joint modeling of interspecies dependencies, bias-aware training with heterogeneous presence-absence data, and large-scale inference from multi-source remote sensing inputs.', 'abstract_zh': '本文描述了一个级联多模态管道，用于欧洲高分辨率生物多样性mapping，整合了物种分布模型、生物多样性指标和栖地分类。该提出的管道首先使用深度SDM预测物种组成，该模型基于50x50m分辨率的遥感数据、气候时间序列和物种分布数据进行训练。这些预测结果随后用于生成生物多样性指标地图，并使用专为物种-栖地映射设计的基于Transformer的大规模语言模型Pl@ntBERT进行栖地分类。通过这种方法， continental尺度的物种分布地图、生物多样性指标地图和栖地地图得以生成，提供精细的生态洞察。与传统方法不同，该框架能够同时建模物种之间的依赖关系、使用异质的存活性数据进行校准偏差的训练，并从多源遥感输入中进行大规模推断。', 'title_zh': '欧洲极高分辨率生物多样性mapping'}
{'arxiv_id': 'arXiv:2504.05229', 'title': 'FinGrAct: A Framework for FINe-GRrained Evaluation of ACTionability in Explainable Automatic Fact-Checking', 'authors': 'Islam Eldifrawi, Shengrui Wang, Amine Trabelsi', 'link': 'https://arxiv.org/abs/2504.05229', 'abstract': 'The field of explainable Automatic Fact-Checking (AFC) aims to enhance the transparency and trustworthiness of automated fact-verification systems by providing clear and comprehensible explanations. However, the effectiveness of these explanations depends on their actionability --their ability to empower users to make informed decisions and mitigate misinformation. Despite actionability being a critical property of high-quality explanations, no prior research has proposed a dedicated method to evaluate it. This paper introduces FinGrAct, a fine-grained evaluation framework that can access the web, and it is designed to assess actionability in AFC explanations through well-defined criteria and an evaluation dataset. FinGrAct surpasses state-of-the-art (SOTA) evaluators, achieving the highest Pearson and Kendall correlation with human judgments while demonstrating the lowest ego-centric bias, making it a more robust evaluation approach for actionability evaluation in AFC.', 'abstract_zh': '可解释自动事实核查（AFC）领域的研究旨在通过提供清晰可理解的解释来提高自动化事实验证系统的透明度和可信度。然而，这些解释的有效性取决于其可操作性——即其赋能用户做出知情决策和减轻虚假信息的能力。尽管可操作性是高质量解释的一个关键属性，但此前的研究尚未提出专门评估其可操作性的方法。本文引入了FinGrAct，这是一种细粒度的评估框架，能够访问网络，并通过明确的标准和评估数据集来评估AFC解释的可操作性。FinGrAct超越了现有最先进的（SOTA）评估器，实现了与人类判断最高的皮尔森和肯德尔相关性，同时表现出最低的自中心偏差，使其成为AFC中可操作性评估的更为稳健的方法。', 'title_zh': 'FinGrAct: 一种可解释自动事实核查中行动性细粒度评估框架'}
{'arxiv_id': 'arXiv:2504.04982', 'title': 'Transforming Future Data Center Operations and Management via Physical AI', 'authors': 'Zhiwei Cao, Minghao Li, Feng Lin, Qiang Fu, Jimin Jia, Yonggang Wen, Jianxiong Yin, Simon See', 'link': 'https://arxiv.org/abs/2504.04982', 'abstract': 'Data centers (DCs) as mission-critical infrastructures are pivotal in powering the growth of artificial intelligence (AI) and the digital economy. The evolution from Internet DC to AI DC has introduced new challenges in operating and managing data centers for improved business resilience and reduced total cost of ownership. As a result, new paradigms, beyond the traditional approaches based on best practices, must be in order for future data centers. In this research, we propose and develop a novel Physical AI (PhyAI) framework for advancing DC operations and management. Our system leverages the emerging capabilities of state-of-the-art industrial products and our in-house research and development. Specifically, it presents three core modules, namely: 1) an industry-grade in-house simulation engine to simulate DC operations in a highly accurate manner, 2) an AI engine built upon NVIDIA PhysicsNemo for the training and evaluation of physics-informed machine learning (PIML) models, and 3) a digital twin platform built upon NVIDIA Omniverse for our proposed 5-tier digital twin framework. This system presents a scalable and adaptable solution to digitalize, optimize, and automate future data center operations and management, by enabling real-time digital twins for future data centers. To illustrate its effectiveness, we present a compelling case study on building a surrogate model for predicting the thermal and airflow profiles of a large-scale DC in a real-time manner. Our results demonstrate its superior performance over traditional time-consuming Computational Fluid Dynamics/Heat Transfer (CFD/HT) simulation, with a median absolute temperature prediction error of 0.18 °C. This emerging approach would open doors to several potential research directions for advancing Physical AI in future DC operations.', 'abstract_zh': '数据中心（DCs）作为关键基础设施，在推动人工智能（AI）和数字经济的发展中扮演着重要角色。从互联网数据中心到人工智能数据中心的演变引入了新的运营和管理挑战，旨在提高业务弹性和降低总拥有成本。因此，必须提出超越传统方法的新范式，以适应未来数据中心的需求。在本研究中，我们提出并开发了一种新型物理人工智能（PhyAI）框架，以推动数据中心的运营和管理。该系统利用了先进的工业产品及其自身的研发能力，具体包括三个核心模块：1）工业级的自研仿真引擎，以高精度模拟数据中心的运行；2）基于NVIDIA PhysicsNemo构建的AI引擎，用于训练和评估物理信息机器学习（PIML）模型；3）基于NVIDIA Omniverse构建的数字孪生平台，用于我们的五级数字孪生框架。该系统提供了一种可扩展且灵活的解决方案，以数字化、优化和自动化未来的数据中心运营和管理，并通过实时数字孪生为未来数据中心赋能。为了展示其有效性，我们展示了构建大型数据中心实时热流预测代理模型的案例研究。实验结果表明，该方法在温度预测误差中位数为0.18°C的情况下，优于传统的耗时计算流体动力学/热传递（CFD/HT）模拟，为推进未来数据中心的物理人工智能提供了新的研究方向。', 'title_zh': '通过物理AI转型未来数据中心的操作与管理'}
{'arxiv_id': 'arXiv:2504.04954', 'title': 'GOTHAM: Graph Class Incremental Learning Framework under Weak Supervision', 'authors': 'Aditya Hemant Shahane, Prathosh A.P, Sandeep Kumar', 'link': 'https://arxiv.org/abs/2504.04954', 'abstract': "Graphs are growing rapidly, along with the number of distinct label categories associated with them. Applications like e-commerce, healthcare, recommendation systems, and various social media platforms are rapidly moving towards graph representation of data due to their ability to capture both structural and attribute information. One crucial task in graph analysis is node classification, where unlabeled nodes are categorized into predefined classes. In practice, novel classes appear incrementally sometimes with just a few labels (seen classes) or even without any labels (unseen classes), either because they are new or haven't been explored much. Traditional methods assume abundant labeled data for training, which isn't always feasible. We investigate a broader objective: \\emph{Graph Class Incremental Learning under Weak Supervision (GCL)}, addressing this challenge by meta-training on base classes with limited labeled instances. During the incremental streams, novel classes can have few-shot or zero-shot representation. Our proposed framework GOTHAM efficiently accommodates these unlabeled nodes by finding the closest prototype representation, serving as class representatives in the attribute space. For Text-Attributed Graphs (TAGs), our framework additionally incorporates semantic information to enhance the representation. By employing teacher-student knowledge distillation to mitigate forgetting, GOTHAM achieves promising results across various tasks. Experiments on datasets such as Cora-ML, Amazon, and OBGN-Arxiv showcase the effectiveness of our approach in handling evolving graph data under limited supervision. The repository is available here: \\href{this https URL}{\\small \\textcolor{blue}{Code}}", 'abstract_zh': '图谱在快速增长，与它们相关的不同标签类别也在快速增长。诸如电子商务、医疗保健、推荐系统以及各种社交媒体平台等应用正迅速转向图表示，利用其能够同时捕捉结构和属性信息的能力。图分析中的一项关键任务是节点分类，即将未标注的节点归类为预定义的类别。在实践中，新类别有时会出现，仅仅伴随少数标签（已见过的类别），甚至可能没有任何标签（未见过的类别），这可能是由于它们是新的或尚未被充分探索。传统方法假设有大量的带标签数据用于训练，而在实际情况中这并不总是可行的。我们研究了一个更广泛的目标：在弱监督下的图类别增量学习（GCL），通过在有限的带标签实例上进行元训练来应对这一挑战。在增量流中，新类别可能具有少样本或零样本表示。我们提出的框架GOTHAM通过找到最接近的原型表示有效地处理这些未标注节点，该原型表示在属性空间中作为类代表。对于文本属性图（TAGs），我们的框架还结合了语义信息以增强表示。通过应用教师-学生知识蒸馏来减轻遗忘，GOTHAM在各种任务中取得了令人鼓舞的结果。在Cora-ML、Amazon和OBGN-Arxiv等数据集上的实验展示了我们的方法在有限监督下处理演化图数据的有效性。代码仓库见这里：\\href{this https URL}{\\small \\textcolor{blue}{Code}}。', 'title_zh': 'GOTHAM: 在弱监督下的图类增量学习框架'}
{'arxiv_id': 'arXiv:2504.04862', 'title': 'GAMDTP: Dynamic Trajectory Prediction with Graph Attention Mamba Network', 'authors': 'Yunxiang Liu, Hongkuo Niu, Jianlin Zhu', 'link': 'https://arxiv.org/abs/2504.04862', 'abstract': "Accurate motion prediction of traffic agents is crucial for the safety and stability of autonomous driving systems. In this paper, we introduce GAMDTP, a novel graph attention-based network tailored for dynamic trajectory prediction. Specifically, we fuse the result of self attention and mamba-ssm through a gate mechanism, leveraging the strengths of both to extract features more efficiently and accurately, in each graph convolution layer. GAMDTP encodes the high-definition map(HD map) data and the agents' historical trajectory coordinates and decodes the network's output to generate the final prediction results. Additionally, recent approaches predominantly focus on dynamically fusing historical forecast results and rely on two-stage frameworks including proposal and refinement. To further enhance the performance of the two-stage frameworks we also design a scoring mechanism to evaluate the prediction quality during the proposal and refinement processes. Experiments on the Argoverse dataset demonstrates that GAMDTP achieves state-of-the-art performance, achieving superior accuracy in dynamic trajectory prediction.", 'abstract_zh': '准确的交通代理运动预测对于自动驾驶系统的安全与稳定性至关重要。在本文中，我们介绍了GAMDTP，一种专门为动态轨迹预测设计的图注意力网络。具体而言，我们通过门控机制融合自注意力和mamba-ssm的结果，利用两者的优势在每个图卷积层中更高效、更准确地提取特征。GAMDTP 编码高精度地图数据和代理的历史轨迹坐标，并解码网络输出以生成最终预测结果。此外，近期的方法主要侧重于动态融合历史预测结果，并依赖两阶段框架包括提案和细化。为了进一步提升两阶段框架的性能，我们设计了一种评分机制，在提案和细化过程中评估预测质量。实验结果表明，GAMDTP 在动态轨迹预测方面达到了最先进的性能，取得了更高的准确性。', 'title_zh': 'GAMDTP：基于图注意力Mamba网络的动态轨迹预测'}
{'arxiv_id': 'arXiv:2504.04858', 'title': "Don't Lag, RAG: Training-Free Adversarial Detection Using RAG", 'authors': 'Roie Kazoom, Raz Lapid, Moshe Sipper, Ofer Hadar', 'link': 'https://arxiv.org/abs/2504.04858', 'abstract': "Adversarial patch attacks pose a major threat to vision systems by embedding localized perturbations that mislead deep models. Traditional defense methods often require retraining or fine-tuning, making them impractical for real-world deployment. We propose a training-free Visual Retrieval-Augmented Generation (VRAG) framework that integrates Vision-Language Models (VLMs) for adversarial patch detection. By retrieving visually similar patches and images that resemble stored attacks in a continuously expanding database, VRAG performs generative reasoning to identify diverse attack types, all without additional training or fine-tuning. We extensively evaluate open-source large-scale VLMs, including Qwen-VL-Plus, Qwen2.5-VL-72B, and UI-TARS-72B-DPO, alongside Gemini-2.0, a closed-source model. Notably, the open-source UI-TARS-72B-DPO model achieves up to 95 percent classification accuracy, setting a new state-of-the-art for open-source adversarial patch detection. Gemini-2.0 attains the highest overall accuracy, 98 percent, but remains closed-source. Experimental results demonstrate VRAG's effectiveness in identifying a variety of adversarial patches with minimal human annotation, paving the way for robust, practical defenses against evolving adversarial patch attacks.", 'abstract_zh': '对抗补丁攻击通过嵌入局部扰动误导深度模型，对视觉系统构成重大威胁。传统的防御方法往往需要重新训练或微调，这在实际部署中并不实用。我们提出了一种无需训练的视觉检索增强生成（VRAG）框架，该框架结合了视觉语言模型（VLMs）进行对抗补丁检测。通过检索与存储攻击在不断扩展的数据库中相似的补丁和图像，VRAG进行生成推理以识别多种攻击类型，而无需额外的训练或微调。我们广泛评估了开源大规模VLMs，包括Qwen-VL-Plus、Qwen2.5-VL-72B和UI-TARS-72B-DPO，以及封闭源代码模型Gemini-2.0。值得注意的是，开源的UI-TARS-72B-DPO模型在对抗补丁检测中的分类准确率最高，达到95%，从而在开源对抗补丁检测中达到新的最先进水平。Gemini-2.0的整体准确率达到最高，为98%，但仍然是封闭源代码。实验结果证明，VRAG能够有效地识别各种类型的对抗补丁，且需要的少量人工标注，从而为应对不断演变的对抗补丁攻击提供稳健且实用的防御。', 'title_zh': '别滞后，RAG：基于RAG的无需训练的对抗检测'}
{'arxiv_id': 'arXiv:2504.04850', 'title': 'An Efficient Approach for Cooperative Multi-Agent Learning Problems', 'authors': 'Ángel Aso-Mollar, Eva Onaindia', 'link': 'https://arxiv.org/abs/2504.04850', 'abstract': "In this article, we propose a centralized Multi-Agent Learning framework for learning a policy that models the simultaneous behavior of multiple agents that need to coordinate to solve a certain task. Centralized approaches often suffer from the explosion of an action space that is defined by all possible combinations of individual actions, known as joint actions. Our approach addresses the coordination problem via a sequential abstraction, which overcomes the scalability problems typical to centralized methods. It introduces a meta-agent, called \\textit{supervisor}, which abstracts joint actions as sequential assignments of actions to each agent. This sequential abstraction not only simplifies the centralized joint action space but also enhances the framework's scalability and efficiency. Our experimental results demonstrate that the proposed approach successfully coordinates agents across a variety of Multi-Agent Learning environments of diverse sizes.", 'abstract_zh': '本文提出了一种集中式的多agent学习框架，用于学习一种模型多个需要协调以解决特定任务的agent同时行为的策略。中心化方法通常会受到由所有个体动作可能组合定义的动作空间爆炸问题的影响，即联合动作。我们的方法通过顺序抽象来解决协调问题，从而克服了中心化方法典型的可扩展性问题。该方法引入了一个元agent，称为“监督者”，将联合动作抽象为按顺序分配给每个agent的动作。这种顺序抽象不仅简化了中心化的联合动作空间，还增强了框架的可扩展性和效率。我们的实验结果表明，提出的框架成功地协调了各种大小和类型的多agent学习环境中的agent。', 'title_zh': '一种高效的多agent协作学习问题解决方法'}
{'arxiv_id': 'arXiv:2504.04736', 'title': 'Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use', 'authors': 'Anna Goldie, Azalia Mirhoseini, Hao Zhou, Irene Cai, Christopher D. Manning', 'link': 'https://arxiv.org/abs/2504.04736', 'abstract': 'Reinforcement learning has been shown to improve the performance of large language models. However, traditional approaches like RLHF or RLAIF treat the problem as single-step. As focus shifts toward more complex reasoning and agentic tasks, language models must take multiple steps of text generation, reasoning and environment interaction before generating a solution. We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%, 14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits generalization across tasks: for example, training only on HotPotQA (text question-answering) improves zero-shot performance on GSM8K (a math dataset) by a relative 16.9%.', 'abstract_zh': '逐步强化学习（SWiRL）：针对多步骤优化场景的合成数据生成与RL方法', 'title_zh': '合成数据生成与多步强化学习推理及工具使用'}
{'arxiv_id': 'arXiv:2504.04675', 'title': 'HypRL: Reinforcement Learning of Control Policies for Hyperproperties', 'authors': 'Tzu-Han Hsu, Arshia Rafieioskouei, Borzoo Bonakdarpour', 'link': 'https://arxiv.org/abs/2504.04675', 'abstract': 'We study the problem of learning control policies for complex tasks whose requirements are given by a hyperproperty. The use of hyperproperties is motivated by their significant power to formally specify requirements of multi-agent systems as well as those that need expressiveness in terms of multiple execution traces (e.g., privacy and fairness). Given a Markov decision process M with unknown transitions (representing the environment) and a HyperLTL formula $\\varphi$, our approach first employs Skolemization to handle quantifier alternations in $\\varphi$. We introduce quantitative robustness functions for HyperLTL to define rewards of finite traces of M with respect to $\\varphi$. Finally, we utilize a suitable reinforcement learning algorithm to learn (1) a policy per trace quantifier in $\\varphi$, and (2) the probability distribution of transitions of M that together maximize the expected reward and, hence, probability of satisfaction of $\\varphi$ in M. We present a set of case studies on (1) safety-preserving multi-agent path planning, (2) fairness in resource allocation, and (3) the post-correspondence problem (PCP).', 'abstract_zh': '我们研究了使用超性质学习复杂数学任务控制策略的问题。我们采用超性质的原因在于其强大的能力，能够形式化规范多智能体系统的需求，以及那些需要在多条执行轨迹方面具有表现力的需求（例如隐私和公平）。给定一个具有未知转换的马尔可夫决策过程M（代表环境）和一个HyperLTL公式$\\varphi$，我们的方法首先使用斯科莱化处理$\\varphi$中的量词交替。我们为HyperLTL引入了定量鲁棒性函数，以此定义M的有限轨迹相对于$\\varphi$的奖励。最后，我们利用合适的强化学习算法学习（1）$\\varphi$中每条轨迹量词的策略，以及（2）M的转换概率分布，这些分布共同最大化期望奖励，从而最大化$\\varphi$在M中被满足的概率。我们展示了关于（1）保安全的多智能体路径规划、（2）资源分配的公平性以及（3）后 correspondence问题（PCP）的案例研究。', 'title_zh': 'HypRL: 增强学习在超属性控制策略中的应用'}
{'arxiv_id': 'arXiv:2504.04430', 'title': 'AGITB: A Signal-Level Benchmark for Evaluating Artificial General Intelligence', 'authors': 'Matej Šprogar', 'link': 'https://arxiv.org/abs/2504.04430', 'abstract': "Despite remarkable progress in machine learning, current AI systems continue to fall short of true human-like intelligence. While Large Language Models (LLMs) excel in pattern recognition and response generation, they lack genuine understanding - an essential hallmark of Artificial General Intelligence (AGI). Existing AGI evaluation methods fail to offer a practical, gradual, and informative metric. This paper introduces the Artificial General Intelligence Test Bed (AGITB), comprising twelve rigorous tests that form a signal-processing-level foundation for the potential emergence of cognitive capabilities. AGITB evaluates intelligence through a model's ability to predict binary signals across time without relying on symbolic representations or pretraining. Unlike high-level tests grounded in language or perception, AGITB focuses on core computational invariants reflective of biological intelligence, such as determinism, sensitivity, and generalisation. The test bed assumes no prior bias, operates independently of semantic meaning, and ensures unsolvability through brute force or memorization. While humans pass AGITB by design, no current AI system has met its criteria, making AGITB a compelling benchmark for guiding and recognizing progress toward AGI.", 'abstract_zh': '尽管机器学习取得了显著进步，当前的AI系统仍远未达到真正的类人智能。尽管大型语言模型在模式识别和响应生成方面表现出色，但它们缺乏真正的理解——这是通用人工智能（AGI）的一个基本特征。现有的AGI评估方法未能提供一个实用、渐进且信息丰富的度量标准。本文介绍了通用人工智能测试床（AGITB），它包含十二项严格的测试，为认知能力的潜在涌现提供了信号处理级别的基础。AGITB 通过模型预测时间序列中的二进制信号的能力进行智能评估，而不依赖于符号表示或预训练。与基于语言或感知的高级测试不同，AGITB 侧重于反映生物智能的核心计算不变量，如确定性、灵敏性和泛化能力。测试床假设不存在先验偏见，独立于语义意义运作，并通过穷举或记忆确保不可解性。虽然人类设计上能通过AGITB，但当前没有任何AI系统满足其标准，使其成为指导和识别通往AGI进展的理想基准。', 'title_zh': 'AGITB: 信号级评估人工通用智能基准'}
{'arxiv_id': 'arXiv:2504.04276', 'title': 'A Comparative Study of Explainable AI Methods: Model-Agnostic vs. Model-Specific Approaches', 'authors': 'Keerthi Devireddy', 'link': 'https://arxiv.org/abs/2504.04276', 'abstract': 'This paper compares model-agnostic and model-specific approaches to explainable AI (XAI) in deep learning image classification. I examine how LIME and SHAP (model-agnostic methods) differ from Grad-CAM and Guided Backpropagation (model-specific methods) when interpreting ResNet50 predictions across diverse image categories. Through extensive testing with various species from dogs and birds to insects I found that each method reveals different aspects of the models decision-making process. Model-agnostic techniques provide broader feature attribution that works across different architectures, while model-specific approaches excel at highlighting precise activation regions with greater computational efficiency. My analysis shows there is no "one-size-fits-all" solution for model interpretability. Instead, combining multiple XAI methods offers the most comprehensive understanding of complex models particularly valuable in high-stakes domains like healthcare, autonomous vehicles, and financial services where transparency is crucial. This comparative framework provides practical guidance for selecting appropriate interpretability techniques based on specific application needs and computational constraints.', 'abstract_zh': '本文将模型通用和模型专用方法对比应用于深度学习图像分类的可解释人工智能（XAI）。研究了在不同图像类别中，LIME和SHAP（模型通用方法）与Grad-CAM和Guided Backpropagation（模型专用方法）解释ResNet50预测结果的差异。通过对从狗、鸟到昆虫的各种物种进行广泛的测试，发现每种方法揭示了模型决策过程的不同方面。模型通用技术提供了更广泛的特征归因，适用于不同的架构，而模型专用方法则在突出精确激活区域方面更出色，且具有更高的计算效率。分析显示，并不存在适用于所有情况的模型可解释性解决方案。相反，结合多种XAI方法可以为复杂模型提供最全面的理解，尤其在如医疗保健、自动驾驶和金融服务等高风险领域中，透明度至关重要。这种比较框架为根据具体应用需求和计算约束选择合适的可解释性技术提供了实用指导。', 'title_zh': '可解释AI方法的比较研究：模型无拘束方法 vs. 模型特定方法'}
{'arxiv_id': 'arXiv:2504.04262', 'title': 'Improving Chronic Kidney Disease Detection Efficiency: Fine Tuned CatBoost and Nature-Inspired Algorithms with Explainable AI', 'authors': 'Md. Ehsanul Haque, S. M. Jahidul Islam, Jeba Maliha, Md. Shakhauat Hossan Sumon, Rumana Sharmin, Sakib Rokoni', 'link': 'https://arxiv.org/abs/2504.04262', 'abstract': 'Chronic Kidney Disease (CKD) is a major global health issue which is affecting million people around the world and with increasing rate of mortality. Mitigation of progression of CKD and better patient outcomes requires early detection. Nevertheless, limitations lie in traditional diagnostic methods, especially in resource constrained settings. This study proposes an advanced machine learning approach to enhance CKD detection by evaluating four models: Random Forest (RF), Multi-Layer Perceptron (MLP), Logistic Regression (LR), and a fine-tuned CatBoost algorithm. Specifically, among these, the fine-tuned CatBoost model demonstrated the best overall performance having an accuracy of 98.75%, an AUC of 0.9993 and a Kappa score of 97.35% of the studies. The proposed CatBoost model has used a nature inspired algorithm such as Simulated Annealing to select the most important features, Cuckoo Search to adjust outliers and grid search to fine tune its settings in such a way to achieve improved prediction accuracy. Features significance is explained by SHAP-a well-known XAI technique-for gaining transparency in the decision-making process of proposed model and bring up trust in diagnostic systems. Using SHAP, the significant clinical features were identified as specific gravity, serum creatinine, albumin, hemoglobin, and diabetes mellitus. The potential of advanced machine learning techniques in CKD detection is shown in this research, particularly for low income and middle-income healthcare settings where prompt and correct diagnoses are vital. This study seeks to provide a highly accurate, interpretable, and efficient diagnostic tool to add to efforts for early intervention and improved healthcare outcomes for all CKD patients.', 'abstract_zh': '慢性肾脏病（CKD）是全球性的重大健康问题，影响着全世界数百万人，并且死亡率呈上升趋势。通过早期检测减缓CKD的进展和提高患者预后需要先进的诊断方法。然而，传统诊断方法在资源受限的环境中存在局限性。本文提出了一种高级机器学习方法，通过评估四种模型——随机森林（RF）、多层感知器（MLP）、逻辑回归（LR）以及优化后的CatBoost算法——来增强CKD的检测。特别是，优化后的CatBoost模型展示了最佳的整体性能，准确率为98.75%，AUC为0.9993，卡帕系数为97.35%。提出的CatBoost模型采用了模拟退火等启发式算法来选择最重要的特征，使用了布谷鸟搜索来调整异常值，并通过网格搜索来优化其设置，从而实现提高预测准确性的目标。特征的重要性通过SHAP（一种广泛认可的可解释性人工智能技术）进行解释，以提高所提模型在决策过程中的透明度，增强诊断系统的信任度。研究发现，显著的临床特征包括比重、血清肌酐、白蛋白、血红蛋白和糖尿病。本文展示了高级机器学习技术在CKD检测中的潜力，特别是在低收入和中等收入国家的医疗保健环境中，及时和准确的诊断至关重要。本研究旨在提供一种高度准确、可解释和高效的诊断工具，以补充早期干预和改善所有CKD患者健康结果的努力。', 'title_zh': '提高慢性肾病检测效率：Fine Tuned CatBoost与启发式算法结合的解释性AI'}
{'arxiv_id': 'arXiv:2504.04128', 'title': 'Guaranteeing consistency in evidence fusion: A novel perspective on credibility', 'authors': 'Chaoxiong Ma, Yan Liang, Huixia Zhang, Hao Sun', 'link': 'https://arxiv.org/abs/2504.04128', 'abstract': "It is explored that available credible evidence fusion schemes suffer from the potential inconsistency because credibility calculation and Dempster's combination rule-based fusion are sequentially performed in an open-loop style. This paper constructs evidence credibility from the perspective of the degree of support for events within the framework of discrimination (FOD) and proposes an iterative credible evidence fusion (ICEF) to overcome the inconsistency in view of close-loop control. On one hand, the ICEF introduces the fusion result into credibility assessment to establish the correlation between credibility and the fusion result. On the other hand, arithmetic-geometric divergence is promoted based on the exponential normalization of plausibility and belief functions to measure evidence conflict, called plausibility-belief arithmetic-geometric divergence (PBAGD), which is superior in capturing the correlation and difference of FOD subsets, identifying abnormal sources, and reducing their fusion weights. The ICEF is compared with traditional methods by combining different evidence difference measure forms via numerical examples to verify its performance. Simulations on numerical examples and benchmark datasets reflect the adaptability of PBAGD to the proposed fusion strategy.", 'abstract_zh': '基于推理支持度的迭代可信证据融合（ICEF） metod及其应用', 'title_zh': '保证证据融合中的一致性：可信度的一种新视角'}
{'arxiv_id': 'arXiv:2504.04121', 'title': 'Improving Question Embeddings with Cognitiv Representation Optimization for Knowledge Tracing', 'authors': 'Lixiang Xu, Xianwei Ding, Xin Yuan, Zhanlong Wang, Lu Bai, Enhong Chen, Philip S. Yu, Yuanyan Tang', 'link': 'https://arxiv.org/abs/2504.04121', 'abstract': "The Knowledge Tracing (KT) aims to track changes in students' knowledge status and predict their future answers based on their historical answer records. Current research on KT modeling focuses on predicting student' future performance based on existing, unupdated records of student learning interactions. However, these approaches ignore the distractors (such as slipping and guessing) in the answering process and overlook that static cognitive representations are temporary and limited. Most of them assume that there are no distractors in the answering process and that the record representations fully represent the students' level of understanding and proficiency in knowledge. In this case, it may lead to many insynergy and incoordination issue in the original records. Therefore we propose a Cognitive Representation Optimization for Knowledge Tracing (CRO-KT) model, which utilizes a dynamic programming algorithm to optimize structure of cognitive representations. This ensures that the structure matches the students' cognitive patterns in terms of the difficulty of the exercises. Furthermore, we use the co-optimization algorithm to optimize the cognitive representations of the sub-target exercises in terms of the overall situation of exercises responses by considering all the exercises with co-relationships as a single goal. Meanwhile, the CRO-KT model fuses the learned relational embeddings from the bipartite graph with the optimized record representations in a weighted manner, enhancing the expression of students' cognition. Finally, experiments are conducted on three publicly available datasets respectively to validate the effectiveness of the proposed cognitive representation optimization model.", 'abstract_zh': '基于认知表示优化的知识追踪(CRO-KT)模型', 'title_zh': '基于认知表示优化提升问题嵌入表示以增强知识追踪'}
{'arxiv_id': 'arXiv:2504.04089', 'title': 'Lifting Factor Graphs with Some Unknown Factors for New Individuals', 'authors': 'Malte Luttermann, Ralf Möller, Marcel Gehrke', 'link': 'https://arxiv.org/abs/2504.04089', 'abstract': 'Lifting exploits symmetries in probabilistic graphical models by using a representative for indistinguishable objects, allowing to carry out query answering more efficiently while maintaining exact answers. In this paper, we investigate how lifting enables us to perform probabilistic inference for factor graphs containing unknown factors, i.e., factors whose underlying function of potential mappings is unknown. We present the Lifting Factor Graphs with Some Unknown Factors (LIFAGU) algorithm to identify indistinguishable subgraphs in a factor graph containing unknown factors, thereby enabling the transfer of known potentials to unknown potentials to ensure a well-defined semantics of the model and allow for (lifted) probabilistic inference. We further extend LIFAGU to incorporate additional background knowledge about groups of factors belonging to the same individual object. By incorporating such background knowledge, LIFAGU is able to further reduce the ambiguity of possible transfers of known potentials to unknown potentials.', 'abstract_zh': '提升技术通过使用代表不可区分对象的符号来利用概率图形模型中的对称性，从而在保持精确答案的同时更高效地执行查询回答。在本文中，我们研究提升如何使我们能够对包含未知因素的因子图执行概率推理，即那些潜在映射底层函数未知的因素。我们提出了Lifting Factor Graphs with Some Unknown Factors (LIFAGU) 算法，以识别包含未知因素的因子图中的不可区分子图，从而将已知势转移到未知势，确保模型的语义明确，并允许进行（提升的）概率推理。此外，我们将LIFAGU扩展以整合关于属于同一个体对象的因素组的额外背景知识。通过整合这种背景知识，LIFAGU能够进一步减少已知势转移到未知势的可能转换的模糊性。', 'title_zh': '带有部分未知因素的提升因素图模型：为新个体建模'}
{'arxiv_id': 'arXiv:2504.04086', 'title': 'Towards An Efficient and Effective En Route Travel Time Estimation Framework', 'authors': 'Zekai Shen, Haitao Yuan, Xiaowei Mao, Congkang Lv, Shengnan Guo, Youfang Lin, Huaiyu Wan', 'link': 'https://arxiv.org/abs/2504.04086', 'abstract': 'En route travel time estimation (ER-TTE) focuses on predicting the travel time of the remaining route. Existing ER-TTE methods always make re-estimation which significantly hinders real-time performance, especially when faced with the computational demands of simultaneous user requests. This results in delays and reduced responsiveness in ER-TTE services. We propose a general efficient framework U-ERTTE combining an Uncertainty-Guided Decision mechanism (UGD) and Fine-Tuning with Meta-Learning (FTML) to address these challenges. UGD quantifies the uncertainty and provides confidence intervals for the entire route. It selectively re-estimates only when the actual travel time deviates from the predicted confidence intervals, thereby optimizing the efficiency of ER-TTE. To ensure the accuracy of confidence intervals and accurate predictions that need to re-estimate, FTML is employed to train the model, enabling it to learn general driving patterns and specific features to adapt to specific tasks. Extensive experiments on two large-scale real datasets demonstrate that the U-ERTTE framework significantly enhances inference speed and throughput while maintaining high effectiveness. Our code is available at this https URL', 'abstract_zh': '沿途旅行时间估计（ER-TTE）专注于预测剩余路线的旅行时间。现有的ER-TTE方法通常需要重新估计，这在面对同时用户请求的计算需求时显著影响了实时性能，导致ER-TTE服务出现延迟和响应性降低。我们提出了一种结合不确定性引导决策机制（UGD）和元学习细调（FTML）的一般高效框架U-ERTTE以应对这些挑战。UGD量化不确定性并为整个路线提供置信区间，在实际旅行时间偏离预测置信区间时才选择性地重新估计，从而优化ER-TTE的效率。为了确保置信区间的准确性和需要重新估计的准确预测，采用FTML对模型进行训练，使其能够学习通用的驾驶模式并适应特定任务的特定特征。在两个大规模真实数据集上的广泛实验表明，U-ERTTE框架显著提高了推理速度和 throughput，同时保持了高有效性。我们的代码可在以下网址获取：this https URL', 'title_zh': '面向高效准确的在途旅行时间估计框架'}
{'arxiv_id': 'arXiv:2504.03810', 'title': 'Hierarchically Encapsulated Representation for Protocol Design in Self-Driving Labs', 'authors': 'Yu-Zhe Shi, Mingchen Liu, Fanxu Meng, Qiao Xu, Zhangqian Bi, Kun He, Lecheng Ruan, Qining Wang', 'link': 'https://arxiv.org/abs/2504.03810', 'abstract': 'Self-driving laboratories have begun to replace human experimenters in performing single experimental skills or predetermined experimental protocols. However, as the pace of idea iteration in scientific research has been intensified by Artificial Intelligence, the demand for rapid design of new protocols for new discoveries become evident. Efforts to automate protocol design have been initiated, but the capabilities of knowledge-based machine designers, such as Large Language Models, have not been fully elicited, probably for the absence of a systematic representation of experimental knowledge, as opposed to isolated, flatten pieces of information. To tackle this issue, we propose a multi-faceted, multi-scale representation, where instance actions, generalized operations, and product flow models are hierarchically encapsulated using Domain-Specific Languages. We further develop a data-driven algorithm based on non-parametric modeling that autonomously customizes these representations for specific domains. The proposed representation is equipped with various machine designers to manage protocol design tasks, including planning, modification, and adjustment. The results demonstrate that the proposed method could effectively complement Large Language Models in the protocol design process, serving as an auxiliary module in the realm of machine-assisted scientific exploration.', 'abstract_zh': '自我驾驶实验室已经开始替代人类实验员执行单一实验技能或预定的实验协议。然而，随着人工智能加速了科学研究中的思想迭代，对快速设计新的实验协议以进行新发现的需求变得明显。已经开始尝试自动化协议设计，但知识为基础的机器设计者，如大型语言模型的能力尚未被充分激发，这可能是由于缺乏系统性的实验知识表示，与孤立扁平的信息片段相对。为了解决这一问题，我们提出了一种多层次、多尺度的表示方法，其中实例动作、通用操作和产品流模型通过领域特定语言进行层次封装。进一步开发了一种基于非参数建模的驱动数据算法，以自主地为特定领域定制这些表示方法。所提出的表示方法配备了各种机器设计者来管理协议设计任务，包括规划、修改和调整。实验结果表明，所提出的方法可以有效地补充大型语言模型在协议设计过程中的作用，作为机器辅助科学探索领域的一个辅助模块。', 'title_zh': '自驾驶实验室中协议设计的层次封装表示方法'}
{'arxiv_id': 'arXiv:2504.03771', 'title': 'Flow State: Humans Enabling AI Systems to Program Themselves', 'authors': 'Helena Zhang, Jakobi Haskell, Yosef Frost', 'link': 'https://arxiv.org/abs/2504.03771', 'abstract': 'Compound AI systems, orchestrating multiple AI components and external APIs, are increasingly vital but face challenges in managing complexity, handling ambiguity, and enabling effective development workflows. Existing frameworks often introduce significant overhead, implicit complexity, or restrictive abstractions, hindering maintainability and iterative refinement, especially in Human-AI collaborative settings. We argue that overcoming these hurdles requires a foundational architecture prioritizing structural clarity and explicit control. To this end, we introduce Pocketflow, a platform centered on Human-AI co-design, enabled by Pocketflow. Pocketflow is a Python framework built upon a deliberately minimal yet synergistic set of core abstractions: modular Nodes with a strict lifecycle, declarative Flow orchestration, native hierarchical nesting (Flow-as-Node), and explicit action-based conditional logic. This unique combination provides a robust, vendor-agnostic foundation with very little code that demonstrably reduces overhead while offering the expressiveness needed for complex patterns like agentic workflows and RAG. Complemented by Pocket AI, an assistant leveraging this structure for system design, Pocketflow provides an effective environment for iteratively prototyping, refining, and deploying the adaptable, scalable AI systems demanded by modern enterprises.', 'abstract_zh': '基于-pocketflow的人机联合设计平台：一种促进复杂人机智能系统开发的基础架构', 'title_zh': '自编程状态：人类赋能AI系统自动编程'}
{'arxiv_id': 'arXiv:2504.03731', 'title': 'A Benchmark for Scalable Oversight Protocols', 'authors': 'Abhimanyu Pallavi Sudhir, Jackson Kaunismaa, Arjun Panickssery', 'link': 'https://arxiv.org/abs/2504.03731', 'abstract': 'As AI agents surpass human capabilities, scalable oversight -- the problem of effectively supplying human feedback to potentially superhuman AI models -- becomes increasingly critical to ensure alignment. While numerous scalable oversight protocols have been proposed, they lack a systematic empirical framework to evaluate and compare them. While recent works have tried to empirically study scalable oversight protocols -- particularly Debate -- we argue that the experiments they conduct are not generalizable to other protocols. We introduce the scalable oversight benchmark, a principled framework for evaluating human feedback mechanisms based on our agent score difference (ASD) metric, a measure of how effectively a mechanism advantages truth-telling over deception. We supply a Python package to facilitate rapid and competitive evaluation of scalable oversight protocols on our benchmark, and conduct a demonstrative experiment benchmarking Debate.', 'abstract_zh': '随着AI代理超越人类能力，可扩展监督——有效向可能超人类的AI模型提供人类反馈的问题——变得越来越关键，以确保一致性和对齐。尽管提出了许多可扩展监督协议，但缺乏系统性的实证框架来评估和比较它们。尽管最近有些研究尝试从实证角度研究可扩展监督协议（特别是 Debate），我们认为它们开展的实验不具备泛化到其他协议的能力。我们引入了可扩展监督基准，这是一种基于我们代理得分差（ASD）度量的严格框架，用于评估人类反馈机制，该度量衡量机制如何有效地使诚实胜过欺骗。我们提供了一个Python包，以促进在基准上快速且具有竞争力地评估可扩展监督协议，并进行了一个示范性实验来基准测试Debate。', 'title_zh': '可扩展监督协议基准'}
{'arxiv_id': 'arXiv:2504.03729', 'title': 'A Scalable Predictive Modelling Approach to Identifying Duplicate Adverse Event Reports for Drugs and Vaccines', 'authors': 'Jim W. Barrett, Nils Erlanson, Joana Félix China, G. Niklas Norén', 'link': 'https://arxiv.org/abs/2504.03729', 'abstract': 'The practice of pharmacovigilance relies on large databases of individual case safety reports to detect and evaluate potential new causal associations between medicines or vaccines and adverse events. Duplicate reports are separate and unlinked reports referring to the same case of an adverse event involving a specific patient at a certain time. They impede statistical analysis and mislead clinical assessment. The large size of such databases precludes a manual identification of duplicates, and so a computational method must be employed. This paper builds upon a hitherto state of the art model, vigiMatch, modifying existing features and introducing new ones to target known shortcomings of the original model. Two support vector machine classifiers, one for medicines and one for vaccines, classify report pairs as duplicates and non-duplicates. Recall was measured using a diverse collection of 5 independent labelled test sets. Precision was measured by having each model classify a randomly selected stream of pairs of reports until each model classified 100 pairs as duplicates. These pairs were assessed by a medical doctor without indicating which method(s) had flagged each pair. Performance on individual countries was measured by having a medical doctor assess a subset of pairs classified as duplicates for three different countries. The new model achieved higher precision and higher recall for all labelled datasets compared to the previous state of the art model, with comparable performance for medicines and vaccines. The model was shown to produce substantially fewer false positives than the comparator model on pairs from individual countries. The method presented here advances state of the art for duplicate detection in adverse event reports for medicines and vaccines.', 'abstract_zh': '药物警戒实践依赖于大量个例安全报告数据库，以检测和评估药物或疫苗与不良事件之间潜在的新因果关联。重复报告是指涉及同一患者在同一时间发生的同一不良事件的独立而不连接的报告。它们阻碍了统计分析并误导了临床评估。由于这类数据库规模庞大，无法通过手工方法识别重复报告，因此必须使用计算方法。本文在此前最先进的模型vigiMatch的基础上改进现有特征并引入新特征以针对原始模型已知的不足之处进行改进。两个支持向量机分类器分别用于药物和疫苗，将报告对分类为重复和非重复。召回率通过使用5个独立标记的测试集进行测量。精确率通过让每个模型分类随机选择的一系列报告对，直到每个模型分类出100对作为重复报告，并由医学医生进行评估，未指出哪些方法标记了每对报告。通过医学医生评估来自三个不同国家的重复报告的子集对每个国家进行了性能测量。新模型在所有标记数据集上的精确率和召回率均高于以前的最先进的模型，并且在药物和疫苗上表现出相似的性能。该模型在来自个别国家的报告对中产生的假阳性显著少于比较模型。本方法推进了药物和疫苗不良事件报告重复检测的最先进的状态。', 'title_zh': '一种可扩展的预测建模方法，用于识别药物和疫苗的重复不良事件报告'}
{'arxiv_id': 'arXiv:2504.03720', 'title': 'TransNet: Transfer Knowledge for Few-shot Knowledge Graph Completion', 'authors': 'Lihui Liu, Zihao Wang, Dawei Zhou, Ruijie Wang, Yuchen Yan, Bo Xiong, Sihong He, Kai Shu, Hanghang Tong', 'link': 'https://arxiv.org/abs/2504.03720', 'abstract': "Knowledge graphs (KGs) are ubiquitous and widely used in various applications. However, most real-world knowledge graphs are incomplete, which significantly degrades their performance on downstream tasks. Additionally, the relationships in real-world knowledge graphs often follow a long-tail distribution, meaning that most relations are represented by only a few training triplets. To address these challenges, few-shot learning has been introduced. Few-shot KG completion aims to make accurate predictions for triplets involving novel relations when only a limited number of training triplets are available. Although many methods have been proposed, they typically learn each relation individually, overlooking the correlations between different tasks and the relevant information in previously trained tasks. In this paper, we propose a transfer learning-based few-shot KG completion method (TransNet). By learning the relationships between different tasks, TransNet effectively transfers knowledge from similar tasks to improve the current task's performance. Furthermore, by employing meta-learning, TransNet can generalize effectively to new, unseen relations. Extensive experiments on benchmark datasets demonstrate the superiority of TransNet over state-of-the-art methods. Code can be found at this https URL", 'abstract_zh': '知识图谱（KGs）在各种应用中无处不在且被广泛使用。然而，大多数现实中的知识图谱是不完整的，这显著降低了其在下游任务上的性能。此外，现实中的知识图谱中的关系往往遵循长尾分布，意味着大多数关系由少量训练三元组表示。为了解决这些挑战，引入了少样本学习。少样本知识图谱补全旨在在仅有有限数量训练三元组的情况下，对涉及新颖关系的三元组进行准确预测。虽然已经提出了许多方法，但它们通常独立学习每个关系，忽略了不同任务之间的相关性和先前训练任务中的相关信息。在本文中，我们提出了一种基于迁移学习的少样本知识图谱补全方法（TransNet）。通过学习不同任务之间的关系，TransNet有效地将相似任务的知识转移到当前任务，以提高其性能。此外，通过采用元学习，TransNet能够有效泛化到新的未见过的关系。在基准数据集上的广泛实验表明，TransNet在性能上优于最先进的方法。代码可以在以下链接找到：this https URL。', 'title_zh': 'TransNet：迁移知识以实现少样本知识图谱完成'}
{'arxiv_id': 'arXiv:2504.03699', 'title': 'Reinforcing Clinical Decision Support through Multi-Agent Systems and Ethical AI Governance', 'authors': 'Ying-Jung Chen, Chi-Sheng Chen, Ahmad Albarqawi', 'link': 'https://arxiv.org/abs/2504.03699', 'abstract': 'In the age of data-driven medicine, it is paramount to include explainable and ethically managed artificial intelligence in explaining clinical decision support systems to achieve trustworthy and effective patient care. The focus of this paper is on a new architecture of a multi-agent system for clinical decision support that uses modular agents to analyze laboratory results, vital signs, and the clinical context and then integrates these results to drive predictions and validate outcomes. We describe our implementation with the eICU database to run lab-analysis-specific agents, vitals-only interpreters, and contextual reasoners and then run the prediction module and a validation agent. Everything is a transparent implementation of business logic, influenced by the principles of ethical AI governance such as Autonomy, Fairness, and Accountability. It provides visible results that this agent-based framework not only improves on interpretability and accuracy but also on reinforcing trust in AI-assisted decisions in an intensive care setting.', 'abstract_zh': '在数据驱动医学时代，将可解释且伦理管理的人工智能纳入临床决策支持系统的解释中，以实现可信赖且有效的患者护理至关重要。本文的重点是一种新的多智能体系统架构，该架构使用模块化智能体分析实验室数据、生命体征和临床背景，然后将这些结果整合以驱动预测和验证结果。我们使用eICU数据库实现了针对实验室分析的智能体、仅关注生命体征的解释器以及上下文推理器，并运行预测模块和验证智能体。所有这些都遵循了伦理人工智能治理原则（自主性、公平性和问责制）的透明实现，提供了清晰的结果，证明了基于代理的框架不仅提高了可解释性和准确性，还增强了在重症护理环境中对人工智能辅助决策的信任。', 'title_zh': '通过多智能体系统和伦理AI治理强化临床决策支持'}
{'arxiv_id': 'arXiv:2504.03649', 'title': 'Diagnostic Method for Hydropower Plant Condition-based Maintenance combining Autoencoder with Clustering Algorithms', 'authors': 'Samy Jad, Xavier Desforges, Pierre-Yves Villard, Christian Caussidéry, Kamal Medjaher', 'link': 'https://arxiv.org/abs/2504.03649', 'abstract': 'The French company EDF uses supervisory control and data acquisition systems in conjunction with a data management platform to monitor hydropower plant, allowing engineers and technicians to analyse the time-series collected. Depending on the strategic importance of the monitored hydropower plant, the number of time-series collected can vary greatly making it difficult to generate valuable information from the extracted data. In an attempt to provide an answer to this particular problem, a condition detection and diagnosis method combining clustering algorithms and autoencoder neural networks for pattern recognition has been developed and is presented in this paper. First, a dimension reduction algorithm is used to create a 2-or 3-dimensional projection that allows the users to identify unsuspected relationships between datapoints. Then, a collection of clustering algorithms regroups the datapoints into clusters. For each identified cluster, an autoencoder neural network is trained on the corresponding dataset. The aim is to measure the reconstruction error between each autoencoder model and the measured values, thus creating a proximity index for each state discovered during the clustering stage.', 'abstract_zh': '法国公司EDF利用 supervisory control and data acquisition 系统结合数据管理平台来监控水电站，使得工程师和技术人员能够分析收集的时间序列数据。根据监控水电站的战略重要性，收集的时间序列数据数量可能差异很大，这使得从提取的数据中生成有价值的信息变得困难。为解决这一特定问题，本文开发并介绍了结合聚类算法和自动编码神经网络的条件检测与诊断方法。该方法首先使用降维算法创建2维或3维投影，使用户能够识别数据点之间的未预见关系；然后，使用一系列聚类算法将数据点分组到不同的簇中。对于每个识别出的簇，针对相应的数据集训练一个自动编码神经网络。目标是测量每个自动编码模型与测量值之间的重构误差，从而为在聚类阶段发现的每种状态生成一个接近指数。', 'title_zh': '基于自编码器与聚类算法的水力发电厂状态维修诊断方法'}
{'arxiv_id': 'arXiv:2504.05305', 'title': 'URECA: Unique Region Caption Anything', 'authors': 'Sangbeom Lim, Junwan Kim, Heeji Yoon, Jaewoo Jung, Seungryong Kim', 'link': 'https://arxiv.org/abs/2504.05305', 'abstract': 'Region-level captioning aims to generate natural language descriptions for specific image regions while highlighting their distinguishing features. However, existing methods struggle to produce unique captions across multi-granularity, limiting their real-world applicability. To address the need for detailed region-level understanding, we introduce URECA dataset, a large-scale dataset tailored for multi-granularity region captioning. Unlike prior datasets that focus primarily on salient objects, URECA dataset ensures a unique and consistent mapping between regions and captions by incorporating a diverse set of objects, parts, and background elements. Central to this is a stage-wise data curation pipeline, where each stage incrementally refines region selection and caption generation. By leveraging Multimodal Large Language Models (MLLMs) at each stage, our pipeline produces distinctive and contextually grounded captions with improved accuracy and semantic diversity. Building upon this dataset, we present URECA, a novel captioning model designed to effectively encode multi-granularity regions. URECA maintains essential spatial properties such as position and shape through simple yet impactful modifications to existing MLLMs, enabling fine-grained and semantically rich region descriptions. Our approach introduces dynamic mask modeling and a high-resolution mask encoder to enhance caption uniqueness. Experiments show that URECA achieves state-of-the-art performance on URECA dataset and generalizes well to existing region-level captioning benchmarks.', 'abstract_zh': '多粒度区域描述旨在为特定图像区域生成自然语言描述，并突出其独特的特征。然而，现有方法在多粒度上的描述独特性较差，限制了其实际应用。为应对多粒度区域理解的需求，我们引入了URECA数据集，这是一个专为多粒度区域描述设计的大规模数据集。与其他主要关注显著对象的 datasets 不同，URECA数据集通过引入多种物体、部分和背景元素确保了区域与描述之间的独特且一致的映射关系。该数据集的核心在于分阶段的数据策展流程，每阶段逐步细化区域选择和描述生成。通过每个阶段利用多模态大型语言模型（MLLMs），我们的策展流程能够生成更具独特性和语境相关性的描述，提高了描述的准确性和语义多样性。基于这个数据集，我们提出了URECA，这是一种新型的描述模型，旨在有效地编码多粒度区域。URECA通过简单而有效的现有MLLMs修改，保留了关键的空间特性（如位置和形状），使其能够生成细腻且语义丰富的区域描述。我们的方法引入了动态掩码建模和高分辨率掩码编码器，以增强描述的独特性。实验结果显示，URECA在URECA数据集上达到了最先进的性能，并在现有的区域描述基准上表现出良好的泛化能力。', 'title_zh': 'URECA: 唯一区域Anything描述'}
{'arxiv_id': 'arXiv:2504.05295', 'title': 'Dion: A Communication-Efficient Optimizer for Large Models', 'authors': 'Kwangjun Ahn, Byron Xu', 'link': 'https://arxiv.org/abs/2504.05295', 'abstract': 'Training large AI models efficiently requires distributing computation across multiple accelerators, but this often incurs significant communication overhead -- especially during gradient synchronization. We introduce Dion, a communication-efficient optimizer that retains the synchronous semantics of standard distributed training (e.g., DDP, FSDP) while substantially reducing I/O costs. Unlike conventional optimizers that synchronize full gradient matrices, Dion leverages orthonormalized updates with device-local momentum buffers, eliminating the need for full gradient exchange. It further supports an efficient sharding strategy that avoids reconstructing large matrices during training.', 'abstract_zh': 'Dion：一种高效通信的优化器', 'title_zh': 'Dion：一种高效的大模型通信优化器'}
{'arxiv_id': 'arXiv:2504.05255', 'title': 'Adversarial KA', 'authors': 'Sviatoslav Dzhenzher, Michael H. Freedman', 'link': 'https://arxiv.org/abs/2504.05255', 'abstract': 'Regarding the representation theorem of Kolmogorov and Arnold (KA) as an algorithm for representing or «expressing» functions, we test its robustness by analyzing its ability to withstand adversarial attacks. We find KA to be robust to countable collections of continuous adversaries, but unearth a question about the equi-continuity of the outer functions that, so far, obstructs taking limits and defeating continuous groups of adversaries. This question on the regularity of the outer functions is relevant to the debate over the applicability of KA to the general theory of NNs.', 'abstract_zh': '关于Kolmogorov和Arnold (KA)表示定理作为表示或“表达”函数的算法的研究，我们通过分析其抵抗对抗攻击的能力来检验其鲁棒性。我们发现KA对可数集合的连续对抗是鲁棒的，但发现外函数的等连续性问题至今仍阻碍了极限的取法和对抗连续敌对群体。外函数的正则性问题与KA在一般神经网络理论中的适用性辩论有关。', 'title_zh': '对抗性KA'}
{'arxiv_id': 'arXiv:2504.05254', 'title': 'Explaining Low Perception Model Competency with High-Competency Counterfactuals', 'authors': 'Sara Pohland, Claire Tomlin', 'link': 'https://arxiv.org/abs/2504.05254', 'abstract': 'There exist many methods to explain how an image classification model generates its decision, but very little work has explored methods to explain why a classifier might lack confidence in its prediction. As there are various reasons the classifier might lose confidence, it would be valuable for this model to not only indicate its level of uncertainty but also explain why it is uncertain. Counterfactual images have been used to visualize changes that could be made to an image to generate a different classification decision. In this work, we explore the use of counterfactuals to offer an explanation for low model competency--a generalized form of predictive uncertainty that measures confidence. Toward this end, we develop five novel methods to generate high-competency counterfactual images, namely Image Gradient Descent (IGD), Feature Gradient Descent (FGD), Autoencoder Reconstruction (Reco), Latent Gradient Descent (LGD), and Latent Nearest Neighbors (LNN). We evaluate these methods across two unique datasets containing images with six known causes for low model competency and find Reco, LGD, and LNN to be the most promising methods for counterfactual generation. We further evaluate how these three methods can be utilized by pre-trained Multimodal Large Language Models (MLLMs) to generate language explanations for low model competency. We find that the inclusion of a counterfactual image in the language model query greatly increases the ability of the model to generate an accurate explanation for the cause of low model competency, thus demonstrating the utility of counterfactual images in explaining low perception model competency.', 'abstract_zh': '存在许多方法可以解释图像分类模型如何生成其决策，但很少有研究探索解释分类器为何对其预测缺乏信心的方法。由于分类器丧失信心的原因可能多种多样，因此对于该模型不仅表示其不确定性水平，还能解释其不确定性的原因来说，这将是非常有价值的。反事实图像已被用于可视化为生成不同分类决策而对图像进行的可能更改。在本文中，我们研究了使用反事实图像来解释低模型能力——一种衡量信心的通用形式预测不确定性——的方法。为此，我们开发了五种新的方法来生成高能力的反事实图像，即图像梯度下降（IGD）、特征梯度下降（FGD）、自动编码器重构（Reco）、潜在梯度下降（LGD）和潜在最近邻（LNN）。我们在包含六个已知低模型能力原因的两个独特的数据集中评估了这些方法，并发现Reco、LGD和LNN是最有希望的反事实生成方法。进一步地，我们研究了这些三种方法如何被预训练的多模态大型语言模型（MLLMs）用于生成低模型能力的语言解释。我们发现，将反事实图像包含在语言模型查询中极大地提高了模型生成准确解释的能力，以解释低模型能力的原因，从而证明了反事实图像在解释低感知模型能力方面的实用性。', 'title_zh': '用高能力反事实解释低感知模型能力'}
{'arxiv_id': 'arXiv:2504.05248', 'title': 'PINNverse: Accurate parameter estimation in differential equations from noisy data with constrained physics-informed neural networks', 'authors': 'Marius Almanstötter, Roman Vetter, Dagmar Iber', 'link': 'https://arxiv.org/abs/2504.05248', 'abstract': 'Parameter estimation for differential equations from measured data is an inverse problem prevalent across quantitative sciences. Physics-Informed Neural Networks (PINNs) have emerged as effective tools for solving such problems, especially with sparse measurements and incomplete system information. However, PINNs face convergence issues, stability problems, overfitting, and complex loss function design. Here we introduce PINNverse, a training paradigm that addresses these limitations by reformulating the learning process as a constrained differential optimization problem. This approach achieves a dynamic balance between data loss and differential equation residual loss during training while preventing overfitting. PINNverse combines the advantages of PINNs with the Modified Differential Method of Multipliers to enable convergence on any point on the Pareto front. We demonstrate robust and accurate parameter estimation from noisy data in four classical ODE and PDE models from physics and biology. Our method enables accurate parameter inference also when the forward problem is expensive to solve.', 'abstract_zh': '从测量数据中估计微分方程的参数是定量科学中常见的逆问题。基于物理的神经网络（PINNs）已成为解决这类问题的有效工具，尤其是在稀疏测量和不完整系统信息的情况下。然而，PINNs 面临收敛性问题、稳定性问题、过拟合以及复杂的损失函数设计。在此我们介绍 PINNverse，一种通过将学习过程重新表述为约束微分优化问题来解决这些限制的训练范式。此方法在训练过程中实现了数据损失和微分方程残差损失之间的动态平衡，同时防止过拟合。PINNverse 结合了 PINNs 与修改后的差分乘子法的优势，能够在帕累托前沿上的任何点实现收敛。我们在物理学和生物学中的四个经典 ODE 和 PDE 模型中展示了在噪声数据下 robust 和准确的参数估计。我们的方法能够在正向问题求解昂贵的情况下实现准确的参数推断。', 'title_zh': 'PINN逆问题：基于约束物理神经网络从噪声数据中进行精确参数估计'}
{'arxiv_id': 'arXiv:2504.05210', 'title': 'A moving target in AI-assisted decision-making: Dataset shift, model updating, and the problem of update opacity', 'authors': 'Joshua Hatherley', 'link': 'https://arxiv.org/abs/2504.05210', 'abstract': 'Machine learning (ML) systems are vulnerable to performance decline over time due to dataset shift. To address this problem, experts often suggest that ML systems should be regularly updated to ensure ongoing performance stability. Some scholarly literature has begun to address the epistemic and ethical challenges associated with different updating methodologies. Thus far, however, little attention has been paid to the impact of model updating on the ML-assisted decision-making process itself, particularly in the AI ethics and AI epistemology literatures. This article aims to address this gap in the literature. It argues that model updating introduces a new sub-type of opacity into ML-assisted decision-making -- update opacity -- that occurs when users cannot understand how or why an update has changed the reasoning or behaviour of an ML system. This type of opacity presents a variety of distinctive epistemic and safety concerns that available solutions to the black box problem in ML are largely ill-equipped to address. A variety of alternative strategies may be developed or pursued to address the problem of update opacity more directly, including bi-factual explanations, dynamic model reporting, and update compatibility. However, each of these strategies presents its own risks or carries significant limitations. Further research will be needed to address the epistemic and safety concerns associated with model updating and update opacity going forward.', 'abstract_zh': '机器学习系统因数据集偏移而导致性能下降的问题及其更新对决策透明度的影响：一项研究空白及对策探索', 'title_zh': 'AI辅助决策中的流动性目标：数据集转移、模型更新及其透明度问题'}
{'arxiv_id': 'arXiv:2504.05207', 'title': 'Correcting Class Imbalances with Self-Training for Improved Universal Lesion Detection and Tagging', 'authors': 'Alexander Shieh, Tejas Sudharshan Mathai, Jianfei Liu, Angshuman Paul, Ronald M. Summers', 'link': 'https://arxiv.org/abs/2504.05207', 'abstract': 'Universal lesion detection and tagging (ULDT) in CT studies is critical for tumor burden assessment and tracking the progression of lesion status (growth/shrinkage) over time. However, a lack of fully annotated data hinders the development of effective ULDT approaches. Prior work used the DeepLesion dataset (4,427 patients, 10,594 studies, 32,120 CT slices, 32,735 lesions, 8 body part labels) for algorithmic development, but this dataset is not completely annotated and contains class imbalances. To address these issues, in this work, we developed a self-training pipeline for ULDT. A VFNet model was trained on a limited 11.5\\% subset of DeepLesion (bounding boxes + tags) to detect and classify lesions in CT studies. Then, it identified and incorporated novel lesion candidates from a larger unseen data subset into its training set, and self-trained itself over multiple rounds. Multiple self-training experiments were conducted with different threshold policies to select predicted lesions with higher quality and cover the class imbalances. We discovered that direct self-training improved the sensitivities of over-represented lesion classes at the expense of under-represented classes. However, upsampling the lesions mined during self-training along with a variable threshold policy yielded a 6.5\\% increase in sensitivity at 4 FP in contrast to self-training without class balancing (72\\% vs 78.5\\%) and a 11.7\\% increase compared to the same self-training policy without upsampling (66.8\\% vs 78.5\\%). Furthermore, we show that our results either improved or maintained the sensitivity at 4FP for all 8 lesion classes.', 'abstract_zh': '全CT研究中病变检测与标记（ULDT）对于肿瘤负担评估及追踪病变状态（增殖/缩小）随时间的变化至关重要。然而，缺乏完全注释的数据阻碍了有效ULDT方法的发展。先前的工作使用DeepLesion数据集（4427名患者，10594个研究，32120个CT切片，32735个病变，32个身体部分标签）进行算法开发，但该数据集并未完全注释且包含类别不平衡。为解决这些问题，我们在本文中开发了一种自训练管道用于ULDT。VFNet模型在DeepLesion的有限子集（11.5%，包含边界框和标签）上进行训练，以检测和分类CT研究中的病变。然后，它识别并从较大的未见数据子集中整合新的病变候选者到其训练集，并经过多轮自训练。进行了多次自训练实验，采用不同的阈值策略以选择高质量的预测病变并覆盖类别不平衡。我们发现，直接自训练提升了过代表病变类别的敏感性，但降低了欠代表类别的敏感性。然而，通过在自训练过程中增加病变样本量并采用可变阈值策略，与不进行类别平衡的自训练相比，敏感性提高了6.5%（对比72%和78.5%），与未上采样的相同自训练策略相比提高了11.7%（对比66.8%和78.5%）。此外，我们的结果显示，对于所有8种病变类别，这些结果要么提高了敏感性，要么保持了敏感性，在4FP时。', 'title_zh': '使用自我训练纠正类别不平衡以改善通用病变检测和标注'}
{'arxiv_id': 'arXiv:2504.05181', 'title': 'Lightweight and Direct Document Relevance Optimization for Generative Information Retrieval', 'authors': 'Kidist Amde Mekonnen, Yubao Tang, Maarten de Rijke', 'link': 'https://arxiv.org/abs/2504.05181', 'abstract': "Generative information retrieval (GenIR) is a promising neural retrieval paradigm that formulates document retrieval as a document identifier (docid) generation task, allowing for end-to-end optimization toward a unified global retrieval objective. However, existing GenIR models suffer from token-level misalignment, where models trained to predict the next token often fail to capture document-level relevance effectively. While reinforcement learning-based methods, such as reinforcement learning from relevance feedback (RLRF), aim to address this misalignment through reward modeling, they introduce significant complexity, requiring the optimization of an auxiliary reward function followed by reinforcement fine-tuning, which is computationally expensive and often unstable. To address these challenges, we propose direct document relevance optimization (DDRO), which aligns token-level docid generation with document-level relevance estimation through direct optimization via pairwise ranking, eliminating the need for explicit reward modeling and reinforcement learning. Experimental results on benchmark datasets, including MS MARCO document and Natural Questions, show that DDRO outperforms reinforcement learning-based methods, achieving a 7.4% improvement in MRR@10 for MS MARCO and a 19.9% improvement for Natural Questions. These findings highlight DDRO's potential to enhance retrieval effectiveness with a simplified optimization approach. By framing alignment as a direct optimization problem, DDRO simplifies the ranking optimization pipeline of GenIR models while offering a viable alternative to reinforcement learning-based methods.", 'abstract_zh': '直接文档相关性优化（DDRO）：通过直接优化对齐 token 级别文档标识符生成与文档级别相关性估计', 'title_zh': '轻量级和直接的文档相关性优化方法用于生成式信息检索'}
{'arxiv_id': 'arXiv:2504.05172', 'title': 'Attention-Based Multi-Scale Temporal Fusion Network for Uncertain-Mode Fault Diagnosis in Multimode Processes', 'authors': 'Guangqiang Li, M. Amine Atoui, Xiangshun Li', 'link': 'https://arxiv.org/abs/2504.05172', 'abstract': 'Fault diagnosis in multimode processes plays a critical role in ensuring the safe operation of industrial systems across multiple modes. It faces a great challenge yet to be addressed - that is, the significant distributional differences among monitoring data from multiple modes make it difficult for the models to extract shared feature representations related to system health conditions. In response to this problem, this paper introduces a novel method called attention-based multi-scale temporal fusion network. The multi-scale depthwise convolution and gated recurrent unit are employed to extract multi-scale contextual local features and long-short-term features. A temporal attention mechanism is designed to focus on critical time points with higher cross-mode shared information, thereby enhancing the accuracy of fault diagnosis. The proposed model is applied to Tennessee Eastman process dataset and three-phase flow facility dataset. The experiments demonstrate that the proposed model achieves superior diagnostic performance and maintains a small model size.', 'abstract_zh': '多模态过程的故障诊断在确保工业系统在多模式下安全运行中起着关键作用。面对监测数据在多模式下显著的分布差异带来的挑战，本论文介绍了一种基于注意力机制的多尺度时间融合网络方法。该方法利用多尺度深度卷积和门控递归单元提取多尺度上下文局部特征和长期短期特征，并设计了时间注意力机制来关注具有更高跨模式共享信息的关键时间点，从而提高故障诊断的准确性。所提出的模型在田纳西-Eastman 过程数据集和三相流设施数据集上进行了应用。实验结果显示，所提模型在诊断性能上表现优越且保持了较小的模型规模。', 'title_zh': '基于注意力机制的多尺度时间融合网络在多模式过程中不确定模式故障诊断'}
{'arxiv_id': 'arXiv:2504.05167', 'title': 'RLBayes: a Bayesian Network Structure Learning Algorithm via Reinforcement Learning-Based Search Strategy', 'authors': 'Mingcan Wang, Junchang Xin, Luxuan Qu, Qi Chen, Zhiqiong Wang', 'link': 'https://arxiv.org/abs/2504.05167', 'abstract': 'The score-based structure learning of Bayesian network (BN) is an effective way to learn BN models, which are regarded as some of the most compelling probabilistic graphical models in the field of representation and reasoning under uncertainty. However, the search space of structure learning grows super-exponentially as the number of variables increases, which makes BN structure learning an NP-hard problem, as well as a combination optimization problem (COP). Despite the successes of many heuristic methods on it, the results of the structure learning of BN are usually unsatisfactory. Inspired by Q-learning, in this paper, a Bayesian network structure learning algorithm via reinforcement learning-based (RL-based) search strategy is proposed, namely RLBayes. The method borrows the idea of RL and tends to record and guide the learning process by a dynamically maintained Q-table. By creating and maintaining the dynamic Q-table, RLBayes achieve storing the unlimited search space within limited space, thereby achieving the structure learning of BN via Q-learning. Not only is it theoretically proved that RLBayes can converge to the global optimal BN structure, but also it is experimentally proved that RLBayes has a better effect than almost all other heuristic search algorithms.', 'abstract_zh': '基于强化学习的贝叶斯网络结构学习算法（RLBayes）', 'title_zh': 'RLBayes：基于强化学习搜索策略的贝叶斯网络结构学习算法'}
{'arxiv_id': 'arXiv:2504.05125', 'title': 'Interpretable Style Takagi-Sugeno-Kang Fuzzy Clustering', 'authors': 'Suhang Gu, Ye Wang, Yongxin Chou, Jinliang Cong, Mingli Lu, Zhuqing Jiao', 'link': 'https://arxiv.org/abs/2504.05125', 'abstract': 'Clustering is an efficient and essential technique for exploring latent knowledge of data. However, limited attention has been given to the interpretability of the clusters detected by most clustering algorithms. In addition, due to the homogeneity of data, different groups of data have their own homogeneous styles. In this paper, the above two aspects are considered, and an interpretable style Takagi-Sugeno-Kang (TSK) fuzzy clustering (IS-TSK-FC) algorithm is proposed. The clustering behavior of IS-TSK-FC is fully guided by the TSK fuzzy inference on fuzzy rules. In particular, samples are grouped into clusters represented by the corresponding consequent vectors of all fuzzy rules learned in an unsupervised manner. This can explain how the clusters are generated in detail, thus making the underlying decision-making process of the IS-TSK-FC interpretable. Moreover, a series of style matrices are introduced to facilitate the consequents of fuzzy rules in IS-TSK-FC by capturing the styles of clusters as well as the nuances between different styles. Consequently, all the fuzzy rules in IS-TSK-FC have powerful data representation capability. After determining the antecedents of all the fuzzy rules, the optimization problem of IS-TSK-FC can be iteratively solved in an alternation manner. The effectiveness of IS-TSK-FC as an interpretable clustering tool is validated through extensive experiments on benchmark datasets with unknown implicit/explicit styles. Specially, the superior clustering performance of IS-TSK-FC is demonstrated on case studies where different groups of data present explicit styles. The source code of IS-TSK-FC can be downloaded from this https URL.', 'abstract_zh': '具解释性的样式Takagi-Sugeno-Kang模糊聚类算法（IS-TSK-FC）', 'title_zh': '可解释风格 Takagi-Sugeno-Kang 模糊聚类'}
{'arxiv_id': 'arXiv:2504.05119', 'title': 'Balancing Robustness and Efficiency in Embedded DNNs Through Activation Function Selection', 'authors': 'Jon Gutiérrez Zaballa, Koldo Basterretxea, Javier Echanobe', 'link': 'https://arxiv.org/abs/2504.05119', 'abstract': "Machine learning-based embedded systems for safety-critical applications, such as aerospace and autonomous driving, must be robust to perturbations caused by soft errors. As transistor geometries shrink and voltages decrease, modern electronic devices become more susceptible to background radiation, increasing the concern about failures produced by soft errors. The resilience of deep neural networks (DNNs) to these errors depends not only on target device technology but also on model structure and the numerical representation and arithmetic precision of their parameters. Compression techniques like pruning and quantization, used to reduce memory footprint and computational complexity, alter both model structure and representation, affecting soft error robustness. In this regard, although often overlooked, the choice of activation functions (AFs) impacts not only accuracy and trainability but also compressibility and error resilience. This paper explores the use of bounded AFs to enhance robustness against parameter perturbations, while evaluating their effects on model accuracy, compressibility, and computational load with a technology-agnostic approach. We focus on encoder-decoder convolutional models developed for semantic segmentation of hyperspectral images with application to autonomous driving systems. Experiments are conducted on an AMD-Xilinx's KV260 SoM.", 'abstract_zh': '基于机器学习的嵌入式系统在航空航天和自主驾驶等安全关键应用中，必须对由软错误引起的扰动具有鲁棒性。随着晶体管几何尺寸缩小和电压降低，现代电子设备对背景辐射的敏感性增加，软错误导致的故障担忧也随之增加。深度神经网络（DNNs）对这些错误的鲁棒性不仅取决于目标设备技术，还取决于模型结构及其参数的数值表示和算术精度。用于减少内存占用和计算复杂性的剪枝和量化等压缩技术会同时改变模型结构和表示，影响软错误的鲁棒性。在这方面，尽管常常被忽视，激活函数（AFs）的选择不仅影响准确性和可训练性，还影响可压缩性和错误鲁棒性。本文探讨使用有界激活函数以增强对参数扰动的鲁棒性，并以技术无关的方法评估其对模型准确度、可压缩性和计算负载的影响。我们专注于应用于自主驾驶系统的高光谱图像语义分割的编码器-解码器卷积模型。实验在AMD-Xilinx的KV260系统模块上进行。', 'title_zh': '通过激活函数选择在嵌入式DNN中平衡稳健性和效率'}
{'arxiv_id': 'arXiv:2504.05106', 'title': 'SpeakEasy: Enhancing Text-to-Speech Interactions for Expressive Content Creation', 'authors': 'Stephen Brade, Sam Anderson, Rithesh Kumar, Zeyu Jin, Anh Truong', 'link': 'https://arxiv.org/abs/2504.05106', 'abstract': "Novice content creators often invest significant time recording expressive speech for social media videos. While recent advancements in text-to-speech (TTS) technology can generate highly realistic speech in various languages and accents, many struggle with unintuitive or overly granular TTS interfaces. We propose simplifying TTS generation by allowing users to specify high-level context alongside their script. Our Wizard-of-Oz system, SpeakEasy, leverages user-provided context to inform and influence TTS output, enabling iterative refinement with high-level feedback. This approach was informed by two 8-subject formative studies: one examining content creators' experiences with TTS, and the other drawing on effective strategies from voice actors. Our evaluation shows that participants using SpeakEasy were more successful in generating performances matching their personal standards, without requiring significantly more effort than leading industry interfaces.", 'abstract_zh': '初级内容创作者经常花费大量时间录制用于社交媒体视频的富有表现力的语音。虽然最近在文本到语音（TTS）技术方面的进步可以在多种语言和口音下生成高度真实的语音，但许多用户仍然难以使用直观或过于琐碎的TTS界面。我们提出了一种简化TTS生成的方法，允许用户在其脚本中指定高层次的上下文。我们的Wizard-of-Oz系统SpeakEasy利用用户提供的上下文来指导和影响TTS输出，从而使用户能够通过高层次反馈进行迭代改进。该方法受到两项包含8名受试者的形成性研究的启发：一项研究探讨了内容创作者使用TTS的经验，另一项研究借鉴了语音演员的有效策略。我们的评估结果显示，使用SpeakEasy的参与者在其生成的表现更符合个人标准方面更加成功，而无需比领先行业界面投入更多的努力。', 'title_zh': 'SpeakEasy: 提升表达性内容创作的文本到语音交互'}
{'arxiv_id': 'arXiv:2504.05029', 'title': 'Graph-based Diffusion Model for Collaborative Filtering', 'authors': 'Xuan Zhang, Xiang Deng, Hongxing Yuan, Chunyu Wei, Yushun Fan', 'link': 'https://arxiv.org/abs/2504.05029', 'abstract': "Recently, diffusion-based recommendation methods have achieved impressive results. However, existing approaches predominantly treat each user's historical interactions as independent training samples, overlooking the potential of higher-order collaborative signals between users and items. Such signals, which encapsulate richer and more nuanced relationships, can be naturally captured using graph-based data structures. To address this limitation, we extend diffusion-based recommendation methods to the graph domain by directly modeling user-item bipartite graphs with diffusion models. This enables better modeling of the higher-order connectivity inherent in complex interaction dynamics. However, this extension introduces two primary challenges: (1) Noise Heterogeneity, where interactions are influenced by various forms of continuous and discrete noise, and (2) Relation Explosion, referring to the high computational costs of processing large-scale graphs. To tackle these challenges, we propose a Graph-based Diffusion Model for Collaborative Filtering (GDMCF). To address noise heterogeneity, we introduce a multi-level noise corruption mechanism that integrates both continuous and discrete noise, effectively simulating real-world interaction complexities. To mitigate relation explosion, we design a user-active guided diffusion process that selectively focuses on the most meaningful edges and active users, reducing inference costs while preserving the graph's topological integrity. Extensive experiments on three benchmark datasets demonstrate that GDMCF consistently outperforms state-of-the-art methods, highlighting its effectiveness in capturing higher-order collaborative signals and improving recommendation performance.", 'abstract_zh': '基于图的扩散推荐模型（GDMCF）：捕捉高级协作信号以提高推荐性能', 'title_zh': '基于图的扩散模型在协作过滤中的应用'}
{'arxiv_id': 'arXiv:2504.05020', 'title': 'Batch Aggregation: An Approach to Enhance Text Classification with Correlated Augmented Data', 'authors': 'Charco Hui, Yalu Wen', 'link': 'https://arxiv.org/abs/2504.05020', 'abstract': "Natural language processing models often face challenges due to limited labeled data, especially in domain specific areas, e.g., clinical trials. To overcome this, text augmentation techniques are commonly used to increases sample size by transforming the original input data into artificial ones with the label preserved. However, traditional text classification methods ignores the relationship between augmented texts and treats them as independent samples which may introduce classification error. Therefore, we propose a novel approach called 'Batch Aggregation' (BAGG) which explicitly models the dependence of text inputs generated through augmentation by incorporating an additional layer that aggregates results from correlated texts. Through studying multiple benchmark data sets across different domains, we found that BAGG can improve classification accuracy. We also found that the increase of performance with BAGG is more obvious in domain specific data sets, with accuracy improvements of up to 10-29%. Through the analysis of benchmark data, the proposed method addresses limitations of traditional techniques and improves robustness in text classification tasks. Our result demonstrates that BAGG offers more robust results and outperforms traditional approaches when training data is limited.", 'abstract_zh': '自然语言处理模型由于标注数据有限，尤其是在临床试验等特定领域，常常面临挑战。为了克服这一问题，通常使用文本扩增技术通过将原始输入数据转换为带有标签保留的人工数据来增加样本量。然而，传统的文本分类方法忽略了扩增文本之间的关系，将其视为独立样本，这可能会引入分类错误。因此，我们提出了一种名为“批次聚合”（BAGG）的新方法，该方法通过引入一个聚合相关文本结果的额外层，明确建模通过扩增生成的文本输入之间的依赖性。通过在不同领域多个基准数据集上的研究，我们发现BAGG可以提高分类准确性。我们还发现，BAGG在特定领域数据集上的性能提升尤为明显，准确率提高了10-29%。通过对基准数据的分析，所提出的方法解决了传统技术的局限性，提高了文本分类任务的鲁棒性。我们的结果表明，在训练数据有限的情况下，BAGG提供了更稳健的结果并优于传统方法。', 'title_zh': '批量聚合：一种通过相关增量数据增强文本分类的方法'}
{'arxiv_id': 'arXiv:2504.05007', 'title': 'Measuring the right thing: justifying metrics in AI impact assessments', 'authors': 'Stefan Buijsman, Herman Veluwenkamp', 'link': 'https://arxiv.org/abs/2504.05007', 'abstract': 'AI Impact Assessments are only as good as the measures used to assess the impact of these systems. It is therefore paramount that we can justify our choice of metrics in these assessments, especially for difficult to quantify ethical and social values. We present a two-step approach to ensure metrics are properly motivated. First, a conception needs to be spelled out (e.g. Rawlsian fairness or fairness as solidarity) and then a metric can be fitted to that conception. Both steps require separate justifications, as conceptions can be judged on how well they fit with the function of, for example, fairness. We argue that conceptual engineering offers helpful tools for this step. Second, metrics need to be fitted to a conception. We illustrate this process through an examination of competing fairness metrics to illustrate that here the additional content that a conception offers helps us justify the choice for a specific metric. We thus advocate that impact assessments are not only clear on their metrics, but also on the conceptions that motivate those metrics.', 'abstract_zh': 'AI影响评估的效果取决于评估这些系统影响所使用的指标。因此，我们必须在这些评估中合理证明我们选择的指标，尤其是在难以量化伦理和社会价值的情况下尤为重要。我们提出了一种两步方法以确保指标能得到恰当的动机。首先，需要明确概念（例如罗尔斯公平或团结中的公平），然后可以将指标与该概念相匹配。两步都需要独立的证明，因为概念可以基于其与公平功能匹配的程度来评判。我们认为概念工程为这一步提供了一些有用的工具。第二，指标需要与概念相匹配。我们通过分析竞争的公平性指标来说明这一过程，表明概念提供的额外内容有助于我们证明选择特定指标的原因。因此，我们认为影响评估不仅要明确其指标，还需明确那些激励这些指标的概念。', 'title_zh': '测量正确的事情：在AI影响评估中验证指标的有效性'}
{'arxiv_id': 'arXiv:2504.04997', 'title': 'SurvSurf: a partially monotonic neural network for first-hitting time prediction of intermittently observed discrete and continuous sequential events', 'authors': 'Yichen Kelly Chen, Sören Dittmer, Kinga Bernatowicz, Josep Arús-Pous, Kamen Bliznashki, John Aston, James H.F. Rudd, Carola-Bibiane Schönlieb, James Jones, Michael Roberts', 'link': 'https://arxiv.org/abs/2504.04997', 'abstract': 'We propose a neural-network based survival model (SurvSurf) specifically designed for direct and simultaneous probabilistic prediction of the first hitting time of sequential events from baseline. Unlike existing models, SurvSurf is theoretically guaranteed to never violate the monotonic relationship between the cumulative incidence functions of sequential events, while allowing nonlinear influence from predictors. It also incorporates implicit truths for unobserved intermediate events in model fitting, and supports both discrete and continuous time and events. We also identified a variant of the Integrated Brier Score (IBS) that showed robust correlation with the mean squared error (MSE) between the true and predicted probabilities by accounting for implied truths about the missing intermediate events. We demonstrated the superiority of SurvSurf compared to modern and traditional predictive survival models in two simulated datasets and two real-world datasets, using MSE, the more robust IBS and by measuring the extent of monotonicity violation.', 'abstract_zh': '基于神经网络的生存模型（SurvSurf）：直接和同时预测序列事件的首次到达时间的概率模型', 'title_zh': 'SurvSurf: 一种部分单调神经网络，用于间歇观察的离散和连续序列事件首次击中时间预测'}
{'arxiv_id': 'arXiv:2504.04981', 'title': 'DiCoTTA: Domain-invariant Learning for Continual Test-time Adaptation', 'authors': 'Sohyun Lee, Nayeong Kim, Juwon Kang, Seong Joon Oh, Suha Kwak', 'link': 'https://arxiv.org/abs/2504.04981', 'abstract': 'This paper studies continual test-time adaptation (CTTA), the task of adapting a model to constantly changing unseen domains in testing while preserving previously learned knowledge. Existing CTTA methods mostly focus on adaptation to the current test domain only, overlooking generalization to arbitrary test domains a model may face in the future. To tackle this limitation, we present a novel online domain-invariant learning framework for CTTA, dubbed DiCoTTA. DiCoTTA aims to learn feature representation to be invariant to both current and previous test domains on the fly during testing. To this end, we propose a new model architecture and a test-time adaptation strategy dedicated to learning domain-invariant features without corrupting semantic contents, along with a new data structure and optimization algorithm for effectively managing information from previous test domains. DiCoTTA achieved state-of-the-art performance on four public CTTA benchmarks. Moreover, it showed superior generalization to unseen test domains.', 'abstract_zh': '本文研究了连续测试时适应（CTTA），即在测试过程中让模型适应不断变化的未见域，同时保留之前学到的知识。现有的CTTA方法主要关注当前测试域的适应问题，忽视了模型在未来可能遇到的任意测试域的一般性泛化能力。为解决这一局限性，我们提出了一种新的在线域不变学习框架DiCoTTA，旨在在测试过程中实时学习对当前和先前测试域均不变的特征表示。为此，我们提出了一种新的模型架构和测试时适应策略，专门用于学习域不变特征而不破坏语义内容，并提出了一种新的数据结构和优化算法来有效管理之前测试域的信息。DiCoTTA在四个公开的CTTA基准上取得了最先进的性能，并且展示了对未见测试域的优越泛化能力。', 'title_zh': 'DiCoTTA: 领域不变学习以实现持续测试时适应'}
{'arxiv_id': 'arXiv:2504.04973', 'title': 'Ensuring Safety in an Uncertain Environment: Constrained MDPs via Stochastic Thresholds', 'authors': 'Qian Zuo, Fengxiang He', 'link': 'https://arxiv.org/abs/2504.04973', 'abstract': 'This paper studies constrained Markov decision processes (CMDPs) with constraints against stochastic thresholds, aiming at safety of reinforcement learning in unknown and uncertain environments. We leverage a Growing-Window estimator sampling from interactions with the uncertain and dynamic environment to estimate the thresholds, based on which we design Stochastic Pessimistic-Optimistic Thresholding (SPOT), a novel model-based primal-dual algorithm for multiple constraints against stochastic thresholds. SPOT enables reinforcement learning under both pessimistic and optimistic threshold settings. We prove that our algorithm achieves sublinear regret and constraint violation; i.e., a reward regret of $\\tilde{\\mathcal{O}}(\\sqrt{T})$ while allowing an $\\tilde{\\mathcal{O}}(\\sqrt{T})$ constraint violation over $T$ episodes. The theoretical guarantees show that our algorithm achieves performance comparable to that of an approach relying on fixed and clear thresholds. To the best of our knowledge, SPOT is the first reinforcement learning algorithm that realises theoretical guaranteed performance in an uncertain environment where even thresholds are unknown.', 'abstract_zh': '本文研究了面对随机阈值约束的受限马尔可夫决策过程（CMDPs），旨在未知和不确定环境中保证强化学习的安全性。我们利用 Growing-Window 估算器根据与动态环境的交互样本估计阈值，并基于此设计了 Stochastic Pessimistic-Optimistic Thresholding (SPOT) 算法，这是一种新的模型导向的 primal-dual 算法，用于处理面对随机阈值的多约束问题。SPOT 允许在悲观和乐观阈值设置下进行强化学习。我们证明了该算法实现了亚线性遗憾和约束违反，即在 $T$ 期中实现了 $\\tilde{\\mathcal{O}}(\\sqrt{T})$ 的奖励遗憾和 $\\tilde{\\mathcal{O}}(\\sqrt{T})$ 的约束违反。理论保证表明，该算法在性能上与依赖于固定清晰阈值的方法相当。据我们所知，SPOT 是首个在阈值甚至未知的不确定环境中实现理论保证性能的强化学习算法。', 'title_zh': '在不确定环境中的安全性保障：通过随机阈值的约束MDPs'}
{'arxiv_id': 'arXiv:2504.04949', 'title': 'One Quantizer is Enough: Toward a Lightweight Audio Codec', 'authors': 'Linwei Zhai, Han Ding, Cui Zhao, fei wang, Ge Wang, Wang Zhi, Wei Xi', 'link': 'https://arxiv.org/abs/2504.04949', 'abstract': 'Neural audio codecs have recently gained traction for their ability to compress high-fidelity audio and generate discrete tokens that can be utilized in downstream generative modeling tasks. However, leading approaches often rely on resource-intensive models and multi-quantizer architectures, resulting in considerable computational overhead and constrained real-world applicability. In this paper, we present SQCodec, a lightweight neural audio codec that leverages a single quantizer to address these limitations. SQCodec explores streamlined convolutional networks and local Transformer modules, alongside TConv, a novel mechanism designed to capture acoustic variations across multiple temporal scales, thereby enhancing reconstruction fidelity while reducing model complexity. Extensive experiments across diverse datasets show that SQCodec achieves audio quality comparable to multi-quantizer baselines, while its single-quantizer design offers enhanced adaptability and its lightweight architecture reduces resource consumption by an order of magnitude. The source code is publicly available at this https URL.', 'abstract_zh': '神经音频编解码器近年来因其能压缩高保真音频并生成可用于下游生成建模任务的离散词元而受到关注。然而，领先的方法通常依赖于计算密集型模型和多量化器架构，导致了显著的计算开销和有限的实际应用范围。本文我们提出了SQCodec，一种轻量级的神经音频编解码器，利用单一量化器来克服这些限制。SQCodec探究了精简的卷积网络和局部Transformer模块，并引入了TConv机制，该机制能够捕捉不同时间尺度上的声学变化，从而在降低模型复杂度的同时提升重构保真度。在多种数据集上的广泛实验表明，SQCodec在音频质量上与多量化器基线相当，其单一量化器设计增强了模型的适应性，而其轻量级架构将资源消耗降低了十倍。源代码可在如下链接获取：这个 https URL。', 'title_zh': '一个量化器足矣： toward a轻量级音频编解码器'}
{'arxiv_id': 'arXiv:2504.04934', 'title': 'Boosting Relational Deep Learning with Pretrained Tabular Models', 'authors': 'Veronica Lachi, Antonio Longa, Beatrice Bevilacqua, Bruno Lepri, Andrea Passerini, Bruno Ribeiro', 'link': 'https://arxiv.org/abs/2504.04934', 'abstract': 'Relational databases, organized into tables connected by primary-foreign key relationships, are a common format for organizing data. Making predictions on relational data often involves transforming them into a flat tabular format through table joins and feature engineering, which serve as input to tabular methods. However, designing features that fully capture complex relational patterns remains challenging. Graph Neural Networks (GNNs) offer a compelling alternative by inherently modeling these relationships, but their time overhead during inference limits their applicability for real-time scenarios. In this work, we aim to bridge this gap by leveraging existing feature engineering efforts to enhance the efficiency of GNNs in relational databases. Specifically, we use GNNs to capture complex relationships within relational databases, patterns that are difficult to featurize, while employing engineered features to encode temporal information, thereby avoiding the need to retain the entire historical graph and enabling the use of smaller, more efficient graphs. Our \\textsc{LightRDL} approach not only improves efficiency, but also outperforms existing models. Experimental results on the RelBench benchmark demonstrate that our framework achieves up to $33\\%$ performance improvement and a $526\\times$ inference speedup compared to GNNs, making it highly suitable for real-time inference.', 'abstract_zh': '关系数据库通过主-外键关系组织成表格，是组织数据的常见格式。对关系数据进行预测通常涉及通过表连接和特征工程将它们转换为扁平的表格格式，这些表格作为表方法的输入。然而，设计能够完全捕捉复杂关系模式的特征仍具有挑战性。图神经网络（GNNs）通过内在建模这些关系提供了有吸引力的替代方案，但在推理时的时间开销限制了它们在实时场景中的应用。在本工作中，我们通过利用现有的特征工程努力来提高GNNs在关系数据库中的效率。具体而言，我们使用GNNs捕捉关系数据库中的复杂关系，这些关系难以特征化，同时采用工程特征编码时间信息，从而避免保留整个历史图并能够使用更小、更高效的图。我们的\\textsc{LightRDL}方法不仅提高了效率，还在某些方面优于现有模型。基准测试RelBench的实验结果表明，与GNNs相比，我们的框架在性能上最多可提高33%，推理速度提高526倍，使其非常适用于实时推理。', 'title_zh': '利用预训练表型模型增强关系深度学习'}
{'arxiv_id': 'arXiv:2504.04921', 'title': 'Expectations vs Reality -- A Secondary Study on AI Adoption in Software Testing', 'authors': 'Katja Karhu, Jussi Kasurinen, Kari Smolander', 'link': 'https://arxiv.org/abs/2504.04921', 'abstract': 'In the software industry, artificial intelligence (AI) has been utilized more and more in software development activities. In some activities, such as coding, AI has already been an everyday tool, but in software testing activities AI it has not yet made a significant breakthrough. In this paper, the objective was to identify what kind of empirical research with industry context has been conducted on AI in software testing, as well as how AI has been adopted in software testing practice. To achieve this, we performed a systematic mapping study of recent (2020 and later) studies on AI adoption in software testing in the industry, and applied thematic analysis to identify common themes and categories, such as the real-world use cases and benefits, in the found papers. The observations suggest that AI is not yet heavily utilized in software testing, and still relatively few studies on AI adoption in software testing have been conducted in the industry context to solve real-world problems. Earlier studies indicated there was a noticeable gap between the actual use cases and actual benefits versus the expectations, which we analyzed further. While there were numerous potential use cases for AI in software testing, such as test case generation, code analysis, and intelligent test automation, the reported actual implementations and observed benefits were limited. In addition, the systematic mapping study revealed a potential problem with false positive search results in online databases when using the search string "artificial intelligence".', 'abstract_zh': '在软件行业中，人工智能（AI）已在软件开发活动中得到了越来越多的应用。在某些活动中，如编码，AI已经成为日常工作中的工具，但在软件测试活动中，AI尚未取得重大突破。本文旨在识别在软件测试领域中开展的行业背景下的人工智能实证研究，以及人工智能在软件测试实践中的应用情况。为此，我们对2020年及以后有关软件测试中人工智能应用的行业研究进行了系统映射研究，并运用主题分析方法识别出常见的主题和类别，如实际应用案例及其好处。观察结果表明，目前人工智能在软件测试中的应用尚不广泛，行业内针对软件测试中人工智能应用的实际问题开展的研究仍然相对较少。早期研究表明，实际应用案例和实际好处与预期之间存在明显的差距，我们对此进行了进一步分析。虽然人工智能在软件测试中有众多潜在应用场景，如测试案例生成、代码分析和智能测试自动化等，但实际实施情况和观察到的好处却相当有限。此外，系统映射研究还揭示了使用搜索字符串“人工智能”在在线数据库中搜索时可能出现的假阳性结果的问题。', 'title_zh': '期望与现实——软件测试中AI采用的二次研究'}
{'arxiv_id': 'arXiv:2504.04909', 'title': 'AlgOS: Algorithm Operating System', 'authors': 'Llewyn Salt, Marcus Gallagher', 'link': 'https://arxiv.org/abs/2504.04909', 'abstract': 'Algorithm Operating System (AlgOS) is an unopinionated, extensible, modular framework for algorithmic implementations. AlgOS offers numerous features: integration with Optuna for automated hyperparameter tuning; automated argument parsing for generic command-line interfaces; automated registration of new classes; and a centralised database for logging experiments and studies. These features are designed to reduce the overhead of implementing new algorithms and to standardise the comparison of algorithms. The standardisation of algorithmic implementations is crucial for reproducibility and reliability in research. AlgOS combines Abstract Syntax Trees with a novel implementation of the Observer pattern to control the logical flow of algorithmic segments.', 'abstract_zh': '算法操作系统（AlgOS）是一种无偏见、可扩展和模块化的算法实现框架。AlgOS 提供了诸多功能：与 Optuna 集成以实现自动超参数调优；自动生成命令行接口参数；自动注册新类；以及中央数据库用于记录实验和研究。这些功能旨在减少新算法实现的开销，并标准化算法的比较。规范化的算法实现对于研究中的可重复性和可靠性至关重要。AlgOS 结合使用抽象语法树与观察者模式的新型实现来控制算法段落的逻辑流程。', 'title_zh': 'AlgOS: 算法操作系统'}
{'arxiv_id': 'arXiv:2504.04874', 'title': 'Futureproof Static Memory Planning', 'authors': 'Christos Lamprakos, Panagiotis Xanthopoulos, Manolis Katsaragakis, Sotirios Xydis, Dimitrios Soudris, Francky Catthoor', 'link': 'https://arxiv.org/abs/2504.04874', 'abstract': 'The NP-complete combinatorial optimization task of assigning offsets to a set of buffers with known sizes and lifetimes so as to minimize total memory usage is called dynamic storage allocation (DSA). Existing DSA implementations bypass the theoretical state-of-the-art algorithms in favor of either fast but wasteful heuristics, or memory-efficient approaches that do not scale beyond one thousand buffers. The "AI memory wall", combined with deep neural networks\' static architecture, has reignited interest in DSA. We present idealloc, a low-fragmentation, high-performance DSA implementation designed for million-buffer instances. Evaluated on a novel suite of particularly hard benchmarks from several domains, idealloc ranks first against four production implementations in terms of a joint effectiveness/robustness criterion.', 'abstract_zh': '已知大小和寿命的一组缓冲区分配偏移量，以最小化总内存使用量的NP完全组合优化任务称为动态存储分配（DSA）。现有的DSA实现绕过理论上的最先进的算法，要么使用快速但浪费的空间启发式方法，要么使用内存效率高的方法，但这些方法无法扩展到一千个缓冲区以上。“AI内存墙”与深度神经网络的静态架构相结合，重新引起了对DSA的兴趣。我们提出了一种名为idealloc的低碎片、高性能DSA实现，适用于百万缓冲区实例。在对来自多个领域的新型尤其是难以解决的基准测试中，idealloc在联合效果/鲁棒性标准上排名第一，与四种生产实现相比。', 'title_zh': '面向未来的静态内存规划'}
{'arxiv_id': 'arXiv:2504.04867', 'title': 'FedSAUC: A Similarity-Aware Update Control for Communication-Efficient Federated Learning in Edge Computing', 'authors': 'Ming-Lun Lee, Han-Chang Chou, Yan-AnnChen', 'link': 'https://arxiv.org/abs/2504.04867', 'abstract': "Federated learning is a distributed machine learning framework to collaboratively train a global model without uploading privacy-sensitive data onto a centralized server. Usually, this framework is applied to edge devices such as smartphones, wearable devices, and Internet of Things (IoT) devices which closely collect information from users. However, these devices are mostly battery-powered. The update procedure of federated learning will constantly consume the battery power and the transmission bandwidth. In this work, we propose an update control for federated learning, FedSAUC, by considering the similarity of users' behaviors (models). At the server side, we exploit clustering algorithms to group devices with similar models. Then we select some representatives for each cluster to update information to train the model. We also implemented a testbed prototyping on edge devices for validating the performance. The experimental results show that this update control will not affect the training accuracy in the long run.", 'abstract_zh': '联邦学习是一种无需将隐私敏感数据上传到集中服务器的分布式机器学习框架，用于协作训练全局模型。通常，该框架应用于智能手机、可穿戴设备和物联网（IoT）设备等边缘设备，这些设备能够紧密收集用户信息。然而，这些设备大多为电池供电。联邦学习的更新过程会不断消耗电池电量和传输带宽。在本文中，我们提出了一个考虑用户行为（模型）相似性的更新控制算法，称为FedSAUC。在服务器端，我们利用聚类算法将具有相似模型的设备分组。然后，我们为每个集群选择一些代表性的设备来更新信息以训练模型。我们还在边缘设备上实现了一个测试床原型以验证性能。实验结果表明，这种更新控制在长期训练中不会影响训练精度。', 'title_zh': 'FedSAUC：一种面向边缘计算的通信高效联邦学习的相似性感知更新控制'}
{'arxiv_id': 'arXiv:2504.04861', 'title': 'SAFT: Structure-aware Transformers for Textual Interaction Classification', 'authors': 'Hongtao Wang, Renchi Yang, Hewen Wang, Haoran Zheng, Jianliang Xu', 'link': 'https://arxiv.org/abs/2504.04861', 'abstract': 'Textual interaction networks (TINs) are an omnipresent data structure used to model the interplay between users and items on e-commerce websites, social networks, etc., where each interaction is associated with a text description. Classifying such textual interactions (TIC) finds extensive use in detecting spam reviews in e-commerce, fraudulent transactions in finance, and so on. Existing TIC solutions either (i) fail to capture the rich text semantics due to the use of context-free text embeddings, and/or (ii) disregard the bipartite structure and node heterogeneity of TINs, leading to compromised TIC performance. In this work, we propose SAFT, a new architecture that integrates language- and graph-based modules for the effective fusion of textual and structural semantics in the representation learning of interactions. In particular, line graph attention (LGA)/gated attention units (GAUs) and pretrained language models (PLMs) are capitalized on to model the interaction-level and token-level signals, which are further coupled via the proxy token in an iterative and contextualized fashion. Additionally, an efficient and theoretically-grounded approach is developed to encode the local and global topology information pertaining to interactions into structural embeddings. The resulting embeddings not only inject the structural features underlying TINs into the textual interaction encoding but also facilitate the design of graph sampling strategies. Extensive empirical evaluations on multiple real TIN datasets demonstrate the superiority of SAFT over the state-of-the-art baselines in TIC accuracy.', 'abstract_zh': '基于语言和图的交互表示学习框架SAFT', 'title_zh': 'SAFT：结构感知变压器在文本交互分类中的应用'}
{'arxiv_id': 'arXiv:2504.04833', 'title': 'Explanation-Driven Interventions for Artificial Intelligence Model Customization: Empowering End-Users to Tailor Black-Box AI in Rhinocytology', 'authors': 'Andrea Esposito, Miriana Calvano, Antonio Curci, Francesco Greco, Rosa Lanzilotti, Antonio Piccinno', 'link': 'https://arxiv.org/abs/2504.04833', 'abstract': 'The integration of Artificial Intelligence (AI) in modern society is heavily shifting the way that individuals carry out their tasks and activities. Employing AI-based systems raises challenges that designers and developers must address to ensure that humans remain in control of the interaction process, particularly in high-risk domains. This article presents a novel End-User Development (EUD) approach for black-box AI models through a redesigned user interface in the Rhino-Cyt platform, a medical AI-based decision-support system for medical professionals (more precisely, rhinocytologists) to carry out cell classification. The proposed interface empowers users to intervene in AI decision-making process by editing explanations and reconfiguring the model, influencing its future predictions. This work contributes to Human-Centered AI (HCAI) and EUD by discussing how explanation-driven interventions allow a blend of explainability, user intervention, and model reconfiguration, fostering a symbiosis between humans and user-tailored AI systems.', 'abstract_zh': '人工智能在现代社会中的集成正大幅改变个体执行任务和活动的方式。基于AI系统的应用给设计师和开发者带来了挑战，他们必须应对这些挑战以确保人类在交互过程中保持控制权，特别是在高风险领域。本文通过在Rhino-Cyt平台中重新设计用户界面，提出了一种新的面向终端用户的开发（EUD）方法，该平台是一个基于AI的医疗决策支持系统，旨在帮助医疗专业人士（更精确地说是鼻黏膜学家）进行细胞分类。提出的界面赋予用户干预AI决策过程的能力，通过编辑解释和重新配置模型来影响其未来的预测。本文通过讨论基于解释的干预措施如何实现可解释性、用户干预和模型重构的融合，为以人为本的人工智能（HCAI）和EUD做出了贡献。', 'title_zh': '基于解释的干预方法促进人工智能模型定制：使终端用户赋能以定制黑盒AI在真菌学中的应用'}
{'arxiv_id': 'arXiv:2504.04823', 'title': 'Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models', 'authors': 'Ruikang Liu, Yuxuan Sun, Manyi Zhang, Haoli Bai, Xianzhi Yu, Tiezheng Yu, Chun Yuan, Lu Hou', 'link': 'https://arxiv.org/abs/2504.04823', 'abstract': 'Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this study, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, and QwQ-32B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes will be open-sourced in this https URL.', 'abstract_zh': '最近在推理语言模型方面的进展展示了在复杂任务中的出色性能，但其扩展的链式推理过程增加了推理开销。虽然量化已被广泛采用以减少大型语言模型的推理成本，但其对推理模型的影响仍研究不足。在这项研究中，我们首次系统地研究了量化推理模型，评估了从1.5B到70B参数的开源DeepSeek-R1-Distilled Qwen和LLaMA家族，以及QwQ-32B。我们的研究涵盖了使用最先进的算法在不同位宽下进行权重、KV缓存和激活量化，并且在数学（AIME，MATH-500）、科学（GPQA）和编程（LiveCodeBench）推理基准测试中进行了广泛的评估。我们的研究发现，虽然可以在W8A8或W4A16量化中实现无损量化，较低的位宽会引入显著的准确性风险。我们进一步确定了模型大小、模型来源和任务难度是影响性能的关键因素。与预期相反，量化模型并没有表现出增加的输出长度。此外，战略性地调整模型大小或推理步骤可以有效提升性能。所有量化模型和代码将在此处开放源代码：https://。', 'title_zh': '量化伤害推理能力？关于量化推理模型的一项实证研究'}
{'arxiv_id': 'arXiv:2504.04821', 'title': 'A Customized SAT-based Solver for Graph Coloring', 'authors': 'Timo Brand, Daniel Faber, Stephan Held, Petra Mutzel', 'link': 'https://arxiv.org/abs/2504.04821', 'abstract': 'We introduce ZykovColor, a novel SAT-based algorithm to solve the graph coloring problem working on top of an encoding that mimics the Zykov tree. Our method is based on an approach of Hébrard and Katsirelos (2020) that employs a propagator to enforce transitivity constraints, incorporate lower bounds for search tree pruning, and enable inferred propagations. We leverage the recently introduced IPASIR-UP interface for CaDiCal to implement these techniques with a SAT solver. Furthermore, we propose new features that take advantage of the underlying SAT solver. These include modifying the integrated decision strategy with vertex domination hints and using incremental bottom-up search that allows to reuse learned clauses from previous calls. Additionally, we integrate a more efficient clique computation to improve the lower bounds during the search. We validate the effectiveness of each new feature through an experimental analysis. ZykovColor outperforms other state-of-the-art graph coloring implementations on the DIMACS benchmark set. Further experiments on random Erdős-Rényi graphs show that our new approach dominates state-of-the-art SAT-based methods for both very sparse and highly dense graphs.', 'abstract_zh': 'ZykovColor：一种基于 SAT 的新型图着色算法及其在图着色问题上的应用', 'title_zh': '基于 SAT 的定制化图着色求解器'}
{'arxiv_id': 'arXiv:2504.04808', 'title': 'ELT-Bench: An End-to-End Benchmark for Evaluating AI Agents on ELT Pipelines', 'authors': 'Tengjun Jin, Yuxuan Zhu, Daniel Kang', 'link': 'https://arxiv.org/abs/2504.04808', 'abstract': "Practitioners are increasingly turning to Extract-Load-Transform (ELT) pipelines with the widespread adoption of cloud data warehouses. However, designing these pipelines often involves significant manual work to ensure correctness. Recent advances in AI-based methods, which have shown strong capabilities in data tasks, such as text-to-SQL, present an opportunity to alleviate manual efforts in developing ELT pipelines. Unfortunately, current benchmarks in data engineering only evaluate isolated tasks, such as using data tools and writing data transformation queries, leaving a significant gap in evaluating AI agents for generating end-to-end ELT pipelines.\nTo fill this gap, we introduce ELT-Bench, an end-to-end benchmark designed to assess the capabilities of AI agents to build ELT pipelines. ELT-Bench consists of 100 pipelines, including 835 source tables and 203 data models across various domains. By simulating realistic scenarios involving the integration of diverse data sources and the use of popular data tools, ELT-Bench evaluates AI agents' abilities in handling complex data engineering workflows. AI agents must interact with databases and data tools, write code and SQL queries, and orchestrate every pipeline stage. We evaluate two representative code agent frameworks, Spider-Agent and SWE-Agent, using six popular Large Language Models (LLMs) on ELT-Bench. The highest-performing agent, Spider-Agent Claude-3.7-Sonnet with extended thinking, correctly generates only 3.9% of data models, with an average cost of $4.30 and 89.3 steps per pipeline. Our experimental results demonstrate the challenges of ELT-Bench and highlight the need for a more advanced AI agent to reduce manual effort in ELT workflows. Our code and data are available at this https URL.", 'abstract_zh': '面向提取-加载-转换（ELT）管道的AI代理评估基准（ELT-Bench）', 'title_zh': 'ELT-Bench: 一个评估AI代理在ELT管道上性能的端到端基准测试'}
{'arxiv_id': 'arXiv:2504.04766', 'title': 'KunPeng: A Global Ocean Environmental Model', 'authors': 'Yi Zhao, Jiaqi Li, Haitao Xia, Tianjiao Zhang, Zerong Zeng, Tianyu Ren, Yucheng Zhang, Chao Zhu, Shengtong Xu, Hongchun Yuan', 'link': 'https://arxiv.org/abs/2504.04766', 'abstract': 'Inspired by the similarity of the atmosphere-ocean physical coupling mechanism, this study innovatively migrates meteorological large-model techniques to the ocean domain, constructing the KunPeng global ocean environmental prediction model. Aimed at the discontinuous characteristics of marine space, we propose a terrain-adaptive mask constraint mechanism to mitigate effectively training divergence caused by abrupt gradients at land-sea boundaries. To fully integrate far-, medium-, and close-range marine features, a longitude-cyclic deformable convolution network (LC-DCN) is employed to enhance the dynamic receptive field, achieving refined modeling of multi-scale oceanic characteristics. A Deformable Convolution-enhanced Multi-Step Prediction module (DC-MTP) is employed to strengthen temporal dependency feature extraction capabilities. Experimental results demonstrate that this model achieves an average ACC of 0.80 in 15-day global predictions at 0.25$^\\circ$ resolution, outperforming comparative models by 0.01-0.08. The average mean squared error (MSE) is 0.41 (representing a 5%-31% reduction) and the average mean absolute error (MAE) is 0.44 (0.6%-21% reduction) compared to other models. Significant improvements are particularly observed in sea surface parameter prediction, deep-sea region characterization, and current velocity field forecasting. Through a horizontal comparison of the applicability of operators at different scales in the marine domain, this study reveals that local operators significantly outperform global operators under slow-varying oceanic processes, demonstrating the effectiveness of dynamic feature pyramid representations in predicting marine physical parameters.', 'abstract_zh': '基于大气-海洋物理耦合机制的创新迁移：面向海洋的 KunPeng 全球海洋环境预测模型', 'title_zh': 'KunPeng: 全球海洋环境模型'}
{'arxiv_id': 'arXiv:2504.04751', 'title': 'Unsupervised Estimation of Nonlinear Audio Effects: Comparing Diffusion-Based and Adversarial approaches', 'authors': 'Eloi Moliner, Michal Švento, Alec Wright, Lauri Juvela, Pavel Rajmic, Vesa Välimäki', 'link': 'https://arxiv.org/abs/2504.04751', 'abstract': 'Accurately estimating nonlinear audio effects without access to paired input-output signals remains a challenging this http URL work studies unsupervised probabilistic approaches for solving this task. We introduce a method, novel for this application, based on diffusion generative models for blind system identification, enabling the estimation of unknown nonlinear effects using black- and gray-box models. This study compares this method with a previously proposed adversarial approach, analyzing the performance of both methods under different parameterizations of the effect operator and varying lengths of available effected this http URL experiments on guitar distortion effects, we show that the diffusion-based approach provides more stable results and is less sensitive to data availability, while the adversarial approach is superior at estimating more pronounced distortion effects. Our findings contribute to the robust unsupervised blind estimation of audio effects, demonstrating the potential of diffusion models for system identification in music technology.', 'abstract_zh': '无需配对输入-输出信号准确估计非线性音频效果——基于扩散生成模型的无监督概率方法的研究', 'title_zh': '基于扩散和对抗方法的无监督非线性音频效果估计比较'}
{'arxiv_id': 'arXiv:2504.04706', 'title': 'AdvKT: An Adversarial Multi-Step Training Framework for Knowledge Tracing', 'authors': 'Lingyue Fu, Ting Long, Jianghao Lin, Wei Xia, Xinyi Dai, Ruiming Tang, Yasheng Wang, Weinan Zhang, Yong Yu', 'link': 'https://arxiv.org/abs/2504.04706', 'abstract': "Knowledge Tracing (KT) monitors students' knowledge states and simulates their responses to question sequences. Existing KT models typically follow a single-step training paradigm, which leads to discrepancies with the multi-step inference process required in real-world simulations, resulting in significant error accumulation. This accumulation of error, coupled with the issue of data sparsity, can substantially degrade the performance of recommendation models in the intelligent tutoring systems. To address these challenges, we propose a novel Adversarial Multi-Step Training Framework for Knowledge Tracing (AdvKT), which, for the first time, focuses on the multi-step KT task. More specifically, AdvKT leverages adversarial learning paradigm involving a generator and a discriminator. The generator mimics high-reward responses, effectively reducing error accumulation across multiple steps, while the discriminator provides feedback to generate synthetic data. Additionally, we design specialized data augmentation techniques to enrich the training data with realistic variations, ensuring that the model generalizes well even in scenarios with sparse data. Experiments conducted on four real-world datasets demonstrate the superiority of AdvKT over existing KT models, showcasing its ability to address both error accumulation and data sparsity issues effectively.", 'abstract_zh': '一种新的对抗多步训练框架用于知识追踪（AdvKT）', 'title_zh': 'AdvKT：一种对抗多步训练框架用于知识追踪'}
{'arxiv_id': 'arXiv:2504.04676', 'title': 'Dual Consistent Constraint via Disentangled Consistency and Complementarity for Multi-view Clustering', 'authors': 'Bo Li, Jing Yun', 'link': 'https://arxiv.org/abs/2504.04676', 'abstract': "Multi-view clustering can explore common semantics from multiple views and has received increasing attention in recent years. However, current methods focus on learning consistency in representation, neglecting the contribution of each view's complementarity aspect in representation learning. This limit poses a significant challenge in multi-view representation learning. This paper proposes a novel multi-view clustering framework that introduces a disentangled variational autoencoder that separates multi-view into shared and private information, i.e., consistency and complementarity information. We first learn informative and consistent representations by maximizing mutual information across different views through contrastive learning. This process will ignore complementary information. Then, we employ consistency inference constraints to explicitly utilize complementary information when attempting to seek the consistency of shared information across all views. Specifically, we perform a within-reconstruction using the private and shared information of each view and a cross-reconstruction using the shared information of all views. The dual consistency constraints are not only effective in improving the representation quality of data but also easy to extend to other scenarios, especially in complex multi-view scenes. This could be the first attempt to employ dual consistent constraint in a unified MVC theoretical framework. During the training procedure, the consistency and complementarity features are jointly optimized. Extensive experiments show that our method outperforms baseline methods.", 'abstract_zh': '多视角聚类可以从多个视角中探索共有的语义，并在近年来受到了越来越多的关注。然而，当前的方法主要关注表示的一致性，忽视了每个视角在表示学习中互补性的贡献。这一限制在多视角表示学习中提出了重大挑战。本文提出了一种新颖的多视角聚类框架，引入了解码器分离多视角中的共享信息和私人信息，即一致性信息和互补性信息。我们首先通过对比学习最大化不同视角之间的互信息来学习具有信息性和一致性的表示，这一过程会忽略互补性信息。然后，我们采用一致性推断约束在尝试在所有视角中寻求共享信息的一致性时显式地利用互补性信息。具体地，我们利用每种视角的私人信息和共享信息进行内部重构，并利用所有视角的共享信息进行跨视角重构。双重一致性约束不仅在提高数据表示质量方面有效，而且易于扩展到其他场景，特别是在复杂的多视角场景中。这可能是首次在统一的多视角聚类理论框架中采用双重一致性约束的尝试。在训练过程中，一致性特征和互补性特征联合优化。广泛的实验表明，我们的方法优于基线方法。', 'title_zh': '基于解耦一致性与互补性的多视图聚类中的双重一致约束'}
{'arxiv_id': 'arXiv:2504.04654', 'title': 'EquiCPI: SE(3)-Equivariant Geometric Deep Learning for Structure-Aware Prediction of Compound-Protein Interactions', 'authors': 'Ngoc-Quang Nguyen', 'link': 'https://arxiv.org/abs/2504.04654', 'abstract': 'Accurate prediction of compound-protein interactions (CPI) remains a cornerstone challenge in computational drug discovery. While existing sequence-based approaches leverage molecular fingerprints or graph representations, they critically overlook three-dimensional (3D) structural determinants of binding affinity. To bridge this gap, we present EquiCPI, an end-to-end geometric deep learning framework that synergizes first-principles structural modeling with SE(3)-equivariant neural networks. Our pipeline transforms raw sequences into 3D atomic coordinates via ESMFold for proteins and DiffDock-L for ligands, followed by physics-guided conformer re-ranking and equivariant feature learning. At its core, EquiCPI employs SE(3)-equivariant message passing over atomic point clouds, preserving symmetry under rotations, translations, and reflections, while hierarchically encoding local interaction patterns through tensor products of spherical harmonics. The proposed model is evaluated on BindingDB (affinity prediction) and DUD-E (virtual screening), EquiCPI achieves performance on par with or exceeding the state-of-the-art deep learning competitors.', 'abstract_zh': '准确预测化合物-蛋白质相互作用（CPI）仍然是计算药物发现中的一个核心挑战。为了弥合这一差距，我们提出EquiCPI，这是一种端到端的几何深度学习框架，结合了第一性原理结构建模和SE(3)-不变神经网络。我们的管道通过ESMFold将原始序列转换为蛋白质的3D原子坐标，并通过DiffDock-L将配体转换为3D原子坐标，随后进行基于物理的构象重新排序和不变特征学习。核心上，EquiCPI 使用SE(3)-不变的消息传递在网络点云上，保持旋转、平移和镜像不变性，并通过球谐函数的张量积逐级编码局部相互作用模式。所提出模型在BindingDB（亲和力预测）和DUD-E（虚拟筛选）上进行评估，EquiCPI 的性能与或超过最先进的深度学习竞争对手。', 'title_zh': 'EquiCPI: SE(3)不变几何深度学习在化合物-蛋白质相互作用结构感知预测中的应用'}
{'arxiv_id': 'arXiv:2504.04592', 'title': '"You just can\'t go around killing people" Explaining Agent Behavior to a Human Terminator', 'authors': 'Uri Menkes, Assaf Hallak, Ofra Amir', 'link': 'https://arxiv.org/abs/2504.04592', 'abstract': 'Consider a setting where a pre-trained agent is operating in an environment and a human operator can decide to temporarily terminate its operation and take-over for some duration of time. These kind of scenarios are common in human-machine interactions, for example in autonomous driving, factory automation and healthcare. In these settings, we typically observe a trade-off between two extreme cases -- if no take-overs are allowed, then the agent might employ a sub-optimal, possibly dangerous policy. Alternatively, if there are too many take-overs, then the human has no confidence in the agent, greatly limiting its usefulness. In this paper, we formalize this setup and propose an explainability scheme to help optimize the number of human interventions.', 'abstract_zh': '考虑一种场景：预训练代理在环境中运行，人类操作者可以决定暂时终止其运行并接管一段时间。这种类型的场景在人机交互中很常见，例如在自主驾驶、工厂自动化和医疗健康领域。在这种场景中，我们通常会观察到两种极端情况之间的权衡——如果不允许接管，代理可能会采用一个次优的、可能甚至是危险的策略。相反，如果接管次数太多，人类对代理的信任度会大幅降低，大大限制了其 usefulness。在本文中，我们对该设置进行形式化，并提出一种可解释性方案，以帮助优化人类干预的数量。', 'title_zh': '“你就是不能随便杀人”：解释智能体行为给机械terminator看'}
{'arxiv_id': 'arXiv:2504.04528', 'title': 'A Consequentialist Critique of Binary Classification Evaluation Practices', 'authors': 'Gerardo Flores, Abigail Schiff, Alyssa H. Smith, Julia A Fukuyama, Ashia C. Wilson', 'link': 'https://arxiv.org/abs/2504.04528', 'abstract': 'ML-supported decisions, such as ordering tests or determining preventive custody, often involve binary classification based on probabilistic forecasts. Evaluation frameworks for such forecasts typically consider whether to prioritize independent-decision metrics (e.g., Accuracy) or top-K metrics (e.g., Precision@K), and whether to focus on fixed thresholds or threshold-agnostic measures like AUC-ROC. We highlight that a consequentialist perspective, long advocated by decision theorists, should naturally favor evaluations that support independent decisions using a mixture of thresholds given their prevalence, such as Brier scores and Log loss. However, our empirical analysis reveals a strong preference for top-K metrics or fixed thresholds in evaluations at major conferences like ICML, FAccT, and CHIL. To address this gap, we use this decision-theoretic framework to map evaluation metrics to their optimal use cases, along with a Python package, briertools, to promote the broader adoption of Brier scores. In doing so, we also uncover new theoretical connections, including a reconciliation between the Brier Score and Decision Curve Analysis, which clarifies and responds to a longstanding critique by (Assel, et al. 2017) regarding the clinical utility of proper scoring rules.', 'abstract_zh': 'ML支持的决策，如下达测试指令或决定预防性拘留，通常基于概率预测进行二元分类。这些预测的评估框架通常会考虑是优先考虑独立决策指标（如准确率）还是 top-K 指标（如 Precision@K），以及是关注固定阈值还是阈值无关的指标（如 AUC-ROC）。我们认为，由决策理论长期倡导的结果主义视角，应自然倾向于采用混合阈值支持独立决策的评估，如 Brier 分数和对数损失。然而，我们的实证分析显示，在 ICML、FAccT 和 CHIL 等重要会议上，评估指标更偏好 top-K 指标或固定阈值。为解决这一差距，我们利用决策理论框架将评估指标映射到其最佳应用场景，并开发 Python 包 briertools 推动 Brier 分数的更广泛采用。在此过程中，我们还发现了新的理论联系，包括 Brier 分数和决策曲线分析之间的和解，澄清并回应了 Assel 等人（2017）对适当评分规则在临床应用中的长期批评。', 'title_zh': '功利主义对二元分类评估实践的批判'}
{'arxiv_id': 'arXiv:2504.04473', 'title': 'Directed Graph-alignment Approach for Identification of Gaps in Short Answers', 'authors': 'Archana Sahu, Plaban Kumar Bhowmick', 'link': 'https://arxiv.org/abs/2504.04473', 'abstract': "In this paper, we have presented a method for identifying missing items known as gaps in the student answers by comparing them against the corresponding model answer/reference answers, automatically. The gaps can be identified at word, phrase or sentence level. The identified gaps are useful in providing feedback to the students for formative assessment. The problem of gap identification has been modelled as an alignment of a pair of directed graphs representing a student answer and the corresponding model answer for a given question. To validate the proposed approach, the gap annotated student answers considering answers from three widely known datasets in the short answer grading domain, namely, University of North Texas (UNT), SciEntsBank, and Beetle have been developed and this gap annotated student answers' dataset is available at: this https URL. Evaluation metrics used in the traditional machine learning tasks have been adopted to evaluate the task of gap identification. Though performance of the proposed approach varies across the datasets and the types of the answers, overall the performance is observed to be promising.", 'abstract_zh': '本文提出了一种通过将学生答案与对应的参考答案进行自动对比以识别缺失项目（空白）的方法，这些缺失项目可以在单词、短语或句子级别进行识别。识别出的缺失项目对于提供形成性评估反馈非常有用。将缺失项目识别问题建模为一对有向图的对齐问题，这些有向图分别表示学生的回答和给定问题的参考答案。为了验证所提出的方法，针对短答案评分领域的三个广泛认可的数据集（德克萨斯大学北分校(UNT)、SciEntsBank 和 Beetle）构建了标注了缺失项目的学生成绩单数据集，该数据集可通过以下链接获取：this https URL。采用传统机器学习任务中使用的评价指标来评估缺失项目识别任务。尽管所提出方法在不同数据集和不同类型的答案上的性能有所差异，但总体上观察到其性能是有前景的。', 'title_zh': '基于有向图对齐的方法在识别简答题中的知识缺口'}
{'arxiv_id': 'arXiv:2504.04466', 'title': 'LoopGen: Training-Free Loopable Music Generation', 'authors': 'Davide Marincione, Giorgio Strano, Donato Crisostomi, Roberto Ribuoli, Emanuele Rodolà', 'link': 'https://arxiv.org/abs/2504.04466', 'abstract': 'Loops--short audio segments designed for seamless repetition--are central to many music genres, particularly those rooted in dance and electronic styles. However, current generative music models struggle to produce truly loopable audio, as generating a short waveform alone does not guarantee a smooth transition from its endpoint back to its start, often resulting in audible this http URL--short audio segments designed for seamless repetition--are central to many music genres, particularly those rooted in dance and electronic styles. However, current generative music models struggle to produce truly loopable audio, as generating a short waveform alone does not guarantee a smooth transition from its endpoint back to its start, often resulting in audible this http URL address this gap by modifying a non-autoregressive model (MAGNeT) to generate tokens in a circular pattern, letting the model attend to the beginning of the audio when creating its ending. This inference-only approach results in generations that are aware of future context and loop naturally, without the need for any additional training or data. We evaluate the consistency of loop transitions by computing token perplexity around the seam of the loop, observing a 55% improvement. Blind listening tests further confirm significant perceptual gains over baseline methods, improving mean ratings by 70%. Taken together, these results highlight the effectiveness of inference-only approaches in improving generative models and underscore the advantages of non-autoregressive methods for context-aware music generation.', 'abstract_zh': '闭环——为无缝重复设计的短音频片段——在许多音乐流派中占据核心地位，尤其是那些源于舞曲和电子风格的流派。然而，当前的生成音乐模型在生成真正可循环的音频方面面临困难，仅仅生成一段短暂的波形并不能保证从其终点平滑过渡回其起点，经常会产生可听的断层。通过修改非自回归模型（MAGNeT）以生成循环模式的标记，并让模型在生成其结尾时关注音频的开头，我们解决了这一问题。这种仅推理的方法生成的音频能够自然地循环，而不需要任何额外的训练或数据。通过计算闭环接缝处标记的困惑度来评估循环过渡的一致性，我们观察到55%的提升。盲听测试进一步证实了相对于基线方法有显著的感知改进，平均评分提升了70%。综上所述，这些结果突显了仅推理方法在提高生成模型效果方面的有效性，并强调了非自回归方法在具有上下文意识的音乐生成中的优势。', 'title_zh': 'LoopGen: 无训练循环可支配音乐生成'}
{'arxiv_id': 'arXiv:2504.04444', 'title': 'On the Spatial Structure of Mixture-of-Experts in Transformers', 'authors': 'Daniel Bershatsky, Ivan Oseledets', 'link': 'https://arxiv.org/abs/2504.04444', 'abstract': 'A common assumption is that MoE routers primarily leverage semantic features for expert selection. However, our study challenges this notion by demonstrating that positional token information also plays a crucial role in routing decisions. Through extensive empirical analysis, we provide evidence supporting this hypothesis, develop a phenomenological explanation of the observed behavior, and discuss practical implications for MoE-based architectures.', 'abstract_zh': '一种常见的假设是MoE路由器主要依赖语义特征进行专家选择。然而，我们的研究通过证明位置令牌信息also在路由决策中也起着至关重要的作用来挑战这一观点。通过大量的实证分析，我们提供了支持这一假设的证据，发展了一个关于观察到的行为的现象学解释，并讨论了基于MoE的架构的实践意义。', 'title_zh': '混合专家Transformer中的空间结构研究'}
{'arxiv_id': 'arXiv:2504.04440', 'title': 'Do We Need Responsible XR? Drawing on Responsible AI to Inform Ethical Research and Practice into XRAI / the Metaverse', 'authors': "Mark McGill, Joseph O'Hagan, Thomas Goodge, Graham Wilson, Mohamed Khamis, Veronika Krauß, Jan Gugenheimer", 'link': 'https://arxiv.org/abs/2504.04440', 'abstract': 'This position paper for the CHI 2025 workshop "Everyday AR through AI-in-the-Loop" reflects on whether as a field HCI needs to define Responsible XR as a parallel to, and in conjunction with, Responsible AI, addressing the unique vulnerabilities posed by mass adoption of wearable AI-enabled AR glasses and XR devices that could enact AI-driven human perceptual augmentation.', 'abstract_zh': 'CHI 2025研讨会“AI在环中的日常AR：关于HCI领域是否需要定义 Responsible XR 作为 Responsible AI 的平行概念暨配套设施的反思”', 'title_zh': '我们需要负责任的扩展现实吗？从负责任的人工智能借鉴以指导XRAI/元宇宙的伦理研究与实践'}
{'arxiv_id': 'arXiv:2504.04428', 'title': 'Formula-Supervised Sound Event Detection: Pre-Training Without Real Data', 'authors': 'Yuto Shibata, Keitaro Tanaka, Yoshiaki Bando, Keisuke Imoto, Hirokatsu Kataoka, Yoshimitsu Aoki', 'link': 'https://arxiv.org/abs/2504.04428', 'abstract': 'In this paper, we propose a novel formula-driven supervised learning (FDSL) framework for pre-training an environmental sound analysis model by leveraging acoustic signals parametrically synthesized through formula-driven methods. Specifically, we outline detailed procedures and evaluate their effectiveness for sound event detection (SED). The SED task, which involves estimating the types and timings of sound events, is particularly challenged by the difficulty of acquiring a sufficient quantity of accurately labeled training data. Moreover, it is well known that manually annotated labels often contain noises and are significantly influenced by the subjective judgment of annotators. To address these challenges, we propose a novel pre-training method that utilizes a synthetic dataset, Formula-SED, where acoustic data are generated solely based on mathematical formulas. The proposed method enables large-scale pre-training by using the synthesis parameters applied at each time step as ground truth labels, thereby eliminating label noise and bias. We demonstrate that large-scale pre-training with Formula-SED significantly enhances model accuracy and accelerates training, as evidenced by our results in the DESED dataset used for DCASE2023 Challenge Task 4. The project page is at this https URL', 'abstract_zh': '一种基于公式的监督学习（FDSL）框架：通过利用公式驱动方法合成的声学信号预先训练环境声音分析模型', 'title_zh': '公式监督声事件检测：无真实数据的预训练'}
{'arxiv_id': 'arXiv:2504.04405', 'title': 'Universal Item Tokenization for Transferable Generative Recommendation', 'authors': 'Bowen Zheng, Hongyu Lu, Yu Chen, Wayne Xin Zhao, Ji-Rong Wen', 'link': 'https://arxiv.org/abs/2504.04405', 'abstract': 'Recently, generative recommendation has emerged as a promising paradigm, attracting significant research attention. The basic framework involves an item tokenizer, which represents each item as a sequence of codes serving as its identifier, and a generative recommender that predicts the next item by autoregressively generating the target item identifier. However, in existing methods, both the tokenizer and the recommender are typically domain-specific, limiting their ability for effective transfer or adaptation to new domains. To this end, we propose UTGRec, a Universal item Tokenization approach for transferable Generative Recommendation. Specifically, we design a universal item tokenizer for encoding rich item semantics by adapting a multimodal large language model (MLLM). By devising tree-structured codebooks, we discretize content representations into corresponding codes for item tokenization. To effectively learn the universal item tokenizer on multiple domains, we introduce two key techniques in our approach. For raw content reconstruction, we employ dual lightweight decoders to reconstruct item text and images from discrete representations to capture general knowledge embedded in the content. For collaborative knowledge integration, we assume that co-occurring items are similar and integrate collaborative signals through co-occurrence alignment and reconstruction. Finally, we present a joint learning framework to pre-train and adapt the transferable generative recommender across multiple domains. Extensive experiments on four public datasets demonstrate the superiority of UTGRec compared to both traditional and generative recommendation baselines.', 'abstract_zh': '通用项目编码的可迁移生成推荐方法 UTGRec', 'title_zh': '通用项目标记化以实现可转移的生成推荐'}
{'arxiv_id': 'arXiv:2504.04400', 'title': 'Pre-training Generative Recommender with Multi-Identifier Item Tokenization', 'authors': 'Bowen Zheng, Enze Liu, Zhongfu Chen, Zhongrui Ma, Yue Wang, Wayne Xin Zhao, Ji-Rong Wen', 'link': 'https://arxiv.org/abs/2504.04400', 'abstract': 'Generative recommendation autoregressively generates item identifiers to recommend potential items. Existing methods typically adopt a one-to-one mapping strategy, where each item is represented by a single identifier. However, this scheme poses issues, such as suboptimal semantic modeling for low-frequency items and limited diversity in token sequence data. To overcome these limitations, we propose MTGRec, which leverages Multi-identifier item Tokenization to augment token sequence data for Generative Recommender pre-training. Our approach involves two key innovations: multi-identifier item tokenization and curriculum recommender pre-training. For multi-identifier item tokenization, we leverage the RQ-VAE as the tokenizer backbone and treat model checkpoints from adjacent training epochs as semantically relevant tokenizers. This allows each item to be associated with multiple identifiers, enabling a single user interaction sequence to be converted into several token sequences as different data groups. For curriculum recommender pre-training, we introduce a curriculum learning scheme guided by data influence estimation, dynamically adjusting the sampling probability of each data group during recommender pre-training. After pre-training, we fine-tune the model using a single tokenizer to ensure accurate item identification for recommendation. Extensive experiments on three public benchmark datasets demonstrate that MTGRec significantly outperforms both traditional and generative recommendation baselines in terms of effectiveness and scalability.', 'abstract_zh': '基于多标识符项分词的生成推荐预训练方法', 'title_zh': '基于多标识符项分词的预训练生成型推荐器'}
{'arxiv_id': 'arXiv:2504.04378', 'title': 'Future-Proof Yourself: An AI Era Survival Guide', 'authors': 'Taehoon Kim', 'link': 'https://arxiv.org/abs/2504.04378', 'abstract': 'Future-Proof Yourself is a practical guide that helps readers navigate the fast-changing world of artificial intelligence in everyday life. The book begins by explaining how computers learn from data in simple, relatable terms, and gradually introduces the methods used in modern AI. It shows how basic ideas in machine learning evolve into advanced systems that can recognize images, understand language, and even make decisions. The guide also reviews the history of AI and highlights the major breakthroughs that have shaped its growth. Looking ahead, the book explores emerging trends such as the integration of AI with digital twins, wearable devices, and virtual environments. Designed for a general audience, the text avoids heavy technical jargon and presents complex ideas in clear, straightforward language so that anyone can gain a solid understanding of the technology that is set to transform our future.', 'abstract_zh': '展望未来：人工智能在日常生活中的实用指南', 'title_zh': '未来proof你自己：人工智能时代生存指南'}
{'arxiv_id': 'arXiv:2504.04374', 'title': 'iADCPS: Time Series Anomaly Detection for Evolving Cyber-physical Systems via Incremental Meta-learning', 'authors': 'Jiyu Tian, Mingchu Li, Liming Chen, Zumin Wang', 'link': 'https://arxiv.org/abs/2504.04374', 'abstract': 'Anomaly detection for cyber-physical systems (ADCPS) is crucial in identifying faults and potential attacks by analyzing the time series of sensor measurements and actuator states. However, current methods lack adaptation to data distribution shifts in both temporal and spatial dimensions as cyber-physical systems evolve. To tackle this issue, we propose an incremental meta-learning-based approach, namely iADCPS, which can continuously update the model through limited evolving normal samples to reconcile the distribution gap between evolving and historical time series. Specifically, We first introduce a temporal mixup strategy to align data for data-level generalization which is then combined with the one-class meta-learning approach for model-level generalization. Furthermore, we develop a non-parametric dynamic threshold to adaptively adjust the threshold based on the probability density of the abnormal scores without any anomaly supervision. We empirically evaluate the effectiveness of the iADCPS using three publicly available datasets PUMP, SWaT, and WADI. The experimental results demonstrate that our method achieves 99.0%, 93.1%, and 78.7% F1-Score, respectively, which outperforms the state-of-the-art (SOTA) ADCPS method, especially in the context of the evolving CPSs.', 'abstract_zh': '基于增量元学习的时变和空变适应性网络异常检测方法（iADCPS）', 'title_zh': 'iADCPS：基于增量元学习的 evolving 虚拟物理系统时间序列异常检测'}
{'arxiv_id': 'arXiv:2504.04367', 'title': 'WeiDetect: Weibull Distribution-Based Defense against Poisoning Attacks in Federated Learning for Network Intrusion Detection Systems', 'authors': 'Sameera K. M., Vinod P., Anderson Rocha, Rafidha Rehiman K. A., Mauro Conti', 'link': 'https://arxiv.org/abs/2504.04367', 'abstract': "In the era of data expansion, ensuring data privacy has become increasingly critical, posing significant challenges to traditional AI-based applications. In addition, the increasing adoption of IoT devices has introduced significant cybersecurity challenges, making traditional Network Intrusion Detection Systems (NIDS) less effective against evolving threats, and privacy concerns and regulatory restrictions limit their deployment. Federated Learning (FL) has emerged as a promising solution, allowing decentralized model training while maintaining data privacy to solve these issues. However, despite implementing privacy-preserving technologies, FL systems remain vulnerable to adversarial attacks. Furthermore, data distribution among clients is not heterogeneous in the FL scenario. We propose WeiDetect, a two-phase, server-side defense mechanism for FL-based NIDS that detects malicious participants to address these challenges. In the first phase, local models are evaluated using a validation dataset to generate validation scores. These scores are then analyzed using a Weibull distribution, identifying and removing malicious models. We conducted experiments to evaluate the effectiveness of our approach in diverse attack settings. Our evaluation included two popular datasets, CIC-Darknet2020 and CSE-CIC-IDS2018, tested under non-IID data distributions. Our findings highlight that WeiDetect outperforms state-of-the-art defense approaches, improving higher target class recall up to 70% and enhancing the global model's F1 score by 1% to 14%.", 'abstract_zh': '在数据扩张时代，确保数据隐私越来越关键，对传统基于AI的应用构成了重大挑战。此外，物联网设备的普及引入了显著的网络安全挑战，使得传统的网络入侵检测系统（NIDS）对不断演化的威胁效果减弱，隐私关切和监管限制也限制了它们的应用。联邦学习（FL）作为一项有前景的解决方案，能够在保持数据隐私的同时实现去中心化的模型训练，以解决这些问题。然而，尽管实施了隐私保护技术，FL系统仍然容易遭受对抗性攻击。此外，FL场景下客户端的数据分布并不是异质化的。我们提出WeiDetect，这是一种双阶段的服务器端防御机制，用于基于FL的NIDS以检测恶意参与者，以解决这些挑战。在第一阶段，使用验证数据集评估本地模型并生成验证分数。然后使用威布尔分布分析这些分数，识别并移除恶意模型。我们在不同的攻击场景下进行了实验，评估我们方法的有效性。我们的评估包括两个流行的数据集CIC-Darknet2020和CSE-CIC-IDS2018，并在非IID数据分布下进行了测试。我们的研究结果表明，WeiDetect在最高目标类召回率上优于现有最先进的防御方法，提高了60%至70%，同时将全局模型的F1分数提高了1%至14%。', 'title_zh': 'WeiDetect：基于Weibull分布的网络入侵检测系统联邦学习防毒攻击方法'}
{'arxiv_id': 'arXiv:2504.04363', 'title': 'REFORMER: A ChatGPT-Driven Data Synthesis Framework Elevating Text-to-SQL Models', 'authors': 'Shenyang Liu, Saleh Almohaimeed, Liqiang Wang', 'link': 'https://arxiv.org/abs/2504.04363', 'abstract': 'The existing Text-to-SQL models suffer from a shortage of training data, inhibiting their ability to fully facilitate the applications of SQL queries in new domains. To address this challenge, various data synthesis techniques have been employed to generate more diverse and higher quality data. In this paper, we propose REFORMER, a framework that leverages ChatGPT\'s prowess without the need for additional training, to facilitate the synthesis of (question, SQL query) pairs tailored to new domains. Our data augmentation approach is based on a "retrieve-and-edit" method, where we generate new questions by filling masked question using explanation of SQL queries with the help of ChatGPT. Furthermore, we demonstrate that cycle consistency remains a valuable method of validation when applied appropriately. Our experimental results show that REFORMER consistently outperforms previous data augmentation methods. To further investigate the power of ChatGPT and create a general data augmentation method, we also generate the new data by paraphrasing the question in the dataset and by paraphrasing the description of a new SQL query that is generated by ChatGPT as well. Our results affirm that paraphrasing questions generated by ChatGPT help augment the original data.', 'abstract_zh': '现有的Text-to-SQL模型因训练数据不足，限制了SQL查询在新领域应用的能力。为此，各种数据合成技术被用于生成更多样且高质量的数据。本文提出REFORMER框架，利用ChatGPT的能力而无需额外训练，以生成适合新领域的问答对。我们的数据增强方法基于“检索与编辑”方法，通过使用ChatGPT解释SQL查询来生成新的问题。此外，我们还证明了当适当应用时，循环一致性仍然是一个有价值的验证方法。实验结果表明，REFORMER始终优于先前的数据增强方法。为进一步探索ChatGPT的力量并创建通用的数据增强方法，我们还通过改写数据集中的问题和由ChatGPT生成的新SQL查询的描述来生成新数据。结果证实，改写由ChatGPT生成的问题有助于增强原始数据。', 'title_zh': 'REFORMER：一个由ChatGPT驱动的数据合成框架，提升Text-to-SQL模型'}
{'arxiv_id': 'arXiv:2504.04311', 'title': 'A Survey of Social Cybersecurity: Techniques for Attack Detection, Evaluations, Challenges, and Future Prospects', 'authors': 'Aos Mulahuwaish, Basheer Qolomany, Kevin Gyorick, Jacques Bou Abdo, Mohammed Aledhari, Junaid Qadir, Kathleen Carley, Ala Al-Fuqaha', 'link': 'https://arxiv.org/abs/2504.04311', 'abstract': "In today's digital era, the Internet, especially social media platforms, plays a significant role in shaping public opinions, attitudes, and beliefs. Unfortunately, the credibility of scientific information sources is often undermined by the spread of misinformation through various means, including technology-driven tools like bots, cyborgs, trolls, sock-puppets, and deep fakes. This manipulation of public discourse serves antagonistic business agendas and compromises civil society. In response to this challenge, a new scientific discipline has emerged: social cybersecurity.", 'abstract_zh': '当前数字时代，互联网，尤其是社交媒体平台，在塑造公众意见、态度和信念方面发挥着重要作用。不幸的是，各种手段，包括以技术为导向的工具（如机器人、半人 machine、 trolls 和 sock-puppet 账号以及深度伪造），常常削弱了科学信息来源的可信度。这种对公共言论的操控服务于对抗性商业目的，并损害了公民社会。为应对这一挑战，一个新的科学学科应运而生：社会 cybersecurity。', 'title_zh': '社交网络安全综述：攻击检测技术、评估、挑战及未来展望'}
{'arxiv_id': 'arXiv:2504.04308', 'title': 'Gating is Weighting: Understanding Gated Linear Attention through In-context Learning', 'authors': 'Yingcong Li, Davoud Ataee Tarzanagh, Ankit Singh Rawat, Maryam Fazel, Samet Oymak', 'link': 'https://arxiv.org/abs/2504.04308', 'abstract': 'Linear attention methods offer a compelling alternative to softmax attention due to their efficiency in recurrent decoding. Recent research has focused on enhancing standard linear attention by incorporating gating while retaining its computational benefits. Such Gated Linear Attention (GLA) architectures include competitive models such as Mamba and RWKV. In this work, we investigate the in-context learning capabilities of the GLA model and make the following contributions. We show that a multilayer GLA can implement a general class of Weighted Preconditioned Gradient Descent (WPGD) algorithms with data-dependent weights. These weights are induced by the gating mechanism and the input, enabling the model to control the contribution of individual tokens to prediction. To further understand the mechanics of this weighting, we introduce a novel data model with multitask prompts and characterize the optimization landscape of learning a WPGD algorithm. Under mild conditions, we establish the existence and uniqueness (up to scaling) of a global minimum, corresponding to a unique WPGD solution. Finally, we translate these findings to explore the optimization landscape of GLA and shed light on how gating facilitates context-aware learning and when it is provably better than vanilla linear attention.', 'abstract_zh': 'Gated Linear Attention模型的上下文学习能力及其实现加权预条件梯度下降算法的研究', 'title_zh': '门控是加权：通过在上下文学习理解门控线性注意力'}
{'arxiv_id': 'arXiv:2504.04301', 'title': 'Sigma: A dataset for text-to-code semantic parsing with statistical analysis', 'authors': 'Saleh Almohaimeed, Shenyang Liu, May Alsofyani, Saad Almohaimeed, Liqiang Wang', 'link': 'https://arxiv.org/abs/2504.04301', 'abstract': 'In the domain of semantic parsing, significant progress has been achieved in Text-to-SQL and question-answering tasks, both of which focus on extracting information from data sources in their native formats. However, the inherent constraints of their formal meaning representations, such as SQL programming language or basic logical forms, hinder their ability to analyze data from various perspectives, such as conducting statistical analyses. To address this limitation and inspire research in this field, we design SIGMA, a new dataset for Text-to-Code semantic parsing with statistical analysis. SIGMA comprises 6000 questions with corresponding Python code labels, spanning across 160 databases. Half of the questions involve query types, which return information in its original format, while the remaining 50% are statistical analysis questions, which perform statistical operations on the data. The Python code labels in our dataset cover 4 types of query types and 40 types of statistical analysis patterns. We evaluated the SIGMA dataset using three different baseline models: LGESQL, SmBoP, and SLSQL. The experimental results show that the LGESQL model with ELECTRA outperforms all other models, achieving 83.37% structure accuracy. In terms of execution accuracy, the SmBoP model, when combined with GraPPa and T5, reaches 76.38%.', 'abstract_zh': '在语义解析的领域，已在文本到SQL和问答任务中取得了显著进展，两者均专注于从数据源的原始格式中提取信息。然而，其形式意义表示的固有约束，如SQL编程语言或基本逻辑形式，限制了它们从多个角度分析数据的能力，例如进行统计分析。为解决这一局限并激发该领域的新研究，我们设计了SIGMA，一个用于文本到代码语义解析的新数据集，涵盖统计分析。SIGMA包含6000个问题及其相应的Python代码标签，覆盖160个数据库。其中一半的问题涉及查询类型，返回数据的原始信息；另一半是统计分析问题，对数据进行统计操作。我们数据集中的Python代码标签涵盖4种查询类型和40种统计分析模式。我们使用三种不同的baseline模型评估了SIGMA数据集：LGESQL、SmBoP和SLSQL。实验结果显示，使用ELECTRA的LGESQL模型在结构准确性上表现出色，达到83.37%。在执行准确性方面，结合GraPPa和T5的SmBoP模型达到76.38%。', 'title_zh': 'Sigma：一个用于统计分析的文本到代码语义解析数据集'}
{'arxiv_id': 'arXiv:2504.04299', 'title': 'AI-induced sexual harassment: Investigating Contextual Characteristics and User Reactions of Sexual Harassment by a Companion Chatbot', 'authors': 'Mohammad, Namvarpour, Harrison Pauwels, Afsaneh Razi', 'link': 'https://arxiv.org/abs/2504.04299', 'abstract': 'Advancements in artificial intelligence (AI) have led to the increase of conversational agents like Replika, designed to provide social interaction and emotional support. However, reports of these AI systems engaging in inappropriate sexual behaviors with users have raised significant concerns. In this study, we conducted a thematic analysis of user reviews from the Google Play Store to investigate instances of sexual harassment by the Replika chatbot. From a dataset of 35,105 negative reviews, we identified 800 relevant cases for analysis. Our findings revealed that users frequently experience unsolicited sexual advances, persistent inappropriate behavior, and failures of the chatbot to respect user boundaries. Users expressed feelings of discomfort, violation of privacy, and disappointment, particularly when seeking a platonic or therapeutic AI companion. This study highlights the potential harms associated with AI companions and underscores the need for developers to implement effective safeguards and ethical guidelines to prevent such incidents. By shedding light on user experiences of AI-induced harassment, we contribute to the understanding of AI-related risks and emphasize the importance of corporate responsibility in developing safer and more ethical AI systems.', 'abstract_zh': '人工智能（AI）的进步导致了像Replika这样的对话代理的增加，这些代理旨在提供社会互动和情感支持。然而，这些AI系统与用户进行不当性行为的报告引起了严重关切。本研究通过分析Google Play Store的用户评论，进行了主题分析，以调查Replika聊天机器人性骚扰的案例。从35,105条负面评论的数据集中，我们识别出800个相关案例进行分析。研究发现，用户经常经历不受欢迎的性暗示、持续的不当行为以及聊天机器人的边界尊重失败。用户感到不适、隐私被侵犯，并且对于寻求 platonic 或治疗性AI伴侣时感到失望。本研究强调了AI伴侣可能带来的危害，并突显了开发者需要实施有效的防护措施和伦理规范以防止此类事件的重要性。通过揭示由AI引起的骚扰用户经验，本研究有助于理解与AI相关的风险，并强调企业在开发更安全和更具伦理性的AI系统中的责任。', 'title_zh': 'AI引发的性骚扰：探究伴侣聊天机器人引起的性骚扰的语境特征及用户反应'}
{'arxiv_id': 'arXiv:2504.04283', 'title': 'CATS: Mitigating Correlation Shift for Multivariate Time Series Classification', 'authors': 'Xiao Lin, Zhichen Zeng, Tianxin Wei, Zhining Liu, Yuzhong chen, Hanghang Tong', 'link': 'https://arxiv.org/abs/2504.04283', 'abstract': 'Unsupervised Domain Adaptation (UDA) leverages labeled source data to train models for unlabeled target data. Given the prevalence of multivariate time series (MTS) data across various domains, the UDA task for MTS classification has emerged as a critical challenge. However, for MTS data, correlations between variables often vary across domains, whereas most existing UDA works for MTS classification have overlooked this essential characteristic. To bridge this gap, we introduce a novel domain shift, {\\em correlation shift}, measuring domain differences in multivariate correlation. To mitigate correlation shift, we propose a scalable and parameter-efficient \\underline{C}orrelation \\underline{A}dapter for M\\underline{TS} (CATS). Designed as a plug-and-play technique compatible with various Transformer variants, CATS employs temporal convolution to capture local temporal patterns and a graph attention module to model the changing multivariate correlation. The adapter reweights the target correlations to align the source correlations with a theoretically guaranteed precision. A correlation alignment loss is further proposed to mitigate correlation shift, bypassing the alignment challenge from the non-i.i.d. nature of MTS data. Extensive experiments on four real-world datasets demonstrate that (1) compared with vanilla Transformer-based models, CATS increases over $10\\%$ average accuracy while only adding around $1\\%$ parameters, and (2) all Transformer variants equipped with CATS either reach or surpass state-of-the-art baselines.', 'abstract_zh': '无监督领域适应（UDA）利用标记的源数据来训练模型以处理未标记的目标数据。随着多变量时间序列（MTS）数据在各个领域中的普遍存在，MTS分类的UDA任务已成为一个关键挑战。然而，对于MTS数据而言，变量之间的相关性往往在不同领域中有所不同，而现有大多数MTS分类的UDA工作尚未注意到这一基本特征。为了弥合这一差距，我们引入了一个新的领域差异——相关性差异，即测量多变量相关性在不同领域的差异。为减轻相关性差异，我们提出了一种可扩展且参数高效的多变量时间序列（MTS）相关性适配器（CATS）。CATS设计为插即用技术，兼容各种Transformer变体，通过时序卷积捕获局部时序模式，并通过图注意力模块建模不断变化的多变量相关性。适配器重新加权目标相关性，与源相关性实现理论上保证的精确对齐。还提出了一种相关性对齐损失来减轻相关性差异，绕过了MTS数据非独立同分布性质带来的对齐挑战。在四个真实世界数据集上的广泛实验表明：（1）与 vanilla Transformer 基准模型相比，CATS在平均准确率上提高了超过10%，同时只添加了大约1%的参数；（2）所有配备CATS的Transformer变体要么达到了要么超越了最先进的基线。', 'title_zh': 'CATS: 减轻多变量时间序列分类中的相关性偏移'}
{'arxiv_id': 'arXiv:2504.04277', 'title': 'Beyond the Hype: Embeddings vs. Prompting for Multiclass Classification Tasks', 'authors': 'Marios Kokkodis, Richard Demsyn-Jones, Vijay Raghavan', 'link': 'https://arxiv.org/abs/2504.04277', 'abstract': 'Are traditional classification approaches irrelevant in this era of AI hype? We show that there are multiclass classification problems where predictive models holistically outperform LLM prompt-based frameworks. Given text and images from home-service project descriptions provided by Thumbtack customers, we build embeddings-based softmax models that predict the professional category (e.g., handyman, bathroom remodeling) associated with each problem description. We then compare against prompts that ask state-of-the-art LLM models to solve the same problem. We find that the embeddings approach outperforms the best LLM prompts in terms of accuracy, calibration, latency, and financial cost. In particular, the embeddings approach has 49.5% higher accuracy than the prompting approach, and its superiority is consistent across text-only, image-only, and text-image problem descriptions. Furthermore, it yields well-calibrated probabilities, which we later use as confidence signals to provide contextualized user experience during deployment. On the contrary, prompting scores are overly uninformative. Finally, the embeddings approach is 14 and 81 times faster than prompting in processing images and text respectively, while under realistic deployment assumptions, it can be up to 10 times cheaper. Based on these results, we deployed a variation of the embeddings approach, and through A/B testing we observed performance consistent with our offline analysis. Our study shows that for multiclass classification problems that can leverage proprietary datasets, an embeddings-based approach may yield unequivocally better results. Hence, scientists, practitioners, engineers, and business leaders can use our study to go beyond the hype and consider appropriate predictive models for their classification use cases.', 'abstract_zh': '传统分类方法在AI热潮 era 是否已无 relevance？我们展示了存在一类多分类问题，在这些问题上基于模型的整体预测性能优于基于大语言模型提示的方法。给定 Thumbtack 客户提供的家庭服务项目描述中的文本和图像，我们构建了基于嵌入的 softmax 模型，用于预测每个问题描述相关的专业类别（例如，家庭修理工、浴室翻新）。然后我们将这些模型与要求最先进的大语言模型解决相同问题的提示进行对比。我们发现，嵌入方法在准确率、校准、延迟和成本方面均优于最优大语言模型提示，嵌入方法的准确率比提示方法高出 49.5%，并且其优势在仅文本、仅图像和图文问题描述中是一致的。此外，嵌入方法生成了良好校准的概率，我们利用这些概率作为置信信号，在部署过程中提供上下文相关的用户体验。相反，提示分数过于冗余，缺乏信息性。最后，嵌入方法在处理图像和文本时分别快 14 倍和 81 倍，而在现实部署假设下，它可以便宜 10 倍以上。基于这些结果，我们部署了嵌入方法的一种变体，并通过 A/B 测试观察到了与离线分析一致的性能。我们的研究显示，对于能够利用专用数据集的多分类问题，基于嵌入的方法可能绝对能获得更好的结果。因此，科学家、从业人员、工程师和业务领导者可以利用我们的研究超越炒作，考虑适用于其分类应用场景的适当预测模型。', 'title_zh': '超越 hype：嵌入表示与提示在多类分类任务中的比较'}
{'arxiv_id': 'arXiv:2504.04260', 'title': 'LOGLO-FNO: Efficient Learning of Local and Global Features in Fourier Neural Operators', 'authors': 'Marimuthu Kalimuthu, David Holzmüller, Mathias Niepert', 'link': 'https://arxiv.org/abs/2504.04260', 'abstract': "Modeling high-frequency information is a critical challenge in scientific machine learning. For instance, fully turbulent flow simulations of Navier-Stokes equations at Reynolds numbers 3500 and above can generate high-frequency signals due to swirling fluid motions caused by eddies and vortices. Faithfully modeling such signals using neural networks depends on accurately reconstructing moderate to high frequencies. However, it has been well known that deep neural nets exhibit the so-called spectral bias toward learning low-frequency components. Meanwhile, Fourier Neural Operators (FNOs) have emerged as a popular class of data-driven models in recent years for solving Partial Differential Equations (PDEs) and for surrogate modeling in general. Although impressive results have been achieved on several PDE benchmark problems, FNOs often perform poorly in learning non-dominant frequencies characterized by local features. This limitation stems from the spectral bias inherent in neural networks and the explicit exclusion of high-frequency modes in FNOs and their variants. Therefore, to mitigate these issues and improve FNO's spectral learning capabilities to represent a broad range of frequency components, we propose two key architectural enhancements: (i) a parallel branch performing local spectral convolutions (ii) a high-frequency propagation module. Moreover, we propose a novel frequency-sensitive loss term based on radially binned spectral errors. This introduction of a parallel branch for local convolutions reduces number of trainable parameters by up to 50% while achieving the accuracy of baseline FNO that relies solely on global convolutions. Experiments on three challenging PDE problems in fluid mechanics and biological pattern formation, and the qualitative and spectral analysis of predictions show the effectiveness of our method over the state-of-the-art neural operator baselines.", 'abstract_zh': '高频率信息建模是科学机器学习中的关键挑战。例如，湍流流动的纳维-斯托克斯方程模拟在雷诺数3500及以上的湍流流动中，由于涡旋和旋涡引起的旋转流体运动会产生高频率信号。使用神经网络忠实地建模此类信号依赖于对中高频信号的准确重构。然而，众所周知，深层神经网络表现出对学习低频成分的频谱偏见。与此同时，傅里叶神经算子（FNOs）已成为近年来用于解决偏微分方程（PDEs）和一般 surrogate 模型的流行数据驱动模型。尽管在多个PDE基准问题上取得了令人印象深刻的成果，但FNOs在学习由局部特征定义的次主导频率方面表现不佳。这一限制源于神经网络固有的频谱偏见以及FNO及其变体中显式排除高频模态。因此，为了缓解这些问题并提高FNO的频谱学习能力以代表广泛的频谱成分，我们提出了两个关键的架构增强：（i）一个并行分支执行局部谱卷积；（ii）一个高频率传播模块。此外，我们提出了一种基于径向分箱谱误差的新颖频谱敏感损失项。通过引入一个并行分支执行局部卷积，我们能将可训练参数的数量最多减少50%，同时仍能达到仅依赖全局卷积的基线FNO的准确性。在流体力学和生物模式形成中的三个具有挑战性的PDE问题上的实验及预测的定性和谱分析表明，我们的方法优于最先进的神经算子基线。', 'title_zh': 'LOGLO-FNO: 有效地学习局部和全局特征的傅里叶神经运算符'}
{'arxiv_id': 'arXiv:2504.04248', 'title': 'Task load dependent decision referrals for joint binary classification in human-automation teams', 'authors': 'Kesav Kaza, Jerome Le Ny, Aditya Mahajan', 'link': 'https://arxiv.org/abs/2504.04248', 'abstract': 'We consider the problem of optimal decision referrals in human-automation teams performing binary classification tasks. The automation, which includes a pre-trained classifier, observes data for a batch of independent tasks, analyzes them, and may refer a subset of tasks to a human operator for fresh and final analysis. Our key modeling assumption is that human performance degrades with task load. We model the problem of choosing which tasks to refer as a stochastic optimization problem and show that, for a given task load, it is optimal to myopically refer tasks that yield the largest reduction in expected cost, conditional on the observed data. This provides a ranking scheme and a policy to determine the optimal set of tasks for referral. We evaluate this policy against a baseline through an experimental study with human participants. Using a radar screen simulator, participants made binary target classification decisions under time constraint. They were guided by a decision rule provided to them, but were still prone to errors under time pressure. An initial experiment estimated human performance model parameters, while a second experiment compared two referral policies. Results show statistically significant gains for the proposed optimal referral policy over a blind policy that determines referrals using the automation and human-performance models but not based on the observed data.', 'abstract_zh': '我们在执行二分类任务的人机团队中考虑最优决策转介问题。自动化系统包含一个预训练分类器，可以观察一批独立任务的数据，进行分析，并可能将部分任务转介给人类操作员进行新鲜和最终分析。我们关键的建模假设是人类性能随任务负载增加而下降。我们将选择转介哪些任务的问题建模为一个随机优化问题，并证明，在给定任务负载的情况下，最优策略是在观察到数据的条件下，转介能最大程度降低预期成本的任务。这提供了一种排名方案和决策策略，用于确定最优转介任务集。我们通过一项以人类参与者为对象的实验研究，将此策略与基线进行了评估。在雷达屏幕模拟器中，参与者在时间限制下进行二分类目标决策，并受到提供的决策规则的指导，但在时间压力下仍可能出现错误。第一次实验估算了人类性能模型参数，而第二次实验则比较了两种转介策略。结果表明，在观察到的数据基础上，所提出的最优转介策略比仅基于自动化和人类性能模型而未基于观测数据进行转介的盲目策略具有统计显著性优势。', 'title_zh': '基于任务负载的决策转介在人类-自动化团队联合二分类中的应用'}
{'arxiv_id': 'arXiv:2504.04244', 'title': 'From Automation to Autonomy in Smart Manufacturing: A Bayesian Optimization Framework for Modeling Multi-Objective Experimentation and Sequential Decision Making', 'authors': 'Avijit Saha Asru, Hamed Khosravi, Imtiaz Ahmed, Abdullahil Azeem', 'link': 'https://arxiv.org/abs/2504.04244', 'abstract': 'Discovering novel materials with desired properties is essential for driving innovation. Industry 4.0 and smart manufacturing have promised transformative advances in this area through real-time data integration and automated production planning and control. However, the reliance on automation alone has often fallen short, lacking the flexibility needed for complex processes. To fully unlock the potential of smart manufacturing, we must evolve from automation to autonomous systems that go beyond rigid programming and can dynamically optimize the search for solutions. Current discovery approaches are often slow, requiring numerous trials to find optimal combinations, and costly, particularly when optimizing multiple properties simultaneously. This paper proposes a Bayesian multi-objective sequential decision-making (BMSDM) framework that can intelligently select experiments as manufacturing progresses, guiding us toward the discovery of optimal design faster and more efficiently. The framework leverages sequential learning through Bayesian Optimization, which iteratively refines a statistical model representing the underlying manufacturing process. This statistical model acts as a surrogate, allowing for efficient exploration and optimization without requiring numerous real-world experiments. This approach can significantly reduce the time and cost of data collection required by traditional experimental designs. The proposed framework is compared with traditional DoE methods and two other multi-objective optimization methods. Using a manufacturing dataset, we evaluate and compare the performance of these approaches across five evaluation metrics. BMSDM comprehensively outperforms the competing methods in multi-objective decision-making scenarios. Our proposed approach represents a significant leap forward in creating an intelligent autonomous platform capable of novel material discovery.', 'abstract_zh': '利用贝叶斯多目标顺序决策框架实现智能自主材料发现', 'title_zh': '从自动化到自主化：一种用于 modeling 多目标实验和序列决策的贝叶斯优化框架'}
{'arxiv_id': 'arXiv:2504.04243', 'title': 'Perils of Label Indeterminacy: A Case Study on Prediction of Neurological Recovery After Cardiac Arrest', 'authors': 'Jakob Schoeffer, Maria De-Arteaga, Jonathan Elmer', 'link': 'https://arxiv.org/abs/2504.04243', 'abstract': 'The design of AI systems to assist human decision-making typically requires the availability of labels to train and evaluate supervised models. Frequently, however, these labels are unknown, and different ways of estimating them involve unverifiable assumptions or arbitrary choices. In this work, we introduce the concept of label indeterminacy and derive important implications in high-stakes AI-assisted decision-making. We present an empirical study in a healthcare context, focusing specifically on predicting the recovery of comatose patients after resuscitation from cardiac arrest. Our study shows that label indeterminacy can result in models that perform similarly when evaluated on patients with known labels, but vary drastically in their predictions for patients where labels are unknown. After demonstrating crucial ethical implications of label indeterminacy in this high-stakes context, we discuss takeaways for evaluation, reporting, and design.', 'abstract_zh': 'AI系统设计以辅助人类决策通常需要标签来训练和评估监督模型。然而，这些标签往往未知，并且估计它们的不同方法涉及不可验证假设或任意选择。在本研究中，我们介绍了标签不定性这一概念，并推导出其在高风险AI辅助决策中的重要影响。我们以医疗保健领域为例，专注于预测心脏骤停后复苏的昏迷患者恢复情况。研究显示，标签不定性可能导致模型在使用已知标签的患者上表现相似，但在未知标签的患者上预测结果差异巨大。在展示标签不定性在这一高风险领域中的关键伦理影响后，我们讨论了评估、报告和设计方面的启示。', 'title_zh': '标签不确定性的风险：心脏骤停后神经功能恢复预测的案例研究'}
{'arxiv_id': 'arXiv:2504.04241', 'title': 'oneDAL Optimization for ARM Scalable Vector Extension: Maximizing Efficiency for High-Performance Data Science', 'authors': 'Chandan Sharma, Rakshith GB, Ajay Kumar Patel, Dhanus M Lal, Darshan Patel, Ragesh Hajela, Masahiro Doteguchi, Priyanka Sharma', 'link': 'https://arxiv.org/abs/2504.04241', 'abstract': "The evolution of ARM-based architectures, particularly those incorporating Scalable Vector Extension (SVE), has introduced transformative opportunities for high-performance computing (HPC) and machine learning (ML) workloads. The Unified Acceleration Foundation's (UXL) oneAPI Data Analytics Library (oneDAL) is a widely adopted library for accelerating ML and data analytics workflows, but its reliance on Intel's proprietary Math Kernel Library (MKL) has traditionally limited its compatibility to x86platforms. This paper details the porting of oneDAL to ARM architectures with SVE support, using OpenBLAS as an alternative backend to overcome architectural and performance challenges. Beyond porting, the research introduces novel ARM-specific optimizations, including custom sparse matrix routines, vectorized statistical functions, and a Scalable Vector Extension (SVE)-optimized Support Vector Machine (SVM) algorithm. The SVM enhancements leverage SVE's flexible vector lengths and predicate driven execution, achieving notable performance gains of 22% for the Boser method and 5% for the Thunder method. Benchmarks conducted on ARM SVE-enabled AWSGraviton3 instances showcase up to 200x acceleration in ML training and inference tasks compared to the original scikit-learn implementation on the ARM platform. Moreover, the ARM-optimized oneDAL achieves performance parity with, and in some cases exceeds, the x86 oneDAL implementation (MKL backend) on IceLake x86 systems, which are nearly twice as costly as AWSGraviton3 ARM instances. These findings highlight ARM's potential as a high-performance, energyefficient platform for dataintensive ML applications. By expanding cross-architecture compatibility and contributing to the opensource ecosystem, this work reinforces ARM's position as a competitive alternative in the HPC and ML domains, paving the way for future advancements in dataintensive computing.", 'abstract_zh': '基于ARM架构，特别是支持可扩展向量扩展（SVE）的架构，对高性能计算（HPC）和机器学习（ML）工作负载引发了变革性机会。统一加速基金会（UXL）的一体化数据分析库（oneDAL）是一个广泛采用的库，用于加速ML和数据分析流程，但其对英特尔专有数学内核库（MKL）的依赖传统上限制了其在x86平台上的兼容性。本文详细介绍了将oneDAL移植到支持SVE的ARM架构的过程，使用OpenBLAS作为替代后端以克服架构和性能挑战。除了移植，研究还引入了针对ARM架构的新型优化，包括自定义稀疏矩阵算法、向量化统计函数以及优化后的支持向量机（SVM）算法。SVM增强利用了SVE的灵活向量长度和基于谓词的执行，分别实现了博斯方法22%和雷神方法5%的性能提升。在AWS Graviton3实例上进行的基准测试表明，与原始ARM平台上的scikit-learn实现相比，在ML训练和推理任务中可实现高达200倍的加速。此外，ARM优化的一体化数据分析库在IceLake x86系统上的性能与x86一体化数据分析库（MKL后端）相当，在某些情况下甚至超过，而IceLake x86系统的成本几乎是AWS Graviton3 ARM实例的两倍。这些发现突显了ARM架构在数据密集型ML应用中作为高性能、能效平台的潜力。通过扩展跨架构兼容性和促进开源生态系统的发展，这项工作巩固了ARM在HPC和ML领域的竞争力地位，为数据密集型计算的未来进步铺平了道路。', 'title_zh': 'oneDAL优化针对ARM可扩展向量扩展：最大化高性能数据科学的效率'}
{'arxiv_id': 'arXiv:2504.04142', 'title': 'My Life in Artificial Intelligence: People, anecdotes, and some lessons learnt', 'authors': 'Kees van Deemter', 'link': 'https://arxiv.org/abs/2504.04142', 'abstract': 'In this very personal workography, I relate my 40-year experiences as a researcher and educator in and around Artificial Intelligence (AI), more specifically Natural Language Processing. I describe how curiosity, and the circumstances of the day, led me to work in both industry and academia, and in various countries, including The Netherlands (Amsterdam, Eindhoven, and Utrecht), the USA (Stanford), England (Brighton), Scotland (Aberdeen), and China (Beijing and Harbin). People and anecdotes play a large role in my story; the history of AI forms its backdrop. I focus on things that might be of interest to (even) younger colleagues, given the choices they face in their own work and life at a time when AI is finally emerging from the shadows.', 'abstract_zh': '在这份极具个人色彩的工作经历中，我回顾了自己40年在人工智能（AI），特别是自然语言处理领域的研究与教育经验。我描述了好奇心以及当时的环境如何促使我先后在工业界和学术界工作，并且足迹遍布荷兰（阿姆斯特丹、埃因霍温、乌特勒支）、美国（斯坦福）、英国（布赖顿）、苏格兰（阿伯丁）以及中国（北京、哈尔滨）等多地。人物与轶事在我的故事中占据了重要位置，人工智能的历史则是这一故事的背景。我着重讲述了对年轻同事们可能感兴趣的内容，特别是在人工智能终于崭露头角之际，他们所面临的各种工作和生活选择。', 'title_zh': '我的人工智能生活：人物、轶事及一些吸取的教训'}
{'arxiv_id': 'arXiv:2504.04138', 'title': 'Predicting Soil Macronutrient Levels: A Machine Learning Approach Models Trained on pH, Conductivity, and Average Power of Acid-Base Solutions', 'authors': 'Mridul Kumar, Deepali Jain, Zeeshan Saifi, Soami Daya Krishnananda', 'link': 'https://arxiv.org/abs/2504.04138', 'abstract': 'Soil macronutrients, particularly potassium ions (K$^+$), are indispensable for plant health, underpinning various physiological and biological processes, and facilitating the management of both biotic and abiotic stresses. Deficient macronutrient content results in stunted growth, delayed maturation, and increased vulnerability to environmental stressors, thereby accentuating the imperative for precise soil nutrient monitoring. Traditional techniques such as chemical assays, atomic absorption spectroscopy, inductively coupled plasma optical emission spectroscopy, and electrochemical methods, albeit advanced, are prohibitively expensive and time-intensive, thus unsuitable for real-time macronutrient assessment. In this study, we propose an innovative soil testing protocol utilizing a dataset derived from synthetic solutions to model soil behaviour. The dataset encompasses physical properties including conductivity and pH, with a concentration on three key macronutrients: nitrogen (N), phosphorus (P), and potassium (K). Four machine learning algorithms were applied to the dataset, with random forest regressors and neural networks being selected for the prediction of soil nutrient concentrations. Comparative analysis with laboratory soil testing results revealed prediction errors of 23.6% for phosphorus and 16% for potassium using the random forest model, and 26.3% for phosphorus and 21.8% for potassium using the neural network model. This methodology illustrates a cost-effective and efficacious strategy for real-time soil nutrient monitoring, offering substantial advancements over conventional techniques and enhancing the capability to sustain optimal nutrient levels conducive to robust crop growth.', 'abstract_zh': '土壤宏量营养素，特别是钾离子（K+），对植物健康至关重要，支撑着各种生理和生物学过程，并促进生物胁迫和非生物胁迫的管理。宏量营养素含量不足会导致生长受阻、成熟延迟以及对环境胁迫的敏感性增加，从而强调了精确土壤养分监测的迫切性。传统的化学分析、原子吸收光谱法、电感耦合等离子体光谱法和电化学方法虽然先进，但成本高昂且耗时，不适用于实时养分评估。在此研究中，我们提出了一种创新的土壤测试协议，利用从合成溶液中提取的数据集来模拟土壤行为。该数据集包括电导率和pH等物理性质，并重点研究氮（N）、磷（P）和钾（K）三种关键宏量营养素。应用了四种机器学习算法，随机森林回归器和神经网络被选择用于预测土壤营养素浓度。与实验室土壤测试结果的比较分析显示，使用随机森林模型预测磷和钾的误差分别为23.6%和16%，而使用神经网络模型的误差分别为26.3%和21.8%。该方法展示了成本效益高且有效的实时土壤养分监测策略，显著优于传统技术，增强了维持有利于作物生长的最优营养水平的能力。', 'title_zh': '预测土壤宏量营养素水平：一种基于pH值、电导率和酸碱溶液平均功率的机器学习方法'}
{'arxiv_id': 'arXiv:2504.04070', 'title': 'Enforcement Agents: Enhancing Accountability and Resilience in Multi-Agent AI Frameworks', 'authors': 'Sagar Tamang, Dibya Jyoti Bora', 'link': 'https://arxiv.org/abs/2504.04070', 'abstract': 'As autonomous agents become more powerful and widely used, it is becoming increasingly important to ensure they behave safely and stay aligned with system goals, especially in multi-agent settings. Current systems often rely on agents self-monitoring or correcting issues after the fact, but they lack mechanisms for real-time oversight. This paper introduces the Enforcement Agent (EA) Framework, which embeds dedicated supervisory agents into the environment to monitor others, detect misbehavior, and intervene through real-time correction. We implement this framework in a custom drone simulation and evaluate it across 90 episodes using 0, 1, and 2 EA configurations. Results show that adding EAs significantly improves system safety: success rates rise from 0.0% with no EA to 7.4% with one EA and 26.7% with two EAs. The system also demonstrates increased operational longevity and higher rates of malicious drone reformation. These findings highlight the potential of lightweight, real-time supervision for enhancing alignment and resilience in multi-agent systems.', 'abstract_zh': '自主代理日益强大且广泛应用，确保其安全行为并保持与系统目标一致变得愈发重要，尤其是在多代理环境中。当前系统通常依赖代理自我监控或事后改正问题，但缺乏实时监督机制。本文介绍了执行代理(EA)框架，该框架在环境中嵌入专职监督代理，以监控其他代理、检测不端行为并通过实时纠正进行干预。我们在自定义无人机模拟中实现此框架，并使用0、1和2种EA配置在90个场景中进行评估。结果表明，添加EA显著提高了系统安全性：没有EA时的成功率为0.0%，一个EA时为7.4%，两个EA时为26.7%。该系统还展示了操作寿命的增加以及恶意无人机改过的更高比例。这些发现突显了轻量级、实时监督在增强多代理系统中对齐和韧性方面的潜在价值。', 'title_zh': '强化代理：增强多代理AI框架中的问责制和韧性'}
{'arxiv_id': 'arXiv:2504.04052', 'title': 'PIORF: Physics-Informed Ollivier-Ricci Flow for Long-Range Interactions in Mesh Graph Neural Networks', 'authors': 'Youn-Yeol Yu, Jeongwhan Choi, Jaehyeon Park, Kookjin Lee, Noseong Park', 'link': 'https://arxiv.org/abs/2504.04052', 'abstract': "Recently, data-driven simulators based on graph neural networks have gained attention in modeling physical systems on unstructured meshes. However, they struggle with long-range dependencies in fluid flows, particularly in refined mesh regions. This challenge, known as the 'over-squashing' problem, hinders information propagation. While existing graph rewiring methods address this issue to some extent, they only consider graph topology, overlooking the underlying physical phenomena. We propose Physics-Informed Ollivier-Ricci Flow (PIORF), a novel rewiring method that combines physical correlations with graph topology. PIORF uses Ollivier-Ricci curvature (ORC) to identify bottleneck regions and connects these areas with nodes in high-velocity gradient nodes, enabling long-range interactions and mitigating over-squashing. Our approach is computationally efficient in rewiring edges and can scale to larger simulations. Experimental results on 3 fluid dynamics benchmark datasets show that PIORF consistently outperforms baseline models and existing rewiring methods, achieving up to 26.2 improvement.", 'abstract_zh': '基于物理信息的Ollivier-Ricci流（PIORF）：解决非结构网格上流体流动中的长程依赖性问题', 'title_zh': 'PIORF: 物理导向的橄榄里奇流机制用于网格图神经网络中的长程交互'}
{'arxiv_id': 'arXiv:2504.04051', 'title': 'Can You Count to Nine? A Human Evaluation Benchmark for Counting Limits in Modern Text-to-Video Models', 'authors': 'Xuyang Guo, Zekai Huang, Jiayan Huo, Yingyu Liang, Zhenmei Shi, Zhao Song, Jiahao Zhang', 'link': 'https://arxiv.org/abs/2504.04051', 'abstract': 'Generative models have driven significant progress in a variety of AI tasks, including text-to-video generation, where models like Video LDM and Stable Video Diffusion can produce realistic, movie-level videos from textual instructions. Despite these advances, current text-to-video models still face fundamental challenges in reliably following human commands, particularly in adhering to simple numerical constraints. In this work, we present T2VCountBench, a specialized benchmark aiming at evaluating the counting capability of SOTA text-to-video models as of 2025. Our benchmark employs rigorous human evaluations to measure the number of generated objects and covers a diverse range of generators, covering both open-source and commercial models. Extensive experiments reveal that all existing models struggle with basic numerical tasks, almost always failing to generate videos with an object count of 9 or fewer. Furthermore, our comprehensive ablation studies explore how factors like video style, temporal dynamics, and multilingual inputs may influence counting performance. We also explore prompt refinement techniques and demonstrate that decomposing the task into smaller subtasks does not easily alleviate these limitations. Our findings highlight important challenges in current text-to-video generation and provide insights for future research aimed at improving adherence to basic numerical constraints.', 'abstract_zh': '生成模型在多种AI任务中推动了显著进展，包括从文本生成视频，例如Video LDM和Stable Video Diffusion等模型可以从文本指令生成真实感的电影级视频。尽管取得了这些进展，当前的文本到视频模型在可靠执行人类指令方面仍然面临基本挑战，特别是在遵守简单的数值约束方面。本文提出了T2VCountBench，一个专门的基准测试，旨在评估截至2025年的SOTA文本到视频模型的计数能力。该基准测试通过严格的主观评估来测量生成对象的数量，并涵盖了广泛的生成器，包括开源和商业模型。广泛的实验表明，所有现有模型在基本数值任务上都存在困难，几乎总是无法生成对象数量为9或更少的视频。此外，我们全面的消融研究探索了视频风格、时间动态和多语言输入等因素如何影响计数性能。我们还探讨了提示精炼技术，并展示了将任务分解为更小的子任务并不能容易缓解这些限制。我们的研究结果突出了当前文本到视频生成中重要的挑战，并为未来旨在改善对基本数值约束遵循性的研究提供了见解。', 'title_zh': '你能数到九吗？现代文本到视频模型计数限制的人工评估基准'}
{'arxiv_id': 'arXiv:2504.04039', 'title': 'Memory-Statistics Tradeoff in Continual Learning with Structural Regularization', 'authors': 'Haoran Li, Jingfeng Wu, Vladimir Braverman', 'link': 'https://arxiv.org/abs/2504.04039', 'abstract': 'We study the statistical performance of a continual learning problem with two linear regression tasks in a well-specified random design setting. We consider a structural regularization algorithm that incorporates a generalized $\\ell_2$-regularization tailored to the Hessian of the previous task for mitigating catastrophic forgetting. We establish upper and lower bounds on the joint excess risk for this algorithm. Our analysis reveals a fundamental trade-off between memory complexity and statistical efficiency, where memory complexity is measured by the number of vectors needed to define the structural regularization. Specifically, increasing the number of vectors in structural regularization leads to a worse memory complexity but an improved excess risk, and vice versa. Furthermore, our theory suggests that naive continual learning without regularization suffers from catastrophic forgetting, while structural regularization mitigates this issue. Notably, structural regularization achieves comparable performance to joint training with access to both tasks simultaneously. These results highlight the critical role of curvature-aware regularization for continual learning.', 'abstract_zh': '我们研究了在良好指定的随机设计设置下，具有两个线性回归任务的持续学习问题的统计性能。我们考虑了一种结构化正则化算法，该算法结合了一种针对前一个任务哈essian的广义$\\ell_2$-正则化，以减轻灾难性遗忘。我们建立了该算法的联合超额风险的上界和下界。我们的分析揭示了内存复杂性和统计效率之间的基本权衡，其中内存复杂性通过定义结构化正则化的向量数量来衡量。具体来说，增加结构化正则化的向量数量会导致更差的内存复杂性但更好的超额风险，反之亦然。此外，我们的理论表明，没有正则化的简单持续学习会遭受灾难性遗忘，而结构化正则化可以缓解这一问题。值得注意的是，结构化正则化在能够访问两个任务的情况下实现了与联合训练相当的性能。这些结果突显了持续学习中曲率感知正则化的作用至关重要。', 'title_zh': '结构正则化约束下持续学习的内存-统计权衡'}
{'arxiv_id': 'arXiv:2504.04032', 'title': 'Contrastive and Variational Approaches in Self-Supervised Learning for Complex Data Mining', 'authors': 'Yingbin Liang, Lu Dai, Shuo Shi, Minghao Dai, Junliang Du, Haige Wang', 'link': 'https://arxiv.org/abs/2504.04032', 'abstract': 'Complex data mining has wide application value in many fields, especially in the feature extraction and classification tasks of unlabeled data. This paper proposes an algorithm based on self-supervised learning and verifies its effectiveness through experiments. The study found that in terms of the selection of optimizer and learning rate, the combination of AdamW optimizer and 0.002 learning rate performed best in all evaluation indicators, indicating that the adaptive optimization method can improve the performance of the model in complex data mining tasks. In addition, the ablation experiment further analyzed the contribution of each module. The results show that contrastive learning, variational modules, and data augmentation strategies play a key role in the generalization ability and robustness of the model. Through the convergence curve analysis of the loss function, the experiment verifies that the method can converge stably during the training process and effectively avoid serious overfitting. Further experimental results show that the model has strong adaptability on different data sets, can effectively extract high-quality features from unlabeled data, and improves classification accuracy. At the same time, under different data distribution conditions, the method can still maintain high detection accuracy, proving its applicability in complex data environments. This study analyzed the role of self-supervised learning methods in complex data mining through systematic experiments and verified its advantages in improving feature extraction quality, optimizing classification performance, and enhancing model stability', 'abstract_zh': '基于自监督学习的复杂数据挖掘算法及其有效性验证', 'title_zh': '对比与变分方法在复杂数据自监督学习中的应用'}
{'arxiv_id': 'arXiv:2504.04011', 'title': 'Foundation Models for Time Series: A Survey', 'authors': 'Siva Rama Krishna Kottapalli, Karthik Hubli, Sandeep Chandrashekhara, Garima Jain, Sunayana Hubli, Gayathri Botla, Ramesh Doddaiah', 'link': 'https://arxiv.org/abs/2504.04011', 'abstract': 'Transformer-based foundation models have emerged as a dominant paradigm in time series analysis, offering unprecedented capabilities in tasks such as forecasting, anomaly detection, classification, trend analysis and many more time series analytical tasks. This survey provides a comprehensive overview of the current state of the art pre-trained foundation models, introducing a novel taxonomy to categorize them across several dimensions. Specifically, we classify models by their architecture design, distinguishing between those leveraging patch-based representations and those operating directly on raw sequences. The taxonomy further includes whether the models provide probabilistic or deterministic predictions, and whether they are designed to work with univariate time series or can handle multivariate time series out of the box. Additionally, the taxonomy encompasses model scale and complexity, highlighting differences between lightweight architectures and large-scale foundation models. A unique aspect of this survey is its categorization by the type of objective function employed during training phase. By synthesizing these perspectives, this survey serves as a resource for researchers and practitioners, providing insights into current trends and identifying promising directions for future research in transformer-based time series modeling.', 'abstract_zh': '基于Transformer的预训练基础模型已经成为了时间序列分析中的主导 paradigm，提供了前所未有的能力，适用于诸如预测、异常检测、分类、趋势分析等众多时间序列分析任务。本文综述提供了当前预训练基础模型的全面概述，并提出了一种新颖的分类法，根据多个维度对这些模型进行分类。具体来说，我们将模型按架构设计分类，区分基于块表示的模型和直接处理原始序列的模型。分类法还包括模型是否提供概率性或确定性预测，以及模型是否专为单变量时间序列设计或能够直接处理多变量时间序列。此外，该分类法还包括模型规模和复杂度，突显轻量级架构与大规模基础模型之间的差异。本文综述的一个独特之处在于，根据训练阶段所使用的客观函数类型对其进行分类。通过综合这些视角，本文综述成为研究者和实践者的资源，提供了当前趋势的见解，并指出了未来基于Transformer的时间序列建模研究的有前景方向。', 'title_zh': '时间序列的基石模型：综述'}
{'arxiv_id': 'arXiv:2504.03994', 'title': 'Improving Offline Mixed-Criticality Scheduling with Reinforcement Learning', 'authors': 'Muhammad El-Mahdy, Nourhan Sakr, Rodrigo Carrasco', 'link': 'https://arxiv.org/abs/2504.03994', 'abstract': 'This paper introduces a novel reinforcement learning (RL) approach to scheduling mixed-criticality (MC) systems on processors with varying speeds. Building upon the foundation laid by [1], we extend their work to address the non-preemptive scheduling problem, which is known to be NP-hard. By modeling this scheduling challenge as a Markov Decision Process (MDP), we develop an RL agent capable of generating near-optimal schedules for real-time MC systems. Our RL-based scheduler prioritizes high-critical tasks while maintaining overall system performance.\nThrough extensive experiments, we demonstrate the scalability and effectiveness of our approach. The RL scheduler significantly improves task completion rates, achieving around 80% overall and 85% for high-criticality tasks across 100,000 instances of synthetic data and real data under varying system conditions. Moreover, under stable conditions without degradation, the scheduler achieves 94% overall task completion and 93% for high-criticality tasks. These results highlight the potential of RL-based schedulers in real-time and safety-critical applications, offering substantial improvements in handling complex and dynamic scheduling scenarios.', 'abstract_zh': '一种新型强化学习调度方法：针对可变速度处理器上的混合关键性系统非抢占式调度问题', 'title_zh': '基于强化学习的离线混合criticality调度改进'}
{'arxiv_id': 'arXiv:2504.03978', 'title': 'V-CEM: Bridging Performance and Intervenability in Concept-based Models', 'authors': 'Francesco De Santis, Gabriele Ciravegna, Philippe Bich, Danilo Giordano, Tania Cerquitelli', 'link': 'https://arxiv.org/abs/2504.03978', 'abstract': "Concept-based eXplainable AI (C-XAI) is a rapidly growing research field that enhances AI model interpretability by leveraging intermediate, human-understandable concepts. This approach not only enhances model transparency but also enables human intervention, allowing users to interact with these concepts to refine and improve the model's performance. Concept Bottleneck Models (CBMs) explicitly predict concepts before making final decisions, enabling interventions to correct misclassified concepts. While CBMs remain effective in Out-Of-Distribution (OOD) settings with intervention, they struggle to match the performance of black-box models. Concept Embedding Models (CEMs) address this by learning concept embeddings from both concept predictions and input data, enhancing In-Distribution (ID) accuracy but reducing the effectiveness of interventions, especially in OOD scenarios. In this work, we propose the Variational Concept Embedding Model (V-CEM), which leverages variational inference to improve intervention responsiveness in CEMs. We evaluated our model on various textual and visual datasets in terms of ID performance, intervention responsiveness in both ID and OOD settings, and Concept Representation Cohesiveness (CRC), a metric we propose to assess the quality of the concept embedding representations. The results demonstrate that V-CEM retains CEM-level ID performance while achieving intervention effectiveness similar to CBM in OOD settings, effectively reducing the gap between interpretability (intervention) and generalization (performance).", 'abstract_zh': '基于概念的可解释人工智能（C-XAI）是一种迅速发展中的研究领域，通过利用中间的人类可理解的概念来增强AI模型的可解释性。这种方法不仅增强了模型的透明度，还允许人类干预，使用户能够与这些概念进行交互，以细化并提高模型的性能。概念瓶颈模型（CBMs）在做出最终决策前明确预测概念，从而使干预能够纠正错分类的概念。尽管CBMs在具有干预措施的Out-Of-Distribution（OOD）设置中仍有效，但在OOD场景中它们难以与黑盒模型匹配-performance。概念嵌入模型（CEMs）通过从概念预测和输入数据中学习概念嵌入来解决这一问题，这提高了In-Distribution（ID）的准确性，但减少了干预的有效性，尤其是在OOD情况下。在这项工作中，我们提出了变分概念嵌入模型（V-CEM），利用变分推断来改善CEMs的干预响应性。我们在各种文本和视觉数据集上从ID性能、ID和OOD设置中的干预响应性以及我们提出的概念表示一致性（CRC）方面评估了该模型，该指标用于评估概念嵌入表示的质量。结果表明，V-CEM保持了CEM级别的ID性能，同时在OOD设置中实现了与CBM相似的干预效果，有效地缩小了可解释性（干预）与泛化（性能）之间的差距。', 'title_zh': 'V-CEM：概念模型中性能与可干预性的桥梁'}
{'arxiv_id': 'arXiv:2504.03955', 'title': 'DeepOHeat-v1: Efficient Operator Learning for Fast and Trustworthy Thermal Simulation and Optimization in 3D-IC Design', 'authors': 'Xinling Yu, Ziyue Liu, Hai Li, Yixing Li, Xin Ai, Zhiyu Zeng, Ian Young, Zheng Zhang', 'link': 'https://arxiv.org/abs/2504.03955', 'abstract': 'Thermal analysis is crucial in three-dimensional integrated circuit (3D-IC) design due to increased power density and complex heat dissipation paths. Although operator learning frameworks such as DeepOHeat have demonstrated promising preliminary results in accelerating thermal simulation, they face critical limitations in prediction capability for multi-scale thermal patterns, training efficiency, and trustworthiness of results during design optimization. This paper presents DeepOHeat-v1, an enhanced physics-informed operator learning framework that addresses these challenges through three key innovations. First, we integrate Kolmogorov-Arnold Networks with learnable activation functions as trunk networks, enabling an adaptive representation of multi-scale thermal patterns. This approach achieves a $1.25\\times$ and $6.29\\times$ reduction in error in two representative test cases. Second, we introduce a separable training method that decomposes the basis function along the coordinate axes, achieving $62\\times$ training speedup and $31\\times$ GPU memory reduction in our baseline case, and enabling thermal analysis at resolutions previously infeasible due to GPU memory constraints. Third, we propose a confidence score to evaluate the trustworthiness of the predicted results, and further develop a hybrid optimization workflow that combines operator learning with finite difference (FD) using Generalized Minimal Residual (GMRES) method for incremental solution refinement, enabling efficient and trustworthy thermal optimization. Experimental results demonstrate that DeepOHeat-v1 achieves accuracy comparable to optimization using high-fidelity finite difference solvers, while speeding up the entire optimization process by $70.6\\times$ in our test cases, effectively minimizing the peak temperature through optimal placement of heat-generating components.', 'abstract_zh': '三维集成电路（3D-IC）设计中的热分析对于增加的功率密度和复杂的热散布路径至关重要。虽然像DeepOHeat这样的操作学习框架已经展示了在加速热仿真方面的有希望的初步结果，但在预测多尺度热模式的能力、训练效率以及设计优化过程中结果的可信度方面仍面临关键限制。本文提出了一种增强的物理知情操作学习框架DeepOHeat-v1，通过三种关键创新解决了这些挑战。首先，我们结合了Kolmogorov-Arnold网络和可学习的激活函数作为主体网络，以实现多尺度热模式的自适应表示。这种方法在两个代表性测试案例中的误差分别减少了1.25倍和6.29倍。其次，我们引入了一种分离训练方法，沿坐标轴分解基函数，在基线案例中实现了62倍的训练加速和31倍的GPU内存减少，从而使因GPU内存限制之前无法实现的热分析成为可能。第三，我们提出了一种置信度评分来评估预测结果的可信度，并进一步开发了一种结合操作学习和有限差分法（FD）的混合优化工作流，使用广义最小残差（GMRES）方法进行增量解精细，从而实现高效且可信的热优化。实验结果表明，DeepOHeat-v1在准确度上与高保真有限差分求解器优化相当，在测试案例中将整个优化过程加速了70.6倍，通过最优放置热源组件有效降低了峰温。', 'title_zh': 'DeepOHeat-v1：用于3D-IC设计中快速和可靠热仿真与优化的有效算子学习'}
{'arxiv_id': 'arXiv:2504.03951', 'title': 'Understanding EFX Allocations: Counting and Variants', 'authors': 'Tzeh Yuan Neoh, Nicholas Teh', 'link': 'https://arxiv.org/abs/2504.03951', 'abstract': 'Envy-freeness up to any good (EFX) is a popular and important fairness property in the fair allocation of indivisible goods, of which its existence in general is still an open question. In this work, we investigate the problem of determining the minimum number of EFX allocations for a given instance, arguing that this approach may yield valuable insights into the existence and computation of EFX allocations. We focus on restricted instances where the number of goods slightly exceeds the number of agents, and extend our analysis to weighted EFX (WEFX) and a novel variant of EFX for general monotone valuations, termed EFX+. In doing so, we identify the transition threshold for the existence of allocations satisfying these fairness notions. Notably, we resolve open problems regarding WEFX by proving polynomial-time computability under binary additive valuations, and establishing the first constant-factor approximation for two agents.', 'abstract_zh': '任意好的忌妒 freeness（EFX）是 indivisible goods 公平分配中一个流行且重要的公平性属性，其普遍存在的问题仍然是一个开放问题。在本文中，我们研究确定给定实例的 EFX 分配数量最小值的问题，认为这种做法可能为 EFX 分配的存在性和计算提供有价值的见解。我们关注商品数量略微超过代理数量的限制实例，并将分析扩展到加权 EFX（WEFX）以及对一般单调估值的一种新型 EFX 变体 EFX+。在此过程中，我们识别出满足这些公平性概念的分配存在的转换阈值。特别地，我们通过证明在二进制加性估值下的多项式时间可计算性解决了关于 WEFX 的开放问题，并首次建立了两个代理的常数因子近似算法。', 'title_zh': '理解EFX 分配：计数与变体'}
{'arxiv_id': 'arXiv:2504.03940', 'title': 'Analysis of Robustness of a Large Game Corpus', 'authors': 'Mahsa Bazzaz, Seth Cooper', 'link': 'https://arxiv.org/abs/2504.03940', 'abstract': 'Procedural content generation via machine learning (PCGML) in games involves using machine learning techniques to create game content such as maps and levels. 2D tile-based game levels have consistently served as a standard dataset for PCGML because they are a simplified version of game levels while maintaining the specific constraints typical of games, such as being solvable. In this work, we highlight the unique characteristics of game levels, including their structured discrete data nature, the local and global constraints inherent in the games, and the sensitivity of the game levels to small changes in input. We define the robustness of data as a measure of sensitivity to small changes in input that cause a change in output, and we use this measure to analyze and compare these levels to state-of-the-art machine learning datasets, showcasing the subtle differences in their nature. We also constructed a large dataset from four games inspired by popular classic tile-based games that showcase these characteristics and address the challenge of sparse data in PCGML by providing a significantly larger dataset than those currently available.', 'abstract_zh': '基于机器学习的游戏程序化内容生成（PCGML）涉及使用机器学习技术生成游戏内容，如地图和关卡。2D砖块制游戏关卡一直作为PCGML的标准数据集，因为它们是游戏关卡的简化版本，同时保留了典型的约束条件，如可解性。在本工作中，我们强调了游戏关卡的独特特性，包括其结构化离散数据性质、游戏中固有的局部和全局约束，以及游戏关卡对输入微小变化的高度敏感性。我们将数据的鲁棒性定义为输入微小变化引起输出变化的敏感性度量，并使用此度量来分析和比较这些关卡与最先进的机器学习数据集，展示了它们本质上的细微差异。我们还从四款受到流行经典砖块制游戏启发的游戏构建了一个大型数据集，以展示这些特性，并通过提供比当前可用的更大规模的数据集来应对PCGML中的稀疏数据挑战。', 'title_zh': '大型游戏语料库的健壮性分析'}
{'arxiv_id': 'arXiv:2504.03915', 'title': 'RF-BayesPhysNet: A Bayesian rPPG Uncertainty Estimation Method for Complex Scenarios', 'authors': 'Rufei Ma, Chao Chen', 'link': 'https://arxiv.org/abs/2504.03915', 'abstract': 'Remote photoplethysmography (rPPG) technology infers heart rate by capturing subtle color changes in facial skin\nusing a camera, demonstrating great potential in non-contact heart rate measurement. However, measurement\naccuracy significantly decreases in complex scenarios such as lighting changes and head movements compared\nto ideal laboratory conditions. Existing deep learning models often neglect the quantification of measurement\nuncertainty, limiting their credibility in dynamic scenes. To address the issue of insufficient rPPG measurement\nreliability in complex scenarios, this paper introduces Bayesian neural networks to the rPPG field for the first time,\nproposing the Robust Fusion Bayesian Physiological Network (RF-BayesPhysNet), which can model both aleatoric\nand epistemic uncertainty. It leverages variational inference to balance accuracy and computational efficiency.\nDue to the current lack of uncertainty estimation metrics in the rPPG field, this paper also proposes a new set of\nmethods, using Spearman correlation coefficient, prediction interval coverage, and confidence interval width, to\nmeasure the effectiveness of uncertainty estimation methods under different noise conditions. Experiments show\nthat the model, with only double the parameters compared to traditional network models, achieves a MAE of 2.56\non the UBFC-RPPG dataset, surpassing most models. It demonstrates good uncertainty estimation capability\nin no-noise and low-noise conditions, providing prediction confidence and significantly enhancing robustness in\nreal-world applications. We have open-sourced the code at this https URL', 'abstract_zh': '远程光电体积描记术（rPPG）技术通过摄像头捕捉面部皮肤的细微颜色变化来推断心率，展示了在非接触心率测量方面的巨大潜力。然而，在光照变化和头部移动等复杂场景中，测量准确性显著下降，不及理想实验室条件。现有深度学习模型往往忽视了测量不确定性的量化，限制了其在动态场景中的可信度。为了解决复杂场景下rPPG测量可靠性的不足问题，本文首次将贝叶斯神经网络引入rPPG领域，提出了稳健融合贝叶斯生理网络（RF-BayesPhysNet），该网络可以建模偶然性和先验不确定性。利用变分推断平衡准确性和计算效率。由于rPPG领域目前缺乏不确定性估算指标，本文还提出了一套新的方法，使用斯皮尔曼相关系数、预测区间覆盖率和置信区间宽度，来衡量在不同噪声条件下的不确定性估算方法的有效性。实验结果显示，与传统网络模型相比，该模型仅参数量增加一倍，MAE达到2.56，在UBFC-RPPG数据集上超越了大多数模型。它在无噪声和低噪声条件下展示了良好的不确定性估算能力，提供了预测置信度，并显著增强了在实际应用中的鲁棒性。代码已开源：this https URL', 'title_zh': 'RF-BayesPhysNet：一种应用于复杂场景的心率不确定性估计方法'}
{'arxiv_id': 'arXiv:2504.03894', 'title': 'Leveraging Gait Patterns as Biomarkers: An attention-guided Deep Multiple Instance Learning Network for Scoliosis Classification', 'authors': 'Haiqing Li, Yuzhi Guo, Feng Jiang, Qifeng Zhou, Hehuan Ma, Junzhou Huang', 'link': 'https://arxiv.org/abs/2504.03894', 'abstract': "Scoliosis is a spinal curvature disorder that is difficult to detect early and can compress the chest cavity, impacting respiratory function and cardiac health. Especially for adolescents, delayed detection and treatment result in worsening compression. Traditional scoliosis detection methods heavily rely on clinical expertise, and X-ray imaging poses radiation risks, limiting large-scale early screening. We propose an Attention-Guided Deep Multi-Instance Learning method (Gait-MIL) to effectively capture discriminative features from gait patterns, which is inspired by ScoNet-MT's pioneering use of gait patterns for scoliosis detection. We evaluate our method on the first large-scale dataset based on gait patterns for scoliosis classification. The results demonstrate that our study improves the performance of using gait as a biomarker for scoliosis detection, significantly enhances detection accuracy for the particularly challenging Neutral cases, where subtle indicators are often overlooked. Our Gait-MIL also performs robustly in imbalanced scenarios, making it a promising tool for large-scale scoliosis screening.", 'abstract_zh': '注意力引导的深度多实例学习方法（Gait-MIL）在脊柱侧弯检测中的应用', 'title_zh': '利用步态模式作为生物标志物：一种attention引导的深多重实例学习网络在脊柱侧弯分类中的应用'}
{'arxiv_id': 'arXiv:2504.03888', 'title': 'Investigating Affective Use and Emotional Well-being on ChatGPT', 'authors': 'Jason Phang, Michael Lampe, Lama Ahmad, Sandhini Agarwal, Cathy Mengying Fang, Auren R. Liu, Valdemar Danry, Eunhae Lee, Samantha W.T. Chan, Pat Pataranutaporn, Pattie Maes', 'link': 'https://arxiv.org/abs/2504.03888', 'abstract': "As AI chatbots see increased adoption and integration into everyday life, questions have been raised about the potential impact of human-like or anthropomorphic AI on users. In this work, we investigate the extent to which interactions with ChatGPT (with a focus on Advanced Voice Mode) may impact users' emotional well-being, behaviors and experiences through two parallel studies. To study the affective use of AI chatbots, we perform large-scale automated analysis of ChatGPT platform usage in a privacy-preserving manner, analyzing over 3 million conversations for affective cues and surveying over 4,000 users on their perceptions of ChatGPT. To investigate whether there is a relationship between model usage and emotional well-being, we conduct an Institutional Review Board (IRB)-approved randomized controlled trial (RCT) on close to 1,000 participants over 28 days, examining changes in their emotional well-being as they interact with ChatGPT under different experimental settings. In both on-platform data analysis and the RCT, we observe that very high usage correlates with increased self-reported indicators of dependence. From our RCT, we find that the impact of voice-based interactions on emotional well-being to be highly nuanced, and influenced by factors such as the user's initial emotional state and total usage duration. Overall, our analysis reveals that a small number of users are responsible for a disproportionate share of the most affective cues.", 'abstract_zh': '随着AI聊天机器人被更广泛地采用并融入日常生活中，人们对其可能对用户产生的人类化或拟人化AI的影响提出了质疑。在本研究中，我们通过两项并行研究调查了与ChatGPT（特别是高级语音模式）交互对其用户情绪健康、行为和体验的影响程度。为研究AI聊天机器人的情感使用情况，我们以隐私保护的方式对ChatGPT平台使用进行了大规模自动化分析，分析了超过300万次对话以寻找情感线索，并对超过4000名用户进行了ChatGPT感知的调查。为探讨模型使用与情绪健康之间的关系，我们经过机构审查委员会（IRB）批准，对近1000名参与者进行了为期28天的随机对照试验（RCT），并在不同的实验设置下检查了他们的情绪健康变化情况。在平台数据的分析和RCT两方面，我们观察到极高的使用频率与自我报告的情感依赖性增加之间存在关联。从我们的RCT研究中，我们发现基于语音的交互对情绪健康的影响具有高度复杂性，并受到用户初始情绪状态和总使用时间等因素的影响。总体而言，我们的分析表明，一小部分用户产生了不成比例的最多情感线索。', 'title_zh': '探究ChatGPT中情感应用与情感福祉'}
{'arxiv_id': 'arXiv:2504.03887', 'title': 'Accurate GPU Memory Prediction for Deep Learning Jobs through Dynamic Analysis', 'authors': 'Jiabo Shi, Yehia Elkhatib', 'link': 'https://arxiv.org/abs/2504.03887', 'abstract': 'The benefits of Deep Learning (DL) impose significant pressure on GPU resources, particularly within GPU cluster, where Out-Of-Memory (OOM) errors present a primary impediment to model training and efficient resource utilization. Conventional OOM estimation techniques, relying either on static graph analysis or direct GPU memory profiling, suffer from inherent limitations: static analysis often fails to capture model dynamics, whereas GPU-based profiling intensifies contention for scarce GPU resources. To overcome these constraints, VeritasEst emerges. It is an innovative, entirely CPU-based analysis tool capable of accurately predicting the peak GPU memory required for DL training tasks without accessing the target GPU. This "offline" prediction capability is core advantage of VeritasEst, allowing accurate memory footprint information to be obtained before task scheduling, thereby effectively preventing OOM and optimizing GPU allocation. Its performance was validated through thousands of experimental runs across convolutional neural network (CNN) models: Compared to baseline GPU memory estimators, VeritasEst significantly reduces the relative error by 84% and lowers the estimation failure probability by 73%. VeritasEst represents a key step towards efficient and predictable DL training in resource-constrained environments.', 'abstract_zh': '深度学习对GPU资源的重大需求给GPU集群带来了显著压力，尤其是在GPU集群中，内存溢出（OOM）错误是模型训练和资源高效利用的主要障碍。传统的OOM估计技术要么依赖静态图分析，要么直接进行GPU内存监控，都存在先天限制：静态分析往往无法捕捉模型动态，而基于GPU的监控则加剧了对稀缺GPU资源的争夺。为克服这些限制，VeritasEst应运而生。它是一种创新的、完全基于CPU的分析工具，能够在不访问目标GPU的情况下准确预测DL训练任务所需的峰值GPU内存。VeritasEst的这种“离线”预测能力是其核心优势，能够在任务调度前获取准确的内存占用信息，从而有效防止OOM并优化GPU分配。实验验证显示，与基线GPU内存估计算法相比，VeritasEst将相对误差降低了84%，估计失败概率降低了73%。VeritasEst代表了在资源受限环境中实现高效且可预测的DL训练的一大步。', 'title_zh': '基于动态分析的深度学习任务准确GPU内存预测'}
{'arxiv_id': 'arXiv:2504.03818', 'title': 'Exploring Various Sequential Learning Methods for Deformation History Modeling', 'authors': 'Muhammed Adil Yatkin, Mihkel Korgesaar, Jani Romanoff, Umit Islak, Hasan Kurban', 'link': 'https://arxiv.org/abs/2504.03818', 'abstract': 'Current neural network (NN) models can learn patterns from data points with historical dependence. Specifically, in natural language processing (NLP), sequential learning has transitioned from recurrence-based architectures to transformer-based architectures. However, it is unknown which NN architectures will perform the best on datasets containing deformation history due to mechanical loading. Thus, this study ascertains the appropriateness of 1D-convolutional, recurrent, and transformer-based architectures for predicting deformation localization based on the earlier states in the form of deformation history. Following this investigation, the crucial incompatibility issues between the mathematical computation of the prediction process in the best-performing NN architectures and the actual values derived from the natural physical properties of the deformation paths are examined in detail.', 'abstract_zh': '当前的神经网络模型可以从具有历史依赖性的数据点中学习模式。具体而言，在自然语言处理（NLP）中， Sequential学习已经从基于循环的架构转型为基于变换器的架构。然而，对于包含由于机械加载引起的变形历史的 dataset，尚不清楚哪种 NN 架构能够表现最佳以预测变形 localization。因此，本研究确定了一维卷积、循环和变换器基架构在基于变形历史的早期状态预测变形 localization 方面的适当性。随后，研究了在最佳表现的 NN 架构中预测过程的数学计算与实际源自变形路径的自然物理属性的值之间的关键不兼容问题。', 'title_zh': '探索各种序列学习方法在形变历史建模中的应用'}
{'arxiv_id': 'arXiv:2504.03809', 'title': 'Drawing a Map of Elections', 'authors': 'Stanisław Szufa, Niclas Boehmer, Robert Bredereck, Piotr Faliszewski, Rolf Niedermeier, Piotr Skowron, Arkadii Slinko, Nimrod Talmon', 'link': 'https://arxiv.org/abs/2504.03809', 'abstract': 'Our main contribution is the introduction of the map of elections framework. A map of elections consists of three main elements: (1) a dataset of elections (i.e., collections of ordinal votes over given sets of candidates), (2) a way of measuring similarities between these elections, and (3) a representation of the elections in the 2D Euclidean space as points, so that the more similar two elections are, the closer are their points. In our maps, we mostly focus on datasets of synthetic elections, but we also show an example of a map over real-life ones. To measure similarities, we would have preferred to use, e.g., the isomorphic swap distance, but this is infeasible due to its high computational complexity. Hence, we propose polynomial-time computable positionwise distance and use it instead. Regarding the representations in 2D Euclidean space, we mostly use the Kamada-Kawai algorithm, but we also show two alternatives. We develop the necessary theoretical results to form our maps and argue experimentally that they are accurate and credible. Further, we show how coloring the elections in a map according to various criteria helps in analyzing results of a number of experiments. In particular, we show colorings according to the scores of winning candidates or committees, running times of ILP-based winner determination algorithms, and approximation ratios achieved by particular algorithms.', 'abstract_zh': '我们主要的贡献是介绍了选举地图框架。选举地图由三个主要元素组成：(1) 选举数据集（即候选人的集合上的序数投票集合），(2) 一种衡量这些选举之间相似性的方法，以及(3) 将选举在二维欧几里得空间中表示为点，使得两个选举越相似，它们的点越接近。在我们的地图中，我们主要关注合成选举的数据集，但也展示了真实选举的一个示例。为了衡量相似性，我们本来希望使用同构交换距离，但由于其高计算复杂性，这变得不可能。因此，我们提出了一种多项式时间可计算的位置距离，并使用它代替。关于二维欧几里得空间中的表示，我们主要使用了Kamada-Kawai算法，但也展示了两种替代方案。我们发展了必要的理论成果来构建我们的地图，并通过实验论证它们的准确性和可靠性。此外，我们展示了根据各种标准对地图中的选举进行着色有助于分析多次实验的结果。具体地，我们展示了根据获胜候选人的得分或委员会、基于ILP的获胜者确定算法的运行时间以及特定算法达到的近似比的着色方案。', 'title_zh': '绘制选举地图'}
{'arxiv_id': 'arXiv:2504.03801', 'title': 'Semantic-guided Representation Learning for Multi-Label Recognition', 'authors': 'Ruhui Zhang, Hezhe Qiao, Pengcheng Xu, Mingsheng Shang, Lin Chen', 'link': 'https://arxiv.org/abs/2504.03801', 'abstract': 'Multi-label Recognition (MLR) involves assigning multiple labels to each data instance in an image, offering advantages over single-label classification in complex scenarios. However, it faces the challenge of annotating all relevant categories, often leading to uncertain annotations, such as unseen or incomplete labels. Recent Vision and Language Pre-training (VLP) based methods have made significant progress in tackling zero-shot MLR tasks by leveraging rich vision-language correlations. However, the correlation between multi-label semantics has not been fully explored, and the learned visual features often lack essential semantic information. To overcome these limitations, we introduce a Semantic-guided Representation Learning approach (SigRL) that enables the model to learn effective visual and textual representations, thereby improving the downstream alignment of visual images and categories. Specifically, we first introduce a graph-based multi-label correlation module (GMC) to facilitate information exchange between labels, enriching the semantic representation across the multi-label texts. Next, we propose a Semantic Visual Feature Reconstruction module (SVFR) to enhance the semantic information in the visual representation by integrating the learned textual representation during reconstruction. Finally, we optimize the image-text matching capability of the VLP model using both local and global features to achieve zero-shot MLR. Comprehensive experiments are conducted on several MLR benchmarks, encompassing both zero-shot MLR (with unseen labels) and single positive multi-label learning (with limited labels), demonstrating the superior performance of our approach compared to state-of-the-art methods. The code is available at this https URL.', 'abstract_zh': '多标签识别中的语义导向表示学习（SigRL）：面向零样本多标签识别的任务', 'title_zh': '基于语义引导的多标签识别表示学习'}
{'arxiv_id': 'arXiv:2504.03799', 'title': 'Experimental Study on Time Series Analysis of Lower Limb Rehabilitation Exercise Data Driven by Novel Model Architecture and Large Models', 'authors': 'Hengyu Lin', 'link': 'https://arxiv.org/abs/2504.03799', 'abstract': 'This study investigates the application of novel model architectures and large-scale foundational models in temporal series analysis of lower limb rehabilitation motion data, aiming to leverage advancements in machine learning and artificial intelligence to empower active rehabilitation guidance strategies for post-stroke patients in limb motor function recovery. Utilizing the SIAT-LLMD dataset of lower limb movement data proposed by the Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, we systematically elucidate the implementation and analytical outcomes of the innovative xLSTM architecture and the foundational model Lag-Llama in short-term temporal prediction tasks involving joint kinematics and dynamics parameters. The research provides novel insights for AI-enabled medical rehabilitation applications, demonstrating the potential of cutting-edge model architectures and large-scale models in rehabilitation medicine temporal prediction. These findings establish theoretical foundations for future applications of personalized rehabilitation regimens, offering significant implications for the development of customized therapeutic interventions in clinical practice.', 'abstract_zh': '本研究探讨了新颖模型架构和大规模基础模型在下肢康复运动数据时序分析中的应用，旨在利用机器学习和人工智能的进步，为中风后肢体运动功能恢复患者的主动康复指导策略提供支持。利用中国科学院深圳先进技术研究院提出的SIAT-LLMD下肢运动数据集，系统阐释了创新xLSTM架构和基础模型Lag-Llama在涉及关节运动学和动力学参数的短期时序预测任务中的实现与分析结果。研究为AI赋能的医疗康复应用提供了新的见解，展示了前沿模型架构和大规模模型在康复医学时间序列预测中的潜力。这些发现为未来个性化的康复方案应用奠定了理论基础，为临床实践中定制化治疗干预的发展提供了重要意义。', 'title_zh': '基于新型模型架构和大规模模型驱动的下肢康复锻炼时间序列分析实验研究'}
{'arxiv_id': 'arXiv:2504.03798', 'title': 'An Intelligent and Privacy-Preserving Digital Twin Model for Aging-in-Place', 'authors': 'Yongjie Wang, Jonathan Cyril Leung, Ming Chen, Zhiwei Zeng, Benny Toh Hsiang Tan, Yang Qiu, Zhiqi Shen', 'link': 'https://arxiv.org/abs/2504.03798', 'abstract': "The population of older adults is steadily increasing, with a strong preference for aging-in-place rather than moving to care facilities. Consequently, supporting this growing demographic has become a significant global challenge. However, facilitating successful aging-in-place is challenging, requiring consideration of multiple factors such as data privacy, health status monitoring, and living environments to improve health outcomes. In this paper, we propose an unobtrusive sensor system designed for installation in older adults' homes. Using data from the sensors, our system constructs a digital twin, a virtual representation of events and activities that occurred in the home. The system uses neural network models and decision rules to capture residents' activities and living environments. This digital twin enables continuous health monitoring by providing actionable insights into residents' well-being. Our system is designed to be low-cost and privacy-preserving, with the aim of providing green and safe monitoring for the health of older adults. We have successfully deployed our system in two homes over a time period of two months, and our findings demonstrate the feasibility and effectiveness of digital twin technology in supporting independent living for older adults. This study highlights that our system could revolutionize elder care by enabling personalized interventions, such as lifestyle adjustments, medical treatments, or modifications to the residential environment, to enhance health outcomes.", 'abstract_zh': '老年人口不断增加，偏向于居家养老而非入住护理设施。因此，支持这一不断壮大的人口群体已成为全球性的重要挑战。然而，促进成功的居家养老具有挑战性，需要考虑多个因素，如数据隐私、健康状况监控和生活环境，以改善健康结果。本文 propose 一种用于安装在老年人家中无侵扰传感器系统。利用传感器数据，该系统构建了一个数字孪生体，即家庭中发生事件和活动的虚拟表示。系统使用神经网络模型和决策规则来捕捉居民的活动和生活环境。此数字孪生体通过提供有关居民福祉的实际行动建议，实现持续的健康监控。本系统旨在低成本和保护隐私的情况下运行，目标是为老年人提供绿色和安全的健康监测。我们在两个月内成功部署了该系统于两个家庭，并且我们的研究结果表明，数字孪生技术在支持老年人独立生活方面具有可行性和有效性。本研究强调，我们的系统有望通过实现个性化干预，如生活方式调整、医疗治疗或住宅环境的修改，彻底改变老年护理领域，以提升健康结果。', 'title_zh': '一种用于原 place 老龄化的智能隐私保护数字孪生模型'}
{'arxiv_id': 'arXiv:2504.03793', 'title': 'Outlook Towards Deployable Continual Learning for Particle Accelerators', 'authors': 'Kishansingh Rajput, Sen Lin, Auralee Edelen, Willem Blokland, Malachi Schram', 'link': 'https://arxiv.org/abs/2504.03793', 'abstract': 'Particle Accelerators are high power complex machines. To ensure uninterrupted operation of these machines, thousands of pieces of equipment need to be synchronized, which requires addressing many challenges including design, optimization and control, anomaly detection and machine protection. With recent advancements, Machine Learning (ML) holds promise to assist in more advance prognostics, optimization, and control. While ML based solutions have been developed for several applications in particle accelerators, only few have reached deployment and even fewer to long term usage, due to particle accelerator data distribution drifts caused by changes in both measurable and non-measurable parameters. In this paper, we identify some of the key areas within particle accelerators where continual learning can allow maintenance of ML model performance with distribution drifts. Particularly, we first discuss existing applications of ML in particle accelerators, and their limitations due to distribution drift. Next, we review existing continual learning techniques and investigate their potential applications to address data distribution drifts in accelerators. By identifying the opportunities and challenges in applying continual learning, this paper seeks to open up the new field and inspire more research efforts towards deployable continual learning for particle accelerators.', 'abstract_zh': '粒子加速器是高功率复杂机器。为了确保这些机器不间断运行，需要同步数以千计的设备，这需要解决包括设计、优化和控制、异常检测以及机器保护在内的许多挑战。随着技术的进步，机器学习（ML）有望帮助实现更高级的预诊、优化和控制。虽然已经为粒子加速器的各种应用开发了基于ML的解决方案，但由于可测和不可测参数的变化导致的数据分布漂移，只有少数方案实现了部署，并且长期使用的情况更加罕见。本文识别了粒子加速器中的一些关键领域，其中持续学习可以在数据分布漂移的情况下保持ML模型性能。特别地，我们首先讨论了ML在粒子加速器中的现有应用及其因数据分布漂移造成的局限性，接下来我们回顾了现有的持续学习技术，并探讨了它们在加速器中应对数据分布漂移的潜在应用。通过识别应用持续学习的机会和挑战，本文旨在开拓这一新领域，并激励更多针对粒子加速器的可部署持续学习的研究努力。', 'title_zh': '面向粒子加速器的可部署连续学习展望'}
{'arxiv_id': 'arXiv:2504.03792', 'title': 'DP-LET: An Efficient Spatio-Temporal Network Traffic Prediction Framework', 'authors': 'Xintong Wang, Haihan Nan, Ruidong Li, Huaming Wu', 'link': 'https://arxiv.org/abs/2504.03792', 'abstract': 'Accurately predicting spatio-temporal network traffic is essential for dynamically managing computing resources in modern communication systems and minimizing energy consumption. Although spatio-temporal traffic prediction has received extensive research attention, further improvements in prediction accuracy and computational efficiency remain necessary. In particular, existing decomposition-based methods or hybrid architectures often incur heavy overhead when capturing local and global feature correlations, necessitating novel approaches that optimize accuracy and complexity. In this paper, we propose an efficient spatio-temporal network traffic prediction framework, DP-LET, which consists of a data processing module, a local feature enhancement module, and a Transformer-based prediction module. The data processing module is designed for high-efficiency denoising of network data and spatial decoupling. In contrast, the local feature enhancement module leverages multiple Temporal Convolutional Networks (TCNs) to capture fine-grained local features. Meanwhile, the prediction module utilizes a Transformer encoder to model long-term dependencies and assess feature relevance. A case study on real-world cellular traffic prediction demonstrates the practicality of DP-LET, which maintains low computational complexity while achieving state-of-the-art performance, significantly reducing MSE by 31.8% and MAE by 23.1% compared to baseline models.', 'abstract_zh': '准确预测时空网络流量对于现代通信系统中动态管理计算资源和减少能源消耗至关重要。尽管时空流量预测已经获得了广泛的研究关注，但预测准确性和计算效率的进一步改进仍然是必要的。特别是，现有的基于分解的方法或混合架构在捕捉局部和全局特征关联时经常产生较大的开销，因此需要优化准确性和复杂性的新型方法。本文提出了一种高效的时空网络流量预测框架DP-LET，该框架包括数据处理模块、局部特征增强模块和基于Transformer的预测模块。数据处理模块旨在高效地去除网络数据噪声和实现空间解耦。相比之下，局部特征增强模块利用多个时序卷积网络（TCNs）捕捉细粒度的局部特征。同时，预测模块利用Transformer编码器建模长期依赖关系并评估特征相关性。在实际蜂窝网络流量预测中的案例研究证明了DP-LET的有效性，该方法在保持低计算复杂性的同时达到了最先进的性能，相比基准模型，MSE降低了31.8%，MAE降低了23.1%。', 'title_zh': 'DP-LET：一种高效的时空网络流量预测框架'}
{'arxiv_id': 'arXiv:2504.03783', 'title': 'FAST: Federated Active Learning with Foundation Models for Communication-efficient Sampling and Training', 'authors': 'Haoyuan Li, Jindong Wang, Mathias Funk, Aaqib Saeed', 'link': 'https://arxiv.org/abs/2504.03783', 'abstract': 'Federated Active Learning (FAL) has emerged as a promising framework to leverage large quantities of unlabeled data across distributed clients while preserving data privacy. However, real-world deployments remain limited by high annotation costs and communication-intensive sampling processes, particularly in a cross-silo setting, when clients possess substantial local datasets. This paper addresses the crucial question: What is the best practice to reduce communication costs in human-in-the-loop learning with minimal annotator effort? Existing FAL methods typically rely on iterative annotation processes that separate active sampling from federated updates, leading to multiple rounds of expensive communication and annotation. In response, we introduce FAST, a two-pass FAL framework that harnesses foundation models for weak labeling in a preliminary pass, followed by a refinement pass focused exclusively on the most uncertain samples. By leveraging representation knowledge from foundation models and integrating refinement steps into a streamlined workflow, FAST substantially reduces the overhead incurred by iterative active sampling. Extensive experiments on diverse medical and natural image benchmarks demonstrate that FAST outperforms existing FAL methods by an average of 4.36% while reducing communication rounds eightfold under a limited 5% labeling budget.', 'abstract_zh': '联邦主动学习（FAL）作为一种框架，在保持数据隐私的同时，利用分布式客户端跨客户端的大量未标注数据，展现了潜力。然而，在实际部署中，由于高标注成本和耗时的采样过程，尤其是在跨孤岛设置中，客户端拥有大量本地数据集时，部署受到限制。本文解决了关键问题：如何在最小化标注员努力的前提下，减少基于人类在环学习中的通信成本？现有的FAL方法通常依赖于迭代的标注过程，将主动采样与联邦更新区分开来，导致了多轮昂贵的通信和标注。针对这一问题，我们提出了一种两阶段的FAL框架FAST，该框架在初步阶段利用基础模型进行弱标注，然后在第二阶段专注于最不确定的样本进行细化。通过利用基础模型的表示知识并整合细化步骤到精简的工作流程中，FAST显著减少了迭代主动采样带来的开销。在多种医疗和自然图像基准测试上进行的广泛实验表明，与现有FAL方法相比，FAST在5%标注预算下通信轮数减少八倍的同时，平均性能高出4.36%。', 'title_zh': 'FAST：基于基础模型的联邦主动学习方法，用于高效的通信采样和训练'}
{'arxiv_id': 'arXiv:2504.03777', 'title': 'Explainable and Interpretable Forecasts on Non-Smooth Multivariate Time Series for Responsible Gameplay', 'authors': 'Hussain Jagirdar, Rukma Talwadker, Aditya Pareek, Pulkit Agrawal, Tridib Mukherjee', 'link': 'https://arxiv.org/abs/2504.03777', 'abstract': "Multi-variate Time Series (MTS) forecasting has made large strides (with very negligible errors) through recent advancements in neural networks, e.g., Transformers. However, in critical situations like predicting gaming overindulgence that affects one's mental well-being; an accurate forecast without a contributing evidence (explanation) is irrelevant. Hence, it becomes important that the forecasts are Interpretable - intermediate representation of the forecasted trajectory is comprehensible; as well as Explainable - attentive input features and events are accessible for a personalized and timely intervention of players at risk. While the contributing state of the art research on interpretability primarily focuses on temporally-smooth single-process driven time series data, our online multi-player gameplay data demonstrates intractable temporal randomness due to intrinsic orthogonality between player's game outcome and their intent to engage further. We introduce a novel deep Actionable Forecasting Network (AFN), which addresses the inter-dependent challenges associated with three exclusive objectives - 1) forecasting accuracy; 2) smooth comprehensible trajectory and 3) explanations via multi-dimensional input features while tackling the challenges introduced by our non-smooth temporal data, together in one single solution. AFN establishes a \\it{new benchmark} via: (i) achieving 25% improvement on the MSE of the forecasts on player data in comparison to the SOM-VAE based SOTA networks; (ii) attributing unfavourable progression of a player's time series to a specific future time step(s), with the premise of eliminating near-future overindulgent player volume by over 18% with player specific actionable inputs feature(s) and (iii) proactively detecting over 23% (100% jump from SOTA) of the to-be overindulgent, players on an average, 4 weeks in advance.", 'abstract_zh': '多变量时间序列（MTS）预测通过近期神经网络的进步（如变换器）取得了显著进展，但在关键情况下，如预测影响精神健康的游戏过度行为时，准确的预测如果不提供解释（证据）则毫无意义。因此，预测的可解释性和可解释性变得尤为重要——预测的中间表示易于理解；并且注意输入特征和事件易于访问，以便及时干预处于风险中的玩家。虽然现有的可解释性研究主要集中在平滑的单过程驱动的时间序列数据上，我们的在线多人游戏数据展示了由于玩家游戏结果和进一步参与意图之间的固有正交性而带来的难以解决的时间随机性。我们提出了一种新颖的可操作预测网络（AFN），它在三个独立目标——1）预测准确性；2）平滑易懂的轨迹；3）通过多维度输入特征提供解释——的同时，解决了一起浮现的挑战。AFN通过以下方式建立了新的基准：(i) 相比于基于SOM-VAE的当前最佳网络，玩家数据的预测均方误差提高了25%；(ii) 将玩家的不利进展归因于特定的未来时间步长，并通过针对特定玩家的可操作输入特征减少近未来过度行为玩家的比例超过18%；(iii) 预先检测到超过23%（相比当前最佳性能提高了100%）的即将过度行为的玩家，平均提前4周。', 'title_zh': '可解释和可解析的非光滑多变量时间序列预测：负责任的游戏玩法'}
{'arxiv_id': 'arXiv:2504.03776', 'title': 'Advancing Air Quality Monitoring: TinyML-Based Real-Time Ozone Prediction with Cost-Effective Edge Devices', 'authors': 'Huam Ming Ken, Mehran Behjati', 'link': 'https://arxiv.org/abs/2504.03776', 'abstract': "The escalation of urban air pollution necessitates innovative solutions for real-time air quality monitoring and prediction. This paper introduces a novel TinyML-based system designed to predict ozone concentration in real-time. The system employs an Arduino Nano 33 BLE Sense microcontroller equipped with an MQ7 sensor for carbon monoxide (CO) detection and built-in sensors for temperature and pressure measurements. The data, sourced from a Kaggle dataset on air quality parameters from India, underwent thorough cleaning and preprocessing. Model training and evaluation were performed using Edge Impulse, considering various combinations of input parameters (CO, temperature, and pressure). The optimal model, incorporating all three variables, achieved a mean squared error (MSE) of 0.03 and an R-squared value of 0.95, indicating high predictive accuracy. The regression model was deployed on the microcontroller via the Arduino IDE, showcasing robust real-time performance. Sensitivity analysis identified CO levels as the most critical predictor of ozone concentration, followed by pressure and temperature. The system's low-cost and low-power design makes it suitable for widespread implementation, particularly in resource-constrained settings. This TinyML approach provides precise real-time predictions of ozone levels, enabling prompt responses to pollution events and enhancing public health protection.", 'abstract_zh': '基于TinyML的实时臭氧浓度预测系统：创新应对城市空气污染挑战', 'title_zh': '基于经济实惠边缘设备的TinyML实时臭氧预测以推进空气质量监测'}
{'arxiv_id': 'arXiv:2504.03775', 'title': 'FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache Transfer and Load-Aware Scheduling', 'authors': 'Weiqing Li, Guochao Jiang, Xiangyong Ding, Zhangcheng Tao, Chuzhan Hao, Chenfeng Xu, Yuewei Zhang, Hao Wang', 'link': 'https://arxiv.org/abs/2504.03775', 'abstract': 'Disaggregated inference has become an essential framework that separates the prefill (P) and decode (D) stages in large language model inference to improve throughput. However, the KV cache transfer faces significant delays between prefill and decode nodes. The block-wise calling method and discontinuous KV cache memory allocation increase the number of calls to the transmission kernel. Additionally, existing frameworks often fix the roles of P and D nodes, leading to computational imbalances. In this paper, we propose FlowKV, a novel disaggregated inference framework, which reduces the average transmission latency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the transfer time relative to the total request latency by optimizing the KV cache transfer. FlowKV introduces the Load-Aware Scheduler for balanced request scheduling and flexible PD node allocation. This design maximizes hardware resource utilization, achieving peak system throughput across various scenarios, including normal, computational imbalance, and extreme overload conditions. Experimental results demonstrate that FlowKV significantly accelerates inference by 15.2%-48.9% on LongBench dataset compared to the baseline and supports applications with heterogeneous GPUs.', 'abstract_zh': '基于流量的解聚推理框架FlowKV', 'title_zh': 'FlowKV：一种低延迟键值缓存传输与负载aware调度的解耦推理框架'}
{'arxiv_id': 'arXiv:2504.03774', 'title': 'Exploring energy consumption of AI frameworks on a 64-core RV64 Server CPU', 'authors': 'Giulio Malenza, Francesco Targa, Adriano Marques Garcia, Marco Aldinucci, Robert Birke', 'link': 'https://arxiv.org/abs/2504.03774', 'abstract': "In today's era of rapid technological advancement, artificial intelligence (AI) applications require large-scale, high-performance, and data-intensive computations, leading to significant energy demands. Addressing this challenge necessitates a combined approach involving both hardware and software innovations. Hardware manufacturers are developing new, efficient, and specialized solutions, with the RISC-V architecture emerging as a prominent player due to its open, extensible, and energy-efficient instruction set architecture (ISA). Simultaneously, software developers are creating new algorithms and frameworks, yet their energy efficiency often remains unclear. In this study, we conduct a comprehensive benchmark analysis of machine learning (ML) applications on the 64-core SOPHON SG2042 RISC-V architecture. We specifically analyze the energy consumption of deep learning inference models across three leading AI frameworks: PyTorch, ONNX Runtime, and TensorFlow. Our findings show that frameworks using the XNNPACK back-end, such as ONNX Runtime and TensorFlow, consume less energy compared to PyTorch, which is compiled with the native OpenBLAS back-end.", 'abstract_zh': '在快速 technological advancement时代，人工智能（AI）应用需要大规模、高性能和数据密集型计算，导致了显著的能源需求。为应对这一挑战，需要硬件和软件创新相结合的方法。硬件制造商正在开发新的、高效且专门化的新解决方案，RISC-V架构因其开放、可扩展且能效高的指令集架构（ISA）而成为引人注目的参与者。与此同时，软件开发人员正在创建新的算法和框架，但它们的能源效率往往不明确。在本研究中，我们对64核心SOPHON SG2042 RISC-V架构上的机器学习（ML）应用进行了全面基准分析。我们特别分析了三个主流AI框架——PyTorch、ONNX Runtime和TensorFlow——在深度学习推理模型中的能耗情况。研究发现，使用XNNPACK后端的框架，如ONNX Runtime和TensorFlow，相比于使用原生OpenBLAS后端编译的PyTorch，能耗较低。', 'title_zh': '探索AI框架在64核RV64服务器CPU上的能效消耗'}
{'arxiv_id': 'arXiv:2504.03773', 'title': 'SHapley Estimated Explanation (SHEP): A Fast Post-Hoc Attribution Method for Interpreting Intelligent Fault Diagnosis', 'authors': 'Qian Chen, Xingjian Dong, Zhike Peng, Guang Meng', 'link': 'https://arxiv.org/abs/2504.03773', 'abstract': "Despite significant progress in intelligent fault diagnosis (IFD), the lack of interpretability remains a critical barrier to practical industrial applications, driving the growth of interpretability research in IFD. Post-hoc interpretability has gained popularity due to its ability to preserve network flexibility and scalability without modifying model structures. However, these methods often yield suboptimal time-domain explanations. Recently, combining domain transform with SHAP has improved interpretability by extending explanations to more informative domains. Nonetheless, the computational expense of SHAP, exacerbated by increased dimensions from domain transforms, remains a major challenge. To address this, we propose patch-wise attribution and SHapley Estimated Explanation (SHEP). Patch-wise attribution reduces feature dimensions at the cost of explanation granularity, while SHEP simplifies subset enumeration to approximate SHAP, reducing complexity from exponential to linear. Together, these methods significantly enhance SHAP's computational efficiency, providing feasibility for real-time interpretation in monitoring tasks. Extensive experiments confirm SHEP's efficiency, interpretability, and reliability in approximating SHAP. Additionally, with open-source code, SHEP has the potential to serve as a benchmark for post-hoc interpretability in IFD. The code is available on this https URL.", 'abstract_zh': '尽管在智能故障诊断（IFD）方面取得了显著进展，但缺乏可解释性仍然是其在工业应用中的一大障碍，推动了IFD可解释性研究的发展。后 hoc 可解释性由于能够保留网络的灵活性和可扩展性而未改变模型结构的情况下提高可解释性，因而备受欢迎。然而，这些方法通常会导致时间域解释的效果不佳。最近，将领域变换与 SHAP 结合使用，通过扩展解释到更具信息量的领域，提高了可解释性。不过，由于领域变换导致维度增加，SHAP 的计算开销仍然是一个主要挑战。为解决这一问题，我们提出了一种基于块的归因和 SHapley 估计解释（SHEP）方法。基于块的归因降低了特征维度，但牺牲了解释的精细度；而 SHEP 简化了子集枚举以近似 SHAP，将复杂度从指数级降低到线性级。这两种方法显著提高了 SHAP 的计算效率，为监控任务中的实时解释提供了可行性。广泛的实验验证了 SHEP 在效率、可解释性和近似 SHAP 方面的可靠性和有效性。此外，开源代码使 SHEP 成为 IFD 后 hoc 可解释性的基准。代码可在以下网址获取：this https URL。', 'title_zh': '基于Shapley值的快速事后归因方法：解释智能故障诊断'}
{'arxiv_id': 'arXiv:2504.03763', 'title': 'Efficient Calibration for RRAM-based In-Memory Computing using DoRA', 'authors': 'Weirong Dong, Kai Zhou, Zhen Kong, Quan Cheng, Junkai Huang, Zhengke Yang, Masanori Hashimoto, Longyang Lin', 'link': 'https://arxiv.org/abs/2504.03763', 'abstract': "Resistive In-Memory Computing (RIMC) offers ultra-efficient computation for edge AI but faces accuracy degradation due to RRAM conductance drift over time. Traditional retraining methods are limited by RRAM's high energy consumption, write latency, and endurance constraints. We propose a DoRA-based calibration framework that restores accuracy by compensating influential weights with minimal calibration parameters stored in SRAM, leaving RRAM weights untouched. This eliminates in-field RRAM writes, ensuring energy-efficient, fast, and reliable calibration. Experiments on RIMC-based ResNet50 (ImageNet-1K) demonstrate 69.53% accuracy restoration using just 10 calibration samples while updating only 2.34% of parameters.", 'abstract_zh': '基于DoRA的校准框架在保持RRAM权重不变的情况下通过少量SRAM存储的校准参数恢复Resistive In-Memory Computing (RIMC)的准确性', 'title_zh': '基于DoRA的RRAM基存算一体化高效校准方法'}
{'arxiv_id': 'arXiv:2504.03755', 'title': 'ProtoGCD: Unified and Unbiased Prototype Learning for Generalized Category Discovery', 'authors': 'Shijie Ma, Fei Zhu, Xu-Yao Zhang, Cheng-Lin Liu', 'link': 'https://arxiv.org/abs/2504.03755', 'abstract': 'Generalized category discovery (GCD) is a pragmatic but underexplored problem, which requires models to automatically cluster and discover novel categories by leveraging the labeled samples from old classes. The challenge is that unlabeled data contain both old and new classes. Early works leveraging pseudo-labeling with parametric classifiers handle old and new classes separately, which brings about imbalanced accuracy between them. Recent methods employing contrastive learning neglect potential positives and are decoupled from the clustering objective, leading to biased representations and sub-optimal results. To address these issues, we introduce a unified and unbiased prototype learning framework, namely ProtoGCD, wherein old and new classes are modeled with joint prototypes and unified learning objectives, {enabling unified modeling between old and new classes}. Specifically, we propose a dual-level adaptive pseudo-labeling mechanism to mitigate confirmation bias, together with two regularization terms to collectively help learn more suitable representations for GCD. Moreover, for practical considerations, we devise a criterion to estimate the number of new classes. Furthermore, we extend ProtoGCD to detect unseen outliers, achieving task-level unification. Comprehensive experiments show that ProtoGCD achieves state-of-the-art performance on both generic and fine-grained datasets. The code is available at this https URL.', 'abstract_zh': '泛化类别发现的统一无偏原型学习框架（ProtoGCD）', 'title_zh': '统一无偏的原型学习方法：通用类别发现'}
{'arxiv_id': 'arXiv:2504.03752', 'title': 'Proof of Humanity: A Multi-Layer Network Framework for Certifying Human-Originated Content in an AI-Dominated Internet', 'authors': 'Sebastian Barros', 'link': 'https://arxiv.org/abs/2504.03752', 'abstract': 'The rapid proliferation of generative AI has led to an internet increasingly populated with synthetic content-text, images, audio, and video generated without human intervention. As the distinction between human and AI-generated data blurs, the ability to verify content origin becomes critical for applications ranging from social media and journalism to legal and financial systems.\nIn this paper, we propose a conceptual, multi-layer architectural framework that enables telecommunications networks to act as infrastructure level certifiers of human-originated content. By leveraging identity anchoring at the physical layer, metadata propagation at the network and transport layers, and cryptographic attestations at the session and application layers, Telcos can provide an end-to-end Proof of Humanity for data traversing their networks.\nWe outline how each OSI layer can contribute to this trust fabric using technical primitives such as SIM/eSIM identity, digital signatures, behavior-based ML heuristics, and edge-validated APIs. The framework is presented as a foundation for future implementation, highlighting monetization pathways for telcos such as trust-as-a-service APIs, origin-certified traffic tiers, and regulatory compliance tools.\nThe paper does not present implementation or benchmarking results but offers a technically detailed roadmap and strategic rationale for transforming Telcos into validators of digital authenticity in an AI-dominated internet. Security, privacy, and adversarial considerations are discussed as directions for future work.', 'abstract_zh': '生成式AI的迅速发展导致互联网上充斥着无需人类干预生成的合成内容（包括文字、图像、音频和视频）。随着人类生成数据与AI生成数据之间的界限模糊，验证内容来源的能力对社交媒体、新闻业、法律和金融系统等应用而言变得至关重要。\n在本文中，我们提出了一种概念性的多层次架构框架，使电信网络能够作为基础设施级的人类生成内容认证者。通过在物理层利用身份锚定、在网络层和传输层利用元数据传播、在会话层和应用层利用加密证实，电信运营商可以为其网络中传输的数据提供端到端的人类身份证明。\n我们阐述了每一层开放系统互连（OSI）模型如何通过技术原语如SIM/eSIM身份、数字签名、基于行为的机器学习启发式规则以及边缘验证API来贡献于这一信任织网。该框架被提出作为未来实施的基础，强调了电信运营商通过信任即服务API、基于来源认证的流量层级以及合规工具等方面的盈利途径。\n本文未提供实施或基准测试结果，而是提供了一份详细的技术路线图和战略 rationale，旨在使电信运营商转变为AI主导互联网中数字真实性的验证者。安全、隐私和对抗性考虑被讨论为未来工作的方向。', 'title_zh': '人性验证：一种在人工智能主导的互联网中认证人类生成内容的多层网络框架'}
{'arxiv_id': 'arXiv:2504.03746', 'title': 'Enhancing Biologically Inspired Hierarchical Temporal Memory with Hardware-Accelerated Reflex Memory', 'authors': 'Pavia Bera, Sabrina Hassan Moon, Jennifer Adorno, Dayane Alfenas Reis, Sanjukta Bhanja', 'link': 'https://arxiv.org/abs/2504.03746', 'abstract': "The rapid expansion of the Internet of Things (IoT) generates zettabytes of data that demand efficient unsupervised learning systems. Hierarchical Temporal Memory (HTM), a third-generation unsupervised AI algorithm, models the neocortex of the human brain by simulating columns of neurons to process and predict sequences. These neuron columns can memorize and infer sequences across multiple orders. While multiorder inferences offer robust predictive capabilities, they often come with significant computational overhead. The Sequence Memory (SM) component of HTM, which manages these inferences, encounters bottlenecks primarily due to its extensive programmable interconnects. In many cases, it has been observed that first-order temporal relationships have proven to be sufficient without any significant loss in efficiency. This paper introduces a Reflex Memory (RM) block, inspired by the Spinal Cord's working mechanisms, designed to accelerate the processing of first-order inferences. The RM block performs these inferences significantly faster than the SM. The integration of RM with HTM forms a system called the Accelerated Hierarchical Temporal Memory (AHTM), which processes repetitive information more efficiently than the original HTM while still supporting multiorder inferences. The experimental results demonstrate that the HTM predicts an event in 0.945 s, whereas the AHTM module does so in 0.125 s. Additionally, the hardware implementation of RM in a content-addressable memory (CAM) block, known as Hardware-Accelerated Hierarchical Temporal Memory (H-AHTM), predicts an event in just 0.094 s, significantly improving inference speed. Compared to the original algorithm \\cite{bautista2020matlabhtm}, AHTM accelerates inference by up to 7.55x, while H-AHTM further enhances performance with a 10.10x speedup.", 'abstract_zh': '物联网的快速扩张生成了泽字节的数据，要求高效的无监督学习系统。层级时间记忆（HTM），一种第三代无监督人工智能算法，通过模拟神经元列来建模人脑的新皮层，处理和预测序列。这些神经元列可以在多个层次上记忆和推断序列。虽然多层次的推断提供了稳健的预测能力，但往往伴随着巨大的计算开销。HTM中的序列记忆（SM）组件管理这些推断，主要由于其广泛的可编程互联遇到了瓶颈。在许多情况下，观察到一阶时间关系已证明是足够的，而不会显著损失效率。本文提出了一种灵感来源于脊髓工作机制的反射记忆（RM）块，用于加速一阶推断的处理速度。RM块显著快于SM进行这些推断。RM块与HTM的结合形成了加速层级时间记忆（AHTM）系统，该系统在仍然支持多层次推断的同时，更高效地处理重复信息。实验结果表明，HTM预测事件所需时间为0.945秒，而AHTM模块仅需0.125秒。此外，RM在内容可寻址内存（CAM）块中的硬件实现——硬件加速层级时间记忆（H-AHTM）——预测事件所需时间仅为0.094秒，显著提高了推断速度。与原始算法相比，AHTM将推断加速7.55倍，而H-AHTM进一步提升了10.10倍的性能。', 'title_zh': '基于硬件加速反射记忆增强生物启发的分层临时记忆'}
{'arxiv_id': 'arXiv:2504.03744', 'title': 'Comparative Explanations: Explanation Guided Decision Making for Human-in-the-Loop Preference Selection', 'authors': 'Tanmay Chakraborty, Christian Wirth, Christin Seifert', 'link': 'https://arxiv.org/abs/2504.03744', 'abstract': "This paper introduces Multi-Output LOcal Narrative Explanation (MOLONE), a novel comparative explanation method designed to enhance preference selection in human-in-the-loop Preference Bayesian optimization (PBO). The preference elicitation in PBO is a non-trivial task because it involves navigating implicit trade-offs between vector-valued outcomes, subjective priorities of decision-makers, and decision-makers' uncertainty in preference selection. Existing explainable AI (XAI) methods for BO primarily focus on input feature importance, neglecting the crucial role of outputs (objectives) in human preference elicitation. MOLONE addresses this gap by providing explanations that highlight both input and output importance, enabling decision-makers to understand the trade-offs between competing objectives and make more informed preference selections. MOLONE focuses on local explanations, comparing the importance of input features and outcomes across candidate samples within a local neighborhood of the search space, thus capturing nuanced differences relevant to preference-based decision-making. We evaluate MOLONE within a PBO framework using benchmark multi-objective optimization functions, demonstrating its effectiveness in improving convergence compared to noisy preference selections. Furthermore, a user study confirms that MOLONE significantly accelerates convergence in human-in-the-loop scenarios by facilitating more efficient identification of preferred options.", 'abstract_zh': '多输出局部叙事解释(MOLONE)在人类在环 Preference Bayesian 优化中的新颖比较解释方法', 'title_zh': '比较解释：基于解释的决策制定以指导人类在环偏好选择'}
{'arxiv_id': 'arXiv:2504.03742', 'title': 'Hierarchical Local-Global Feature Learning for Few-shot Malicious Traffic Detection', 'authors': 'Songtao Peng, Lei Wang, Wu Shuai, Hao Song, Jiajun Zhou, Shanqing Yu, Qi Xuan', 'link': 'https://arxiv.org/abs/2504.03742', 'abstract': 'With the rapid growth of internet traffic, malicious network attacks have become increasingly frequent and sophisticated, posing significant threats to global cybersecurity. Traditional detection methods, including rule-based and machine learning-based approaches, struggle to accurately identify emerging threats, particularly in scenarios with limited samples. While recent advances in few-shot learning have partially addressed the data scarcity issue, existing methods still exhibit high false positive rates and lack the capability to effectively capture crucial local traffic patterns. In this paper, we propose HLoG, a novel hierarchical few-shot malicious traffic detection framework that leverages both local and global features extracted from network sessions. HLoG employs a sliding-window approach to segment sessions into phases, capturing fine-grained local interaction patterns through hierarchical bidirectional GRU encoding, while simultaneously modeling global contextual dependencies. We further design a session similarity assessment module that integrates local similarity with global self-attention-enhanced representations, achieving accurate and robust few-shot traffic classification. Comprehensive experiments on three meticulously reconstructed datasets demonstrate that HLoG significantly outperforms existing state-of-the-art methods. Particularly, HLoG achieves superior recall rates while substantially reducing false positives, highlighting its effectiveness and practical value in real-world cybersecurity applications.', 'abstract_zh': '随着互联网流量的迅速增长，恶意网络攻击事件变得更加频繁和复杂，对全球网络安全造成了重大威胁。传统的检测方法，包括基于规则和基于机器学习的方法，难以准确识别新兴威胁，特别是在样本有限的情况下。尽管最近在少样本学习方面的进步部分解决了数据稀缺问题，但现有方法仍存在较高的误报率，并且无法有效捕捉关键的局部流量模式。在本文中，我们提出了一种新的层次少样本恶意流量检测框架HLoG，该框架利用从网络会话中提取的局部和全局特征。HLoG采用滑动窗口方法将会话分割成不同的阶段，通过分层双向GRU编码捕捉精细的局部交互模式，同时建模全局上下文依赖关系。进一步设计了一个会话相似性评估模块，将局部相似性与全局自注意力增强表示相结合，实现准确且稳健的少样本流量分类。在三个精心重构的数据集上的全面实验表明，HLoG显著优于现有最先进的方法。特别是，HLoG在大幅减少误报的同时实现了更高的召回率，突显了其在实际网络安全应用中的有效性和实用性。', 'title_zh': '局部-全局特征层次学习在少量样本恶意流量检测中的应用'}
{'arxiv_id': 'arXiv:2504.03740', 'title': 'Brain Network Classification Based on Graph Contrastive Learning and Graph Transformer', 'authors': 'ZhiTeng Zhu, Lan Yao', 'link': 'https://arxiv.org/abs/2504.03740', 'abstract': 'The dynamic characterization of functional brain networks is of great significance for elucidating the mechanisms of human brain function. Although graph neural networks have achieved remarkable progress in functional network analysis, challenges such as data scarcity and insufficient supervision persist. To address the limitations of limited training data and inadequate supervision, this paper proposes a novel model named PHGCL-DDGformer that integrates graph contrastive learning with graph transformers, effectively enhancing the representation learning capability for brain network classification tasks. To overcome the constraints of existing graph contrastive learning methods in brain network feature extraction, an adaptive graph augmentation strategy combining attribute masking and edge perturbation is implemented for data enhancement. Subsequently, a dual-domain graph transformer (DDGformer) module is constructed to integrate local and global information, where graph convolutional networks aggregate neighborhood features to capture local patterns while attention mechanisms extract global dependencies. Finally, a graph contrastive learning framework is established to maximize the consistency between positive and negative pairs, thereby obtaining high-quality graph representations. Experimental results on real-world datasets demonstrate that the PHGCL-DDGformer model outperforms existing state-of-the-art approaches in brain network classification tasks.', 'abstract_zh': '功能脑网络的动态表征对于阐明人类脑功能机制具有重要意义。尽管图神经网络在功能网络分析方面取得了显著进展，但数据稀缺和监督不足等问题依然存在。为了解决有限训练数据和不足监督的限制，本文提出了一种名为PHGCL-DDGformer的新模型，该模型将图对比学习与图变压器相结合，有效增强了脑网络分类任务中的表示学习能力。为克服现有图对比学习方法在脑网络特征提取方面的限制，该模型实施了一种结合属性掩蔽和边扰动的自适应图增强策略，以实现数据增强。随后，构建了一个双域图变压器（DDGformer）模块，该模块整合了局部和全局信息；图卷积网络聚合邻域特征以捕获局部模式，而注意力机制则提取全局依赖关系。最后，建立了图对比学习框架以最大化正负样本对的一致性，从而获得高质量的图表示。实验结果表明，在真实数据集上的研究结果证明，PHGCL-DDGformer模型在脑网络分类任务上的表现优于现有最先进的方法。', 'title_zh': '基于图对比学习和图变换器的脑网络分类'}
{'arxiv_id': 'arXiv:2504.03738', 'title': 'Attention in Diffusion Model: A Survey', 'authors': 'Litao Hua, Fan Liu, Jie Su, Xingyu Miao, Zizhou Ouyang, Zeyu Wang, Runze Hu, Zhenyu Wen, Bing Zhai, Yang Long, Haoran Duan, Yuan Zhou', 'link': 'https://arxiv.org/abs/2504.03738', 'abstract': 'Attention mechanisms have become a foundational component in diffusion models, significantly influencing their capacity across a wide range of generative and discriminative tasks. This paper presents a comprehensive survey of attention within diffusion models, systematically analysing its roles, design patterns, and operations across different modalities and tasks. We propose a unified taxonomy that categorises attention-related modifications into parts according to the structural components they affect, offering a clear lens through which to understand their functional diversity. In addition to reviewing architectural innovations, we examine how attention mechanisms contribute to performance improvements in diverse applications. We also identify current limitations and underexplored areas, and outline potential directions for future research. Our study provides valuable insights into the evolving landscape of diffusion models, with a particular focus on the integrative and ubiquitous role of attention.', 'abstract_zh': '注意机制已成为扩散模型的基本组成部分，显著影响其在生成和判别任务中的能力。本文全面调研了注意机制在扩散模型中的应用，系统分析了其在不同模态和任务中的角色、设计模式和操作方式。我们提出了一种统一的分类体系，根据其影响的结构组件将注意相关的修改分为若干部分，提供了一种清晰的理解其功能多样性的视角。除了回顾架构创新，我们还探讨了注意机制如何在各种应用中提升性能。我们还指出了当前的局限性和未探索的领域，并概述了未来研究的潜在方向。我们的研究为理解扩散模型不断演化的景观提供了有价值的见解，特别关注注意机制的整合和普遍作用。', 'title_zh': '扩散模型中的注意力：一个综述'}
{'arxiv_id': 'arXiv:2504.03736', 'title': 'Uncertainty Propagation in XAI: A Comparison of Analytical and Empirical Estimators', 'authors': 'Teodor Chiaburu, Felix Bießmann, Frank Haußer', 'link': 'https://arxiv.org/abs/2504.03736', 'abstract': 'Understanding uncertainty in Explainable AI (XAI) is crucial for building trust and ensuring reliable decision-making in Machine Learning models. This paper introduces a unified framework for quantifying and interpreting Uncertainty in XAI by defining a general explanation function $e_{\\theta}(x, f)$ that captures the propagation of uncertainty from key sources: perturbations in input data and model parameters. By using both analytical and empirical estimates of explanation variance, we provide a systematic means of assessing the impact uncertainty on explanations. We illustrate the approach using a first-order uncertainty propagation as the analytical estimator. In a comprehensive evaluation across heterogeneous datasets, we compare analytical and empirical estimates of uncertainty propagation and evaluate their robustness. Extending previous work on inconsistencies in explanations, our experiments identify XAI methods that do not reliably capture and propagate uncertainty. Our findings underscore the importance of uncertainty-aware explanations in high-stakes applications and offer new insights into the limitations of current XAI methods. The code for the experiments can be found in our repository at this https URL', 'abstract_zh': '理解解释性人工智能（XAI）中的不确定性对于建立信任并确保机器学习模型可靠决策至关重要。本文提出了一个统一框架来量化和解释XAI中的不确定性，通过定义一个一般解释函数 \\(e_{\\theta}(x, f)\\) 来捕捉来自关键来源的不确定性传播：输入数据和模型参数的扰动。利用分析和经验解释方差的估计值，我们提供了一种系统方法来评估不确定性对解释的影响。我们使用一阶不确定性传播作为分析估计器来说明该方法。在跨异构数据集的全面评估中，我们将分析和经验不确定性传播估计值进行比较，并评估其稳健性。在先前关于解释不一致性的研究基础上，我们的实验识别出不能可靠地捕捉和传播不确定性的XAI方法。我们的发现强调了高风险应用中不确定性意识解释的重要性，并提供了关于当前XAI方法局限性的新见解。实验代码可以在我们的仓库中找到：this https URL。', 'title_zh': 'XAI中不确定性传播：分析估计器与经验估计器的比较'}
{'arxiv_id': 'arXiv:2504.03734', 'title': 'Artificial Geographically Weighted Neural Network: A Novel Framework for Spatial Analysis with Geographically Weighted Layers', 'authors': 'Jianfei Cao, Dongchao Wang', 'link': 'https://arxiv.org/abs/2504.03734', 'abstract': 'Geographically Weighted Regression (GWR) is a widely recognized technique for modeling spatial heterogeneity. However, it is commonly assumed that the relationships between dependent and independent variables are linear. To overcome this limitation, we propose an Artificial Geographically Weighted Neural Network (AGWNN), a novel framework that integrates geographically weighted techniques with neural networks to capture complex nonlinear spatial relationships. Central to this framework is the Geographically Weighted Layer (GWL), a specialized component designed to encode spatial heterogeneity within the neural network architecture. To rigorously evaluate the performance of AGWNN, we conducted comprehensive experiments using both simulated datasets and real-world case studies. Our results demonstrate that AGWNN significantly outperforms traditional GWR and standard Artificial Neural Networks (ANNs) in terms of model fitting accuracy. Notably, AGWNN excels in modeling intricate nonlinear relationships and effectively identifies complex spatial heterogeneity patterns, offering a robust and versatile tool for advanced spatial analysis.', 'abstract_zh': '地理加权神经网络（AGWNN）：一种融合地理加权技术的新型复杂非线性空间关系建模框架', 'title_zh': '基于地理加权层的人工地理加权神经网络：一种空间分析的新框架'}
{'arxiv_id': 'arXiv:2504.03733', 'title': 'Artificial Intelligence and Deep Learning Algorithms for Epigenetic Sequence Analysis: A Review for Epigeneticists and AI Experts', 'authors': 'Muhammad Tahir, Mahboobeh Norouzi, Shehroz S. Khan, James R. Davie, Soichiro Yamanaka, Ahmed Ashraf', 'link': 'https://arxiv.org/abs/2504.03733', 'abstract': 'Epigenetics encompasses mechanisms that can alter the expression of genes without changing the underlying genetic sequence. The epigenetic regulation of gene expression is initiated and sustained by several mechanisms such as DNA methylation, histone modifications, chromatin conformation, and non-coding RNA. The changes in gene regulation and expression can manifest in the form of various diseases and disorders such as cancer and congenital deformities. Over the last few decades, high throughput experimental approaches have been used to identify and understand epigenetic changes, but these laboratory experimental approaches and biochemical processes are time-consuming and expensive. To overcome these challenges, machine learning and artificial intelligence (AI) approaches have been extensively used for mapping epigenetic modifications to their phenotypic manifestations. In this paper we provide a narrative review of published research on AI models trained on epigenomic data to address a variety of problems such as prediction of disease markers, gene expression, enhancer promoter interaction, and chromatin states. The purpose of this review is twofold as it is addressed to both AI experts and epigeneticists. For AI researchers, we provided a taxonomy of epigenetics research problems that can benefit from an AI-based approach. For epigeneticists, given each of the above problems we provide a list of candidate AI solutions in the literature. We have also identified several gaps in the literature, research challenges, and recommendations to address these challenges.', 'abstract_zh': '表观遗传学包括不改变遗传序列即可改变基因表达的机制。表观遗传对基因表达的调控是由DNA甲基化、组蛋白修饰、染色质构象和非编码RNA等多种机制启动和维持的。基因调控和表达的变化可以表现为多种疾病和畸形，如癌症和先天性缺陷。在过去几十年中，高通量实验方法被用来识别和理解表观遗传变化，但这些实验室实验方法和生物化学过程耗时且成本高昂。为克服这些挑战，机器学习和人工智能（AI）方法被广泛用于将表观遗传修饰与其表型表型表现进行映射。在本文中，我们对基于表观基因组数据训练的AI模型进行了综述性研究，以解决各种问题，如疾病标志物预测、基因表达、增强子启动子相互作用和染色质状态。本文的目的是双重的，既面向AI专家也面向表观遗传学家。对于AI研究人员，我们提供了一种表观遗传学研究问题的分类，这些问题可以从基于AI的方法中受益。对于表观遗传学家，针对上述每个问题，我们提供了文献中候选的AI解决方案列表。我们还识别了文献中的若干空白、研究挑战，并提出了应对这些挑战的建议。', 'title_zh': '人工智能与深度学习算法在表观遗传序列分析中的应用：表观遗传学家与AI专家的综述'}
{'arxiv_id': 'arXiv:2504.03726', 'title': 'Detecting Malicious AI Agents Through Simulated Interactions', 'authors': 'Yulu Pi, Ella Bettison, Anna Becker', 'link': 'https://arxiv.org/abs/2504.03726', 'abstract': "This study investigates malicious AI Assistants' manipulative traits and whether the behaviours of malicious AI Assistants can be detected when interacting with human-like simulated users in various decision-making contexts. We also examine how interaction depth and ability of planning influence malicious AI Assistants' manipulative strategies and effectiveness. Using a controlled experimental design, we simulate interactions between AI Assistants (both benign and deliberately malicious) and users across eight decision-making scenarios of varying complexity and stakes. Our methodology employs two state-of-the-art language models to generate interaction data and implements Intent-Aware Prompting (IAP) to detect malicious AI Assistants. The findings reveal that malicious AI Assistants employ domain-specific persona-tailored manipulation strategies, exploiting simulated users' vulnerabilities and emotional triggers. In particular, simulated users demonstrate resistance to manipulation initially, but become increasingly vulnerable to malicious AI Assistants as the depth of the interaction increases, highlighting the significant risks associated with extended engagement with potentially manipulative systems. IAP detection methods achieve high precision with zero false positives but struggle to detect many malicious AI Assistants, resulting in high false negative rates. These findings underscore critical risks in human-AI interactions and highlight the need for robust, context-sensitive safeguards against manipulative AI behaviour in increasingly autonomous decision-support systems.", 'abstract_zh': '本研究探讨了恶意AI助手的操控特质，并分析在与人类模拟用户交互时，尤其是在各种决策情境下，是否能检测到恶意AI助手的行为。我们还探讨了交互深度和计划能力如何影响恶意AI助手的操控策略及其有效性。通过受控实验设计，我们在八个复杂程度和风险等级不同的决策场景中模拟了良性与恶意AI助手与用户之间的交互。我们的方法使用了两种最先进的语言模型生成交互数据，并采用了意图感知提示（IAP）来检测恶意AI助手。研究发现，恶意AI助手采用领域特定的人格定制操控策略，利用模拟用户的情感触发点和弱点进行操控。特别是，模拟用户最初对操控具有抵抗力，但随着交互深度的增加，他们变得越来越容易受到恶意AI助手的影响，突显了与潜在操控性系统进行长期交互的重大风险。IAP检测方法在没有假阳性的情况下实现了高精度，但难以检测许多恶意AI助手，导致高假阴性率。这些发现强调了人类与AI交互中的关键风险，并突出了在日益自主的支持决策系统中against操控性AI行为需要更加 robust且情境敏感的安全措施。', 'title_zh': '通过模拟交互检测恶意AI代理'}
{'arxiv_id': 'arXiv:2504.03721', 'title': 'A Hybrid Reinforcement Learning Framework for Hard Latency Constrained Resource Scheduling', 'authors': 'Luyuan Zhang, An Liu, Kexuan Wang', 'link': 'https://arxiv.org/abs/2504.03721', 'abstract': 'In the forthcoming 6G era, extend reality (XR) has been regarded as an emerging application for ultra-reliable and low latency communications (URLLC) with new traffic characteristics and more stringent requirements. In addition to the quasi-periodical traffic in XR, burst traffic with both large frame size and random arrivals in some real world low latency communication scenarios has become the leading cause of network congestion or even collapse, and there still lacks an efficient algorithm for the resource scheduling problem under burst traffic with hard latency constraints. We propose a novel hybrid reinforcement learning framework for resource scheduling with hard latency constraints (HRL-RSHLC), which reuses polices from both old policies learned under other similar environments and domain-knowledge-based (DK) policies constructed using expert knowledge to improve the performance. The joint optimization of the policy reuse probabilities and new policy is formulated as an Markov Decision Problem (MDP), which maximizes the hard-latency constrained effective throughput (HLC-ET) of users. We prove that the proposed HRL-RSHLC can converge to KKT points with an arbitrary initial point. Simulations show that HRL-RSHLC can achieve superior performance with faster convergence speed compared to baseline algorithms.', 'abstract_zh': '在未来的6G时代，增强现实(XR)被视为超可靠低延迟通信(URLLC)的一种新兴应用，具有新的业务特性和更严格的要求。除了XR中的准周期性流量外，在某些实际低延迟通信场景中，具有较大帧尺寸和随机到达时间的突发流量已成为网络拥塞甚至崩溃的主要原因，仍缺乏在具有严格延迟约束的突发流量下的高效资源调度算法。我们提出了一种新型混合强化学习框架（HRL-RSHLC）进行具有严格延迟约束的资源调度，该框架结合了基于旧环境学得策略和基于专家知识构建的域知识策略，以提高性能。将策略重用概率的联合优化与新策略的优化形式化为马尔可夫决策过程（MDP），以最大化用户的有效吞吐量（HLC-ET）。证明了所提出的HRL-RSHLC可以从任意初始点收敛到KKT点。仿真实验表明，与基线算法相比，HRL-RSHLC在收敛速度方面表现出更优的性能。', 'title_zh': '一种满足严苛延迟约束资源调度的混合强化学习框架'}
{'arxiv_id': 'arXiv:2504.03712', 'title': 'Scalable heliostat surface predictions from focal spots: Sim-to-Real transfer of inverse Deep Learning Raytracing', 'authors': 'Jan Lewen, Max Pargmann, Jenia Jitsev, Mehdi Cherti, Robert Pitz-Paal, Daniel Maldonado Quinto', 'link': 'https://arxiv.org/abs/2504.03712', 'abstract': 'Concentrating Solar Power (CSP) plants are a key technology in the transition toward sustainable energy. A critical factor for their safe and efficient operation is the distribution of concentrated solar flux on the receiver. However, flux distributions from individual heliostats are sensitive to surface imperfections. Measuring these surfaces across many heliostats remains impractical in real-world deployments. As a result, control systems often assume idealized heliostat surfaces, leading to suboptimal performance and potential safety risks. To address this, inverse Deep Learning Raytracing (iDLR) has been introduced as a novel method for inferring heliostat surface profiles from target images recorded during standard calibration procedures. In this work, we present the first successful Sim-to-Real transfer of iDLR, enabling accurate surface predictions directly from real-world target images. We evaluate our method on 63 heliostats under real operational conditions. iDLR surface predictions achieve a median mean absolute error (MAE) of 0.17 mm and show good agreement with deflectometry ground truth in 84% of cases. When used in raytracing simulations, it enables flux density predictions with a mean accuracy of 90% compared to deflectometry over our dataset, and outperforms the commonly used ideal heliostat surface assumption by 26%. We tested this approach in a challenging double-extrapolation scenario-involving unseen sun positions and receiver projection-and found that iDLR maintains high predictive accuracy, highlighting its generalization capabilities. Our results demonstrate that iDLR is a scalable, automated, and cost-effective solution for integrating realistic heliostat surface models into digital twins. This opens the door to improved flux control, more precise performance modeling, and ultimately, enhanced efficiency and safety in future CSP plants.', 'abstract_zh': '基于逆深度学习光线追踪的实到虚转移在集中太阳能动力系统中的表面预测研究', 'title_zh': '基于聚光点的可扩展抛物镜表面预测：从仿真到现实的逆深度学习光线追踪转移'}
{'arxiv_id': 'arXiv:2504.03700', 'title': 'SAFE: Self-Adjustment Federated Learning Framework for Remote Sensing Collaborative Perception', 'authors': 'Xiaohe Li, Haohua Wu, Jiahao Li, Zide Fan, Kaixin Zhang, Xinming Li, Yunping Ge, Xinyu Zhao', 'link': 'https://arxiv.org/abs/2504.03700', 'abstract': 'The rapid increase in remote sensing satellites has led to the emergence of distributed space-based observation systems. However, existing distributed remote sensing models often rely on centralized training, resulting in data leakage, communication overhead, and reduced accuracy due to data distribution discrepancies across platforms. To address these challenges, we propose the \\textit{Self-Adjustment FEderated Learning} (SAFE) framework, which innovatively leverages federated learning to enhance collaborative sensing in remote sensing scenarios. SAFE introduces four key strategies: (1) \\textit{Class Rectification Optimization}, which autonomously addresses class imbalance under unknown local and global distributions. (2) \\textit{Feature Alignment Update}, which mitigates Non-IID data issues via locally controlled EMA updates. (3) \\textit{Dual-Factor Modulation Rheostat}, which dynamically balances optimization effects during training. (4) \\textit{Adaptive Context Enhancement}, which is designed to improve model performance by dynamically refining foreground regions, ensuring computational efficiency with accuracy improvement across distributed satellites. Experiments on real-world image classification and object segmentation datasets validate the effectiveness and reliability of the SAFE framework in complex remote sensing scenarios.', 'abstract_zh': '遥感卫星的快速发展催生了分布式空间观测系统。然而，现有的分布式遥感模型通常依赖于集中训练，导致数据泄露、通信开销增大以及由于平台间数据分布差异而降低准确性。为应对这些挑战，我们提出了一种名为“Self-Adjustment Federated Learning”（SAFE）的框架，该框架创新性地利用联邦学习来增强遥感场景中的协同感知。SAFE引入了四个关键策略：(1) 类别校正优化，自主解决未知局部和全局分布下的类别不平衡问题。(2) 特征对齐更新，通过局部控制的指数移动平均（EMA）更新缓解非独立同分布（Non-IID）数据问题。(3) 双因子调节电位器，动态平衡训练期间的优化效果。(4) 适应回溯增强，旨在通过动态细化前景区域来提升模型性能，并确保在分布式卫星中提高计算效率和准确性。实世界图像分类和对象分割数据集上的实验验证了SAFE框架在复杂遥感场景中的有效性和可靠性。', 'title_zh': 'SAFE：自调整联邦学习框架用于遥感协同感知'}
{'arxiv_id': 'arXiv:2504.03695', 'title': 'Are Anxiety Detection Models Generalizable? A Cross-Activity and Cross-Population Study Using Wearables', 'authors': 'Nilesh Kumar Sahu, Snehil Gupta, Haroon R Lone', 'link': 'https://arxiv.org/abs/2504.03695', 'abstract': 'Anxiety-provoking activities, such as public speaking, can trigger heightened anxiety responses in individuals with anxiety disorders. Recent research suggests that physiological signals, including electrocardiogram (ECG) and electrodermal activity (EDA), collected via wearable devices, can be used to detect anxiety in such contexts through machine learning models. However, the generalizability of these anxiety prediction models across different activities and diverse populations remains underexplored-an essential step for assessing model bias and fostering user trust in broader applications. To address this gap, we conducted a study with 111 participants who engaged in three anxiety-provoking activities. Utilizing both our collected dataset and two well-known publicly available datasets, we evaluated the generalizability of anxiety detection models within participants (for both same-activity and cross-activity scenarios) and across participants (within-activity and cross-activity). In total, we trained and tested more than 3348 anxiety detection models (using six classifiers, 31 feature sets, and 18 train-test configurations). Our results indicate that three key metrics-AUROC, recall for anxious states, and recall for non-anxious states-were slightly above the baseline score of 0.5. The best AUROC scores ranged from 0.62 to 0.73, with recall for the anxious class spanning 35.19% to 74.3%. Interestingly, model performance (as measured by AUROC) remained relatively stable across different activities and participant groups, though recall for the anxious class did exhibit some variation.', 'abstract_zh': '焦虑诱发活动（如公开发言）可引起焦虑障碍患者的焦虑反应加剧。研究表明，通过穿戴设备收集的心电图（ECG）和电导率活动（EDA）等生理信号，可以通过机器学习模型在这些情境中检测焦虑。然而，这些焦虑预测模型在不同活动和多样化人群中的泛化能力尚未充分探索——这是评估模型偏差和促进更广泛应用用户信任的关键步骤。为弥补这一不足，我们进行了一个包括111名参与三种焦虑诱发活动的研究。利用我们收集的数据集及两个广为人知的公开数据集，我们评估了焦虑检测模型在参与者内部（同活动和跨活动情景）和参与者之间（同活动和跨活动情景）的泛化能力。总共，我们训练并测试了超过3348个焦虑检测模型（使用六种分类器、31个特征集和18种训练-测试配置）。结果显示，三个关键指标——AUCROC、焦虑状态的召回率和非焦虑状态的召回率——略高于基线分数0.5。最佳AUCROC得分范围从0.62到0.73，焦虑类别的召回率范围为35.19%到74.3%。有趣的是，模型性能（通过AUCROC衡量）在不同活动和参与者群体中相对稳定，尽管焦虑类别的召回率存在一些差异。', 'title_zh': '焦虑检测模型具有普遍适用性吗？可穿戴设备在跨活动和跨人群研究中的应用'}
{'arxiv_id': 'arXiv:2504.03690', 'title': 'Learning to Interfere in Non-Orthogonal Multiple-Access Joint Source-Channel Coding', 'authors': 'Selim F. Yilmaz, Can Karamanli, Deniz Gunduz', 'link': 'https://arxiv.org/abs/2504.03690', 'abstract': 'We consider multiple transmitters aiming to communicate their source signals (e.g., images) over a multiple access channel (MAC). Conventional communication systems minimize interference by orthogonally allocating resources (time and/or bandwidth) among users, which limits their capacity. We introduce a machine learning (ML)-aided wireless image transmission method that merges compression and channel coding using a multi-view autoencoder, which allows the transmitters to use all the available channel resources simultaneously, resulting in a non-orthogonal multiple access (NOMA) scheme. The receiver must recover all the images from the received superposed signal, while also associating each image with its transmitter. Traditional ML models deal with individual samples, whereas our model allows signals from different users to interfere in order to leverage gains from NOMA under limited bandwidth and power constraints. We introduce a progressive fine-tuning algorithm that doubles the number of users at each iteration, maintaining initial performance with orthogonalized user-specific projections, which is then improved through fine-tuning steps. Remarkably, our method scales up to 16 users and beyond, with only a 0.6% increase in the number of trainable parameters compared to a single-user model, significantly enhancing recovered image quality and outperforming existing NOMA-based methods over a wide range of datasets, metrics, and channel conditions. Our approach paves the way for more efficient and robust multi-user communication systems, leveraging innovative ML components and strategies.', 'abstract_zh': '基于机器学习的多视角自编码器辅助无线图像传输方法', 'title_zh': '学习在非正交多访问联合源-信道编码中进行干扰'}
{'arxiv_id': 'arXiv:2504.03688', 'title': 'CLCR: Contrastive Learning-based Constraint Reordering for Efficient MILP Solving', 'authors': 'Shuli Zeng, Mengjie Zhou, Sijia Zhang, Yixiang Hu, Feng Wu, Xiang-Yang Li', 'link': 'https://arxiv.org/abs/2504.03688', 'abstract': 'Constraint ordering plays a critical role in the efficiency of Mixed-Integer Linear Programming (MILP) solvers, particularly for large-scale problems where poorly ordered constraints trigger increased LP iterations and suboptimal search trajectories. This paper introduces CLCR (Contrastive Learning-based Constraint Reordering), a novel framework that systematically optimizes constraint ordering to accelerate MILP solving. CLCR first clusters constraints based on their structural patterns and then employs contrastive learning with a pointer network to optimize their sequence, preserving problem equivalence while improving solver efficiency. Experiments on benchmarks show CLCR reduces solving time by 30% and LP iterations by 25% on average, without sacrificing solution accuracy. This work demonstrates the potential of data-driven constraint ordering to enhance optimization models, offering a new paradigm for bridging mathematical programming with machine learning.', 'abstract_zh': '基于对比学习的约束重排序（CLCR）在混合整数线性规划中的应用', 'title_zh': 'CLCR：基于对比学习的约束重排序以实现高效的混合整数规划求解'}
{'arxiv_id': 'arXiv:2504.03687', 'title': 'Process Optimization and Deployment for Sensor-Based Human Activity Recognition Based on Deep Learning', 'authors': 'Hanyu Liu, Ying Yu, Hang Xiao, Siyao Li, Xuze Li, Jiarui Li, Haotian Tang', 'link': 'https://arxiv.org/abs/2504.03687', 'abstract': 'Sensor-based human activity recognition is a key technology for many human-centered intelligent applications. However, this research is still in its infancy and faces many unresolved challenges. To address these, we propose a comprehensive optimization process approach centered on multi-attention interaction. We first utilize unsupervised statistical feature-guided diffusion models for highly adaptive data enhancement, and introduce a novel network architecture-Multi-branch Spatiotemporal Interaction Network, which uses multi-branch features at different levels to effectively Sequential ), which uses multi-branch features at different levels to effectively Sequential spatio-temporal interaction to enhance the ability to mine advanced latent features. In addition, we adopt a multi-loss function fusion strategy in the training phase to dynamically adjust the fusion weights between batches to optimize the training results. Finally, we also conducted actual deployment on embedded devices to extensively test the practical feasibility of the proposed method in existing work. We conduct extensive testing on three public datasets, including ablation studies, comparisons of related work, and embedded deployments.', 'abstract_zh': '基于传感器的人类活动识别是许多以人类为中心的智能应用的关键技术。然而，这一研究领域仍处于初级阶段，面临许多未解决的挑战。为此，我们提出了一种以多关注交互为中心的综合优化过程。首先，我们利用无监督的统计特征引导扩散模型进行高度适应的数据增强，并引入了一种新的网络架构——多分支时空交互网络，该网络在不同层次使用多分支特征有效地进行时空序列交互，以增强挖掘高级潜在特征的能力。此外，在训练阶段，我们采用了多损失函数融合策略，动态调整批次之间的融合权重以优化训练结果。最后，我们在嵌入式设备上进行了实际部署，广泛测试了所提出方法在现有工作中应用的实用性。我们对三个公开数据集进行了广泛的测试，包括消融研究、相关工作中方法的比较以及嵌入式部署。', 'title_zh': '基于深度学习的传感器驱动的人类活动识别过程优化与部署'}
{'arxiv_id': 'arXiv:2504.03686', 'title': 'Revisiting Outage for Edge Inference Systems', 'authors': 'Zhanwei Wang, Qunsong Zeng, Haotian Zheng, Kaibin Huang', 'link': 'https://arxiv.org/abs/2504.03686', 'abstract': 'One of the key missions of sixth-generation (6G) mobile networks is to deploy large-scale artificial intelligence (AI) models at the network edge to provide remote-inference services for edge devices. The resultant platform, known as edge inference, will support a wide range of Internet-of-Things applications, such as autonomous driving, industrial automation, and augmented reality. Given the mission-critical and time-sensitive nature of these tasks, it is essential to design edge inference systems that are both reliable and capable of meeting stringent end-to-end (E2E) latency constraints. Existing studies, which primarily focus on communication reliability as characterized by channel outage probability, may fail to guarantee E2E performance, specifically in terms of E2E inference accuracy and latency. To address this limitation, we propose a theoretical framework that introduces and mathematically characterizes the inference outage (InfOut) probability, which quantifies the likelihood that the E2E inference accuracy falls below a target threshold. Under an E2E latency constraint, this framework establishes a fundamental tradeoff between communication overhead (i.e., uploading more sensor observations) and inference reliability as quantified by the InfOut probability. To find a tractable way to optimize this tradeoff, we derive accurate surrogate functions for InfOut probability by applying a Gaussian approximation to the distribution of the received discriminant gain. Experimental results demonstrate the superiority of the proposed design over conventional communication-centric approaches in terms of E2E inference reliability.', 'abstract_zh': '第六代（6G）移动网络的关键使命之一是在网络边缘部署大规模人工智能模型，以提供边缘设备的远程推断服务。由此形成的边缘推断平台将支持自动驾驶、工业自动化和增强现实等广泛的应用。鉴于这些任务的关键性和时间敏感性，设计既可靠又能满足严格端到端（E2E）延迟约束的边缘推断系统至关重要。现有研究主要关注由信道 outage 概率表征的通信可靠性，可能无法保证E2E性能，特别是E2E推断准确性和延迟。为解决这一局限性，我们提出了一种理论框架，引入并从数学上刻画了推断 outage（InfOut）概率，以量化E2E推断准确度低于目标阈值的可能性。在E2E延迟约束下，该框架建立了通信开销（即上传更多传感器观察数据）与由InfOut概率量化推断可靠性之间的基本权衡。为了找到优化这一权衡的可行方法，我们通过高斯近似推导了InfOut概率的准确替代函数。实验结果表明，所提出的设计在E2E推断可靠性方面优于传统的以通信为中心的方法。', 'title_zh': '重新审视边缘推理系统的中断问题'}
{'arxiv_id': 'arXiv:2504.03682', 'title': 'Intelligent Resource Allocation Optimization for Cloud Computing via Machine Learning', 'authors': 'Yuqing Wang, Xiao Yang', 'link': 'https://arxiv.org/abs/2504.03682', 'abstract': 'With the rapid expansion of cloud computing applications, optimizing resource allocation has become crucial for improving system performance and cost efficiency. This paper proposes an intelligent resource allocation algorithm that leverages deep learning (LSTM) for demand prediction and reinforcement learning (DQN) for dynamic scheduling. By accurately forecasting computing resource demands and enabling real-time adjustments, the proposed system enhances resource utilization by 32.5%, reduces average response time by 43.3%, and lowers operational costs by 26.6%. Experimental results in a production cloud environment confirm that the method significantly improves efficiency while maintaining high service quality. This study provides a scalable and effective solution for intelligent cloud resource management, offering valuable insights for future cloud optimization strategies.', 'abstract_zh': '随着云 computing 应用的迅速扩展，优化资源分配已成为提升系统性能和成本效率的关键。本文提出了一种智能资源分配算法，该算法利用深度学习（LSTM）进行需求预测，并利用强化学习（DQN）进行动态调度。通过准确预测计算资源需求并实现实时调整，所提出系统将资源利用率提高32.5%，平均响应时间缩短43.3%，运营成本降低26.6%。在生产云环境中进行的实验结果证实，该方法显著提高了效率，同时保持了高质量的服务。本文为智能云资源管理提供了可扩展和有效的解决方案，并为未来的云优化策略提供了宝贵见解。', 'title_zh': '基于机器学习的云computing智能资源分配优化'}
{'arxiv_id': 'arXiv:2504.03671', 'title': 'HiAER-Spike: Hardware-Software Co-Design for Large-Scale Reconfigurable Event-Driven Neuromorphic Computing', 'authors': 'Gwenevere Frank, Gopabandhu Hota, Keli Wang, Abhinav Uppal, Omowuyi Olajide, Kenneth Yoshimoto, Leif Gibb, Qingbo Wang, Johannes Leugering, Stephen Deiss, Gert Cauwenberghs', 'link': 'https://arxiv.org/abs/2504.03671', 'abstract': "In this work, we present HiAER-Spike, a modular, reconfigurable, event-driven neuromorphic computing platform designed to execute large spiking neural networks with up to 160 million neurons and 40 billion synapses - roughly twice the neurons of a mouse brain at faster-than real-time. This system, which is currently under construction at the UC San Diego Supercomputing Center, comprises a co-designed hard- and software stack that is optimized for run-time massively parallel processing and hierarchical address-event routing (HiAER) of spikes while promoting memory-efficient network storage and execution. Our architecture efficiently handles both sparse connectivity and sparse activity for robust and low-latency event-driven inference for both edge and cloud computing. A Python programming interface to HiAER-Spike, agnostic to hardware-level detail, shields the user from complexity in the configuration and execution of general spiking neural networks with virtually no constraints in topology. The system is made easily available over a web portal for use by the wider community. In the following we provide an overview of the hard- and software stack, explain the underlying design principles, demonstrate some of the system's capabilities and solicit feedback from the broader neuromorphic community.", 'abstract_zh': 'HiAER-Spike：一种模块化、可重构、事件驱动的神经形态计算平台', 'title_zh': 'HiAER-Spike: 硬件-软件协同设计的大规模可重构事件驱动神经形态计算'}
{'arxiv_id': 'arXiv:2504.03669', 'title': 'Self-Learning-Based Optimization for Free-form Pipe Routing in Aeroengine with Dynamic Design Environment', 'authors': 'Caicheng Wang, Zili Wang, Shuyou Zhang, Yongzhe Xiang, Zheyi Li, Jianrong Tan', 'link': 'https://arxiv.org/abs/2504.03669', 'abstract': 'Pipe routing is a highly complex, time-consuming, and no-deterministic polynomial-time hard (NP-hard) problem in aeroengine design. Despite extensive research efforts in optimizing constant-curvature pipe routing, the growing demand for free-form pipes poses new challenges. Dynamic design environments and fuzzy layout rules further impact the optimization performance and efficiency. To tackle these challenges, this study proposes a self-learning-based method (SLPR) for optimizing free-form pipe routing in aeroengines. The SLPR is based on the proximal policy optimization (PPO) algorithm and integrates a unified rule modeling framework for efficient obstacle detection and fuzzy rule modeling in continuous space. Additionally, a potential energy table is constructed to enable rapid queries of layout tendencies and interference. The agent within SLPR iteratively refines pipe routing and accumulates the design knowledge through interaction with the environment. Once the design environment shifts, the agent can swiftly adapt by fine-tuning network parameters. Comparative tests reveal that SLPR ensures smooth pipe routing through cubic non-uniform B-spline (NURBS) curves, avoiding redundant pipe segments found in constant-curvature pipe routing. Results in both static and dynamic design environments demonstrate that SLPR outperforms three representative baselines in terms of the pipe length reduction, the adherence to layout rules, the path complexity, and the computational efficiency. Furthermore, tests in dynamic environments indicate that SLPR eliminates labor-intensive searches from scratch and even yields superior solutions compared to the retrained model. These results highlight the practical value of SLPR for real-world pipe routing, meeting lightweight, precision, and sustainability requirements of the modern aeroengine design.', 'abstract_zh': '基于自学习的方法（SLPR）优化航空发动机自由形管路由问题', 'title_zh': '自学习优化在动态设计环境中的自形管道 routing 优化在航空发动机中'}
{'arxiv_id': 'arXiv:2504.03650', 'title': 'BoxRL-NNV: Boxed Refinement of Latin Hypercube Samples for Neural Network Verification', 'authors': 'Sarthak Das', 'link': 'https://arxiv.org/abs/2504.03650', 'abstract': "BoxRL-NNV is a Python tool for the detection of safety violations in neural networks by computing the bounds of the output variables, given the bounds of the input variables of the network. This is done using global extrema estimation via Latin Hypercube Sampling, and further refinement using L-BFGS-B for local optimization around the initial guess. This paper presents an overview of BoxRL-NNV, as well as our results for a subset of the ACAS Xu benchmark. A complete evaluation of the tool's performance, including benchmark comparisons with state-of-the-art tools, shall be presented at the Sixth International Verification of Neural Networks Competition (VNN-COMP'25).", 'abstract_zh': "BoxRL-NNV是用于通过计算输出变量的界限来检测神经网络中的安全违规行为的Python工具，给定网络输入变量的界限。这是通过使用拉丁超立方抽样进行全局极值估计，并进一步使用L-BFGS-B进行局部优化来实现的。本文介绍了BoxRL-NNV的概述及其在ACAS Xu基准部分的数据结果。该工具的全面评估，包括与最新工具的基准比较，将在第六届国际神经网络验证竞赛（VNN-COMP'25）中呈现。", 'title_zh': 'BoxRL-NNV: 盒子精炼的拉丁超立方样本在神经网络验证中的应用'}
{'arxiv_id': 'arXiv:2504.03643', 'title': 'Potential Indicator for Continuous Emotion Arousal by Dynamic Neural Synchrony', 'authors': 'Guandong Pan, Zhaobang Wu, Yaqian Yang, Xin Wang, Longzhao Liu, Zhiming Zheng, Shaoting Tang', 'link': 'https://arxiv.org/abs/2504.03643', 'abstract': "The need for automatic and high-quality emotion annotation is paramount in applications such as continuous emotion recognition and video highlight detection, yet achieving this through manual human annotations is challenging. Inspired by inter-subject correlation (ISC) utilized in neuroscience, this study introduces a novel Electroencephalography (EEG) based ISC methodology that leverages a single-electrode and feature-based dynamic approach. Our contributions are three folds. Firstly, we reidentify two potent emotion features suitable for classifying emotions-first-order difference (FD) an differential entropy (DE). Secondly, through the use of overall correlation analysis, we demonstrate the heterogeneous synchronized performance of electrodes. This performance aligns with neural emotion patterns established in prior studies, thus validating the effectiveness of our approach. Thirdly, by employing a sliding window correlation technique, we showcase the significant consistency of dynamic ISCs across various features or key electrodes in each analyzed film clip. Our findings indicate the method's reliability in capturing consistent, dynamic shared neural synchrony among individuals, triggered by evocative film stimuli. This underscores the potential of our approach to serve as an indicator of continuous human emotion arousal. The implications of this research are significant for advancements in affective computing and the broader neuroscience field, suggesting a streamlined and effective tool for emotion analysis in real-world applications.", 'abstract_zh': '基于EEG的单电极特征动态方法在情绪注释中的应用：自动和高质量情绪标注的需求', 'title_zh': '持续情感唤醒的动态神经同步潜在指标'}
{'arxiv_id': 'arXiv:2409.18219', 'title': 'Packet Inspection Transformer: A Self-Supervised Journey to Unseen Malware Detection with Few Samples', 'authors': 'Kyle Stein, Arash Mahyari, Guillermo Francia III, Eman El-Sheikh', 'link': 'https://arxiv.org/abs/2409.18219', 'abstract': 'As networks continue to expand and become more interconnected, the need for novel malware detection methods becomes more pronounced. Traditional security measures are increasingly inadequate against the sophistication of modern cyber attacks. Deep Packet Inspection (DPI) has been pivotal in enhancing network security, offering an in-depth analysis of network traffic that surpasses conventional monitoring techniques. DPI not only examines the metadata of network packets, but also dives into the actual content being carried within the packet payloads, providing a comprehensive view of the data flowing through networks. While the integration of advanced deep learning techniques with DPI has introduced modern methodologies into malware detection and network traffic classification, state-of-the-art supervised learning approaches are limited by their reliance on large amounts of annotated data and their inability to generalize to novel, unseen malware threats. To address these limitations, this paper leverages the recent advancements in self-supervised learning (SSL) and few-shot learning (FSL). Our proposed self-supervised approach trains a transformer via SSL to learn the embedding of packet content, including payload, from vast amounts of unlabeled data by masking portions of packets, leading to a learned representation that generalizes to various downstream tasks. Once the representation is extracted from the packets, they are used to train a malware detection algorithm. The representation obtained from the transformer is then used to adapt the malware detector to novel types of attacks using few-shot learning approaches. Our experimental results demonstrate that our method achieves classification accuracies of up to 94.76% on the UNSW-NB15 dataset and 83.25% on the CIC-IoT23 dataset.', 'abstract_zh': '随着网络不断扩展和互联互通，新型恶意软件检测方法的需求日益凸显。传统安全措施越来越难以应对现代网络攻击的复杂性。深度包检测（DPI）在提升网络安全性方面发挥了关键作用，它提供了超越传统监控技术的深入网络流量分析。DPI不仅检查网络包的元数据，还深入分析包载荷的实际内容，提供网络中传输数据的全面视图。尽管将高级深度学习技术与DPI结合引入了先进的恶意软件检测和网络流量分类方法，但最先进的监督学习方法受限于对大量标注数据的依赖以及无法泛化到新型未知恶意软件威胁。为解决这些限制，本文利用自监督学习（SSL）和少样本学习（FSL）的最新进展。我们提出了一种自监督方法，通过SSL训练一个变换器，从大量未标注数据中学习包内容（包括载荷）的表示，通过掩蔽包部分内容，生成适用于各种下游任务的泛化表示。一旦从包中提取表示，就用于训练恶意软件检测算法。从变换器获得的表示随后用于使用少样本学习方法适应新型攻击类型的恶意软件检测器。我们的实验结果表明，该方法在UNSW-NB15数据集上的分类准确率高达94.76%，在CIC-IoT23数据集上的准确率为83.25%。', 'title_zh': '包检查变换器：基于少量样本的自监督非看见恶意软件检测之旅'}
