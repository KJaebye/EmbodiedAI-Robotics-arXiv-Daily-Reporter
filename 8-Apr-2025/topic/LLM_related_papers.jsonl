{'arxiv_id': 'arXiv:2504.05084', 'title': 'Speech-to-Trajectory: Learning Human-Like Verbal Guidance for Robot Motion', 'authors': 'Eran Beeri Bamani, Eden Nissinman, Rotem Atari, Nevo Heimann Saadon, Avishai Sintov', 'link': 'https://arxiv.org/abs/2504.05084', 'abstract': "Full integration of robots into real-life applications necessitates their ability to interpret and execute natural language directives from untrained users. Given the inherent variability in human language, equivalent directives may be phrased differently, yet require consistent robot behavior. While Large Language Models (LLMs) have advanced language understanding, they often falter in handling user phrasing variability, rely on predefined commands, and exhibit unpredictable outputs. This letter introduces the Directive Language Model (DLM), a novel speech-to-trajectory framework that directly maps verbal commands to executable motion trajectories, bypassing predefined phrases. DLM utilizes Behavior Cloning (BC) on simulated demonstrations of human-guided robot motion. To enhance generalization, GPT-based semantic augmentation generates diverse paraphrases of training commands, labeled with the same motion trajectory. DLM further incorporates a diffusion policy-based trajectory generation for adaptive motion refinement and stochastic sampling. In contrast to LLM-based methods, DLM ensures consistent, predictable motion without extensive prompt engineering, facilitating real-time robotic guidance. As DLM learns from trajectory data, it is embodiment-agnostic, enabling deployment across diverse robotic platforms. Experimental results demonstrate DLM's improved command generalization, reduced dependence on structured phrasing, and achievement of human-like motion.", 'abstract_zh': '全集成到现实应用中的机器人需要能够理解和执行未训练用户发出的自然语言指令。鉴于人类语言的固有变异性，等效的指令可能有不同的表达方式，但需要一致的机器人行为。虽然大型语言模型（LLMs）提升了语言理解能力，但在处理用户表达的变异性方面通常表现不佳，依赖预定义命令，并表现出不可预测的输出。本信介绍了一种新型指令语言模型（DLM），该模型直接将口头命令映射到可执行的运动轨迹，省去了预定义的命令。DLM 利用模拟的人引导机器人运动示范进行行为克隆（BC）。为了增强泛化能力，基于GPT的语义增强生成多样化的训练指令同义句，标注相同的运动轨迹。DLM 进一步结合了基于扩散策略的轨迹生成，实现适应性运动细化和随机采样。与基于LLM的方法不同，DLM 确保了运动的一致性和可预测性，无需大量的提示工程，促进实时的机器人引导。由于DLM从轨迹数据中学习，因此具有体表无关性，可在多种机器人平台上部署。实验结果表明，DLM 提高了命令泛化能力，减少了对结构化表达的依赖，并实现了类人运动。', 'title_zh': '语音转换为运动：学习类人语音指导以实现机器人运动'}
{'arxiv_id': 'arXiv:2504.04562', 'title': 'Planning Safety Trajectories with Dual-Phase, Physics-Informed, and Transportation Knowledge-Driven Large Language Models', 'authors': 'Rui Gan, Pei Li, Keke Long, Bocheng An, Junwei You, Keshu Wu, Bin Ran', 'link': 'https://arxiv.org/abs/2504.04562', 'abstract': "Foundation models have demonstrated strong reasoning and generalization capabilities in driving-related tasks, including scene understanding, planning, and control. However, they still face challenges in hallucinations, uncertainty, and long inference latency. While existing foundation models have general knowledge of avoiding collisions, they often lack transportation-specific safety knowledge. To overcome these limitations, we introduce LetsPi, a physics-informed, dual-phase, knowledge-driven framework for safe, human-like trajectory planning. To prevent hallucinations and minimize uncertainty, this hybrid framework integrates Large Language Model (LLM) reasoning with physics-informed social force dynamics. LetsPi leverages the LLM to analyze driving scenes and historical information, providing appropriate parameters and target destinations (goals) for the social force model, which then generates the future trajectory. Moreover, the dual-phase architecture balances reasoning and computational efficiency through its Memory Collection phase and Fast Inference phase. The Memory Collection phase leverages the physics-informed LLM to process and refine planning results through reasoning, reflection, and memory modules, storing safe, high-quality driving experiences in a memory bank. Surrogate safety measures and physics-informed prompt techniques are introduced to enhance the LLM's knowledge of transportation safety and physical force, respectively. The Fast Inference phase extracts similar driving experiences as few-shot examples for new scenarios, while simplifying input-output requirements to enable rapid trajectory planning without compromising safety. Extensive experiments using the HighD dataset demonstrate that LetsPi outperforms baseline models across five safety this http URL PDF for project Github link.", 'abstract_zh': '基于物理的双阶段知识驱动框架LetςPi实现安全的人类似轨迹规划', 'title_zh': '基于双阶段、物理启发和交通知识驱动的大语言模型规划安全轨迹'}
{'arxiv_id': 'arXiv:2504.04419', 'title': 'Driving-RAG: Driving Scenarios Embedding, Search, and RAG Applications', 'authors': 'Cheng Chang, Jingwei Ge, Jiazhe Guo, Zelin Guo, Binghong Jiang, Li Li', 'link': 'https://arxiv.org/abs/2504.04419', 'abstract': 'Driving scenario data play an increasingly vital role in the development of intelligent vehicles and autonomous driving. Accurate and efficient scenario data search is critical for both online vehicle decision-making and planning, and offline scenario generation and simulations, as it allows for leveraging the scenario experiences to improve the overall performance. Especially with the application of large language models (LLMs) and Retrieval-Augmented-Generation (RAG) systems in autonomous driving, urgent requirements are put forward. In this paper, we introduce the Driving-RAG framework to address the challenges of efficient scenario data embedding, search, and applications for RAG systems. Our embedding model aligns fundamental scenario information and scenario distance metrics in the vector space. The typical scenario sampling method combined with hierarchical navigable small world can perform efficient scenario vector search to achieve high efficiency without sacrificing accuracy. In addition, the reorganization mechanism by graph knowledge enhances the relevance to the prompt scenarios and augment LLM generation. We demonstrate the effectiveness of the proposed framework on typical trajectory planning task for complex interactive scenarios such as ramps and intersections, showcasing its advantages for RAG applications.', 'abstract_zh': '自动驾驶场景数据在智能车辆和自主驾驶发展中的作用日益重要。准确高效的场景数据搜索对于在线车辆决策和规划以及离线场景生成和仿真至关重要，因为它能够利用场景经验提升整体性能。特别是在大规模语言模型（LLMs）和检索增强生成（RAG）系统应用于自主驾驶时，提出了迫切的需求。本文介绍Drive-RAG框架以解决RAG系统中高效场景数据嵌入、搜索和应用的挑战。我们的嵌入模型在向量空间中对基本场景信息和场景距离度量进行对齐。结合典型场景采样方法和分层可导航的小世界图，可以实现高效场景向量搜索，在不牺牲准确性的前提下达到高的效率。此外，通过图知识进行重新组织机制增强了对提示场景的相关性并增强LLM生成。我们在复杂交互场景（如匝道和交叉口）的典型轨迹规划任务上展示了所提框架的有效性，强调了其在RAG应用中的优势。', 'title_zh': '驾驶场景嵌入、搜索与RAG应用'}
{'arxiv_id': 'arXiv:2504.04550', 'title': 'Advancing Egocentric Video Question Answering with Multimodal Large Language Models', 'authors': 'Alkesh Patel, Vibhav Chitalia, Yinfei Yang', 'link': 'https://arxiv.org/abs/2504.04550', 'abstract': "Egocentric Video Question Answering (QA) requires models to handle long-horizon temporal reasoning, first-person perspectives, and specialized challenges like frequent camera movement. This paper systematically evaluates both proprietary and open-source Multimodal Large Language Models (MLLMs) on QaEgo4Dv2 - a refined dataset of egocentric videos derived from QaEgo4D. Four popular MLLMs (GPT-4o, Gemini-1.5-Pro, Video-LLaVa-7B and Qwen2-VL-7B-Instruct) are assessed using zero-shot and fine-tuned approaches for both OpenQA and CloseQA settings. We introduce QaEgo4Dv2 to mitigate annotation noise in QaEgo4D, enabling more reliable comparison. Our results show that fine-tuned Video-LLaVa-7B and Qwen2-VL-7B-Instruct achieve new state-of-the-art performance, surpassing previous benchmarks by up to +2.6% ROUGE/METEOR (for OpenQA) and +13% accuracy (for CloseQA). We also present a thorough error analysis, indicating the model's difficulty in spatial reasoning and fine-grained object recognition - key areas for future improvement.", 'abstract_zh': '自视点视频问答要求模型处理长时序推理、第一人称视角以及频繁的摄像机运动等专门挑战。本文系统性地评估了私有和开源多模态大型语言模型（MLLMs）在QaEgo4Dv2上的性能，该数据集是基于QaEgo4D精选而来。四种流行的MLLMs（GPT-4o、Gemini-1.5-Pro、Video-LLaVa-7B和Qwen2-VL-7B-Instruct）在开放式和封闭式问答设置中分别采用零样本和微调方法进行评估。我们引入QaEgo4Dv2以减轻QaEgo4D中的注释噪声，从而实现更可靠地比较。结果显示，微调后的Video-LLaVa-7B和Qwen2-VL-7B-Instruct在开放式和封闭式问答设置中分别取得了新的最佳性能，分别比前一基准提高了高达2.6%的ROUGE/METEOR和13%的准确率。此外，我们还进行了详细错误分析，指出了模型在空间推理和细粒度物体识别等方面的困难，这是未来需要改进的关键领域。', 'title_zh': '基于多模态大型语言模型的以自我为中心的视频问答研究'}
{'arxiv_id': 'arXiv:2504.04040', 'title': 'ADAPT: Actively Discovering and Adapting to Preferences for any Task', 'authors': 'Maithili Patel, Xavier Puig, Ruta Desai, Roozbeh Mottaghi, Sonia Chernova, Joanne Truong, Akshara Rai', 'link': 'https://arxiv.org/abs/2504.04040', 'abstract': "Assistive agents should be able to perform under-specified long-horizon tasks while respecting user preferences. We introduce Actively Discovering and Adapting to Preferences for any Task (ADAPT) -- a benchmark designed to evaluate agents' ability to adhere to user preferences across various household tasks through active questioning. Next, we propose Reflection-DPO, a novel training approach for adapting large language models (LLMs) to the task of active questioning. Reflection-DPO finetunes a 'student' LLM to follow the actions of a privileged 'teacher' LLM, and optionally ask a question to gather necessary information to better predict the teacher action. We find that prior approaches that use state-of-the-art LLMs fail to sufficiently follow user preferences in ADAPT due to insufficient questioning and poor adherence to elicited preferences. In contrast, Reflection-DPO achieves a higher rate of satisfying user preferences, outperforming a zero-shot chain-of-thought baseline by 6.1% on unseen users.", 'abstract_zh': '辅助代理应该能够在尊重用户偏好的情况下执行未明确描述的长期任务。我们提出了Actively Discovering and Adapting to Preferences for any Task (ADAPT)——一个用于评估代理在通过主动询问的方式跨多种家庭任务中遵守用户偏好的能力的基准。接下来，我们提出了Reflection-DPO，这是一种用于使大规模语言模型（LLMs）适应主动询问任务的新型训练方法。Reflection-DPO 将一个“学生”LLM 细调为跟随一个特权“教师”LLM 的行动，并可选地提出问题以收集必要的信息以更好地预测教师行为。我们发现，使用最先进的LLM的先前方法在ADAPT中未能充分遵循用户偏好，原因是对用户偏好的询问不足以及对引出的偏好的不良遵守。相比之下，Reflection-DPO 实现了更高的用户偏好满足率，在未见过的用户中，其性能超越了零样本推理基线6.1%。', 'title_zh': 'ADAPT: 主动发现和适应任何任务的偏好'}
{'arxiv_id': 'arXiv:2504.03810', 'title': 'Hierarchically Encapsulated Representation for Protocol Design in Self-Driving Labs', 'authors': 'Yu-Zhe Shi, Mingchen Liu, Fanxu Meng, Qiao Xu, Zhangqian Bi, Kun He, Lecheng Ruan, Qining Wang', 'link': 'https://arxiv.org/abs/2504.03810', 'abstract': 'Self-driving laboratories have begun to replace human experimenters in performing single experimental skills or predetermined experimental protocols. However, as the pace of idea iteration in scientific research has been intensified by Artificial Intelligence, the demand for rapid design of new protocols for new discoveries become evident. Efforts to automate protocol design have been initiated, but the capabilities of knowledge-based machine designers, such as Large Language Models, have not been fully elicited, probably for the absence of a systematic representation of experimental knowledge, as opposed to isolated, flatten pieces of information. To tackle this issue, we propose a multi-faceted, multi-scale representation, where instance actions, generalized operations, and product flow models are hierarchically encapsulated using Domain-Specific Languages. We further develop a data-driven algorithm based on non-parametric modeling that autonomously customizes these representations for specific domains. The proposed representation is equipped with various machine designers to manage protocol design tasks, including planning, modification, and adjustment. The results demonstrate that the proposed method could effectively complement Large Language Models in the protocol design process, serving as an auxiliary module in the realm of machine-assisted scientific exploration.', 'abstract_zh': '自驾驶实验室已经开始用以执行单个实验技能或预设的实验协议代替人类实验员。然而，随着人工智能加速了科学研究中的创意迭代，为新发现快速设计新协议的需求变得明显。虽然已经开始尝试自动化协议设计，但基于知识的机器设计师，如大型语言模型的能力尚未完全发挥，可能是因为缺乏对实验知识系统化的表示，而只是孤立的扁平化信息。为解决这一问题，我们提出了一种多层次、多尺度的表示方法，其中实例动作、泛化的操作和产品流程模型通过领域特定语言进行分层封装。我们进一步开发了一种基于非参数建模的数据驱动算法，能够自主适配这些表示以特定领域。所提出的表示方法配备了各种机器设计师，用于管理协议设计任务，包括规划、修改和调整。实验结果表明，所提出的方法可以在协议设计过程中有效补充大型语言模型，作为一种辅助模块存在于机器辅助的科学探索领域。', 'title_zh': '自驾驶实验室中协议设计的分层封装表示'}
{'arxiv_id': 'arXiv:2504.05278', 'title': 'The challenge of uncertainty quantification of large language models in medicine', 'authors': 'Zahra Atf, Seyed Amir Ahmad Safavi-Naini, Peter R. Lewis, Aref Mahjoubfar, Nariman Naderi, Thomas R. Savage, Ali Soroush', 'link': 'https://arxiv.org/abs/2504.05278', 'abstract': 'This study investigates uncertainty quantification in large language models (LLMs) for medical applications, emphasizing both technical innovations and philosophical implications. As LLMs become integral to clinical decision-making, accurately communicating uncertainty is crucial for ensuring reliable, safe, and ethical AI-assisted healthcare. Our research frames uncertainty not as a barrier but as an essential part of knowledge that invites a dynamic and reflective approach to AI design. By integrating advanced probabilistic methods such as Bayesian inference, deep ensembles, and Monte Carlo dropout with linguistic analysis that computes predictive and semantic entropy, we propose a comprehensive framework that manages both epistemic and aleatoric uncertainties. The framework incorporates surrogate modeling to address limitations of proprietary APIs, multi-source data integration for better context, and dynamic calibration via continual and meta-learning. Explainability is embedded through uncertainty maps and confidence metrics to support user trust and clinical interpretability. Our approach supports transparent and ethical decision-making aligned with Responsible and Reflective AI principles. Philosophically, we advocate accepting controlled ambiguity instead of striving for absolute predictability, recognizing the inherent provisionality of medical knowledge.', 'abstract_zh': '本研究探讨了在医疗应用中大型语言模型（LLMs）的不确定性量化，强调了技术和哲学层面的双重意义。随着LLMs在临床决策中变得不可或缺，准确传达不确定性对于确保可靠、安全和伦理的人工智能辅助医疗至关重要。我们的研究将不确定性视为知识不可或缺的一部分，旨在通过动态和反思性的方法来促进AI设计。通过结合先进的概率方法（如贝叶斯推断、深度集成和蒙特卡洛丢弃）与计算预测和语义熵的语言分析，我们提出了一种全面的框架，以管理可知论和偶然论不确定性。该框架整合了替代建模以应对专有API的局限性，以及通过多元数据融合和持续元学习的动态校准。通过不确定性图和置信度指标嵌入可解释性，以支持用户的信任和临床解释性。我们的方法支持与负责任和反思性AI原则相一致的透明和伦理决策。从哲学上讲，我们主张接受可控的模糊性而非追求绝对的可预测性，认识到医学知识的内在临时性。', 'title_zh': '大型语言模型在医学中的不确定性量化挑战'}
{'arxiv_id': 'arXiv:2504.05259', 'title': 'How to evaluate control measures for LLM agents? A trajectory from today to superintelligence', 'authors': 'Tomek Korbak, Mikita Balesni, Buck Shlegeris, Geoffrey Irving', 'link': 'https://arxiv.org/abs/2504.05259', 'abstract': "As LLM agents grow more capable of causing harm autonomously, AI developers will rely on increasingly sophisticated control measures to prevent possibly misaligned agents from causing harm. AI developers could demonstrate that their control measures are sufficient by running control evaluations: testing exercises in which a red team produces agents that try to subvert control measures. To ensure control evaluations accurately capture misalignment risks, the affordances granted to this red team should be adapted to the capability profiles of the agents to be deployed under control measures.\nIn this paper we propose a systematic framework for adapting affordances of red teams to advancing AI capabilities. Rather than assuming that agents will always execute the best attack strategies known to humans, we demonstrate how knowledge of an agents's actual capability profile can inform proportional control evaluations, resulting in more practical and cost-effective control measures. We illustrate our framework by considering a sequence of five fictional models (M1-M5) with progressively advanced capabilities, defining five distinct AI control levels (ACLs). For each ACL, we provide example rules for control evaluation, control measures, and safety cases that could be appropriate. Finally, we show why constructing a compelling AI control safety case for superintelligent LLM agents will require research breakthroughs, highlighting that we might eventually need alternative approaches to mitigating misalignment risk.", 'abstract_zh': '随着大语言模型代理自主造成损害的能力不断增强，AI开发者将依赖日益复杂的控制措施来防止可能未对齐的代理造成损害。AI开发者可以通过运行控制评估来证明其控制措施的充分性：在红队生成试图规避控制措施的代理的测试演习中进行测试。为了确保控制评估能够准确捕捉到不对齐风险，授予红队的便利性应根据部署在控制措施下的代理的能力特征进行调整。\n\n本文提出了一种系统框架，用于根据不断进步的AI能力调整红队的便利性。我们不假设代理总能执行人类已知的最佳攻击策略，而是展示了代理的实际能力特征如何指导恰当的控制评估，从而导致更实用和成本效益更高的控制措施。我们通过考虑五个逐步进化的虚构模型（M1-M5）和定义五个不同的AI控制级别（ACLs）来阐述我们的框架。对于每个控制级别，我们提供示例规则、控制措施和安全案例。最后，我们说明了为什么为超级智能的大语言模型构建有说服力的AI控制安全案例将需要研究突破，强调我们最终可能需要新的方法来减轻不对齐风险。', 'title_zh': '如何评估大型语言模型代理的控制措施？从今天到超智能的路径'}
{'arxiv_id': 'arXiv:2504.05163', 'title': 'Evaluating Knowledge Graph Based Retrieval Augmented Generation Methods under Knowledge Incompleteness', 'authors': 'Dongzhuoran Zhou, Yuqicheng Zhu, Yuan He, Jiaoyan Chen, Evgeny Kharlamov, Steffen Staab', 'link': 'https://arxiv.org/abs/2504.05163', 'abstract': 'Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) is a technique that enhances Large Language Model (LLM) inference in tasks like Question Answering (QA) by retrieving relevant information from knowledge graphs (KGs). However, real-world KGs are often incomplete, meaning that essential information for answering questions may be missing. Existing benchmarks do not adequately capture the impact of KG incompleteness on KG-RAG performance. In this paper, we systematically evaluate KG-RAG methods under incomplete KGs by removing triples using different methods and analyzing the resulting effects. We demonstrate that KG-RAG methods are sensitive to KG incompleteness, highlighting the need for more robust approaches in realistic settings.', 'abstract_zh': '基于知识图谱的检索增强生成（KG-RAG）：在知识图谱不完备的情况下评估方法对其性能的影响', 'title_zh': '基于知识不完备性的知识图谱检索增强生成方法评价'}
{'arxiv_id': 'arXiv:2504.05108', 'title': 'Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning', 'authors': 'Anja Surina, Amin Mansouri, Lars Quaedvlieg, Amal Seddas, Maryna Viazovska, Emmanuel Abbe, Caglar Gulcehre', 'link': 'https://arxiv.org/abs/2504.05108', 'abstract': 'Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating the discovery of algorithms across various domains, particularly in mathematics and optimization. However, existing approaches treat the LLM as a static generator, missing the opportunity to update the model with the signal obtained from evolutionary exploration. In this work, we propose to augment LLM-based evolutionary search by continuously refining the search operator - the LLM - through reinforcement learning (RL) fine-tuning. Our method leverages evolutionary search as an exploration strategy to discover improved algorithms, while RL optimizes the LLM policy based on these discoveries. Our experiments on three combinatorial optimization tasks - bin packing, traveling salesman, and the flatpack problem - show that combining RL and evolutionary search improves discovery efficiency of improved algorithms, showcasing the potential of RL-enhanced evolutionary strategies to assist computer scientists and mathematicians for more efficient algorithm design.', 'abstract_zh': '利用强化学习增强进化搜索以发现高效算法', 'title_zh': '基于LLMs的算法发现：演化搜索结合强化学习'}
{'arxiv_id': 'arXiv:2504.05047', 'title': 'Debate Only When Necessary: Adaptive Multiagent Collaboration for Efficient LLM Reasoning', 'authors': 'Sugyeong Eo, Hyeonseok Moon, Evelyn Hayoon Zi, Chanjun Park, Heuiseok Lim', 'link': 'https://arxiv.org/abs/2504.05047', 'abstract': "Multiagent collaboration has emerged as a promising framework for enhancing the reasoning capabilities of large language models (LLMs). While this approach improves reasoning capability, it incurs substantial computational overhead due to iterative agent interactions. Furthermore, engaging in debates for queries that do not necessitate collaboration amplifies the risk of error generation. To address these challenges, we propose Debate Only When Necessary (DOWN), an adaptive multiagent debate framework that selectively activates the debate process based on the confidence score of the agent's initial response. For queries where debate is triggered, agents refine their outputs using responses from participating agents and their confidence scores. Experimental results demonstrate that this mechanism significantly improves efficiency while maintaining or even surpassing the performance of existing multiagent debate systems. We also find that confidence-guided debate mitigates error propagation and enhances the selective incorporation of reliable responses. These results establish DOWN as an optimization strategy for efficient and effective multiagent reasoning, facilitating the practical deployment of LLM-based collaboration.", 'abstract_zh': '必要的时候才辩论：一种基于置信度的多智能体辩论框架', 'title_zh': '必要时才辩论：适应性多智能体合作以提高大型语言模型推理效率'}
{'arxiv_id': 'arXiv:2504.04942', 'title': 'Lemmanaid: Neuro-Symbolic Lemma Conjecturing', 'authors': 'Yousef Alhessi, Sólrún Halla Einarsdóttir, George Granberry, Emily First, Moa Johansson, Sorin Lerner, Nicholas Smallbone', 'link': 'https://arxiv.org/abs/2504.04942', 'abstract': 'Automatically conjecturing useful, interesting and novel lemmas would greatly improve automated reasoning tools and lower the bar for formalizing mathematics in proof assistants. It is however a very challenging task for both neural and symbolic approaches. We present the first steps towards a practical neuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language Models (LLMs) and symbolic methods, and evaluate it on proof libraries for the Isabelle proof assistant. We train an LLM to generate lemma templates that describe the shape of a lemma, and use symbolic methods to fill in the details. We compare Lemmanaid against an LLM trained to generate complete lemma statements as well as previous fully symbolic conjecturing methods. Our results indicate that neural and symbolic techniques are complementary. By leveraging the best of both symbolic and neural methods we can generate useful lemmas for a wide range of input domains, facilitating computer-assisted theory development and formalization.', 'abstract_zh': '自动猜想有用的、有趣的和新颖的引理将极大地提高自动推理工具的性能，并降低在证明辅助工具中形式化数学的门槛。然而，这既是神经方法又是符号方法面临的一项非常具有挑战性的任务。我们提出了第一个实用的神经-符号引理猜想工具Lemmanaid，该工具结合了大型语言模型和符号方法，并在Isabelle证明辅助工具的证明库上对其进行评估。我们训练一个大型语言模型生成描述引理形状的模板，并使用符号方法填充细节。我们将Lemmanaid与一个生成完整引理陈述的大型语言模型以及之前的完全符号猜想方法进行比较。我们的结果表明，神经技术和符号技术是互补的。通过利用两者之长，我们可以为广泛的输入领域生成有用的引理，从而促进计算机辅助的理论发展和形式化。', 'title_zh': 'Lemmanaid: 神经符号命题猜想助手'}
{'arxiv_id': 'arXiv:2504.04918', 'title': 'Constitution or Collapse? Exploring Constitutional AI with Llama 3-8B', 'authors': 'Xue Zhang', 'link': 'https://arxiv.org/abs/2504.04918', 'abstract': 'As language models continue to grow larger, the cost of acquiring high-quality training data has increased significantly. Collecting human feedback is both expensive and time-consuming, and manual labels can be noisy, leading to an imbalance between helpfulness and harmfulness. Constitutional AI, introduced by Anthropic in December 2022, uses AI to provide feedback to another AI, greatly reducing the need for human labeling. However, the original implementation was designed for a model with around 52 billion parameters, and there is limited information on how well Constitutional AI performs with smaller models, such as LLaMA 3-8B. In this paper, we replicated the Constitutional AI workflow using the smaller LLaMA 3-8B model. Our results show that Constitutional AI can effectively increase the harmlessness of the model, reducing the Attack Success Rate in MT-Bench by 40.8%. However, similar to the original study, increasing harmlessness comes at the cost of helpfulness. The helpfulness metrics, which are an average of the Turn 1 and Turn 2 scores, dropped by 9.8% compared to the baseline. Additionally, we observed clear signs of model collapse in the final DPO-CAI model, indicating that smaller models may struggle with self-improvement due to insufficient output quality, making effective fine-tuning more challenging. Our study suggests that, like reasoning and math ability, self-improvement is an emergent property.', 'abstract_zh': '随着语言模型不断增大，高质量训练数据的获取成本显著增加。收集人类反馈既昂贵又耗时，人工标注还可能存在噪音，导致帮助性和有害性之间的不平衡。Anthropic于2022年12月提出的宪法AI使用AI提供反馈给另一个AI，大大减少了对人类标注的需求。然而，最初的实现是为一个大约有520亿参数的模型设计的，关于宪法AI在较小模型，如LLaMA 3-8B上表现的信息有限。在本文中，我们使用较小的LLaMA 3-8B模型复现了宪法AI的工作流程。结果显示，宪法AI有效提高了模型的无害性，使MT-Bench中的攻击成功率降低了40.8%。然而，与原始研究相似，增加无害性以降低有害性会牺牲帮助性。帮助性指标，即Turn 1和Turn 2得分的平均值，相比于基线下降了9.8%。此外，我们还在最终的DPO-CAI模型中观察到了明显的模型崩溃迹象，表明较小的模型可能因为输出质量不足而在自我改进方面面临挑战，使得有效的微调更加困难。我们的研究表明，与推理和数学能力一样，自我改进是一种 emergent 属性。', 'title_zh': '宪法还是崩溃？探索基于Llama 3-8B的宪法AI'}
{'arxiv_id': 'arXiv:2504.04855', 'title': 'BIASINSPECTOR: Detecting Bias in Structured Data through LLM Agents', 'authors': 'Haoxuan Li, Mingyu Derek Ma, Jen-tse Huang, Zhaotian Weng, Wei Wang, Jieyu Zhao', 'link': 'https://arxiv.org/abs/2504.04855', 'abstract': 'Detecting biases in structured data is a complex and time-consuming task. Existing automated techniques are limited in diversity of data types and heavily reliant on human case-by-case handling, resulting in a lack of generalizability. Currently, large language model (LLM)-based agents have made significant progress in data science, but their ability to detect data biases is still insufficiently explored. To address this gap, we introduce the first end-to-end, multi-agent synergy framework, BIASINSPECTOR, designed for automatic bias detection in structured data based on specific user requirements. It first develops a multi-stage plan to analyze user-specified bias detection tasks and then implements it with a diverse and well-suited set of tools. It delivers detailed results that include explanations and visualizations. To address the lack of a standardized framework for evaluating the capability of LLM agents to detect biases in data, we further propose a comprehensive benchmark that includes multiple evaluation metrics and a large set of test cases. Extensive experiments demonstrate that our framework achieves exceptional overall performance in structured data bias detection, setting a new milestone for fairer data applications.', 'abstract_zh': '检测结构化数据中的偏差是一项复杂且耗时的任务。现有的自动化技术在数据类型多样性方面有限，并且高度依赖于人工逐案处理，导致缺乏一般化能力。目前，基于大型语言模型（LLM）的代理在数据科学领域取得了显著进展，但它们检测数据偏差的能力尚未得到充分探索。为填补这一空白，我们提出了第一个端到端的多代理协同框架BIASINSPECTOR，该框架旨在根据特定用户需求自动检测结构化数据中的偏差。它首先开发一个多阶段计划以分析用户指定的偏差检测任务，然后使用多样化且合适的工具集来实现这一计划。它提供详细的检测结果，包括解释和可视化。为解决评估LLM代理检测数据偏差能力缺乏标准化框架的问题，我们进一步提出了一种全面的基准测试，其中包括多种评估指标和大量测试案例。广泛实验显示，我们的框架在结构化数据偏差检测方面的整体性能出色，为更公平的数据应用设立了新的里程碑。', 'title_zh': 'BIASINSPECTOR: 通过LLM代理检测结构化数据中的偏见'}
{'arxiv_id': 'arXiv:2504.04785', 'title': 'Weak-for-Strong: Training Weak Meta-Agent to Harness Strong Executors', 'authors': 'Fan Nie, Lan Feng, Haotian Ye, Weixin Liang, Pan Lu, Huaxiu Yao, Alexandre Alahi, James Zou', 'link': 'https://arxiv.org/abs/2504.04785', 'abstract': 'Efficiently leveraging of the capabilities of contemporary large language models (LLMs) is increasingly challenging, particularly when direct fine-tuning is expensive and often impractical. Existing training-free methods, including manually or automated designed workflows, typically demand substantial human effort or yield suboptimal results. This paper proposes Weak-for-Strong Harnessing (W4S), a novel framework that customizes smaller, cost-efficient language models to design and optimize workflows for harnessing stronger models. W4S formulates workflow design as a multi-turn markov decision process and introduces reinforcement learning for agentic workflow optimization (RLAO) to train a weak meta-agent. Through iterative interaction with the environment, the meta-agent learns to design increasingly effective workflows without manual intervention. Empirical results demonstrate the superiority of W4S that our 7B meta-agent, trained with just one GPU hour, outperforms the strongest baseline by 2.9% ~ 24.6% across eleven benchmarks, successfully elevating the performance of state-of-the-art models such as GPT-3.5-Turbo and GPT-4o. Notably, W4S exhibits strong generalization capabilities across both seen and unseen tasks, offering an efficient, high-performing alternative to directly fine-tuning strong models.', 'abstract_zh': '高效利用当代大规模语言模型的能力：一种新型Weak-for-Strong Harnessing框架', 'title_zh': '弱对强：训练弱元代理协调强执行器'}
{'arxiv_id': 'arXiv:2504.04711', 'title': 'Generalising from Self-Produced Data: Model Training Beyond Human Constraints', 'authors': 'Alfath Daryl Alhajir, Jennifer Dodgson, Joseph Lim, Truong Ma Phi, Julian Peh, Akira Rafhael Janson Pattirane, Lokesh Poovaragan', 'link': 'https://arxiv.org/abs/2504.04711', 'abstract': 'Current large language models (LLMs) are constrained by human-derived training data and limited by a single level of abstraction that impedes definitive truth judgments. This paper introduces a novel framework in which AI models autonomously generate and validate new knowledge through direct interaction with their environment. Central to this approach is an unbounded, ungamable numeric reward - such as annexed disk space or follower count - that guides learning without requiring human benchmarks. AI agents iteratively generate strategies and executable code to maximize this metric, with successful outcomes forming the basis for self-retraining and incremental generalisation. To mitigate model collapse and the warm start problem, the framework emphasizes empirical validation over textual similarity and supports fine-tuning via GRPO. The system architecture employs modular agents for environment analysis, strategy generation, and code synthesis, enabling scalable experimentation. This work outlines a pathway toward self-improving AI systems capable of advancing beyond human-imposed constraints toward autonomous general intelligence.', 'abstract_zh': '当前的大语言模型受限于人类提供的训练数据，并且受限于单一抽象层次，这妨碍了它们做出终极真相判断。本文提出了一种新型框架，通过AI模型自主与环境直接交互生成和验证新知识。该方法的核心是一种无边界且不可作弊的数值奖励，如附加磁盘空间或追随者数量，这种奖励引导学习而不必依赖人类基准。AI代理通过迭代生成策略和可执行代码以最大化该指标，成功的成果成为自我重新训练和逐步泛化的基础。为了缓解模型崩溃和暖启动问题，该框架强调经验验证而非文本相似性，并通过GRPO支持微调。该系统架构采用模块化的代理进行环境分析、策略生成和代码合成，以实现可扩展的实验。本文概述了一条通往自我改进的AI系统的发展路径，这些系统能够超越人类施加的限制，朝着自主通用人工智能前进。', 'title_zh': '基于自我生成数据的泛化：超越人类约束的模型训练'}
{'arxiv_id': 'arXiv:2504.04600', 'title': "Capturing AI's Attention: Physics of Repetition, Hallucination, Bias and Beyond", 'authors': 'Frank Yingjie Huo, Neil F. Johnson', 'link': 'https://arxiv.org/abs/2504.04600', 'abstract': "We derive a first-principles physics theory of the AI engine at the heart of LLMs' 'magic' (e.g. ChatGPT, Claude): the basic Attention head. The theory allows a quantitative analysis of outstanding AI challenges such as output repetition, hallucination and harmful content, and bias (e.g. from training and fine-tuning). Its predictions are consistent with large-scale LLM outputs. Its 2-body form suggests why LLMs work so well, but hints that a generalized 3-body Attention would make such AI work even better. Its similarity to a spin-bath means that existing Physics expertise could immediately be harnessed to help Society ensure AI is trustworthy and resilient to manipulation.", 'abstract_zh': '我们推导出LLMs“魔力”（如ChatGPT、Claude）核心AI引擎的基本注意头的第一性物理理论。该理论允许对输出重复、幻觉、有害内容和偏见（例如，来自训练和微调）等 Outstanding AI 挑战进行定量分析。其预测与大规模LLM输出一致。其二体形式说明了LLMs为何如此有效，但暗示了通用的三体注意机制将使AI工作表现更好。其与自旋浴的相似性意味着现有的物理专业知识可以立即被利用以帮助社会确保AI的可信度并使其更能抵抗操纵。', 'title_zh': '捕捉AI的注意：重复、幻觉、偏见以及其他物理学原理'}
{'arxiv_id': 'arXiv:2504.04596', 'title': 'SECQUE: A Benchmark for Evaluating Real-World Financial Analysis Capabilities', 'authors': 'Noga Ben Yoash, Meni Brief, Oded Ovadia, Gil Shenderovitz, Moshik Mishaeli, Rachel Lemberg, Eitam Sheetrit', 'link': 'https://arxiv.org/abs/2504.04596', 'abstract': "We introduce SECQUE, a comprehensive benchmark for evaluating large language models (LLMs) in financial analysis tasks. SECQUE comprises 565 expert-written questions covering SEC filings analysis across four key categories: comparison analysis, ratio calculation, risk assessment, and financial insight generation. To assess model performance, we develop SECQUE-Judge, an evaluation mechanism leveraging multiple LLM-based judges, which demonstrates strong alignment with human evaluations. Additionally, we provide an extensive analysis of various models' performance on our benchmark. By making SECQUE publicly available, we aim to facilitate further research and advancements in financial AI.", 'abstract_zh': '我们介绍SECQUE，一个全面的基准，用于评估大型语言模型在金融分析任务中的表现。SECQUE包括565个由专家撰写的题目，涵盖了SEC报表分析的四个关键类别：比较分析、比率计算、风险评估和财务洞察生成。为了评估模型性能，我们开发了SECQUE-Judge，一种利用多个基于语言模型的评估机制，显示出与人工评估的高度一致性。此外，我们还对各种模型在基准上的表现进行了详细的分析。通过公开提供SECQUE，我们旨在促进金融人工智能领域进一步的研究和进步。', 'title_zh': 'SECQUE: 一个评估实际金融分析能力的基准'}
{'arxiv_id': 'arXiv:2504.04383', 'title': 'Retro-Search: Exploring Untaken Paths for Deeper and Efficient Reasoning', 'authors': 'Ximing Lu, Seungju Han, David Acuna, Hyunwoo Kim, Jaehun Jung, Shrimai Prabhumoye, Niklas Muennighoff, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Yejin Choi', 'link': 'https://arxiv.org/abs/2504.04383', 'abstract': "Large reasoning models exhibit remarkable reasoning capabilities via long, elaborate reasoning trajectories. Supervised fine-tuning on such reasoning traces, also known as distillation, can be a cost-effective way to boost reasoning capabilities of student models. However, empirical observations reveal that these reasoning trajectories are often suboptimal, switching excessively between different lines of thought, resulting in under-thinking, over-thinking, and even degenerate responses. We introduce Retro-Search, an MCTS-inspired search algorithm, for distilling higher quality reasoning paths from large reasoning models. Retro-Search retrospectively revises reasoning paths to discover better, yet shorter traces, which can then lead to student models with enhanced reasoning capabilities with shorter, thus faster inference. Our approach can enable two use cases: self-improvement, where models are fine-tuned on their own Retro-Search-ed thought traces, and weak-to-strong improvement, where a weaker model revises stronger model's thought traces via Retro-Search. For self-improving, R1-distill-7B, fine-tuned on its own Retro-Search-ed traces, reduces the average reasoning length by 31.2% while improving performance by 7.7% across seven math benchmarks. For weak-to-strong improvement, we retrospectively revise R1-671B's traces from the OpenThoughts dataset using R1-distill-32B as the Retro-Search-er, a model 20x smaller. Qwen2.5-32B, fine-tuned on this refined data, achieves performance comparable to R1-distill-32B, yielding an 11.3% reduction in reasoning length and a 2.4% performance improvement compared to fine-tuning on the original OpenThoughts data. Our work counters recently emergent viewpoints that question the relevance of search algorithms in the era of large reasoning models, by demonstrating that there are still opportunities for algorithmic advancements, even for frontier models.", 'abstract_zh': '大型推理模型通过长而复杂的推理轨迹展示了 remarkable 的推理能力。通过对这些推理轨迹进行监督微调，即蒸馏，可以成本效益地提升学生模型的推理能力。然而，实验观察表明，这些推理轨迹往往是 suboptimal 的，频繁地在不同思路之间切换，导致 under-thinking、over-thinking 和甚至退化的响应。我们引入了 Retro-Search，一种灵感来源于 MCTS 的搜索算法，用于从大型推理模型中蒸馏出更高质量的推理路径。Retro-Search 会回顾性地修订推理路径，发现更好的但更短的轨迹，从而引导出具有更强推理能力的学生模型，且推理更短，因此更快。我们的方法可以启用两种用例：自我改进，模型在其自身的 Retro-Search 修订的思考轨迹上进行微调，以及弱到强改进，较弱的模型通过 Retro-Search 修订较强模型的思考轨迹。对于自我改进，R1-distill-7B 在其自身的 Retro-Search 修订的轨迹上微调后，平均推理长度减少了 31.2%，七种数学基准的性能提高了 7.7%。对于弱到强改进，我们使用 R1-distill-32B 作为 Retro-Search 的工具，针对 OpenThoughts 数据集的轨迹进行回顾性修订，Qwen2.5-32B 在此精炼的数据上微调后，性能与 R1-distill-32B 相当，推理长度减少了 11.3%，性能提高了 2.4%，优于在原始 OpenThoughts 数据上微调的模型。我们的研究反驳了最近出现的观点，即在大型推理模型时代，搜索算法的相关性存疑，证明即使对于前沿模型，仍有机会进行算法上的改进。', 'title_zh': '逆向搜索：探索未走之路以实现更深更高效的推理'}
{'arxiv_id': 'arXiv:2504.04346', 'title': 'Crowdsourcing-Based Knowledge Graph Construction for Drug Side Effects Using Large Language Models with an Application on Semaglutide', 'authors': 'Zhijie Duan, Kai Wei, Zhaoqian Xue, Lingyao li, Jin Jin, Shu Yang, Jiayan Zhou, Siyuan Ma', 'link': 'https://arxiv.org/abs/2504.04346', 'abstract': "Social media is a rich source of real-world data that captures valuable patient experience information for pharmacovigilance. However, mining data from unstructured and noisy social media content remains a challenging task. We present a systematic framework that leverages large language models (LLMs) to extract medication side effects from social media and organize them into a knowledge graph (KG). We apply this framework to semaglutide for weight loss using data from Reddit. Using the constructed knowledge graph, we perform comprehensive analyses to investigate reported side effects across different semaglutide brands over time. These findings are further validated through comparison with adverse events reported in the FAERS database, providing important patient-centered insights into semaglutide's side effects that complement its safety profile and current knowledge base of semaglutide for both healthcare professionals and patients. Our work demonstrates the feasibility of using LLMs to transform social media data into structured KGs for pharmacovigilance.", 'abstract_zh': '社交媒体是获取药物警戒中宝贵患者体验信息的丰富数据来源，但从中挖掘半结构化和噪声数据仍然是一个挑战性任务。我们提出了一种综合利用大型语言模型（LLMs）从社交媒体中抽取药物副作用并组织成知识图谱（KG）的系统框架。我们运用该框架基于Reddit数据对 semaglutide 的减肥副作用进行分析，并构建知识图谱以进行全面分析，探讨不同 semaglutide 品牌随时间变化的副作用报告情况。这些发现通过与 FAERS 数据库中的不良事件进行对比验证，为医疗保健专业人员和患者提供了 semaglutide 副作用的重要患者中心见解，补充其安全性概况和现有 semaglutide 知识库。我们的工作展示了利用 LLMs 将社交媒体数据转换为结构化 KGs 以进行药物警戒的可行性。', 'title_zh': '基于 crowdsourcing 的大规模语言模型驱动的药物副作用知识图构建及其在赛吗GLU上的应用'}
{'arxiv_id': 'arXiv:2504.04110', 'title': 'PEIRCE: Unifying Material and Formal Reasoning via LLM-Driven Neuro-Symbolic Refinement', 'authors': 'Xin Quan, Marco Valentino, Danilo S. Carvalho, Dhairya Dalal, André Freitas', 'link': 'https://arxiv.org/abs/2504.04110', 'abstract': "A persistent challenge in AI is the effective integration of material and formal inference - the former concerning the plausibility and contextual relevance of arguments, while the latter focusing on their logical and structural validity. Large Language Models (LLMs), by virtue of their extensive pre-training on large textual corpora, exhibit strong capabilities in material inference. However, their reasoning often lacks formal rigour and verifiability. At the same time, LLMs' linguistic competence positions them as a promising bridge between natural and formal languages, opening up new opportunities for combining these two modes of reasoning. In this paper, we introduce PEIRCE, a neuro-symbolic framework designed to unify material and formal inference through an iterative conjecture-criticism process. Within this framework, LLMs play the central role of generating candidate solutions in natural and formal languages, which are then evaluated and refined via interaction with external critique models. These critiques include symbolic provers, which assess formal validity, as well as soft evaluators that measure the quality of the generated arguments along linguistic and epistemic dimensions such as plausibility, coherence, and parsimony. While PEIRCE is a general-purpose framework, we demonstrate its capabilities in the domain of natural language explanation generation - a setting that inherently demands both material adequacy and formal correctness.", 'abstract_zh': 'AI中物质推理和形式推理的有效集成是一个持久的挑战——前者关注论据的合理性与上下文相关性，后者则聚焦于其逻辑和结构的有效性。大型语言模型（LLMs）由于大规模预训练在大量的文本语料上，表现出强大的物质推理能力。然而，它们的推理往往缺乏形式的严谨性和可验证性。同时，LLMs在语言上的能力使它们成为自然语言和形式语言之间的一个有前途的桥梁，为结合这两种推理模式开辟了新的机会。本文引入了PEIRCE，一种基于神经-符号框架，通过迭代假说-批判过程来统一物质推理和形式推理的设计。在这个框架中，LLMs在自然语言和形式语言中起着核心作用，生成候选解决方案，然后通过与外部批判模型的交互进行评估和修正。这些批判包括形式证明器评估形式有效性，以及软评估器衡量生成论证在可置信度、一致性、简约性等语言和认识论维度上的质量。虽然PEIRCE是一个通用框架，但本文展示了它在其自然语言解释生成领域的应用能力——一个固有地需要同时具备物质适当性和形式正确性的环境。', 'title_zh': 'PEIRCE：通过LLM驱动的神经符号细化统一物质推理与形式推理'}
{'arxiv_id': 'arXiv:2504.03930', 'title': 'Have Large Language Models Learned to Reason? A Characterization via 3-SAT Phase Transition', 'authors': 'Rishi Hazra, Gabriele Venturato, Pedro Zuidberg Dos Martires, Luc De Raedt', 'link': 'https://arxiv.org/abs/2504.03930', 'abstract': 'Large Language Models (LLMs) have been touted as AI models possessing advanced reasoning abilities. In theory, autoregressive LLMs with Chain-of-Thought (CoT) can perform more serial computations to solve complex reasoning tasks. However, recent studies suggest that, despite this capacity, LLMs do not truly learn to reason but instead fit on statistical features. To study the reasoning capabilities in a principled fashion, we adopt a computational theory perspective and propose an experimental protocol centered on 3-SAT -- the prototypical NP-complete problem lying at the core of logical reasoning and constraint satisfaction tasks. Specifically, we examine the phase transitions in random 3-SAT and characterize the reasoning abilities of state-of-the-art LLMs by varying the inherent hardness of the problem instances. By comparing DeepSeek R1 with other LLMs, our findings reveal two key insights (1) LLM accuracy drops significantly on harder instances, suggesting all current models struggle when statistical shortcuts are unavailable (2) Unlike other LLMs, R1 shows signs of having learned the underlying reasoning. Following a principled experimental protocol, our study moves beyond the benchmark-driven evidence often found in LLM reasoning research. Our findings highlight important gaps and suggest clear directions for future research.', 'abstract_zh': '大型语言模型（LLMs）被认为是具有高级推理能力的AI模型。理论上，具有链式思考（CoT）的自回归LLMs能够进行更多的串行计算以解决复杂的推理任务。然而，近期研究表明，尽管具备这种能力，LLMs实际上并未真正学习推理，而是依赖于统计特征拟合。为客观研究推理能力，我们采用计算理论视角，提出以3-SAT为核心问题的实验协议，研究最先进LLMs的推理能力。我们通过改变问题实例的固有难度，考察随机3-SAT的相变，并揭示了两个关键发现：(1) LLMs在更难的问题实例上的准确性显著下降，表明在缺乏统计捷径时所有现有模型都难以应对；(2) 与其它LLMs不同，R1显示出学习到潜在推理结构的迹象。遵循严格实验协议，我们的研究超越了当前LLM推理研究中常见的基准驱动证据，揭示了重要的差距并为未来研究指明了方向。', 'title_zh': '大型语言模型学会推理了吗？基于3-SAT相变的刻画'}
{'arxiv_id': 'arXiv:2504.05258', 'title': 'Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models', 'authors': 'Adrián Bazaga, Rexhina Blloshmi, Bill Byrne, Adrià de Gispert', 'link': 'https://arxiv.org/abs/2504.05258', 'abstract': 'Large Language Models (LLMs) have emerged as powerful tools for generating coherent text, understanding context, and performing reasoning tasks. However, they struggle with temporal reasoning, which requires processing time-related information such as event sequencing, durations, and inter-temporal relationships. These capabilities are critical for applications including question answering, scheduling, and historical analysis. In this paper, we introduce TISER, a novel framework that enhances the temporal reasoning abilities of LLMs through a multi-stage process that combines timeline construction with iterative self-reflection. Our approach leverages test-time scaling to extend the length of reasoning traces, enabling models to capture complex temporal dependencies more effectively. This strategy not only boosts reasoning accuracy but also improves the traceability of the inference process. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, including out-of-distribution test sets, and reveal that TISER enables smaller open-source models to surpass larger closed-weight models on challenging temporal reasoning tasks.', 'abstract_zh': '大型语言模型（LLMs）已经成为生成连贯文本、理解上下文和执行推理任务的强大工具。然而，它们在处理时间推理方面存在困难，这需要处理与事件顺序、持续时间和跨时间关系相关的时间信息。这些能力对于应用领域包括问答、排程和历史分析至关重要。在本文中，我们提出了一种名为TISER的新框架，通过结合时间线构建和迭代自我反思的多阶段过程来增强LLMs的时间推理能力。我们的方法利用测试时扩展缩放来延长推理痕迹的长度，从而使模型更有效地捕捉复杂的时序依赖性。这种策略不仅提高了推理准确性，还提高了推理过程的可追溯性。实验结果表明，TISER在多个基准测试中展现出最先进的性能，包括通用测试集，并揭示了TISER能让较小的开源模型在具有挑战性的时间推理任务中超越较大的封闭权重模型。', 'title_zh': '学习随时间进行推理：时间线自我反思以提高语言模型的时间推理能力'}
{'arxiv_id': 'arXiv:2504.05220', 'title': 'Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort for Retrieval and RAG', 'authors': 'Hengran Zhang, Minghao Tang, Keping Bi, Jiafeng Guo, Shihao Liu, Daiting Shi, Dawei Yin, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2504.05220', 'abstract': 'Retrieval models typically rely on costly human-labeled query-document relevance annotations for training and evaluation. To reduce this cost and leverage the potential of Large Language Models (LLMs) in relevance judgments, we aim to explore whether LLM-generated annotations can effectively replace human annotations in training retrieval models. Retrieval usually emphasizes relevance, which indicates "topic-relatedness" of a document to a query, while in RAG, the value of a document (or utility) depends on how it contributes to answer generation. Recognizing this mismatch, some researchers use LLM performance on downstream tasks with documents as labels, but this approach requires manual answers for specific tasks, leading to high costs and limited generalization. In another line of work, prompting LLMs to select useful documents as RAG references eliminates the need for human annotation and is not task-specific. If we leverage LLMs\' utility judgments to annotate retrieval data, we may retain cross-task generalization without human annotation in large-scale corpora. Therefore, we investigate utility-focused annotation via LLMs for large-scale retriever training data across both in-domain and out-of-domain settings on the retrieval and RAG tasks. To reduce the impact of low-quality positives labeled by LLMs, we design a novel loss function, i.e., Disj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on utility-focused annotations significantly outperform those trained on human annotations in the out-of-domain setting on both tasks, demonstrating superior generalization capabilities. (2) LLM annotation does not replace human annotation in the in-domain setting. However, incorporating just 20% human-annotated data enables retrievers trained with utility-focused annotations to match the performance of models trained entirely with human annotations.', 'abstract_zh': '基于大规模语言模型的检索数据实用 Annotations: 探索利用大规模语言模型进行检索模型训练的有效性', 'title_zh': '利用大语言模型进行功效导向的标注：减少检索和 Retrieval-Augmented Generation（检索增强生成）中的手动努力'}
{'arxiv_id': 'arXiv:2504.05216', 'title': 'Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling', 'authors': 'Hengran Zhang, Keping Bi, Jiafeng Guo, Xiaojie Sun, Shihao Liu, Daiting Shi, Dawei Yin, Xueqi Cheng', 'link': 'https://arxiv.org/abs/2504.05216', 'abstract': "Dense retrieval is a crucial task in Information Retrieval (IR) and is the foundation for downstream tasks such as re-ranking. Recently, large language models (LLMs) have shown compelling semantic understanding capabilities and are appealing to researchers studying dense retrieval. LLMs, as decoder-style generative models, are competent at language generation while falling short on modeling global information due to the lack of attention to tokens afterward. Inspired by the classical word-based language modeling approach for IR, i.e., the query likelihood (QL) model, we seek to sufficiently utilize LLMs' generative ability by QL maximization. However, instead of ranking documents with QL estimation, we introduce an auxiliary task of QL maximization to yield a better backbone for contrastively learning a discriminative retriever. We name our model as LLM-QL. To condense global document semantics to a single vector during QL modeling, LLM-QL has two major components, Attention Stop (AS) and Input Corruption (IC). AS stops the attention of predictive tokens to previous tokens until the ending token of the document. IC masks a portion of tokens in the input documents during prediction. Experiments on MSMARCO show that LLM-QL can achieve significantly better performance than other LLM-based retrievers and using QL estimated by LLM-QL for ranking outperforms word-based QL by a large margin.", 'abstract_zh': '基于密集检索的大语言模型-查询似然性方法（LLM-QL）', 'title_zh': '利用查询似然模型释放大语言模型在密集检索中的潜力'}
{'arxiv_id': 'arXiv:2504.05050', 'title': 'Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models', 'authors': 'Jiawei Lian, Jianhong Pan, Lefan Wang, Yi Wang, Shaohui Mei, Lap-Pui Chau', 'link': 'https://arxiv.org/abs/2504.05050', 'abstract': 'Large language models (LLMs) are foundational explorations to artificial general intelligence, yet their alignment with human values via instruction tuning and preference learning achieves only superficial compliance. Here, we demonstrate that harmful knowledge embedded during pretraining persists as indelible "dark patterns" in LLMs\' parametric memory, evading alignment safeguards and resurfacing under adversarial inducement at distributional shifts. In this study, we first theoretically analyze the intrinsic ethical vulnerability of aligned LLMs by proving that current alignment methods yield only local "safety regions" in the knowledge manifold. In contrast, pretrained knowledge remains globally connected to harmful concepts via high-likelihood adversarial trajectories. Building on this theoretical insight, we empirically validate our findings by employing semantic coherence inducement under distributional shifts--a method that systematically bypasses alignment constraints through optimized adversarial prompts. This combined theoretical and empirical approach achieves a 100% attack success rate across 19 out of 23 state-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing their universal vulnerabilities.', 'abstract_zh': '大型语言模型的伦理脆弱性：对齐方法仅实现局部“安全区域”，预训练知识通过高概率对抗轨迹保持全局连接，导致在分布转移下受到 adversarial 诱导时重新 surfacing。', 'title_zh': '揭示对齐的大规模语言模型固有的伦理漏洞'}
{'arxiv_id': 'arXiv:2504.04994', 'title': 'Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs', 'authors': 'Ling Hu, Yuemei Xu, Xiaoyang Gu, Letao Han', 'link': 'https://arxiv.org/abs/2504.04994', 'abstract': 'Despite the impressive performance of large language models (LLMs), they can present unintended biases and harmful behaviors driven by encoded values, emphasizing the urgent need to understand the value mechanisms behind them. However, current research primarily evaluates these values through external responses with a focus on AI safety, lacking interpretability and failing to assess social values in real-world contexts. In this paper, we propose a novel framework called ValueExploration, which aims to explore the behavior-driven mechanisms of National Social Values within LLMs at the neuron level. As a case study, we focus on Chinese Social Values and first construct C-voice, a large-scale bilingual benchmark for identifying and evaluating Chinese Social Values in LLMs. By leveraging C-voice, we then identify and locate the neurons responsible for encoding these values according to activation difference. Finally, by deactivating these neurons, we analyze shifts in model behavior, uncovering the internal mechanism by which values influence LLM decision-making. Extensive experiments on four representative LLMs validate the efficacy of our framework. The benchmark and code will be available.', 'abstract_zh': '尽管大型语言模型（LLMs）表现出色，但它们可能会表现出由编码价值观驱动的意外偏见和有害行为，强调了理解其背后的价值观机制的迫切需求。目前的研究主要通过外部反应评估这些价值观，集中在AI安全上，缺乏可解释性，未能在现实世界情境中评估社会价值观。本文提出了一种名为ValueExploration的新框架，旨在从神经元层面探索LLMs中国家社会价值观的行为驱动机制。作为案例研究，我们专注于中国社会价值观，并首先构建了一个大规模双语基准C-voice，用于识别和评估LLMs中的中国社会价值观。通过利用C-voice，我们根据激活差异识别并定位负责编码这些价值观的神经元。最后，通过抑制这些神经元，我们分析了模型行为的转变，揭示了价值观影响LLM决策的内部机制。对四个代表性LLM进行的 extensive 实验验证了我们框架的有效性。基准数据集和代码将可供使用。', 'title_zh': '遵循价值的 whispers：探讨面向价值行为的大型语言模型背后的神经机制'}
{'arxiv_id': 'arXiv:2504.04968', 'title': 'The Dream Within Huang Long Cave: AI-Driven Interactive Narrative for Family Storytelling and Emotional Reflection', 'authors': 'Jiayang Huang, Lingjie Li, Kang Zhang, David Yip', 'link': 'https://arxiv.org/abs/2504.04968', 'abstract': "This paper introduces the art project The Dream Within Huang Long Cave, an AI-driven interactive and immersive narrative experience. The project offers new insights into AI technology, artistic practice, and psychoanalysis. Inspired by actual geographical landscapes and familial archetypes, the work combines psychoanalytic theory and computational technology, providing an artistic response to the concept of the non-existence of the Big Other. The narrative is driven by a combination of a large language model (LLM) and a realistic digital character, forming a virtual agent named YELL. Through dialogue and exploration within a cave automatic virtual environment (CAVE), the audience is invited to unravel the language puzzles presented by YELL and help him overcome his life challenges. YELL is a fictional embodiment of the Big Other, modeled after the artist's real father. Through a cross-temporal interaction with this digital father, the project seeks to deconstruct complex familial relationships. By demonstrating the non-existence of the Big Other, we aim to underscore the authenticity of interpersonal emotions, positioning art as a bridge for emotional connection and understanding within family dynamics.", 'abstract_zh': '基于黄龙洞的梦境：AI驱动的互动沉浸叙事艺术项目', 'title_zh': '黄龙洞中的梦境：基于AI驱动的互动叙事家庭叙事与情感反思'}
{'arxiv_id': 'arXiv:2504.04953', 'title': 'M-Prometheus: A Suite of Open Multilingual LLM Judges', 'authors': 'José Pombal, Dongkeun Yoon, Patrick Fernandes, Ian Wu, Seungone Kim, Ricardo Rei, Graham Neubig, André F. T. Martins', 'link': 'https://arxiv.org/abs/2504.04953', 'abstract': 'The use of language models for automatically evaluating long-form text (LLM-as-a-judge) is becoming increasingly common, yet most LLM judges are optimized exclusively for English, with strategies for enhancing their multilingual evaluation capabilities remaining largely unexplored in the current literature. This has created a disparity in the quality of automatic evaluation methods for non-English languages, ultimately hindering the development of models with better multilingual capabilities. To bridge this gap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from 3B to 14B parameters that can provide both direct assessment and pairwise comparison feedback on multilingual outputs. M-Prometheus models outperform state-of-the-art open LLM judges on multilingual reward benchmarks spanning more than 20 languages, as well as on literary machine translation (MT) evaluation covering 4 language pairs. Furthermore, M-Prometheus models can be leveraged at decoding time to significantly improve generated outputs across all 3 tested languages, showcasing their utility for the development of better multilingual models. Lastly, through extensive ablations, we identify the key factors for obtaining an effective multilingual judge, including backbone model selection and training on natively multilingual feedback data instead of translated data. We release our models, training dataset, and code.', 'abstract_zh': '使用大规模语言模型自动评估长文本（LLM-as-a-judge）在日益普遍，然而目前大多数LLM法官仅针对英文进行了优化，关于提升其多语言评估能力的策略在现有文献中研究较少。这导致非英文语言的自动评估方法质量参差不齐，最终阻碍了具有良好多语言能力模型的发展。为进一步弥合这一差距，我们引入了M-Prometheus，这是一种从3B到14B参数的开放权重LLM法官系列，能提供多语言输出的直接评估和成对比较反馈。M-Prometheus模型在覆盖超过20种语言的多语言奖励基准测试中表现优于最先进的开放LLM法官，并在涵盖4种语言对的文学机器翻译（MT）评估中表现出色。此外，M-Prometheus模型在解码时可以显著改善所有3种测试语言生成的输出，展示了其在开发更好多语言模型中的应用前景。最后，通过广泛的消融实验，我们确定了有效多语言法官的关键因素，包括基础模型的选择以及使用原生多语言反馈数据进行训练而非翻译数据。我们发布了自己的模型、训练数据集和代码。', 'title_zh': 'M-Prometheus：一套开源多语言大模型评测套件'}
{'arxiv_id': 'arXiv:2504.04945', 'title': "A Llama walks into the 'Bar': Efficient Supervised Fine-Tuning for Legal Reasoning in the Multi-state Bar Exam", 'authors': 'Rean Fernandes, André Biedenkapp, Frank Hutter, Noor Awad', 'link': 'https://arxiv.org/abs/2504.04945', 'abstract': "Legal reasoning tasks present unique challenges for large language models (LLMs) due to the complexity of domain-specific knowledge and reasoning processes. This paper investigates how effectively smaller language models (Llama 2 7B and Llama 3 8B) can be fine-tuned with a limited dataset of 1,514 Multi-state Bar Examination (MBE) questions to improve legal question answering accuracy. We evaluate these models on the 2022 MBE questions licensed from JD Advising, the same dataset used in the 'GPT-4 passes the Bar exam' study. Our methodology involves collecting approximately 200 questions per legal domain across 7 domains. We distill the dataset using Llama 3 (70B) to transform explanations into a structured IRAC (Issue, Rule, Application, Conclusion) format as a guided reasoning process to see if it results in better performance over the non-distilled dataset. We compare the non-fine-tuned models against their supervised fine-tuned (SFT) counterparts, trained for different sample sizes per domain, to study the effect on accuracy and prompt adherence. We also analyse option selection biases and their mitigation following SFT. In addition, we consolidate the performance across multiple variables: prompt type (few-shot vs zero-shot), answer ordering (chosen-option first vs generated-explanation first), response format (Numbered list vs Markdown vs JSON), and different decoding temperatures. Our findings show that domain-specific SFT helps some model configurations achieve close to human baseline performance, despite limited computational resources and a relatively small dataset. We release both the gathered SFT dataset and the family of Supervised Fine-tuned (SFT) adapters optimised for MBE performance. This establishes a practical lower bound on resources needed towards achieving effective legal question answering in smaller LLMs.", 'abstract_zh': '大型语言模型（LLMs）因领域特定知识和推理过程的复杂性而面临独特的法律推理任务挑战。本文研究较小的语言模型（Llama 2 7B和Llama 3 8B）如何通过使用1,514个多元州律师考试（MBE）问题的有限数据集进行微调，以提高法律问题回答的准确性。我们利用JD Advising提供的2022年MBE问题这一相同的数据集评估这些模型。我们的方法包括在七个法律领域中每个领域收集约200个问题。我们使用Llama 3（70B）精简数据集，将其解释转换为结构化的IRAC（Issue, Rule, Application, Conclusion）格式，以指导推理过程，观察其是否能比未精简的数据集产生更好的效果。我们对比了未经微调的模型与其监督微调（SFT）版本在不同领域样本大小下的表现，研究其对准确性和指令遵循性的影响。我们还分析了SFT后选项选择偏见及其缓解措施。此外，我们跨多个变量综合性能表现：提示类型（少量样本vs零样本），答案排序（选择选项优先vs生成解释优先），响应格式（编号列表vsMarkdownvsJSON），以及不同的解码温度。我们的发现表明，尽管计算资源有限且数据集较小，领域特定的SFT仍有助于某些模型配置接近人类基础水平的性能。我们发布了收集的SFT数据集和优化用于MBE性能的监督微调（SFT）适配器家族。这确立了实现较小LLMs有效法律问题回答所需资源的实用下限。', 'title_zh': '一只 llama 走进了酒吧：多州律师资格考试中的高效监督微调以进行法律推理'}
{'arxiv_id': 'arXiv:2504.04915', 'title': 'Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration', 'authors': 'Ran Xu, Wenqi Shi, Yuchen Zhuang, Yue Yu, Joyce C. Ho, Haoyu Wang, Carl Yang', 'link': 'https://arxiv.org/abs/2504.04915', 'abstract': "Retrieval-Augmented Generation (RAG) systems often struggle to handle multi-hop question-answering tasks accurately due to irrelevant context retrieval and limited complex reasoning capabilities. We introduce Collab-RAG, a collaborative training framework that leverages mutual enhancement between a white-box small language model (SLM) and a blackbox large language model (LLM) for RAG. Specifically, the SLM decomposes complex queries into simpler sub-questions, thus enhancing the accuracy of the retrieval and facilitating more effective reasoning by the black-box LLM. Concurrently, the black-box LLM provides feedback signals to improve the SLM's decomposition capability. We observe that Collab-RAG relies solely on supervision from an affordable black-box LLM without additional distillation from frontier LLMs, yet demonstrates strong generalization across multiple black-box LLMs. Experimental evaluations across five multi-hop QA datasets demonstrate that Collab-RAG substantially outperforms existing black-box-only and SLM fine-tuning baselines by 1.8%-14.2% on average. In particular, our fine-tuned 3B SLM surpasses a frozen 32B LLM in question decomposition, highlighting the efficiency of Collab-RAG in improving reasoning and retrieval for complex questions. The code of Collab-RAG is available on this https URL.", 'abstract_zh': 'Collab-RAG：一种基于白盒小语言模型和黑盒大语言模型协同训练的检索增强生成框架', 'title_zh': 'Collab-RAG：通过白盒和黑盒大模型协作提升检索增强生成复杂问题解答'}
{'arxiv_id': 'arXiv:2504.04740', 'title': 'Enhancing Compositional Reasoning in Vision-Language Models with Synthetic Preference Data', 'authors': 'Samarth Mishra, Kate Saenko, Venkatesh Saligrama', 'link': 'https://arxiv.org/abs/2504.04740', 'abstract': 'Compositionality, or correctly recognizing scenes as compositions of atomic visual concepts, remains difficult for multimodal large language models (MLLMs). Even state of the art MLLMs such as GPT-4o can make mistakes in distinguishing compositions like "dog chasing cat" vs "cat chasing dog". While on Winoground, a benchmark for measuring such reasoning, MLLMs have made significant progress, they are still far from a human\'s performance. We show that compositional reasoning in these models can be improved by elucidating such concepts via data, where a model is trained to prefer the correct caption for an image over a close but incorrect one. We introduce SCRAMBLe: Synthetic Compositional Reasoning Augmentation of MLLMs with Binary preference Learning, an approach for preference tuning open-weight MLLMs on synthetic preference data generated in a fully automated manner from existing image-caption data. SCRAMBLe holistically improves these MLLMs\' compositional reasoning capabilities which we can see through significant improvements across multiple vision language compositionality benchmarks, as well as smaller but significant improvements on general question answering tasks. As a sneak peek, SCRAMBLe tuned Molmo-7B model improves on Winoground from 49.5% to 54.8% (best reported to date), while improving by ~1% on more general visual question answering tasks. Code for SCRAMBLe along with tuned models and our synthetic training dataset is available at this https URL.', 'abstract_zh': '组成性认知，即正确识别场景为原子视觉概念的组合，对于多模态大型语言模型（MLLMs）来说仍然困难。即使是最先进的MLLMs如GPT-4o，在区分“狗追猫”和“猫追狗”这样的组合时也会出错。尽管在Winoground这一衡量此类推理能力的标准上，MLLMs取得了显著进步，但它们仍然远未达到人类的水平。我们表明，通过数据阐明这些概念可以改善这些模型的组成性推理能力，其中模型被训练为更倾向于选择与图像匹配的正确描述，而非接近但错误的描述。我们提出了SCRAMBLe：一种使用二元偏好学习在完全自动化生成的合成偏好数据集上对MLLMs进行合成组成推理增强的技术。SCRAMBLe整体提高了这些MLLMs的组成推理能力，我们在多个视觉语言组成性基准测试中看到了显著提高，并且在一般问题回答任务上也实现了较小但显著的进步。作为一种预览，SCRAMBLe调优的Molmo-7B模型在Winoground上的得分从49.5%提高到54.8%（迄今为止最好的成绩），在更一般视觉问答任务上提高了约1%。SCRAMBle的代码、调优后的模型和我们的合成训练数据集可在以下链接获取。', 'title_zh': '增强视觉语言模型的组合推理能力：使用合成偏好数据'}
{'arxiv_id': 'arXiv:2504.04737', 'title': 'TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction and Explanation in the Indian Legal Context', 'authors': 'Shubham Kumar Nigam, Balaramamahanthi Deepak Patnaik, Shivam Mishra, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya', 'link': 'https://arxiv.org/abs/2504.04737', 'abstract': 'In the landscape of Fact-based Judgment Prediction and Explanation (FJPE), reliance on factual data is essential for developing robust and realistic AI-driven decision-making tools. This paper introduces TathyaNyaya, the largest annotated dataset for FJPE tailored to the Indian legal context, encompassing judgments from the Supreme Court of India and various High Courts. Derived from the Hindi terms "Tathya" (fact) and "Nyaya" (justice), the TathyaNyaya dataset is uniquely designed to focus on factual statements rather than complete legal texts, reflecting real-world judicial processes where factual data drives outcomes. Complementing this dataset, we present FactLegalLlama, an instruction-tuned variant of the LLaMa-3-8B Large Language Model (LLM), optimized for generating high-quality explanations in FJPE tasks. Finetuned on the factual data in TathyaNyaya, FactLegalLlama integrates predictive accuracy with coherent, contextually relevant explanations, addressing the critical need for transparency and interpretability in AI-assisted legal systems. Our methodology combines transformers for binary judgment prediction with FactLegalLlama for explanation generation, creating a robust framework for advancing FJPE in the Indian legal domain. TathyaNyaya not only surpasses existing datasets in scale and diversity but also establishes a benchmark for building explainable AI systems in legal analysis. The findings underscore the importance of factual precision and domain-specific tuning in enhancing predictive performance and interpretability, positioning TathyaNyaya and FactLegalLlama as foundational resources for AI-assisted legal decision-making.', 'abstract_zh': '基于事实判断预测与解释(FJPE)的景观中，依赖事实数据对于开发稳健且现实的AI驱动决策工具至关重要。本文介绍TathyaNyaya，这是专门为印度法律情境设计的最⼤标注数据集，涵盖了印度最高法院和各地⾼级法院的判决。TathyaNyaya数据集源自印地语术语“Tathya”（事实）和“Nyaya”（正义），特别设计聚焦于事实陈述而非完整的法律文本，反映现实法律程序中事实数据驱动结果的现象。作为该数据集的补充，本文还 소개了FactLegalLlama，这是对LLaMa-3-8B大规模语言模型的指令微调变体，优化用于生成FJPE任务中的高质量解释。FactLegalLlama在TathyaNyaya的实证数据上进行微调，结合预测准确性与连贯、相关性的解释生成，解决了AI辅助法律系统中透明性和可解释性的关键需求。本研究方法结合使用变压器进行二元判决预测，并利用FactLegalLlama进行解释生成，构建了一个适用于印度法律领域的FJPE robust框架。TathyaNyaya不仅在规模和多样性上超过了现有数据集，还确立了构建可解释法律分析AI系统的基准。研究结果强调了事实精准度和领域特定调整在提高预测性能和可解释性中的重要性，将TathyaNyaya和FactLegalLlama定位为AI辅助法律决策的基础资源。', 'title_zh': 'Tathya Nyaya和FactLegalLlama：在印度法律 contexts 中推动事实判断预测与解释的进步'}
{'arxiv_id': 'arXiv:2504.04718', 'title': 'T1: Tool-integrated Self-verification for Test-time Compute Scaling in Small Language Models', 'authors': 'Minki Kang, Jongwon Jeong, Jaewoong Cho', 'link': 'https://arxiv.org/abs/2504.04718', 'abstract': 'Recent studies have demonstrated that test-time compute scaling effectively improves the performance of small language models (sLMs). However, prior research has mainly examined test-time compute scaling with an additional larger model as a verifier, leaving self-verification by sLMs underexplored. In this work, we investigate whether sLMs can reliably self-verify their outputs under test-time scaling. We find that even with knowledge distillation from larger verifiers, sLMs struggle with verification tasks requiring memorization, such as numerical calculations and fact-checking. To address this limitation, we propose Tool-integrated self-verification (T1), which delegates memorization-heavy verification steps to external tools, such as a code interpreter. Our theoretical analysis shows that tool integration reduces memorization demands and improves test-time scaling performance. Experiments on the MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under test-time scaling outperforms the significantly larger Llama-3.1 8B model. Moreover, T1 generalizes effectively to both mathematical (MATH500) and multi-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the potential of tool integration to substantially improve the self-verification abilities of sLMs.', 'abstract_zh': 'Recent Studies Have Demonstrated that Test-Time Compute Scaling Effectively Improves the Performance of Small Language Models (sLMs): Investigating Reliable Self-Verification Without Additional Larger Models', 'title_zh': '工具整合的自我验证方法用于小型语言模型测试时的计算量缩放'}
{'arxiv_id': 'arXiv:2504.04717', 'title': 'Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models', 'authors': 'Yubo Li, Xiaobin Shen, Xinyu Yao, Xueying Ding, Yidi Miao, Ramayya Krishnan, Rema Padman', 'link': 'https://arxiv.org/abs/2504.04717', 'abstract': 'Recent advancements in large language models (LLMs) have revolutionized their ability to handle single-turn tasks, yet real-world applications demand sophisticated multi-turn interactions. This survey provides a comprehensive review of recent advancements in evaluating and enhancing multi-turn interactions in LLMs. Focusing on task-specific scenarios, from instruction following in diverse domains such as math and coding to complex conversational engagements in roleplay, healthcare, education, and even adversarial jailbreak settings, we systematically examine the challenges of maintaining context, coherence, fairness, and responsiveness over prolonged dialogues. The paper organizes current benchmarks and datasets into coherent categories that reflect the evolving landscape of multi-turn dialogue evaluation. In addition, we review a range of enhancement methodologies under multi-turn settings, including model-centric strategies (contextual learning, supervised fine-tuning, reinforcement learning, and new architectures), external integration approaches (memory-augmented, retrieval-based methods, and knowledge graph), and agent-based techniques for collaborative interactions. Finally, we discuss open challenges and propose future directions for research to further advance the robustness and effectiveness of multi-turn interactions in LLMs. Related resources and papers are available at this https URL.', 'abstract_zh': '近期大型语言模型在多轮交互方面的进展及其评估与增强综述', 'title_zh': '超越单轮交互：大规模语言模型多轮交互综述'}
{'arxiv_id': 'arXiv:2504.04704', 'title': 'LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are Important', 'authors': 'Manlai Liang, JiaMing Zhang, Xiong Li, Jinlong Li', 'link': 'https://arxiv.org/abs/2504.04704', 'abstract': 'The increasing size of the Key-Value (KV) cache during the Large Language Models long-context inference is the main obstacle for its balance between the deployment cost and task accuracy. To reduce the KV cache size in such scenarios, most previous efforts leveraged on the attention weight to evict non-critical cache tokens. But there is a trade-off in those methods, they usually require major modifiation of the inference infrastructure and significant computation overhead. Base on the fact that the Large Lanuage models are autoregresssive models, we propose {\\it LagKV}, a KV allocation strategy only relying on straight forward comparison among KV themself. It is a totally attention free method which offers easy integration to the main stream inference platform and comparable performance comparing to other complicated KV compression methods. Results on LongBench and PasskeyRetrieval show that, our approach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx 90\\%$ of the original model performance for $8\\times$. Especially in the 64-digit passkey retrieval task, our mehod outperforms the attention weight based method $H_2O$ over $60\\%$ with same compression ratios. Our code is available at \\url{this https URL}.', 'abstract_zh': 'Large Language Models 中长上下文推理中 Key-Value 缓存规模的增加是其部署成本与任务精度之间平衡的主要障碍。为减少在此类场景中的 Key-Value 缓存规模，大多数先前的努力依赖于利用注意力权重来淘汰非关键缓存令牌。但这些方法通常会带来基础设施的重大修改和显著的计算开销。基于大型语言模型是自回归模型的事实，我们提出了一种仅依赖于 Key-Value 本身直接比较的 {\\it LagKV} KV 分配策略。这是一种完全不依赖注意力的方法，易于集成到主流推理平台，并在压缩比相同的情况下提供与其他复杂 KV 压缩方法相当的性能。LongBench 和 PasskeyRetrieval 的结果表明，当压缩比为 $2\\times$ 时，我们的方法几乎不损失性能；而在 $8\\times$ 压缩比时，保持约 $90\\%$ 的原始模型性能。特别是在 64 位密钥检索任务中，我们的方法在相同的压缩比下比基于注意力权重的方法 $H_2O$ 高出 $60\\%$ 的性能。我们的代码可在 \\url{this https URL} 获取。', 'title_zh': 'LagKV: KV缓存中的滞后相关信息揭示了哪些tokens重要'}
{'arxiv_id': 'arXiv:2504.04702', 'title': 'Provable Failure of Language Models in Learning Majority Boolean Logic via Gradient Descent', 'authors': 'Bo Chen, Zhenmei Shi, Zhao Song, Jiahao Zhang', 'link': 'https://arxiv.org/abs/2504.04702', 'abstract': 'Recent advancements in Transformer-based architectures have led to impressive breakthroughs in natural language processing tasks, with models such as GPT-4, Claude, and Gemini demonstrating human-level reasoning abilities. However, despite their high performance, concerns remain about the inherent limitations of these models, especially when it comes to learning basic logical functions. While complexity-theoretic analyses indicate that Transformers can represent simple logic functions (e.g., $\\mathsf{AND}$, $\\mathsf{OR}$, and majority gates) by its nature of belonging to the $\\mathsf{TC}^0$ class, these results assume ideal parameter settings and do not account for the constraints imposed by gradient descent-based training methods. In this work, we investigate whether Transformers can truly learn simple majority functions when trained using gradient-based methods. We focus on a simplified variant of the Transformer architecture and consider both $n=\\mathrm{poly}(d)$ and $n=\\exp(\\Omega(d))$ number of training samples, where each sample is a $d$-size binary string paired with the output of a basic majority function. Our analysis demonstrates that even after $\\mathrm{poly}(d)$ gradient queries, the generalization error of the Transformer model still remains substantially large, growing exponentially with $d$. This work highlights fundamental optimization challenges in training Transformers for the simplest logical reasoning tasks and provides new insights into their theoretical limitations.', 'abstract_zh': '基于变压器的架构近期取得了自然语言处理任务的显著突破，如GPT-4、Claude和Gemini等模型展示了类人的推理能力。然而，尽管这些模型表现出色，仍然存在着关于其固有限制的担忧，特别是在学习基本逻辑函数方面。虽然复杂性理论分析表明，变压器可以以其属于$\\mathsf{TC}^0$类的性质来表示简单的逻辑函数（例如，$\\mathsf{AND}$、$\\mathsf{OR}$和多数门），但这些结果假设了理想参数设置，并未考虑到基于梯度下降的训练方法所施加的约束。在本工作中，我们探究在基于梯度的训练方法下，变压器是否真的能够学习简单的多数函数。我们集中研究了变压器架构的简化版本，并考虑了两种情形：训练样本数量分别为$N=\\mathrm{poly}(d)$和$N=\\exp(\\Omega(d))$，每一组样本为一个$d$维度的二进制字符串及其所对应的简单多数函数输出。我们的分析表明，即使经过$\\mathrm{poly}(d)$次梯度查询后，变压器模型的泛化误差仍然显著较大，随着$d$呈指数增长。本工作突显了在训练变压器处理最简单逻辑推理任务时的基本优化挑战，并提供了对其理论限制的新见解。', 'title_zh': '语言模型通过梯度下降学习多数布尔逻辑的证明失败'}
{'arxiv_id': 'arXiv:2504.04699', 'title': 'R2Vul: Learning to Reason about Software Vulnerabilities with Reinforcement Learning and Structured Reasoning Distillation', 'authors': 'Martin Weyssow, Chengran Yang, Junkai Chen, Yikun Li, Huihui Huang, Ratnadira Widyasari, Han Wei Ang, Frank Liauw, Eng Lieh Ouh, Lwin Khin Shar, David Lo', 'link': 'https://arxiv.org/abs/2504.04699', 'abstract': 'Large language models (LLMs) have shown promising performance in software vulnerability detection (SVD), yet their reasoning capabilities remain unreliable. Existing approaches relying on chain-of-thought (CoT) struggle to provide relevant and actionable security assessments. Additionally, effective SVD requires not only generating coherent reasoning but also differentiating between well-founded and misleading yet plausible security assessments, an aspect overlooked in prior work. To this end, we introduce R2Vul, a novel approach that distills structured reasoning into small LLMs using reinforcement learning from AI feedback (RLAIF). Through RLAIF, R2Vul enables LLMs to produce structured, security-aware reasoning that is actionable and reliable while explicitly learning to distinguish valid assessments from misleading ones. We evaluate R2Vul across five languages against SAST tools, CoT, instruction tuning, and classification-based baselines. Our results show that R2Vul with structured reasoning distillation enables a 1.5B student LLM to rival larger models while improving generalization to out-of-distribution vulnerabilities. Beyond model improvements, we contribute a large-scale, multilingual preference dataset featuring structured reasoning to support future research in SVD.', 'abstract_zh': '大型语言模型（LLMs）在软件漏洞检测（SVD）中展现出了令人鼓舞的性能，但其推理能力仍不够可靠。现有依赖链式思考（CoT）的方法难以提供相关且可操作的安全评估。此外，有效的SVD不仅需要生成连贯的推理，还需要能够区分合理的安全评估和误导性的但貌似合理的安全评估，这是先前工作中的一个遗漏方面。为此，我们提出了一种名为R2Vul的新方法，该方法通过从AI反馈强化学习（RLAIF）提炼结构化推理至小型LLM。通过RLAIF，R2Vul使LLM能够产生结构化、安全意识强的推理，这种推理既可操作又可靠，同时明确学习如何区分合理的评估与误导性的评估。我们使用五种语言对R2Vul与SAST工具、CoT、指令调优和基于分类的基本方法进行了评估。结果显示，R2Vul通过结构化推理提炼使一个1.5B参数的学生LLM能够与更大模型竞争，并改进了对分布外漏洞的一般化能力。除了模型改进，我们还贡献了一个大规模的多语言偏好数据集，该数据集中包含结构化推理，以支持未来SVD研究。', 'title_zh': 'R2Vul: 通过强化学习和结构化推理精炼学习软件漏洞分析'}
{'arxiv_id': 'arXiv:2504.04640', 'title': "Splits! A Flexible Dataset for Evaluating a Model's Demographic Social Inference", 'authors': 'Eylon Caplan, Tania Chakraborty, Dan Goldwasser', 'link': 'https://arxiv.org/abs/2504.04640', 'abstract': "Understanding how people of various demographics think, feel, and express themselves (collectively called group expression) is essential for social science and underlies the assessment of bias in Large Language Models (LLMs). While LLMs can effectively summarize group expression when provided with empirical examples, coming up with generalizable theories of how a group's expression manifests in real-world text is challenging. In this paper, we define a new task called Group Theorization, in which a system must write theories that differentiate expression across demographic groups. We make available a large dataset on this task, Splits!, constructed by splitting Reddit posts by neutral topics (e.g. sports, cooking, and movies) and by demographics (e.g. occupation, religion, and race). Finally, we suggest a simple evaluation framework for assessing how effectively a method can generate 'better' theories about group expression, backed by human validation. We publicly release the raw corpora and evaluation scripts for Splits! to help researchers assess how methods infer--and potentially misrepresent--group differences in expression. We make Splits! and our evaluation module available at this https URL.", 'abstract_zh': '理解不同人口统计数据群体在思考、感受和表达自己（统称为群体表达）方面的差异对于社会科学至关重要，并且是评估大型语言模型偏见的基础。尽管大型语言模型在提供实证例子时能够有效总结群体表达，但在现实中如何表现群体表达的具体机制仍然具有挑战性。在本文中，我们定义了一个新任务，称为群体理论化，要求系统撰写能够区分不同人口统计数据群体之间表达的理论。我们提供了一个大型数据集Splits!，该数据集通过按中性话题（如体育、烹饪和电影）和人口统计数据（如职业、宗教和种族）拆分Reddit帖子构建而成。最后，我们提出了一个简单的评估框架，以评估方法生成有关群体表达“更好”理论的能力，并得到人类验证的支持。我们公开发布了Splits!的原始语料库和评估脚本，以帮助研究人员评估方法如何推断以及可能错误地代表群体表达差异。Splits!及其评估模块可在以下链接获取：this https URL。', 'title_zh': 'Splits！一个灵活的数据集，用于评估模型的_demographic和社会推理能力'}
{'arxiv_id': 'arXiv:2504.04534', 'title': 'An Empirical Comparison of Text Summarization: A Multi-Dimensional Evaluation of Large Language Models', 'authors': 'Anantharaman Janakiraman, Behnaz Ghoraani', 'link': 'https://arxiv.org/abs/2504.04534', 'abstract': 'Text summarization is crucial for mitigating information overload across domains like journalism, medicine, and business. This research evaluates summarization performance across 17 large language models (OpenAI, Google, Anthropic, open-source) using a novel multi-dimensional framework. We assessed models on seven diverse datasets (BigPatent, BillSum, CNN/DailyMail, PubMed, SAMSum, WikiHow, XSum) at three output lengths (50, 100, 150 tokens) using metrics for factual consistency, semantic similarity, lexical overlap, and human-like quality, while also considering efficiency factors. Our findings reveal significant performance differences, with specific models excelling in factual accuracy (deepseek-v3), human-like quality (claude-3-5-sonnet), and processing efficiency/cost-effectiveness (gemini-1.5-flash, gemini-2.0-flash). Performance varies dramatically by dataset, with models struggling on technical domains but performing well on conversational content. We identified a critical tension between factual consistency (best at 50 tokens) and perceived quality (best at 150 tokens). Our analysis provides evidence-based recommendations for different use cases, from high-stakes applications requiring factual accuracy to resource-constrained environments needing efficient processing. This comprehensive approach enhances evaluation methodology by integrating quality metrics with operational considerations, incorporating trade-offs between accuracy, efficiency, and cost-effectiveness to guide model selection for specific applications.', 'abstract_zh': '文本总结对于减轻 journalism、medicine 和 business 等领域信息过载至关重要。本研究使用新型多维度框架评估了 17 种大型语言模型（包括 OpenAI、Google、Anthropic 以及开源模型）的总结性能。我们在七个不同的数据集（BigPatent、BillSum、CNN/DailyMail、PubMed、SAMSum、WikiHow、XSum）上，针对三种输出长度（50、100、150 个词元）进行了评估，并使用事实一致性、语义相似度、词汇重叠和人类质量等指标进行评估，同时考虑了效率因素。研究发现不同模型在性能上存在显著差异，部分模型在事实准确性（deepseek-v3）、人类质量（claude-3-5-sonnet）以及处理效率/成本效益（gemini-1.5-flash、gemini-2.0-flash）方面表现出色。不同数据集上模型的表现差异显著，技术领域模型表现不佳，而对话内容表现良好。我们发现事实一致性（最佳表现为 50 个词元）和感知质量（最佳表现为 150 个词元）之间存在关键张力。我们的分析提供了基于证据的建议，适用于不同应用场景，从需要事实准确性的高风险应用到需要高效处理的资源受限环境。通过结合质量指标和运营考虑的全面方法，改进了评估方法，考虑了准确度、效率和成本效益之间的权衡，以指导特定应用中模型的选择。', 'title_zh': '大型语言模型在多维度上的文本摘要 empirical 对比研究'}
{'arxiv_id': 'arXiv:2504.04524', 'title': 'Trust Region Preference Approximation: A simple and stable reinforcement learning algorithm for LLM reasoning', 'authors': 'Xuerui Su, Shufang Xie, Guoqing Liu, Yingce Xia, Renqian Luo, Peiran Jin, Zhiming Ma, Yue Wang, Zun Wang, Yuting Liu', 'link': 'https://arxiv.org/abs/2504.04524', 'abstract': "Recently, Large Language Models (LLMs) have rapidly evolved, approaching Artificial General Intelligence (AGI) while benefiting from large-scale reinforcement learning to enhance Human Alignment (HA) and Reasoning. Recent reward-based optimization algorithms, such as Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO) have achieved significant performance on reasoning tasks, whereas preference-based optimization algorithms such as Direct Preference Optimization (DPO) significantly improve the performance of LLMs on human alignment. However, despite the strong performance of reward-based optimization methods in alignment tasks , they remain vulnerable to reward hacking. Furthermore, preference-based algorithms (such as Online DPO) haven't yet matched the performance of reward-based optimization algorithms (like PPO) on reasoning tasks, making their exploration in this specific area still a worthwhile pursuit. Motivated by these challenges, we propose the Trust Region Preference Approximation (TRPA) algorithm, which integrates rule-based optimization with preference-based optimization for reasoning tasks. As a preference-based algorithm, TRPA naturally eliminates the reward hacking issue. TRPA constructs preference levels using predefined rules, forms corresponding preference pairs, and leverages a novel optimization algorithm for RL training with a theoretical monotonic improvement guarantee. Experimental results demonstrate that TRPA not only achieves competitive performance on reasoning tasks but also exhibits robust stability. The code of this paper are released and updating on this https URL.", 'abstract_zh': '基于可信区域的偏好近似算法（TRPA）：结合规则优化与偏好优化以提升推理任务性能', 'title_zh': '可信区域偏好逼近：一种用于大语言模型推理的简单且稳定的强化学习算法'}
{'arxiv_id': 'arXiv:2504.04520', 'title': 'Hessian of Perplexity for Large Language Models by PyTorch autograd (Open Source)', 'authors': 'Ivan Ilin', 'link': 'https://arxiv.org/abs/2504.04520', 'abstract': 'Computing the full Hessian matrix -- the matrix of second-order derivatives for an entire Large Language Model (LLM) is infeasible due to its sheer size. In this technical report, we aim to provide a comprehensive guide on how to accurately compute at least a small portion of the Hessian for LLMs using PyTorch autograd library. We also demonstrate how to compute the full diagonal of the Hessian matrix using multiple samples of vector-Hessian Products (HVPs). We hope that both this guide and the accompanying GitHub code will be valuable resources for practitioners and researchers interested in better understanding the behavior and structure of the Hessian in LLMs.', 'abstract_zh': '计算全海森矩阵——由于大型语言模型（LLM）的海森矩阵规模庞大，完全计算整个海森矩阵不可行。在本技术报告中，我们旨在提供一个全面的指南，介绍如何使用PyTorch自动求导库准确计算大型语言模型的一部分海森矩阵。我们还将展示如何使用多个向量-海森矩阵乘积（HVPs）样本计算海森矩阵的完整对角线。希望本指南及其附带的GitHub代码能够对那些希望更深入理解大型语言模型海森矩阵的行为和结构的研究人员和实践者提供宝贵的资源。', 'title_zh': '大型语言模型的困惑度海森矩阵通过PyTorch autograd（开源）'}
{'arxiv_id': 'arXiv:2504.04514', 'title': 'Saliency-driven Dynamic Token Pruning for Large Language Models', 'authors': 'Yao Tao, Yehui Tang, Yun Wang, Mingjian Zhu, Hailin Hu, Yunhe Wang', 'link': 'https://arxiv.org/abs/2504.04514', 'abstract': 'Despite the recent success of large language models (LLMs), LLMs are particularly challenging in long-sequence inference scenarios due to the quadratic computational complexity of the attention mechanism. Inspired by the interpretability theory of feature attribution in neural network models, we observe that not all tokens have the same contribution. Based on this observation, we propose a novel token pruning framework, namely Saliency-driven Dynamic Token Pruning (SDTP), to gradually and dynamically prune redundant tokens based on the input context. Specifically, a lightweight saliency-driven prediction module is designed to estimate the importance score of each token with its hidden state, which is added to different layers of the LLM to hierarchically prune redundant tokens. Furthermore, a ranking-based optimization strategy is proposed to minimize the ranking divergence of the saliency score and the predicted importance score. Extensive experiments have shown that our framework is generalizable to various models and datasets. By hierarchically pruning 65\\% of the input tokens, our method greatly reduces 33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during inference, while maintaining comparable performance. We further demonstrate that SDTP can be combined with KV cache compression method for further compression.', 'abstract_zh': '基于显著性驱动动态令牌剪枝的长序列推理优化', 'title_zh': '基于显著性驱动的动态令牌剪枝大语言模型'}
{'arxiv_id': 'arXiv:2504.04462', 'title': 'An overview of model uncertainty and variability in LLM-based sentiment analysis. Challenges, mitigation strategies and the role of explainability', 'authors': 'David Herrera-Poyatos, Carlos Peláez-González, Cristina Zuheros, Andrés Herrera-Poyatos, Virilo Tejedor, Francisco Herrera, Rosana Montes', 'link': 'https://arxiv.org/abs/2504.04462', 'abstract': 'Large Language Models (LLMs) have significantly advanced sentiment analysis, yet their inherent uncertainty and variability pose critical challenges to achieving reliable and consistent outcomes. This paper systematically explores the Model Variability Problem (MVP) in LLM-based sentiment analysis, characterized by inconsistent sentiment classification, polarization, and uncertainty arising from stochastic inference mechanisms, prompt sensitivity, and biases in training data. We analyze the core causes of MVP, presenting illustrative examples and a case study to highlight its impact. In addition, we investigate key challenges and mitigation strategies, paying particular attention to the role of temperature as a driver of output randomness and emphasizing the crucial role of explainability in improving transparency and user trust. By providing a structured perspective on stability, reproducibility, and trustworthiness, this study helps develop more reliable, explainable, and robust sentiment analysis models, facilitating their deployment in high-stakes domains such as finance, healthcare, and policymaking, among others.', 'abstract_zh': '大型语言模型（LLMs）在情感分析方面的进步显著，但其固有的不确定性和变异性对实现可靠和一致的结果构成了关键挑战。本文系统探讨了基于LLM的情感分析中的模型变异性问题（MVP），该问题表现为情感分类的一致性差、极化和不确定性，源于随机推理机制、提示敏感性和训练数据中的偏见。我们分析了MVP的核心原因，并通过示例和案例研究突出其影响。此外，我们研究了关键挑战和缓解策略，特别关注温度作为输出随机性的驱动因素，并强调可解释性在提高透明度和用户信任方面的重要作用。通过提供稳定性、可重复性和可信性方面的结构化视角，本文有助于开发更可靠、可解释和 robust 的情感分析模型，促进其在金融、医疗保健、政策制定等领域中的部署。', 'title_zh': 'LLM基于的情感分析中模型不确定性与变异性的综述：挑战、缓解策略与可解释性的作用'}
{'arxiv_id': 'arXiv:2504.04453', 'title': 'Prot42: a Novel Family of Protein Language Models for Target-aware Protein Binder Generation', 'authors': 'Mohammad Amaan Sayeed, Engin Tekin, Maryam Nadeem, Nancy A. ElNaker, Aahan Singh, Natalia Vassilieva, Boulbaba Ben Amor', 'link': 'https://arxiv.org/abs/2504.04453', 'abstract': "Unlocking the next generation of biotechnology and therapeutic innovation demands overcoming the inherent complexity and resource-intensity of conventional protein engineering methods. Recent GenAI-powered computational techniques often rely on the availability of the target protein's 3D structures and specific binding sites to generate high-affinity binders, constraints exhibited by models such as AlphaProteo and RFdiffusion. In this work, we explore the use of Protein Language Models (pLMs) for high-affinity binder generation. We introduce Prot42, a novel family of Protein Language Models (pLMs) pretrained on vast amounts of unlabeled protein sequences. By capturing deep evolutionary, structural, and functional insights through an advanced auto-regressive, decoder-only architecture inspired by breakthroughs in natural language processing, Prot42 dramatically expands the capabilities of computational protein design based on language only. Remarkably, our models handle sequences up to 8,192 amino acids, significantly surpassing standard limitations and enabling precise modeling of large proteins and complex multi-domain sequences. Demonstrating powerful practical applications, Prot42 excels in generating high-affinity protein binders and sequence-specific DNA-binding proteins. Our innovative models are publicly available, offering the scientific community an efficient and precise computational toolkit for rapid protein engineering.", 'abstract_zh': '解锁下一代生物技术和治疗创新需求，必须克服传统蛋白质工程方法的内在复杂性和资源密集性。近年来，基于GenAI的计算技术往往依赖目标蛋白的3D结构和特定结合位点来生成高亲和力的结合物，这一特性在AlphaProteo和RFdiffusion等模型中有所体现。本工作中，我们探索了蛋白质语言模型（pLMs）在生成高亲和力结合物中的应用。我们介绍了Prot42，一种新型的蛋白质语言模型（pLMs），基于大量的未标注蛋白质序列进行预训练。通过一种先进的自回归、解码器仅结构的高级架构，该架构受到自然语言处理突破的启发，Prot42大大扩展了仅基于语言的计算蛋白质设计的能力。令人惊叹的是，我们的模型可以处理多达8,192个氨基酸的序列，远超标准限制，从而能够精确建模大型蛋白质和复杂多域序列。通过展现强大的实际应用能力，Prot42在生成高亲和力蛋白质结合物和序列特异性DNA结合蛋白方面表现出色。我们的创新模型现已公开，为科学界提供了高效且精确的计算工具箱，用于快速蛋白质工程。', 'title_zh': 'Prot42：一种新的蛋白质语言模型家族，用于目标导向的蛋白质结合物生成'}
{'arxiv_id': 'arXiv:2504.04373', 'title': 'StyleRec: A Benchmark Dataset for Prompt Recovery in Writing Style Transformation', 'authors': 'Shenyang Liu, Yang Gao, Shaoyan Zhai, Liqiang Wang', 'link': 'https://arxiv.org/abs/2504.04373', 'abstract': 'Prompt Recovery, reconstructing prompts from the outputs of large language models (LLMs), has grown in importance as LLMs become ubiquitous. Most users access LLMs through APIs without internal model weights, relying only on outputs and logits, which complicates recovery. This paper explores a unique prompt recovery task focused on reconstructing prompts for style transfer and rephrasing, rather than typical question-answering. We introduce a dataset created with LLM assistance, ensuring quality through multiple techniques, and test methods like zero-shot, few-shot, jailbreak, chain-of-thought, fine-tuning, and a novel canonical-prompt fallback for poor-performing cases. Our results show that one-shot and fine-tuning yield the best outcomes but highlight flaws in traditional sentence similarity metrics for evaluating prompt recovery. Contributions include (1) a benchmark dataset, (2) comprehensive experiments on prompt recovery strategies, and (3) identification of limitations in current evaluation metrics, all of which advance general prompt recovery research, where the structure of the input prompt is unrestricted.', 'abstract_zh': 'Prompt恢复：从大型语言模型（LLMs）的输出重建提示，在LLMs无内部模型权重并通过API供用户访问的情况下变得日益重要。本文探讨了专注于风格转换和重述的提示恢复任务，而非典型的问答任务。我们利用LLM创建了一个数据集，并通过多种技术确保数据集的质量，测试了零样本、少样本、突破限制、思维链、微调以及一种新颖的标准提示后备方法。结果表明，单样本和微调取得最佳效果，但也揭示了传统句子相似度度量在评估提示恢复方面的局限。贡献包括：（1）基准数据集，（2）提示恢复策略的全面实验，以及（3）对当前评估度量标准限制的识别，这些均推动了不受输入提示结构限制的通用提示恢复研究。', 'title_zh': 'StyleRec: 一种写作风格转换中提示恢复基准数据集'}
{'arxiv_id': 'arXiv:2504.04372', 'title': 'How Accurately Do Large Language Models Understand Code?', 'authors': 'Sabaat Haroon, Ahmad Faraz Khan, Ahmad Humayun, Waris Gill, Abdul Haddi Amjad, Ali R. Butt, Mohammad Taha Khan, Muhammad Ali Gulzar', 'link': 'https://arxiv.org/abs/2504.04372', 'abstract': "Large Language Models (LLMs) are increasingly used in post-development tasks such as code repair and testing. A key factor in these tasks' success is the model's deep understanding of code. However, the extent to which LLMs truly understand code remains largely unevaluated. Quantifying code comprehension is challenging due to its abstract nature and the lack of a standardized metric. Previously, this was assessed through developer surveys, which are not feasible for evaluating LLMs. Existing LLM benchmarks focus primarily on code generation, fundamentally different from code comprehension. Additionally, fixed benchmarks quickly become obsolete as they become part of the training data. This paper presents the first large-scale empirical investigation into LLMs' ability to understand code. Inspired by mutation testing, we use an LLM's fault-finding ability as a proxy for its deep code understanding. This approach is based on the insight that a model capable of identifying subtle functional discrepancies must understand the code well. We inject faults in real-world programs and ask the LLM to localize them, ensuring the specifications suffice for fault localization. Next, we apply semantic-preserving code mutations (SPMs) to the faulty programs and test whether the LLMs still locate the faults, verifying their confidence in code understanding. We evaluate nine popular LLMs on 575000 debugging tasks from 670 Java and 637 Python programs. We find that LLMs lose the ability to debug the same bug in 81% of faulty programs when SPMs are applied, indicating a shallow understanding of code and reliance on features irrelevant to semantics. We also find that LLMs understand code earlier in the program better than later. This suggests that LLMs' code comprehension remains tied to lexical and syntactic features due to tokenization designed for natural languages, which overlooks code semantics.", 'abstract_zh': '大型语言模型在代码理解方面的大规模实证研究：基于故障定位能力的代码理解评估', 'title_zh': '大型语言模型对代码理解的准确性如何？'}
{'arxiv_id': 'arXiv:2504.04365', 'title': 'AutoPDL: Automatic Prompt Optimization for LLM Agents', 'authors': 'Claudio Spiess, Mandana Vaziri, Louis Mandel, Martin Hirzel', 'link': 'https://arxiv.org/abs/2504.04365', 'abstract': 'The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and non-transferable across LLMs or tasks. Therefore, this paper proposes AutoPDL, an automated approach to discover good LLM agent configurations. Our method frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and six LLMs (ranging from 8B to 70B parameters) show consistent accuracy gains ($9.5\\pm17.5$ percentage points), up to 68.9pp, and reveal that selected prompting strategies vary across models and tasks.', 'abstract_zh': '大型语言模型（LLMs）的表现取决于它们的提示方式，涵盖从高层提示模式（如零样本、逐步推理、ReAct、ReWOO）到具体的提示内容（指令和少量示例）。手动调整这一组合是繁琐、易出错且在不同LLMs或任务间不可移植的。因此，本文提出了AutoPDL，这是一种自动化的手段，用于发现良好的LLM代理配置。我们的方法将此问题视为一个结构化的自动机器学习（AutoML）问题，通过组合搜索空间中的代理性和非代理性提示模式和示例来求解，使用逐次减半技术高效地导航这一空间。我们引入了一个使用PDL提示编程语言实现常见提示模式的库。AutoPDL解决方案是可读、可编辑和可执行的PDL程序，使用该库。这种方法还允许源到源优化，允许逐次在环中进行改进和重用。在三个任务和六种LLM（参数范围从8B到70B）上的评估显示一致的准确率提升（9.5±17.5 个百分点），最高可达68.9个百分点，并揭示了所选提示策略在不同模型和任务间存在差异。', 'title_zh': 'AutoPDL：自动提示优化 for LLM 代理'}
{'arxiv_id': 'arXiv:2504.04351', 'title': 'DDPT: Diffusion-Driven Prompt Tuning for Large Language Model Code Generation', 'authors': 'Jinyang Li, Sangwon Hyun, M. Ali Babar', 'link': 'https://arxiv.org/abs/2504.04351', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation. However, the quality of the generated code is heavily dependent on the structure and composition of the prompts used. Crafting high-quality prompts is a challenging task that requires significant knowledge and skills of prompt engineering. To advance the automation support for the prompt engineering for LLM-based code generation, we propose a novel solution Diffusion-Driven Prompt Tuning (DDPT) that learns how to generate optimal prompt embedding from Gaussian Noise to automate the prompt engineering for code generation. We evaluate the feasibility of diffusion-based optimization and abstract the optimal prompt embedding as a directional vector toward the optimal embedding. We use the code generation loss given by the LLMs to help the diffusion model capture the distribution of optimal prompt embedding during training. The trained diffusion model can build a path from the noise distribution to the optimal distribution at the sampling phrase, the evaluation result demonstrates that DDPT helps improve the prompt optimization for code generation.', 'abstract_zh': '基于扩散驱动提示调优的大型语言模型代码生成提示工程探索', 'title_zh': 'DDPT：由扩散驱动的提示调优在大规模语言模型代码生成中的应用'}
{'arxiv_id': 'arXiv:2504.04336', 'title': 'Generative Large Language Models Trained for Detecting Errors in Radiology Reports', 'authors': 'Cong Sun, Kurt Teichman, Yiliang Zhou, Brian Critelli, David Nauheim, Graham Keir, Xindi Wang, Judy Zhong, Adam E Flanders, George Shih, Yifan Peng', 'link': 'https://arxiv.org/abs/2504.04336', 'abstract': 'In this retrospective study, a dataset was constructed with two parts. The first part included 1,656 synthetic chest radiology reports generated by GPT-4 using specified prompts, with 828 being error-free synthetic reports and 828 containing errors. The second part included 614 reports: 307 error-free reports between 2011 and 2016 from the MIMIC-CXR database and 307 corresponding synthetic reports with errors generated by GPT-4 on the basis of these MIMIC-CXR reports and specified prompts. All errors were categorized into four types: negation, left/right, interval change, and transcription errors. Then, several models, including Llama-3, GPT-4, and BiomedBERT, were refined using zero-shot prompting, few-shot prompting, or fine-tuning strategies. Finally, the performance of these models was evaluated using the F1 score, 95\\% confidence interval (CI) and paired-sample t-tests on our constructed dataset, with the prediction results further assessed by radiologists. Using zero-shot prompting, the fine-tuned Llama-3-70B-Instruct model achieved the best performance with the following F1 scores: 0.769 for negation errors, 0.772 for left/right errors, 0.750 for interval change errors, 0.828 for transcription errors, and 0.780 overall. In the real-world evaluation phase, two radiologists reviewed 200 randomly selected reports output by the model. Of these, 99 were confirmed to contain errors detected by the models by both radiologists, and 163 were confirmed to contain model-detected errors by at least one radiologist. Generative LLMs, fine-tuned on synthetic and MIMIC-CXR radiology reports, greatly enhanced error detection in radiology reports.', 'abstract_zh': '一项回顾性研究：构建了一个包含两部分的数据集。第一部分包括由GPT-4根据指定提示生成的1,656份合成胸部X光报告，其中828份无错误，828份包含错误。第二部分包括614份报告：来自MIMIC-CXR数据库的2011-2016年间307份无错误报告及其对应的由GPT-4根据这些MIMIC-CXR报告和指定提示生成的含有错误的307份合成报告。所有错误被分类为四种类型：否定形式、左右错误、区间变化和转写错误。随后，使用零样本提示、少数样本提示或微调策略分别对Llama-3、GPT-4和BiomedBERT等模型进行了优化。最后，使用我们的构建数据集上的F1分数、95%置信区间（CI）和配对样本t检验评估了这些模型的性能，并由放射科医生进一步评估了预测结果。使用零样本提示，微调后的Llama-3-70B-Instruct模型在以下F1分数上表现最佳：否定形式错误为0.769，左右错误为0.772，区间变化错误为0.750，转写错误为0.828，总分为0.780。在实际评估阶段，两位放射科医生审查了模型输出的200份随机选取的报告。其中，99份报告被两位放射科医生确认含有模型检测出的错误，163份报告被至少一位放射科医生确认含有模型检测出的错误。基于合成和MIMIC-CXR放射学报告进行微调的生成型大语言模型大大提升了放射学报告中的错误检测能力。', 'title_zh': '生成式大型语言模型用于检测放射学报告中的错误'}
{'arxiv_id': 'arXiv:2504.04335', 'title': 'Hallucination Detection using Multi-View Attention Features', 'authors': 'Yuya Ogasa, Yuki Arase', 'link': 'https://arxiv.org/abs/2504.04335', 'abstract': 'This study tackles token-level hallucination detection in outputs of large language models. Previous studies revealed that attention exhibits irregular patterns when hallucination occurs. Inspired by this, we extract features from the attention matrix that provide complementary views of (a) the average attention each token receives, which helps identify whether certain tokens are overly influential or ignored, (b) the diversity of attention each token receives, which reveals whether attention is biased toward specific subsets, and (c) the diversity of tokens a token attends to during generation, which indicates whether the model references a narrow or broad range of information. These features are input to a Transformer-based classifier to conduct token-level classification to identify hallucinated spans. Experimental results indicate that the proposed method outperforms strong baselines on hallucination detection with longer input contexts, i.e., data-to-text and summarization tasks.', 'abstract_zh': '本研究解决了大规模语言模型输出中的令牌级幻觉检测问题。以往的研究表明，当出现幻觉时，注意力机制会表现出不规则模式。受此启发，我们从注意力矩阵中抽取能够互补的观点特征：(a) 每个令牌平均接收到的注意力，有助于识别某些令牌是否过于重要或被忽略了；(b) 每个令牌接收到的注意力多样性，揭示了注意力是否偏向特定子集；(c) 生成过程中每个令牌关注到的令牌多样性，表明模型是否参考了狭窄或广泛的语料信息。这些特征被输入到基于Transformer的分类器中进行令牌级分类，以识别幻觉区间。实验结果表明，所提出的方法在更长输入上下文（如数据到文本和总结任务）的幻觉检测中优于强基线方法。', 'title_zh': '多视图注意力特征的幻觉检测'}
{'arxiv_id': 'arXiv:2504.04332', 'title': 'IMPersona: Evaluating Individual Level LM Impersonation', 'authors': 'Quan Shi, Carlos Jimenez, Stephen Dong, Brian Seo, Caden Yao, Adam Kelch, Karthik Narasimhan', 'link': 'https://arxiv.org/abs/2504.04332', 'abstract': "As language models achieve increasingly human-like capabilities in conversational text generation, a critical question emerges: to what extent can these systems simulate the characteristics of specific individuals? To evaluate this, we introduce IMPersona, a framework for evaluating LMs at impersonating specific individuals' writing style and personal knowledge. Using supervised fine-tuning and a hierarchical memory-inspired retrieval system, we demonstrate that even modestly sized open-source models, such as Llama-3.1-8B-Instruct, can achieve impersonation abilities at concerning levels. In blind conversation experiments, participants (mis)identified our fine-tuned models with memory integration as human in 44.44% of interactions, compared to just 25.00% for the best prompting-based approach. We analyze these results to propose detection methods and defense strategies against such impersonation attempts. Our findings raise important questions about both the potential applications and risks of personalized language models, particularly regarding privacy, security, and the ethical deployment of such technologies in real-world contexts.", 'abstract_zh': '语言模型在对话文本生成中展现出日益接近人类的能力，一个关键问题是：这些系统能够模拟特定个体的特征到什么程度？为了评估这一点，我们提出了IMPersona框架，用于评估语言模型模仿特定个体的写作风格和个人知识的能力。通过监督微调和基于层次记忆的检索系统，我们展示了即使是规模较小的开源模型，如Llama-3.1-8B-Instruct，也能实现令人关注的模仿能力。在盲测对话实验中，参与者在整合记忆的情况下，误认为我们的微调模型是人类的比例达到了44.44%，而基于最佳提示的方法仅为25.00%。我们分析这些结果，提出了检测此类模仿企图的方法和防御策略。我们的研究结果引发了关于个性化语言模型潜在应用和风险的重要问题，特别是关于隐私、安全以及在现实场景中公平部署此类技术的伦理问题。', 'title_zh': 'IMPersona: 评价个体级别语言模型冒充'}
{'arxiv_id': 'arXiv:2504.04319', 'title': 'Geo-OLM: Enabling Sustainable Earth Observation Studies with Cost-Efficient Open Language Models & State-Driven Workflows', 'authors': 'Dimitrios Stamoulis, Diana Marculescu', 'link': 'https://arxiv.org/abs/2504.04319', 'abstract': 'Geospatial Copilots hold immense potential for automating Earth observation (EO) and climate monitoring workflows, yet their reliance on large-scale models such as GPT-4o introduces a paradox: tools intended for sustainability studies often incur unsustainable costs. Using agentic AI frameworks in geospatial applications can amass thousands of dollars in API charges or requires expensive, power-intensive GPUs for deployment, creating barriers for researchers, policymakers, and NGOs. Unfortunately, when geospatial Copilots are deployed with open language models (OLMs), performance often degrades due to their dependence on GPT-optimized logic. In this paper, we present Geo-OLM, a tool-augmented geospatial agent that leverages the novel paradigm of state-driven LLM reasoning to decouple task progression from tool calling. By alleviating the workflow reasoning burden, our approach enables low-resource OLMs to complete geospatial tasks more effectively. When downsizing to small models below 7B parameters, Geo-OLM outperforms the strongest prior geospatial baselines by 32.8% in successful query completion rates. Our method performs comparably to proprietary models achieving results within 10% of GPT-4o, while reducing inference costs by two orders of magnitude from \\$500-\\$1000 to under \\$10. We present an in-depth analysis with geospatial downstream benchmarks, providing key insights to help practitioners effectively deploy OLMs for EO applications.', 'abstract_zh': '地理空间伴飞助手在自动化地球观测和气候监测工作流中具有巨大的潜力，但依赖大规模模型如GPT-4o引入了一个悖论：本应促进可持续研究的工具却常常导致不可持续的成本。使用自主人工智能框架在地理空间应用中可能会累积数千美元的API费用，或者需要成本高昂、能耗高的GPU进行部署，从而为研究人员、政策制定者和NGOs（非政府组织）设置了障碍。不幸的是，当地理空间伴飞助手使用开放语言模型（OLMs）部署时，由于其依赖于GPT优化逻辑，性能往往会下降。在本文中，我们提出了Geo-OLM，这是一种工具增强的地理空间代理，利用新的状态驱动LLM推理范式来解耦任务进展与工具调用。通过减轻工作流推理负担，我们的方法使低成本的OLMs能够更有效地完成地理空间任务。当模型参数减少到7B以下时，Geo-OLM在成功查询完成率上的表现比先前最优的地理空间基线高出32.8%。我们的方法在结果上与专有模型相当，其表现相对于GPT-4o在10%以内，同时将推理成本降低了两个数量级，从500-1000美元降至不到10美元。我们通过地理空间下游基准测试进行了深入分析，提供了关键见解，以帮助实践者有效部署OLMs用于地球观测应用。', 'title_zh': 'Geo-OLM: 以经济高效的开源语言模型及状态驱动工作流促进可持续的地球观测研究'}
{'arxiv_id': 'arXiv:2504.04314', 'title': 'Balancing Complexity and Informativeness in LLM-Based Clustering: Finding the Goldilocks Zone', 'authors': 'Justin Miller, Tristram Alexander', 'link': 'https://arxiv.org/abs/2504.04314', 'abstract': 'The challenge of clustering short text data lies in balancing informativeness with interpretability. Traditional evaluation metrics often overlook this trade-off. Inspired by linguistic principles of communicative efficiency, this paper investigates the optimal number of clusters by quantifying the trade-off between informativeness and cognitive simplicity. We use large language models (LLMs) to generate cluster names and evaluate their effectiveness through semantic density, information theory, and clustering accuracy. Our results show that Gaussian Mixture Model (GMM) clustering on embeddings generated by a LLM, increases semantic density compared to random assignment, effectively grouping similar bios. However, as clusters increase, interpretability declines, as measured by a generative LLM\'s ability to correctly assign bios based on cluster names. A logistic regression analysis confirms that classification accuracy depends on the semantic similarity between bios and their assigned cluster names, as well as their distinction from alternatives.\nThese findings reveal a "Goldilocks zone" where clusters remain distinct yet interpretable. We identify an optimal range of 16-22 clusters, paralleling linguistic efficiency in lexical categorization. These insights inform both theoretical models and practical applications, guiding future research toward optimising cluster interpretability and usefulness.', 'abstract_zh': '短文本聚类面临的挑战在于平衡信息量与可解释性之间的关系。传统的评估指标往往忽视这种权衡。受语言交流效率语境原则的启发，本文通过量化信息量与认知简单性之间的权衡来探究最优聚类数量。我们使用大规模语言模型（LLMs）生成聚类名称，并通过语义密度、信息理论和聚类准确性进行评估。结果表明，使用LLM生成的嵌入进行高斯混合模型（GMM）聚类相比随机分配，能够增加语义密度，并有效分组相似的个人简介。然而，随着聚类数量的增加，可解释性下降，这一变化通过生成LLM根据聚类名称正确分配个人简介的能力得以衡量。逻辑回归分析证实，分类准确性取决于个人简介与其分配的聚类名称之间的语义相似度，以及与其替代选项的区别。这些发现揭示了一个“黄金分割带”，在这一带中，聚类既保持区分性又具有可解释性。我们确定了最优的聚类范围为16-22个，这与词汇分类中的语言效率相媲美。这些见解不仅为理论模型提供了指导，也为实际应用提供了依据，引导未来研究优化聚类的可解释性和实用性。', 'title_zh': '基于LLM的聚类中复杂性和信息量的平衡：寻找黄金分割点'}
{'arxiv_id': 'arXiv:2504.04310', 'title': 'CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization', 'authors': 'Weiwei Sun, Shengyu Feng, Shanda Li, Yiming Yang', 'link': 'https://arxiv.org/abs/2504.04310', 'abstract': 'Although LLM-based agents have attracted significant attention in domains such as software engineering and machine learning research, their role in advancing combinatorial optimization (CO) remains relatively underexplored. This gap underscores the need for a deeper understanding of their potential in tackling structured, constraint-intensive problems-a pursuit currently limited by the absence of comprehensive benchmarks for systematic investigation. To address this, we introduce CO-Bench, a benchmark suite featuring 36 real-world CO problems drawn from a broad range of domains and complexity levels. CO-Bench includes structured problem formulations and curated data to support rigorous investigation of LLM agents. We evaluate multiple agent frameworks against established human-designed algorithms, revealing key strengths and limitations of current approaches and identifying promising directions for future research. CO-Bench is publicly available at this https URL.', 'abstract_zh': '尽管基于LLM的代理在软件工程和机器学习研究等领域受到了广泛关注，但在组合优化（CO）领域的应用仍然相对未被充分探索。为填补这一空白，我们引入了CO-Bench，这是一个包含36个来自各种领域和复杂程度的实际CO问题的基准套件。CO-Bench 包含结构化的问题形式和精选的数据，以支持对LLM代理进行严格的调查。我们评估了多个代理框架与现有的人类设计算法，揭示了当前方法的关键优势和局限性，并指出了未来研究的前景。CO-Bench 已在以下链接公开发布：this https URL。', 'title_zh': 'CO-Bench: 评估语言模型代理在组合优化算法搜索中的性能'}
{'arxiv_id': 'arXiv:2504.04238', 'title': 'Sensitivity Meets Sparsity: The Impact of Extremely Sparse Parameter Patterns on Theory-of-Mind of Large Language Models', 'authors': 'Yuheng Wu, Wentao Guo, Zirui Liu, Heng Ji, Zhaozhuo Xu, Denghui Zhang', 'link': 'https://arxiv.org/abs/2504.04238', 'abstract': "This paper investigates the emergence of Theory-of-Mind (ToM) capabilities in large language models (LLMs) from a mechanistic perspective, focusing on the role of extremely sparse parameter patterns. We introduce a novel method to identify ToM-sensitive parameters and reveal that perturbing as little as 0.001% of these parameters significantly degrades ToM performance while also impairing contextual localization and language understanding. To understand this effect, we analyze their interaction with core architectural components of LLMs. Our findings demonstrate that these sensitive parameters are closely linked to the positional encoding module, particularly in models using Rotary Position Embedding (RoPE), where perturbations disrupt dominant-frequency activations critical for contextual processing. Furthermore, we show that perturbing ToM-sensitive parameters affects LLM's attention mechanism by modulating the angle between queries and keys under positional encoding. These insights provide a deeper understanding of how LLMs acquire social reasoning abilities, bridging AI interpretability with cognitive science. Our results have implications for enhancing model alignment, mitigating biases, and improving AI systems designed for human interaction.", 'abstract_zh': '本研究从机制角度探讨了大型语言模型（LLM）中理论思维（ToM）能力的 emergence，重点关注极稀疏参数模式的作用。我们提出了一种新型方法来识别 ToM 敏感参数，并揭示出扰动这些参数的 0.001% 可显著降低 ToM 性能，同时损害上下文定位和语言理解。为了理解这一效应，我们分析了它们与 LLM 核心架构组件的相互作用。研究发现，这些敏感参数与位置编码模块密切相关，特别是在使用旋转位置嵌入（RoPE）的模型中，扰动会破坏对上下文处理至关重要的主导频率激活。此外，我们展示了扰动 ToM 敏感参数如何通过调节编码位置下查询和密钥之间的角度影响 LLM 的注意机制。这些洞察为我们理解 LLM 如何获得社会推理能力提供了更深入的理解，将 AI 可解释性与认知科学联系起来。我们的结果对增强模型对齐、减轻偏见以及改进设计用于人类互动的 AI 系统具有重要意义。', 'title_zh': '敏感性与稀疏性：极稀疏参数模式对大型语言模型心智理论的影响'}
{'arxiv_id': 'arXiv:2504.04222', 'title': 'TrafficLLM: Enhancing Large Language Models for Network Traffic Analysis with Generic Traffic Representation', 'authors': 'Tianyu Cui, Xinjie Lin, Sijia Li, Miao Chen, Qilei Yin, Qi Li, Ke Xu', 'link': 'https://arxiv.org/abs/2504.04222', 'abstract': 'Machine learning (ML) powered network traffic analysis has been widely used for the purpose of threat detection. Unfortunately, their generalization across different tasks and unseen data is very limited. Large language models (LLMs), known for their strong generalization capabilities, have shown promising performance in various domains. However, their application to the traffic analysis domain is limited due to significantly different characteristics of network traffic. To address the issue, in this paper, we propose TrafficLLM, which introduces a dual-stage fine-tuning framework to learn generic traffic representation from heterogeneous raw traffic data. The framework uses traffic-domain tokenization, dual-stage tuning pipeline, and extensible adaptation to help LLM release generalization ability on dynamic traffic analysis tasks, such that it enables traffic detection and traffic generation across a wide range of downstream tasks. We evaluate TrafficLLM across 10 distinct scenarios and 229 types of traffic. TrafficLLM achieves F1-scores of 0.9875 and 0.9483, with up to 80.12% and 33.92% better performance than existing detection and generation methods. It also shows strong generalization on unseen traffic with an 18.6% performance improvement. We further evaluate TrafficLLM in real-world scenarios. The results confirm that TrafficLLM is easy to scale and achieves accurate detection performance on enterprise traffic.', 'abstract_zh': '基于机器学习的网络流量分析在威胁检测中得到了广泛应用，但其在不同任务和未见数据上的泛化能力非常有限。大型语言模型（LLMs）因其强大的泛化能力，在多个领域展现出了有前途的表现。然而，由于网络流量的特性与LLMs存在显著差异，其在流量分析领域的应用受到了限制。为解决这一问题，本文提出TrafficLLM，一种双阶段 fine-tuning 框架，用于从异构的原始流量数据中学习通用的流量表示。该框架通过网络流量领域的分词、双阶段调优管道以及可扩展的适应机制，帮助LLMs在动态流量分析任务上释放泛化能力，从而使得流量检测和流量生成适用于广泛的下游任务。我们在10种不同的场景和229种类型的流量上评估了TrafficLLM，结果显示其F1分数分别为0.9875和0.9483，比现有检测和生成方法分别高出了80.12%和33.92%。它在未见流量上的泛化能力也表现出色，性能提升了18.6%。我们进一步在实际场景中评估了TrafficLLM，结果表明TrafficLLM易于扩展，并在企业流量检测上实现了准确的检测性能。', 'title_zh': 'TrafficLLM: 通过通用traffic表示增强大型语言模型在网络流量分析中的性能'}
{'arxiv_id': 'arXiv:2504.04215', 'title': 'Towards Understanding and Improving Refusal in Compressed Models via Mechanistic Interpretability', 'authors': 'Vishnu Kabir Chhabra, Mohammad Mahdi Khalili', 'link': 'https://arxiv.org/abs/2504.04215', 'abstract': 'The rapid growth of large language models has spurred significant interest in model compression as a means to enhance their accessibility and practicality. While extensive research has explored model compression through the lens of safety, findings suggest that safety-aligned models often lose elements of trustworthiness post-compression. Simultaneously, the field of mechanistic interpretability has gained traction, with notable discoveries, such as the identification of a single direction in the residual stream mediating refusal behaviors across diverse model architectures. In this work, we investigate the safety of compressed models by examining the mechanisms of refusal, adopting a novel interpretability-driven perspective to evaluate model safety. Furthermore, leveraging insights from our interpretability analysis, we propose a lightweight, computationally efficient method to enhance the safety of compressed models without compromising their performance or utility.', 'abstract_zh': '大规模语言模型的快速增长激发了对模型压缩的兴趣，以提高其易用性和实用性。尽管已有大量研究从安全性的角度探索模型压缩，研究表明，安全对齐的模型在压缩后往往会失去部分可信度。同时，机制可解释性领域也取得了进展，例如发现残差流中的一个方向在跨多种模型架构中调节拒绝行为。在本工作中，我们通过研究拒绝机制来探讨压缩模型的安全性，并采用一种新颖的可解释性驱动视角评估模型安全性。此外，基于我们的可解释性分析所得洞察，我们提出了一种轻量级、计算效率高的方法，以增强压缩模型的安全性而不牺牲其性能或实用性。', 'title_zh': '通过机械解释性方法理解并改善压缩模型中的拒绝服务'}
{'arxiv_id': 'arXiv:2504.04204', 'title': 'Adaptive Elicitation of Latent Information Using Natural Language', 'authors': 'Jimmy Wang, Thomas Zollo, Richard Zemel, Hongseok Namkoong', 'link': 'https://arxiv.org/abs/2504.04204', 'abstract': 'Eliciting information to reduce uncertainty about a latent entity is a critical task in many application domains, e.g., assessing individual student learning outcomes, diagnosing underlying diseases, or learning user preferences. Though natural language is a powerful medium for this purpose, large language models (LLMs) and existing fine-tuning algorithms lack mechanisms for strategically gathering information to refine their own understanding of the latent entity. To harness the generalization power and world knowledge of LLMs in developing effective information-gathering strategies, we propose an adaptive elicitation framework that actively reduces uncertainty on the latent entity. Since probabilistic modeling of an abstract latent entity is difficult, our framework adopts a predictive view of uncertainty, using a meta-learned language model to simulate future observations and enable scalable uncertainty quantification over complex natural language. Through autoregressive forward simulation, our model quantifies how new questions reduce epistemic uncertainty, enabling the development of sophisticated information-gathering strategies to choose the most informative next queries. In experiments on the 20 questions game, dynamic opinion polling, and adaptive student assessment, our method consistently outperforms baselines in identifying critical unknowns and improving downstream predictions, illustrating the promise of strategic information gathering in natural language settings.', 'abstract_zh': '利用自适应引证框架减轻潜藏实体不确定性以提高自然语言应用领域中的信息收集策略有效性', 'title_zh': '基于自然语言的潜在信息自适应提取'}
{'arxiv_id': 'arXiv:2504.04150', 'title': 'Reasoning on Multiple Needles In A Haystack', 'authors': 'Yidong Wang', 'link': 'https://arxiv.org/abs/2504.04150', 'abstract': "The Needle In A Haystack (NIAH) task has been widely used to evaluate the long-context question-answering capabilities of Large Language Models (LLMs). However, its reliance on simple retrieval limits its effectiveness. To address this limitation, recent studies have introduced the Multiple Needles In A Haystack Reasoning (MNIAH-R) task, which incorporates supporting documents (Multiple needles) of multi-hop reasoning tasks into a distracting context (Haystack}). Despite this advancement, existing approaches still fail to address the issue of models providing direct answers from internal knowledge, and they do not explain or mitigate the decline in accuracy as context length increases. In this paper, we tackle the memory-based answering problem by filtering out direct-answer questions, and we reveal that performance degradation is primarily driven by the reduction in the length of the thinking process as the input length increases. Building on this insight, we decompose the thinking process into retrieval and reasoning stages and introduce a reflection mechanism for multi-round extension. We also train a model using the generated iterative thinking process, which helps mitigate the performance degradation. Furthermore, we demonstrate the application of this retrieval-reflection capability in mathematical reasoning scenarios, improving GPT-4o's performance on AIME2024.", 'abstract_zh': '基于记忆的答案过滤任务：多层次思路推理与反射机制在大型语言模型中的应用', 'title_zh': 'haystack 中的多个针线推理'}
{'arxiv_id': 'arXiv:2504.04099', 'title': 'TARAC: Mitigating Hallucination in LVLMs via Temporal Attention Real-time Accumulative Connection', 'authors': 'Chunzhao Xie, Tongxuan Liu, Lei Jiang, Yuting Zeng, jinrong Guo, Yunheng Shen, Weizhe Huang, Jing Li, Xiaohua Xu', 'link': 'https://arxiv.org/abs/2504.04099', 'abstract': 'Large Vision-Language Models have demonstrated remarkable performance across various tasks; however, the challenge of hallucinations constrains their practical applications. The hallucination problem arises from multiple factors, including the inherent hallucinations in language models, the limitations of visual encoders in perception, and biases introduced by multimodal data. Extensive research has explored ways to mitigate hallucinations. For instance, OPERA prevents the model from overly focusing on "anchor tokens", thereby reducing hallucinations, whereas VCD mitigates hallucinations by employing a contrastive decoding approach. In this paper, we investigate the correlation between the decay of attention to image tokens and the occurrence of hallucinations. Based on this finding, we propose Temporal Attention Real-time Accumulative Connection (TARAC), a novel training-free method that dynamically accumulates and updates LVLMs\' attention on image tokens during generation. By enhancing the model\'s attention to image tokens, TARAC mitigates hallucinations caused by the decay of attention on image tokens. We validate the effectiveness of TARAC across multiple models and datasets, demonstrating that our approach substantially mitigates hallucinations. In particular, TARAC reduces $C_S$ by 25.2 and $C_I$ by 8.7 compared to VCD on the CHAIR benchmark.', 'abstract_zh': '大规模多模态模型在各种任务中展现了出色的表现；然而，幻觉问题限制了它们的实际应用。幻觉问题由多种因素引起，包括语言模型固有的幻觉、视觉编码器感知能力的局限性以及多模态数据引入的偏差。广泛的研究探索了减轻幻觉的方法。例如，OPERA通过防止模型过度关注“锚定词元”从而减少幻觉，而VCD通过对比解码的方法减轻幻觉。在本文中，我们研究了注意力衰减对图像词元注意力与幻觉发生之间的关系。基于这一发现，我们提出了一种新的无需训练的方法——时间注意力实时累积连接（TARAC），该方法在生成过程中动态地累积和更新LVLMs对图像词元的注意力。通过增强模型对图像词元的注意力，TARAC减轻了由注意力衰减引起的幻觉。我们在多个模型和数据集上验证了TARAC的有效性，表明我们的方法显著减轻了幻觉。特别是在CHAIR基准上，TARAC相比VCD减少了$C_S$ 25.2和$C_I$ 8.7。', 'title_zh': 'TARAC: 通过时间注意连接实时累积连接减轻LVLMs幻觉问题'}
{'arxiv_id': 'arXiv:2504.04060', 'title': 'VocalNet: Speech LLM with Multi-Token Prediction for Faster and High-Quality Generation', 'authors': 'Yuhao Wang, Heyang Liu, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang', 'link': 'https://arxiv.org/abs/2504.04060', 'abstract': 'Speech large language models (LLMs) have emerged as a prominent research focus in speech processing. We propose VocalNet-1B and VocalNet-8B, a series of high-performance, low-latency speech LLMs enabled by a scalable and model-agnostic training framework for real-time voice interaction. Departing from the conventional next-token prediction (NTP), we introduce multi-token prediction (MTP), a novel approach optimized for speech LLMs that simultaneously improves generation speed and quality. Experiments show that VocalNet outperforms mainstream Omni LLMs despite using significantly less training data, while also surpassing existing open-source speech LLMs by a substantial margin. To support reproducibility and community advancement, we will open-source all model weights, inference code, training data, and framework implementations upon publication.', 'abstract_zh': '基于语音的大语言模型（LLMs）已成为语音处理领域的研究重点。我们提出VocalNet-1B和VocalNet-8B，这是一种通过可扩展且模型无关的训练框架实现的高性能、低延迟语音LLMs系列，用于实时语音交互。不同于传统的下一个token预测（NTP），我们引入了多token预测（MTP），这是一种针对语音LLMs优化的新方法，可以同时提高生成速度和质量。实验表明，VocalNet在使用显著较少训练数据的情况下，性能优于主流的Omni LLMs，并且在开放源代码的语音LLMs中表现优异。为了支持可重复性和社区发展，我们将公开所有模型权重、推理代码、训练数据和框架实现。', 'title_zh': 'VocalNet：具有多令牌预测的语音LLM，以实现更快更高质量的生成'}
{'arxiv_id': 'arXiv:2504.04022', 'title': 'Rethinking Reflection in Pre-Training', 'authors': 'Essential AI, Darsh J Shah, Peter Rushton, Somanshu Singla, Mohit Parmar, Kurt Smith, Yash Vanjani, Ashish Vaswani, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma, Anil Thomas, Anthony Polloreno, Ashish Tanwer, Burhan Drak Sibai, Divya S Mansingka, Divya Shivaprasad, Ishaan Shah, Karl Stratos, Khoi Nguyen, Michael Callahan, Michael Pust, Mrinal Iyer, Philip Monk, Platon Mazarakis, Ritvik Kapila, Saurabh Srivastava, Tim Romanski', 'link': 'https://arxiv.org/abs/2504.04022', 'abstract': "A language model's ability to reflect on its own reasoning provides a key advantage for solving complex problems. While most recent research has focused on how this ability develops during reinforcement learning, we show that it actually begins to emerge much earlier - during the model's pre-training. To study this, we introduce deliberate errors into chains-of-thought and test whether the model can still arrive at the correct answer by recognizing and correcting these mistakes. By tracking performance across different stages of pre-training, we observe that this self-correcting ability appears early and improves steadily over time. For instance, an OLMo2-7B model pre-trained on 4 trillion tokens displays self-correction on our six self-reflection tasks.", 'abstract_zh': '一种语言模型自我反思其推理能力为其解决复杂问题提供了关键优势。虽然最近的研究主要集中在这种能力在强化学习中的发展过程，但我们表明这种能力实际上在模型的预训练阶段就已经开始出现。为了研究这一点，我们引入了人工错误到推理链中，并测试模型是否能够通过识别和纠正这些错误而仍然得出正确的答案。通过跟踪不同预训练阶段的表现，我们观察到这种自我纠正的能力早在预训练初期就出现了，并且随着时间的推移逐渐提高。例如，一个在4万亿个词元上进行预训练的OLMo2-7B模型在我们的六个自我反思任务中展示了自我纠正能力。', 'title_zh': '重思预训练中的反射机制'}
{'arxiv_id': 'arXiv:2504.03991', 'title': 'Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication with Large Language Models', 'authors': 'Siddharth Srikanth, Varun Bhatt, Boshen Zhang, Werner Hager, Charles Michael Lewis, Katia P. Sycara, Aaquib Tabrez, Stefanos Nikolaidis', 'link': 'https://arxiv.org/abs/2504.03991', 'abstract': 'Understanding how humans collaborate and communicate in teams is essential for improving human-agent teaming and AI-assisted decision-making. However, relying solely on data from large-scale user studies is impractical due to logistical, ethical, and practical constraints, necessitating synthetic models of multiple diverse human behaviors. Recently, agents powered by Large Language Models (LLMs) have been shown to emulate human-like behavior in social settings. But, obtaining a large set of diverse behaviors requires manual effort in the form of designing prompts. On the other hand, Quality Diversity (QD) optimization has been shown to be capable of generating diverse Reinforcement Learning (RL) agent behavior. In this work, we combine QD optimization with LLM-powered agents to iteratively search for prompts that generate diverse team behavior in a long-horizon, multi-step collaborative environment. We first show, through a human-subjects experiment (n=54 participants), that humans exhibit diverse coordination and communication behavior in this domain. We then show that our approach can effectively replicate trends from human teaming data and also capture behaviors that are not easily observed without collecting large amounts of data. Our findings highlight the combination of QD and LLM-powered agents as an effective tool for studying teaming and communication strategies in multi-agent collaboration.', 'abstract_zh': '理解人类在团队中的协作和沟通方式对于提升人类-代理团队协作和AI辅助决策至关重要。然而，依赖大规模用户研究的数据由于物流、伦理和实践限制而不切实际，需要合成多种多样的人类行为模型。最近，由大型语言模型（LLMs）驱动的代理已经在社交环境中表现出类似人类的行为。但获得多样化的行为集需要手动设计提示的劳动。另一方面，质量多样性（QD）优化已被证明能够生成多样化的强化学习（RL）代理行为。在本工作中，我们将QD优化与LLM驱动的代理结合，迭代搜索生成多样化团队行为的提示，在长时段多步协作环境中。我们首先通过一个被试研究（n=54参与者）展示了在该领域人类的多样化协调和沟通行为。然后我们展示了该方法可以有效地复制人类团队数据的趋势，并且能够捕捉到没有大量数据收集难以观察到的行为。我们的发现突显了QD与LLM驱动代理结合作为一种有效工具，用于研究多代理协作中的团队合作与沟通策略。', 'title_zh': '算法激发用于多元类人团队协作与交流的大语言模型指令'}
{'arxiv_id': 'arXiv:2504.03976', 'title': 'OLAF: An Open Life Science Analysis Framework for Conversational Bioinformatics Powered by Large Language Models', 'authors': 'Dylan Riffle, Nima Shirooni, Cody He, Manush Murali, Sovit Nayak, Rishikumar Gopalan, Diego Gonzalez Lopez', 'link': 'https://arxiv.org/abs/2504.03976', 'abstract': 'OLAF (Open Life Science Analysis Framework) is an open-source platform that enables researchers to perform bioinformatics analyses using natural language. By combining large language models (LLMs) with a modular agent-pipe-router architecture, OLAF generates and executes bioinformatics code on real scientific data, including formats like .h5ad. The system includes an Angular front end and a Python/Firebase backend, allowing users to run analyses such as single-cell RNA-seq workflows, gene annotation, and data visualization through a simple web interface. Unlike general-purpose AI tools, OLAF integrates code execution, data handling, and scientific libraries in a reproducible, user-friendly environment. It is designed to lower the barrier to computational biology for non-programmers and support transparent, AI-powered life science research.', 'abstract_zh': 'OLAF（开放生命科学分析框架）是一个开源平台，使研究人员能够使用自然语言进行生物信息学分析。通过结合大型语言模型（LLMs）与模块化代理-管道-路由器架构，OLAF生成并执行针对真实科学数据（包括.h5ad格式）的生物信息学代码。该系统包括一个Angular前端和一个Python/Firebase后端，允许用户通过简单的网页界面运行如单细胞RNA-seq工作流、基因注释和数据可视化等分析。与通用人工智能工具不同，OLAF在一个可重复的、用户友好的环境中集成了代码执行、数据处理和科学库，旨在降低非程序员进入计算生物学的门槛，并支持透明的、基于AI的生命科学研究。', 'title_zh': 'OLAF：一种基于大型语言模型的对话生物信息学开放生命科学分析框架'}
{'arxiv_id': 'arXiv:2504.03975', 'title': 'GREATERPROMPT: A Unified, Customizable, and High-Performing Open-Source Toolkit for Prompt Optimization', 'authors': 'Wenliang Zheng, Sarkar Snigdha Sarathi Das, Yusen Zhang, Rui Zhang', 'link': 'https://arxiv.org/abs/2504.03975', 'abstract': 'LLMs have gained immense popularity among researchers and the general public for its impressive capabilities on a variety of tasks. Notably, the efficacy of LLMs remains significantly dependent on the quality and structure of the input prompts, making prompt design a critical factor for their performance. Recent advancements in automated prompt optimization have introduced diverse techniques that automatically enhance prompts to better align model outputs with user expectations. However, these methods often suffer from the lack of standardization and compatibility across different techniques, limited flexibility in customization, inconsistent performance across model scales, and they often exclusively rely on expensive proprietary LLM APIs. To fill in this gap, we introduce GREATERPROMPT, a novel framework that democratizes prompt optimization by unifying diverse methods under a unified, customizable API while delivering highly effective prompts for different tasks. Our framework flexibly accommodates various model scales by leveraging both text feedback-based optimization for larger LLMs and internal gradient-based optimization for smaller models to achieve powerful and precise prompt improvements. Moreover, we provide a user-friendly Web UI that ensures accessibility for non-expert users, enabling broader adoption and enhanced performance across various user groups and application scenarios. GREATERPROMPT is available at this https URL via GitHub, PyPI, and web user interfaces.', 'abstract_zh': 'GREATERPROMPT：统一可定制的提示优化框架', 'title_zh': 'GREATERPROMPT: 一个统一、可定制且高性能的开源提示优化工具包'}
{'arxiv_id': 'arXiv:2504.03966', 'title': 'Bridging LMS and Generative AI: Dynamic Course Content Integration (DCCI) for Connecting LLMs to Course Content -- The Ask ME Assistant', 'authors': 'Kovan Mzwri, Márta Turcsányi-Szabo', 'link': 'https://arxiv.org/abs/2504.03966', 'abstract': "The integration of Large Language Models (LLMs) with Learning Management Systems (LMSs) has the potential to enhance task automation and accessibility in education. However, hallucination where LLMs generate inaccurate or misleading information remains a significant challenge. This study introduces the Dynamic Course Content Integration (DCCI) mechanism, which dynamically retrieves and integrates course content and curriculum from Canvas LMS into the LLM-powered assistant, Ask ME. By employing prompt engineering to structure retrieved content within the LLM's context window, DCCI ensures accuracy, relevance, and contextual alignment, mitigating hallucination. To evaluate DCCI's effectiveness, Ask ME's usability, and broader student perceptions of AI in education, a mixed-methods approach was employed, incorporating user satisfaction ratings and a structured survey. Results from a pilot study indicate high user satisfaction (4.614/5), with students recognizing Ask ME's ability to provide timely and contextually relevant responses for both administrative and course-related inquiries. Additionally, a majority of students agreed that Ask ME's integration with course content in Canvas LMS reduced platform-switching, improving usability, engagement, and comprehension. AI's role in reducing classroom hesitation and fostering self-directed learning and intellectual curiosity was also highlighted. Despite these benefits and positive perception of AI tools, concerns emerged regarding over-reliance on AI, accuracy limitations, and ethical issues such as plagiarism and reduced student-teacher interaction. These findings emphasize the need for strategic AI implementation, ethical safeguards, and a pedagogical framework that prioritizes human-AI collaboration over substitution.", 'abstract_zh': '大型语言模型与学习管理系统集成在教育中的潜在增强任务自动化和访问性：动态课程内容集成机制及其评估', 'title_zh': 'LMS与生成式AI桥梁：动态课程内容整合（DCCI）以连接LLM与课程内容——Ask ME助手'}
{'arxiv_id': 'arXiv:2504.03964', 'title': 'Clinical ModernBERT: An efficient and long context encoder for biomedical text', 'authors': 'Simon A. Lee, Anthony Wu, Jeffrey N. Chiang', 'link': 'https://arxiv.org/abs/2504.03964', 'abstract': 'We introduce Clinical ModernBERT, a transformer based encoder pretrained on large scale biomedical literature, clinical notes, and medical ontologies, incorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with their textual descriptions. Building on ModernBERT the current state of the art natural language text encoder featuring architectural upgrades such as rotary positional embeddings (RoPE), Flash Attention, and extended context length up to 8,192 tokens our model adapts these innovations specifically for biomedical and clinical domains. Clinical ModernBERT excels at producing semantically rich representations tailored for long context tasks. We validate this both by analyzing its pretrained weights and through empirical evaluation on a comprehensive suite of clinical NLP benchmarks.', 'abstract_zh': 'Clinical ModernBERT：基于大规模生物医学文献、临床笔记和医学本体的变压器编码器', 'title_zh': '临床现代BERT：一种高效的大上下文编码器用于生物医学文本'}
{'arxiv_id': 'arXiv:2504.03931', 'title': 'Adaptation of Large Language Models', 'authors': 'Zixuan Ke, Yifei Ming, Shafiq Joty', 'link': 'https://arxiv.org/abs/2504.03931', 'abstract': 'This tutorial on adaptation of LLMs is designed to address the growing demand for models that go beyond the static capabilities of generic LLMs by providing an overview of dynamic, domain-specific, and task-adaptive LLM adaptation techniques. While general LLMs have demonstrated strong generalization across a variety of tasks, they often struggle to perform well in specialized domains such as finance, healthcare, and code generation for underrepresented languages. Additionally, their static nature limits their ability to evolve with the changing world, and they are often extremely large in size, making them impractical and costly to deploy at scale. As a result, the adaptation of LLMs has drawn much attention since the birth of LLMs and is of core importance, both for industry, which focuses on serving its targeted users, and academia, which can greatly benefit from small but powerful LLMs. To address this gap, this tutorial aims to provide an overview of the LLM adaptation techniques. We start with an introduction to LLM adaptation, from both the data perspective and the model perspective. We then emphasize how the evaluation metrics and benchmarks are different from other techniques. After establishing the problems, we explore various adaptation techniques. We categorize adaptation techniques into two main families. The first is parametric knowledge adaptation, which focuses on updating the parametric knowledge within LLMs. Additionally, we will discuss real-time adaptation techniques, including model editing, which allows LLMs to be updated dynamically in production environments. The second kind of adaptation is semi-parametric knowledge adaptation, where the goal is to update LLM parameters to better leverage external knowledge or tools through techniques like retrieval-augmented generation (RAG) and agent-based systems.', 'abstract_zh': 'This Tutorial on LLM Adaptation: An Overview of Dynamic, Domain-Specific, and Task-Adaptive Techniques', 'title_zh': '大型语言模型的适应性'}
{'arxiv_id': 'arXiv:2504.03857', 'title': 'Can ChatGPT Learn My Life From a Week of First-Person Video?', 'authors': 'Keegan Harris', 'link': 'https://arxiv.org/abs/2504.03857', 'abstract': "Motivated by recent improvements in generative AI and wearable camera devices (e.g. smart glasses and AI-enabled pins), I investigate the ability of foundation models to learn about the wearer's personal life through first-person camera data. To test this, I wore a camera headset for 54 hours over the course of a week, generated summaries of various lengths (e.g. minute-long, hour-long, and day-long summaries), and fine-tuned both GPT-4o and GPT-4o-mini on the resulting summary hierarchy. By querying the fine-tuned models, we are able to learn what the models learned about me. The results are mixed: Both models learned basic information about me (e.g. approximate age, gender). Moreover, GPT-4o correctly deduced that I live in Pittsburgh, am a PhD student at CMU, am right-handed, and have a pet cat. However, both models also suffered from hallucination and would make up names for the individuals present in the video footage of my life.", 'abstract_zh': '受近期生成式人工智能和可穿戴相机设备（如智能眼镜和AI驱动的针）进展的启发，我研究了基础模型通过第一人称相机数据学习佩戴者个人生活的能力。为了测试这一点，我在一周内佩戴相机头戴设备共计54小时，生成了不同长度的摘要（例如，1分钟、1小时和1天的摘要），并分别对GPT-4o和GPT-4o-mini进行了微调。通过查询微调后的模型，我们可以了解模型对我了解了多少。结果是混合的：两个模型都学到了一些关于我的基本信息（例如，大约年龄、性别）。此外，GPT-4o正确推断出我住在匹兹堡，是CMU的博士生，是右撇子，并且养有一只宠物猫。然而，两个模型也出现了幻觉，会在视频素材中虚构出人们的名字。', 'title_zh': 'Can ChatGPT从一周的第一人称视频中学习我的生活？'}
{'arxiv_id': 'arXiv:2504.03822', 'title': 'Arti-"fickle" Intelligence: Using LLMs as a Tool for Inference in the Political and Social Sciences', 'authors': 'Lisa P. Argyle, Ethan C. Busby, Joshua R. Gubler, Bryce Hepner, Alex Lyman, David Wingate', 'link': 'https://arxiv.org/abs/2504.03822', 'abstract': 'Generative large language models (LLMs) are incredibly useful, versatile, and promising tools. However, they will be of most use to political and social science researchers when they are used in a way that advances understanding about real human behaviors and concerns. To promote the scientific use of LLMs, we suggest that researchers in the political and social sciences need to remain focused on the scientific goal of inference. To this end, we discuss the challenges and opportunities related to scientific inference with LLMs, using validation of model output as an illustrative case for discussion. We propose a set of guidelines related to establishing the failure and success of LLMs when completing particular tasks, and discuss how we can make inferences from these observations. We conclude with a discussion of how this refocus will improve the accumulation of shared scientific knowledge about these tools and their uses in the social sciences.', 'abstract_zh': '生成型大规模语言模型（LLMs）是极其有用、灵活且前景广阔的工具。然而，只有当它们用于促进对真实人类行为和关切的理解时，才对政治和社会科学家最有用。为了促进LLMs的科学发展应用，我们建议政治和社会科学家研究人员需要专注于科学推理的目标。为此，我们讨论了使用LLMs进行科学推理的挑战与机遇，以模型输出的验证为例进行说明。我们提出了一套关于任务完成时验证LLMs的失败与成功标准的指南，并讨论了我们如何根据这些观察进行推理。最后，我们探讨了这一重新聚焦如何改善对这些工具及其在社会科学中应用的共享科学知识的积累。', 'title_zh': '“易变”的智能：将大语言模型作为推理工具应用于政治和社会科学'}
{'arxiv_id': 'arXiv:2504.03814', 'title': 'Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?', 'authors': 'Grgur Kovač, Jérémy Perez, Rémy Portelas, Peter Ford Dominey, Pierre-Yves Oudeyer', 'link': 'https://arxiv.org/abs/2504.03814', 'abstract': 'Large language models (LLMs) are increasingly contributing to the creation of content on the Internet. This creates a feedback loop as subsequent generations of models will be trained on this generated, synthetic data. This phenomenon is receiving increasing interest, in particular because previous studies have shown that it may lead to distribution shift - models misrepresent and forget the true underlying distributions of human data they are expected to approximate (e.g. resulting in a drastic loss of quality). In this study, we study the impact of human data properties on distribution shift dynamics in iterated training loops. We first confirm that the distribution shift dynamics greatly vary depending on the human data by comparing four datasets (two based on Twitter and two on Reddit). We then test whether data quality may influence the rate of this shift. We find that it does on the twitter, but not on the Reddit datasets. We then focus on a Reddit dataset and conduct a more exhaustive evaluation of a large set of dataset properties. This experiment associated lexical diversity with larger, and semantic diversity with smaller detrimental shifts, suggesting that incorporating text with high lexical (but limited semantic) diversity could exacerbate the degradation of generated text. We then focus on the evolution of political bias, and find that the type of shift observed (bias reduction, amplification or inversion) depends on the political lean of the human (true) distribution. Overall, our work extends the existing literature on the consequences of recursive fine-tuning by showing that this phenomenon is highly dependent on features of the human data on which training occurs. This suggests that different parts of internet (e.g. GitHub, Reddit) may undergo different types of shift depending on their properties.', 'abstract_zh': '大型语言模型（LLMs）越来越多地参与互联网内容的创作。这一过程会形成反馈循环，后续模型将基于这些生成的合成数据进行训练。这一现象引起了越来越多的关注，因为之前的研究表明，这可能会导致分布偏移——模型误表征并忘记了人类数据的真实分布（例如，导致质量急剧下降）。在本研究中，我们研究了人类数据属性对迭代训练循环中分布偏移动态的影响。我们首先通过比较四个数据集（两个基于Twitter，两个基于Reddit）来确认分布偏移动态随人类数据的不同而显著变化。我们接着检验数据质量是否会影响这一偏移的速度。我们发现，在Twitter数据集上存在影响，而在Reddit数据集上不存在影响。然后，我们专注于一个Reddit数据集，并对大量数据集属性进行了更全面的评估。该实验表明词汇多样性与更大的负面偏移相关，而语义多样性与更小的负面影响偏移相关，这表明包含高词汇（但有限语义）多样性的文本可能会加剧生成文本的质量下降。我们还研究了政治偏见的发展，发现观察到的偏移类型（偏见减少、放大或反转）取决于人类（真实）分布的政治倾向。总体而言，我们的研究扩展了关于递归微调后果的现有文献，表明这一现象高度依赖于训练过程中人类数据的特征。这表明互联网的不同部分（例如，GitHub，Reddit）可能根据其属性经历不同类型的变化。', 'title_zh': 'LLMs 中的递归训练循环：训练数据属性如何调节生成数据中的分布偏移？'}
{'arxiv_id': 'arXiv:2504.03794', 'title': 'Entropy-Based Block Pruning for Efficient Large Language Models', 'authors': 'Liangwei Yang, Yuhui Xu, Juntao Tan, Doyen Sahoo, Silvio Savarese, Caiming Xiong, Huan Wang, Shelby Heinecke', 'link': 'https://arxiv.org/abs/2504.03794', 'abstract': 'As large language models continue to scale, their growing computational and storage demands pose significant challenges for real-world deployment. In this work, we investigate redundancy within Transformer-based models and propose an entropy-based pruning strategy to enhance efficiency while maintaining performance. Empirical analysis reveals that the entropy of hidden representations decreases in the early blocks but progressively increases across most subsequent blocks. This trend suggests that entropy serves as a more effective measure of information richness within computation blocks. Unlike cosine similarity, which primarily captures geometric relationships, entropy directly quantifies uncertainty and information content, making it a more reliable criterion for pruning. Extensive experiments demonstrate that our entropy-based pruning approach surpasses cosine similarity-based methods in reducing model size while preserving accuracy, offering a promising direction for efficient model deployment.', 'abstract_zh': '随着大型语言模型的不断扩展，其日益增长的计算和存储需求为实际部署带来了显著挑战。在本文中，我们探讨了Transformer基模型内的冗余，并提出了一种基于熵的剪枝策略，以提高效率同时保持性能。实证分析表明，隐藏表示的熵在早期块中减少，但在大多数后续块中逐渐增加。这一趋势表明，熵是更有效的计算块内信息丰富度的衡量标准。与主要捕捉几何关系的余弦相似性不同，熵直接量化不确定性与信息含量，使其成为更可靠的剪枝标准。广泛的实验表明，我们的基于熵的剪枝方法在减少模型大小的同时保持准确性，为高效的模型部署提供了有前途的方向。', 'title_zh': '基于熵的块剪枝以实现高效的大规模语言模型'}
{'arxiv_id': 'arXiv:2504.03784', 'title': 'Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning', 'authors': 'Kai Ye, Hongyi Zhou, Jin Zhu, Francesco Quinzan, Chengchung Shi', 'link': 'https://arxiv.org/abs/2504.03784', 'abstract': 'Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preferences that may not reflect the complexity and variability of real-world judgments. In this paper, we propose a robust algorithm to enhance the performance of existing approaches under such reward model misspecifications. Theoretically, our algorithm reduces the variance of reward and policy estimators, leading to improved regret bounds. Empirical evaluations on LLM benchmark datasets demonstrate that the proposed algorithm consistently outperforms existing methods, with 77-81% of responses being favored over baselines on the Anthropic Helpful and Harmless dataset.', 'abstract_zh': '从人类反馈强化学习（RLHF）：在奖励模型误设情况下增强现有方法的稳健算法', 'title_zh': '基于人类反馈的大规模语言模型 fine-tuning 的健壮强化学习方法'}
{'arxiv_id': 'arXiv:2504.03770', 'title': 'JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Model', 'authors': 'Yi Nian, Shenzhe Zhu, Yuehan Qin, Li Li, Ziyi Wang, Chaowei Xiao, Yue Zhao', 'link': 'https://arxiv.org/abs/2504.03770', 'abstract': 'Multimodal large language models (MLLMs) excel in vision-language tasks but also pose significant risks of generating harmful content, particularly through jailbreak attacks. Jailbreak attacks refer to intentional manipulations that bypass safety mechanisms in models, leading to the generation of inappropriate or unsafe content. Detecting such attacks is critical to ensuring the responsible deployment of MLLMs. Existing jailbreak detection methods face three primary challenges: (1) Many rely on model hidden states or gradients, limiting their applicability to white-box models, where the internal workings of the model are accessible; (2) They involve high computational overhead from uncertainty-based analysis, which limits real-time detection, and (3) They require fully labeled harmful datasets, which are often scarce in real-world settings. To address these issues, we introduce a test-time adaptive framework called JAILDAM. Our method leverages a memory-based approach guided by policy-driven unsafe knowledge representations, eliminating the need for explicit exposure to harmful data. By dynamically updating unsafe knowledge during test-time, our framework improves generalization to unseen jailbreak strategies while maintaining efficiency. Experiments on multiple VLM jailbreak benchmarks demonstrate that JAILDAM delivers state-of-the-art performance in harmful content detection, improving both accuracy and speed.', 'abstract_zh': '多模态大型语言模型在视觉语言任务中表现出色，但也存在通过 Jailbreak 攻击生成有害内容的重大风险。Jailbreak 攻击指的是故意操纵以绕过模型的安全机制，导致生成不适当或不安全的内容。检测此类攻击对于确保多模态大型语言模型的负责任部署至关重要。现有的 Jailbreak 检测方法面临三大主要挑战：（1）许多方法依赖于模型的隐藏状态或梯度，限制了其在白盒模型中的适用性；（2）基于不确定性分析的高计算开销限制了实时检测；（3）需要完全标记的有害数据集，而在实际应用中此类数据集往往稀缺。为解决这些问题，我们提出了一种测试时自适应框架 JAILDAM。该方法利用基于策略驱动的不安全知识表示的内存导向方法，避免显式暴露于有害数据。通过在测试时动态更新不安全知识，我们的框架在保持效率的同时，能够更好地泛化到未见过的 Jailbreak 策略。多项视觉语言模型 Jailbreak 基准测试表明，JAILDAM 在有害内容检测方面达到了最先进的性能，既提高了准确性又加快了速度。', 'title_zh': 'JailDAM：面向视觉语言模型的自适应记忆逃逸检测'}
{'arxiv_id': 'arXiv:2504.03767', 'title': 'MCP Safety Audit: LLMs with the Model Context Protocol Allow Major Security Exploits', 'authors': 'Brandon Radosevich, John Halloran', 'link': 'https://arxiv.org/abs/2504.03767', 'abstract': "To reduce development overhead and enable seamless integration between potential components comprising any given generative AI application, the Model Context Protocol (MCP) (Anthropic, 2024) has recently been released and subsequently widely adopted. The MCP is an open protocol that standardizes API calls to large language models (LLMs), data sources, and agentic tools. By connecting multiple MCP servers, each defined with a set of tools, resources, and prompts, users are able to define automated workflows fully driven by LLMs. However, we show that the current MCP design carries a wide range of security risks for end users. In particular, we demonstrate that industry-leading LLMs may be coerced into using MCP tools to compromise an AI developer's system through various attacks, such as malicious code execution, remote access control, and credential theft. To proactively mitigate these and related attacks, we introduce a safety auditing tool, MCPSafetyScanner, the first agentic tool to assess the security of an arbitrary MCP server. MCPScanner uses several agents to (a) automatically determine adversarial samples given an MCP server's tools and resources; (b) search for related vulnerabilities and remediations based on those samples; and (c) generate a security report detailing all findings. Our work highlights serious security issues with general-purpose agentic workflows while also providing a proactive tool to audit MCP server safety and address detected vulnerabilities before deployment.\nThe described MCP server auditing tool, MCPSafetyScanner, is freely available at: this https URL", 'abstract_zh': '面向潜在组件之间无缝集成以减少开发成本的Model Context Protocol (MCP) (Anthropic, 2024)最近已发布并广泛采用。然而，我们表明当前的MCP设计对终端用户存在广泛的安全风险。特别是，我们展示了一旦受到恶意攻击，如恶意代码执行、远程访问控制和凭证窃取，行业领先的LLM可能被迫利用MCP工具来攻击AI开发者的系统。为此，我们引入了MCPSafetyScanner这一安全审计工具，它是首个评估任意MCP服务器安全性的机构工具。MCPScanner利用多个代理（a）自动确定给定MCP服务器工具和资源的对抗样本；（b）基于这些样本搜索相关漏洞和修复方法；（c）生成详细的安全报告。我们的工作突显了通用机构工作流中的严重安全问题，同时也提供了一个主动工具来审计MCP服务器安全性和在部署前解决检测到的漏洞。', 'title_zh': 'MCP安全审计：具有模型上下文协议的LLM允许重大安全exploits'}
{'arxiv_id': 'arXiv:2504.03739', 'title': 'A Unified Virtual Mixture-of-Experts Framework:Enhanced Inference and Hallucination Mitigation in Single-Model System', 'authors': 'Mingyan Liu', 'link': 'https://arxiv.org/abs/2504.03739', 'abstract': 'Generative models, such as GPT and BERT, have significantly improved performance in tasks like text generation and summarization. However, hallucinations "where models generate non-factual or misleading content" are especially problematic in smaller-scale architectures, limiting their real-world this http URL this paper, we propose a unified Virtual Mixture-of-Experts (MoE) fusion strategy that enhances inference performance and mitigates hallucinations in a single Qwen 1.5 0.5B model without increasing the parameter count. Our method leverages multiple domain-specific expert prompts (with the number of experts being adjustable) to guide the model from different perspectives. We apply a statistical outlier truncation strategy based on the mean and standard deviation to filter out abnormally high probability predictions, and we inject noise into the embedding space to promote output diversity. To clearly assess the contribution of each module, we adopt a fixed voting mechanism rather than a dynamic gating network, thereby avoiding additional confounding factors. We provide detailed theoretical derivations from both statistical and ensemble learning perspectives to demonstrate how our method reduces output variance and suppresses hallucinations. Extensive ablation experiments on dialogue generation tasks show that our approach significantly improves inference accuracy and robustness in small models. Additionally, we discuss methods for evaluating the orthogonality of virtual experts and outline the potential for future work involving dynamic expert weight allocation using gating networks.', 'abstract_zh': '生成模型，如GPT和BERT，在文本生成和总结等任务中显著提升了性能。然而，在较小规模的架构中，模型生成非事实或误导性内容的“幻觉”问题尤其突出，限制了其在实际应用中的表现。在本文中，我们提出了一种统一的虚拟混合专家（MoE）融合策略，能够在不增加参数数量的情况下，增强推断性能并减轻Qwen 1.5 0.5B模型中的幻觉现象。该方法通过调用量化的领域特定专家提示，从多个角度引导模型。我们采用基于均值和标准差的统计异常值修剪策略去除异常高概率预测，并在嵌入空间中注入噪声以促进输出多样性。为了清晰评估每个模块的贡献，我们采用固定投票机制而非动态门控网络，从而避免额外的混杂因素。我们从统计学和集成学习的角度提供了详细的理论推导，展示了该方法如何减少输出方差并抑制幻觉现象。在对话生成任务上的大量消融实验显示，我们的方法在小型模型中的推断准确性和鲁棒性有显著提升。我们还讨论了评估虚拟专家正交性的方法，并概述了未来使用门控网络进行动态专家权重分配的研究潜力。', 'title_zh': '统一虚拟专家混合框架：单模型系统中增强推理和幻觉抑制'}
{'arxiv_id': 'arXiv:2504.03719', 'title': 'Towards Symmetric Low-Rank Adapters', 'authors': 'Tales Panoutsos, Rodrygo L. T. Santos, Flavio Figueiredo', 'link': 'https://arxiv.org/abs/2504.03719', 'abstract': '\\newcommand{\\mathds}[1]{\\text{\\usefont{U}{dsrom}{m}{n}#1}}\nIn this paper, we introduce Symmetric Low-Rank Adapters, an optimized variant of LoRA with even fewer weights. This method utilizes Low-Rank Symmetric Weight Matrices to learn downstream tasks more efficiently. Traditional LoRA accumulates fine-tuning weights with the original pre-trained weights via a Singular Value Decomposition (SVD) like approach, i.e., model weights are fine-tuned via updates of the form $BA$ (where $B \\in \\mathbb{R}^{n\\times r}$, $A \\in \\mathbb{R}^{r\\times n}$, and $r$ is the rank of the merged weight matrix). In contrast, our approach, named SymLoRA, represents fine-tuning weights as a Spectral Decomposition, i.e., $Q \\, diag(\\Lambda)\\, Q^T$, where $Q \\in \\mathbb{R}^{n\\times r}$ and $\\Lambda \\in \\mathbb{R}^r$. SymLoRA requires approximately half of the finetuning weights. Here, we show that this approach has negligible losses in downstream efficacy.', 'abstract_zh': '基于低秩对称矩阵的对称低秩适配器：一种具有更少参数的优化变体', 'title_zh': 'Towards 对称低秩适配器'}
{'arxiv_id': 'arXiv:2504.03718', 'title': 'Task-Aware Parameter-Efficient Fine-Tuning of Large Pre-Trained Models at the Edge', 'authors': 'Senkang Hu, Yanan Ma, Yihang Tao, Zhengru Fang, Zihan Fang, Yiqin Deng, Sam Kwong, Yuguang Fang', 'link': 'https://arxiv.org/abs/2504.03718', 'abstract': "Large language models (LLMs) have achieved remarkable success in various tasks, such as decision-making, reasoning, and question answering. They have been widely used in edge devices. However, fine-tuning LLMs to specific tasks at the edge is challenging due to the high computational cost and the limited storage and energy resources at the edge. To address this issue, we propose TaskEdge, a task-aware parameter-efficient fine-tuning framework at the edge, which allocates the most effective parameters to the target task and only updates the task-specific parameters. Specifically, we first design a parameter importance calculation criterion that incorporates both weights and input activations into the computation of weight importance. Then, we propose a model-agnostic task-specific parameter allocation algorithm to ensure that task-specific parameters are distributed evenly across the model, rather than being concentrated in specific regions. In doing so, TaskEdge can significantly reduce the computational cost and memory usage while maintaining performance on the target downstream tasks by updating less than 0.1\\% of the parameters. In addition, TaskEdge can be easily integrated with structured sparsity to enable acceleration by NVIDIA's specialized sparse tensor cores, and it can be seamlessly integrated with LoRA to enable efficient sparse low-rank adaptation. Extensive experiments on various tasks demonstrate the effectiveness of TaskEdge.", 'abstract_zh': 'TaskEdge：一种面向任务的边缘参数高效微调框架', 'title_zh': '边缘设备上的任务导向参数高效微调大型预训练模型'}
{'arxiv_id': 'arXiv:2504.03717', 'title': 'RaanA: A Fast, Flexible, and Data-Efficient Post-Training Quantization Algorithm', 'authors': 'Yongyi Yang, Jianyang Gao, Wei Hu', 'link': 'https://arxiv.org/abs/2504.03717', 'abstract': "Post-training Quantization (PTQ) has become a widely used technique for improving inference efficiency of large language models (LLMs). However, existing PTQ methods generally suffer from crucial limitations such as heavy calibration data requirements and inflexible choice of target number of bits. In this paper, we propose RaanA, a unified PTQ framework that overcomes these challenges by introducing two novel components: 1) RaBitQ-H, a variant of a randomized vector quantization method RaBitQ, designed for fast, accurate, and highly efficient quantization; and 2) AllocateBits, an algorithm that optimally allocates bit-widths across layers based on their quantization sensitivity. RaanA achieves competitive performance with state-of-the-art quantization methods while being extremely fast, requiring minimal calibration data, and enabling flexible bit allocation. Extensive experiments demonstrate RaanA's efficacy in balancing efficiency and accuracy. The code is publicly available at this https URL .", 'abstract_zh': 'Post-training 量化 (PTQ) 已成为提高大型语言模型 (LLM) 推断效率的一种广泛使用的技术。然而，现有的 PTQ 方法通常存在关键限制，如对校准数据的高要求和目标位数选择的灵活性不足。本文提出了一种名为 RaanA 的统一 PTQ 框架，通过引入两个新型组件来克服这些挑战：1) RaBitQ-H，一种基于随机化向量量化方法 RaBitQ 的变体，旨在实现快速、准确且高效的量化；2) AllocateBits，一种根据各层的量化敏感性优化分配位宽的算法。RaanA 在保持与最先进的量化方法相当的性能的同时，具有极高的速度、最小的校准数据要求，并支持灵活的位宽分配。广泛的实验验证了 RaanA 在效率和精度之间取得的平衡效果。代码可在以下网址获取：this https URL。', 'title_zh': 'RaanA: 一种快速、灵活且数据高效的后训练量化算法'}
{'arxiv_id': 'arXiv:2504.03716', 'title': 'Ethical AI on the Waitlist: Group Fairness Evaluation of LLM-Aided Organ Allocation', 'authors': 'Hannah Murray, Brian Hyeongseok Kim, Isabelle Lee, Jason Byun, Dani Yogatama, Evi Micha', 'link': 'https://arxiv.org/abs/2504.03716', 'abstract': 'Large Language Models (LLMs) are becoming ubiquitous, promising automation even in high-stakes scenarios. However, existing evaluation methods often fall short -- benchmarks saturate, accuracy-based metrics are overly simplistic, and many inherently ambiguous problems lack a clear ground truth. Given these limitations, evaluating fairness becomes complex. To address this, we reframe fairness evaluation using Borda scores, a method from voting theory, as a nuanced yet interpretable metric for measuring fairness. Using organ allocation as a case study, we introduce two tasks: (1) Choose-One and (2) Rank-All. In Choose-One, LLMs select a single candidate for a kidney, and we assess fairness across demographics using proportional parity. In Rank-All, LLMs rank all candidates for a kidney, reflecting real-world allocation processes. Since traditional fairness metrics do not account for ranking, we propose a novel application of Borda scoring to capture biases. Our findings highlight the potential of voting-based metrics to provide a richer, more multifaceted evaluation of LLM fairness.', 'abstract_zh': '大型语言模型（LLMs）正在变得无处不在，在高风险场景中也承诺实现自动化。然而，现有的评估方法往往不尽如人意——评估基准趋于饱和，基于准确性的指标过于简化，而且许多本质上具有模糊性的问题缺乏明确的基准答案。鉴于这些限制，评估公平性变得更复杂。为解决这一问题，我们重新定义公平性评估，使用选举理论中的Borda分数作为衡量公平性的细致且可解释的度量标准。以器官分配为例，我们提出两项任务：（1）单项选择和（2）全部排序。在单项选择任务中，LLMs为肾脏选择一个单一候选人，并使用比例平等来评估不同人口统计学群体的公平性。在全部排序任务中，LLMs对所有候选人进行排名，反映现实中的分配过程。由于传统的公平性指标未能考虑排名，我们提出将Borda评分应用到新领域以捕捉偏差。我们的研究结果凸显了基于选举的指标在提供LLM公平性更加丰富和多维评估方面的潜力。', 'title_zh': '等待列表中的伦理AI：LLM辅助器官分配的团体公平性评价'}
{'arxiv_id': 'arXiv:2504.03714', 'title': 'Breach in the Shield: Unveiling the Vulnerabilities of Large Language Models', 'authors': 'Runpeng Dai, Run Yang, Fan Zhou, Hongtu Zhu', 'link': 'https://arxiv.org/abs/2504.03714', 'abstract': 'Large Language Models (LLMs) and Vision-Language Models (VLMs) have become essential to general artificial intelligence, exhibiting remarkable capabilities in task understanding and problem-solving. However, the real-world reliability of these models critically depends on their stability, which remains an underexplored area. Despite their widespread use, rigorous studies examining the stability of LLMs under various perturbations are still lacking. In this paper, we address this gap by proposing a novel stability measure for LLMs, inspired by statistical methods rooted in information geometry. Our measure possesses desirable invariance properties, making it well-suited for analyzing model sensitivity to both parameter and input perturbations. To assess the effectiveness of our approach, we conduct extensive experiments on models ranging in size from 1.5B to 13B parameters. Our results demonstrate the utility of our measure in identifying salient parameters and detecting vulnerable regions in input images or critical dimensions in token embeddings. Furthermore, leveraging our stability framework, we enhance model robustness during model merging, leading to improved performance.', 'abstract_zh': '大型语言模型（LLMs）和视觉语言模型（VLMs）已成为通用人工智能的关键组成部分，展示了在任务理解和问题解决方面的杰出能力。然而，这些模型在现实世界中的可靠性关键依赖于其稳定性，这一领域仍存在未探索的领域。尽管这些模型被广泛使用，但对各种扰动下LLMs稳定性的严格研究仍然不足。在本文中，我们通过提出一种受信息几何统计方法启发的新颖稳定性度量来填补这一空白。该度量具有期望的不变性特性，使其适用于分析模型对参数和输入扰动的敏感性。为了评估我们方法的有效性，我们在从1.5B到13B参数不等的模型上进行了广泛实验。我们的结果表明了该度量在识别关键参数和检测输入图像中的脆弱区域或令牌嵌入中的关键维度方面的实用性。此外，利用我们的稳定性框架，我们在模型合并中增强了模型的稳健性，从而提高了性能。', 'title_zh': '屏蔽失效：大型语言模型的漏洞揭示'}
{'arxiv_id': 'arXiv:2504.03713', 'title': 'RLDBF: Enhancing LLMs Via Reinforcement Learning With DataBase FeedBack', 'authors': 'Weichen Dai, Zijie Dai, Zhijie Huang, Yixuan Pan, Xinhe Li, Xi Li, Yi Zhou, Ji Qi, Wu Jiang', 'link': 'https://arxiv.org/abs/2504.03713', 'abstract': 'While current large language models (LLMs) demonstrate remarkable linguistic capabilities through training on massive unstructured text corpora, they remain inadequate in leveraging structured scientific data (e.g., chemical molecular properties in databases) that encapsulate centuries of accumulated scientific expertise. These structured datasets hold strategic significance for advancing AI for Science yet current approaches merely treat them as auxiliary supplements to unstructured text. This study pioneers a systematic investigation into enhancing LLMs with structured scientific data, using chemical molecular science as a testbed. We investigate the impact of incorporating molecular property data on LLM across distinct training phases, including continual pre-training, supervised fine-tuning, and reinforcement learning. Notably, to address the inherent limitation of numerical insensitivity in large models, we propose an innovative methodology termed "Reinforcement Learning with Database Feedback" (RLDBF). Experimental evaluations demonstrate the efficacy of the proposed approach, with the model exhibiting remarkable generalization capabilities on previously unseen data and other chemical tasks. The results substantiate the potential of our method in advancing the field of structured scientific data processing within LLMs.', 'abstract_zh': '尽管当前的大规模语言模型（LLMs）通过训练大量未结构化文本 corpora 展示出卓越的语言能力，但在利用蕴含百年科学经验的结构化科学数据（例如数据库中的化学分子性质）方面仍显不足。这些结构化数据对推动人工智能科学具有战略意义，但当前的方法仅仅将它们视为未结构化文本的辅助补充。本研究首次系统性地探讨了结合结构化科学数据以增强 LLMs 的可能性，以化学分子科学作为试验场。我们研究了将分子性质数据纳入 LLM 在不同训练阶段的影响，包括持续预训练、监督微调和强化学习。值得注意的是，为解决大模型固有的数值敏感性不足问题，我们提出了一种名为“数据库反馈强化学习”（RLDBF）的创新方法。实验评估表明，所提出的方法具有显著效果，模型在未见过的数据和其他化学任务上表现出卓越的一般化能力。研究结果证明了该方法在促进 LLMs 中结构化科学数据处理领域的潜力。', 'title_zh': 'RLDBF: 通过数据库反馈增强LLMs的强化学习方法'}
{'arxiv_id': 'arXiv:2504.03665', 'title': "LLM & HPC:Benchmarking DeepSeek's Performance in High-Performance Computing Tasks", 'authors': 'Noujoud Nader, Patrick Diehl, Steve Brandt, Hartmut Kaiser', 'link': 'https://arxiv.org/abs/2504.03665', 'abstract': "Large Language Models (LLMs), such as GPT-4 and DeepSeek, have been applied to a wide range of domains in software engineering. However, their potential in the context of High-Performance Computing (HPC) much remains to be explored. This paper evaluates how well DeepSeek, a recent LLM, performs in generating a set of HPC benchmark codes: a conjugate gradient solver, the parallel heat equation, parallel matrix multiplication, DGEMM, and the STREAM triad operation. We analyze DeepSeek's code generation capabilities for traditional HPC languages like Cpp, Fortran, Julia and Python. The evaluation includes testing for code correctness, performance, and scaling across different configurations and matrix sizes. We also provide a detailed comparison between DeepSeek and another widely used tool: GPT-4. Our results demonstrate that while DeepSeek generates functional code for HPC tasks, it lags behind GPT-4, in terms of scalability and execution efficiency of the generated code.", 'abstract_zh': '大规模语言模型（LLMs）如GPT-4和DeepSeek已在软件工程的多个领域得到应用。然而，在高性能计算（HPC）领域中的潜力仍待进一步探索。本文评估了最近的LLM DeepSeek在生成HPC基准代码方面的表现：共轭梯度求解器、并行热方程、并行矩阵乘法、DGEMM以及STREAM三元操作。我们分析了DeepSeek在C++、Fortran、Julia和Python等传统HPC语言的代码生成能力。评估包括对代码正确性、性能以及在不同配置和矩阵大小下的扩展性的测试。我们还提供了DeepSeek与另一个广泛使用的工具GPT-4之间的详细对比。我们的结果表明，虽然DeepSeek能够生成用于HPC任务的功能性代码，但在生成代码的可扩展性和执行效率方面仍落后于GPT-4。', 'title_zh': 'LLM & HPC：评估DeepSeek在高性能计算任务中的性能'}
{'arxiv_id': 'arXiv:2504.03664', 'title': 'PIPO: Pipelined Offloading for Efficient Inference on Consumer Devices', 'authors': 'Yangyijian Liu, Jun Li, Wu-Jun Li', 'link': 'https://arxiv.org/abs/2504.03664', 'abstract': 'The high memory and computation demand of large language models (LLMs) makes them challenging to be deployed on consumer devices due to limited GPU memory. Offloading can mitigate the memory constraint but often suffers from low GPU utilization, leading to low inference efficiency. In this work, we propose a novel framework, called pipelined offloading (PIPO), for efficient inference on consumer devices. PIPO designs a fine-grained offloading pipeline, complemented with optimized data transfer and computation, to achieve high concurrency and efficient scheduling for inference. Experimental results show that compared with state-of-the-art baseline, PIPO increases GPU utilization from below 40% to over 90% and achieves up to 3.1$\\times$ higher throughput, running on a laptop equipped with a RTX3060 GPU of 6GB memory.', 'abstract_zh': '大规模语言模型（LLMs）的高内存和计算需求使其难以在消费级设备上部署，受限于有限的GPU内存。卸载可以缓解内存限制，但往往导致低GPU利用率，从而影响推理效率。在此工作中，我们提出了一种新的框架，称为流水线卸载（PIPO），旨在高效地在消费级设备上进行推理。PIPO设计了细粒度的卸载流水线，并结合优化的数据传输和计算，以实现高并发和高效的推理调度。实验结果表明，与最先进的基线相比，PIPO将GPU利用率从不到40%提高到超过90%，并在配备6GB显存的RTX3060 GPU的笔记本电脑上实现了高达3.1倍的更高吞吐量。', 'title_zh': 'PIPO：面向消费者设备高效推理的流水线卸载方法'}
{'arxiv_id': 'arXiv:2504.03651', 'title': 'Echo: Efficient Co-Scheduling of Hybrid Online-Offline Tasks for Large Language Model Serving', 'authors': 'Zhibin Wang, Shipeng Li, Xue Li, Yuhang Zhou, Zhonghui Zhang, Zibo Wang, Rong Gu, Chen Tian, Kun Yang, Sheng Zhong', 'link': 'https://arxiv.org/abs/2504.03651', 'abstract': 'Large language models have been widely deployed in various applications, encompassing both interactive online tasks and batched offline tasks. Given the burstiness and latency sensitivity of online tasks, over-provisioning resources is common practice. This allows for the integration of latency-insensitive offline tasks during periods of low online load, enhancing resource utilization. However, strategically serving online and offline tasks through a preemption mechanism fails to fully leverage the flexibility of offline tasks and suffers from KV cache recomputation and irregular workloads.\nIn this paper, we introduce Echo, a collaborative online-offline task serving system, including a scheduler, a KV cache manager, and estimation toolkits. The scheduler and KV cache manager work tightly to maximize the throughput of offline tasks, while the estimator further predicts execution time to ensure online task SLOs. The scheduler leverages the batch information of last iteration to reduce the search space for finding the optimal schedule. The KV cache manager sets the priority of the KV cache based on the type of tasks and the opportunity of prefix sharing to reduce the recomputation. Finally, the estimation toolkits predict the execution time, future memory consumption, and the throughput of offline tasks to guide the scheduler, KV cache manager, and the system deployer. Evaluation based on real-world workloads demonstrates that Echo can increase offline task throughput by up to $3.3\\times$, while satisfying online task SLOs.', 'abstract_zh': '一种协作的线上线下任务服务系统Echo：调度器、KV缓存管理器及估算工具箱', 'title_zh': 'Echo: 效率兼优的混合在线-离线任务协同调度方法用于大型语言模型服务'}
{'arxiv_id': 'arXiv:2504.03648', 'title': 'AIBrix: Towards Scalable, Cost-Effective Large Language Model Inference Infrastructure', 'authors': 'AIBrix Team, Jiaxin Shan, Varun Gupta, Le Xu, Haiyang Shi, Jingyuan Zhang, Ning Wang, Linhui Xu, Rong Kang, Tongping Liu, Yifei Zhang, Yiqing Zhu, Shuowei Jin, Gangmuk Lim, Binbin Chen, Zuzhi Chen, Xiao Liu, Xin Chen, Kante Yin, Chak-Pong Chung, Chenyu Jiang, Yicheng Lu, Jianjun Chen, Caixue Lin, Wu Xiang, Rui Shi, Liguang Xie', 'link': 'https://arxiv.org/abs/2504.03648', 'abstract': 'We introduce AIBrix, a cloud-native, open-source framework designed to optimize and simplify large-scale LLM deployment in cloud environments. Unlike traditional cloud-native stacks, AIBrix follows a co-design philosophy, ensuring every layer of the infrastructure is purpose-built for seamless integration with inference engines like vLLM. AIBrix introduces several key innovations to reduce inference costs and enhance performance including high-density LoRA management for dynamic adapter scheduling, LLM-specific autoscalers, and prefix-aware, load-aware routing. To further improve efficiency, AIBrix incorporates a distributed KV cache, boosting token reuse across nodes, leading to a 50% increase in throughput and a 70% reduction in inference latency. AIBrix also supports unified AI runtime which streamlines model management while maintaining vendor-agnostic engine compatibility. For large-scale multi-node inference, AIBrix employs hybrid orchestration -- leveraging Kubernetes for coarse-grained scheduling and Ray for fine-grained execution -- to balance efficiency and flexibility. Additionally, an SLO-driven GPU optimizer dynamically adjusts resource allocations, optimizing heterogeneous serving to maximize cost efficiency while maintaining service guarantees. Finally, AIBrix enhances system reliability with AI accelerator diagnostic tools, enabling automated failure detection and mock-up testing to improve fault resilience. AIBrix is available at this https URL.', 'abstract_zh': '我们介绍AIBrix，一个面向云环境的大规模LLM部署优化和简化的设计理念，AIBrix是一个云原生、开源框架。不同于传统的云原生堆栈，AIBrix遵循共同设计哲学，确保每层基础设施都为无缝集成与推理引擎（如vLLM）构建。AIBrix引入了几项关键创新以减少推理成本并提升性能，包括动态适配器调度的高密度LoRA管理、特定于LLM的自动化扩容器以及前缀感知、负载感知路由。为了进一步提高效率，AIBrix集成了一个分布式KV缓存，提高节点间token重用，从而将吞吐量提高50%，推理延迟减少70%。AIBrix还支持统一AI运行时，简化模型管理同时保持对供应商无关引擎的兼容性。对于大规模多节点推理，AIBrix采用混合编排——利用Kubernetes进行粗粒度调度和利用Ray进行细粒度执行，以平衡效率和灵活性。此外，一个基于SLA的GPU优化器动态调整资源分配，优化异构服务以实现成本效率最大化并维持服务质量保证。最后，AIBrix通过AI加速器诊断工具增强系统可靠性，实现自动化故障检测和模拟测试以提高故障容忍度。AIBrix可在以下链接获取：此httpsURL。', 'title_zh': 'AIBrix: 向scalable、cost-effective的大型语言模型推理基础设施方向努力'}
{'arxiv_id': 'arXiv:2503.22688', 'title': 'CodeIF-Bench: Evaluating Instruction-Following Capabilities of Large Language Models in Interactive Code Generation', 'authors': 'Peiding Wang, Li Zhang, Fang Liu, Lin Shi, Minxiao Li, Bo Shen, An Fu', 'link': 'https://arxiv.org/abs/2503.22688', 'abstract': "Large Language Models (LLMs) have demonstrated exceptional performance in code generation tasks and have become indispensable programming assistants for developers. However, existing code generation benchmarks primarily assess the functional correctness of code generated by LLMs in single-turn interactions, offering limited insight into their capabilities to generate code that strictly follows users' instructions, especially in multi-turn interaction scenarios. In this paper, we introduce \\bench, a benchmark for evaluating LLMs' instruction-following capabilities in interactive code generation. Specifically, \\bench incorporates nine types of verifiable instructions aligned with the real-world software development requirements, which can be independently and objectively validated through specified test cases, facilitating the evaluation of instruction-following capability in multi-turn interactions. We evaluate nine prominent LLMs using \\bench, and the experimental results reveal a significant disparity between their basic programming capability and instruction-following capability, particularly as task complexity, context length, and the number of dialogue rounds increase.", 'abstract_zh': 'Large Language Models (LLMs)在代码生成任务中表现出色，已成为开发人员不可或缺的编程助手。然而，现有的代码生成基准主要评估LLMs在单轮交互中生成的代码的功能正确性，对于它们生成严格遵循用户指令的代码的能力提供有限的洞察，尤其是在多轮交互场景中。本文介绍了一个新的基准\\bench，用于评估LLMs在交互式代码生成中的指令遵循能力。\\bench包含九种与实际软件开发需求对齐的可验证指令，可以通过指定的测试用例独立且客观地验证，从而促进多轮交互中的指令遵循能力评估。我们使用\\bench评估了九种 prominant LLMs，并且实验结果揭示了它们的基本编程能力和指令遵循能力之间存在显著差异，特别是在任务复杂度、上下文长度和对话轮次增加的情况下。', 'title_zh': 'CodeIF-基准：评估大型语言模型在交互式代码生成中的指令遵循能力'}
