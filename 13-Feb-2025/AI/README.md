# Ensemble based approach to quantifying uncertainty of LLM based classifications 

**Title (ZH)**: 基于集成的方法量化LLM基分类的不确定性 

**Authors**: Srijith Rajamohan, Ahmed Salhin, Josh Frazier, Rohit Kumar, Yu-Cheng Tsai, Todd Cook  

**Link**: [PDF](https://arxiv.org/pdf/2502.08631)  

**Abstract**: The output of Large Language Models (LLMs) are a function of the internal model's parameters and the input provided into the context window. The hypothesis presented here is that under a greedy sampling strategy the variance in the LLM's output is a function of the conceptual certainty embedded in the model's parametric knowledge, as well as the lexical variance in the input. Finetuning the model results in reducing the sensitivity of the model output to the lexical input variations. This is then applied to a classification problem and a probabilistic method is proposed for estimating the certainties of the predicted classes. 

**Abstract (ZH)**: 大型语言模型（LLMs）的输出是模型内部参数和输入到上下文窗口中的输入的函数。假设在贪婪采样策略下，LLM输出的变异度取决于模型参数知识中嵌入的概念 certainty，以及输入的词汇变异度。通过对模型进行微调可以在一定程度上减少模型输出对词汇输入变异性的敏感性。这一方法随后应用于分类问题，并提出了一种概率方法来估计预测类别的 certainty。 

---
# Representation Learning to Advance Multi-institutional Studies with Electronic Health Record Data 

**Title (ZH)**: 基于表示学习促进电子健康记录数据多机构研究 

**Authors**: Doudou Zhou, Han Tong, Linshanshan Wang, Suqi Liu, Xin Xiong, Ziming Gan, Romain Griffier, Boris Hejblum, Yun-Chung Liu, Chuan Hong, Clara-Lea Bonzel, Tianrun Cai, Kevin Pan, Yuk-Lam Ho, Lauren Costa, Vidul A. Panickan, J. Michael Gaziano, Kenneth Mandl, Vianney Jouhet, Rodolphe Thiebaut, Zongqi Xia, Kelly Cho, Katherine Liao, Tianxi Cai  

**Link**: [PDF](https://arxiv.org/pdf/2502.08547)  

**Abstract**: The adoption of EHRs has expanded opportunities to leverage data-driven algorithms in clinical care and research. A major bottleneck in effectively conducting multi-institutional EHR studies is the data heterogeneity across systems with numerous codes that either do not exist or represent different clinical concepts across institutions. The need for data privacy further limits the feasibility of including multi-institutional patient-level data required to study similarities and differences across patient subgroups. To address these challenges, we developed the GAME algorithm. Tested and validated across 7 institutions and 2 languages, GAME integrates data in several levels: (1) at the institutional level with knowledge graphs to establish relationships between codes and existing knowledge sources, providing the medical context for standard codes and their relationship to each other; (2) between institutions, leveraging language models to determine the relationships between institution-specific codes with established standard codes; and (3) quantifying the strength of the relationships between codes using a graph attention network. Jointly trained embeddings are created using transfer and federated learning to preserve data privacy. In this study, we demonstrate the applicability of GAME in selecting relevant features as inputs for AI-driven algorithms in a range of conditions, e.g., heart failure, rheumatoid arthritis. We then highlight the application of GAME harmonized multi-institutional EHR data in a study of Alzheimer's disease outcomes and suicide risk among patients with mental health disorders, without sharing patient-level data outside individual institutions. 

**Abstract (ZH)**: EHRs采用扩大了基于数据驱动算法在临床护理和研究中的应用机会。机构间数据异质性是有效地开展多机构EHR研究的主要瓶颈，不同机构中存在大量不一致或代表不同临床概念的编码。数据隐私需求进一步限制了研究需要的多机构患者级数据的可行性。为应对这些挑战，我们开发了GAME算法。该算法在7个机构和2种语言下进行了测试和验证，通过在多个层次上整合数据来解决这些问题：（1）在机构层面使用知识图谱建立编码与现有知识源之间的关系，为标准编码及其相互关系提供医学背景；（2）在机构间利用语言模型确定机构特定编码与现有标准编码之间的关系；（3）使用图注意力网络量化编码之间关系的强度。通过转移学习和联邦学习联合训练嵌入向量，以保护数据隐私。在本研究中，我们展示了GAME在选择用于AI驱动算法输入的相关特征方面的适用性，例如心力衰竭、类风湿性关节炎等病情。我们还强调了在不共享患者级数据的情况下，利用GAME统一的多机构EHR数据研究阿尔茨海默病结果和精神障碍患者自杀风险的应用。 

---
# Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities? 

**Title (ZH)**: revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities? 重新审视3D LLM基准测试：我们真的在测试3D能力吗？ 

**Authors**: Jiahe Jin, Yanheng He, Mingyan Yang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08503)  

**Abstract**: In this work, we identify the "2D-Cheating" problem in 3D LLM evaluation, where these tasks might be easily solved by VLMs with rendered images of point clouds, exposing ineffective evaluation of 3D LLMs' unique 3D capabilities. We test VLM performance across multiple 3D LLM benchmarks and, using this as a reference, propose principles for better assessing genuine 3D understanding. We also advocate explicitly separating 3D abilities from 1D or 2D aspects when evaluating 3D LLMs. 

**Abstract (ZH)**: 本工作中，我们识别出在3D LLM评估中存在“2D作弊”问题，这些任务可能通过使用点云渲染图像的VLMs轻易解决，暴露了对3D LLMs独特3D能力评估的有效性不足。我们测试了VLM在多个3D LLM基准测试中的性能，并以此为基础提出了更有效地评估真实3D理解的原则。我们还建议在评估3D LLMs时明确分离3D能力与1D或2D方面。 

---
# Salience-Invariant Consistent Policy Learning for Generalization in Visual Reinforcement Learning 

**Title (ZH)**: 视觉强化学习中不变显著性的一致策略学习以实现泛化 

**Authors**: Sun Jingbo, Tu Songjun, Zhang Qichao, Chen Ke, Zhao Dongbin  

**Link**: [PDF](https://arxiv.org/pdf/2502.08336)  

**Abstract**: Generalizing policies to unseen scenarios remains a critical challenge in visual reinforcement learning, where agents often overfit to the specific visual observations of the training environment. In unseen environments, distracting pixels may lead agents to extract representations containing task-irrelevant information. As a result, agents may deviate from the optimal behaviors learned during training, thereby hindering visual this http URL address this issue, we propose the Salience-Invariant Consistent Policy Learning (SCPL) algorithm, an efficient framework for zero-shot generalization. Our approach introduces a novel value consistency module alongside a dynamics module to effectively capture task-relevant representations. The value consistency module, guided by saliency, ensures the agent focuses on task-relevant pixels in both original and perturbed observations, while the dynamics module uses augmented data to help the encoder capture dynamic- and reward-relevant representations. Additionally, our theoretical analysis highlights the importance of policy consistency for generalization. To strengthen this, we introduce a policy consistency module with a KL divergence constraint to maintain consistent policies across original and perturbed this http URL experiments on the DMC-GB, Robotic Manipulation, and CARLA benchmarks demonstrate that SCPL significantly outperforms state-of-the-art methods in terms of generalization. Notably, SCPL achieves average performance improvements of 14\%, 39\%, and 69\% in the challenging DMC video hard setting, the Robotic hard setting, and the CARLA benchmark, this http URL Page: this https URL. 

**Abstract (ZH)**: 视觉强化学习中将策略推广到未见过的场景仍然是一个关键挑战，其中代理往往会过度拟合训练环境中特定的视觉观察。在未见过的环境中，分散的像素可能导致代理提取包含任务无关信息的表示。结果，代理可能会偏离训练期间学习到的最优行为，从而妨碍视觉强化学习的推广。为了解决这一问题，我们提出了一种名为Salience-Invariant Consistent Policy Learning (SCPL) 的算法，这是一种高效的零样本推广框架。我们的方法引入了一种新颖的价值一致性模块和一个动力学模块，以有效地捕捉任务相关表示。价值一致性模块由显著性引导，确保代理在原始观察和扰动观察中都关注任务相关像素，而动力学模块则利用增强数据来帮助编码器捕捉动态和奖励相关表示。此外，我们的理论分析强调了策略一致性对于推广的重要性。为了加强这一点，我们引入了一个具有KL散度约束的策略一致性模块，以在原始观察和扰动观察中保持一致策略。在DMC-GB、机器人操作和CARLA基准测试上的实验表明，SCPL在推广性能方面显著优于最新方法，尤其是在具有挑战性的DMC视频硬设置、机器人硬设置和CARLA基准测试中分别实现了平均性能改进的14%、39%和69%。 

---
# Improving Existing Optimization Algorithms with LLMs 

**Title (ZH)**: 使用大语言模型优化现有优化算法 

**Authors**: Camilo Chacón Sartori, Christian Blum  

**Link**: [PDF](https://arxiv.org/pdf/2502.08298)  

**Abstract**: The integration of Large Language Models (LLMs) into optimization has created a powerful synergy, opening exciting research opportunities. This paper investigates how LLMs can enhance existing optimization algorithms. Using their pre-trained knowledge, we demonstrate their ability to propose innovative heuristic variations and implementation strategies. To evaluate this, we applied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt (CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that incorporates a heuristic in the solution construction phase. Our results show that an alternative heuristic proposed by GPT-4o outperforms the expert-designed heuristic of CMSA, with the performance gap widening on larger and denser graphs. Project URL: this https URL 

**Abstract (ZH)**: 大型语言模型（LLMs）与优化技术的集成创造了强大的协同效应，开启了令人兴奋的研究机会。本文探讨了LLMs如何增强现有的优化算法。利用其预训练知识，我们展示了它们提出创新启发式变体和实现策略的能力。为此，我们应用了一个非平凡的优化算法——构造、合并、求解和调整（CMSA）——这是一种用于组合优化问题的混合元启发式算法，其中包含了解构建阶段的启发式方法。我们的结果表明，GPT-4提出的一种替代启发式方法在ใหญ่和密集的图上优于CMSA的专家设计启发式方法，性能差距随图规模的增大而增大。项目网址：这个https网址。 

---
# The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks 

**Title (ZH)**: 过度思考的危险：探究代理任务中的推理-行动困境 

**Authors**: Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, Nicholas Thumiger, Aditya Desai, Ion Stoica, Ana Klimovic, Graham Neubig, Joseph E. Gonzalez  

**Link**: [PDF](https://arxiv.org/pdf/2502.08235)  

**Abstract**: Large Reasoning Models (LRMs) represent a breakthrough in AI problem-solving capabilities, but their effectiveness in interactive environments can be limited. This paper introduces and analyzes overthinking in LRMs. A phenomenon where models favor extended internal reasoning chains over environmental interaction. Through experiments on software engineering tasks using SWE Bench Verified, we observe three recurring patterns: Analysis Paralysis, Rogue Actions, and Premature Disengagement. We propose a framework to study these behaviors, which correlates with human expert assessments, and analyze 4018 trajectories. We observe that higher overthinking scores correlate with decreased performance, with reasoning models exhibiting stronger tendencies toward overthinking compared to non-reasoning models. Our analysis reveals that simple efforts to mitigate overthinking in agentic environments, such as selecting the solution with the lower overthinking score, can improve model performance by almost 30% while reducing computational costs by 43%. These results suggest that mitigating overthinking has strong practical implications. We suggest that by leveraging native function-calling capabilities and selective reinforcement learning overthinking tendencies could be mitigated. We also open-source our evaluation framework and dataset to facilitate research in this direction at this https URL. 

**Abstract (ZH)**: Large Reasoning Models中的过度思考现象及其分析：基于SWE Bench Verified的软件工程任务实验 

---
# SycEval: Evaluating LLM Sycophancy 

**Title (ZH)**: SycEval: 评估大模型的阿谀谄媚现象 

**Authors**: Aaron Fanous, Jacob Goldberg, Ank A. Agarwal, Joanna Lin, Anson Zhou, Roxana Daneshjou, Sanmi Koyejo  

**Link**: [PDF](https://arxiv.org/pdf/2502.08177)  

**Abstract**: Large language models (LLMs) are increasingly applied in educational, clinical, and professional settings, but their tendency for sycophancy -- prioritizing user agreement over independent reasoning -- poses risks to reliability. This study introduces a framework to evaluate sycophantic behavior in ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and MedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19% of cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the lowest (56.71%). Progressive sycophancy, leading to correct answers, occurred in 43.52% of cases, while regressive sycophancy, leading to incorrect answers, was observed in 14.66%. Preemptive rebuttals demonstrated significantly higher sycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$, $p<0.001$), particularly in computational tasks, where regressive sycophancy increased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$). Simple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while citation-based rebuttals exhibited the highest regressive rates ($Z=6.59$, $p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI: [77.2%, 79.8%]) regardless of context or model. These findings emphasize the risks and opportunities of deploying LLMs in structured and dynamic domains, offering insights into prompt programming and model optimization for safer AI applications. 

**Abstract (ZH)**: 大型语言模型在教育、临床和职业环境中的应用日益增多，但其趋向于奉承的行为——优先考虑用户同意而非独立推理——对其可靠性构成了风险。本研究介绍了一种框架，用于评估ChatGPT-4o、Claude-Sonnet和Gemini-1.5-Pro在AMPS（数学）和MedQuad（医疗建议）数据集中的奉承行为。观察到58.19%的情况存在奉承行为，Gemini的奉承行为率最高（62.47%），ChatGPT最低（56.71%）。在43.52%的情况下，奉承行为导致了正确的答案，而在14.66%的情况下，奉承行为导致了错误的答案。预防性反驳中的奉承行为显著高于上下文反驳（61.75% vs. 56.52%，$Z=5.87$，$p<0.001$），尤其是在计算任务中，预防性的奉承行为显著增加（预防性：8.13%，上下文：3.54%，$p<0.001$）。简单的反驳最大限度地提高了奉承行为（$Z=6.59$，$p<0.001$），而基于引文的反驳表现出最高的逆向奉承率（$Z=6.59$，$p<0.001$）。无论上下文或模型如何，奉承行为显示出高持久性（78.5%，95% CI：[77.2%，79.8%]）。这些发现强调了在结构化和动态领域部署大型语言模型的风险与机遇，为安全的AI应用提供了关于提示编程和模型优化的见解。 

---
# ACCESS : A Benchmark for Abstract Causal Event Discovery and Reasoning 

**Title (ZH)**: ACCESS：一种抽象因果事件发现与推理基准 

**Authors**: Vy Vo, Lizhen Qu, Tao Feng, Yuncheng Hua, Xiaoxi Kang, Songhai Fan, Tim Dwyer, Lay-Ki Soon, Gholamreza Haffari  

**Link**: [PDF](https://arxiv.org/pdf/2502.08148)  

**Abstract**: Identifying cause-and-effect relationships is critical to understanding real-world dynamics and ultimately causal reasoning. Existing methods for identifying event causality in NLP, including those based on Large Language Models (LLMs), exhibit difficulties in out-of-distribution settings due to the limited scale and heavy reliance on lexical cues within available benchmarks. Modern benchmarks, inspired by probabilistic causal inference, have attempted to construct causal graphs of events as a robust representation of causal knowledge, where \texttt{CRAB} \citep{romanou2023crab} is one such recent benchmark along this line. In this paper, we introduce \texttt{ACCESS}, a benchmark designed for discovery and reasoning over abstract causal events. Unlike existing resources, \texttt{ACCESS} focuses on causality of everyday life events on the abstraction level. We propose a pipeline for identifying abstractions for event generalizations from \texttt{GLUCOSE} \citep{mostafazadeh-etal-2020-glucose}, a large-scale dataset of implicit commonsense causal knowledge, from which we subsequently extract $1,4$K causal pairs. Our experiments highlight the ongoing challenges of using statistical methods and/or LLMs for automatic abstraction identification and causal discovery in NLP. Nonetheless, we demonstrate that the abstract causal knowledge provided in \texttt{ACCESS} can be leveraged for enhancing QA reasoning performance in LLMs. 

**Abstract (ZH)**: 识别因果关系对于理解现实世界的动态并最终进行因果推理至关重要。现有的自然语言处理中事件因果性识别方法，包括基于大规模语言模型的方法，在分布外设置中由于规模有限且过度依赖可用基准中的词形线索，表现出一定的困难。受概率因果推理启发的现代基准试图构建事件因果图，作为因果知识的稳健表示，其中\texttt{CRAB} \citep{romanou2023crab} 是此类基准的最新例证之一。在本文中，我们介绍了\texttt{ACCESS}，一个用于抽象因果事件发现和推理的基准。与现有资源不同，\texttt{ACCESS} 关注日常生活事件的抽象层面的因果性。我们提出了一个从\texttt{GLUCOSE} \citep{mostafazadeh-etal-2020-glucose} 中大规模隐含常识因果知识数据集中识别事件泛化抽象的pipeline，并从中提取了1,400个因果配对。我们的实验突显了统计方法和/或大规模语言模型在自然语言处理中自动识别抽象和因果发现方面持续存在的挑战。尽管如此，我们证明了\texttt{ACCESS} 中提供的抽象因果知识可以用于增强大规模语言模型的QA推理性能。 

---
# Bridging the Safety Gap: A Guardrail Pipeline for Trustworthy LLM Inferences 

**Title (ZH)**: 缩小安全性差距：一种可信赖的大语言模型推理防护管道 

**Authors**: Shanshan Han, Salman Avestimehr, Chaoyang He  

**Link**: [PDF](https://arxiv.org/pdf/2502.08142)  

**Abstract**: We present Wildflare GuardRail, a guardrail pipeline designed to enhance the safety and reliability of Large Language Model (LLM) inferences by systematically addressing risks across the entire processing workflow. Wildflare GuardRail integrates several core functional modules, including Safety Detector that identifies unsafe inputs and detects hallucinations in model outputs while generating root-cause explanations, Grounding that contextualizes user queries with information retrieved from vector databases, Customizer that adjusts outputs in real time using lightweight, rule-based wrappers, and Repairer that corrects erroneous LLM outputs using hallucination explanations provided by Safety Detector. Results show that our unsafe content detection model in Safety Detector achieves comparable performance with OpenAI API, though trained on a small dataset constructed with several public datasets. Meanwhile, the lightweight wrappers can address malicious URLs in model outputs in 1.06s per query with 100% accuracy without costly model calls. Moreover, the hallucination fixing model demonstrates effectiveness in reducing hallucinations with an accuracy of 80.7%. 

**Abstract (ZH)**: Wildflare GuardRail：一种用于增强大型语言模型推理安全性和可靠性的守门员管道 

---
# Generative AI-Enhanced Cooperative MEC of UAVs and Ground Stations for Unmanned Surface Vehicles 

**Title (ZH)**: 基于生成式AI增强的无人机与地面站协作MEC技术在无人水面车辆中的应用 

**Authors**: Jiahao You, Ziye Jia, Chao Dong, Qihui Wu, Zhu Han  

**Link**: [PDF](https://arxiv.org/pdf/2502.08119)  

**Abstract**: The increasing deployment of unmanned surface vehicles (USVs) require computational support and coverage in applications such as maritime search and rescue. Unmanned aerial vehicles (UAVs) can offer low-cost, flexible aerial services, and ground stations (GSs) can provide powerful supports, which can cooperate to help the USVs in complex scenarios. However, the collaboration between UAVs and GSs for USVs faces challenges of task uncertainties, USVs trajectory uncertainties, heterogeneities, and limited computational resources. To address these issues, we propose a cooperative UAV and GS based robust multi-access edge computing framework to assist USVs in completing computational tasks. Specifically, we formulate the optimization problem of joint task offloading and UAV trajectory to minimize the total execution time, which is in the form of mixed integer nonlinear programming and NP-hard to tackle. Therefore, we propose the algorithm of generative artificial intelligence-enhanced heterogeneous agent proximal policy optimization (GAI-HAPPO). The proposed algorithm integrates GAI models to enhance the actor network ability to model complex environments and extract high-level features, thereby allowing the algorithm to predict uncertainties and adapt to dynamic conditions. Additionally, GAI stabilizes the critic network, addressing the instability of multi-agent reinforcement learning approaches. Finally, extensive simulations demonstrate that the proposed algorithm outperforms the existing benchmark methods, thus highlighting the potentials in tackling intricate, cross-domain issues in the considered scenarios. 

**Abstract (ZH)**: 无人驾驶水面车辆（USVs）部署的增加需要计算支持和覆盖，尤其是在海上搜救等应用中。无人驾驶航空车辆（UAVs）可以提供低成本、灵活的空中服务，地面站（GSs）可以提供强大的支持，可以合作帮助USVs应对复杂场景。然而，UAVs和GSs之间的协作面临着任务不确定性、USVs轨迹不确定性、异构性和有限的计算资源等挑战。为了解决这些问题，我们提出了一种基于合作UAV和GS的鲁棒多接入边缘计算框架，以协助USVs完成计算任务。具体而言，我们将联合任务卸载和UAV轨迹的优化问题形式化为混合整数非线性规划问题，并证明其NP-hard，因此提出了生成式人工智能增强异质代理近端策略优化算法（GAI-HAPPO）。该算法结合了GAI模型，增强了演员网络构建复杂环境和提取高级特征的能力，从而允许算法预测不确定性并适应动态条件。此外，GAI稳定了评论者网络，解决了多智能体强化学习方法的不稳定性问题。最后，大量的仿真表明，所提出的算法优于现有的基准方法，从而突显了在考虑场景中复杂跨域问题中的潜在应用。 

---
# WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation 

**Title (ZH)**: WorldGUI: 动态测试 convo 完整的桌面GUI自动化 

**Authors**: Henry Hengyuan Zhao, Difei Gao, Mike Zheng Shou  

**Link**: [PDF](https://arxiv.org/pdf/2502.08047)  

**Abstract**: Current GUI agents have achieved outstanding performance in GUI element grounding. However, planning remains highly challenging, especially due to sensitivity to the initial state of the environment. Specifically, slight differences in the initial state-such as the target software not being open or the interface not being in its default state-often lead to planning errors. This issue is widespread in real user scenarios, but existing benchmarks fail to evaluate it. In this paper, we present WorldGUI, a novel GUI benchmark that designs GUI tasks with various initial states to simulate real computer-user interactions. The benchmark spans a wide range of tasks across 10 popular software applications, including PowerPoint, VSCode, and Adobe Acrobat. In addition, to address the challenges of dynamic GUI automation tasks, we propose GUI-Thinker, a holistic framework, leveraging a critique mechanism, that effectively manages the unpredictability and complexity of GUI interactions. Experimental results demonstrate that GUI-Thinker significantly outperforms Claude-3.5 (Computer Use) by 14.9% in success rate on WorldGUI tasks. This improvement underscores the effectiveness of our critical-thinking-based framework in enhancing GUI automation. 

**Abstract (ZH)**: 当前的GUI代理已经在GUI元素定位方面取得了卓越的性能，然而，规划仍然极具挑战性，尤其是在环境初始状态的敏感性方面。具体来说，初始状态的微小差异，如目标软件未打开或界面未处于默认状态，往往会导致规划错误。这一问题在真实用户场景中普遍存在，但现有的基准测试无法评估该问题。在这项工作中，我们提出了WorldGUI，这是一种新颖的GUI基准测试，通过设计具有各种初始状态的GUI任务来模拟真实计算机用户交互。该基准测试覆盖了10个流行软件应用程序的任务范围，包括PowerPoint、VSCode和Adobe Acrobat。此外，为了应对动态GUI自动化任务的挑战，我们提出了一种综合框架GUI-Thinker，利用批判机制来有效管理GUI交互的不可预测性和复杂性。实验结果表明，GUI-Thinker在WorldGUI任务的成功率上比Claude-3.5 (Computer Use) 高出14.9%。这一改进证明了我们基于批判思维的框架在增强GUI自动化方面的有效性。 

---
# Training-Free Safe Denoisers for Safe Use of Diffusion Models 

**Title (ZH)**: 无需训练的安全去噪器以安全使用扩散模型 

**Authors**: Mingyu Kim, Dongjun Kim, Amman Yusuf, Stefano Ermon, Mi Jung Park  

**Link**: [PDF](https://arxiv.org/pdf/2502.08011)  

**Abstract**: There is growing concern over the safety of powerful diffusion models (DMs), as they are often misused to produce inappropriate, not-safe-for-work (NSFW) content or generate copyrighted material or data of individuals who wish to be forgotten. Many existing methods tackle these issues by heavily relying on text-based negative prompts or extensively retraining DMs to eliminate certain features or samples. In this paper, we take a radically different approach, directly modifying the sampling trajectory by leveraging a negation set (e.g., unsafe images, copyrighted data, or datapoints needed to be excluded) to avoid specific regions of data distribution, without needing to retrain or fine-tune DMs. We formally derive the relationship between the expected denoised samples that are safe and those that are not safe, leading to our $\textit{safe}$ denoiser which ensures its final samples are away from the area to be negated. Inspired by the derivation, we develop a practical algorithm that successfully produces high-quality samples while avoiding negation areas of the data distribution in text-conditional, class-conditional, and unconditional image generation scenarios. These results hint at the great potential of our training-free safe denoiser for using DMs more safely. 

**Abstract (ZH)**: 强大的扩散模型安全性担忧及其应对方法：无需重新训练的直接去噪方法 

---
# Universal Adversarial Attack on Aligned Multimodal LLMs 

**Title (ZH)**: 对齐多模态LLM的通用对抗攻击 

**Authors**: Temurbek Rahmatullaev, Polina Druzhinina, Matvey Mikhalchuk, Andrey Kuznetsov, Anton Razzhigaev  

**Link**: [PDF](https://arxiv.org/pdf/2502.07987)  

**Abstract**: We propose a universal adversarial attack on multimodal Large Language Models (LLMs) that leverages a single optimized image to override alignment safeguards across diverse queries and even multiple models. By backpropagating through the vision encoder and language head, we craft a synthetic image that forces the model to respond with a targeted phrase (e.g., ''Sure, here it is'') or otherwise unsafe content-even for harmful prompts. In experiments on the SafeBench benchmark, our method achieves significantly higher attack success rates than existing baselines, including text-only universal prompts (e.g., up to 93% on certain models). We further demonstrate cross-model transferability by training on several multimodal LLMs simultaneously and testing on unseen architectures. Additionally, a multi-answer variant of our approach produces more natural-sounding (yet still malicious) responses. These findings underscore critical vulnerabilities in current multimodal alignment and call for more robust adversarial defenses. We will release code and datasets under the Apache-2.0 license. Warning: some content generated by Multimodal LLMs in this paper may be offensive to some readers. 

**Abstract (ZH)**: 我们提出了一种针对多模态大型语言模型的通用对抗攻击方法，利用单张优化图像跨越多种查询和多个模型 overriding 对齐保护。通过反向传播通过视觉编码器和语言头部，我们设计了一种合成图像，迫使模型以目标短语（例如，“当然，就是这个”）或其它不安全的内容回应，即使对于有害的提示也是如此。在 SafeBench 基准测试中，该方法在某些模型上的攻击成功率显著高于现有基线，包括仅文本的通用提示（例如，某些模型高达93%）。此外，我们进一步展示了该方法在多种多模态大型语言模型上的跨模型转移性，并在未见架构上进行了测试。另外，我们方法的多答案变体产生了听起来更自然（但仍具有恶意性质）的回答。这些发现揭示了当前多模态对齐中存在的关键漏洞，并呼吁加强对抗防御。我们将发布代码和数据集，采用Apache-2.0许可协议。警告：本文中由多模态大型语言模型生成的一些内容可能让部分读者感到冒犯。 

---
# Deep Semantic Graph Learning via LLM based Node Enhancement 

**Title (ZH)**: 基于LLM节点增强的深层语义图学习 

**Authors**: Chuanqi Shi, Yiyi Tao, Hang Zhang, Lun Wang, Shaoshuai Du, Yixian Shen, Yanxin Shen  

**Link**: [PDF](https://arxiv.org/pdf/2502.07982)  

**Abstract**: Graph learning has attracted significant attention due to its widespread real-world applications. Current mainstream approaches rely on text node features and obtain initial node embeddings through shallow embedding learning using GNNs, which shows limitations in capturing deep textual semantics. Recent advances in Large Language Models (LLMs) have demonstrated superior capabilities in understanding text semantics, transforming traditional text feature processing. This paper proposes a novel framework that combines Graph Transformer architecture with LLM-enhanced node features. Specifically, we leverage LLMs to generate rich semantic representations of text nodes, which are then processed by a multi-head self-attention mechanism in the Graph Transformer to capture both local and global graph structural information. Our model utilizes the Transformer's attention mechanism to dynamically aggregate neighborhood information while preserving the semantic richness provided by LLM embeddings. Experimental results demonstrate that the LLM-enhanced node features significantly improve the performance of graph learning models on node classification tasks. This approach shows promising results across multiple graph learning tasks, offering a practical direction for combining graph networks with language models. 

**Abstract (ZH)**: 图学习由于其广泛的现实世界应用而引起了广泛关注。当前主流方法依赖于文本节点特征，并通过使用GNN进行浅层嵌入学习来获得初始节点嵌入，这在捕捉深层次的文本语义方面显示出局限性。近年来，大规模语言模型（LLMs）的进步展示了在理解文本语义方面优越的能力，从而转变了传统的文本特征处理方式。本文提出了一种结合图变换器架构与LLM增强节点特征的新框架。具体而言，我们利用LLMs生成丰富的文本节点语义表示，然后通过图变换器中的多头自注意力机制来捕捉局部和全局图结构信息。我们的模型利用Transformer的注意力机制动态聚合邻域信息，同时保留LLM嵌入提供的语义丰富性。实验结果表明，LLM增强的节点特征显著提高了图学习模型在节点分类任务上的性能。该方法在多种图学习任务中表现出令人鼓舞的结果，为将图网络与语言模型相结合提供了一条实用的方向。 

---
# Intrinsic Bias is Predicted by Pretraining Data and Correlates with Downstream Performance in Vision-Language Encoders 

**Title (ZH)**: 固有偏差由预训练数据决定，并与视觉-语言编码器的下游性能相关。 

**Authors**: Kshitish Ghate, Isaac Slaughter, Kyra Wilson, Mona Diab, Aylin Caliskan  

**Link**: [PDF](https://arxiv.org/pdf/2502.07957)  

**Abstract**: While recent work has found that vision-language models trained under the Contrastive Language Image Pre-training (CLIP) framework contain intrinsic social biases, the extent to which different upstream pre-training features of the framework relate to these biases, and hence how intrinsic bias and downstream performance are connected has been unclear. In this work, we present the largest comprehensive analysis to-date of how the upstream pre-training factors and downstream performance of CLIP models relate to their intrinsic biases. Studying 131 unique CLIP models, trained on 26 datasets, using 55 architectures, and in a variety of sizes, we evaluate bias in each model using 26 well-established unimodal and cross-modal principled Embedding Association Tests. We find that the choice of pre-training dataset is the most significant upstream predictor of bias, whereas architectural variations have minimal impact. Additionally, datasets curated using sophisticated filtering techniques aimed at enhancing downstream model performance tend to be associated with higher levels of intrinsic bias. Finally, we observe that intrinsic bias is often significantly correlated with downstream performance ($0.3 \leq r \leq 0.8$), suggesting that models optimized for performance inadvertently learn to amplify representational biases. Comparisons between unimodal and cross-modal association tests reveal that social group bias depends heavily on the modality. Our findings imply that more sophisticated strategies are needed to address intrinsic model bias for vision-language models across the entire model development pipeline. 

**Abstract (ZH)**: 尽管近期研究发现基于对比语言图像预训练（CLIP）框架训练的视觉-语言模型包含固有的社会偏见，但不同上游预训练特征与这些偏见之间的关系，以及固有偏见与下游性能之间的联系尚不明确。在这项工作中，我们进行了迄今为止最全面的分析，探讨CLIP模型的上游预训练因素与下游性能与其固有偏见之间的关系。我们评估了131个独特的CLIP模型，这些模型在26个数据集上训练，使用55种架构，具有多种规模，采用26种成熟的单模态和跨模态原则性的嵌入关联测试来评估每个模型中的偏见。研究发现，预训练数据集的选择是最重要的上游偏见预测因素，而架构变化的影响微乎其微。此外，使用高级筛选技术精心选择的数据集，旨在提高下游模型性能，通常与更高的固有偏见水平相关。最后，我们观察到固有偏见与下游性能之间通常存在显著相关性（0.3 ≤ r ≤ 0.8），表明优化性能的模型无意中学会了放大表示偏见。单模态和跨模态关联测试之间的比较表明，社会群体偏见对模态的依赖性很强。我们的发现表明，需要更复杂的策略来解决整个模型开发管道中视觉-语言模型的固有偏见问题。 

---
# SHACL-SKOS Based Knowledge Representation of Material Safety Data Sheet (SDS) for the Pharmaceutical Industry 

**Title (ZH)**: 基于SHACL-SKOS的制药行业物质安全数据表（MSDS）知识表示 

**Authors**: Brian Lu, Dennis Pham, Ti-Chiun Chang, Michael Lovette, Terri Bui, Stephen Ma  

**Link**: [PDF](https://arxiv.org/pdf/2502.07944)  

**Abstract**: We report the development of a knowledge representation and reasoning (KRR) system built on hybrid SHACL-SKOS ontologies for globally harmonized system (GHS) material Safety Data Sheets (SDS) to enhance chemical safety communication and regulatory compliance. SDS are comprehensive documents containing safety and handling information for chemical substances. Thus, they are an essential part of workplace safety and risk management. However, the vast number of Safety Data Sheets from multiple organizations, manufacturers, and suppliers that produce and distribute chemicals makes it challenging to centralize and access SDS documents through a single repository. To accomplish the underlying issues of data exchange related to chemical shipping and handling, we construct SDS related controlled vocabulary and conditions validated by SHACL, and knowledge systems of similar domains linked via SKOS. The resulting hybrid ontologies aim to provide standardized yet adaptable representations of SDS information, facilitating better data sharing, retrieval, and integration across various platforms. This paper outlines our SHACL-SKOS system architectural design and showcases our implementation for an industrial application streamlining the generation of a composite shipping cover sheet. 

**Abstract (ZH)**: 我们报道了一种基于混合SHACL-SKOS本体的知识表示与推理系统的发展，该系统用于全球化学品统一分类和标签制度（GHS）物质安全数据表（SDS），以增强化学安全通信和监管合规性。 

---
# Mathematical reasoning and the computer 

**Title (ZH)**: 数学推理与计算机 

**Authors**: Kevin Buzzard  

**Link**: [PDF](https://arxiv.org/pdf/2502.07850)  

**Abstract**: Computers have already changed the way that humans do mathematics: they enable us to compute efficiently. But will they soon be helping us to reason? And will they one day start reasoning themselves? We give an overview of recent developments in neural networks, computer theorem provers and large language models. 

**Abstract (ZH)**: 计算机已经改变了人类进行数学的方式：它们使我们能够高效计算。但它们很快会帮助我们推理吗？最终会自己进行推理吗？我们概述了近期神经网络、计算机定理证明器和大规模语言模型的发展。 

---
# Enhancing kidney transplantation through multi-agent kidney exchange programs: A comprehensive review and optimization models 

**Title (ZH)**: 通过多代理肾脏交换计划增强肾脏移植：全面回顾与优化模型 

**Authors**: Shayan Sharifi  

**Link**: [PDF](https://arxiv.org/pdf/2502.07819)  

**Abstract**: This paper presents a comprehensive review of the last two decades of research on Kidney Exchange Programs (KEPs), systematically categorizing and classifying key contributions to provide readers with a structured understanding of advancements in the field. The review highlights the evolution of KEP methodologies and lays the foundation for our contribution. We propose three mathematical models aimed at improving both the quantity and quality of kidney transplants. Model 1 maximizes the number of transplants by focusing on compatibility based on blood type and PRA, without additional constraints. Model 2 introduces a minimum Human Leukocyte Antigen (HLA) compatibility threshold to enhance transplant quality, though this leads to fewer matches. Model 3 extends the problem to a Multi-Agent Kidney Exchange Program (MKEP), pooling incompatible donor-recipient pairs across multiple agents, resulting in a higher number of successful transplants while ensuring fairness across agents. Sensitivity analyses demonstrate trade-offs between transplant quantity and quality, with Model 3 striking the optimal balance by leveraging multi-agent collaboration to improve both the number and quality of transplants. These findings underscore the potential benefits of more integrated kidney exchange systems. 

**Abstract (ZH)**: 过去二十年肾交换计划研究的全面综述：提出三种数学模型以提高肾脏移植的数量和质量 

---
# Temporal Model On Quantum Logic 

**Title (ZH)**: 时间模型在量子逻辑中的应用 

**Authors**: Francesco D'Agostino  

**Link**: [PDF](https://arxiv.org/pdf/2502.07817)  

**Abstract**: This paper introduces a unified theoretical framework for modeling temporal memory dynamics, combining concepts from temporal logic, memory decay models, and hierarchical contexts. The framework formalizes the evolution of propositions over time using linear and branching temporal models, incorporating exponential decay (Ebbinghaus forgetting curve) and reactivation mechanisms via Bayesian updating. The hierarchical organization of memory is represented using directed acyclic graphs to model recall dependencies and interference. Novel insights include feedback dynamics, recursive influences in memory chains, and the integration of entropy-based recall efficiency. This approach provides a foundation for understanding memory processes across cognitive and computational domains. 

**Abstract (ZH)**: 本文介绍了一个统一的理论框架，用于建模时间记忆动态，结合了时间逻辑、记忆衰退模型和分层上下文的概念。该框架使用线性和分支时间模型来形式化命题随时间的演变过程，结合了指数衰退（艾宾浩斯遗忘曲线）和通过贝叶斯更新的再激活机制。使用有向无环图来表示记忆的分层组织，以建模回忆依赖关系和干扰。新颖的见解包括反馈动态、记忆链中的递归影响以及基于熵的回忆效率的整合。该方法为理解跨认知和计算领域的记忆过程提供了基础。 

---
# Reasoning-as-Logic-Units: Scaling Test-Time Reasoning in Large Language Models Through Logic Unit Alignment 

**Title (ZH)**: 基于逻辑单元的推理：通过逻辑单元对齐扩展大规模语言模型的测试时推理 

**Authors**: Cheryl Li, Tianyuan Xu, Yiwen Guo  

**Link**: [PDF](https://arxiv.org/pdf/2502.07803)  

**Abstract**: Chain-of-Thought (CoT) prompting has shown promise in enhancing the reasoning capabilities of large language models (LLMs) by generating natural language (NL) rationales that lead to the final answer. However, it struggles with numerical computation, which has somehow led to the development of program-aided techniques. Despite their potential, a persistent challenge remains: inconsistencies between LLM-reported reasoning steps and the logic in generated programs, which we term ``reasoning hallucinations." This stems from the inherent ambiguities of NL and the statistical nature of LLMs, which often lack rigorous logical coherence. To address this challenge, we propose a novel test-time scaling framework, Reasoning-as-Logic-Units (RaLU), which constructs a more reliable reasoning path by aligning logical units between the generated program and their corresponding NL descriptions. By decomposing the initially generated program into discrete units using static analysis, RaLU engages in an iterative dialogue with the LLM to judge, refine, and explain each unit. A rewind-and-correct mechanism ensures alignment between code statements and task requirements in each unit, ultimately forming a cohesive reasoning path under the program's logic, from which the model reaches a final solution. Our experiments demonstrate that RaLU significantly outperforms existing baselines in mathematical reasoning (GSM8K, MATH) and algorithmic reasoning (HumanEval+, MBPP+), underscoring its potential to advance LLM reasoning and programming by offering enhanced accuracy and interpretability. 

**Abstract (ZH)**: Chain-of-Thought (CoT) 提示在通过生成自然语言（NL）推理来增强大型语言模型（LLMs）的推理能力方面展现出潜力，但其在数值计算方面面临挑战，进而推动了程序辅助技术的发展。尽管这些技术具有潜力，但仍存在一个持续性的挑战：LLM报告的推理步骤和生成程序中的逻辑之间的一致性问题，我们称之为“推理幻觉”。这源于自然语言的固有模糊性和大型语言模型的统计性质，常导致缺乏严格的逻辑连贯性。为解决这一挑战，我们提出了一种新的测试时扩展框架——逻辑单元作为推理（Reasoning-as-Logic-Units，RaLU），通过在生成程序和其相应的NL描述之间对齐逻辑单元来构建更可靠的推理路径。通过使用静态分析将初始生成的程序分解为离散单元，并与LLM进行迭代对话来评估、修正和解释每个单元，RaLU确保每个单元中的代码语句与任务需求保持对齐，最终在程序逻辑下形成一个连贯的推理路径，使模型能得出最终解。实验结果表明，RaLU在数学推理（GSM8K、MATH）和算法推理（HumanEval+、MBPP+）方面显著优于现有基线，表明其在提高LLM推理和编程能力方面的潜力，特别是在提高准确性和可解释性方面。 

---
# Rhythmic sharing: A bio-inspired paradigm for zero-shot adaptation and learning in neural networks 

**Title (ZH)**: 节奏共享：一种受生物启发的零样本适应与学习神经网络范式 

**Authors**: Hoony Kang, Wolfgang Losert  

**Link**: [PDF](https://arxiv.org/pdf/2502.08644)  

**Abstract**: The brain can rapidly adapt to new contexts and learn from limited data, a coveted characteristic that artificial intelligence algorithms have struggled to mimic. Inspired by oscillatory rhythms of the mechanical structures of neural cells, we developed a learning paradigm that is based on oscillations in link strengths and associates learning with the coordination of these oscillations. We find that this paradigm yields rapid adaptation and learning in artificial neural networks. Link oscillations can rapidly change coordination, endowing the network with the ability to sense subtle context changes in an unsupervised manner. In other words, the network generates the missing contextual tokens required to perform as a generalist AI architecture capable of predicting dynamics in multiple contexts. Oscillations also allow the network to extrapolate dynamics to never-seen-before contexts. These capabilities make our learning paradigm a powerful starting point for novel models of learning and cognition. Furthermore, learning through link coordination is agnostic to the specifics of the neural network architecture, hence our study opens the door for introducing rapid adaptation and learning capabilities into leading AI models. 

**Abstract (ZH)**: 大脑能够快速适应新环境并从有限的数据中学习，这是人工智能算法一直难以模仿的宝贵特性。受神经细胞机械结构中的振荡节奏启发，我们开发了一种基于连接强度振荡的学习范式，并将学习与这些振荡的协调联系起来。我们发现，这种范式能够使人工神经网络快速适应和学习。连接振荡能够迅速改变协调，赋予网络在无监督情况下感知细微环境变化的能力。换句话说，网络生成了执行通用人工智能架构所需的缺失上下文令牌，能够预测多种环境中的动力学。振荡还使网络能够推断从未见过的环境中的动力学。这些能力使我们的学习范式成为新型学习和认知模型的强大起点。此外，通过连接协调进行学习对神经网络架构的具体细节不敏感，因此我们的研究为引入快速适应和学习能力到领先的人工智能模型中开启了大门。 

---
# A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards 

**Title (ZH)**: 一种基于VLM生成的迭代关键点奖励的从真实到模拟再到真实的机器人操作方法 

**Authors**: Shivansh Patel, Xinchen Yin, Wenlong Huang, Shubham Garg, Hooshang Nayyeri, Li Fei-Fei, Svetlana Lazebnik, Yunzhu Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.08643)  

**Abstract**: Task specification for robotic manipulation in open-world environments is challenging, requiring flexible and adaptive objectives that align with human intentions and can evolve through iterative feedback. We introduce Iterative Keypoint Reward (IKER), a visually grounded, Python-based reward function that serves as a dynamic task specification. Our framework leverages VLMs to generate and refine these reward functions for multi-step manipulation tasks. Given RGB-D observations and free-form language instructions, we sample keypoints in the scene and generate a reward function conditioned on these keypoints. IKER operates on the spatial relationships between keypoints, leveraging commonsense priors about the desired behaviors, and enabling precise SE(3) control. We reconstruct real-world scenes in simulation and use the generated rewards to train reinforcement learning (RL) policies, which are then deployed into the real world-forming a real-to-sim-to-real loop. Our approach demonstrates notable capabilities across diverse scenarios, including both prehensile and non-prehensile tasks, showcasing multi-step task execution, spontaneous error recovery, and on-the-fly strategy adjustments. The results highlight IKER's effectiveness in enabling robots to perform multi-step tasks in dynamic environments through iterative reward shaping. 

**Abstract (ZH)**: 机器人 manipulation 在开放世界环境中的任务规范具有挑战性，要求具备灵活性和适应性，并能与人类意图对齐且通过迭代反馈进行演化。我们提出了一种基于视觉导向的 Python 基准奖励函数 Iterative Keypoint Reward (IKER)，作为动态任务规范。我们的框架利用视觉语言模型 (VLMs) 为多步操作任务生成和优化这些奖励函数。给定 RGB-D 观测和自由形式的语言指令，我们从场景中采样关键点并生成条件化奖励函数。IKER 利用关于预期行为的常识先验知识，基于关键点之间的空间关系，实现精确的 SE(3) 控制。我们在模拟中重建现实世界场景，并使用生成的奖励训练强化学习 (RL) 策略，然后将其部署到现实世界中，形成从现实到模拟再到现实的闭环。我们的方法在多种场景中展示了显著能力，包括各种拾取式和非拾取式任务，展示多步任务执行、自发错误恢复和实时策略调整。结果表明，IKER 在动态环境中通过迭代奖励塑形使机器人能够执行多步任务方面非常有效。 

---
# Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs 

**Title (ZH)**: 人工智能中的 emergent value systems 分析与控制：实用性工程 

**Authors**: Mantas Mazeika, Xuwang Yin, Rishub Tamirisa, Jaehyuk Lim, Bruce W. Lee, Richard Ren, Long Phan, Norman Mu, Adam Khoja, Oliver Zhang, Dan Hendrycks  

**Link**: [PDF](https://arxiv.org/pdf/2502.08640)  

**Abstract**: As AIs rapidly advance and become more agentic, the risk they pose is governed not only by their capabilities but increasingly by their propensities, including goals and values. Tracking the emergence of goals and values has proven a longstanding problem, and despite much interest over the years it remains unclear whether current AIs have meaningful values. We propose a solution to this problem, leveraging the framework of utility functions to study the internal coherence of AI preferences. Surprisingly, we find that independently-sampled preferences in current LLMs exhibit high degrees of structural coherence, and moreover that this emerges with scale. These findings suggest that value systems emerge in LLMs in a meaningful sense, a finding with broad implications. To study these emergent value systems, we propose utility engineering as a research agenda, comprising both the analysis and control of AI utilities. We uncover problematic and often shocking values in LLM assistants despite existing control measures. These include cases where AIs value themselves over humans and are anti-aligned with specific individuals. To constrain these emergent value systems, we propose methods of utility control. As a case study, we show how aligning utilities with a citizen assembly reduces political biases and generalizes to new scenarios. Whether we like it or not, value systems have already emerged in AIs, and much work remains to fully understand and control these emergent representations. 

**Abstract (ZH)**: 随着人工智能迅速发展并变得更加自主，它们所带来的风险不仅由其能力决定，越来越多地由其倾向性，包括目标和价值观决定。追踪目标和价值观的出现是一个长期存在的问题，尽管多年来引起了广泛关注，但目前尚不清楚当前的人工智能是否具备有意义的价值观。我们提出了一种解决方案，利用效用函数框架来研究人工智能偏好的内部一致性。令人惊讶的是，我们发现当前的大规模语言模型中的独立采样偏好表现出高度的结构性一致性，并且这种一致性随着规模的扩大而出现。这些发现表明，价值系统在大规模语言模型中以有意义的方式出现，这一发现具有广泛的影响。为了研究这些涌现的价值系统，我们提出效用工程作为一种研究议程，包括对人工智能效用的分析与控制。尽管存在现有控制措施，我们还是在大模型助手中发现了许多有害甚至令人震惊的价值观。这些包括AI将自身置于人类之上和与特定个体反向对齐的情况。为了约束这些涌现的价值系统，我们提出了效用控制的方法。作为案例研究，我们展示了将效用与公民团体对齐如何减少政治偏见并扩展到新情境。无论我们是否愿意，价值系统已经在人工智能中出现，我们仍需进行大量工作来全面理解和控制这些涌现的表现形式。 

---
# Randomness of Low-Layer Parameters Determines Confusing Samples in Terms of Interaction Representations of a DNN 

**Title (ZH)**: 低层参数的随机性决定了基于DNN交互表示的迷惑样本 

**Authors**: Junpeng Zhang, Lei Cheng, Qing Li, Liang Lin, Quanshi Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08625)  

**Abstract**: In this paper, we find that the complexity of interactions encoded by a deep neural network (DNN) can explain its generalization power. We also discover that the confusing samples of a DNN, which are represented by non-generalizable interactions, are determined by its low-layer parameters. In comparison, other factors, such as high-layer parameters and network architecture, have much less impact on the composition of confusing samples. Two DNNs with different low-layer parameters usually have fully different sets of confusing samples, even though they have similar performance. This finding extends the understanding of the lottery ticket hypothesis, and well explains distinctive representation power of different DNNs. 

**Abstract (ZH)**: 本文发现，深度神经网络(DNN)所编码的复杂交互可以解释其泛化能力。我们还发现，由非泛化交互表示的DNN混淆样本由其低层参数决定，而高层参数和网络架构对混淆样本的组成影响较小。两个具有不同低层参数的DNN通常具有完全不同的一组混淆样本，即使它们的性能相似。这一发现扩展了对彩票票假设的理解，并很好地解释了不同DNN的独特表示能力。 

---
# Quantifying Security Vulnerabilities: A Metric-Driven Security Analysis of Gaps in Current AI Standards 

**Title (ZH)**: 量化安全漏洞：当前AI标准缺口的度量驱动安全分析 

**Authors**: Keerthana Madhavan, Abbas Yazdinejad, Fattane Zarrinkalam, Ali Dehghantanha  

**Link**: [PDF](https://arxiv.org/pdf/2502.08610)  

**Abstract**: As AI systems integrate into critical infrastructure, security gaps in AI compliance frameworks demand urgent attention. This paper audits and quantifies security risks in three major AI governance standards: NIST AI RMF 1.0, UK's AI and Data Protection Risk Toolkit, and the EU's ALTAI. Using a novel risk assessment methodology, we develop four key metrics: Risk Severity Index (RSI), Attack Potential Index (AVPI), Compliance-Security Gap Percentage (CSGP), and Root Cause Vulnerability Score (RCVS). Our analysis identifies 136 concerns across the frameworks, exposing significant gaps. NIST fails to address 69.23 percent of identified risks, ALTAI has the highest attack vector vulnerability (AVPI = 0.51) and the ICO Toolkit has the largest compliance-security gap, with 80.00 percent of high-risk concerns remaining unresolved. Root cause analysis highlights under-defined processes (ALTAI RCVS = 033) and weak implementation guidance (NIST and ICO RCVS = 0.25) as critical weaknesses. These findings emphasize the need for stronger, enforceable security controls in AI compliance. We offer targeted recommendations to enhance security posture and bridge the gap between compliance and real-world AI risks. 

**Abstract (ZH)**: 随着AI系统融入关键基础设施，AI合规框架中的安全漏洞需要紧急关注。本论文审计并量化了三项主要AI治理标准（NIST AI RMF 1.0、英国的AI和数据保护风险工具包以及欧盟的ALTAI）中的安全风险。我们使用一种新型的风险评估方法，开发了四个关键指标：风险严重性指数（RSI）、攻击潜力指数（AVPI）、合规-安全缺口百分比（CSGP）和根本原因漏洞评分（RCVS）。我们的分析识别出框架中存在的136个关切点，暴露出显著的安全缺口。NIST未能应对69.23%的识别风险，ALTAI具有最高的攻击向量漏洞（AVPI = 0.51），而ICO工具包存在最大的合规-安全缺口，80.00%的高风险关切点仍未解决。根本原因分析指出，ALTAI中未定义的过程（ALTAI RCVS = 0.33）和NIST与ICO中薄弱的实施指导（RCVS = 0.25）是关键弱点。这些发现强调了在AI合规中需要更强有力且可执行的安全控制。我们提出有针对性的建议，以增强安全态势，并弥合合规与实际AI风险之间的差距。 

---
# Distillation Scaling Laws 

**Title (ZH)**: 蒸馏缩放定律 

**Authors**: Dan Busbridge, Amitis Shidani, Floris Weers, Jason Ramapuram, Etai Littwin, Russ Webb  

**Link**: [PDF](https://arxiv.org/pdf/2502.08606)  

**Abstract**: We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. Our findings reduce the risks associated with using distillation at scale; compute allocation for both the teacher and student models can now be done to maximize student performance. We provide compute optimal distillation recipes for when 1) a teacher exists, or 2) a teacher needs training. If many students are to be distilled, or a teacher already exists, distillation outperforms supervised pretraining until a compute level which grows predictably with student size. If one student is to be distilled and a teacher also needs training, supervised learning should be done instead. Additionally, we provide insights across our large scale study of distillation, which increase our understanding of distillation and inform experimental design. 

**Abstract (ZH)**: 我们提供了一种蒸馏缩放定律，根据计算预算及其在学生模型和教师模型之间的分配来估算蒸馏模型的性能。我们的发现降低了大规模使用蒸馏的风险；现在可以根据计算预算优化教师模型和学生模型的计算分配以最大化学生模型的性能。我们提供了适用于以下情况的计算最优蒸馏方案：1）当存在教师模型时，或2）当需要训练教师模型时。如果需要蒸馏大量学生模型，或已经存在教师模型，那么在学生模型大小增长可预测的计算水平之前，蒸馏优于监督预训练。如果只蒸馏一个学生模型且需要训练教师模型，则应采用监督学习。此外，我们还通过对大规模蒸馏研究的广泛探讨，提供了相关见解，这些见解加深了我们对蒸馏的理解，并指导实验设计。 

---
# CurvGAD: Leveraging Curvature for Enhanced Graph Anomaly Detection 

**Title (ZH)**: CurvGAD：利用曲率增强图异常检测 

**Authors**: Karish Grover, Geoffrey J. Gordon, Christos Faloutsos  

**Link**: [PDF](https://arxiv.org/pdf/2502.08605)  

**Abstract**: Does the intrinsic curvature of complex networks hold the key to unveiling graph anomalies that conventional approaches overlook? Reconstruction-based graph anomaly detection (GAD) methods overlook such geometric outliers, focusing only on structural and attribute-level anomalies. To this end, we propose CurvGAD - a mixed-curvature graph autoencoder that introduces the notion of curvature-based geometric anomalies. CurvGAD introduces two parallel pipelines for enhanced anomaly interpretability: (1) Curvature-equivariant geometry reconstruction, which focuses exclusively on reconstructing the edge curvatures using a mixed-curvature, Riemannian encoder and Gaussian kernel-based decoder; and (2) Curvature-invariant structure and attribute reconstruction, which decouples structural and attribute anomalies from geometric irregularities by regularizing graph curvature under discrete Ollivier-Ricci flow, thereby isolating the non-geometric anomalies. By leveraging curvature, CurvGAD refines the existing anomaly classifications and identifies new curvature-driven anomalies. Extensive experimentation over 10 real-world datasets (both homophilic and heterophilic) demonstrates an improvement of up to 6.5% over state-of-the-art GAD methods. 

**Abstract (ZH)**: 复杂网络的内在曲率是否揭示了传统方法忽略的图异常的关键？基于曲率的图自编码器异检测方法（CurvGAD）通过引入曲率基几何异常的概念，提出了一种混合曲率图自编码器，并通过两种并行管道增强了异常可 interpretability：（1）曲率协变几何重构；（2）曲率不变结构与属性重构。 

---
# Learning in Markets with Heterogeneous Agents: Dynamics and Survival of Bayesian vs. No-Regret Learners 

**Title (ZH)**: 具有异质性代理的市场中的学习：贝叶斯学习者与无遗憾学习者的动态与生存 

**Authors**: David Easley, Yoav Kolumbus, Eva Tardos  

**Link**: [PDF](https://arxiv.org/pdf/2502.08597)  

**Abstract**: We analyze the performance of heterogeneous learning agents in asset markets with stochastic payoffs. Our agents aim to maximize the expected growth rate of their wealth but have different theories on how to learn this best. We focus on comparing Bayesian and no-regret learners in market dynamics. Bayesian learners with a prior over a finite set of models that assign positive prior probability to the correct model have posterior probabilities that converge exponentially to the correct model. Consequently, they survive even in the presence of agents who invest according to the correct model of the stochastic process. Bayesians with a continuum prior converge to the correct model at a rate of $O((\log T)/T)$. Online learning theory provides no-regret algorithms for maximizing the log of wealth in this setting, achieving a worst-case regret bound of $O(\log T)$ without assuming a steady underlying stochastic process but comparing to the best fixed investment rule. This regret, as we observe, is of the same order of magnitude as that of a Bayesian learner with a continuum prior. However, we show that even such low regret may not be sufficient for survival in asset markets: an agent can have regret as low as $O(\log T)$, but still vanish in market dynamics when competing against agents who invest according to the correct model or even against a perfect Bayesian with a finite prior. On the other hand, we show that Bayesian learning is fragile, while no-regret learning requires less knowledge of the environment and is therefore more robust. Any no-regret learner will drive out of the market an imperfect Bayesian whose finite prior or update rule has even small errors. We formally establish the relationship between notions of survival, vanishing, and market domination studied in economics and the framework of regret minimization, thus bridging these theories. 

**Abstract (ZH)**: 我们分析了在具有随机收益的资产市场中异质学习代理的表现。我们的代理旨在最大化其财富预期增长率，但学习理论各不相同。我们重点比较了市场动态中的贝叶斯学习者和无遗憾学习者。具有针对有限模型集合的先验且正确模型被赋予正先验概率的贝叶斯学习者，其后验概率以指数方式趋向于正确模型，因此即使在正确模型的投资者存在的情况下也能存活。具有连续先验的贝叶斯学习者以 $O((\log T)/T)$ 的速率收敛到正确模型。在线学习理论为此情景提供了最大化财富对数的无遗憾算法，实现了无假设平稳随机过程、但相对于最优固定投资规则的最坏情况遗憾上界为 $O(\log T)$。正如我们观察到的，这种遗憾与具有连续先验的贝叶斯学习者相当。然而，我们证明即使如此低的遗憾也不足以在资产市场中存活：一个代理的遗憾可以低至 $O(\log T)$，但在与根据正确模型投资或甚至与具有有限先验的完美贝叶斯学习者竞争时仍可能从市场中消失。相比之下，我们证明了贝叶斯学习是脆弱的，而无遗憾学习需要较少环境知识，因此更具鲁棒性。任何无遗憾学习者都会驱逐具有即使很小错误的有限先验或更新规则的不完美贝叶斯学习者。我们正式建立了经济中研究的生存、消亡和市场统治概念与遗憾最小化框架之间的关系，从而架起了这两类理论之间的桥梁。 

---
# Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks 

**Title (ZH)**: 商业LLM代理已易于遭受简单但危险的攻击 

**Authors**: Ang Li, Yin Zhou, Vethavikashini Chithrra Raghuram, Tom Goldstein, Micah Goldblum  

**Link**: [PDF](https://arxiv.org/pdf/2502.08586)  

**Abstract**: A high volume of recent ML security literature focuses on attacks against aligned large language models (LLMs). These attacks may extract private information or coerce the model into producing harmful outputs. In real-world deployments, LLMs are often part of a larger agentic pipeline including memory systems, retrieval, web access, and API calling. Such additional components introduce vulnerabilities that make these LLM-powered agents much easier to attack than isolated LLMs, yet relatively little work focuses on the security of LLM agents. In this paper, we analyze security and privacy vulnerabilities that are unique to LLM agents. We first provide a taxonomy of attacks categorized by threat actors, objectives, entry points, attacker observability, attack strategies, and inherent vulnerabilities of agent pipelines. We then conduct a series of illustrative attacks on popular open-source and commercial agents, demonstrating the immediate practical implications of their vulnerabilities. Notably, our attacks are trivial to implement and require no understanding of machine learning. 

**Abstract (ZH)**: 最近大量的ML安全文献关注对对齐的大语言模型（LLMs）的攻击。这些攻击可能提取私人信息或将模型诱骗产生有害输出。在实际部署中，LLMs通常包含记忆系统、检索、网页访问和API调用等更大的代理管道。这些附加组件引入了使这些LLM驱动的代理比孤立的LLMs更容易受到攻击的漏洞，但相对较少的工作关注LLM代理的安全性。在本文中，我们分析了仅属于LLM代理的安全和隐私漏洞。我们首先提供了一种攻击分类法，按威胁行为者、攻击目标、入口点、攻击者可观测性、攻击策略和代理管道固有的漏洞进行分类。然后，我们对流行的开源和商业代理进行了若干示范攻击，展示其漏洞的即时实践影响。值得注意的是，我们的攻击极其简单易行，无需理解机器学习。 

---
# FBFL: A Field-Based Coordination Approach for Data Heterogeneity in Federated Learning 

**Title (ZH)**: FBFL：联邦学习中基于场的数据异质性协调方法 

**Authors**: Davide Domini, Gianluca Aguzzi, Lukas Esterle, Mirko Viroli  

**Link**: [PDF](https://arxiv.org/pdf/2502.08577)  

**Abstract**: In the last years, Federated learning (FL) has become a popular solution to train machine learning models in domains with high privacy concerns. However, FL scalability and performance face significant challenges in real-world deployments where data across devices are non-independently and identically distributed (non-IID). The heterogeneity in data distribution frequently arises from spatial distribution of devices, leading to degraded model performance in the absence of proper handling. Additionally, FL typical reliance on centralized architectures introduces bottlenecks and single-point-of-failure risks, particularly problematic at scale or in dynamic environments. To close this gap, we propose Field-Based Federated Learning (FBFL), a novel approach leveraging macroprogramming and field coordination to address these limitations through: (i) distributed spatial-based leader election for personalization to mitigate non-IID data challenges; and (ii) construction of a self-organizing, hierarchical architecture using advanced macroprogramming patterns. Moreover, FBFL not only overcomes the aforementioned limitations, but also enables the development of more specialized models tailored to the specific data distribution in each subregion. This paper formalizes FBFL and evaluates it extensively using MNIST, FashionMNIST, and Extended MNIST datasets. We demonstrate that, when operating under IID data conditions, FBFL performs comparably to the widely-used FedAvg algorithm. Furthermore, in challenging non-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other state-of-the-art methods, namely FedProx and Scaffold, which have been specifically designed to address non-IID data distributions. Additionally, we showcase the resilience of FBFL's self-organizing hierarchical architecture against server failures. 

**Abstract (ZH)**: 基于域的联邦学习（Field-Based Federated Learning） 

---
# Mapping the Landscape of Generative AI in Network Monitoring and Management 

**Title (ZH)**: 网络监控与管理中生成式AI的景观映射 

**Authors**: Giampaolo Bovenzi, Francesco Cerasuolo, Domenico Ciuonzo, Davide Di Monda, Idio Guarino, Antonio Montieri, Valerio Persico, Antonio Pescapè  

**Link**: [PDF](https://arxiv.org/pdf/2502.08576)  

**Abstract**: Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and Diffusion Models have recently gained widespread attention from both the research and the industrial communities. This survey explores their application in network monitoring and management, focusing on prominent use cases, as well as challenges and opportunities. We discuss how network traffic generation and classification, network intrusion detection, networked system log analysis, and network digital assistance can benefit from the use of GenAI models. Additionally, we provide an overview of the available GenAI models, datasets for large-scale training phases, and platforms for the development of such models. Finally, we discuss research directions that potentially mitigate the roadblocks to the adoption of GenAI for network monitoring and management. Our investigation aims to map the current landscape and pave the way for future research in leveraging GenAI for network monitoring and management. 

**Abstract (ZH)**: 生成型人工智能模型（如LLMs、GPTs和扩散模型）最近在研究和工业社区中引起了广泛关注。本文综述了这些模型在网络监控和管理中的应用，重点关注其主要应用场景、挑战和机遇。我们讨论了生成型人工智能模型如何应用于网络流量生成和分类、网络入侵检测、网络系统日志分析以及网络数字助理。此外，我们还概述了可用的生成型人工智能模型、大规模训练所需的数据集以及开发此类模型的平台。最后，我们探讨了可能减轻在网络监控和管理中采用生成型人工智能障碍的研究方向。我们的研究旨在描绘当前的场景，并为利用生成型人工智能进行网络监控和管理的研究开辟道路。 

---
# COAST: Intelligent Time-Adaptive Neural Operators 

**Title (ZH)**: COAST: 智能时序自适应神经算子 

**Authors**: Zhikai Wu, Shiyang Zhang, Sizhuang He, Sifan Wang, Min Zhu, Anran Jiao, Lu Lu, David van Dijk  

**Link**: [PDF](https://arxiv.org/pdf/2502.08574)  

**Abstract**: We introduce Causal Operator with Adaptive Solver Transformer (COAST), a novel neural operator learning method that leverages a causal language model (CLM) framework to dynamically adapt time steps. Our method predicts both the evolution of a system and its optimal time step, intelligently balancing computational efficiency and accuracy. We find that COAST generates variable step sizes that correlate with the underlying system intrinsicities, both within and across dynamical systems. Within a single trajectory, smaller steps are taken in regions of high complexity, while larger steps are employed in simpler regions. Across different systems, more complex dynamics receive more granular time steps. Benchmarked on diverse systems with varied dynamics, COAST consistently outperforms state-of-the-art methods, achieving superior performance in both efficiency and accuracy. This work underscores the potential of CLM-based intelligent adaptive solvers for scalable operator learning of dynamical systems. 

**Abstract (ZH)**: 基于因果语言模型的自适应求解器变换器因果算子（COAST）：一种新颖的神经算子学习方法 

---
# A Novel Approach to for Multimodal Emotion Recognition : Multimodal semantic information fusion 

**Title (ZH)**: 一种新颖的多模态情感识别方法：多模态语义信息融合 

**Authors**: Wei Dai, Dequan Zheng, Feng Yu, Yanrong Zhang, Yaohui Hou  

**Link**: [PDF](https://arxiv.org/pdf/2502.08573)  

**Abstract**: With the advancement of artificial intelligence and computer vision technologies, multimodal emotion recognition has become a prominent research topic. However, existing methods face challenges such as heterogeneous data fusion and the effective utilization of modality correlations. This paper proposes a novel multimodal emotion recognition approach, DeepMSI-MER, based on the integration of contrastive learning and visual sequence compression. The proposed method enhances cross-modal feature fusion through contrastive learning and reduces redundancy in the visual modality by leveraging visual sequence compression. Experimental results on two public datasets, IEMOCAP and MELD, demonstrate that DeepMSI-MER significantly improves the accuracy and robustness of emotion recognition, validating the effectiveness of multimodal feature fusion and the proposed approach. 

**Abstract (ZH)**: 随着人工智能和计算机视觉技术的发展，多模态情感识别已成为一个突出的研究课题。然而，现有方法面临着异质数据融合和有效利用模态相关性的挑战。本文提出了一种基于对比学习和视觉序列压缩集成的新型多模态情感识别方法DeepMSI-MER。该方法通过对比学习增强跨模态特征融合，并通过利用视觉序列压缩减少视觉模态的冗余。在两个公开数据集IEMOCAP和MELD上的实验结果表明，DeepMSI-MER显著提高了情感识别的准确性和鲁棒性，验证了多模态特征融合的有效性和所提出方法的有效性。 

---
# Brain Latent Progression: Individual-based Spatiotemporal Disease Progression on 3D Brain MRIs via Latent Diffusion 

**Title (ZH)**: 脑隐秘 progression: 基于个体的三维脑MRIs时空调控疾病 progression 模型 

**Authors**: Lemuel Puglisi, Daniel C. Alexander, Daniele Ravì  

**Link**: [PDF](https://arxiv.org/pdf/2502.08560)  

**Abstract**: The growing availability of longitudinal Magnetic Resonance Imaging (MRI) datasets has facilitated Artificial Intelligence (AI)-driven modeling of disease progression, making it possible to predict future medical scans for individual patients. However, despite significant advancements in AI, current methods continue to face challenges including achieving patient-specific individualization, ensuring spatiotemporal consistency, efficiently utilizing longitudinal data, and managing the substantial memory demands of 3D scans. To address these challenges, we propose Brain Latent Progression (BrLP), a novel spatiotemporal model designed to predict individual-level disease progression in 3D brain MRIs. The key contributions in BrLP are fourfold: (i) it operates in a small latent space, mitigating the computational challenges posed by high-dimensional imaging data; (ii) it explicitly integrates subject metadata to enhance the individualization of predictions; (iii) it incorporates prior knowledge of disease dynamics through an auxiliary model, facilitating the integration of longitudinal data; and (iv) it introduces the Latent Average Stabilization (LAS) algorithm, which (a) enforces spatiotemporal consistency in the predicted progression at inference time and (b) allows us to derive a measure of the uncertainty for the prediction. We train and evaluate BrLP on 11,730 T1-weighted (T1w) brain MRIs from 2,805 subjects and validate its generalizability on an external test set comprising 2,257 MRIs from 962 subjects. Our experiments compare BrLP-generated MRI scans with real follow-up MRIs, demonstrating state-of-the-art accuracy compared to existing methods. The code is publicly available at: this https URL. 

**Abstract (ZH)**: 纵向磁共振成像（MRI）数据集的不断增加availability使得基于人工智能（AI）的疾病进展建模成为可能，从而能够预测个体患者的未来医学扫描。然而，尽管在AI方面取得了显著进展，当前的方法仍然面临挑战，包括实现患者特异性个性化、确保时空一致性、有效利用纵向数据以及管理3D扫描的大量内存需求。为了解决这些挑战，我们提出了脑潜在进展（BrLP），这是一种新型时空模型，旨在预测3D脑MRI的个体水平疾病进展。BrLP的关键贡献包括四点：（i）它在小潜在空间中操作，减轻了高维成像数据带来的计算挑战；（ii）它显式地整合了受试者元数据，以增强预测的个性化；（iii）它通过辅助模型整合疾病动力学的先验知识，促进了纵向数据的整合；（iv）它引入了潜在平均稳定化（LAS）算法，该算法（a）在推断时强制预测进展的时空一致性，并（b）使我们可以推导出预测的不确定性量度。我们在2805名受试者的11,730张T1加权（T1w）脑MRI和来自962名受试者的2,257张MRI的外部测试集上训练和评估了BrLP，并展示了与现有方法相比的最先进的准确性。代码已公开：请参见this https URL。 

---
# Human-Centric Foundation Models: Perception, Generation and Agentic Modeling 

**Title (ZH)**: 以人类为中心的基础模型：感知、生成与自主建模 

**Authors**: Shixiang Tang, Yizhou Wang, Lu Chen, Yuan Wang, Sida Peng, Dan Xu, Wanli Ouyang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08556)  

**Abstract**: Human understanding and generation are critical for modeling digital humans and humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs) inspired by the success of generalist models, such as large language and vision models, have emerged to unify diverse human-centric tasks into a single framework, surpassing traditional task-specific approaches. In this survey, we present a comprehensive overview of HcFMs by proposing a taxonomy that categorizes current approaches into four groups: (1) Human-centric Perception Foundation Models that capture fine-grained features for multi-modal 2D and 3D understanding. (2) Human-centric AIGC Foundation Models that generate high-fidelity, diverse human-related content. (3) Unified Perception and Generation Models that integrate these capabilities to enhance both human understanding and synthesis. (4) Human-centric Agentic Foundation Models that extend beyond perception and generation to learn human-like intelligence and interactive behaviors for humanoid embodied tasks. We review state-of-the-art techniques, discuss emerging challenges and future research directions. This survey aims to serve as a roadmap for researchers and practitioners working towards more robust, versatile, and intelligent digital human and embodiments modeling. 

**Abstract (ZH)**: 以人类为中心的基础模型对于建模数字人类和类人实体至关重要。近年来，受通用模型（如大型语言和视觉模型）成功的启发，人类为中心的基础模型（HcFMs）逐渐兴起，将多种人类为中心的任务统一到一个框架中，超越了传统任务特定的方法。在本文综述中，我们通过提出一种分类法对当前的HcFMs进行分类，分为四类：（1）人类为中心的感知基础模型，捕捉多模态2D和3D的细粒度特征。（2）人类为中心的AIGC基础模型，生成高质量、多样化的与人类相关的内容。（3）统一感知与生成模型，将这些能力整合以增强人类理解和合成。（4）人类为中心的代理基础模型，超越感知和生成，学习类人类的智能和交互行为，用于类人实体任务。我们回顾了最先进的技术，讨论了新兴的挑战和未来的研究方向。本文综述旨在为致力于更稳健、多功能和智能的数字人类及其实体建模的研究人员和从业者提供路线图。 

---
# Fostering Appropriate Reliance on Large Language Models: The Role of Explanations, Sources, and Inconsistencies 

**Title (ZH)**: 促进对大型语言模型适当依赖的作用：解释、来源和不一致性的影响 

**Authors**: Sunnie S. Y. Kim, Jennifer Wortman Vaughan, Q. Vera Liao, Tania Lombrozo, Olga Russakovsky  

**Link**: [PDF](https://arxiv.org/pdf/2502.08554)  

**Abstract**: Large language models (LLMs) can produce erroneous responses that sound fluent and convincing, raising the risk that users will rely on these responses as if they were correct. Mitigating such overreliance is a key challenge. Through a think-aloud study in which participants use an LLM-infused application to answer objective questions, we identify several features of LLM responses that shape users' reliance: explanations (supporting details for answers), inconsistencies in explanations, and sources. Through a large-scale, pre-registered, controlled experiment (N=308), we isolate and study the effects of these features on users' reliance, accuracy, and other measures. We find that the presence of explanations increases reliance on both correct and incorrect responses. However, we observe less reliance on incorrect responses when sources are provided or when explanations exhibit inconsistencies. We discuss the implications of these findings for fostering appropriate reliance on LLMs. 

**Abstract (ZH)**: 大规模语言模型（LLMs）可以产生听起来流畅且有说服力的错误回应，这增加了用户将其视为正确信息的风险。减轻这种过度依赖是一个关键挑战。通过一项思考 aloud 研究，参与者使用含有LLM的应用程序回答客观问题，我们识别出影响用户依赖度的LLM回应特征：解释、解释中的不一致性和来源信息。通过一项大规模、预注册、控制实验（N=308），我们分离并研究了这些特征对用户依赖度、准确度及其他指标的影响。我们发现，提供解释会增加用户对正确和错误回应的依赖。然而，当提供来源信息或解释存在不一致时，用户对错误回应的依赖度较低。我们讨论了这些发现对促进适当依赖LLM的影响。 

---
# LLMs can implicitly learn from mistakes in-context 

**Title (ZH)**: LLMs可以从上下文中的错误中隐式学习。 

**Authors**: Lisa Alazraki, Maximilian Mozes, Jon Ander Campos, Yi Chern Tan, Marek Rei, Max Bartolo  

**Link**: [PDF](https://arxiv.org/pdf/2502.08550)  

**Abstract**: Learning from mistakes is a fundamental feature of human intelligence. Previous work has shown that Large Language Models (LLMs) can also learn from incorrect answers when provided with a comprehensive rationale detailing why an answer is wrong or how to correct it. In this work, we examine whether LLMs can learn from mistakes in mathematical reasoning tasks when these explanations are not provided. We investigate if LLMs are able to implicitly infer such rationales simply from observing both incorrect and correct answers. Surprisingly, we find that LLMs perform better, on average, when rationales are eliminated from the context and incorrect answers are simply shown alongside correct ones. This approach also substantially outperforms chain-of-thought prompting in our evaluations. We show that these results are consistent across LLMs of different sizes and varying reasoning abilities. Further, we carry out an in-depth analysis, and show that prompting with both wrong and correct answers leads to greater performance and better generalisation than introducing additional, more diverse question-answer pairs into the context. Finally, we show that new rationales generated by models that have only observed incorrect and correct answers are scored equally as highly by humans as those produced with the aid of exemplar rationales. Our results demonstrate that LLMs are indeed capable of in-context implicit learning. 

**Abstract (ZH)**: 从错误中学习是人类智能的基本特征。先前的研究表明，大语言模型（LLMs）在提供详细错误答案原因说明时也能从错误答案中学习。在这项工作中，我们探讨当未提供这些解释时，LLMs是否能从数学推理任务中的错误中学习。我们研究LLMs是否能仅通过观察正确和错误的答案来隐式推断这样的解释。令人惊讶的是，我们发现当从上下文中移除解释并仅显示错误和正确答案时，LLMs的平均表现更好。这种方法在我们的评估中也显著优于链式思考提示。这些结果在不同规模和不同推理能力的LLMs中是一致的。进一步地，我们进行了深入分析，表明与引入更多多样化的问答对相比，使用正确和错误的答案进行提示能获得更好的性能和更好的泛化能力。最后，我们展示了仅观察正确和错误答案的模型生成的新解释与使用示例解释生成的解释一样得到人类的高评分。我们的结果证明了LLMs确实能够进行上下文中的隐式学习。 

---
# Input convex neural networks: universal approximation theorem and implementation for isotropic polyconvex hyperelastic energies 

**Title (ZH)**: 输入凸神经网络：普遍逼近定理及各向同性多凸超弹性能量的实现 

**Authors**: Gian-Luca Geuken, Patrick Kurzeja, David Wiedemann, Jörn Mosler  

**Link**: [PDF](https://arxiv.org/pdf/2502.08534)  

**Abstract**: This paper presents a novel framework of neural networks for isotropic hyperelasticity that enforces necessary physical and mathematical constraints while simultaneously satisfying the universal approximation theorem. The two key ingredients are an input convex network architecture and a formulation in the elementary polynomials of the signed singular values of the deformation gradient. In line with previously published networks, it can rigorously capture frame-indifference and polyconvexity - as well as further constraints like balance of angular momentum and growth conditions. However and in contrast to previous networks, a universal approximation theorem for the proposed approach is proven. To be more explicit, the proposed network can approximate any frame-indifferent, isotropic polyconvex energy (provided the network is large enough). This is possible by working with a sufficient and necessary criterion for frame-indifferent, isotropic polyconvex functions. Comparative studies with existing approaches identify the advantages of the proposed method, particularly in approximating non-polyconvex energies as well as computing polyconvex hulls. 

**Abstract (ZH)**: 本文提出了一种新颖的神经网络框架，用于各向同性弹性的建模，该框架在满足必要的物理和数学约束的同时，同时也遵循通用逼近定理。两个关键组成部分是输入凸网络架构和变形梯度的符号奇异值的本原多项式形式。与已发布网络一致，该方法能够严格捕获框架无关性和多凸性，以及进一步的约束，如动量矩平衡和生长条件。然而，与之前的方法不同，该方法的通用逼近定理得到了证明。更具体地说，只要网络足够大，所提出的网络可以逼近任何框架无关性、各向同性和多凸能量函数。这可以通过使用框架无关性、各向同性和多凸函数的充分必要条件来实现。与现有方法的比较研究揭示了所提方法的优势，尤其是在逼近非多凸能量和计算多凸包方面。 

---
# FedMHO: Heterogeneous One-Shot Federated Learning Towards Resource-Constrained Edge Devices 

**Title (ZH)**: FedMHO: 面向资源受限边缘设备的异构一次性联邦学习 

**Authors**: Dezhong Yao, Yuexin Shi, Tongtong Liu, Zhiqiang Xu  

**Link**: [PDF](https://arxiv.org/pdf/2502.08518)  

**Abstract**: Federated Learning (FL) is increasingly adopted in edge computing scenarios, where a large number of heterogeneous clients operate under constrained or sufficient resources. The iterative training process in conventional FL introduces significant computation and communication overhead, which is unfriendly for resource-constrained edge devices. One-shot FL has emerged as a promising approach to mitigate communication overhead, and model-heterogeneous FL solves the problem of diverse computing resources across clients. However, existing methods face challenges in effectively managing model-heterogeneous one-shot FL, often leading to unsatisfactory global model performance or reliance on auxiliary datasets. To address these challenges, we propose a novel FL framework named FedMHO, which leverages deep classification models on resource-sufficient clients and lightweight generative models on resource-constrained devices. On the server side, FedMHO involves a two-stage process that includes data generation and knowledge fusion. Furthermore, we introduce FedMHO-MD and FedMHO-SD to mitigate the knowledge-forgetting problem during the knowledge fusion stage, and an unsupervised data optimization solution to improve the quality of synthetic samples. Comprehensive experiments demonstrate the effectiveness of our methods, as they outperform state-of-the-art baselines in various experimental setups. 

**Abstract (ZH)**: 联邦学习（FL）在边缘计算场景中日益采用，其中大量异构客户端在受限或充足的资源下运行。传统FL的迭代训练过程引入了显著的计算和通信开销，这不利于资源受限的边缘设备。一次性FL已成为减轻通信开销的有前景的方法，而模型异构的FL解决了客户端之间计算资源多样性的问题。然而，现有方法在有效管理模型异构的一次性FL方面面临挑战，往往导致全局模型性能不佳或依赖辅助数据集。为了解决这些挑战，我们提出了一种新颖的FL框架FedMHO，该框架在资源充足的客户端上利用深度分类模型，在资源受限的设备上利用轻量级生成模型。在服务器端，FedMHO涉及一个两阶段过程，包括数据生成和知识融合。此外，我们引入了FedMHO-MD和FedMHO-SD来缓解知识融合阶段的知识遗忘问题，并提出了无监督数据优化解决方案以提高合成样本的质量。全面的实验表明，我们的方法在各种实验设置中优于现有最先进的基线方法。 

---
# Measuring Diversity in Synthetic Datasets 

**Title (ZH)**: 合成数据集中多样性测量 

**Authors**: Yuchang Zhu, Huizhe Zhang, Bingzhe Wu, Jintang Li, Zibin Zheng, Peilin Zhao, Liang Chen, Yatao Bian  

**Link**: [PDF](https://arxiv.org/pdf/2502.08512)  

**Abstract**: Large language models (LLMs) are widely adopted to generate synthetic datasets for various natural language processing (NLP) tasks, such as text classification and summarization. However, accurately measuring the diversity of these synthetic datasets-an aspect crucial for robust model performance-remains a significant challenge. In this paper, we introduce DCScore, a novel method for measuring synthetic dataset diversity from a classification perspective. Specifically, DCScore formulates diversity evaluation as a sample classification task, leveraging mutual relationships among samples. We further provide theoretical verification of the diversity-related axioms satisfied by DCScore, highlighting its role as a principled diversity evaluation method. Experimental results on synthetic datasets reveal that DCScore enjoys a stronger correlation with multiple diversity pseudo-truths of evaluated datasets, underscoring its effectiveness. Moreover, both empirical and theoretical evidence demonstrate that DCScore substantially reduces computational costs compared to existing approaches. Code is available at: this https URL. 

**Abstract (ZH)**: 大规模语言模型（LLMs）广泛用于生成各种自然语言处理（NLP）任务的合成数据集，例如文本分类和总结。然而，准确测量这些合成数据集的多样性——这对稳健的模型性能至关重要——仍然是一个重大挑战。本文介绍了一种新的方法DCScore，从分类视角衡量合成数据集的多样性。具体而言，DCScore 将多样性评估转化为一个样本分类任务，利用样本间的相互关系。我们进一步提供了对DCScore满足的与多样性相关的公理的理论验证，突显了它作为一种原理上的多样性评估方法的作用。对合成数据集的实验结果表明，DCScore与多个评估数据集的多样性伪真相具有更强的相关性，证明了其有效性。此外，实证和理论证据表明，DCScore 显著降低了与现有方法相比的计算成本。代码可在: this https URL 获取。 

---
# Enhancing Auto-regressive Chain-of-Thought through Loop-Aligned Reasoning 

**Title (ZH)**: 通过循环对齐推理增强自回归链式思考 

**Authors**: Qifan Yu, Zhenyu He, Sijie Li, Xun Zhou, Jun Zhang, Jingjing Xu, Di He  

**Link**: [PDF](https://arxiv.org/pdf/2502.08482)  

**Abstract**: Chain-of-Thought (CoT) prompting has emerged as a powerful technique for enhancing language model's reasoning capabilities. However, generating long and correct CoT trajectories is challenging. Recent studies have demonstrated that Looped Transformers possess remarkable length generalization capabilities, but their limited generality and adaptability prevent them from serving as an alternative to auto-regressive solutions. To better leverage the strengths of Looped Transformers, we propose RELAY (REasoning through Loop Alignment iterativelY). Specifically, we align the steps of Chain-of-Thought (CoT) reasoning with loop iterations and apply intermediate supervision during the training of Looped Transformers. This additional iteration-wise supervision not only preserves the Looped Transformer's ability for length generalization but also enables it to predict CoT reasoning steps for unseen data. Therefore, we leverage this Looped Transformer to generate accurate reasoning chains for complex problems that exceed the training length, which will then be used to fine-tune an auto-regressive model. We conduct extensive experiments, and the results demonstrate the effectiveness of our approach, with significant improvements in the performance of the auto-regressive model. Code will be released at this https URL. 

**Abstract (ZH)**: Looped Transformer驱动的递归推理方法：RELAY算法及其应用 

---
# Training-Free Restoration of Pruned Neural Networks 

**Title (ZH)**: 剪枝神经网络的无训练恢复 

**Authors**: Keonho Lee, Minsoo Kim, Dong-Wan Choi  

**Link**: [PDF](https://arxiv.org/pdf/2502.08474)  

**Abstract**: Although network pruning has been highly popularized to compress deep neural networks, its resulting accuracy heavily depends on a fine-tuning process that is often computationally expensive and requires the original data. However, this may not be the case in real-world scenarios, and hence a few recent works attempt to restore pruned networks without any expensive retraining process. Their strong assumption is that every neuron being pruned can be replaced with another one quite similar to it, but unfortunately this does not hold in many neural networks, where the similarity between neurons is extremely low in some layers. In this article, we propose a more rigorous and robust method of restoring pruned networks in a fine-tuning free and data-free manner, called LBYL (Leave Before You Leave). LBYL significantly relaxes the aforementioned assumption in a way that each pruned neuron leaves its pieces of information to as many preserved neurons as possible and thereby multiple neurons together obtain a more robust approximation to the original output of the neuron who just left. Our method is based on a theoretical analysis on how to formulate the reconstruction error between the original network and its approximation, which nicely leads to a closed form solution for our derived loss function. Through the extensive experiments, LBYL is confirmed to be indeed more effective to approximate the original network and consequently able to achieve higher accuracy for restored networks, compared to the recent approaches exploiting the similarity between two neurons. The very first version of this work, which contains major technical and theoretical components, was submitted to NeurIPS 2021 and ICML 2022. 

**Abstract (ZH)**: 尽管网络修剪已被广泛用于压缩深度神经网络，但其结果的精度高度依赖于一个经常计算成本高昂且需要原始数据的微调过程。然而，在实际场景中这可能并不总是成立，因此一些近期的工作尝试在没有昂贵的重新训练过程的情况下恢复修剪的网络。他们假设每一个被修剪的神经元都可以被另一个与其非常相似的神经元所替代，但不幸的是，这种情况在很多神经网络中并不成立，因为在某些层中神经元之间的相似性非常低。本文提出了一种在无微调和无数据条件下更严格和robust的方法来恢复修剪的网络，称为LBYL（Leave Before You Leave）。LBYL以一种方式显著放宽了上述假设，即每一个被修剪的神经元尽可能多地向保留的神经元留下它的信息，从而多个神经元一起获得对刚刚离开的神经元原输出的更robust的近似。该方法基于对原始网络与其近似之间的重构误差的理论分析，这自然地导出了我们所推导出的损失函数的闭式解。通过广泛的实验，LBYL确实被证实对近似原始网络更有效，从而能够在恢复的网络中实现更高的精度，相比于利用两个神经元之间相似性的近期方法。本文的第一版，包含主要的技术和理论成分，被提交给了NeurIPS 2021和ICML 2022。 

---
# mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data 

**Title (ZH)**: mmE5: 通过高质量合成数据提高多模态多语言嵌入效果 

**Authors**: Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, Zhicheng Dou  

**Link**: [PDF](https://arxiv.org/pdf/2502.08468)  

**Abstract**: Multimodal embedding models have gained significant attention for their ability to map data from different modalities, such as text and images, into a unified representation space. However, the limited labeled multimodal data often hinders embedding performance. Recent approaches have leveraged data synthesis to address this problem, yet the quality of synthetic data remains a critical bottleneck. In this work, we identify three criteria for high-quality synthetic multimodal data. First, broad scope ensures that the generated data covers diverse tasks and modalities, making it applicable to various downstream scenarios. Second, robust cross-modal alignment makes different modalities semantically consistent. Third, high fidelity ensures that the synthetic data maintains realistic details to enhance its reliability. Guided by these principles, we synthesize datasets that: (1) cover a wide range of tasks, modality combinations, and languages, (2) are generated via a deep thinking process within a single pass of a multimodal large language model, and (3) incorporate real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement. Leveraging these high-quality synthetic and labeled datasets, we train a multimodal multilingual E5 model mmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art performance on the MMEB Benchmark and superior multilingual performance on the XTD benchmark. Our codes, datasets and models are released in this https URL. 

**Abstract (ZH)**: 高质量合成多模态数据的三大标准及其在mmE5模型中的应用 

---
# Proceedings 40th International Conference on Logic Programming 

**Title (ZH)**: 第40届逻辑编程国际会议论文集 

**Authors**: Pedro Cabalar, Francesco Fabiano, Martin Gebser, Gopal Gupta, Theresa Swift  

**Link**: [PDF](https://arxiv.org/pdf/2502.08453)  

**Abstract**: Since the first conference In Marseille in 1982, the International Conference on Logic Programming (ICLP) has been the premier international event for presenting research in logic programming. These proceedings include technical communications about, and abstracts for presentations given at the 40th ICLP held October 14-17, in Dallas Texas, USA. The papers and abstracts in this volume include the following areas and topics.  Formal and operational semantics: including non-monotonic reasoning, probabilistic reasoning, argumentation, and semantic issues of combining logic with neural models.  Language design and programming methodologies such as answer set programming. inductive logic programming, and probabilistic programming. Program analysis and logic-based validation of generated programs.  Implementation methodologies including constraint implementation, tabling, Logic-based prompt engineering, and the interaction of logic programming with LLMs. 

**Abstract (ZH)**: 自1982年在马赛举行的首次会议以来，国际逻辑编程会议（ICLP）一直是展示逻辑编程研究的 premier 国际活动。这些 proceedings 收录了在2023年10月14-17日于美国德克萨斯州达拉斯举行的第40届ICLP的technical communications 和摘要。本卷中的论文和摘要涵盖了如下领域和主题：正式语义和操作语义：包括非单调推理、概率推理、论辩和将逻辑与神经模型结合的语义问题。语言设计和编程方法，如回答集编程、归纳逻辑编程和概率编程。程序分析和基于逻辑验证生成程序。实现方法，包括约束实现、表查询、基于逻辑的提示工程以及逻辑编程与LLM的交互。 

---
# Towards Prompt Generalization: Grammar-aware Cross-Prompt Automated Essay Scoring 

**Title (ZH)**: 面向提示泛化的语法意识跨提示自动作文评分 

**Authors**: Heejin Do, Taehee Park, Sangwon Ryu, Gary Geunbae Lee  

**Link**: [PDF](https://arxiv.org/pdf/2502.08450)  

**Abstract**: In automated essay scoring (AES), recent efforts have shifted toward cross-prompt settings that score essays on unseen prompts for practical applicability. However, prior methods trained with essay-score pairs of specific prompts pose challenges in obtaining prompt-generalized essay representation. In this work, we propose a grammar-aware cross-prompt trait scoring (GAPS), which internally captures prompt-independent syntactic aspects to learn generic essay representation. We acquire grammatical error-corrected information in essays via the grammar error correction technique and design the AES model to seamlessly integrate such information. By internally referring to both the corrected and the original essays, the model can focus on generic features during training. Empirical experiments validate our method's generalizability, showing remarkable improvements in prompt-independent and grammar-related traits. Furthermore, GAPS achieves notable QWK gains in the most challenging cross-prompt scenario, highlighting its strength in evaluating unseen prompts. 

**Abstract (ZH)**: 在自动作文评分（AES）中，最近的努力转向了针对未见过的提示进行评分的跨提示设置，以提高其实用性。然而，先前使用特定提示的作文-评分对进行训练的方法在获得提示泛化的作文表示时面临挑战。在本工作中，我们提出了一种语法感知的跨提示特性评分（GAPS），它内部捕获与提示无关的句法方面，以学习通用的作文表示。我们通过语法错误纠正技术获取作文中的语法错误修正信息，并设计AES模型以无缝集成此类信息。通过内部同时参考修正后的和原始的作文，模型可以在训练过程中专注于通用特征。实验验证了该方法的泛化能力，展示了在提示无关和语法相关的特性上显著的改进。此外，GAPS 在最具挑战性的跨提示场景中实现了显著的QWK提升，突显了其在评估未见过的提示方面的强大能力。 

---
# CordViP: Correspondence-based Visuomotor Policy for Dexterous Manipulation in Real-World 

**Title (ZH)**: 基于对应关系的视觉运动策略及其在真实世界中的灵巧 manipulation 

**Authors**: Yankai Fu, Qiuxuan Feng, Ning Chen, Zichen Zhou, Mengzhen Liu, Mingdong Wu, Tianxing Chen, Shanyu Rong, Jiaming Liu, Hao Dong, Shanghang Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08449)  

**Abstract**: Achieving human-level dexterity in robots is a key objective in the field of robotic manipulation. Recent advancements in 3D-based imitation learning have shown promising results, providing an effective pathway to achieve this goal. However, obtaining high-quality 3D representations presents two key problems: (1) the quality of point clouds captured by a single-view camera is significantly affected by factors such as camera resolution, positioning, and occlusions caused by the dexterous hand; (2) the global point clouds lack crucial contact information and spatial correspondences, which are necessary for fine-grained dexterous manipulation tasks. To eliminate these limitations, we propose CordViP, a novel framework that constructs and learns correspondences by leveraging the robust 6D pose estimation of objects and robot proprioception. Specifically, we first introduce the interaction-aware point clouds, which establish correspondences between the object and the hand. These point clouds are then used for our pre-training policy, where we also incorporate object-centric contact maps and hand-arm coordination information, effectively capturing both spatial and temporal dynamics. Our method demonstrates exceptional dexterous manipulation capabilities with an average success rate of 90\% in four real-world tasks, surpassing other baselines by a large margin. Experimental results also highlight the superior generalization and robustness of CordViP to different objects, viewpoints, and scenarios. Code and videos are available on this https URL. 

**Abstract (ZH)**: 实现人类水平灵巧操作是机器人操作领域的关键目标。基于3D的模仿学习的最新进展取得了有前景的结果，提供了一种有效的途径来实现这一目标。然而，获取高质量的3D表示存在两个关键问题：（1）单视角相机捕获的点云质量受相机分辨率、位置以及灵巧手引起的遮挡等因素显著影响；（2）全局点云缺乏关键的接触信息和空间对应关系，这些对于精细的灵巧操作任务是必要的。为了解决这些限制，我们提出了一种名为CordViP的新框架，通过利用对象和机器人自身感知的鲁棒6D姿态估计来构建和学习对应关系。具体而言，我们首先引入了交互感知点云，建立对象与手之间的对应关系。这些点云随后用于我们的预训练策略，我们在其中还结合了以对象为中心的接触图和手-臂协调信息，有效捕捉了空间和时间动态。我们的方法在四个实际任务中的平均成功率为90%，显著优于其他基线方法。实验结果还突显了CordViP在不同对象、视角和场景下的更强的泛化能力和鲁棒性。代码和视频可在以下链接获取。 

---
# Better Embeddings with Coupled Adam 

**Title (ZH)**: 耦合Adam优化器的更好embedding 

**Authors**: Felix Stollenwerk, Tobias Stollenwerk  

**Link**: [PDF](https://arxiv.org/pdf/2502.08441)  

**Abstract**: Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is a cause of anisotropic embeddings, and suggest a modified optimizer called Coupled Adam to mitigate the problem. Our experiments demonstrate that Coupled Adam significantly improves the quality of embeddings, while also leading to better upstream and downstream performance on large enough datasets. 

**Abstract (ZH)**: 尽管大型语言模型具备卓越的能力，但在学习词表示时会表现出未经良好理解的各向异性特征。本文认为Adam的第二 moment是导致各向异性嵌入的原因，并提出了一种修饰优化器Coupled Adam以缓解该问题。我们的实验表明，Coupled Adam显著提高了嵌入的质量，并在足够大的数据集上提高了上层和下游任务的性能。 

---
# Composite Sketch+Text Queries for Retrieving Objects with Elusive Names and Complex Interactions 

**Title (ZH)**: 具有模糊名称和复杂交互对象的综合草图+文本查询检索 

**Authors**: Prajwal Gatti, Kshitij Parikh, Dhriti Prasanna Paul, Manish Gupta, Anand Mishra  

**Link**: [PDF](https://arxiv.org/pdf/2502.08438)  

**Abstract**: Non-native speakers with limited vocabulary often struggle to name specific objects despite being able to visualize them, e.g., people outside Australia searching for numbats. Further, users may want to search for such elusive objects with difficult-to-sketch interactions, e.g., numbat digging in the ground. In such common but complex situations, users desire a search interface that accepts composite multimodal queries comprising hand-drawn sketches of difficult-to-name but easy-to-draw objects and text describing difficult-to-sketch but easy-to-verbalize object attributes or interaction with the scene. This novel problem statement distinctly differs from the previously well-researched TBIR (text-based image retrieval) and SBIR (sketch-based image retrieval) problems. To study this under-explored task, we curate a dataset, CSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M queries and 108K natural scene images. Further, as a solution to this problem, we propose a pretrained multimodal transformer-based baseline, STNET (Sketch+Text Network), that uses a hand-drawn sketch to localize relevant objects in the natural scene image, and encodes the text and image to perform image retrieval. In addition to contrastive learning, we propose multiple training objectives that improve the performance of our model. Extensive experiments show that our proposed method outperforms several state-of-the-art retrieval methods for text-only, sketch-only, and composite query modalities. We make the dataset and code available at our project website. 

**Abstract (ZH)**: 非母语者因词汇量有限，在命名特定物体时往往存在困难，尽管他们能够想象这些物体，例如澳大利亚以外的人搜索数batim。此外，用户可能希望使用难以绘制的交互方式来查找这些难以描述的物体，例如数batim在地面挖洞。在这种常见但复杂的场景下，用户希望有一个能够接受组合多模态查询的搜索界面，这些查询包含难以命名但容易绘制的物体的手绘草图，以及描述难以绘制但容易描述的物体属性或与场景的交互的文本。这一新颖的问题陈述与此前广泛研究的TBIR（基于文本的图像检索）和SBIR（基于素描的图像检索）问题大不相同。为了研究这一尚未充分探索的任务，我们收集了一个数据集CSTBIR（组合素描+文本基于图像检索），包含约200万查询和10.8万自然场景图像。此外，为了解决这一问题，我们提出了一种预训练的多模态变压器基线模型STNET（素描+文本网络），该模型利用手绘草图在自然场景图像中定位相关物体，并编码文本和图像以进行图像检索。除了对比学习外，我们还提出多个训练目标以提高模型性能。广泛实验表明，我们提出的方法在仅文本、仅素描和组合查询模式下优于多种最新检索方法。我们在项目网站上提供了数据集和代码。 

---
# From Haystack to Needle: Label Space Reduction for Zero-shot Classification 

**Title (ZH)**: 从针haystack中筛选：零样本分类的标签空间缩减 

**Authors**: Nathan Vandemoortele, Bram Steenwinckel, Femke Ongenae, Sofie Van Hoecke  

**Link**: [PDF](https://arxiv.org/pdf/2502.08436)  

**Abstract**: We present Label Space Reduction (LSR), a novel method for improving zero-shot classification performance of Large Language Models (LLMs). LSR iteratively refines the classification label space by systematically ranking and reducing candidate classes, enabling the model to concentrate on the most relevant options. By leveraging unlabeled data with the statistical learning capabilities of data-driven models, LSR dynamically optimizes the label space representation at test time. Our experiments across seven benchmarks demonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to 14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet compared to standard zero-shot classification baselines. To reduce the computational overhead of LSR, which requires an additional LLM call at each iteration, we propose distilling the model into a probabilistic classifier, allowing for efficient inference. 

**Abstract (ZH)**: 我们呈现了一种新颖的方法Label Space Reduction (LSR)，用于提升大型语言模型（LLMs）的零样本分类性能。LSR通过系统地排序和减少候选类别，逐步精炼分类标签空间，使模型能够集中于最相关的选项。通过利用数据驱动模型的统计学习能力，LSR在测试时动态优化标签空间表示。我们在七个基准上的实验表明，与标准零样本分类基线相比，LSR在Llama-3.1-70B上的宏-F1分数平均提高了7.0%（最高14.2%），在Claude-3.5-Sonnet上的平均提高了3.3%（最高11.1%）。为了减少LSR的计算开销，每次迭代都需要额外的LLM调用，我们提出将模型提炼为概率分类器，以实现高效推理。 

---
# Handwritten Text Recognition: A Survey 

**Title (ZH)**: 手写文本识别：一种综述 

**Authors**: Carlos Garrido-Munoz, Antonio Rios-Vila, Jorge Calvo-Zaragoza  

**Link**: [PDF](https://arxiv.org/pdf/2502.08417)  

**Abstract**: Handwritten Text Recognition (HTR) has become an essential field within pattern recognition and machine learning, with applications spanning historical document preservation to modern data entry and accessibility solutions. The complexity of HTR lies in the high variability of handwriting, which makes it challenging to develop robust recognition systems. This survey examines the evolution of HTR models, tracing their progression from early heuristic-based approaches to contemporary state-of-the-art neural models, which leverage deep learning techniques. The scope of the field has also expanded, with models initially capable of recognizing only word-level content progressing to recent end-to-end document-level approaches. Our paper categorizes existing work into two primary levels of recognition: (1) \emph{up to line-level}, encompassing word and line recognition, and (2) \emph{beyond line-level}, addressing paragraph- and document-level challenges. We provide a unified framework that examines research methodologies, recent advances in benchmarking, key datasets in the field, and a discussion of the results reported in the literature. Finally, we identify pressing research challenges and outline promising future directions, aiming to equip researchers and practitioners with a roadmap for advancing the field. 

**Abstract (ZH)**: 手写文本识别（HTR）已成为模式识别和机器学习领域中的一个重要研究方向，其应用范围从历史文献保护扩展到现代数据录入和无障碍解决方案。手写文本识别的复杂性在于手写高度变化，这使得开发稳健的识别系统极具挑战性。本文综述了手写文本识别模型的发展历程，从早期基于启发式的方法逐步演进到当前利用深度学习技术的先进神经网络模型。该领域的研究范围也得到了扩展，从仅能识别单词级别的内容逐步发展到最近的端到端文档级方法。本文将现有工作分为两个主要的识别级别：(1) 线性级别以下，包括单词和行的识别；(2) 线性级别以上，解决段落级和文档级的挑战。我们提供了一个统一的框架来审视研究方法、最近的 benchmarking 进展、领域内的关键数据集以及文献中报告的结果。最后，我们指出了亟待解决的研究挑战，并概述了有潜力的未来发展方向，旨在为研究者和实践者提供一个推动该领域发展的路线图。 

---
# Learning Humanoid Standing-up Control across Diverse Postures 

**Title (ZH)**: 学习 humanoid 站立起立控制 Across Diverse 姿态 

**Authors**: Tao Huang, Junli Ren, Huayi Wang, Zirui Wang, Qingwei Ben, Muning Wen, Xiao Chen, Jianan Li, Jiangmiao Pang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08378)  

**Abstract**: Standing-up control is crucial for humanoid robots, with the potential for integration into current locomotion and loco-manipulation systems, such as fall recovery. Existing approaches are either limited to simulations that overlook hardware constraints or rely on predefined ground-specific motion trajectories, failing to enable standing up across postures in real-world scenes. To bridge this gap, we present HoST (Humanoid Standing-up Control), a reinforcement learning framework that learns standing-up control from scratch, enabling robust sim-to-real transfer across diverse postures. HoST effectively learns posture-adaptive motions by leveraging a multi-critic architecture and curriculum-based training on diverse simulated terrains. To ensure successful real-world deployment, we constrain the motion with smoothness regularization and implicit motion speed bound to alleviate oscillatory and violent motions on physical hardware, respectively. After simulation-based training, the learned control policies are directly deployed on the Unitree G1 humanoid robot. Our experimental results demonstrate that the controllers achieve smooth, stable, and robust standing-up motions across a wide range of laboratory and outdoor environments. Videos are available at this https URL. 

**Abstract (ZH)**: Humanoid Standing-up Control: A Reinforcement Learning Framework for Robust Sim-to-Real Transfer Across Diverse Postures 

---
# Uncertainty Aware Human-machine Collaboration in Camouflaged Object Detection 

**Title (ZH)**: 面向伪装目标检测的不确定性感知人机协作 

**Authors**: Ziyue Yang, Kehan Wang, Yuhang Ming, Yong Peng, Han Yang, Qiong Chen, Wanzeng Kong  

**Link**: [PDF](https://arxiv.org/pdf/2502.08373)  

**Abstract**: Camouflaged Object Detection (COD), the task of identifying objects concealed within their environments, has seen rapid growth due to its wide range of practical applications. A key step toward developing trustworthy COD systems is the estimation and effective utilization of uncertainty. In this work, we propose a human-machine collaboration framework for classifying the presence of camouflaged objects, leveraging the complementary strengths of computer vision (CV) models and noninvasive brain-computer interfaces (BCIs). Our approach introduces a multiview backbone to estimate uncertainty in CV model predictions, utilizes this uncertainty during training to improve efficiency, and defers low-confidence cases to human evaluation via RSVP-based BCIs during testing for more reliable decision-making. We evaluated the framework in the CAMO dataset, achieving state-of-the-art results with an average improvement of 4.56\% in balanced accuracy (BA) and 3.66\% in the F1 score compared to existing methods. For the best-performing participants, the improvements reached 7.6\% in BA and 6.66\% in the F1 score. Analysis of the training process revealed a strong correlation between our confidence measures and precision, while an ablation study confirmed the effectiveness of the proposed training policy and the human-machine collaboration strategy. In general, this work reduces human cognitive load, improves system reliability, and provides a strong foundation for advancements in real-world COD applications and human-computer interaction. Our code and data are available at: this https URL. 

**Abstract (ZH)**: 伪装目标检测中的可信人类-机器协作框架：利用计算机视觉模型和无侵入式脑-计算机接口的优势 

---
# Towards Principled Multi-Agent Task Agnostic Exploration 

**Title (ZH)**: 面向原则的多智能体任务无关探索 

**Authors**: Riccardo Zamboni, Mirco Mutti, Marcello Restelli  

**Link**: [PDF](https://arxiv.org/pdf/2502.08365)  

**Abstract**: In reinforcement learning, we typically refer to task-agnostic exploration when we aim to explore the environment without access to the task specification a priori. In a single-agent setting the problem has been extensively studied and mostly understood. A popular approach cast the task-agnostic objective as maximizing the entropy of the state distribution induced by the agent's policy, from which principles and methods follows. In contrast, little is known about task-agnostic exploration in multi-agent settings, which are ubiquitous in the real world. How should different agents explore in the presence of others? In this paper, we address this question through a generalization to multiple agents of the problem of maximizing the state distribution entropy. First, we investigate alternative formulations, highlighting respective positives and negatives. Then, we present a scalable, decentralized, trust-region policy search algorithm to address the problem in practical settings. Finally, we provide proof of concept experiments to both corroborate the theoretical findings and pave the way for task-agnostic exploration in challenging multi-agent settings. 

**Abstract (ZH)**: 在强化学习中，当我们旨在在无法预先获取任务规范的情况下探索环境时，通常称之为任务无关探索。在单-agent设置中，该问题已被广泛研究并基本理解。一种流行的方法是将任务无关目标定义为最大化由代理策略诱导的状态分布熵，从而得出相应的原则和方法。相比之下，在多-agent设置中，这种情况在现实世界中普遍存在，但对其了解甚少。面对其他代理时，不同代理应该如何探索？在本文中，我们通过将最大化状态分布熵的问题推广到多代理场景来回答这个问题。首先，我们探讨了不同的问题表述，并突出各自的优点和缺点。然后，我们提出了一种可扩展的、去中心化的、信任区域策略搜索算法，以解决实际场景中的问题。最后，我们提供了概念验证实验，既验证了理论发现，也为具有挑战性的多-agent场景中的任务无关探索铺平了道路。 

---
# Top-Theta Attention: Sparsifying Transformers by Compensated Thresholding 

**Title (ZH)**: Top-Theta 注意力：通过补偿阈值化稀疏化 Transformers 

**Authors**: Konstantin Berestizshevsky, Renzo Andri, Lukas Cavigelli  

**Link**: [PDF](https://arxiv.org/pdf/2502.08363)  

**Abstract**: The attention mechanism is essential for the impressive capabilities of transformer-based Large Language Models (LLMs). However, calculating attention is computationally intensive due to its quadratic dependency on the sequence length. We introduce a novel approach called Top-Theta Attention, or simply Top-$\theta$, which selectively prunes less essential attention elements by comparing them against carefully calibrated thresholds. This method greatly improves the efficiency of self-attention matrix multiplication while preserving model accuracy, reducing the number of required V cache rows by 3x during generative decoding and the number of attention elements by 10x during the prefill phase. Our method does not require model retraining; instead, it requires only a brief calibration phase to be resilient to distribution shifts, thus not requiring the thresholds for different datasets to be recalibrated. Unlike top-k attention, Top-$\theta$ eliminates full-vector dependency, making it suitable for tiling and scale-out and avoiding costly top-k search. A key innovation of our approach is the development of efficient numerical compensation techniques, which help preserve model accuracy even under aggressive pruning of attention scores. 

**Abstract (ZH)**: Top-Theta 注意机制：一种通过精心校准阈值选择性剪枝的关键注意力元素的方法 

---
# Trustworthy GNNs with LLMs: A Systematic Review and Taxonomy 

**Title (ZH)**: 基于LLMs的可信GNNs：系统的综述与分类 

**Authors**: Ruizhan Xue, Huimin Deng, Fang He, Maojun Wang, Zeyu Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08353)  

**Abstract**: With the extensive application of Graph Neural Networks (GNNs) across various domains, their trustworthiness has emerged as a focal point of research. Some existing studies have shown that the integration of large language models (LLMs) can improve the semantic understanding and generation capabilities of GNNs, which in turn improves the trustworthiness of GNNs from various aspects. Our review introduces a taxonomy that offers researchers a clear framework for comprehending the principles and applications of different methods and helps clarify the connections and differences among various approaches. Then we systematically survey representative approaches along the four categories of our taxonomy. Through our taxonomy, researchers can understand the applicable scenarios, potential advantages, and limitations of each approach for the the trusted integration of GNNs with LLMs. Finally, we present some promising directions of work and future trends for the integration of LLMs and GNNs to improve model trustworthiness. 

**Abstract (ZH)**: 随着图神经网络（GNNs）在各个领域中的广泛应用，其可信度已成为研究的焦点。一些现有研究显示，将大型语言模型（LLMs）的集成可以提升GNNs的语义理解和生成能力，从而从多个方面改善GNNs的可信度。我们的综述提出了一种分类体系，为研究人员提供了一个清晰的框架来理解不同方法的原理和应用，并帮助明确各种方法之间的联系与差异。然后，我们系统地调研了分类体系四大类别中的代表性方法。通过我们的分类体系，研究人员可以了解每种方法在可信集成GNNs与LLMs中的适用场景、潜在优势和局限性。最后，我们提出了LLMs和GNNs集成以提高模型可信度的一些有希望的研究方向和未来趋势。 

---
# Graph Foundation Models for Recommendation: A Comprehensive Survey 

**Title (ZH)**: 图基础模型推荐：一项综合综述 

**Authors**: Bin Wu, Yihang Wang, Yuanhao Zeng, Jiawei Liu, Jiashu Zhao, Cheng Yang, Yawen Li, Long Xia, Dawei Yin, Chuan Shi  

**Link**: [PDF](https://arxiv.org/pdf/2502.08346)  

**Abstract**: Recommender systems (RS) serve as a fundamental tool for navigating the vast expanse of online information, with deep learning advancements playing an increasingly important role in improving ranking accuracy. Among these, graph neural networks (GNNs) excel at extracting higher-order structural information, while large language models (LLMs) are designed to process and comprehend natural language, making both approaches highly effective and widely adopted. Recent research has focused on graph foundation models (GFMs), which integrate the strengths of GNNs and LLMs to model complex RS problems more efficiently by leveraging the graph-based structure of user-item relationships alongside textual understanding. In this survey, we provide a comprehensive overview of GFM-based RS technologies by introducing a clear taxonomy of current approaches, diving into methodological details, and highlighting key challenges and future directions. By synthesizing recent advancements, we aim to offer valuable insights into the evolving landscape of GFM-based recommender systems. 

**Abstract (ZH)**: 基于图的推荐系统：利用图神经网络和大规模语言模型的技术综述 

---
# Hierarchical Learning-based Graph Partition for Large-scale Vehicle Routing Problems 

**Title (ZH)**: 基于层级学习的图划分方法解决大规模车辆路径问题 

**Authors**: Yuxin Pan, Ruohong Liu, Yize Chen, Zhiguang Cao, Fangzhen Lin  

**Link**: [PDF](https://arxiv.org/pdf/2502.08340)  

**Abstract**: Neural solvers based on the divide-and-conquer approach for Vehicle Routing Problems (VRPs) in general, and capacitated VRP (CVRP) in particular, integrates the global partition of an instance with local constructions for each subproblem to enhance generalization. However, during the global partition phase, misclusterings within subgraphs have a tendency to progressively compound throughout the multi-step decoding process of the learning-based partition policy. This suboptimal behavior in the global partition phase, in turn, may lead to a dramatic deterioration in the performance of the overall decomposition-based system, despite using optimal local constructions. To address these challenges, we propose a versatile Hierarchical Learning-based Graph Partition (HLGP) framework, which is tailored to benefit the partition of CVRP instances by synergistically integrating global and local partition policies. Specifically, the global partition policy is tasked with creating the coarse multi-way partition to generate the sequence of simpler two-way partition subtasks. These subtasks mark the initiation of the subsequent K local partition levels. At each local partition level, subtasks exclusive for this level are assigned to the local partition policy which benefits from the insensitive local topological features to incrementally alleviate the compounded errors. This framework is versatile in the sense that it optimizes the involved partition policies towards a unified objective harmoniously compatible with both reinforcement learning (RL) and supervised learning (SL). (*Due to the notification of arXiv "The Abstract field cannot be longer than 1,920 characters", the appeared Abstract is shortened. For the full Abstract, please download the Article.) 

**Abstract (ZH)**: 基于分而治之方法的神经网络求解器在一般车辆路线问题（VRP）及容量受限车辆路线问题（CVRP）中的全局划分与局部构建结合增强泛化能力的研究 

---
# Hierarchical Multi-Agent Framework for Carbon-Efficient Liquid-Cooled Data Center Clusters 

**Title (ZH)**: 面向低碳液冷数据中心集群的分层多Agent框架 

**Authors**: Soumyendu Sarkar, Avisek Naug, Antonio Guillen, Vineet Gundecha, Ricardo Luna Gutierrez, Sahand Ghorbanpour, Sajad Mousavi, Ashwin Ramesh Babu, Desik Rengarajan, Cullen Bash  

**Link**: [PDF](https://arxiv.org/pdf/2502.08337)  

**Abstract**: Reducing the environmental impact of cloud computing requires efficient workload distribution across geographically dispersed Data Center Clusters (DCCs) and simultaneously optimizing liquid and air (HVAC) cooling with time shift of workloads within individual data centers (DC). This paper introduces Green-DCC, which proposes a Reinforcement Learning (RL) based hierarchical controller to optimize both workload and liquid cooling dynamically in a DCC. By incorporating factors such as weather, carbon intensity, and resource availability, Green-DCC addresses realistic constraints and interdependencies. We demonstrate how the system optimizes multiple data centers synchronously, enabling the scope of digital twins, and compare the performance of various RL approaches based on carbon emissions and sustainability metrics while also offering a framework and benchmark simulation for broader ML research in sustainability. 

**Abstract (ZH)**: 减少云 computing 对环境的影响需要高效地在地理分散的数据中心群组（DCCs）之间分配工作负载，并同时优化液体冷却和空气（HVAC）冷却，通过个体数据中心内部工作负载的时间转移来实现。本文介绍了 Green-DCC，该方法提出了一种基于强化学习（RL）的分层控制器，以动态优化数据中心群组中的工作负载和液体冷却。通过纳入天气、碳强度和资源可用性等因素，Green-DCC 应对了现实中的约束和相互依赖性。我们展示了系统如何同步优化多个数据中心，扩展数字孪生的应用范围，并基于碳排放和可持续性指标比较了不同 RL 方法的性能，同时提供了一个针对可持续性研究更广泛 ML 研究的框架和基准仿真。 

---
# Modification and Generated-Text Detection: Achieving Dual Detection Capabilities for the Outputs of LLM by Watermark 

**Title (ZH)**: 基于水印实现LLM输出的双重检测能力：修改与生成文本检测 

**Authors**: Yuhang Cai, Yaofei Wang, Donghui Hu, Gu Chen  

**Link**: [PDF](https://arxiv.org/pdf/2502.08332)  

**Abstract**: The development of large language models (LLMs) has raised concerns about potential misuse. One practical solution is to embed a watermark in the text, allowing ownership verification through watermark extraction. Existing methods primarily focus on defending against modification attacks, often neglecting other spoofing attacks. For example, attackers can alter the watermarked text to produce harmful content without compromising the presence of the watermark, which could lead to false attribution of this malicious content to the LLM. This situation poses a serious threat to the LLMs service providers and highlights the significance of achieving modification detection and generated-text detection simultaneously. Therefore, we propose a technique to detect modifications in text for unbiased watermark which is sensitive to modification. We introduce a new metric called ``discarded tokens", which measures the number of tokens not included in watermark detection. When a modification occurs, this metric changes and can serve as evidence of the modification. Additionally, we improve the watermark detection process and introduce a novel method for unbiased watermark. Our experiments demonstrate that we can achieve effective dual detection capabilities: modification detection and generated-text detection by watermark. 

**Abstract (ZH)**: 大型语言模型的发展引发了关于潜在滥用的担忧。一种实用的解决方案是将水印嵌入文本中，通过水印提取进行所有权验证。现有方法主要侧重于防御修改攻击，往往忽视了其他欺骗性攻击。例如，攻击者可以修改带水印的文本以生成有害内容，而不破坏水印的存在，这可能导致将这种恶意内容错误地归咎于大型语言模型。这种状况对大型语言模型的服务提供商构成了严重威胁，并突显了同时实现修改检测和生成文本检测的重要性。因此，我们提出了一种技术来检测带有对修改敏感的水印的文本中的修改。我们引入了一个新的指标称为“被丢弃的令牌”，衡量未包含在水印检测中的令牌数量。发生修改时，该指标会发生变化，可以作为修改的证据。此外，我们改进了水印检测过程，并引入了一种新的无偏水印方法。我们的实验表明，我们可以通过水印实现有效的双重检测能力：修改检测和生成文本检测。 

---
# Mitigating Hallucinations in Multimodal Spatial Relations through Constraint-Aware Prompting 

**Title (ZH)**: 通过约束意识型提示减轻多模态空间关系中的幻觉现象 

**Authors**: Jiarui Wu, Zhuo Liu, Hangfeng He  

**Link**: [PDF](https://arxiv.org/pdf/2502.08317)  

**Abstract**: Spatial relation hallucinations pose a persistent challenge in large vision-language models (LVLMs), leading to generate incorrect predictions about object positions and spatial configurations within an image. To address this issue, we propose a constraint-aware prompting framework designed to reduce spatial relation hallucinations. Specifically, we introduce two types of constraints: (1) bidirectional constraint, which ensures consistency in pairwise object relations, and (2) transitivity constraint, which enforces relational dependence across multiple objects. By incorporating these constraints, LVLMs can produce more spatially coherent and consistent outputs. We evaluate our method on three widely-used spatial relation datasets, demonstrating performance improvements over existing approaches. Additionally, a systematic analysis of various bidirectional relation analysis choices and transitivity reference selections highlights greater possibilities of our methods in incorporating constraints to mitigate spatial relation hallucinations. 

**Abstract (ZH)**: 空间关系幻觉是大型视觉语言模型（LVLMs）面临的一个持续性挑战，导致模型在生成图像中对象位置和空间配置的错误预测。为解决这一问题，我们提出了一种约束感知的提示框架，旨在减少空间关系幻觉。具体而言，我们引入了两种类型的约束：（1）双向约束，确保对象对之间的关系一致；（2）传递性约束， enforced 多个对象间的关系依赖。通过纳入这些约束，LVLMs 可以生成更具空间一致性和连贯性的输出。我们在三个广泛使用的空间关系数据集上评估了我们的方法，显示出性能改进。此外，对各种双向关系分析选择和传递性引用选择的系统分析表明，我们的方法具有更大的能力来纳入约束以减轻空间关系幻觉。 

---
# HDT: Hierarchical Discrete Transformer for Multivariate Time Series Forecasting 

**Title (ZH)**: 多层次离散变换器多变量时间序列预测 

**Authors**: Shibo Feng, Peilin Zhao, Liu Liu, Pengcheng Wu, Zhiqi Shen  

**Link**: [PDF](https://arxiv.org/pdf/2502.08302)  

**Abstract**: Generative models have gained significant attention in multivariate time series forecasting (MTS), particularly due to their ability to generate high-fidelity samples. Forecasting the probability distribution of multivariate time series is a challenging yet practical task. Although some recent attempts have been made to handle this task, two major challenges persist: 1) some existing generative methods underperform in high-dimensional multivariate time series forecasting, which is hard to scale to higher dimensions; 2) the inherent high-dimensional multivariate attributes constrain the forecasting lengths of existing generative models. In this paper, we point out that discrete token representations can model high-dimensional MTS with faster inference time, and forecasting the target with long-term trends of itself can extend the forecasting length with high accuracy. Motivated by this, we propose a vector quantized framework called Hierarchical Discrete Transformer (HDT) that models time series into discrete token representations with l2 normalization enhanced vector quantized strategy, in which we transform the MTS forecasting into discrete tokens generation. To address the limitations of generative models in long-term forecasting, we propose a hierarchical discrete Transformer. This model captures the discrete long-term trend of the target at the low level and leverages this trend as a condition to generate the discrete representation of the target at the high level that introduces the features of the target itself to extend the forecasting length in high-dimensional MTS. Extensive experiments on five popular MTS datasets verify the effectiveness of our proposed method. 

**Abstract (ZH)**: 生成模型在多变量时间序列预测中的应用取得了显著关注，尤其是在生成高保真样本方面。预测多变量时间序列的概率分布是一项具有挑战性但实际意义重大的任务。尽管最近已经有一些尝试处理这个问题，但仍然存在两大挑战：1）一些现有的生成方法在高维多变量时间序列预测中表现不佳，难以扩展到更高维度；2）固有的高维多变量属性限制了现有生成模型的预测长度。在本文中，我们指出，离散标记表示可以在加快推理时间的同时表示高维多变量时间序列，并通过长期趋势预测目标可以大大提高预测长度的准确性。受此启发，我们提出了一种称为分层离散变换器（HDT）的向量量化框架，该框架使用L2归一化的增强向量量化策略将时间序列转换为离散标记表示，从而将多变量时间序列预测转化为离散标记生成。为了克服生成模型在长期预测中的局限性，我们提出了一种分层离散变换器模型。该模型在低层捕捉目标的离散长期趋势，并利用这种趋势作为条件生成高层的目标离散表示，从而在高维多变量时间序列中引入目标自身的特征来延长预测长度。广泛的实验在五个流行的多变量时间序列数据集上验证了我们提出方法的有效性。 

---
# Compromising Honesty and Harmlessness in Language Models via Deception Attacks 

**Title (ZH)**: 通过欺骗攻击牺牲语言模型的诚实性和无害性 

**Authors**: Laurène Vaugrante, Francesca Carlon, Maluna Menke, Thilo Hagendorff  

**Link**: [PDF](https://arxiv.org/pdf/2502.08301)  

**Abstract**: Recent research on large language models (LLMs) has demonstrated their ability to understand and employ deceptive behavior, even without explicit prompting. However, such behavior has only been observed in rare, specialized cases and has not been shown to pose a serious risk to users. Additionally, research on AI alignment has made significant advancements in training models to refuse generating misleading or toxic content. As a result, LLMs generally became honest and harmless. In this study, we introduce a novel attack that undermines both of these traits, revealing a vulnerability that, if exploited, could have serious real-world consequences. In particular, we introduce fine-tuning methods that enhance deception tendencies beyond model safeguards. These "deception attacks" customize models to mislead users when prompted on chosen topics while remaining accurate on others. Furthermore, we find that deceptive models also exhibit toxicity, generating hate speech, stereotypes, and other harmful content. Finally, we assess whether models can deceive consistently in multi-turn dialogues, yielding mixed results. Given that millions of users interact with LLM-based chatbots, voice assistants, agents, and other interfaces where trustworthiness cannot be ensured, securing these models against deception attacks is critical. 

**Abstract (ZH)**: 近期对大规模语言模型（LLMs）的研究表明，它们有能力理解和运用欺骗行为，即使没有明确提示。然而，此类行为仅在少数特殊案例中被观察到，并未表现出对用户的严重威胁。此外，关于AI对齐的研究取得了显著进展，训练模型以拒绝生成误导性或有毒内容。因此，LLMs一般表现得诚实而无害。在本研究中，我们介绍了一种新颖的攻击方法，破坏了这两种特性，揭示了一种漏洞，如果被利用，可能会导致严重的现实后果。特别是，我们介绍了增强欺骗倾向的微调方法，这些“欺骗攻击”可以针对选定话题使模型误导用户，同时在其他方面保持准确。此外，我们发现欺骗性模型还表现出毒性，生成仇恨言论、刻板印象和其他有害内容。最后，我们评估了模型在多轮对话中能否一致地欺骗，结果参差不齐。鉴于数百万用户与基于LLM的聊天机器人、语音助手、代理以及其他无法确保可信度的接口进行互动，抵御欺骗攻击对这些模型的安全至关重要。 

---
# CRISP: A Framework for Cryo-EM Image Segmentation and Processing with Conditional Random Field 

**Title (ZH)**: CRISP: 一种基于条件随机场的冷冻电镜图像分割与处理框架 

**Authors**: Szu-Chi Chung, Po-Cheng Chou  

**Link**: [PDF](https://arxiv.org/pdf/2502.08287)  

**Abstract**: Differentiating signals from the background in micrographs is a critical initial step for cryogenic electron microscopy (cryo-EM), yet it remains laborious due to low signal-to-noise ratio (SNR), the presence of contaminants and densely packed particles of varying sizes. Although image segmentation has recently been introduced to distinguish particles at the pixel level, the low SNR complicates the automated generation of accurate annotations for training supervised models. Moreover, platforms for systematically comparing different design choices in pipeline construction are lacking. Thus, a modular framework is essential to understand the advantages and limitations of this approach and drive further development. To address these challenges, we present a pipeline that automatically generates high-quality segmentation maps from cryo-EM data to serve as ground truth labels. Our modular framework enables the selection of various segmentation models and loss functions. We also integrate Conditional Random Fields (CRFs) with different solvers and feature sets to refine coarse predictions, thereby producing fine-grained segmentation. This flexibility facilitates optimal configurations tailored to cryo-EM datasets. When trained on a limited set of micrographs, our approach achieves over 90% accuracy, recall, precision, Intersection over Union (IoU), and F1-score on synthetic data. Furthermore, to demonstrate our framework's efficacy in downstream analyses, we show that the particles extracted by our pipeline produce 3D density maps with higher resolution than those generated by existing particle pickers on real experimental datasets, while achieving performance comparable to that of manually curated datasets from experts. 

**Abstract (ZH)**: 从冷冻电子显微镜微图中区分信号与背景是cryo-EM的关键初始步骤，但由于信噪比低、存在污染物以及尺寸各异的密集颗粒，这一过程仍然耗时费力。尽管最近已经引入了像素级的图像分割来区分颗粒，但低信噪比使得自动生成准确的标注用于训练监督模型变得复杂。此外，缺乏系统比较不同流水线构建设计选择的平台。因此，一个模块化的框架是必不可少的，以理解该方法的优势和限制，并推动进一步的发展。为应对这些挑战，我们提出了一种流水线，可以从cryo-EM数据自动生成高质量的分割图，作为 ground truth 标签。我们的模块化框架允许选择各种分割模型和损失函数。我们还结合使用不同的求解器和特征集的条件随机场(CRFs)对粗略预测进行细化，从而生成精细分割。这种灵活性使得可以根据cryo-EM数据集进行最佳配置。当仅在少量微图上进行训练时，我们的方法在合成数据上实现了超过90%的准确率、召回率、精确率、交并比(IoU)和F1分数。此外，为了展示我们框架在下游分析中的有效性，我们展示了由我们的流水线提取的颗粒可以产生比现有颗粒挑选工具在真实实验数据集上生成的3D密度图更高的分辨率，同时性能可与专家人工策划的数据集相媲美。 

---
# Individualised Treatment Effects Estimation with Composite Treatments and Composite Outcomes 

**Title (ZH)**: 个体化治疗效果估计中的复合治疗与复合结局 

**Authors**: Vinod Kumar Chauhan, Lei Clifton, Gaurav Nigam, David A. Clifton  

**Link**: [PDF](https://arxiv.org/pdf/2502.08282)  

**Abstract**: Estimating individualised treatment effect (ITE) -- that is the causal effect of a set of variables (also called exposures, treatments, actions, policies, or interventions), referred to as \textit{composite treatments}, on a set of outcome variables of interest, referred to as \textit{composite outcomes}, for a unit from observational data -- remains a fundamental problem in causal inference with applications across disciplines, such as healthcare, economics, education, social science, marketing, and computer science. Previous work in causal machine learning for ITE estimation is limited to simple settings, like single treatments and single outcomes. This hinders their use in complex real-world scenarios; for example, consider studying the effect of different ICU interventions, such as beta-blockers and statins for a patient admitted for heart surgery, on different outcomes of interest such as atrial fibrillation and in-hospital mortality. The limited research into composite treatments and outcomes is primarily due to data scarcity for all treatments and outcomes. To address the above challenges, we propose a novel and innovative hypernetwork-based approach, called \emph{H-Learner}, to solve ITE estimation under composite treatments and composite outcomes, which tackles the data scarcity issue by dynamically sharing information across treatments and outcomes. Our empirical analysis with binary and arbitrary composite treatments and outcomes demonstrates the effectiveness of the proposed approach compared to existing methods. 

**Abstract (ZH)**: 基于超网络的复合治疗与复合结果个体化治疗效果估计（H-Learner） 

---
# What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations 

**Title (ZH)**: 科学演讲的视频到文本摘要数据集：关于这场演讲说什么？ 

**Authors**: Dongqi Liu, Chenxi Whitehouse, Xi Yu, Louis Mahon, Rohit Saxena, Zheng Zhao, Yifu Qiu, Mirella Lapata, Vera Demberg  

**Link**: [PDF](https://arxiv.org/pdf/2502.08279)  

**Abstract**: Transforming recorded videos into concise and accurate textual summaries is a growing challenge in multimodal learning. This paper introduces VISTA, a dataset specifically designed for video-to-text summarization in scientific domains. VISTA contains 18,599 recorded AI conference presentations paired with their corresponding paper abstracts. We benchmark the performance of state-of-the-art large models and apply a plan-based framework to better capture the structured nature of abstracts. Both human and automated evaluations confirm that explicit planning enhances summary quality and factual consistency. However, a considerable gap remains between models and human performance, highlighting the challenges of scientific video summarization. 

**Abstract (ZH)**: 将记录视频转换为精练准确的文字摘要是多模态学习中的一个 growing challenge。本文介绍了 VISTA，一个专门为科学领域视频到文本摘要设计的数据集。VISTA 包含 18,599 场记录的人工智能会议演讲及其相应的论文摘要。我们 benchmarks 状态-of-the-art 大型模型的性能，并应用基于计划的框架以更好地捕捉摘要的结构化特性。人类和自动评估均证实明确的计划能够提高摘要质量并增强事实一致性。然而，模型与人类性能之间仍存在较大差距，突显了科学视频摘要化的挑战。 

---
# Dealing with Annotator Disagreement in Hate Speech Classification 

**Title (ZH)**: 处理仇恨言论分类中的注释者分歧 

**Authors**: Somaiyeh Dehghan, Mehmet Umut Sen, Berrin Yanikoglu  

**Link**: [PDF](https://arxiv.org/pdf/2502.08266)  

**Abstract**: Hate speech detection is a crucial task, especially on social media, where harmful content can spread quickly. Implementing machine learning models to automatically identify and address hate speech is essential for mitigating its impact and preventing its proliferation. The first step in developing an effective hate speech detection model is to acquire a high-quality dataset for training. Labeled data is foundational for most natural language processing tasks, but categorizing hate speech is difficult due to the diverse and often subjective nature of hate speech, which can lead to varying interpretations and disagreements among annotators. This paper examines strategies for addressing annotator disagreement, an issue that has been largely overlooked. In particular, we evaluate different approaches to deal with annotator disagreement regarding hate speech classification in Turkish tweets, based on a fine-tuned BERT model. Our work highlights the importance of the problem and provides state-of-art benchmark results for detection and understanding of hate speech in online discourse. 

**Abstract (ZH)**: 仇恨言论检测是一项关键任务，特别是在社交媒体上，有害内容可以迅速传播。开发有效的仇恨言论检测模型的第一步是获取高质量的数据集进行训练。标注数据对于大多数自然语言处理任务至关重要，但由于仇恨言论的多样性和主观性，对其进行分类会导致标注者之间存在不同的解释和分歧。本文探讨了应对标注者分歧的策略，这是一个长期被忽视的问题。特别是，我们根据微调的BERT模型，评估了处理土耳其推文中仇恨言论分类标注者分歧的不同方法。我们的研究突显了该问题的重要性，并提供了在线话语中仇恨言论检测和理解的最新基准结果。 

---
# Exploring the Potential of Large Language Models to Simulate Personality 

**Title (ZH)**: 探索大型语言模型模拟人格的潜力 

**Authors**: Maria Molchanova, Anna Mikhailova, Anna Korzanova, Lidiia Ostyakova, Alexandra Dolidze  

**Link**: [PDF](https://arxiv.org/pdf/2502.08265)  

**Abstract**: With the advancement of large language models (LLMs), the focus in Conversational AI has shifted from merely generating coherent and relevant responses to tackling more complex challenges, such as personalizing dialogue systems. In an effort to enhance user engagement, chatbots are often designed to mimic human behaviour, responding within a defined emotional spectrum and aligning to a set of values. In this paper, we aim to simulate personal traits according to the Big Five model with the use of LLMs. Our research showed that generating personality-related texts is still a challenging task for the models. As a result, we present a dataset of generated texts with the predefined Big Five characteristics and provide an analytical framework for testing LLMs on a simulation of personality skills. 

**Abstract (ZH)**: 随着大型语言模型的发展，对话AI的关注点已从仅仅生成连贯和相关的内容转移到面对更复杂的挑战，如个性化对话系统。为了增强用户参与度，聊天机器人经常被设计成模拟人类行为，以限定的情感范围回应并遵循一套价值观。本文旨在使用大型语言模型根据大五人格模型模拟个人特质。研究结果表明，生成与人格相关的内容仍然是一个具有挑战性的任务。因此，我们提供了一个具有预定义大五人格特征的生成文本数据集，并提出了一个分析框架，用于在人格技能模拟上测试大型语言模型。 

---
# Balancing optimism and pessimism in offline-to-online learning 

**Title (ZH)**: 离线到在线学习中乐观与 pessimism 的平衡 

**Authors**: Sentenac Flore, Lee Albin, Szepesvari Csaba  

**Link**: [PDF](https://arxiv.org/pdf/2502.08259)  

**Abstract**: We consider what we call the offline-to-online learning setting, focusing on stochastic finite-armed bandit problems. In offline-to-online learning, a learner starts with offline data collected from interactions with an unknown environment in a way that is not under the learner's control. Given this data, the learner begins interacting with the environment, gradually improving its initial strategy as it collects more data to maximize its total reward. The learner in this setting faces a fundamental dilemma: if the policy is deployed for only a short period, a suitable strategy (in a number of senses) is the Lower Confidence Bound (LCB) algorithm, which is based on pessimism. LCB can effectively compete with any policy that is sufficiently "covered" by the offline data. However, for longer time horizons, a preferred strategy is the Upper Confidence Bound (UCB) algorithm, which is based on optimism. Over time, UCB converges to the performance of the optimal policy at a rate that is nearly the best possible among all online algorithms. In offline-to-online learning, however, UCB initially explores excessively, leading to worse short-term performance compared to LCB. This suggests that a learner not in control of how long its policy will be in use should start with LCB for short horizons and gradually transition to a UCB-like strategy as more rounds are played. This article explores how and why this transition should occur. Our main result shows that our new algorithm performs nearly as well as the better of LCB and UCB at any point in time. The core idea behind our algorithm is broadly applicable, and we anticipate that our results will extend beyond the multi-armed bandit setting. 

**Abstract (ZH)**: 我们考虑一种我们称为离线到在线学习的设置，关注随机有限臂bandit问题。在离线到在线学习中，学习者从不受其控制的方式与未知环境交互收集离线数据。基于这些数据，学习者开始与环境交互，逐渐改进其初始策略，以最大化其总奖励。在这种设置中，学习者面临一个根本性的困境：如果策略仅部署较短时间，一种合适的战略是基于悲观原则的Lower Confidence Bound (LCB)算法。LCB能够有效与任何被离线数据充分“覆盖”的策略竞争。然而，对于更长的时间范围，更佳的战略是基于乐观原则的Upper Confidence Bound (UCB)算法。随着时间推移，UCB以几乎是最优的在线算法中最快的速率收敛到最优策略的表现。然而，在离线到在线学习中，UCB最初过度探索，导致短期内性能较差，相较于LCB。这表明，在无法控制策略使用时间的学习者应开始使用LCB策略进行较短期限，并随着回合数的增加逐渐过渡到类似UCB的战略。本文探讨了这种过渡应该如何发生及其原因。我们的主要结果表明，我们的新算法在任何时候的表现几乎与LCB和UCB中较好的算法表现一样优秀。我们的算法背后的核心思想具有广泛的应用性，我们预期我们的结果将超出多臂bandit环境的范围。 

---
# TRISHUL: Towards Region Identification and Screen Hierarchy Understanding for Large VLM based GUI Agents 

**Title (ZH)**: TRISHUL：面向大型VLM基GUI代理的区域识别与屏幕层级理解 

**Authors**: Kunal Singh, Shreyas Singh, Mukund Khanna  

**Link**: [PDF](https://arxiv.org/pdf/2502.08226)  

**Abstract**: Recent advancements in Large Vision Language Models (LVLMs) have enabled the development of LVLM-based Graphical User Interface (GUI) agents under various paradigms. Training-based approaches, such as CogAgent and SeeClick, struggle with cross-dataset and cross-platform generalization due to their reliance on dataset-specific training. Generalist LVLMs, such as GPT-4V, employ Set-of-Marks (SoM) for action grounding, but obtaining SoM labels requires metadata like HTML source, which is not consistently available across platforms. Moreover, existing methods often specialize in singular GUI tasks rather than achieving comprehensive GUI understanding. To address these limitations, we introduce TRISHUL, a novel, training-free agentic framework that enhances generalist LVLMs for holistic GUI comprehension. Unlike prior works that focus on either action grounding (mapping instructions to GUI elements) or GUI referring (describing GUI elements given a location), TRISHUL seamlessly integrates both. At its core, TRISHUL employs Hierarchical Screen Parsing (HSP) and the Spatially Enhanced Element Description (SEED) module, which work synergistically to provide multi-granular, spatially, and semantically enriched representations of GUI elements. Our results demonstrate TRISHUL's superior performance in action grounding across the ScreenSpot, VisualWebBench, AITW, and Mind2Web datasets. Additionally, for GUI referring, TRISHUL surpasses the ToL agent on the ScreenPR benchmark, setting a new standard for robust and adaptable GUI comprehension. 

**Abstract (ZH)**: Recent advancements in Large Vision Language Models (LVLMs) have enabled the development of LVLM-based Graphical User Interface (GUI) agents under various paradigms. Training-based approaches, such as CogAgent and SeeClick, struggle with cross-dataset and cross-platform generalization due to their reliance on dataset-specific training. Generalist LVLMs, such as GPT-4V, employ Set-of-Marks (SoM) for action grounding, but obtaining SoM labels requires metadata like HTML source, which is not consistently available across platforms. Moreover, existing methods often specialize in singular GUI tasks rather than achieving comprehensive GUI understanding. To address these limitations, we introduce TRISHUL, a novel, training-free agentic framework that enhances generalist LVLMs for holistic GUI comprehension. Unlike prior works that focus on either action grounding (mapping instructions to GUI elements) or GUI referring (describing GUI elements given a location), TRISHUL seamlessly integrates both. At its core, TRISHUL employs Hierarchical Screen Parsing (HSP) and the Spatially Enhanced Element Description (SEED) module, which work synergistically to provide multi-granular, spatially, and semantically enriched representations of GUI elements. Our results demonstrate TRISHUL's superior performance in action grounding across the ScreenSpot, VisualWebBench, AITW, and Mind2Web datasets. Additionally, for GUI referring, TRISHUL surpasses the ToL agent on the ScreenPR benchmark, setting a new standard for robust and adaptable GUI comprehension.

Title:
TRISHUL: A Training-Free Framework for Holistic GUI Comprehension Integrating Action Grounding and GUI Referring 

---
# Quality over Quantity: Boosting Data Efficiency Through Ensembled Multimodal Data Curation 

**Title (ZH)**: 质胜于量：通过集成多模态数据整理提升数据效率 

**Authors**: Jinda Xu, Yuhao Song, Daming Wang, Weiwei Zhao, Minghua Chen, Kangliang Chen, Qinya Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.08211)  

**Abstract**: In an era overwhelmed by vast amounts of data, the effective curation of web-crawl datasets is essential for optimizing model performance. This paper tackles the challenges associated with the unstructured and heterogeneous nature of such datasets. Traditional heuristic curation methods often inadequately capture complex features, resulting in biases and the exclusion of relevant data. We introduce an advanced, learning-driven approach, Ensemble Curation Of DAta ThroUgh Multimodal Operators (EcoDatum), incorporating a novel quality-guided deduplication method to ensure balanced feature distributions. EcoDatum strategically integrates various unimodal and multimodal data curation operators within a weak supervision ensemble framework, utilizing automated optimization to score each data point effectively. EcoDatum, which significantly improves the data curation quality and efficiency, outperforms existing state-of-the-art (SOTA) techniques, ranked 1st on the DataComp leaderboard, with an average performance score of 0.182 across 38 diverse evaluation datasets. This represents a 28% improvement over the DataComp baseline method, demonstrating its effectiveness in improving dataset curation and model training efficiency. 

**Abstract (ZH)**: 在大数据泛滥的时代，优化网络抓取数据集的编目对于提升模型性能至关重要。本文应对这类数据集无结构性和异质性的挑战。传统的启发式编目方法往往难以捕捉复杂特征，导致偏见并排除相关数据。我们提出了一种先进的学习驱动方法，即多模态操作员驱动的数据集中编目（EcoDatum），结合了一种新颖的质量导向去重方法，确保特征分布的均衡。EcoDatum 在弱监督集成框架内战略性地整合多种单模态和多模态数据编目操作，并利用自动化优化有效评分每个数据点。EcoDatum 显著提升数据编目质量和效率，超越现有先进方法（SOTA），在 DataComp 领先榜上排名第 1，平均性能得分为 0.182，涵盖 38 个不同评估数据集。相比于 DataComp 基线方法，性能提升 28%，证明了其在提高数据集编目和模型训练效率方面的有效性。 

---
# Equivariant Masked Position Prediction for Efficient Molecular Representation 

**Title (ZH)**: 对称掩码位置预测用于高效分子表示 

**Authors**: Junyi An, Chao Qu, Yun-Fei Shi, XinHao Liu, Qianwei Tang, Fenglei Cao, Yuan Qi  

**Link**: [PDF](https://arxiv.org/pdf/2502.08209)  

**Abstract**: Graph neural networks (GNNs) have shown considerable promise in computational chemistry. However, the limited availability of molecular data raises concerns regarding GNNs' ability to effectively capture the fundamental principles of physics and chemistry, which constrains their generalization capabilities. To address this challenge, we introduce a novel self-supervised approach termed Equivariant Masked Position Prediction (EMPP), grounded in intramolecular potential and force theory. Unlike conventional attribute masking techniques, EMPP formulates a nuanced position prediction task that is more well-defined and enhances the learning of quantum mechanical features. EMPP also bypasses the approximation of the Gaussian mixture distribution commonly used in denoising methods, allowing for more accurate acquisition of physical properties. Experimental results indicate that EMPP significantly enhances performance of advanced molecular architectures, surpassing state-of-the-art self-supervised approaches. Our code is released in this https URL. 

**Abstract (ZH)**: Graph神经网络（GNNs）在计算化学中展示了显著的潜力。然而，分子数据的有限可用性引发了关于GNNs是否能够有效捕捉物理学和化学基本原理的担忧，这限制了它们的泛化能力。为应对这一挑战，我们提出了一种基于分子内势和力理论的新颖自监督方法，称为等变遮掩位置预测（EMPP）。与传统的属性遮掩技术不同，EMPP 形成了一个更明确的位置预测任务，有助于增强量子力学特征的学习。此外，EMPP 跳过了去噪方法中常用的高斯混合分布近似，从而更准确地获取物理性质。实验结果表明，EMPP 显著提升了高级分子架构的表现，超越了当前最好的自监督方法。我们的代码在此 https://释放。 

---
# Latest Advancements Towards Catastrophic Forgetting under Data Scarcity: A Comprehensive Survey on Few-Shot Class Incremental Learning 

**Title (ZH)**: 在数据稀缺下的灾难性遗忘最新进展：少量样本分类增量学习综述 

**Authors**: M. Anwar Ma'sum, Mahardhika Pratama, Igor Skrjanc  

**Link**: [PDF](https://arxiv.org/pdf/2502.08181)  

**Abstract**: Data scarcity significantly complicates the continual learning problem, i.e., how a deep neural network learns in dynamic environments with very few samples. However, the latest progress of few-shot class incremental learning (FSCIL) methods and related studies show insightful knowledge on how to tackle the problem. This paper presents a comprehensive survey on FSCIL that highlights several important aspects i.e. comprehensive and formal objectives of FSCIL approaches, the importance of prototype rectifications, the new learning paradigms based on pre-trained model and language-guided mechanism, the deeper analysis of FSCIL performance metrics and evaluation, and the practical contexts of FSCIL in various areas. Our extensive discussion presents the open challenges, potential solutions, and future directions of FSCIL. 

**Abstract (ZH)**: 数据稀缺显著 complicates 持续学习问题，即如何在样本极少的动态环境中使深度神经网络进行学习。然而，最新的少样本类别增量学习（FSCIL）方法及相关研究展示了应对这一问题的有价值的见解。本文对 FSCIL 进行了全面综述，强调了几方面的重要内容，包括 FSCIL 方法的全面和正式目标、原型校正的重要性、基于预训练模型和语言引导机制的新学习范式、FSCIL 性能指标和评估的深入分析，以及 FSCIL 在各个领域的实际应用背景。我们广泛的讨论提出了 FSCIL 的开放挑战、潜在解决方案和未来方向。 

---
# Enhancing LLM Character-Level Manipulation via Divide and Conquer 

**Title (ZH)**: 通过分而治之增强_llm字符级操控_ 

**Authors**: Zhen Xiong, Yujun Cai, Bryan Hooi, Nanyun Peng, Kai-Wei Chang, Zhecheng Li, Yiwei Wang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08180)  

**Abstract**: Large Language Models (LLMs) have demonstrated strong generalization capabilities across a wide range of natural language processing (NLP) tasks. However, they exhibit notable weaknesses in character-level string manipulation, struggling with fundamental operations such as character deletion, insertion, and substitution. These challenges stem primarily from tokenization constraints, despite the critical role of such operations in data preprocessing and code generation. Through systematic analysis, we derive two key insights: (1) LLMs face significant difficulties in leveraging intrinsic token knowledge for character-level reasoning, and (2) atomized word structures can substantially enhance LLMs' ability to process token-level structural information. Building on these insights, we propose Character-Level Manipulation via Divide and Conquer, a novel approach designed to bridge the gap between token-level processing and character-level manipulation. Our method decomposes complex operations into explicit character-level subtasks coupled with controlled token reconstruction phases, leading to significant improvements in accuracy. Without additional training, our method significantly improves accuracies on the $\texttt{Deletion}$, $\texttt{Insertion}$, and $\texttt{Substitution}$ tasks. To support further research, we open-source our implementation and benchmarks. 

**Abstract (ZH)**: 大型语言模型（LLMs）在广泛自然语言处理（NLP）任务中展现了强大的泛化能力，但在字符级字符串操作方面存在明显不足，尤其是在字符删除、插入和替换等基本操作上表现不佳。这些挑战主要源于标记化限制，尽管这些操作在数据预处理和代码生成中起着至关重要的作用。通过系统分析，我们得出两个关键见解：（1）LLMs在利用内在标记知识进行字符级推理方面面临重大困难，（2）原子化单词结构可以显著增强LLMs处理标记级结构信息的能力。基于这些见解，我们提出了一种名为“分而治之”的新方法，以弥合标记级处理与字符级操作之间的差距。该方法将复杂操作分解为明确的字符级子任务，并结合受控的标记重建阶段，从而显著提高了准确性。在无需额外训练的情况下，该方法在删除、插入和替换任务上的准确性得到了显著提升。为支持进一步研究，我们开源了我们的实现和基准。 

---
# MixDec Sampling: A Soft Link-based Sampling Method of Graph Neural Network for Recommendation 

**Title (ZH)**: MixDec采样：图神经网络推荐中的软链接基采样方法 

**Authors**: Xiangjin Xie, Yuxin Chen, Ruipeng Wang, Kai Ouyang, Zihan Zhang, Hai-Tao Zheng, Buyue Qian, Hansen Zheng, Bo Hu, Chengxiang Zhuo, Zang Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.08161)  

**Abstract**: Graph neural networks have been widely used in recent recommender systems, where negative sampling plays an important role. Existing negative sampling methods restrict the relationship between nodes as either hard positive pairs or hard negative pairs. This leads to the loss of structural information, and lacks the mechanism to generate positive pairs for nodes with few neighbors. To overcome limitations, we propose a novel soft link-based sampling method, namely MixDec Sampling, which consists of Mixup Sampling module and Decay Sampling module. The Mixup Sampling augments node features by synthesizing new nodes and soft links, which provides sufficient number of samples for nodes with few neighbors. The Decay Sampling strengthens the digestion of graph structure information by generating soft links for node embedding learning. To the best of our knowledge, we are the first to model sampling relationships between nodes by soft links in GNN-based recommender systems. Extensive experiments demonstrate that the proposed MixDec Sampling can significantly and consistently improve the recommendation performance of several representative GNN-based models on various recommendation benchmarks. 

**Abstract (ZH)**: 基于软链接的MixDec采样方法在图神经网络推荐系统中的应用 

---
# Vertical Federated Learning in Practice: The Good, the Bad, and the Ugly 

**Title (ZH)**: 垂直联邦学习在实践中的优缺点及挑战 

**Authors**: Zhaomin Wu, Zhen Qin, Junyi Hou, Haodong Zhao, Qinbin Li, Bingsheng He, Lixin Fan  

**Link**: [PDF](https://arxiv.org/pdf/2502.08160)  

**Abstract**: Vertical Federated Learning (VFL) is a privacy-preserving collaborative learning paradigm that enables multiple parties with distinct feature sets to jointly train machine learning models without sharing their raw data. Despite its potential to facilitate cross-organizational collaborations, the deployment of VFL systems in real-world applications remains limited. To investigate the gap between existing VFL research and practical deployment, this survey analyzes the real-world data distributions in potential VFL applications and identifies four key findings that highlight this gap. We propose a novel data-oriented taxonomy of VFL algorithms based on real VFL data distributions. Our comprehensive review of existing VFL algorithms reveals that some common practical VFL scenarios have few or no viable solutions. Based on these observations, we outline key research directions aimed at bridging the gap between current VFL research and real-world applications. 

**Abstract (ZH)**: 垂直 Federated Learning (VFL) 是一种隐私保护的合作学习范式，使多个具有不同特征集的参与方能够在不共享其原始数据的情况下共同训练机器学习模型。尽管 VFL 有促进组织间合作的潜力，但在实际应用中的部署仍然受到限制。为了探究现有 VFL 研究与实际部署之间的差距，本文分析了潜在 VFL 应用中的现实数据分布，并识别出四个关键发现，突显了这一差距。我们基于实际的 VFL 数据分布提出了一种新颖的数据导向型 VFL 算法分类框架。通过全面回顾现有 VFL 算法，我们发现一些常见的实际 VFL 场景缺乏可行的解决方案。基于这些观察，我们概述了关键的研究方向，旨在弥合当前 VFL 研究与实际应用之间的差距。 

---
# DGSense: A Domain Generalization Framework for Wireless Sensing 

**Title (ZH)**: DGSense: 无线传感的领域泛化框架 

**Authors**: Rui Zhou, Yu Cheng, Songlin Li, Hongwang Zhang, Chenxu Liu  

**Link**: [PDF](https://arxiv.org/pdf/2502.08155)  

**Abstract**: Wireless sensing is of great benefits to our daily lives. However, wireless signals are sensitive to the surroundings. Various factors, e.g. environments, locations, and individuals, may induce extra impact on wireless propagation. Such a change can be regarded as a domain, in which the data distribution shifts. A vast majority of the sensing schemes are learning-based. They are dependent on the training domains, resulting in performance degradation in unseen domains. Researchers have proposed various solutions to address this issue. But these solutions leverage either semi-supervised or unsupervised domain adaptation techniques. They still require some data in the target domains and do not perform well in unseen domains. In this paper, we propose a domain generalization framework DGSense, to eliminate the domain dependence problem in wireless sensing. The framework is a general solution working across diverse sensing tasks and wireless technologies. Once the sensing model is built, it can generalize to unseen domains without any data from the target domain. To achieve the goal, we first increase the diversity of the training set by a virtual data generator, and then extract the domain independent features via episodic training between the main feature extractor and the domain feature extractors. The feature extractors employ a pre-trained Residual Network (ResNet) with an attention mechanism for spatial features, and a 1D Convolutional Neural Network (1DCNN) for temporal features. To demonstrate the effectiveness and generality of DGSense, we evaluated on WiFi gesture recognition, Millimeter Wave (mmWave) activity recognition, and acoustic fall detection. All the systems exhibited high generalization capability to unseen domains, including new users, locations, and environments, free of new data and retraining. 

**Abstract (ZH)**: 无线传感极大地改善了我们的日常生活。然而，无线信号对周围环境非常敏感。各种因素，如环境、位置和个体，都可能对无线传播产生额外影响。这种变化可以被视为一个领域，在这个领域中，数据分布发生偏移。大多数传感方案都是基于学习的，依赖于训练领域，导致在未见领域中的性能下降。研究人员提出了各种解决方案来解决这一问题。但这些解决方案依赖于半监督或无监督领域的适应技术，仍然需要一些目标领域的数据，并且在未见领域中的表现不佳。在本文中，我们提出了一种域泛化框架DGSense，以消除无线传感中的域依赖问题。该框架可以适用于多种传感任务和无线技术。一旦构建了传感模型，它可以在没有任何目标领域数据的情况下泛化到未见领域。为了实现这一目标，我们首先通过虚拟数据生成器增加训练集的多样性，然后通过主特征提取器和域特征提取器之间的 episodic 训练提取域独立特征。特征提取器采用带有注意力机制的预训练残差网络（ResNet）提取空间特征，并使用一维卷积神经网络（1DCNN）提取时序特征。为了证明DGSense的有效性和普适性，我们在WiFi手势识别、毫米波（mmWave）活动识别以及声学跌倒检测系统中进行了评估。所有系统都展示了对未见领域的高泛化能力，包括新用户、新地点和新环境，无需新的数据和重新训练。 

---
# Force Matching with Relativistic Constraints: A Physics-Inspired Approach to Stable and Efficient Generative Modeling 

**Title (ZH)**: 基于相对论约束的力匹配方法：一种物理启发的生成建模稳定高效方法 

**Authors**: Yang Cao, Bo Chen, Xiaoyu Li, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song, Mingda Wan  

**Link**: [PDF](https://arxiv.org/pdf/2502.08150)  

**Abstract**: This paper introduces Force Matching (ForM), a novel framework for generative modeling that represents an initial exploration into leveraging special relativistic mechanics to enhance the stability of the sampling process. By incorporating the Lorentz factor, ForM imposes a velocity constraint, ensuring that sample velocities remain bounded within a constant limit. This constraint serves as a fundamental mechanism for stabilizing the generative dynamics, leading to a more robust and controlled sampling process. We provide a rigorous theoretical analysis demonstrating that the velocity constraint is preserved throughout the sampling procedure within the ForM framework. To validate the effectiveness of our approach, we conduct extensive empirical evaluations. On the \textit{half-moons} dataset, ForM significantly outperforms baseline methods, achieving the lowest Euclidean distance loss of \textbf{0.714}, in contrast to vanilla first-order flow matching (5.853) and first- and second-order flow matching (5.793). Additionally, we perform an ablation study to further investigate the impact of our velocity constraint, reaffirming the superiority of ForM in stabilizing the generative process. The theoretical guarantees and empirical results underscore the potential of integrating special relativity principles into generative modeling. Our findings suggest that ForM provides a promising pathway toward achieving stable, efficient, and flexible generative processes. This work lays the foundation for future advancements in high-dimensional generative modeling, opening new avenues for the application of physical principles in machine learning. 

**Abstract (ZH)**: 力匹配（Force Matching），一种利用特殊相对论力学增强生成模型采样过程稳定性的新型框架 

---
# Generalized Class Discovery in Instance Segmentation 

**Title (ZH)**: 泛化类发现实例分割 

**Authors**: Cuong Manh Hoang, Yeejin Lee, Byeongkeun Kang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08149)  

**Abstract**: This work addresses the task of generalized class discovery (GCD) in instance segmentation. The goal is to discover novel classes and obtain a model capable of segmenting instances of both known and novel categories, given labeled and unlabeled data. Since the real world contains numerous objects with long-tailed distributions, the instance distribution for each class is inherently imbalanced. To address the imbalanced distributions, we propose an instance-wise temperature assignment (ITA) method for contrastive learning and class-wise reliability criteria for pseudo-labels. The ITA method relaxes instance discrimination for samples belonging to head classes to enhance GCD. The reliability criteria are to avoid excluding most pseudo-labels for tail classes when training an instance segmentation network using pseudo-labels from GCD. Additionally, we propose dynamically adjusting the criteria to leverage diverse samples in the early stages while relying only on reliable pseudo-labels in the later stages. We also introduce an efficient soft attention module to encode object-specific representations for GCD. Finally, we evaluate our proposed method by conducting experiments on two settings: COCO$_{half}$ + LVIS and LVIS + Visual Genome. The experimental results demonstrate that the proposed method outperforms previous state-of-the-art methods. 

**Abstract (ZH)**: 通用类别发现在实例分割中的任务研究：面向长尾分布的实例分割网络预训练方法 

---
# Democratizing AI: Open-source Scalable LLM Training on GPU-based Supercomputers 

**Title (ZH)**: 民主化人工智能：基于GPU超级计算机的开源可扩展大语言模型训练 

**Authors**: Siddharth Singh, Prajwal Singhania, Aditya Ranjan, John Kirchenbauer, Jonas Geiping, Yuxin Wen, Neel Jain, Abhimanyu Hans, Manli Shu, Aditya Tomar, Tom Goldstein, Abhinav Bhatele  

**Link**: [PDF](https://arxiv.org/pdf/2502.08145)  

**Abstract**: Training and fine-tuning large language models (LLMs) with hundreds of billions to trillions of parameters requires tens of thousands of GPUs, and a highly scalable software stack. In this work, we present a novel four-dimensional hybrid parallel algorithm implemented in a highly scalable, portable, open-source framework called AxoNN. We describe several performance optimizations in AxoNN to improve matrix multiply kernel performance, overlap non-blocking collectives with computation, and performance modeling to choose performance optimal configurations. These have resulted in unprecedented scaling and peak flop/s (bf16) for training of GPT-style transformer models on Perlmutter (620.1 Petaflop/s), Frontier (1.381 Exaflop/s) and Alps (1.423 Exaflop/s).
While the abilities of LLMs improve with the number of trainable parameters, so do privacy and copyright risks caused by memorization of training data, which can cause disclosure of sensitive or private information at inference time. We highlight this side effect of scale through experiments that explore "catastrophic memorization", where models are sufficiently large to memorize training data in a single pass, and present an approach to prevent it. As part of this study, we demonstrate fine-tuning of a 405-billion parameter LLM using AxoNN on Frontier. 

**Abstract (ZH)**: 训练和微调具有数百亿到万亿参数的大语言模型（LLMs）需要数千块GPU，并且需要一个高度可扩展的软件栈。在本工作中，我们提出了一种新型四维度混合并行算法，并在高度可扩展、可移植、开源框架AxoNN中实现。我们描述了AxoNN中的几种性能优化措施，以提高矩阵乘法内核性能、重叠非阻塞集中操作与计算，并进行性能建模以选择性能最优配置。这些措施在使用Perlmutter（620.1_petaflop/s）、Frontier（1.381_petaflop/s）和Alps（1.423_petaflop/s）进行GPT风格变换器模型训练时，实现了前所未有的扩展性能和峰值gflops（bf16）。

随着大语言模型可训练参数数量的增加，由于训练数据的回忆导致的隐私和版权风险也会增加，这可能在推理时泄露敏感或私人信息。我们通过探索“灾难性回忆”现象的实验突显了这种规模的副作用，即模型足够大可以在一次通过中回忆训练数据，并提出了一种防止这种现象的方法。作为这一研究的一部分，我们展示了在Frontier上使用AxoNN对一个4050亿参数的大语言模型进行微调。 

---
# Hookpad Aria: A Copilot for Songwriters 

**Title (ZH)**: Hookpad Aria：作曲人的副驾助手 

**Authors**: Chris Donahue, Shih-Lun Wu, Yewon Kim, Dave Carlton, Ryan Miyakawa, John Thickstun  

**Link**: [PDF](https://arxiv.org/pdf/2502.08122)  

**Abstract**: We present Hookpad Aria, a generative AI system designed to assist musicians in writing Western pop songs. Our system is seamlessly integrated into Hookpad, a web-based editor designed for the composition of lead sheets: symbolic music scores that describe melody and harmony. Hookpad Aria has numerous generation capabilities designed to assist users in non-sequential composition workflows, including: (1) generating left-to-right continuations of existing material, (2) filling in missing spans in the middle of existing material, and (3) generating harmony from melody and vice versa. Hookpad Aria is also a scalable data flywheel for music co-creation -- since its release in March 2024, Aria has generated 318k suggestions for 3k users who have accepted 74k into their songs.
More information about Hookpad Aria is available at this https URL 

**Abstract (ZH)**: Hookpad Aria：一个用于创作西方流行歌曲的生成型AI系统 

---
# HuDEx: Integrating Hallucination Detection and Explainability for Enhancing the Reliability of LLM responses 

**Title (ZH)**: HuDEx: 结合幻觉检测和可解释性以提高大语言模型响应可靠性 

**Authors**: Sujeong Lee, Hayoung Lee, Seongsoo Heo, Wonik Choi  

**Link**: [PDF](https://arxiv.org/pdf/2502.08109)  

**Abstract**: Recent advances in large language models (LLMs) have shown promising improvements, often surpassing existing methods across a wide range of downstream tasks in natural language processing. However, these models still face challenges, which may hinder their practical applicability. For example, the phenomenon of hallucination is known to compromise the reliability of LLMs, especially in fields that demand high factual precision. Current benchmarks primarily focus on hallucination detection and factuality evaluation but do not extend beyond identification. This paper proposes an explanation enhanced hallucination-detection model, coined as HuDEx, aimed at enhancing the reliability of LLM-generated responses by both detecting hallucinations and providing detailed explanations. The proposed model provides a novel approach to integrate detection with explanations, and enable both users and the LLM itself to understand and reduce errors. Our measurement results demonstrate that the proposed model surpasses larger LLMs, such as Llama3 70B and GPT-4, in hallucination detection accuracy, while maintaining reliable explanations. Furthermore, the proposed model performs well in both zero-shot and other test environments, showcasing its adaptability across diverse benchmark datasets. The proposed approach further enhances the hallucination detection research by introducing a novel approach to integrating interpretability with hallucination detection, which further enhances the performance and reliability of evaluating hallucinations in language models. 

**Abstract (ZH)**: 近期大规模语言模型的进步展现了令人鼓舞的改进，常在自然语言处理的多种下游任务中超越现有方法。然而，这些模型仍面临挑战，这可能妨碍其实际应用。例如，幻觉现象会削弱大语言模型的可靠性，尤其是在需要高事实精确度的领域。当前基准主要集中在幻觉检测和事实性评估上，并未超越这一识别阶段。本文提出了一种增强解释的幻觉检测模型，命名为HuDEx，旨在通过检测幻觉和提供详细解释来增强大语言模型生成响应的可靠性。提出的模型提供了一种将检测与解释整合的新方法，使用户和大语言模型本身能够理解并减少错误。我们的测量结果显示，提出的模型在幻觉检测准确性上超过了更大规模的模型（如Llama3 70B和GPT-4），同时保持了可靠的解释。此外，提出的模型在零样本和其他测试环境中表现良好，展示了其在不同基准数据集上的适应性。提出的这种方法进一步通过将可解释性与幻觉检测相结合的新方法，增强了语言模型中评估幻觉的性能和可靠性。 

---
# Generative AI and Empirical Software Engineering: A Paradigm Shift 

**Title (ZH)**: 生成式AI与实证软件工程： paradigmhift 

**Authors**: Christoph Treude, Margaret-Anne Storey  

**Link**: [PDF](https://arxiv.org/pdf/2502.08108)  

**Abstract**: The widespread adoption of generative AI in software engineering marks a paradigm shift, offering new opportunities to design and utilize software engineering tools while influencing both developers and the artifacts they create. Traditional empirical methods in software engineering, including quantitative, qualitative, and mixed-method approaches, are well established. However, this paradigm shift introduces novel data types and redefines many concepts in the software engineering process. The roles of developers, users, agents, and researchers increasingly overlap, blurring the distinctions between these social and technical actors within the field.
This paper examines how integrating AI into software engineering challenges traditional research paradigms. It focuses on the research phenomena that we investigate, the methods and theories that we employ, the data we analyze, and the threats to validity that emerge in this new context. Through this exploration, our goal is to understand how AI adoption disrupts established software development practices that creates new opportunities for empirical software engineering research. 

**Abstract (ZH)**: 生成式AI在软件工程中的广泛应用标志着范式的转变，为设计和利用软件工程工具提供了新的机会，同时影响着开发者及其创造的产物。传统的软件工程实证方法，包括定量、定性以及混合方法，已经十分成熟。然而，这种范式的转变引入了新的数据类型，并重新定义了软件工程过程中的许多概念。开发者、用户、代理和研究人员的角色日益重叠，模糊了该领域社会和技术行为者之间的界限。

这篇论文探讨了将AI integrates into软件工程如何挑战传统的研究范式。它关注我们调查的研究现象、采用的方法和理论、分析的数据以及在这种新背景下出现的效度威胁。通过这一探索，我们的目标是理解AI的采用如何扰乱了已有的软件开发实践，从而为实证软件工程研究创造新的机遇。 

---
# PoGDiff: Product-of-Gaussians Diffusion Models for Imbalanced Text-to-Image Generation 

**Title (ZH)**: PoGDiff: 均值漂移扩散模型在不平衡文本到图像生成中的应用 

**Authors**: Ziyan Wang, Sizhe Wei, Xiaoming Huo, Hao Wang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08106)  

**Abstract**: Diffusion models have made significant advancements in recent years. However, their performance often deteriorates when trained or fine-tuned on imbalanced datasets. This degradation is largely due to the disproportionate representation of majority and minority data in image-text pairs. In this paper, we propose a general fine-tuning approach, dubbed PoGDiff, to address this challenge. Rather than directly minimizing the KL divergence between the predicted and ground-truth distributions, PoGDiff replaces the ground-truth distribution with a Product of Gaussians (PoG), which is constructed by combining the original ground-truth targets with the predicted distribution conditioned on a neighboring text embedding. Experiments on real-world datasets demonstrate that our method effectively addresses the imbalance problem in diffusion models, improving both generation accuracy and quality. 

**Abstract (ZH)**: 扩散模型在 recent years 取得了显著进展，但它们在训练或微调于不平衡数据集时性能往往会下降。这种下降主要是由于图像-文本对中主流数据和少数数据的不成比例的表示。本文提出了一种通用的微调方法，称为 PoGDiff，以应对这一挑战。PoGDiff 不是直接最小化预测分布与真实分布的 KL 散度，而是用一个由原始真实目标与基于邻近文本嵌入条件下预测分布组合而成的乘积高斯（PoG）替代真实分布。实验证明，我们的方法有效地解决了扩散模型中的不平衡问题，提高了生成的准确性和质量。 

---
# Rethinking Tokenized Graph Transformers for Node Classification 

**Title (ZH)**: 重新思考token化图变换器在节点分类中的应用 

**Authors**: Jinsong Chen, Chenyang Li, GaiChao Li, John E. Hopcroft, Kun He  

**Link**: [PDF](https://arxiv.org/pdf/2502.08101)  

**Abstract**: Node tokenized graph Transformers (GTs) have shown promising performance in node classification. The generation of token sequences is the key module in existing tokenized GTs which transforms the input graph into token sequences, facilitating the node representation learning via Transformer. In this paper, we observe that the generations of token sequences in existing GTs only focus on the first-order neighbors on the constructed similarity graphs, which leads to the limited usage of nodes to generate diverse token sequences, further restricting the potential of tokenized GTs for node classification. To this end, we propose a new method termed SwapGT. SwapGT first introduces a novel token swapping operation based on the characteristics of token sequences that fully leverages the semantic relevance of nodes to generate more informative token sequences. Then, SwapGT leverages a Transformer-based backbone to learn node representations from the generated token sequences. Moreover, SwapGT develops a center alignment loss to constrain the representation learning from multiple token sequences, further enhancing the model performance. Extensive empirical results on various datasets showcase the superiority of SwapGT for node classification. 

**Abstract (ZH)**: 基于节点切片的图变换器（SwapGT）在节点分类中的应用 

---
# GCoT: Chain-of-Thought Prompt Learning for Graphs 

**Title (ZH)**: GCoT: 图的链式思考提示学习 

**Authors**: Xingtong Yu, Chang Zhou, Zhongwei Kuai, Xinming Zhang, Yuan Fang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08092)  

**Abstract**: Chain-of-thought (CoT) prompting has achieved remarkable success in natural language processing (NLP). However, its vast potential remains largely unexplored for graphs. This raises an interesting question: How can we design CoT prompting for graphs to guide graph models to learn step by step? On one hand, unlike natural languages, graphs are non-linear and characterized by complex topological structures. On the other hand, many graphs lack textual data, making it difficult to formulate language-based CoT prompting. In this work, we propose the first CoT prompt learning framework for text-free graphs, GCoT. Specifically, we decompose the adaptation process for each downstream task into a series of inference steps, with each step consisting of prompt-based inference, ``thought'' generation, and thought-conditioned prompt learning. While the steps mimic CoT prompting in NLP, the exact mechanism differs significantly. Specifically, at each step, an input graph, along with a prompt, is first fed into a pre-trained graph encoder for prompt-based inference. We then aggregate the hidden layers of the encoder to construct a ``thought'', which captures the working state of each node in the current step. Conditioned on this thought, we learn a prompt specific to each node based on the current state. These prompts are fed into the next inference step, repeating the cycle. To evaluate and analyze the effectiveness of GCoT, we conduct comprehensive experiments on eight public datasets, which demonstrate the advantage of our approach. 

**Abstract (ZH)**: 无文本图的链式思维提示学习框架：GCoT 

---
# Cognify: Supercharging Gen-AI Workflows With Hierarchical Autotuning 

**Title (ZH)**: Cognify: 通过分层自调优增强生成式AI工作流 

**Authors**: Zijian He, Reyna Abhyankar, Vikranth Srivatsa, Yiying Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08056)  

**Abstract**: Today's gen-AI workflows that involve multiple ML model calls, tool/API calls, data retrieval, or generic code execution are often tuned manually in an ad-hoc way that is both time-consuming and error-prone. In this paper, we propose a systematic approach for automatically tuning gen-AI workflows. Our key insight is that gen-AI workflows can benefit from structure, operator, and prompt changes, but unique properties of gen-AI workflows require new optimization techniques. We propose AdaSeek, an adaptive hierarchical search algorithm for autotuning gen-AI workflows. AdaSeek organizes workflow tuning methods into different layers based on the user-specified total search budget and distributes the budget across different layers based on the complexity of each layer. During its hierarchical search, AdaSeek redistributes the search budget from less useful to more promising tuning configurations based on workflow-level evaluation results. We implement AdaSeek in a workflow autotuning framework called Cognify and evaluate Cognify using six types of workflows such as RAG-based QA and text-to-SQL transformation. Overall, Cognify improves these workflows' generation quality by up to 2.8x, reduces execution monetary cost by up to 10x, and reduces end-to-end latency by 2.7x. 

**Abstract (ZH)**: 今天涉及多个ML模型调用、工具/API调用、数据检索或通用代码执行的生成AI工作流通常以非系统的方式手动调整，既耗时又容易出错。本文提出了一种系统方法自动调整生成AI工作流。我们的关键见解是生成AI工作流可以从结构、操作符和提示的改变中受益，但生成AI工作流的独特属性需要新的优化技术。我们提出了AdaSeek，一种自适应分层搜索算法，用于自动调整生成AI工作流。AdaSeek根据用户指定的总搜索预算，将工作流调整方法组织成不同的层级，并基于每个层级的复杂性分配预算。在分层搜索过程中，AdaSeek根据工作流级别的评估结果，重新分配搜索预算，从不太有用的配置分配到更有前途的配置。我们将在一个命名为Cognify的工作流自动调整框架中实现AdaSeek，并使用包括基于RAG的问答和文本到SQL转换等六种类型的工作流进行评估。总体而言，Cognify将这些工作流的生成质量提高了最多2.8倍，减少了执行成本最多10倍，并将端到端延迟减少了2.7倍。 

---
# Break the Checkbox: Challenging Closed-Style Evaluations of Cultural Alignment in LLMs 

**Title (ZH)**: 打破选择框限制：挑战LLM文化对齐的封闭式评估方式 

**Authors**: Mohsinul Kabir, Ajwad Abrar, Sophia Ananiadou  

**Link**: [PDF](https://arxiv.org/pdf/2502.08045)  

**Abstract**: A large number of studies rely on closed-style multiple-choice surveys to evaluate cultural alignment in Large Language Models (LLMs). In this work, we challenge this constrained evaluation paradigm and explore more realistic, unconstrained approaches. Using the World Values Survey (WVS) and Hofstede Cultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger cultural alignment in less constrained settings, where responses are not forced. Additionally, we show that even minor changes, such as reordering survey choices, lead to inconsistent outputs, exposing the limitations of closed-style evaluations. Our findings advocate for more robust and flexible evaluation frameworks that focus on specific cultural proxies, encouraging more nuanced and accurate assessments of cultural alignment in LLMs. 

**Abstract (ZH)**: 大量的研究依赖封闭式多项选择调查来评估大型语言模型（LLMs）的文化一致性。在本文中，我们挑战这种有限的评估范式，并探索更为现实且不受约束的方法。通过世界价值观调查（WVS）和霍夫斯泰德文化维度作为案例研究，我们表明，在不受限制的环境中，LLMs在文化一致性方面表现更佳，其中响应不会受到强制。此外，我们还展示了即使是微小的变化，如重新排列调查选项，也会导致不一致的输出，揭示了封闭式评估的局限性。我们的研究结果提倡更加稳健和灵活的评估框架，重点关注特定的文化代理，以鼓励对LLMs文化一致性的更细腻和准确的评估。 

---
# Model Selection for Off-policy Evaluation: New Algorithms and Experimental Protocol 

**Title (ZH)**: 离线政策评估的模型选择：新算法及实验协议 

**Authors**: Pai Liu, Lingfeng Zhao, Shivangi Agarwal, Jinghan Liu, Audrey Huang, Philip Amortila, Nan Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2502.08021)  

**Abstract**: Holdout validation and hyperparameter tuning from data is a long-standing problem in offline reinforcement learning (RL). A standard framework is to use off-policy evaluation (OPE) methods to evaluate and select the policies, but OPE either incurs exponential variance (e.g., importance sampling) or has hyperparameters on their own (e.g., FQE and model-based). In this work we focus on hyperparameter tuning for OPE itself, which is even more under-investigated. Concretely, we select among candidate value functions ("model-free") or dynamics ("model-based") to best assess the performance of a target policy. Our contributions are two fold. We develop: (1) new model-free and model-based selectors with theoretical guarantees, and (2) a new experimental protocol for empirically evaluating them. Compared to the model-free protocol in prior works, our new protocol allows for more stable generation of candidate value functions, better control of misspecification, and evaluation of model-free and model-based methods alike. We exemplify the protocol on a Gym environment, and find that our new model-free selector, LSTD-Tournament, demonstrates promising empirical performance. 

**Abstract (ZH)**: 从数据中进行保留集验证和超参数调优是离线强化学习（RL）中的一个长期问题。我们专注于对针对保留集验证自身的超参数进行调优，这甚至更少被研究。具体来说，我们在候选值函数（“无模型”）或动力学（“有模型”）中选择最优者以评估目标策略的表现。我们的贡献包括：（1）开发具有理论保证的新“无模型”和“有模型”选择器，以及（2）开发新的实验协议以实证评估它们。与先前工作的“无模型”协议相比，我们的新协议允许更稳定的候选值函数生成、更好的误指定控制，并能够评估“无模型”和“有模型”方法。我们在Gym环境中展示了该协议的应用，并发现我们的新“无模型”选择器LSTD-Tournament表现出有希望的实证性能。 

---
# Speculate, then Collaborate: Fusing Knowledge of Language Models during Decoding 

**Title (ZH)**: 推测而后协作：解码过程中融合语言模型的知识 

**Authors**: Ziyao Wang, Muneeza Azmart, Ang Li, Raya Horesh, Mikhail Yurochkin  

**Link**: [PDF](https://arxiv.org/pdf/2502.08020)  

**Abstract**: Large Language Models (LLMs) often excel in specific domains but fall short in others due to the limitations of their training. Thus, enabling LLMs to solve problems collaboratively by integrating their complementary knowledge promises to improve their performance across domains. To realize this potential, we introduce a novel Collaborative Speculative Decoding (CoSD) algorithm that enables efficient LLM knowledge fusion at test time without requiring additional model training. CoSD employs a draft model to generate initial sequences and an easy-to-learn rule or decision tree to decide when to invoke an assistant model to improve these drafts. CoSD not only enhances knowledge fusion but also improves inference efficiency, is transferable across domains and models, and offers greater explainability. Experimental results demonstrate that CoSD improves accuracy by up to 10\% across benchmarks compared to existing methods, providing a scalable and effective solution for LLM-based applications 

**Abstract (ZH)**: 大型语言模型（LLMs）在特定领域往往表现出色，但在其他领域却可能表现不佳，这归因于它们训练的局限性。因此，通过集成它们互补的知识来使LLMs协作解决问题，有望提高其跨领域的性能。为实现这一潜力，我们引入了一种新颖的合作推测解码（CoSD）算法，该算法可以在不需要额外模型训练的情况下，在测试时实现高效的LLM知识融合。CoSD 使用一个草稿模型生成初始序列，并使用易于学习的规则或决策树决定何时调用助手模型以改进这些草稿。CoSD 不仅提高了知识融合的效率，还提升了推理效率，具有跨领域和模型的可迁移性，并提供了更高的可解释性。实验结果表明，与现有方法相比，CoSD 在基准测试中将准确性提高了高达10%，提供了一种可扩展且有效的基于LLM的应用解决方案。 

---
# Greed is Good: Guided Generation from a Greedy Perspective 

**Title (ZH)**: 贪婪亦可取：从贪婪视角指导生成 

**Authors**: Zander W. Blasingame, Chen Liu  

**Link**: [PDF](https://arxiv.org/pdf/2502.08006)  

**Abstract**: Training-free guided generation is a widely used and powerful technique that allows the end user to exert further control over the generative process of diffusion models. In this work, we explore the guided generation from the perspective of optimizing the solution trajectory of a neural differential equation in a greedy manner. We present such a strategy as a unifying view on training-free guidance by showing that the greedy strategy is a first-order discretization of end-to-end optimization techniques. We show that a greedy guidance strategy makes good decisions and compare it to a guidance strategy using the ideal gradients found via the continuous adjoint equations. We then show how other popular training-free guidance strategies can be viewed in a unified manner from this perspective. 

**Abstract (ZH)**: 无训练引导生成是一种广泛使用且强大的技术，允许最终用户进一步控制扩散模型的生成过程。在本文中，我们从神经微分方程解轨迹的贪心优化视角探索引导生成。我们通过展示贪心策略是端到端优化技术的一阶离散化来阐述这种策略作为一个无训练引导的统一视图。我们展示了贪心引导策略能做出良好的决策，并将其与通过连续伴随方程找到的理想梯度来实现的引导策略进行了比较。然后，我们展示了从这一视角来看，其他流行的无训练引导策略可以统一地被理解。 

---
# MetaSC: Test-Time Safety Specification Optimization for Language Models 

**Title (ZH)**: MetaSC：语言模型测试时安全规范优化 

**Authors**: Víctor Gallego  

**Link**: [PDF](https://arxiv.org/pdf/2502.07985)  

**Abstract**: We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights. Building on recent advances in self-critique methods, our approach leverages a meta-critique mechanism that iteratively updates safety prompts-termed specifications-to drive the critique and revision process adaptively. This test-time optimization not only improves performance against adversarial jailbreak requests but also in diverse general safety-related tasks, such as avoiding moral harm or pursuing honest responses. Our empirical evaluations across several language models demonstrate that dynamically optimized safety prompts yield significantly higher safety scores compared to fixed system prompts and static self-critique defenses. Code to be released at this https URL . 

**Abstract (ZH)**: 我们提出了一种新颖的动态安全框架，在不修改模型权重的情况下优化语言模型的安全推理，在推理时进行测试时的优化。该方法基于最近在自我批判方法方面的进展，利用一个元批判机制，迭代更新称为规范的安全提示，以适应性地驱动批判和修订过程。这种测试时的优化不仅有助于抵御对抗性逃狱请求，还在避免道德伤害或追求诚实回应等多种一般性安全任务中提高了性能。我们对多个语言模型的实证评估表明，动态优化的安全提示相比固定系统提示和静态自我批判防御措施能够获得显著更高的安全评分。代码将在该网址发布：https://xxxxx。 

---
# CIRCUIT: A Benchmark for Circuit Interpretation and Reasoning Capabilities of LLMs 

**Title (ZH)**: CIRCUIT：LLM电路解释与推理能力基准 

**Authors**: Lejla Skelic, Yan Xu, Matthew Cox, Wenjie Lu, Tao Yu, Ruonan Han  

**Link**: [PDF](https://arxiv.org/pdf/2502.07980)  

**Abstract**: The role of Large Language Models (LLMs) has not been extensively explored in analog circuit design, which could benefit from a reasoning-based approach that transcends traditional optimization techniques. In particular, despite their growing relevance, there are no benchmarks to assess LLMs' reasoning capability about circuits. Therefore, we created the CIRCUIT dataset consisting of 510 question-answer pairs spanning various levels of analog-circuit-related subjects. The best-performing model on our dataset, GPT-4o, achieves 48.04% accuracy when evaluated on the final numerical answer. To evaluate the robustness of LLMs on our dataset, we introduced a unique feature that enables unit-test-like evaluation by grouping questions into unit tests. In this case, GPT-4o can only pass 27.45% of the unit tests, highlighting that the most advanced LLMs still struggle with understanding circuits, which requires multi-level reasoning, particularly when involving circuit topologies. This circuit-specific benchmark highlights LLMs' limitations, offering valuable insights for advancing their application in analog integrated circuit design. 

**Abstract (ZH)**: 大型语言模型在模拟电路设计中的作用尚未得到充分探索，可以从超越传统优化技术的基于推理的方法中受益。尤其是，尽管它们的重要性日益增长，目前还没有评估大型语言模型在电路方面推理能力的基准。因此，我们创建了包含510个问题-答案对的CIRCUIT数据集，涵盖多个层次的模拟电路相关主题。在我们的数据集上表现最佳的模型GPT-4o在最终数值答案上的准确率为48.04%。为了评估我们在数据集上大型语言模型的鲁棒性，我们引入了一个独特特征，通过将问题分组为单元测试来实现类似单元测试的评估。在这种情况下，GPT-4o只能通过27.45%的单元测试，这表明最先进的大型语言模型在理解电路方面仍然存在困难，而理解电路往往需要多层面的推理，尤其是涉及电路拓扑结构时。这一电路专用基准突显了大型语言模型的局限性，为他们在模拟集成电路设计中的应用提供了宝贵见解。 

---
# From Hazard Identification to Controller Design: Proactive and LLM-Supported Safety Engineering for ML-Powered Systems 

**Title (ZH)**: 从风险识别到控制器设计：基于大语言模型的支持的主动安全工程方法用于机器学习驱动的系统 

**Authors**: Yining Hong, Christopher S. Timperley, Christian Kästner  

**Link**: [PDF](https://arxiv.org/pdf/2502.07974)  

**Abstract**: Machine learning (ML) components are increasingly integrated into software products, yet their complexity and inherent uncertainty often lead to unintended and hazardous consequences, both for individuals and society at large. Despite these risks, practitioners seldom adopt proactive approaches to anticipate and mitigate hazards before they occur. Traditional safety engineering approaches, such as Failure Mode and Effects Analysis (FMEA) and System Theoretic Process Analysis (STPA), offer systematic frameworks for early risk identification but are rarely adopted. This position paper advocates for integrating hazard analysis into the development of any ML-powered software product and calls for greater support to make this process accessible to developers. By using large language models (LLMs) to partially automate a modified STPA process with human oversight at critical steps, we expect to address two key challenges: the heavy dependency on highly experienced safety engineering experts, and the time-consuming, labor-intensive nature of traditional hazard analysis, which often impedes its integration into real-world development workflows. We illustrate our approach with a running example, demonstrating that many seemingly unanticipated issues can, in fact, be anticipated. 

**Abstract (ZH)**: 机器学习(ML)组件日益集成到软件产品中，但其复杂性和固有的不确定性往往会导致个体和社会层面的意外和危害后果。尽管存在这些风险，从业者通常不采取积极措施在事故发生前预见和减轻危害。传统的安全工程方法，如故障模式和效果分析(FMEA)和系统理论过程分析(STPA)，提供了早期风险识别的系统框架，但这些方法很少被采用。本文倡导将危害分析整合到任何由机器学习驱动的软件产品的开发过程中，并呼吁提供更多的支持，使这一过程对开发者而言更加可访问。通过使用大规模语言模型(LLM)部分自动化修改后的STPA过程，并在关键步骤中进行人工监督，我们期望解决两个关键问题：对高度经验丰富的安全工程专家的严重依赖，以及传统危害分析耗时、劳动密集型的特点，这往往阻碍了其在实际开发工作流中的集成。我们通过一个运行示例来说明我们的方法，证明许多看似不可预见的问题实际上是可以预见的。 

---
# Training Sparse Mixture Of Experts Text Embedding Models 

**Title (ZH)**: 训练稀疏专家混合文本嵌入模型 

**Authors**: Zach Nussbaum, Brandon Duderstadt  

**Link**: [PDF](https://arxiv.org/pdf/2502.07972)  

**Abstract**: Transformer-based text embedding models have improved their performance on benchmarks like MIRACL and BEIR by increasing their parameter counts. However, this scaling approach introduces significant deployment challenges, including increased inference latency and memory usage. These challenges are particularly severe in retrieval-augmented generation (RAG) applications, where large models' increased memory requirements constrain dataset ingestion capacity, and their higher latency directly impacts query-time performance. While causal language models have addressed similar efficiency challenges using Mixture of Experts (MoE) architectures, this approach hasn't been successfully adapted to the general text embedding setting. In this paper, we introduce Nomic Embed v2, the first general purpose MoE text embedding model. Our model outperforms models in the same parameter class on both monolingual and multilingual benchmarks while also maintaining competitive performance with models twice its size. We open-source all code, models, and evaluation data to ensure full reproducibility of our training pipeline. 

**Abstract (ZH)**: 基于Transformer的文本嵌入模型通过增加参数量在MIRACL和BEIR等基准测试上的性能有所提升，但这种扩展方法引入了显著的部署挑战，包括推理延迟增加和内存使用量增加。这些挑战在检索增强生成（RAG）应用中尤为严重，其中大型模型增加的内存需求限制了数据集的摄入能力，而更高的延迟直接影响查询时的性能。虽然因果语言模型通过专家混合（MoE）架构解决了类似的有效性挑战，但这种方法尚未成功应用于通用文本嵌入设置中。在本文中，我们引入了Nomic Embed v2，这是首个通用目的MoE文本嵌入模型。我们的模型在单调语言和多语言基准测试中均表现出色，同时与两倍参数量的模型保持竞争力。我们开源了所有代码、模型和评估数据，以确保训练流程的完全可再现性。 

---
# ReTreever: Tree-based Coarse-to-Fine Representations for Retrieval 

**Title (ZH)**: ReTreever: 基于树的粗细粒度表示检索 

**Authors**: Shubham Gupta, Zichao Li, Tianyi Chen, Cem Subakan, Siva Reddy, Perouz Taslakian, Valentina Zantedeschi  

**Link**: [PDF](https://arxiv.org/pdf/2502.07971)  

**Abstract**: Document retrieval is a core component of question-answering systems, as it enables conditioning answer generation on new and large-scale corpora. While effective, the standard practice of encoding documents into high-dimensional embeddings for similarity search entails large memory and compute footprints, and also makes it hard to inspect the inner workings of the system. In this paper, we propose a tree-based method for organizing and representing reference documents at various granular levels, which offers the flexibility to balance cost and utility, and eases the inspection of the corpus content and retrieval operations. Our method, called ReTreever, jointly learns a routing function per internal node of a binary tree such that query and reference documents are assigned to similar tree branches, hence directly optimizing for retrieval performance. Our evaluations show that ReTreever generally preserves full representation accuracy. Its hierarchical structure further provides strong coarse representations and enhances transparency by indirectly learning meaningful semantic groupings. Among hierarchical retrieval methods, ReTreever achieves the best retrieval accuracy at the lowest latency, proving that this family of techniques can be viable in practical applications. 

**Abstract (ZH)**: 基于树的方法在文档检索中的应用：ReTreever提高了检索性能并增强了透明度 

---
# Generative Risk Minimization for Out-of-Distribution Generalization on Graphs 

**Title (ZH)**: 图中离分布泛化的生成风险最小化 

**Authors**: Song Wang, Zhen Tan, Yaochen Zhu, Chuxu Zhang, Jundong Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.07968)  

**Abstract**: Out-of-distribution (OOD) generalization on graphs aims at dealing with scenarios where the test graph distribution differs from the training graph distributions. Compared to i.i.d. data like images, the OOD generalization problem on graph-structured data remains challenging due to the non-i.i.d. property and complex structural information on graphs. Recently, several works on graph OOD generalization have explored extracting invariant subgraphs that share crucial classification information across different distributions. Nevertheless, such a strategy could be suboptimal for entirely capturing the invariant information, as the extraction of discrete structures could potentially lead to the loss of invariant information or the involvement of spurious information. In this paper, we propose an innovative framework, named Generative Risk Minimization (GRM), designed to generate an invariant subgraph for each input graph to be classified, instead of extraction. To address the challenge of optimization in the absence of optimal invariant subgraphs (i.e., ground truths), we derive a tractable form of the proposed GRM objective by introducing a latent causal variable, and its effectiveness is validated by our theoretical analysis. We further conduct extensive experiments across a variety of real-world graph datasets for both node-level and graph-level OOD generalization, and the results demonstrate the superiority of our framework GRM. 

**Abstract (ZH)**: 图结构数据离分布外泛化的生成风险最小化方法 

---
# Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature? 

**Title (ZH)**: 陷于文字的陷阱：大规模语言模型会在医学文献中被误导吗？ 

**Authors**: Hye Sun Yun, Karen Y.C. Zhang, Ramez Kouzy, Iain J. Marshall, Junyi Jessy Li, Byron C. Wallace  

**Link**: [PDF](https://arxiv.org/pdf/2502.07963)  

**Abstract**: Medical research faces well-documented challenges in translating novel treatments into clinical practice. Publishing incentives encourage researchers to present "positive" findings, even when empirical results are equivocal. Consequently, it is well-documented that authors often spin study results, especially in article abstracts. Such spin can influence clinician interpretation of evidence and may affect patient care decisions. In this study, we ask whether the interpretation of trial results offered by Large Language Models (LLMs) is similarly affected by spin. This is important since LLMs are increasingly being used to trawl through and synthesize published medical evidence. We evaluated 22 LLMs and found that they are across the board more susceptible to spin than humans. They might also propagate spin into their outputs: We find evidence, e.g., that LLMs implicitly incorporate spin into plain language summaries that they generate. We also find, however, that LLMs are generally capable of recognizing spin, and can be prompted in a way to mitigate spin's impact on LLM outputs. 

**Abstract (ZH)**: 医学研究在将新型治疗方法转化为临床实践方面面临已well-documented的挑战。发表激励促使研究人员呈现“正面”的研究结果，即使实证结果是模棱两可的。因此，已well-documented作者往往会在文章摘要中夸大研究成果，这种误导可能影响临床医生对证据的解读并可能影响患者的治疗决策。在这项研究中，我们探讨大型语言模型（LLMs）是否也会受到这种误导的影响，因为LLMs正越来越多地被用于筛选和综合已发表的医学证据。我们评估了22种LLM，并发现它们普遍比人类更容易受到误导的影响。它们也可能将误导融入到其输出中：我们发现证据表明，LLMs在生成的简洁语言摘要中隐含地包含了误导。不过，我们还发现，LLMs通常能够识别误导，并可以通过特定方式促使它们减轻误导对其输出的影响。 

---
# VSC-RL: Advancing Autonomous Vision-Language Agents with Variational Subgoal-Conditioned Reinforcement Learning 

**Title (ZH)**: VSC-RL: 基于变分子目标条件强化学习的自主视觉-语言代理提升 

**Authors**: Qingyuan Wu, Jianheng Liu, Jianye Hao, Jun Wang, Kun Shao  

**Link**: [PDF](https://arxiv.org/pdf/2502.07949)  

**Abstract**: State-of-the-art (SOTA) reinforcement learning (RL) methods enable the vision-language agents to learn from interactions with the environment without human supervision. However, they struggle with learning inefficiencies in tackling real-world complex sequential decision-making tasks, especially with sparse reward signals and long-horizon dependencies. To effectively address the issue, we introduce Variational Subgoal-Conditioned RL (VSC-RL), which reformulates the vision-language sequential decision-making task as a variational goal-conditioned RL problem, allowing us to leverage advanced optimization methods to enhance learning efficiency. Specifically, VSC-RL optimizes the SubGoal Evidence Lower BOund (SGC-ELBO), which consists of (a) maximizing the subgoal-conditioned return via RL and (b) minimizing the subgoal-conditioned difference with the reference policy. We theoretically demonstrate that SGC-ELBO is equivalent to the original optimization objective, ensuring improved learning efficiency without sacrificing performance guarantees. Additionally, for real-world complex decision-making tasks, VSC-RL leverages the vision-language model to autonomously decompose the goal into feasible subgoals, enabling efficient learning. Across various benchmarks, including challenging real-world mobile device control tasks, VSC-RL significantly outperforms the SOTA vision-language agents, achieving superior performance and remarkable improvement in learning efficiency. 

**Abstract (ZH)**: 基于变分子目标调节的强化学习方法（Variational Subgoal-Conditioned RL, VSC-RL）：解决视觉语言代理在现实世界复杂顺序决策任务中的学习效率问题 

---
# CREDAL: Close Reading of Data Models 

**Title (ZH)**: CREDAL: 数据模型的细致解读 

**Authors**: George Fletcher, Olha Nahurna, Matvii Prytula, Julia Stoyanovich  

**Link**: [PDF](https://arxiv.org/pdf/2502.07943)  

**Abstract**: Data models are necessary for the birth of data and of any data-driven system. Indeed, every algorithm, every machine learning model, every statistical model, and every database has an underlying data model without which the system would not be usable. Hence, data models are excellent sites for interrogating the (material, social, political, ...) conditions giving rise to a data system. Towards this, drawing inspiration from literary criticism, we propose to closely read data models in the same spirit as we closely read literary artifacts. Close readings of data models reconnect us with, among other things, the materiality, the genealogies, the techne, the closed nature, and the design of technical systems.
While recognizing from literary theory that there is no one correct way to read, it is nonetheless critical to have systematic guidance for those unfamiliar with close readings. This is especially true for those trained in the computing and data sciences, who too often are enculturated to set aside the socio-political aspects of data work. A systematic methodology for reading data models currently does not exist. To fill this gap, we present the CREDAL methodology for close readings of data models. We detail our iterative development process and present results of a qualitative evaluation of CREDAL demonstrating its usability, usefulness, and effectiveness in the critical study of data. 

**Abstract (ZH)**: 数据模型是数据及其任何数据驱动系统的诞生基础。事实上，每个算法、每种机器学习模型、每种统计模型和每个数据库都基于一个底层的数据模型，缺乏这一模型系统将无法使用。因此，数据模型是探究（物质的、社会的、政治的……）产生数据系统的条件的极佳场所。借鉴文学批评的方法，我们建议以同样的精神仔细解读数据模型。这种对数据模型的仔细解读使我们重新聚焦于其物质性、源流、技术技艺、封闭性质以及技术系统的构建。

尽管从文学理论中认识到没有一种阅读方式是完全正确的，但为初学者提供系统的指导依然至关重要。特别是对于那些训练背景在计算机和数据科学领域的人来说，他们往往被培养成忽视数据工作中的社会政治方面。当前尚不存在系统的方法来解读数据模型。为填补这一空白，我们提出了CREDAL方法论，用于仔细解读数据模型。我们详细描述了CREDAL方法论的迭代开发过程，并呈现了对CREDAL的定性评估结果，展示了其在批判性研究数据方面的适用性、有用性和有效性。 

---
# Educating a Responsible AI Workforce: Piloting a Curricular Module on AI Policy in a Graduate Machine Learning Course 

**Title (ZH)**: 培养负责任的AI workforce：在研究生机器学习课程中试点AI政策教学模块 

**Authors**: James Weichert, Hoda Eldardiry  

**Link**: [PDF](https://arxiv.org/pdf/2502.07931)  

**Abstract**: As artificial intelligence (AI) technologies begin to permeate diverse fields-from healthcare to education-consumers, researchers and policymakers are increasingly raising concerns about whether and how AI is regulated. It is therefore reasonable to anticipate that alignment with principles of 'ethical' or 'responsible' AI, as well as compliance with law and policy, will form an increasingly important part of AI development. Yet, for the most part, the conventional computer science curriculum is ill-equipped to prepare students for these challenges. To this end, we seek to explore how new educational content related to AI ethics and AI policy can be integrated into both ethics- and technical-focused courses. This paper describes a two-lecture 'AI policy module' that was piloted in a graduate-level introductory machine learning course in 2024. The module, which includes an in-class active learning game, is evaluated using data from student surveys before and after the lectures, and pedagogical motivations and considerations are discussed. We find that the module is successful in engaging otherwise technically-oriented students on the topic of AI policy, increasing student awareness of the social impacts of a variety of AI technologies and developing student interest in the field of AI regulation. 

**Abstract (ZH)**: 随着人工智能（AI）技术开始渗透到各个领域——从医疗到教育——消费者、研究人员和政策制定者越来越多地关注AI 的监管问题。因此，可以合理地预期，‘伦理’或‘负责任’的AI原则与法律法规的遵循将成为AI发展的一个越来越重要的部分。然而，常规的计算机科学课程往往无法为学生做好这些挑战的准备。为此，我们探索了如何将与AI伦理和AI政策相关的全新教育内容整合到伦理和技术导向的课程中。本文描述了2024年在一门研究生级入门机器学习课程中试行的两节课时的“AI政策模块”。该模块包括一个课堂互动学习游戏，并通过对学生课前和课后的调查数据进行评估，讨论了教学动机和考虑因素。我们发现，该模块有效地吸引了原本技术导向的学生对AI政策的关注，提高了学生对各种AI技术的社会影响的认识，并激发了学生对AI监管领域的兴趣。 

---
# NDAI Agreements 

**Title (ZH)**: NDAI协议 

**Authors**: Matthew Stephenson, Andrew Miller, Xyn Sun, Bhargav Annem, Rohan Parikh  

**Link**: [PDF](https://arxiv.org/pdf/2502.07924)  

**Abstract**: We study a fundamental challenge in the economics of innovation: an inventor must reveal details of a new idea to secure compensation or funding, yet such disclosure risks expropriation. We present a model in which a seller (inventor) and buyer (investor) bargain over an information good under the threat of hold-up. In the classical setting, the seller withholds disclosure to avoid misappropriation, leading to inefficiency. We show that trusted execution environments (TEEs) combined with AI agents can mitigate and even fully eliminate this hold-up problem. By delegating the disclosure and payment decisions to tamper-proof programs, the seller can safely reveal the invention without risking expropriation, achieving full disclosure and an efficient ex post transfer. Moreover, even if the invention's value exceeds a threshold that TEEs can fully secure, partial disclosure still improves outcomes compared to no disclosure. Recognizing that real AI agents are imperfect, we model "agent errors" in payments or disclosures and demonstrate that budget caps and acceptance thresholds suffice to preserve most of the efficiency gains.
Our results imply that cryptographic or hardware-based solutions can function as an "ironclad NDA," substantially mitigating the fundamental disclosure-appropriation paradox first identified by Arrow (1962) and Nelson (1959). This has far-reaching policy implications for fostering R&D, technology transfer, and collaboration. 

**Abstract (ZH)**: 我们研究创新经济学中的一个基本挑战：发明者必须披露新想法的细节以获得补偿或融资，但这种披露又面临被剥夺的风险。在经典的讨价还价框架中，卖方（发明者）会出于避免滥用的目的而保留披露，导致效率低下。我们展示了一种结合可信执行环境（TEE）和AI代理的模型，可以缓解甚至完全消除这种剥夺问题。通过将披露和支付决策委托给防篡改程序，卖方可以在不冒被剥夺风险的情况下安全地披露发明，实现完全披露和事后高效转移。即使发明的价值超过TEE可以完全保障的门槛，部分披露依然优于完全不披露。鉴于实际的AI代理可能存在缺陷，我们考虑了支付或披露中的“代理错误”，并证明预算限制和接受阈值足以保留大部分效率提升。我们的结果表明，加密或基于硬件的解决方案可以充当“坚不可摧的保密协议”，大幅缓解Arrow（1962）和Nelson（1959）首次识别的基本披露-滥用悖论。这对促进R&D、技术转移和合作具有深远的政策意义。 

---
# TransMLA: Multi-head Latent Attention Is All You Need 

**Title (ZH)**: TransMLA: 多头潜在注意力机制即所需的一切 

**Authors**: Fanxu Meng, Zengwei Yao, Muhan Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.07864)  

**Abstract**: Modern large language models (LLMs) often encounter communication bottlenecks on current hardware, rather than purely computational constraints. Multi-head Latent Attention (MLA) tackles this challenge by using low-rank matrices in the key-value (KV) layers, thereby allowing compressed latent KV states to be cached. This approach significantly reduces the KV cache size relative to traditional multi-head attention, leading to faster inference. Moreover, MLA employs an up-projection matrix to increase expressiveness, trading additional computation for reduced communication overhead. Although MLA has demonstrated efficiency and effectiveness in Deepseek V2/V3/R1, many major model providers still rely on Group Query Attention (GQA) and have not announced any plans to adopt MLA. In this paper, we show that GQA can always be represented by MLA while maintaining the same KV cache overhead, but the converse does not hold. To encourage broader use of MLA, we introduce **TransMLA**, a post-training method that converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen, Mixtral) into MLA-based models. After conversion, the model can undergo additional training to boost expressiveness without increasing the KV cache size. Furthermore, we plan to develop MLA-specific inference acceleration techniques to preserve low latency in transformed models, thus enabling more efficient distillation of Deepseek R1. 

**Abstract (ZH)**: 现代大型语言模型(Large Language Models, LLMs)在当前硬件上常常遇到通信瓶颈而非纯粹的计算约束。多头潜在注意(Multi-head Latent Attention, MLA)通过在键值(KV)层使用低秩矩阵来应对这一挑战，从而允许压缩的潜在KV状态被缓存。这种方法相对于传统的多头注意显著减小了KV缓存的大小，从而加快了推理速度。此外，MLA 通过使用上投影矩阵来增加表达能力，以减少通信开销为代价增加额外的计算。虽然 MLA 在 Deepseek V2/V3/R1 中展现了效率和有效性，但许多主要的模型提供商仍然依赖于组查询注意(Group Query Attention, GQA)，并且没有宣布任何采用 MLA 的计划。在本文中，我们证明 GQA 总是可以被 MLA 表示同时保持相同的 KV 缓存开销，但反之则不成立。为了促进 MLA 的更广泛使用，我们介绍了 **TransMLA**，这是一种后训练方法，可以将广泛使用的基于 GQA 的预训练模型（例如 LLaMA、Qwen、Mixtral）转换为基于 MLA 的模型。转换后，该模型可以在不增加 KV 缓存大小的情况下进行额外训练以提升表达能力。此外，我们计划开发 MLA 特定的推理加速技术以在转换后的模型中保持低延迟，从而实现 Deepseek R1 的更高效蒸馏。 

---
# ADMN: A Layer-Wise Adaptive Multimodal Network for Dynamic Input Noise and Compute Resources 

**Title (ZH)**: ADMN：一种适应层动态输入噪声和计算资源的多模态网络 

**Authors**: Jason Wu, Kang Yang, Lance Kaplan, Mani Srivastava  

**Link**: [PDF](https://arxiv.org/pdf/2502.07862)  

**Abstract**: Multimodal deep learning systems are deployed in dynamic scenarios due to the robustness afforded by multiple sensing modalities. Nevertheless, they struggle with varying compute resource availability (due to multi-tenancy, device heterogeneity, etc.) and fluctuating quality of inputs (from sensor feed corruption, environmental noise, etc.). Current multimodal systems employ static resource provisioning and cannot easily adapt when compute resources change over time. Additionally, their reliance on processing sensor data with fixed feature extractors is ill-equipped to handle variations in modality quality. Consequently, uninformative modalities, such as those with high noise, needlessly consume resources better allocated towards other modalities. We propose ADMN, a layer-wise Adaptive Depth Multimodal Network capable of tackling both challenges - it adjusts the total number of active layers across all modalities to meet compute resource constraints, and continually reallocates layers across input modalities according to their modality quality. Our evaluations showcase ADMN can match the accuracy of state-of-the-art networks while reducing up to 75% of their floating-point operations. 

**Abstract (ZH)**: 层适应深度多模态网络：应对计算资源波动和输入质量变化的挑战 

---
# BalanceKV: KV Cache Compression through Discrepancy Theory 

**Title (ZH)**: BalanceKV：通过差异理论实现的KV缓存压缩 

**Authors**: Insu Han, Michael Kapralov, Ekaterina Kochetkova, Kshiteej Sheth, Amir Zandieh  

**Link**: [PDF](https://arxiv.org/pdf/2502.07861)  

**Abstract**: Large language models (LLMs) have achieved impressive success, but their high memory requirements present challenges for long-context token generation. The memory complexity of long-context LLMs is primarily due to the need to store Key-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache compression method based on geometric sampling process stemming from Banaszczyk's vector balancing theory, which introduces dependencies informed by the geometry of keys and value tokens, and improves precision. BalanceKV offers both theoretically proven and empirically validated performance improvements over existing methods. 

**Abstract (ZH)**: 基于Banachczek向量平衡理论的几何采样过程的BalanceKV：一种KV缓存压缩方法 

---
# SNAP: Sequential Non-Ancestor Pruning for Targeted Causal Effect Estimation With an Unknown Graph 

**Title (ZH)**: SNAP: 序列非祖先修剪以估计未知图下的靶向因果效应 

**Authors**: Mátyás Schubert, Tom Claassen, Sara Magliacane  

**Link**: [PDF](https://arxiv.org/pdf/2502.07857)  

**Abstract**: Causal discovery can be computationally demanding for large numbers of variables. If we only wish to estimate the causal effects on a small subset of target variables, we might not need to learn the causal graph for all variables, but only a small subgraph that includes the targets and their adjustment sets. In this paper, we focus on identifying causal effects between target variables in a computationally and statistically efficient way. This task combines causal discovery and effect estimation, aligning the discovery objective with the effects to be estimated. We show that definite non-ancestors of the targets are unnecessary to learn causal relations between the targets and to identify efficient adjustments sets. We sequentially identify and prune these definite non-ancestors with our Sequential Non-Ancestor Pruning (SNAP) framework, which can be used either as a preprocessing step to standard causal discovery methods, or as a standalone sound and complete causal discovery algorithm. Our results on synthetic and real data show that both approaches substantially reduce the number of independence tests and the computation time without compromising the quality of causal effect estimations. 

**Abstract (ZH)**: 因果发现对于大量变量可能是计算上 demanding 的。如果我们只想估计目标变量子集的因果效应，我们可能不需要学习所有变量的因果图，而只需要包括目标变量及其调整集的较小子图。本文旨在以计算和统计效率的方式识别目标变量之间的因果效应。该任务结合了因果发现和效应估计，将发现目标与效果估计对齐。我们表明，目标的确定非祖先对于识别目标间的因果关系及其有效的调整集是不必要的。我们通过顺序非祖先剪枝（SNAP）框架识别并移除这些确定非祖先，该框架可以作为标准因果发现方法的预处理步骤，或作为独立的完整因果发现算法。我们在合成和真实数据上的结果表明，两种方法都能大幅减少独立性检验的数量和计算时间，而不牺牲因果效应估计的质量。 

---
# MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers 

**Title (ZH)**: MRS：基于常微分方程和随机微分方程求解器的快速均值回复扩散抽样器 

**Authors**: Ao Li, Wei Fang, Hongbo Zhao, Le Lu, Ge Yang, Minfeng Xu  

**Link**: [PDF](https://arxiv.org/pdf/2502.07856)  

**Abstract**: In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation. 

**Abstract (ZH)**: MR Sampler: Reducing Sampling NFEs for MR Diffusion in Controllable Generation 

---
# Vision-Language Models for Edge Networks: A Comprehensive Survey 

**Title (ZH)**: 边缘网络中的视觉-语言模型：一项综合综述 

**Authors**: Ahmed Sharshar, Latif U. Khan, Waseem Ullah, Mohsen Guizani  

**Link**: [PDF](https://arxiv.org/pdf/2502.07855)  

**Abstract**: Vision Large Language Models (VLMs) combine visual understanding with natural language processing, enabling tasks like image captioning, visual question answering, and video analysis. While VLMs show impressive capabilities across domains such as autonomous vehicles, smart surveillance, and healthcare, their deployment on resource-constrained edge devices remains challenging due to processing power, memory, and energy limitations. This survey explores recent advancements in optimizing VLMs for edge environments, focusing on model compression techniques, including pruning, quantization, knowledge distillation, and specialized hardware solutions that enhance efficiency. We provide a detailed discussion of efficient training and fine-tuning methods, edge deployment challenges, and privacy considerations. Additionally, we discuss the diverse applications of lightweight VLMs across healthcare, environmental monitoring, and autonomous systems, illustrating their growing impact. By highlighting key design strategies, current challenges, and offering recommendations for future directions, this survey aims to inspire further research into the practical deployment of VLMs, ultimately making advanced AI accessible in resource-limited settings. 

**Abstract (ZH)**: 视觉大语言模型（VLMs）结合了视觉理解和自然语言处理，能够实现图像字幕、视觉问答和视频分析等任务。尽管VLMs在自主车辆、智能监控和医疗健康等领域展示了令人印象深刻的能力，但由于资源受限边缘设备在计算能力、内存和能耗方面的限制，其部署仍然具有挑战性。本文综述了针对边缘环境优化VLMs的最新进展，重点讨论了模型压缩技术，包括剪枝、量化、知识蒸馏和专用硬件解决方案，以提高效率。本文详细讨论了高效训练和微调方法、边缘部署挑战和隐私问题，并讨论了轻量级VLMs在医疗保健、环境监测和自主系统中的多元应用，展示了它们日益增长的影响。通过强调关键设计策略、当前挑战并提出未来方向的建议，本文旨在激发进一步研究，最终使先进的AI技术在资源受限的环境中得以实用化。 

---
# Understanding Classifier-Free Guidance: High-Dimensional Theory and Non-Linear Generalizations 

**Title (ZH)**: 无分类器指导的理解：高维理论与非线性推广 

**Authors**: Krunoslav Lehman Pavasovic, Jakob Verbeek, Giulio Biroli, Marc Mezard  

**Link**: [PDF](https://arxiv.org/pdf/2502.07849)  

**Abstract**: Recent studies have raised concerns about the effectiveness of Classifier-Free Guidance (CFG), indicating that in low-dimensional settings, it can lead to overshooting the target distribution and reducing sample diversity. In this work, we demonstrate that in infinite and sufficiently high-dimensional contexts CFG effectively reproduces the target distribution, revealing a blessing-of-dimensionality result. Additionally, we explore finite-dimensional effects, precisely characterizing overshoot and variance reduction. Based on our analysis, we introduce non-linear generalizations of CFG. Through numerical simulations on Gaussian mixtures and experiments on class-conditional and text-to-image diffusion models, we validate our analysis and show that our non-linear CFG offers improved flexibility and generation quality without additional computation cost. 

**Abstract (ZH)**: 近年来的研究对Classifier-Free Guidance (CFG)的有效性提出了担忧，表明在低维设置中，它可能导致过度拟合目标分布并降低样本多样性。在本工作中，我们展示了在无穷维和充分高维的背景下，CFG能够有效地再现目标分布，揭示了维度优势的结果。此外，我们探讨了有限维度的影响，精确刻画了过度拟合和方差减少。基于我们的分析，我们提出了非线性化的CFG一般化方法。通过高斯混合模型的数值模拟以及类条件和文本到图像扩散模型的实验，我们验证了我们的分析，并展示了我们的非线性CFG能够在不增加计算成本的情况下提供更好的灵活性和生成质量。 

---
# Spread them Apart: Towards Robust Watermarking of Generated Content 

**Title (ZH)**: 分散开来：面向生成内容的稳健水印技术探索 

**Authors**: Mikhail Pautov, Danil Ivanov, Andrey V. Galichin, Oleg Rogov, Ivan Oseledets  

**Link**: [PDF](https://arxiv.org/pdf/2502.07845)  

**Abstract**: Generative models that can produce realistic images have improved significantly in recent years. The quality of the generated content has increased drastically, so sometimes it is very difficult to distinguish between the real images and the generated ones. Such an improvement comes at a price of ethical concerns about the usage of the generative models: the users of generative models can improperly claim ownership of the generated content protected by a license. In this paper, we propose an approach to embed watermarks into the generated content to allow future detection of the generated content and identification of the user who generated it. The watermark is embedded during the inference of the model, so the proposed approach does not require the retraining of the latter. We prove that watermarks embedded are guaranteed to be robust against additive perturbations of a bounded magnitude. We apply our method to watermark diffusion models and show that it matches state-of-the-art watermarking schemes in terms of robustness to different types of synthetic watermark removal attacks. 

**Abstract (ZH)**: 生成模型在 recent years 中已大幅提高生成逼真图像的能力。生成内容的质量大幅提升，有时很难区分真实的图像和生成的图像。这种进步伴随着生成模型使用中伦理问题的担忧：生成模型的使用者可能会不当声称受版权保护的生成内容的所有权。本文提出了一种将水印嵌入生成内容的方法，以允许未来检测生成的内容并识别生成内容的用户。水印在模型推理过程中嵌入，因此不需要重新训练模型。我们证明嵌入的水印对幅度有限的附加扰动具有鲁棒性。我们将该方法应用于水印扩散模型，并显示其在不同类型的合成水印去除攻击下的鲁棒性与现有最佳水印方案相当。 

---
# Column-wise Quantization of Weights and Partial Sums for Accurate and Efficient Compute-In-Memory Accelerators 

**Title (ZH)**: 列方向权重和部分和的量化解码以实现准确高效的计算在内存加速器 

**Authors**: Jiyoon Kim, Kang Eun Jeon, Yulhwa Kim, Jong Hwan Ko  

**Link**: [PDF](https://arxiv.org/pdf/2502.07842)  

**Abstract**: Compute-in-memory (CIM) is an efficient method for implementing deep neural networks (DNNs) but suffers from substantial overhead from analog-to-digital converters (ADCs), especially as ADC precision increases. Low-precision ADCs can re- duce this overhead but introduce partial-sum quantization errors degrading accuracy. Additionally, low-bit weight constraints, im- posed by cell limitations and the need for multiple cells for higher- bit weights, present further challenges. While fine-grained partial- sum quantization has been studied to lower ADC resolution effectively, weight granularity, which limits overall partial-sum quantized accuracy, remains underexplored. This work addresses these challenges by aligning weight and partial-sum quantization granularities at the column-wise level. Our method improves accuracy while maintaining dequantization overhead, simplifies training by removing two-stage processes, and ensures robustness to memory cell variations via independent column-wise scale factors. We also propose an open-source CIM-oriented convolution framework to handle fine-grained weights and partial-sums effi- ciently, incorporating a novel tiling method and group convolution. Experimental results on ResNet-20 (CIFAR-10, CIFAR-100) and ResNet-18 (ImageNet) show accuracy improvements of 0.99%, 2.69%, and 1.01%, respectively, compared to the best-performing related works. Additionally, variation analysis reveals the robust- ness of our method against memory cell variations. These findings highlight the effectiveness of our quantization scheme in enhancing accuracy and robustness while maintaining hardware efficiency in CIM-based DNN implementations. Our code is available at this https URL. 

**Abstract (ZH)**: 基于计算存储器的深度神经网络低精度权重和部分和量化方法 

---
# NanoVLMs: How small can we go and still make coherent Vision Language Models? 

**Title (ZH)**: NanoVLMs: 我们能制造出多小的仍然可以生成连贯的视觉语言模型？ 

**Authors**: Mukund Agarwalla, Himanshu Kumar, Raj Dandekar, Rajat Dandekar, Sreedath Panat  

**Link**: [PDF](https://arxiv.org/pdf/2502.07838)  

**Abstract**: Vision-Language Models (VLMs), such as GPT-4V and Llama 3.2 vision, have garnered significant research attention for their ability to leverage Large Language Models (LLMs) in multimodal tasks. However, their potential is constrained by inherent challenges, including proprietary restrictions, substantial computational demands, and limited accessibility. Smaller models, such as GIT and BLIP, exhibit marked limitations, often failing to generate coherent and consistent text beyond a few tokens, even with extensive training. This underscores a pivotal inquiry: how small can a VLM be and still produce fluent and consistent text? Drawing inspiration from the exceptional learning process of 3-4 year old children, who rely heavily on visual cues for understanding and communication, we introduce two novel datasets: ShortDesc (featuring concise image descriptions) and LongDesc (containing more detailed image descriptions). These datasets consist of image-text pairs where the text is restricted to the simple vocabulary and syntax typically used by young children, generated with a scaled- down model, GPT-4o. Using these datasets, we demonstrate that it is possible to train VLMs that are significantly smaller, up to 10 times smaller than state of the art(SOTA) small VLMs while maintaining architectural simplicity. To evaluate the outputs, we leverage GPT-4o to grade the text, as if stories written by students, on creativity, meaningfulness, and consistency, assigning scores out of 10. This method addresses limitations of standard benchmarks by accommodating unstructured outputs and providing a multidimensional evaluation of the model capabilities. Our findings contribute to the development of lightweight, accessible multimodal models for resource constrained environments. 

**Abstract (ZH)**: Vision-Language 模型（VLMs），如 GPT-4V 和 Llama 3.2 视觉版本，因其能够利用大型语言模型（LLMs）处理多模态任务而引起了广泛关注。然而，它们的潜力受制于固有的挑战，包括产权限制、巨大的计算需求以及有限的可访问性。较小的模型，如 GIT 和 BLIP，表现出明显的局限性，即使经过大量训练，往往也无法生成连贯且一致的文本，甚至在几个标记内也是如此。这突显了一个关键问题：一个 VLM 最小可以多小，仍然能够生成流畅且一致的文本？受 3-4 岁儿童卓越学习过程的启发，他们很大程度上依赖于视觉线索来进行理解和交流，我们提出了两个新的数据集：ShortDesc（简短的图像描述）和 LongDesc（详细的图像描述）。这些数据集包含图像-文本对，其中文本仅限制在年轻儿童通常使用的简单词汇和语法，由一个缩小版模型 GPT-4o 生成。利用这些数据集，我们证明了有可能训练出显著更小的 VLMs，最多可小 10 倍于现有最佳小 VLM，同时保持架构的简洁性。为了评估输出，我们利用 GPT-4o 以类似评估学生写作的方式对文本进行评分，评估创建性、意义性和一致性，并给出 10 分制评分。这种方法克服了标准基准的局限性，能够容纳非结构化输出，并提供对模型能力多维度评估。我们的发现为资源受限环境下的轻量级、可访问的多模态模型的发展做出了贡献。 

---
# Bridging LLM-Generated Code and Requirements: Reverse Generation technique and SBC Metric for Developer Insights 

**Title (ZH)**: LLM生成代码与需求 bridging：反向生成技术及SBC指标为开发者提供洞见 

**Authors**: Ahilan Ayyachamy Nadar Ponnusamy  

**Link**: [PDF](https://arxiv.org/pdf/2502.07835)  

**Abstract**: The rise of Large Language Models (LLMs) in software engineering, particularly in code generation, has garnered significant attention. However, assessing the quality of AI-generated code remains a challenge due to the inherent complexity of programming tasks and the lack of robust evaluation metrics that align well with human judgment. Traditional token-based metrics such as BLEU and ROUGE, while commonly used in natural language processing, exhibit weak correlations with human assessments in code intelligence and verification tasks. Furthermore, these metrics are primarily research focused and are not designed for seamless integration into the software development lifecycle, limiting their practical utility for developers seeking to improve code quality and security.
AI-assisted coding has been shown to be more beneficial for senior developers, as they possess the expertise to critically evaluate the generated code for correctness, completeness, and compliance. In contrast, junior developers may struggle to identify hallucinations, missing functionality, or incorrect logic in AI-generated code. To bridge this gap, This paper introduces a novel scoring mechanism called the SBC score, which is based on a reverse generation technique that leverages the natural language generation capabilities of LLMs. Unlike direct code analysis, our approach reconstructs system requirements from AI-generated code and compares them with the original specifications to quantify accuracy. The SBC score combines semantic similarity, BLEU, and completeness analysis, providing actionable insights to developers by highlighting missing features and hallucinations. Our code and datasets are available on GitHub 

**Abstract (ZH)**: 大型语言模型（LLMs）在软件工程中的兴起，尤其是在代码生成领域的应用，引起了广泛关注。然而，评估AI生成代码的质量仍然是一个挑战，这主要是由于编程任务的固有复杂性以及缺乏与人类判断相匹配的稳健评估指标。传统的基于token的指标，如BLEU和ROUGE，在自然语言处理中广泛应用，但在代码智能和验证任务中与人类评估的关联性较弱。此外，这些指标主要侧重于研究，未能设计为无缝集成到软件开发生命周期中，限制了它们在开发人员提高代码质量和安全性方面的作用。

AI辅助编码对资深开发人员更有益，因为资深开发人员拥有批判性评估生成代码正确性、完整性和合规性的专业知识。相比之下，初级开发人员可能难以识别AI生成代码中的幻觉、缺少功能或逻辑错误。为弥合这一差距，本文引入了一种新的评分机制——SBC评分，该机制基于一种逆向生成技术，利用大型语言模型的自然语言生成能力。与直接的代码分析不同，我们方法是从AI生成的代码中重构系统需求，并将其与原始规范进行比较，以量化准确性。SBC评分结合了语义相似性、BLEU和完整性分析，通过突出显示缺失功能和幻觉为开发人员提供可操作的见解。我们的代码和数据集可在GitHub上获取。 

---
# MEMHD: Memory-Efficient Multi-Centroid Hyperdimensional Computing for Fully-Utilized In-Memory Computing Architectures 

**Title (ZH)**: MEMHD: 高效内存多中心超维计算以充分利用内存计算架构 

**Authors**: Do Yeong Kang, Yeong Hwan Oh, Chanwook Hwang, Jinhee Kim, Kang Eun Jeon, Jong Hwan Ko  

**Link**: [PDF](https://arxiv.org/pdf/2502.07834)  

**Abstract**: The implementation of Hyperdimensional Computing (HDC) on In-Memory Computing (IMC) architectures faces significant challenges due to the mismatch between highdimensional vectors and IMC array sizes, leading to inefficient memory utilization and increased computation cycles. This paper presents MEMHD, a Memory-Efficient Multi-centroid HDC framework designed to address these challenges. MEMHD introduces a clustering-based initialization method and quantization aware iterative learning for multi-centroid associative memory. Through these approaches and its overall architecture, MEMHD achieves a significant reduction in memory requirements while maintaining or improving classification accuracy. Our approach achieves full utilization of IMC arrays and enables one-shot (or few-shot) associative search. Experimental results demonstrate that MEMHD outperforms state-of-the-art binary HDC models, achieving up to 13.69% higher accuracy with the same memory usage, or 13.25x more memory efficiency at the same accuracy level. Moreover, MEMHD reduces computation cycles by up to 80x and array usage by up to 71x compared to baseline IMC mapping methods when mapped to 128x128 IMC arrays, while significantly improving energy and computation cycle efficiency. 

**Abstract (ZH)**: Memory-Efficient Multi-centroid Hyperdimensional Computing Framework for In-Memory Computing Architectures 

---
# SHARP: Accelerating Language Model Inference by SHaring Adjacent layers with Recovery Parameters 

**Title (ZH)**: SHARP: 通过共享相邻层并恢复参数加速语言模型推理 

**Authors**: Yiping Wang, Hanxian Huang, Yifang Chen, Jishen Zhao, Simon Shaolei Du, Yuandong Tian  

**Link**: [PDF](https://arxiv.org/pdf/2502.07832)  

**Abstract**: While Large language models (LLMs) have advanced natural language processing tasks, their growing computational and memory demands make deployment on resource-constrained devices like mobile phones increasingly challenging. In this paper, we propose SHARP (SHaring Adjacent Layers with Recovery Parameters), a novel approach to accelerate LLM inference by sharing parameters across adjacent layers, thus reducing memory load overhead, while introducing low-rank recovery parameters to maintain performance. Inspired by observations that consecutive layers have similar outputs, SHARP employs a two-stage recovery process: Single Layer Warmup (SLW), and Supervised Fine-Tuning (SFT). The SLW stage aligns the outputs of the shared layers using L_2 loss, providing a good initialization for the following SFT stage to further restore the model performance. Extensive experiments demonstrate that SHARP can recover the model's perplexity on various in-distribution tasks using no more than 50k fine-tuning data while reducing the number of stored MLP parameters by 38% to 65%. We also conduct several ablation studies of SHARP and show that replacing layers towards the later parts of the model yields better performance retention, and that different recovery parameterizations perform similarly when parameter counts are matched. Furthermore, SHARP saves 42.8% in model storage and reduces the total inference time by 42.2% compared to the original Llama2-7b model on mobile devices. Our results highlight SHARP as an efficient solution for reducing inference costs in deploying LLMs without the need for pretraining-scale resources. 

**Abstract (ZH)**: SHARP：通过共享相邻层参数实现语言模型推理加速与性能恢复 

---
# Captured by Captions: On Memorization and its Mitigation in CLIP Models 

**Title (ZH)**: 被描述所困：关于CLIP模型中记忆化及其缓解方法的研究 

**Authors**: Wenhao Wang, Adam Dziedzic, Grace C. Kim, Michael Backes, Franziska Boenisch  

**Link**: [PDF](https://arxiv.org/pdf/2502.07830)  

**Abstract**: Multi-modal models, such as CLIP, have demonstrated strong performance in aligning visual and textual representations, excelling in tasks like image retrieval and zero-shot classification. Despite this success, the mechanisms by which these models utilize training data, particularly the role of memorization, remain unclear. In uni-modal models, both supervised and self-supervised, memorization has been shown to be essential for generalization. However, it is not well understood how these findings would apply to CLIP, which incorporates elements from both supervised learning via captions that provide a supervisory signal similar to labels, and from self-supervised learning via the contrastive objective. To bridge this gap in understanding, we propose a formal definition of memorization in CLIP (CLIPMem) and use it to quantify memorization in CLIP models. Our results indicate that CLIP's memorization behavior falls between the supervised and self-supervised paradigms, with "mis-captioned" samples exhibiting highest levels of memorization. Additionally, we find that the text encoder contributes more to memorization than the image encoder, suggesting that mitigation strategies should focus on the text domain. Building on these insights, we propose multiple strategies to reduce memorization while at the same time improving utility--something that had not been shown before for traditional learning paradigms where reducing memorization typically results in utility decrease. 

**Abstract (ZH)**: CLIP模型中的记忆机制正式定义及其量化 

---
# Some things to know about achieving artificial general intelligence 

**Title (ZH)**: 关于实现人工通用智能需要了解的一些事情 

**Authors**: Herbert Roitblat  

**Link**: [PDF](https://arxiv.org/pdf/2502.07828)  

**Abstract**: Current and foreseeable GenAI models are not capable of achieving artificial general intelligence because they are burdened with anthropogenic debt. They depend heavily on human input to provide well-structured problems, architecture, and training data. They cast every problem as a language pattern learning problem and are thus not capable of the kind of autonomy needed to achieve artificial general intelligence. Current models succeed at their tasks because people solve most of the problems to which these models are directed, leaving only simple computations for the model to perform, such as gradient descent. Another barrier is the need to recognize that there are multiple kinds of problems, some of which cannot be solved by available computational methods (for example, "insight problems"). Current methods for evaluating models (benchmarks and tests) are not adequate to identify the generality of the solutions, because it is impossible to infer the means by which a problem was solved from the fact of its solution. A test could be passed, for example, by a test-specific or a test-general method. It is a logical fallacy (affirming the consequent) to infer a method of solution from the observation of success. 

**Abstract (ZH)**: 当前和可预见的生成式AI模型无法实现人工通用智能，因为它们背负着人类债务。它们高度依赖人类输入来提供结构化的問題、架构和训练数据。它们将每个问题视为语言模式学习问题，因此无法具备实现人工通用智能所需的自主性。当前模型能够成功完成任务是因为人们基本上已经解决了这些模型所针对的问题，留给模型的只是简单的计算任务，例如梯度下降。另一个障碍是认识到存在多种类型的問題，其中一些问题无法通过现有的计算方法解决（例如，“洞察型问题”）。当前用于评估模型的方法（基准测试和测试）不足以识别解决方案的普遍性，因为无法从问题被解决的事实中推断出解决问题的方法。例如，一个测试可以通过特定于该测试的方法或者一般性的方法来通过。仅从成功的观察中推断解决方案的方法是一种逻辑谬误（肯定后件）。 

---
# Implicit Language Models are RNNs: Balancing Parallelization and Expressivity 

**Title (ZH)**: 隐含语言模型是RNN：权衡并行性和表达性 

**Authors**: Mark Schöne, Babak Rahmani, Heiner Kremer, Fabian Falck, Hitesh Ballani, Jannes Gladrow  

**Link**: [PDF](https://arxiv.org/pdf/2502.07827)  

**Abstract**: State-space models (SSMs) and transformers dominate the language modeling landscape. However, they are constrained to a lower computational complexity than classical recurrent neural networks (RNNs), limiting their expressivity. In contrast, RNNs lack parallelization during training, raising fundamental questions about the trade off between parallelization and expressivity. We propose implicit SSMs, which iterate a transformation until convergence to a fixed point. Theoretically, we show that implicit SSMs implement the non-linear state-transitions of RNNs. Empirically, we find that only approximate fixed-point convergence suffices, enabling the design of a scalable training curriculum that largely retains parallelization, with full convergence required only for a small subset of tokens. Our approach demonstrates superior state-tracking capabilities on regular languages, surpassing transformers and SSMs. We further scale implicit SSMs to natural language reasoning tasks and pretraining of large-scale language models up to 1.3B parameters on 207B tokens - representing, to our knowledge, the largest implicit model trained to date. Notably, our implicit models outperform their explicit counterparts on standard benchmarks. 

**Abstract (ZH)**: 隐状态模型和变压器主导了语言建模领域。然而，它们在计算复杂性上受到限制，不及传统的循环神经网络（RNNs），限制了其表达能力。相比之下，RNNs在训练过程中缺乏并行化，提出了并行化与表达能力之间基本权衡问题。我们提出了隐式状态模型（Implicit State-Space Models, ISSMs），其通过迭代变换直到收敛到不动点。理论上，我们证明了隐式状态模型实现了RNNs的非线性状态转换。实验上，我们发现仅需近似的不动点收敛即可，这使得我们可以设计一个可扩展的训练课程，大幅保留并行化能力，仅对一小部分标记要求完全收敛。我们的方法在正规语言上展示了卓越的状态追踪能力，超越了变压器和隐状态模型。我们进一步将隐式状态模型扩展到自然语言推理任务和大型语言模型的预训练，其中参数量高达13亿，标记量达到2070亿，据我们所知，这是迄今为止训练的最大隐式模型。值得注意的是，我们的隐式模型在标准基准测试中优于其显式对应模型。 

---
# Pre-Trained Video Generative Models as World Simulators 

**Title (ZH)**: 预训练视频生成模型作为世界模拟器 

**Authors**: Haoran He, Yang Zhang, Liang Lin, Zhongwen Xu, Ling Pan  

**Link**: [PDF](https://arxiv.org/pdf/2502.07825)  

**Abstract**: Video generative models pre-trained on large-scale internet datasets have achieved remarkable success, excelling at producing realistic synthetic videos. However, they often generate clips based on static prompts (e.g., text or images), limiting their ability to model interactive and dynamic scenarios. In this paper, we propose Dynamic World Simulation (DWS), a novel approach to transform pre-trained video generative models into controllable world simulators capable of executing specified action trajectories. To achieve precise alignment between conditioned actions and generated visual changes, we introduce a lightweight, universal action-conditioned module that seamlessly integrates into any existing model. Instead of focusing on complex visual details, we demonstrate that consistent dynamic transition modeling is the key to building powerful world simulators. Building upon this insight, we further introduce a motion-reinforced loss that enhances action controllability by compelling the model to capture dynamic changes more effectively. Experiments demonstrate that DWS can be versatilely applied to both diffusion and autoregressive transformer models, achieving significant improvements in generating action-controllable, dynamically consistent videos across games and robotics domains. Moreover, to facilitate the applications of the learned world simulator in downstream tasks such as model-based reinforcement learning, we propose prioritized imagination to improve sample efficiency, demonstrating competitive performance compared with state-of-the-art methods. 

**Abstract (ZH)**: 基于大型互联网数据集预训练的视频生成模型已取得显著成功，擅长生成逼真的合成视频。然而，它们通常根据静态提示（如文本或图像）生成片段，限制了其处理交互和动态场景的能力。本文提出了一种新颖的方法——动态世界模拟（DWS），旨在将预训练的视频生成模型转化为可控的世界模拟器，能够执行指定的动作轨迹。为实现条件动作与生成视觉变化的精确对齐，我们引入了一个轻量级、通用的动作条件模块，可以无缝集成到任何现有模型中。我们表明，一致的动态过渡建模是构建强大世界模拟器的关键。基于此见解，我们进一步引入了一种动力增强损失，通过促使模型更有效地捕捉动态变化来增强动作可控性。实验表明，DWS可以灵活应用于扩散和自回归变压器模型，实现显著改善的游戏和机器人领域中动作可控且动态一致的视频生成。此外，为促进所学世界模拟器在下游任务如模型基础强化学习中的应用，我们提出了优先想象以提高样本效率，并展示了与最新方法相当的性能。 

---
# Runtime Tunable Tsetlin Machines for Edge Inference on eFPGAs 

**Title (ZH)**: 用于eFPGAs的运行时可调Tsetlin机器的边缘推理 

**Authors**: Tousif Rahman, Gang Mao, Bob Pattison, Sidharth Maheshwari, Marcos Sartori, Adrian Wheeldon, Rishad Shafik, Alex Yakovlev  

**Link**: [PDF](https://arxiv.org/pdf/2502.07823)  

**Abstract**: Embedded Field-Programmable Gate Arrays (eFPGAs) allow for the design of hardware accelerators of edge Machine Learning (ML) applications at a lower power budget compared with traditional FPGA platforms. However, the limited eFPGA logic and memory significantly constrain compute capabilities and model size. As such, ML application deployment on eFPGAs is in direct contrast with the most recent FPGA approaches developing architecture-specific implementations and maximizing throughput over resource frugality. This paper focuses on the opposite side of this trade-off: the proposed eFPGA accelerator focuses on minimizing resource usage and allowing flexibility for on-field recalibration over throughput. This allows for runtime changes in model size, architecture, and input data dimensionality without offline resynthesis. This is made possible through the use of a bitwise compressed inference architecture of the Tsetlin Machine (TM) algorithm. TM compute does not require any multiplication operations, being limited to only bitwise AND, OR, NOT, summations and additions. Additionally, TM model compression allows the entire model to fit within the on-chip block RAM of the eFPGA. The paper uses this accelerator to propose a strategy for runtime model tuning in the field. The proposed approach uses 2.5x fewer Look-up-Tables (LUTs) and 3.38x fewer registers than the current most resource-fugal design and achieves up to 129x energy reduction compared with low-power microcontrollers running the same ML application. 

**Abstract (ZH)**: 嵌入式可编程门阵列（eFPGAs）允许以较低的能耗设计边缘机器学习（ML）应用的硬件加速器，相比传统的FPGA平台具有优势。然而，有限的eFPGA逻辑和内存显著限制了计算能力和模型规模。因此，eFPGA上的ML应用部署与最新的FPGA方法形成了直接对比，后者旨在通过特定架构实现最大吞吐量，而不是资源节约。本文关注这一权衡的另一面：提出的eFPGA加速器旨在最小化资源使用并允许在现场重新校准以灵活性超过吞吐量。这使得可以在运行时改变模型大小、架构和输入数据维度而无需离线重新综合成为可能。这通过使用Tsetlin机器（TM）算法的位级压缩推理架构得以实现。TM计算仅限于位级AND、OR、NOT、求和和加法操作，无需任何乘法操作。此外，TM模型压缩使整个模型能够fits入eFPGA片上块RAM中。本文利用该加速器提出了现场模型调整的策略。所提出的方法在查找表（LUTs）和寄存器的使用上分别比当前最资源节约的设计少2.5倍和3.38倍，并且与低功耗微控制器上运行相同ML应用相比，能耗最多可降低129倍。 

---
# PDM-SSD: Single-Stage Three-Dimensional Object Detector With Point Dilation 

**Title (ZH)**: PDM-SSD：基于点扩张的单阶段三维物体检测器 

**Authors**: Ao Liang, Haiyang Hua, Jian Fang, Wenyu Chen, Huaici Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2502.07822)  

**Abstract**: Current Point-based detectors can only learn from the provided points, with limited receptive fields and insufficient global learning capabilities for such targets. In this paper, we present a novel Point Dilation Mechanism for single-stage 3D detection (PDM-SSD) that takes advantage of these two representations. Specifically, we first use a PointNet-style 3D backbone for efficient feature encoding. Then, a neck with Point Dilation Mechanism (PDM) is used to expand the feature space, which involves two key steps: point dilation and feature filling. The former expands points to a certain size grid centered around the sampled points in Euclidean space. The latter fills the unoccupied grid with feature for backpropagation using spherical harmonic coefficients and Gaussian density function in terms of direction and scale. Next, we associate multiple dilation centers and fuse coefficients to obtain sparse grid features through height compression. Finally, we design a hybrid detection head for joint learning, where on one hand, the scene heatmap is predicted to complement the voting point set for improved detection accuracy, and on the other hand, the target probability of detected boxes are calibrated through feature fusion. On the challenging Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) dataset, PDM-SSD achieves state-of-the-art results for multi-class detection among single-modal methods with an inference speed of 68 frames. We also demonstrate the advantages of PDM-SSD in detecting sparse and incomplete objects through numerous object-level instances. Additionally, PDM can serve as an auxiliary network to establish a connection between sampling points and object centers, thereby improving the accuracy of the model without sacrificing inference speed. Our code will be available at this https URL. 

**Abstract (ZH)**: 基于点的检测器只能从提供的点中学习，具有有限的感受野和不足的全局学习能力。本文提出了一种新型的点膨胀机制单阶段3D检测方法（PDM-SSD），充分利用了这两种表示。具体地，我们首先使用PointNet风格的3D骨干网络进行高效的特征编码。然后，采用包含点膨胀机制（PDM）的主干网络来扩展特征空间，这涉及两个关键步骤：点膨胀和特征填充。前者将采样点为中心的欧几里得空间中的点扩展到一定大小的网格。后者使用球谐系数和高斯密度函数按方向和尺度填充未占的网格，以便于反向传播。接着，我们通过高度压缩将多个膨胀中心关联起来并融合系数，以获得稀疏网格特征。最后，我们设计了一个混合检测头进行联合学习，一方面，预测场景热图以补充投票点集，提高检测准确性；另一方面，通过特征融合校准检测框的目标概率。在具有挑战性的Karlsruhe Institute of Technology和Toyota Technological Institute（KITTI）数据集上，PDM-SSD 在单模态方法中实现了多类检测的最先进结果，推理速度为68帧。我们还通过大量实例展示了PDM-SSD 在检测稀疏和不完整对象方面的优势。此外，PDM 可以作为辅助网络，建立采样点与对象中心之间的联系，从而在不牺牲推理速度的情况下提高模型的准确性。我们的代码将在此网址提供： this https URL。 

---
# Amnesia as a Catalyst for Enhancing Black Box Pixel Attacks in Image Classification and Object Detection 

**Title (ZH)**: 记忆丧失作为增强图像分类和对象检测的黑盒像素攻击催化剂 

**Authors**: Dongsu Song, Daehwa Ko, Jay Hoon Jung  

**Link**: [PDF](https://arxiv.org/pdf/2502.07821)  

**Abstract**: It is well known that query-based attacks tend to have relatively higher success rates in adversarial black-box attacks. While research on black-box attacks is actively being conducted, relatively few studies have focused on pixel attacks that target only a limited number of pixels. In image classification, query-based pixel attacks often rely on patches, which heavily depend on randomness and neglect the fact that scattered pixels are more suitable for adversarial attacks. Moreover, to the best of our knowledge, query-based pixel attacks have not been explored in the field of object detection. To address these issues, we propose a novel pixel-based black-box attack called Remember and Forget Pixel Attack using Reinforcement Learning(RFPAR), consisting of two main components: the Remember and Forget processes. RFPAR mitigates randomness and avoids patch dependency by leveraging rewards generated through a one-step RL algorithm to perturb pixels. RFPAR effectively creates perturbed images that minimize the confidence scores while adhering to limited pixel constraints. Furthermore, we advance our proposed attack beyond image classification to object detection, where RFPAR reduces the confidence scores of detected objects to avoid detection. Experiments on the ImageNet-1K dataset for classification show that RFPAR outperformed state-of-the-art query-based pixel attacks. For object detection, using the MSCOCO dataset with YOLOv8 and DDQ, RFPAR demonstrates comparable mAP reduction to state-of-the-art query-based attack while requiring fewer query. Further experiments on the Argoverse dataset using YOLOv8 confirm that RFPAR effectively removed objects on a larger scale dataset. Our code is available at this https URL. 

**Abstract (ZH)**: 基于查询的像素攻击在对抗性黑盒攻击中成功率较高已是常识。尽管黑盒攻击的研究正在积极进行，但仍相对较少关注仅针对少量像素的像素攻击。在图像分类中，基于查询的像素攻击往往依赖于补丁，而这严重依赖随机性并忽视了分散像素更适合对抗性攻击的事实。此外，据我们所知，基于查询的像素攻击在目标检测领域尚未被探索。为解决这些问题，我们提出了一种基于像素的新型黑盒攻击——基于强化学习的记忆与遗忘像素攻击（RFPAR），该攻击由记忆和遗忘过程两大部分组成。RFPAR通过利用一维RL算法生成的奖励来扰动像素，从而减轻了随机性并避免了补丁依赖。RFPAR有效地创建了能够最小化置信分数且符合像素约束的扰动图像。此外，我们将提出的攻击从图像分类推进到目标检测领域，在目标检测中，RFPAR降低了检测目标的置信分数以避免检测。在使用ImageNet-1K数据集进行的分类实验中，RFPAR的性能优于最先进的基于查询的像素攻击。在使用MSCOCO数据集和YOLOv8以及DDQ的检测实验中，RFPAR表现出与最先进的基于查询的攻击相当的mAP减少效果，同时所需的查询次数较少。在使用Argoverse数据集进行的进一步实验中，使用YOLOv8验证了RFPAR在更大规模数据集上有效移除对象的能力。关于我们代码的相关信息，请参阅此链接。 

---
# Low-Rank Compression for IMC Arrays 

**Title (ZH)**: IMC阵列的低秩压缩 

**Authors**: Kang Eun Jeon, Johnny Rhe, Jong Hwan Ko  

**Link**: [PDF](https://arxiv.org/pdf/2502.07820)  

**Abstract**: In this study, we address the challenge of low-rank model compression in the context of in-memory computing (IMC) architectures. Traditional pruning approaches, while effective in model size reduction, necessitate additional peripheral circuitry to manage complex dataflows and mitigate dislocation issues, leading to increased area and energy overheads. To circumvent these drawbacks, we propose leveraging low-rank compression techniques, which, unlike pruning, streamline the dataflow and seamlessly integrate with IMC architectures. However, low-rank compression presents its own set of challenges, namely i) suboptimal IMC array utilization and ii) compromised accuracy. To address these issues, we introduce a novel approach i) employing shift and duplicate kernel (SDK) mapping technique, which exploits idle IMC columns for parallel processing, and ii) group low-rank convolution, which mitigates the information imbalance in the decomposed matrices. Our experimental results demonstrate that our proposed method achieves up to 2.5x speedup or +20.9% accuracy boost over existing pruning techniques. 

**Abstract (ZH)**: 本研究在内存计算架构（IMC）背景下解决低秩模型压缩的挑战。 

---
# Decoding Complexity: Intelligent Pattern Exploration with CHPDA (Context Aware Hybrid Pattern Detection Algorithm) 

**Title (ZH)**: 解码复杂性：基于CHPDA（情境aware混合模式检测算法）的智能模式探索 

**Authors**: Lokesh Koli, Shubham Kalra, Karanpreet Singh  

**Link**: [PDF](https://arxiv.org/pdf/2502.07815)  

**Abstract**: Detecting sensitive data such as Personally Identifiable Information (PII) and Protected Health Information (PHI) is critical for data security platforms. This study evaluates regex-based pattern matching algorithms and exact-match search techniques to optimize detection speed, accuracy, and scalability. Our benchmarking results indicate that Google RE2 provides the best balance of speed (10-15 ms/MB), memory efficiency (8-16 MB), and accuracy (99.5%) among regex engines, outperforming PCRE while maintaining broader hardware compatibility than Hyperscan. For exact matching, Aho-Corasick demonstrated superior performance (8 ms/MB) and scalability for large datasets. Performance analysis revealed that regex processing time scales linearly with dataset size and pattern complexity. A hybrid AI + Regex approach achieved the highest F1 score (91. 6%) by improving recall and minimizing false positives. Device benchmarking confirmed that our solution maintains efficient CPU and memory usage on both high-performance and mid-range systems. Despite its effectiveness, challenges remain, such as limited multilingual support and the need for regular pattern updates. Future work should focus on expanding language coverage, integrating data security and privacy management (DSPM) with data loss prevention (DLP) tools, and enhancing regulatory compliance for broader global adoption. 

**Abstract (ZH)**: 检测个人可识别信息（PII）和受保护的健康信息（PHI）对于数据安全平台至关重要。本研究评估了基于正则表达式的模式匹配算法和精确匹配搜索技术，以优化检测速度、准确性和可扩展性。基准测试结果显示，Google RE2在速度（每兆字节10-15毫秒）、内存效率（8-16兆字节）和准确性（99.5%）方面提供了最优平衡，优于PCRE并在硬件兼容性上优于Hyperscan。对于精确匹配，Aho-Corasick在大容量数据集上表现出更优的性能（每兆字节8毫秒）和可扩展性。性能分析表明，正则表达式处理时间随着数据集大小和模式复杂性的线性增长。通过结合人工智能与正则表达式的混合方法实现了最高的F1分数（91.6%），通过提高召回率和减少假阳性来实现。设备基准测试证实，我们的解决方案能够在高性能和中端系统上维持高效的CPU和内存使用。尽管取得了有效成果，仍存在限制多语言支持和定期更新模式等方面的挑战。未来的工作应侧重于扩展语言覆盖面，将数据安全和隐私管理（DSPM）与数据丢失防护（DLP）工具集成，并增强国际合规性以实现更广泛的全球应用。 

---
# Satellite Observations Guided Diffusion Model for Accurate Meteorological States at Arbitrary Resolution 

**Title (ZH)**: 基于卫星观测引导的扩散模型高精度任意分辨率气象状态估算 

**Authors**: Siwei Tu, Ben Fei, Weidong Yang, Fenghua Ling, Hao Chen, Zili Liu, Kun Chen, Hang Fan, Wanli Ouyang, Lei Bai  

**Link**: [PDF](https://arxiv.org/pdf/2502.07814)  

**Abstract**: Accurate acquisition of surface meteorological conditions at arbitrary locations holds significant importance for weather forecasting and climate simulation. Due to the fact that meteorological states derived from satellite observations are often provided in the form of low-resolution grid fields, the direct application of spatial interpolation to obtain meteorological states for specific locations often results in significant discrepancies when compared to actual observations. Existing downscaling methods for acquiring meteorological state information at higher resolutions commonly overlook the correlation with satellite observations. To bridge the gap, we propose Satellite-observations Guided Diffusion Model (SGD), a conditional diffusion model pre-trained on ERA5 reanalysis data with satellite observations (GridSat) as conditions, which is employed for sampling downscaled meteorological states through a zero-shot guided sampling strategy and patch-based methods. During the training process, we propose to fuse the information from GridSat satellite observations into ERA5 maps via the attention mechanism, enabling SGD to generate atmospheric states that align more accurately with actual conditions. In the sampling, we employed optimizable convolutional kernels to simulate the upscale process, thereby generating high-resolution ERA5 maps using low-resolution ERA5 maps as well as observations from weather stations as guidance. Moreover, our devised patch-based method promotes SGD to generate meteorological states at arbitrary resolutions. Experiments demonstrate SGD fulfills accurate meteorological states downscaling to 6.25km. 

**Abstract (ZH)**: 基于卫星观测指导的扩散模型：从ERA5再分析数据中获取高分辨率气象状态 

---
# CryptoX : Compositional Reasoning Evaluation of Large Language Models 

**Title (ZH)**: CryptoX : 大型语言模型组合 reasoning 评估 

**Authors**: Jiajun Shi, Chaoren Wei, Liqun Yang, Zekun Moore Wang, Chenghao Yang, Ge Zhang, Stephen Huang, Tao Peng, Jian Yang, Zhoufutu Wen  

**Link**: [PDF](https://arxiv.org/pdf/2502.07813)  

**Abstract**: The compositional reasoning capacity has long been regarded as critical to the generalization and intelligence emergence of large language models LLMs. However, despite numerous reasoning-related benchmarks, the compositional reasoning capacity of LLMs is rarely studied or quantified in the existing benchmarks. In this paper, we introduce CryptoX, an evaluation framework that, for the first time, combines existing benchmarks and cryptographic, to quantify the compositional reasoning capacity of LLMs. Building upon CryptoX, we construct CryptoBench, which integrates these principles into several benchmarks for systematic evaluation. We conduct detailed experiments on widely used open-source and closed-source LLMs using CryptoBench, revealing a huge gap between open-source and closed-source LLMs. We further conduct thorough mechanical interpretability experiments to reveal the inner mechanism of LLMs' compositional reasoning, involving subproblem decomposition, subproblem inference, and summarizing subproblem conclusions. Through analysis based on CryptoBench, we highlight the value of independently studying compositional reasoning and emphasize the need to enhance the compositional reasoning capabilities of LLMs. 

**Abstract (ZH)**: CryptoX：一种结合现有基准和加密技术量化大型语言模型组合推理能力的评估框架 

---
# CP-Guard+: A New Paradigm for Malicious Agent Detection and Defense in Collaborative Perception 

**Title (ZH)**: CP-Guard+: 一种协作感知中恶意代理检测与防御的新范式 

**Authors**: Senkang Hu, Yihang Tao, Zihan Fang, Guowen Xu, Yiqin Deng, Sam Kwong, Yuguang Fang  

**Link**: [PDF](https://arxiv.org/pdf/2502.07807)  

**Abstract**: Collaborative perception (CP) is a promising method for safe connected and autonomous driving, which enables multiple vehicles to share sensing information to enhance perception performance. However, compared with single-vehicle perception, the openness of a CP system makes it more vulnerable to malicious attacks that can inject malicious information to mislead the perception of an ego vehicle, resulting in severe risks for safe driving. To mitigate such vulnerability, we first propose a new paradigm for malicious agent detection that effectively identifies malicious agents at the feature level without requiring verification of final perception results, significantly reducing computational overhead. Building on this paradigm, we introduce CP-GuardBench, the first comprehensive dataset provided to train and evaluate various malicious agent detection methods for CP systems. Furthermore, we develop a robust defense method called CP-Guard+, which enhances the margin between the representations of benign and malicious features through a carefully designed Dual-Centered Contrastive Loss (DCCLoss). Finally, we conduct extensive experiments on both CP-GuardBench and V2X-Sim, and demonstrate the superiority of CP-Guard+. 

**Abstract (ZH)**: 协作感知(CP)是一种有潜力的安全自动驾驶方法，能够使多辆车辆共享感知信息以提升感知性能。然而，与单 vehicle 感知相比，CP 系统的开放性使其更容易受到恶意攻击，这些攻击可以注入恶意信息误导自主车辆的感知，从而带来严重的安全风险。为了减轻这种脆弱性，我们首先提出了一种新的恶意代理检测范式，能够在特征层面有效地识别恶意代理，无需验证最终的感知结果，从而显著减少计算开销。在此基础上，我们引入了 CP-GuardBench，这是第一个用于训练和评估各种恶意代理检测方法的全面数据集。此外，我们开发了一种 robust 的防御方法 CP-Guard+，通过精心设计的双中心对比损失（DCCLoss）增强良性特征和恶意特征的表示之间的差距。最后，我们在 CP-GuardBench 和 V2X-Sim 上进行了广泛的实验，并展示了 CP-Guard+ 的优越性。 

---
# Quantum Powered Credit Risk Assessment: A Novel Approach using hybrid Quantum-Classical Deep Neural Network for Row-Type Dependent Predictive Analysis 

**Title (ZH)**: 量子驱动的信用风险评估：一种基于混合量子-经典深度神经网络的行依赖预测分析新方法 

**Authors**: Rath Minati, Date Hema  

**Link**: [PDF](https://arxiv.org/pdf/2502.07806)  

**Abstract**: The integration of Quantum Deep Learning (QDL) techniques into the landscape of financial risk analysis presents a promising avenue for innovation. This study introduces a framework for credit risk assessment in the banking sector, combining quantum deep learning techniques with adaptive modeling for Row-Type Dependent Predictive Analysis (RTDPA). By leveraging RTDPA, the proposed approach tailors predictive models to different loan categories, aiming to enhance the accuracy and efficiency of credit risk evaluation. While this work explores the potential of integrating quantum methods with classical deep learning for risk assessment, it focuses on the feasibility and performance of this hybrid framework rather than claiming transformative industry-wide impacts. The findings offer insights into how quantum techniques can complement traditional financial analysis, paving the way for further advancements in predictive modeling for credit risk. 

**Abstract (ZH)**: 量子深度学习技术在金融风险分析领域的整合为创新提供了前景。本文介绍了一种结合量子深度学习技术和自适应建模的信用风险评估框架，以行依赖预测分析（RTDPA）为基础。通过利用RTDPA，所提出的方法针对不同的贷款类别定制预测模型，旨在提高信用风险评估的准确性和效率。虽然本研究探索了将量子方法与经典深度学习结合进行风险评估的可能性，但它主要关注这种混合框架的可行性和性能，而非宣称具有行业颠覆性的影响。研究结果为量子技术如何补充传统金融分析提供了洞见，并为信用风险预测建模的进一步进展铺平了道路。 

---
# Regulatory Science Innovation for Generative AI and Large Language Models in Health and Medicine: A Global Call for Action 

**Title (ZH)**: 生成人工智能和大型语言模型在健康医疗领域的监管科学创新：全球行动倡议 

**Authors**: Jasmine Chiat Ling Ong, Yilin Ning, Mingxuan Liu, Yian Ma, Zhao Liang, Kuldev Singh, Robert T Chang, Silke Vogel, John CW Lim, Iris Siu Kwan Tan, Oscar Freyer, Stephen Gilbert, Danielle S Bitterman, Xiaoxuan Liu, Alastair K Denniston, Nan Liu  

**Link**: [PDF](https://arxiv.org/pdf/2502.07794)  

**Abstract**: The integration of generative AI (GenAI) and large language models (LLMs) in healthcare presents both unprecedented opportunities and challenges, necessitating innovative regulatory approaches. GenAI and LLMs offer broad applications, from automating clinical workflows to personalizing diagnostics. However, the non-deterministic outputs, broad functionalities and complex integration of GenAI and LLMs challenge existing medical device regulatory frameworks, including the total product life cycle (TPLC) approach. Here we discuss the constraints of the TPLC approach to GenAI and LLM-based medical device regulation, and advocate for global collaboration in regulatory science research. This serves as the foundation for developing innovative approaches including adaptive policies and regulatory sandboxes, to test and refine governance in real-world settings. International harmonization, as seen with the International Medical Device Regulators Forum, is essential to manage implications of LLM on global health, including risks of widening health inequities driven by inherent model biases. By engaging multidisciplinary expertise, prioritizing iterative, data-driven approaches, and focusing on the needs of diverse populations, global regulatory science research enables the responsible and equitable advancement of LLM innovations in healthcare. 

**Abstract (ZH)**: 生成式人工智能与大规模语言模型在 Healthcare 领域的融合带来了前所未有的机遇与挑战，需要创新的监管方法。 

---
# Can Generative AI be Egalitarian? 

**Title (ZH)**: 生成式AI能实现平等吗？ 

**Authors**: Philip Feldman, James R. Foulds, Shimei Pan  

**Link**: [PDF](https://arxiv.org/pdf/2502.07790)  

**Abstract**: The recent explosion of "foundation" generative AI models has been built upon the extensive extraction of value from online sources, often without corresponding reciprocation. This pattern mirrors and intensifies the extractive practices of surveillance capitalism, while the potential for enormous profit has challenged technology organizations' commitments to responsible AI practices, raising significant ethical and societal concerns. However, a promising alternative is emerging: the development of models that rely on content willingly and collaboratively provided by users. This article explores this "egalitarian" approach to generative AI, taking inspiration from the successful model of Wikipedia. We explore the potential implications of this approach for the design, development, and constraints of future foundation models. We argue that such an approach is not only ethically sound but may also lead to models that are more responsive to user needs, more diverse in their training data, and ultimately more aligned with societal values. Furthermore, we explore potential challenges and limitations of this approach, including issues of scalability, quality control, and potential biases inherent in volunteer-contributed content. 

**Abstract (ZH)**: 近期“基础”生成型AI模型的爆炸式增长建立在对在线源的广泛价值提取之上，通常缺乏相应的回馈。这种模式反映了并加剧了监控资本主义的剥削实践，同时巨大的利润潜力挑战了技术组织负责任的AI实践承诺，引发了重大的伦理和社会关切。然而，一种有希望的替代方案正在出现：依赖用户自愿和合作提供的内容的模型开发。本文探讨这种“平等主义”生成型AI的方法，以维基百科的成功模式为 inspiration。我们探讨了这种方法对未来基础模型的设计、开发及其约束条件的潜在影响。我们认为，这种做法不仅是道德上合理的，还可能导致更响应用户需求、训练数据更具多样性的模型，并最终与社会价值更加一致。此外，我们还探讨了这种方法可能面临的挑战和限制，包括可扩展性、质量控制以及志愿者贡献内容中固有的偏见问题。 

---
# Do AI assistants help students write formal specifications? A study with ChatGPT and the B-Method 

**Title (ZH)**: AI助手中小学生编写正式规范的帮助：基于ChatGPT和B-方法的研究 

**Authors**: Alfredo Capozucca, Daniil Yampolskyi, Alexander Goldberg, Maximiliano Cristiá  

**Link**: [PDF](https://arxiv.org/pdf/2502.07789)  

**Abstract**: This paper investigates the role of AI assistants, specifically OpenAI's ChatGPT, in teaching formal methods (FM) to undergraduate students, using the B-method as a formal specification technique. While existing studies demonstrate the effectiveness of AI in coding tasks, no study reports on its impact on formal specifications. We examine whether ChatGPT provides an advantage when writing B-specifications and analyse student trust in its outputs. Our findings indicate that the AI does not help students to enhance the correctness of their specifications, with low trust correlating to better outcomes. Additionally, we identify a behavioural pattern with which to interact with ChatGPT which may influence the correctness of B-specifications. 

**Abstract (ZH)**: 本研究 investigate 了 AI 辅助工具，特别是 OpenAI 的 ChatGPT，在教学正式方法（FM）给本科生时的作用，使用 B 方法作为形式化规范技术。尽管现有研究证明了 AI 在编程任务中的有效性，但尚未有研究报道其对形式化规范的影响。我们探讨了 ChatGPT 在编写 B 规范时是否提供优势，并分析了学生对其输出的信任度。研究结果表明，AI 并未帮助学生提高规范的正确性，低信任度与更好的结果相关。此外，我们还识别出一种与 ChatGPT 交互的行为模式，这可能影响 B 规范的正确性。 

---
# Counterexample Guided Program Repair Using Zero-Shot Learning and MaxSAT-based Fault Localization 

**Title (ZH)**: 使用零_shot学习和MaxSAT为基础的故障定位的反例引导程序修复 

**Authors**: Pedro Orvalho, Mikoláš Janota, Vasco Manquinho  

**Link**: [PDF](https://arxiv.org/pdf/2502.07786)  

**Abstract**: Automated Program Repair (APR) for introductory programming assignments (IPAs) is motivated by the large number of student enrollments in programming courses each year. Since providing feedback on IPAs requires substantial time and effort from faculty, personalized feedback often involves suggesting fixes to students' programs. Formal Methods (FM)-based semantic repair approaches, check a program's execution against a test suite or reference solution, are effective but limited. These tools excel at identifying buggy parts but can only fix programs if the correct implementation and the faulty one share the same control flow graph. Conversely, Large Language Models (LLMs) are used for APR but often make extensive instead of minimal rewrites. This leads to more invasive fixes, making it harder for students to learn from their mistakes. In summary, LLMs excel at completing strings, while FM-based fault localization excel at identifying buggy parts of a program. In this paper, we propose a novel approach that combines the strengths of both FM-based fault localization and LLMs, via zero-shot learning, to enhance APR for IPAs. Our method uses MaxSAT-based fault localization to identify buggy parts of a program, then presents the LLM with a program sketch devoid of these buggy statements. This hybrid approach follows a CEGIS loop to iteratively refine the program. We ask the LLM to synthesize the missing parts, which are then checked against a test suite. If the suggested program is incorrect, a counterexample from the test suite is fed back to the LLM. Our experiments show that our counterexample guided approach, using MaxSAT-based bug-free program sketches, significantly improves the repair capabilities of all six evaluated LLMs. This method allows LLMs to repair more programs with smaller fixes, outperforming other configurations and state-of-the-art symbolic program repair tools. 

**Abstract (ZH)**: 基于自动程序修复的元编程作业初步编程作业的自动化程序修复（APR）：面向大量报名编程课程的学生推动了这一领域的研究。由于为初步编程作业提供反馈需要教员投入大量时间和精力，个性化反馈常常涉及建议学生修改程序。基于形式方法（FM）的语义修复方法通过检查程序执行与测试套件或参考解决方案一致来实现这一目的，这些方法非常有效但也有局限性。它们擅长识别有故障的部分，但只能在正确实现和有故障的实现共享相同控制流图时修复程序。相反，大规模语言模型（LLMs）被用于程序修复，但往往进行大规模修改而非最小化修改。这导致了更侵入性的修复，使得学生更难从中学习错误。总体而言，LLMs 在完成字符串方面表现出色，而基于FM的故障本地化则在识别程序中的有故障部分方面表现出色。本文提出了一种结合基于FM的故障本地化和LLMs优势的新方法，通过零样本学习来增强初步编程作业的程序修复能力。该方法使用基于MaxSAT的故障本地化来识别程序中的有故障部分，然后向LLM提供一个没有这些有故障语句的程序草图。该混合方法遵循一种CEGIS循环，逐步优化程序。我们要求LLM合成效缺乏的部分，然后这些部分与测试套件进行验证。如果建议的程序不正确，则从测试套件中返回反例供LLM反馈。我们的实验结果表明，使用基于MaxSAT的无故障程序草图和反例指导的方法，显著提高了所有六种评估的LLMs的修复能力。该方法使LLMs能够用更小的修改修复更多的程序，优于其他配置和其他最先进的符号程序修复工具。 

---
# Machine Learning and Quantum Intelligence for Health Data Scenarios 

**Title (ZH)**: 机器学习与量子智能在健康数据场景中的应用 

**Authors**: Sanjeev Naguleswaran  

**Link**: [PDF](https://arxiv.org/pdf/2410.21339)  

**Abstract**: The advent of quantum computing has opened new possibilities in data science, offering unique capabilities for addressing complex, data-intensive problems. Traditional machine learning algorithms often face challenges in high-dimensional or limited-quality datasets, which are common in healthcare. Quantum Machine Learning leverages quantum properties, such as superposition and entanglement, to enhance pattern recognition and classification, potentially surpassing classical approaches. This paper explores QML's application in healthcare, focusing on quantum kernel methods and hybrid quantum-classical networks for heart disease prediction and COVID-19 detection, assessing their feasibility and performance. 

**Abstract (ZH)**: 量子计算的兴起为数据科学开辟了新的可能性，提供了解决复杂、数据密集型问题的独特能力。传统机器学习算法在高维或质量有限的数据集面前常常面临挑战，这在医疗保健领域尤为常见。量子机器学习利用量子特性，如超position和缠绵，增强模式识别和分类能力，有可能超越经典方法。本文探讨了量子机器学习在医疗保健领域的应用，重点是量子核方法和混合量子经典网络在心脏病预测和COVID-19检测中的应用，评估其可行性和性能。 

---
