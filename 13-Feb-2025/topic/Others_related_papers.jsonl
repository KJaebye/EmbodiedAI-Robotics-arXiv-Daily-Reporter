{'arxiv_id': 'arXiv:2502.08158', 'title': 'Open-Source Factor Graph Optimization Package for GNSS: Examples and Applications', 'authors': 'Taro Suzuki', 'link': 'https://arxiv.org/abs/2502.08158', 'abstract': 'State estimation methods using factor graph optimization (FGO) have garnered significant attention in global navigation satellite system (GNSS) research. FGO exhibits superior estimation accuracy compared with traditional state estimation methods that rely on least-squares or Kalman filters. However, only a few FGO libraries are specialized for GNSS observations. This paper introduces an open-source GNSS FGO package named gtsam\\_gnss, which has a simple structure and can be easily applied to GNSS research and development. This package separates the preprocessing of GNSS observations from factor optimization. Moreover, it describes the error function of the GNSS factor in a straightforward manner, allowing for general-purpose inputs. This design facilitates the transition from ordinary least-squares-based positioning to FGO and supports user-specific GNSS research. In addition, gtsam\\_gnss includes analytical examples involving various factors using GNSS data in real urban environments. This paper presents three application examples: the use of a robust error model, estimation of integer ambiguity in the carrier phase, and combination of GNSS and inertial measurements from smartphones. The proposed framework demonstrates excellent state estimation performance across all use cases.', 'abstract_zh': '使用因子图优化（FGO）的方法在全球导航卫星系统（GNSS）研究中的状态估计方法受到了广泛关注。FGO相较于依赖最小二乘法或卡尔曼滤波的传统状态估计方法展现出更好的估计精度。然而，专门针对GNSS观测的FGO库并不多。本文介绍了一个开源的GNSS FGO包gtsam\\_gnss，该包结构简单，易于应用于GNSS研究和开发。该包将GNSS观测的预处理与因子优化分离，并以简洁的方式描述GNSS因子的误差函数，支持通用输入。这种设计便于从基于普通最小二乘的位置估计过渡到FGO，并支持用户特定的GNSS研究。此外，gtsam\\_gnss使用实际城市环境中的GNSS数据提供了各种因子的分析示例。本文提出了三个应用示例：使用稳健的误差模型、载波相位整数模糊度估计以及智能手机GNSS和惯性测量的结合。提出的框架在所有应用案例中都表现出优秀的状态估计性能。', 'title_zh': 'GNSS开源因子图优化软件包：示例与应用'}
{'arxiv_id': 'arXiv:2502.08089', 'title': 'A Cooperative Bearing-Rate Approach for Observability-Enhanced Target Motion Estimation', 'authors': 'Canlun Zheng, Hanqing Guo, Shiyu Zhao', 'link': 'https://arxiv.org/abs/2502.08089', 'abstract': 'Vision-based target motion estimation is a fundamental problem in many robotic tasks. The existing methods have the limitation of low observability and, hence, face challenges in tracking highly maneuverable targets. Motivated by the aerial target pursuit task where a target may maneuver in 3D space, this paper studies how to further enhance observability by incorporating the \\emph{bearing rate} information that has not been well explored in the literature. The main contribution of this paper is to propose a new cooperative estimator called STT-R (Spatial-Temporal Triangulation with bearing Rate), which is designed under the framework of distributed recursive least squares. This theoretical result is further verified by numerical simulation and real-world experiments. It is shown that the proposed STT-R algorithm can effectively generate more accurate estimations and effectively reduce the lag in velocity estimation, enabling tracking of more maneuverable targets.', 'abstract_zh': '基于视觉的目标运动估计是许多机器人任务中的一个基本问题。现有的方法存在观测性较低的局限性，因此在跟踪高度机动的目标时面临挑战。受空中目标捕获任务的启发，其中目标可能在三维空间中机动，本文研究如何通过结合文献中尚未充分探索的航向角速率信息来进一步提高观测性。本文的主要贡献是提出了一种新的协同估计器STT-R（空间-时间 triangulation with 航向角速率），该估计器是在分布式递推最小二乘框架下设计的。该理论结果通过数值模拟和实际实验进一步得到验证。研究结果表明，所提出的STT-R算法能够有效生成更准确的估计，并有效减少速度估计滞后，从而实现对更机动目标的跟踪。', 'title_zh': '一种基于协作航向率的方法以增强目标运动估计可观察性'}
{'arxiv_id': 'arXiv:2502.07922', 'title': 'Visual-Haptic Model Mediated Teleoperation for Remote Ultrasound', 'authors': 'David Black, Maria Tirindelli, Septimiu Salcudean, Wolfgang Wein, Marco Esposito', 'link': 'https://arxiv.org/abs/2502.07922', 'abstract': 'Tele-ultrasound has the potential greatly to improve health equity for countless remote communities. However, practical scenarios involve potentially large time delays which cause current implementations of telerobotic ultrasound (US) to fail. Using a local model of the remote environment to provide haptics to the expert operator can decrease teleoperation instability, but the delayed visual feedback remains problematic. This paper introduces a robotic tele-US system in which the local model is not only haptic, but also visual, by re-slicing and rendering a pre-acquired US sweep in real time to provide the operator a preview of what the delayed image will resemble. A prototype system is presented and tested with 15 volunteer operators. It is found that visual-haptic model-mediated teleoperation (MMT) compensates completely for time delays up to 1000 ms round trip in terms of operator effort and completion time while conventional MMT does not. Visual-haptic MMT also significantly outperforms MMT for longer time delays in terms of motion accuracy and force control. This proof-of-concept study suggests that visual-haptic MMT may facilitate remote robotic tele-US.', 'abstract_zh': '基于视觉-触觉模型中介的远程超声系统在远程医疗中的应用潜力与挑战：时间延迟补偿研究', 'title_zh': '视觉-触觉模型介导的远程超声操作'}
{'arxiv_id': 'arXiv:2502.08428', 'title': 'Robot-Initiated Social Control of Sedentary Behavior: Comparing the Impact of Relationship- and Target-Focused Strategies', 'authors': 'Jiaxin Xu, Sterre Anna Mariam van der Horst, Chao Zhang, Raymond H. Cuijpers, Wijnand A. IJsselsteijn', 'link': 'https://arxiv.org/abs/2502.08428', 'abstract': "To design social robots to effectively promote health behavior change, it is essential to understand how people respond to various health communication strategies employed by these robots. This study examines the effectiveness of two types of social control strategies from a social robot, relationship-focused strategies (emphasizing relational consequences) and target-focused strategies (emphasizing health consequences), in encouraging people to reduce sedentary behavior. A two-session lab experiment was conducted (n = 135), where participants first played a game with a robot, followed by the robot persuading them to stand up and move using one of the strategies. Half of the participants joined a second session to have a repeated interaction with the robot. Results showed that relationship-focused strategies motivated participants to stay active longer. Repeated sessions did not strengthen participants' relationship with the robot, but those who felt more attached to the robot responded more actively to the target-focused strategies. These findings offer valuable insights for designing persuasive strategies for social robots in health communication contexts.", 'abstract_zh': '设计社交机器人以有效促进健康行为改变需理解人们对其各种健康沟通策略的反应。本研究探讨了社交机器人采用的两种类型的社会控制策略——关系聚焦策略（强调关系后果）和目标聚焦策略（强调健康后果）——在鼓励人们减少久坐行为方面的有效性。本研究进行了两阶段实验室实验（n=135），参与者首先与机器人玩游戏，之后通过机器人使用其中一种策略劝说他们站立并运动。一半参与者参加了第二次会话以再次与机器人互动。结果显示，关系聚焦策略促使参与者更长时间保持活跃。重复会话并未增强参与者与机器人的关系，但对机器人感觉更依恋的参与者对目标聚焦策略的反应更为积极。这些发现为健康通信背景下设计说服策略提供了宝贵见解。', 'title_zh': '机器人主动发起的社会控制以减少久坐行为：关系导向与目标导向策略的对比影响'}
{'arxiv_id': 'arXiv:2502.08547', 'title': 'Representation Learning to Advance Multi-institutional Studies with Electronic Health Record Data', 'authors': 'Doudou Zhou, Han Tong, Linshanshan Wang, Suqi Liu, Xin Xiong, Ziming Gan, Romain Griffier, Boris Hejblum, Yun-Chung Liu, Chuan Hong, Clara-Lea Bonzel, Tianrun Cai, Kevin Pan, Yuk-Lam Ho, Lauren Costa, Vidul A. Panickan, J. Michael Gaziano, Kenneth Mandl, Vianney Jouhet, Rodolphe Thiebaut, Zongqi Xia, Kelly Cho, Katherine Liao, Tianxi Cai', 'link': 'https://arxiv.org/abs/2502.08547', 'abstract': "The adoption of EHRs has expanded opportunities to leverage data-driven algorithms in clinical care and research. A major bottleneck in effectively conducting multi-institutional EHR studies is the data heterogeneity across systems with numerous codes that either do not exist or represent different clinical concepts across institutions. The need for data privacy further limits the feasibility of including multi-institutional patient-level data required to study similarities and differences across patient subgroups. To address these challenges, we developed the GAME algorithm. Tested and validated across 7 institutions and 2 languages, GAME integrates data in several levels: (1) at the institutional level with knowledge graphs to establish relationships between codes and existing knowledge sources, providing the medical context for standard codes and their relationship to each other; (2) between institutions, leveraging language models to determine the relationships between institution-specific codes with established standard codes; and (3) quantifying the strength of the relationships between codes using a graph attention network. Jointly trained embeddings are created using transfer and federated learning to preserve data privacy. In this study, we demonstrate the applicability of GAME in selecting relevant features as inputs for AI-driven algorithms in a range of conditions, e.g., heart failure, rheumatoid arthritis. We then highlight the application of GAME harmonized multi-institutional EHR data in a study of Alzheimer's disease outcomes and suicide risk among patients with mental health disorders, without sharing patient-level data outside individual institutions.", 'abstract_zh': 'EHRs采用扩大了基于数据驱动算法在临床护理和研究中的应用机会。机构间数据异质性是有效地开展多机构EHR研究的主要瓶颈，不同机构中存在大量不一致或代表不同临床概念的编码。数据隐私需求进一步限制了研究需要的多机构患者级数据的可行性。为应对这些挑战，我们开发了GAME算法。该算法在7个机构和2种语言下进行了测试和验证，通过在多个层次上整合数据来解决这些问题：（1）在机构层面使用知识图谱建立编码与现有知识源之间的关系，为标准编码及其相互关系提供医学背景；（2）在机构间利用语言模型确定机构特定编码与现有标准编码之间的关系；（3）使用图注意力网络量化编码之间关系的强度。通过转移学习和联邦学习联合训练嵌入向量，以保护数据隐私。在本研究中，我们展示了GAME在选择用于AI驱动算法输入的相关特征方面的适用性，例如心力衰竭、类风湿性关节炎等病情。我们还强调了在不共享患者级数据的情况下，利用GAME统一的多机构EHR数据研究阿尔茨海默病结果和精神障碍患者自杀风险的应用。', 'title_zh': '基于表示学习促进电子健康记录数据多机构研究'}
{'arxiv_id': 'arXiv:2502.08148', 'title': 'ACCESS : A Benchmark for Abstract Causal Event Discovery and Reasoning', 'authors': 'Vy Vo, Lizhen Qu, Tao Feng, Yuncheng Hua, Xiaoxi Kang, Songhai Fan, Tim Dwyer, Lay-Ki Soon, Gholamreza Haffari', 'link': 'https://arxiv.org/abs/2502.08148', 'abstract': 'Identifying cause-and-effect relationships is critical to understanding real-world dynamics and ultimately causal reasoning. Existing methods for identifying event causality in NLP, including those based on Large Language Models (LLMs), exhibit difficulties in out-of-distribution settings due to the limited scale and heavy reliance on lexical cues within available benchmarks. Modern benchmarks, inspired by probabilistic causal inference, have attempted to construct causal graphs of events as a robust representation of causal knowledge, where \\texttt{CRAB} \\citep{romanou2023crab} is one such recent benchmark along this line. In this paper, we introduce \\texttt{ACCESS}, a benchmark designed for discovery and reasoning over abstract causal events. Unlike existing resources, \\texttt{ACCESS} focuses on causality of everyday life events on the abstraction level. We propose a pipeline for identifying abstractions for event generalizations from \\texttt{GLUCOSE} \\citep{mostafazadeh-etal-2020-glucose}, a large-scale dataset of implicit commonsense causal knowledge, from which we subsequently extract $1,4$K causal pairs. Our experiments highlight the ongoing challenges of using statistical methods and/or LLMs for automatic abstraction identification and causal discovery in NLP. Nonetheless, we demonstrate that the abstract causal knowledge provided in \\texttt{ACCESS} can be leveraged for enhancing QA reasoning performance in LLMs.', 'abstract_zh': '识别因果关系对于理解现实世界的动态并最终进行因果推理至关重要。现有的自然语言处理中事件因果性识别方法，包括基于大规模语言模型的方法，在分布外设置中由于规模有限且过度依赖可用基准中的词形线索，表现出一定的困难。受概率因果推理启发的现代基准试图构建事件因果图，作为因果知识的稳健表示，其中\\texttt{CRAB} \\citep{romanou2023crab} 是此类基准的最新例证之一。在本文中，我们介绍了\\texttt{ACCESS}，一个用于抽象因果事件发现和推理的基准。与现有资源不同，\\texttt{ACCESS} 关注日常生活事件的抽象层面的因果性。我们提出了一个从\\texttt{GLUCOSE} \\citep{mostafazadeh-etal-2020-glucose} 中大规模隐含常识因果知识数据集中识别事件泛化抽象的pipeline，并从中提取了1,400个因果配对。我们的实验突显了统计方法和/或大规模语言模型在自然语言处理中自动识别抽象和因果发现方面持续存在的挑战。尽管如此，我们证明了\\texttt{ACCESS} 中提供的抽象因果知识可以用于增强大规模语言模型的QA推理性能。', 'title_zh': 'ACCESS：一种抽象因果事件发现与推理基准'}
{'arxiv_id': 'arXiv:2502.08047', 'title': 'WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation', 'authors': 'Henry Hengyuan Zhao, Difei Gao, Mike Zheng Shou', 'link': 'https://arxiv.org/abs/2502.08047', 'abstract': 'Current GUI agents have achieved outstanding performance in GUI element grounding. However, planning remains highly challenging, especially due to sensitivity to the initial state of the environment. Specifically, slight differences in the initial state-such as the target software not being open or the interface not being in its default state-often lead to planning errors. This issue is widespread in real user scenarios, but existing benchmarks fail to evaluate it. In this paper, we present WorldGUI, a novel GUI benchmark that designs GUI tasks with various initial states to simulate real computer-user interactions. The benchmark spans a wide range of tasks across 10 popular software applications, including PowerPoint, VSCode, and Adobe Acrobat. In addition, to address the challenges of dynamic GUI automation tasks, we propose GUI-Thinker, a holistic framework, leveraging a critique mechanism, that effectively manages the unpredictability and complexity of GUI interactions. Experimental results demonstrate that GUI-Thinker significantly outperforms Claude-3.5 (Computer Use) by 14.9% in success rate on WorldGUI tasks. This improvement underscores the effectiveness of our critical-thinking-based framework in enhancing GUI automation.', 'abstract_zh': '当前的GUI代理已经在GUI元素定位方面取得了卓越的性能，然而，规划仍然极具挑战性，尤其是在环境初始状态的敏感性方面。具体来说，初始状态的微小差异，如目标软件未打开或界面未处于默认状态，往往会导致规划错误。这一问题在真实用户场景中普遍存在，但现有的基准测试无法评估该问题。在这项工作中，我们提出了WorldGUI，这是一种新颖的GUI基准测试，通过设计具有各种初始状态的GUI任务来模拟真实计算机用户交互。该基准测试覆盖了10个流行软件应用程序的任务范围，包括PowerPoint、VSCode和Adobe Acrobat。此外，为了应对动态GUI自动化任务的挑战，我们提出了一种综合框架GUI-Thinker，利用批判机制来有效管理GUI交互的不可预测性和复杂性。实验结果表明，GUI-Thinker在WorldGUI任务的成功率上比Claude-3.5 (Computer Use) 高出14.9%。这一改进证明了我们基于批判思维的框架在增强GUI自动化方面的有效性。', 'title_zh': 'WorldGUI: 动态测试 convo 完整的桌面GUI自动化'}
{'arxiv_id': 'arXiv:2502.08011', 'title': 'Training-Free Safe Denoisers for Safe Use of Diffusion Models', 'authors': 'Mingyu Kim, Dongjun Kim, Amman Yusuf, Stefano Ermon, Mi Jung Park', 'link': 'https://arxiv.org/abs/2502.08011', 'abstract': 'There is growing concern over the safety of powerful diffusion models (DMs), as they are often misused to produce inappropriate, not-safe-for-work (NSFW) content or generate copyrighted material or data of individuals who wish to be forgotten. Many existing methods tackle these issues by heavily relying on text-based negative prompts or extensively retraining DMs to eliminate certain features or samples. In this paper, we take a radically different approach, directly modifying the sampling trajectory by leveraging a negation set (e.g., unsafe images, copyrighted data, or datapoints needed to be excluded) to avoid specific regions of data distribution, without needing to retrain or fine-tune DMs. We formally derive the relationship between the expected denoised samples that are safe and those that are not safe, leading to our $\\textit{safe}$ denoiser which ensures its final samples are away from the area to be negated. Inspired by the derivation, we develop a practical algorithm that successfully produces high-quality samples while avoiding negation areas of the data distribution in text-conditional, class-conditional, and unconditional image generation scenarios. These results hint at the great potential of our training-free safe denoiser for using DMs more safely.', 'abstract_zh': '强大的扩散模型安全性担忧及其应对方法：无需重新训练的直接去噪方法', 'title_zh': '无需训练的安全去噪器以安全使用扩散模型'}
{'arxiv_id': 'arXiv:2502.07944', 'title': 'SHACL-SKOS Based Knowledge Representation of Material Safety Data Sheet (SDS) for the Pharmaceutical Industry', 'authors': 'Brian Lu, Dennis Pham, Ti-Chiun Chang, Michael Lovette, Terri Bui, Stephen Ma', 'link': 'https://arxiv.org/abs/2502.07944', 'abstract': 'We report the development of a knowledge representation and reasoning (KRR) system built on hybrid SHACL-SKOS ontologies for globally harmonized system (GHS) material Safety Data Sheets (SDS) to enhance chemical safety communication and regulatory compliance. SDS are comprehensive documents containing safety and handling information for chemical substances. Thus, they are an essential part of workplace safety and risk management. However, the vast number of Safety Data Sheets from multiple organizations, manufacturers, and suppliers that produce and distribute chemicals makes it challenging to centralize and access SDS documents through a single repository. To accomplish the underlying issues of data exchange related to chemical shipping and handling, we construct SDS related controlled vocabulary and conditions validated by SHACL, and knowledge systems of similar domains linked via SKOS. The resulting hybrid ontologies aim to provide standardized yet adaptable representations of SDS information, facilitating better data sharing, retrieval, and integration across various platforms. This paper outlines our SHACL-SKOS system architectural design and showcases our implementation for an industrial application streamlining the generation of a composite shipping cover sheet.', 'abstract_zh': '我们报道了一种基于混合SHACL-SKOS本体的知识表示与推理系统的发展，该系统用于全球化学品统一分类和标签制度（GHS）物质安全数据表（SDS），以增强化学安全通信和监管合规性。', 'title_zh': '基于SHACL-SKOS的制药行业物质安全数据表（MSDS）知识表示'}
{'arxiv_id': 'arXiv:2502.07850', 'title': 'Mathematical reasoning and the computer', 'authors': 'Kevin Buzzard', 'link': 'https://arxiv.org/abs/2502.07850', 'abstract': 'Computers have already changed the way that humans do mathematics: they enable us to compute efficiently. But will they soon be helping us to reason? And will they one day start reasoning themselves? We give an overview of recent developments in neural networks, computer theorem provers and large language models.', 'abstract_zh': '计算机已经改变了人类进行数学的方式：它们使我们能够高效计算。但它们很快会帮助我们推理吗？最终会自己进行推理吗？我们概述了近期神经网络、计算机定理证明器和大规模语言模型的发展。', 'title_zh': '数学推理与计算机'}
{'arxiv_id': 'arXiv:2502.07819', 'title': 'Enhancing kidney transplantation through multi-agent kidney exchange programs: A comprehensive review and optimization models', 'authors': 'Shayan Sharifi', 'link': 'https://arxiv.org/abs/2502.07819', 'abstract': 'This paper presents a comprehensive review of the last two decades of research on Kidney Exchange Programs (KEPs), systematically categorizing and classifying key contributions to provide readers with a structured understanding of advancements in the field. The review highlights the evolution of KEP methodologies and lays the foundation for our contribution. We propose three mathematical models aimed at improving both the quantity and quality of kidney transplants. Model 1 maximizes the number of transplants by focusing on compatibility based on blood type and PRA, without additional constraints. Model 2 introduces a minimum Human Leukocyte Antigen (HLA) compatibility threshold to enhance transplant quality, though this leads to fewer matches. Model 3 extends the problem to a Multi-Agent Kidney Exchange Program (MKEP), pooling incompatible donor-recipient pairs across multiple agents, resulting in a higher number of successful transplants while ensuring fairness across agents. Sensitivity analyses demonstrate trade-offs between transplant quantity and quality, with Model 3 striking the optimal balance by leveraging multi-agent collaboration to improve both the number and quality of transplants. These findings underscore the potential benefits of more integrated kidney exchange systems.', 'abstract_zh': '过去二十年肾交换计划研究的全面综述：提出三种数学模型以提高肾脏移植的数量和质量', 'title_zh': '通过多代理肾脏交换计划增强肾脏移植：全面回顾与优化模型'}
{'arxiv_id': 'arXiv:2502.07817', 'title': 'Temporal Model On Quantum Logic', 'authors': "Francesco D'Agostino", 'link': 'https://arxiv.org/abs/2502.07817', 'abstract': 'This paper introduces a unified theoretical framework for modeling temporal memory dynamics, combining concepts from temporal logic, memory decay models, and hierarchical contexts. The framework formalizes the evolution of propositions over time using linear and branching temporal models, incorporating exponential decay (Ebbinghaus forgetting curve) and reactivation mechanisms via Bayesian updating. The hierarchical organization of memory is represented using directed acyclic graphs to model recall dependencies and interference. Novel insights include feedback dynamics, recursive influences in memory chains, and the integration of entropy-based recall efficiency. This approach provides a foundation for understanding memory processes across cognitive and computational domains.', 'abstract_zh': '本文介绍了一个统一的理论框架，用于建模时间记忆动态，结合了时间逻辑、记忆衰退模型和分层上下文的概念。该框架使用线性和分支时间模型来形式化命题随时间的演变过程，结合了指数衰退（艾宾浩斯遗忘曲线）和通过贝叶斯更新的再激活机制。使用有向无环图来表示记忆的分层组织，以建模回忆依赖关系和干扰。新颖的见解包括反馈动态、记忆链中的递归影响以及基于熵的回忆效率的整合。该方法为理解跨认知和计算领域的记忆过程提供了基础。', 'title_zh': '时间模型在量子逻辑中的应用'}
{'arxiv_id': 'arXiv:2502.08625', 'title': 'Randomness of Low-Layer Parameters Determines Confusing Samples in Terms of Interaction Representations of a DNN', 'authors': 'Junpeng Zhang, Lei Cheng, Qing Li, Liang Lin, Quanshi Zhang', 'link': 'https://arxiv.org/abs/2502.08625', 'abstract': 'In this paper, we find that the complexity of interactions encoded by a deep neural network (DNN) can explain its generalization power. We also discover that the confusing samples of a DNN, which are represented by non-generalizable interactions, are determined by its low-layer parameters. In comparison, other factors, such as high-layer parameters and network architecture, have much less impact on the composition of confusing samples. Two DNNs with different low-layer parameters usually have fully different sets of confusing samples, even though they have similar performance. This finding extends the understanding of the lottery ticket hypothesis, and well explains distinctive representation power of different DNNs.', 'abstract_zh': '本文发现，深度神经网络(DNN)所编码的复杂交互可以解释其泛化能力。我们还发现，由非泛化交互表示的DNN混淆样本由其低层参数决定，而高层参数和网络架构对混淆样本的组成影响较小。两个具有不同低层参数的DNN通常具有完全不同的一组混淆样本，即使它们的性能相似。这一发现扩展了对彩票票假设的理解，并很好地解释了不同DNN的独特表示能力。', 'title_zh': '低层参数的随机性决定了基于DNN交互表示的迷惑样本'}
{'arxiv_id': 'arXiv:2502.08610', 'title': 'Quantifying Security Vulnerabilities: A Metric-Driven Security Analysis of Gaps in Current AI Standards', 'authors': 'Keerthana Madhavan, Abbas Yazdinejad, Fattane Zarrinkalam, Ali Dehghantanha', 'link': 'https://arxiv.org/abs/2502.08610', 'abstract': "As AI systems integrate into critical infrastructure, security gaps in AI compliance frameworks demand urgent attention. This paper audits and quantifies security risks in three major AI governance standards: NIST AI RMF 1.0, UK's AI and Data Protection Risk Toolkit, and the EU's ALTAI. Using a novel risk assessment methodology, we develop four key metrics: Risk Severity Index (RSI), Attack Potential Index (AVPI), Compliance-Security Gap Percentage (CSGP), and Root Cause Vulnerability Score (RCVS). Our analysis identifies 136 concerns across the frameworks, exposing significant gaps. NIST fails to address 69.23 percent of identified risks, ALTAI has the highest attack vector vulnerability (AVPI = 0.51) and the ICO Toolkit has the largest compliance-security gap, with 80.00 percent of high-risk concerns remaining unresolved. Root cause analysis highlights under-defined processes (ALTAI RCVS = 033) and weak implementation guidance (NIST and ICO RCVS = 0.25) as critical weaknesses. These findings emphasize the need for stronger, enforceable security controls in AI compliance. We offer targeted recommendations to enhance security posture and bridge the gap between compliance and real-world AI risks.", 'abstract_zh': '随着AI系统融入关键基础设施，AI合规框架中的安全漏洞需要紧急关注。本论文审计并量化了三项主要AI治理标准（NIST AI RMF 1.0、英国的AI和数据保护风险工具包以及欧盟的ALTAI）中的安全风险。我们使用一种新型的风险评估方法，开发了四个关键指标：风险严重性指数（RSI）、攻击潜力指数（AVPI）、合规-安全缺口百分比（CSGP）和根本原因漏洞评分（RCVS）。我们的分析识别出框架中存在的136个关切点，暴露出显著的安全缺口。NIST未能应对69.23%的识别风险，ALTAI具有最高的攻击向量漏洞（AVPI = 0.51），而ICO工具包存在最大的合规-安全缺口，80.00%的高风险关切点仍未解决。根本原因分析指出，ALTAI中未定义的过程（ALTAI RCVS = 0.33）和NIST与ICO中薄弱的实施指导（RCVS = 0.25）是关键弱点。这些发现强调了在AI合规中需要更强有力且可执行的安全控制。我们提出有针对性的建议，以增强安全态势，并弥合合规与实际AI风险之间的差距。', 'title_zh': '量化安全漏洞：当前AI标准缺口的度量驱动安全分析'}
{'arxiv_id': 'arXiv:2502.08606', 'title': 'Distillation Scaling Laws', 'authors': 'Dan Busbridge, Amitis Shidani, Floris Weers, Jason Ramapuram, Etai Littwin, Russ Webb', 'link': 'https://arxiv.org/abs/2502.08606', 'abstract': 'We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. Our findings reduce the risks associated with using distillation at scale; compute allocation for both the teacher and student models can now be done to maximize student performance. We provide compute optimal distillation recipes for when 1) a teacher exists, or 2) a teacher needs training. If many students are to be distilled, or a teacher already exists, distillation outperforms supervised pretraining until a compute level which grows predictably with student size. If one student is to be distilled and a teacher also needs training, supervised learning should be done instead. Additionally, we provide insights across our large scale study of distillation, which increase our understanding of distillation and inform experimental design.', 'abstract_zh': '我们提供了一种蒸馏缩放定律，根据计算预算及其在学生模型和教师模型之间的分配来估算蒸馏模型的性能。我们的发现降低了大规模使用蒸馏的风险；现在可以根据计算预算优化教师模型和学生模型的计算分配以最大化学生模型的性能。我们提供了适用于以下情况的计算最优蒸馏方案：1）当存在教师模型时，或2）当需要训练教师模型时。如果需要蒸馏大量学生模型，或已经存在教师模型，那么在学生模型大小增长可预测的计算水平之前，蒸馏优于监督预训练。如果只蒸馏一个学生模型且需要训练教师模型，则应采用监督学习。此外，我们还通过对大规模蒸馏研究的广泛探讨，提供了相关见解，这些见解加深了我们对蒸馏的理解，并指导实验设计。', 'title_zh': '蒸馏缩放定律'}
{'arxiv_id': 'arXiv:2502.08605', 'title': 'CurvGAD: Leveraging Curvature for Enhanced Graph Anomaly Detection', 'authors': 'Karish Grover, Geoffrey J. Gordon, Christos Faloutsos', 'link': 'https://arxiv.org/abs/2502.08605', 'abstract': 'Does the intrinsic curvature of complex networks hold the key to unveiling graph anomalies that conventional approaches overlook? Reconstruction-based graph anomaly detection (GAD) methods overlook such geometric outliers, focusing only on structural and attribute-level anomalies. To this end, we propose CurvGAD - a mixed-curvature graph autoencoder that introduces the notion of curvature-based geometric anomalies. CurvGAD introduces two parallel pipelines for enhanced anomaly interpretability: (1) Curvature-equivariant geometry reconstruction, which focuses exclusively on reconstructing the edge curvatures using a mixed-curvature, Riemannian encoder and Gaussian kernel-based decoder; and (2) Curvature-invariant structure and attribute reconstruction, which decouples structural and attribute anomalies from geometric irregularities by regularizing graph curvature under discrete Ollivier-Ricci flow, thereby isolating the non-geometric anomalies. By leveraging curvature, CurvGAD refines the existing anomaly classifications and identifies new curvature-driven anomalies. Extensive experimentation over 10 real-world datasets (both homophilic and heterophilic) demonstrates an improvement of up to 6.5% over state-of-the-art GAD methods.', 'abstract_zh': '复杂网络的内在曲率是否揭示了传统方法忽略的图异常的关键？基于曲率的图自编码器异检测方法（CurvGAD）通过引入曲率基几何异常的概念，提出了一种混合曲率图自编码器，并通过两种并行管道增强了异常可 interpretability：（1）曲率协变几何重构；（2）曲率不变结构与属性重构。', 'title_zh': 'CurvGAD：利用曲率增强图异常检测'}
{'arxiv_id': 'arXiv:2502.08597', 'title': 'Learning in Markets with Heterogeneous Agents: Dynamics and Survival of Bayesian vs. No-Regret Learners', 'authors': 'David Easley, Yoav Kolumbus, Eva Tardos', 'link': 'https://arxiv.org/abs/2502.08597', 'abstract': 'We analyze the performance of heterogeneous learning agents in asset markets with stochastic payoffs. Our agents aim to maximize the expected growth rate of their wealth but have different theories on how to learn this best. We focus on comparing Bayesian and no-regret learners in market dynamics. Bayesian learners with a prior over a finite set of models that assign positive prior probability to the correct model have posterior probabilities that converge exponentially to the correct model. Consequently, they survive even in the presence of agents who invest according to the correct model of the stochastic process. Bayesians with a continuum prior converge to the correct model at a rate of $O((\\log T)/T)$. Online learning theory provides no-regret algorithms for maximizing the log of wealth in this setting, achieving a worst-case regret bound of $O(\\log T)$ without assuming a steady underlying stochastic process but comparing to the best fixed investment rule. This regret, as we observe, is of the same order of magnitude as that of a Bayesian learner with a continuum prior. However, we show that even such low regret may not be sufficient for survival in asset markets: an agent can have regret as low as $O(\\log T)$, but still vanish in market dynamics when competing against agents who invest according to the correct model or even against a perfect Bayesian with a finite prior. On the other hand, we show that Bayesian learning is fragile, while no-regret learning requires less knowledge of the environment and is therefore more robust. Any no-regret learner will drive out of the market an imperfect Bayesian whose finite prior or update rule has even small errors. We formally establish the relationship between notions of survival, vanishing, and market domination studied in economics and the framework of regret minimization, thus bridging these theories.', 'abstract_zh': '我们分析了在具有随机收益的资产市场中异质学习代理的表现。我们的代理旨在最大化其财富预期增长率，但学习理论各不相同。我们重点比较了市场动态中的贝叶斯学习者和无遗憾学习者。具有针对有限模型集合的先验且正确模型被赋予正先验概率的贝叶斯学习者，其后验概率以指数方式趋向于正确模型，因此即使在正确模型的投资者存在的情况下也能存活。具有连续先验的贝叶斯学习者以 $O((\\log T)/T)$ 的速率收敛到正确模型。在线学习理论为此情景提供了最大化财富对数的无遗憾算法，实现了无假设平稳随机过程、但相对于最优固定投资规则的最坏情况遗憾上界为 $O(\\log T)$。正如我们观察到的，这种遗憾与具有连续先验的贝叶斯学习者相当。然而，我们证明即使如此低的遗憾也不足以在资产市场中存活：一个代理的遗憾可以低至 $O(\\log T)$，但在与根据正确模型投资或甚至与具有有限先验的完美贝叶斯学习者竞争时仍可能从市场中消失。相比之下，我们证明了贝叶斯学习是脆弱的，而无遗憾学习需要较少环境知识，因此更具鲁棒性。任何无遗憾学习者都会驱逐具有即使很小错误的有限先验或更新规则的不完美贝叶斯学习者。我们正式建立了经济中研究的生存、消亡和市场统治概念与遗憾最小化框架之间的关系，从而架起了这两类理论之间的桥梁。', 'title_zh': '具有异质性代理的市场中的学习：贝叶斯学习者与无遗憾学习者的动态与生存'}
{'arxiv_id': 'arXiv:2502.08577', 'title': 'FBFL: A Field-Based Coordination Approach for Data Heterogeneity in Federated Learning', 'authors': 'Davide Domini, Gianluca Aguzzi, Lukas Esterle, Mirko Viroli', 'link': 'https://arxiv.org/abs/2502.08577', 'abstract': "In the last years, Federated learning (FL) has become a popular solution to train machine learning models in domains with high privacy concerns. However, FL scalability and performance face significant challenges in real-world deployments where data across devices are non-independently and identically distributed (non-IID). The heterogeneity in data distribution frequently arises from spatial distribution of devices, leading to degraded model performance in the absence of proper handling. Additionally, FL typical reliance on centralized architectures introduces bottlenecks and single-point-of-failure risks, particularly problematic at scale or in dynamic environments. To close this gap, we propose Field-Based Federated Learning (FBFL), a novel approach leveraging macroprogramming and field coordination to address these limitations through: (i) distributed spatial-based leader election for personalization to mitigate non-IID data challenges; and (ii) construction of a self-organizing, hierarchical architecture using advanced macroprogramming patterns. Moreover, FBFL not only overcomes the aforementioned limitations, but also enables the development of more specialized models tailored to the specific data distribution in each subregion. This paper formalizes FBFL and evaluates it extensively using MNIST, FashionMNIST, and Extended MNIST datasets. We demonstrate that, when operating under IID data conditions, FBFL performs comparably to the widely-used FedAvg algorithm. Furthermore, in challenging non-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other state-of-the-art methods, namely FedProx and Scaffold, which have been specifically designed to address non-IID data distributions. Additionally, we showcase the resilience of FBFL's self-organizing hierarchical architecture against server failures.", 'abstract_zh': '基于域的联邦学习（Field-Based Federated Learning）', 'title_zh': 'FBFL：联邦学习中基于场的数据异质性协调方法'}
{'arxiv_id': 'arXiv:2502.08574', 'title': 'COAST: Intelligent Time-Adaptive Neural Operators', 'authors': 'Zhikai Wu, Shiyang Zhang, Sizhuang He, Sifan Wang, Min Zhu, Anran Jiao, Lu Lu, David van Dijk', 'link': 'https://arxiv.org/abs/2502.08574', 'abstract': 'We introduce Causal Operator with Adaptive Solver Transformer (COAST), a novel neural operator learning method that leverages a causal language model (CLM) framework to dynamically adapt time steps. Our method predicts both the evolution of a system and its optimal time step, intelligently balancing computational efficiency and accuracy. We find that COAST generates variable step sizes that correlate with the underlying system intrinsicities, both within and across dynamical systems. Within a single trajectory, smaller steps are taken in regions of high complexity, while larger steps are employed in simpler regions. Across different systems, more complex dynamics receive more granular time steps. Benchmarked on diverse systems with varied dynamics, COAST consistently outperforms state-of-the-art methods, achieving superior performance in both efficiency and accuracy. This work underscores the potential of CLM-based intelligent adaptive solvers for scalable operator learning of dynamical systems.', 'abstract_zh': '基于因果语言模型的自适应求解器变换器因果算子（COAST）：一种新颖的神经算子学习方法', 'title_zh': 'COAST: 智能时序自适应神经算子'}
{'arxiv_id': 'arXiv:2502.08560', 'title': 'Brain Latent Progression: Individual-based Spatiotemporal Disease Progression on 3D Brain MRIs via Latent Diffusion', 'authors': 'Lemuel Puglisi, Daniel C. Alexander, Daniele Ravì', 'link': 'https://arxiv.org/abs/2502.08560', 'abstract': 'The growing availability of longitudinal Magnetic Resonance Imaging (MRI) datasets has facilitated Artificial Intelligence (AI)-driven modeling of disease progression, making it possible to predict future medical scans for individual patients. However, despite significant advancements in AI, current methods continue to face challenges including achieving patient-specific individualization, ensuring spatiotemporal consistency, efficiently utilizing longitudinal data, and managing the substantial memory demands of 3D scans. To address these challenges, we propose Brain Latent Progression (BrLP), a novel spatiotemporal model designed to predict individual-level disease progression in 3D brain MRIs. The key contributions in BrLP are fourfold: (i) it operates in a small latent space, mitigating the computational challenges posed by high-dimensional imaging data; (ii) it explicitly integrates subject metadata to enhance the individualization of predictions; (iii) it incorporates prior knowledge of disease dynamics through an auxiliary model, facilitating the integration of longitudinal data; and (iv) it introduces the Latent Average Stabilization (LAS) algorithm, which (a) enforces spatiotemporal consistency in the predicted progression at inference time and (b) allows us to derive a measure of the uncertainty for the prediction. We train and evaluate BrLP on 11,730 T1-weighted (T1w) brain MRIs from 2,805 subjects and validate its generalizability on an external test set comprising 2,257 MRIs from 962 subjects. Our experiments compare BrLP-generated MRI scans with real follow-up MRIs, demonstrating state-of-the-art accuracy compared to existing methods. The code is publicly available at: this https URL.', 'abstract_zh': '纵向磁共振成像（MRI）数据集的不断增加availability使得基于人工智能（AI）的疾病进展建模成为可能，从而能够预测个体患者的未来医学扫描。然而，尽管在AI方面取得了显著进展，当前的方法仍然面临挑战，包括实现患者特异性个性化、确保时空一致性、有效利用纵向数据以及管理3D扫描的大量内存需求。为了解决这些挑战，我们提出了脑潜在进展（BrLP），这是一种新型时空模型，旨在预测3D脑MRI的个体水平疾病进展。BrLP的关键贡献包括四点：（i）它在小潜在空间中操作，减轻了高维成像数据带来的计算挑战；（ii）它显式地整合了受试者元数据，以增强预测的个性化；（iii）它通过辅助模型整合疾病动力学的先验知识，促进了纵向数据的整合；（iv）它引入了潜在平均稳定化（LAS）算法，该算法（a）在推断时强制预测进展的时空一致性，并（b）使我们可以推导出预测的不确定性量度。我们在2805名受试者的11,730张T1加权（T1w）脑MRI和来自962名受试者的2,257张MRI的外部测试集上训练和评估了BrLP，并展示了与现有方法相比的最先进的准确性。代码已公开：请参见this https URL。', 'title_zh': '脑隐秘 progression: 基于个体的三维脑MRIs时空调控疾病 progression 模型'}
{'arxiv_id': 'arXiv:2502.08534', 'title': 'Input convex neural networks: universal approximation theorem and implementation for isotropic polyconvex hyperelastic energies', 'authors': 'Gian-Luca Geuken, Patrick Kurzeja, David Wiedemann, Jörn Mosler', 'link': 'https://arxiv.org/abs/2502.08534', 'abstract': 'This paper presents a novel framework of neural networks for isotropic hyperelasticity that enforces necessary physical and mathematical constraints while simultaneously satisfying the universal approximation theorem. The two key ingredients are an input convex network architecture and a formulation in the elementary polynomials of the signed singular values of the deformation gradient. In line with previously published networks, it can rigorously capture frame-indifference and polyconvexity - as well as further constraints like balance of angular momentum and growth conditions. However and in contrast to previous networks, a universal approximation theorem for the proposed approach is proven. To be more explicit, the proposed network can approximate any frame-indifferent, isotropic polyconvex energy (provided the network is large enough). This is possible by working with a sufficient and necessary criterion for frame-indifferent, isotropic polyconvex functions. Comparative studies with existing approaches identify the advantages of the proposed method, particularly in approximating non-polyconvex energies as well as computing polyconvex hulls.', 'abstract_zh': '本文提出了一种新颖的神经网络框架，用于各向同性弹性的建模，该框架在满足必要的物理和数学约束的同时，同时也遵循通用逼近定理。两个关键组成部分是输入凸网络架构和变形梯度的符号奇异值的本原多项式形式。与已发布网络一致，该方法能够严格捕获框架无关性和多凸性，以及进一步的约束，如动量矩平衡和生长条件。然而，与之前的方法不同，该方法的通用逼近定理得到了证明。更具体地说，只要网络足够大，所提出的网络可以逼近任何框架无关性、各向同性和多凸能量函数。这可以通过使用框架无关性、各向同性和多凸函数的充分必要条件来实现。与现有方法的比较研究揭示了所提方法的优势，尤其是在逼近非多凸能量和计算多凸包方面。', 'title_zh': '输入凸神经网络：普遍逼近定理及各向同性多凸超弹性能量的实现'}
{'arxiv_id': 'arXiv:2502.08518', 'title': 'FedMHO: Heterogeneous One-Shot Federated Learning Towards Resource-Constrained Edge Devices', 'authors': 'Dezhong Yao, Yuexin Shi, Tongtong Liu, Zhiqiang Xu', 'link': 'https://arxiv.org/abs/2502.08518', 'abstract': 'Federated Learning (FL) is increasingly adopted in edge computing scenarios, where a large number of heterogeneous clients operate under constrained or sufficient resources. The iterative training process in conventional FL introduces significant computation and communication overhead, which is unfriendly for resource-constrained edge devices. One-shot FL has emerged as a promising approach to mitigate communication overhead, and model-heterogeneous FL solves the problem of diverse computing resources across clients. However, existing methods face challenges in effectively managing model-heterogeneous one-shot FL, often leading to unsatisfactory global model performance or reliance on auxiliary datasets. To address these challenges, we propose a novel FL framework named FedMHO, which leverages deep classification models on resource-sufficient clients and lightweight generative models on resource-constrained devices. On the server side, FedMHO involves a two-stage process that includes data generation and knowledge fusion. Furthermore, we introduce FedMHO-MD and FedMHO-SD to mitigate the knowledge-forgetting problem during the knowledge fusion stage, and an unsupervised data optimization solution to improve the quality of synthetic samples. Comprehensive experiments demonstrate the effectiveness of our methods, as they outperform state-of-the-art baselines in various experimental setups.', 'abstract_zh': '联邦学习（FL）在边缘计算场景中日益采用，其中大量异构客户端在受限或充足的资源下运行。传统FL的迭代训练过程引入了显著的计算和通信开销，这不利于资源受限的边缘设备。一次性FL已成为减轻通信开销的有前景的方法，而模型异构的FL解决了客户端之间计算资源多样性的问题。然而，现有方法在有效管理模型异构的一次性FL方面面临挑战，往往导致全局模型性能不佳或依赖辅助数据集。为了解决这些挑战，我们提出了一种新颖的FL框架FedMHO，该框架在资源充足的客户端上利用深度分类模型，在资源受限的设备上利用轻量级生成模型。在服务器端，FedMHO涉及一个两阶段过程，包括数据生成和知识融合。此外，我们引入了FedMHO-MD和FedMHO-SD来缓解知识融合阶段的知识遗忘问题，并提出了无监督数据优化解决方案以提高合成样本的质量。全面的实验表明，我们的方法在各种实验设置中优于现有最先进的基线方法。', 'title_zh': 'FedMHO: 面向资源受限边缘设备的异构一次性联邦学习'}
{'arxiv_id': 'arXiv:2502.08474', 'title': 'Training-Free Restoration of Pruned Neural Networks', 'authors': 'Keonho Lee, Minsoo Kim, Dong-Wan Choi', 'link': 'https://arxiv.org/abs/2502.08474', 'abstract': 'Although network pruning has been highly popularized to compress deep neural networks, its resulting accuracy heavily depends on a fine-tuning process that is often computationally expensive and requires the original data. However, this may not be the case in real-world scenarios, and hence a few recent works attempt to restore pruned networks without any expensive retraining process. Their strong assumption is that every neuron being pruned can be replaced with another one quite similar to it, but unfortunately this does not hold in many neural networks, where the similarity between neurons is extremely low in some layers. In this article, we propose a more rigorous and robust method of restoring pruned networks in a fine-tuning free and data-free manner, called LBYL (Leave Before You Leave). LBYL significantly relaxes the aforementioned assumption in a way that each pruned neuron leaves its pieces of information to as many preserved neurons as possible and thereby multiple neurons together obtain a more robust approximation to the original output of the neuron who just left. Our method is based on a theoretical analysis on how to formulate the reconstruction error between the original network and its approximation, which nicely leads to a closed form solution for our derived loss function. Through the extensive experiments, LBYL is confirmed to be indeed more effective to approximate the original network and consequently able to achieve higher accuracy for restored networks, compared to the recent approaches exploiting the similarity between two neurons. The very first version of this work, which contains major technical and theoretical components, was submitted to NeurIPS 2021 and ICML 2022.', 'abstract_zh': '尽管网络修剪已被广泛用于压缩深度神经网络，但其结果的精度高度依赖于一个经常计算成本高昂且需要原始数据的微调过程。然而，在实际场景中这可能并不总是成立，因此一些近期的工作尝试在没有昂贵的重新训练过程的情况下恢复修剪的网络。他们假设每一个被修剪的神经元都可以被另一个与其非常相似的神经元所替代，但不幸的是，这种情况在很多神经网络中并不成立，因为在某些层中神经元之间的相似性非常低。本文提出了一种在无微调和无数据条件下更严格和robust的方法来恢复修剪的网络，称为LBYL（Leave Before You Leave）。LBYL以一种方式显著放宽了上述假设，即每一个被修剪的神经元尽可能多地向保留的神经元留下它的信息，从而多个神经元一起获得对刚刚离开的神经元原输出的更robust的近似。该方法基于对原始网络与其近似之间的重构误差的理论分析，这自然地导出了我们所推导出的损失函数的闭式解。通过广泛的实验，LBYL确实被证实对近似原始网络更有效，从而能够在恢复的网络中实现更高的精度，相比于利用两个神经元之间相似性的近期方法。本文的第一版，包含主要的技术和理论成分，被提交给了NeurIPS 2021和ICML 2022。', 'title_zh': '剪枝神经网络的无训练恢复'}
{'arxiv_id': 'arXiv:2502.08453', 'title': 'Proceedings 40th International Conference on Logic Programming', 'authors': 'Pedro Cabalar, Francesco Fabiano, Martin Gebser, Gopal Gupta, Theresa Swift', 'link': 'https://arxiv.org/abs/2502.08453', 'abstract': 'Since the first conference In Marseille in 1982, the International Conference on Logic Programming (ICLP) has been the premier international event for presenting research in logic programming. These proceedings include technical communications about, and abstracts for presentations given at the 40th ICLP held October 14-17, in Dallas Texas, USA. The papers and abstracts in this volume include the following areas and topics.  Formal and operational semantics: including non-monotonic reasoning, probabilistic reasoning, argumentation, and semantic issues of combining logic with neural models.  Language design and programming methodologies such as answer set programming. inductive logic programming, and probabilistic programming. Program analysis and logic-based validation of generated programs.  Implementation methodologies including constraint implementation, tabling, Logic-based prompt engineering, and the interaction of logic programming with LLMs.', 'abstract_zh': '自1982年在马赛举行的首次会议以来，国际逻辑编程会议（ICLP）一直是展示逻辑编程研究的 premier 国际活动。这些 proceedings 收录了在2023年10月14-17日于美国德克萨斯州达拉斯举行的第40届ICLP的technical communications 和摘要。本卷中的论文和摘要涵盖了如下领域和主题：正式语义和操作语义：包括非单调推理、概率推理、论辩和将逻辑与神经模型结合的语义问题。语言设计和编程方法，如回答集编程、归纳逻辑编程和概率编程。程序分析和基于逻辑验证生成程序。实现方法，包括约束实现、表查询、基于逻辑的提示工程以及逻辑编程与LLM的交互。', 'title_zh': '第40届逻辑编程国际会议论文集'}
{'arxiv_id': 'arXiv:2502.08450', 'title': 'Towards Prompt Generalization: Grammar-aware Cross-Prompt Automated Essay Scoring', 'authors': 'Heejin Do, Taehee Park, Sangwon Ryu, Gary Geunbae Lee', 'link': 'https://arxiv.org/abs/2502.08450', 'abstract': "In automated essay scoring (AES), recent efforts have shifted toward cross-prompt settings that score essays on unseen prompts for practical applicability. However, prior methods trained with essay-score pairs of specific prompts pose challenges in obtaining prompt-generalized essay representation. In this work, we propose a grammar-aware cross-prompt trait scoring (GAPS), which internally captures prompt-independent syntactic aspects to learn generic essay representation. We acquire grammatical error-corrected information in essays via the grammar error correction technique and design the AES model to seamlessly integrate such information. By internally referring to both the corrected and the original essays, the model can focus on generic features during training. Empirical experiments validate our method's generalizability, showing remarkable improvements in prompt-independent and grammar-related traits. Furthermore, GAPS achieves notable QWK gains in the most challenging cross-prompt scenario, highlighting its strength in evaluating unseen prompts.", 'abstract_zh': '在自动作文评分（AES）中，最近的努力转向了针对未见过的提示进行评分的跨提示设置，以提高其实用性。然而，先前使用特定提示的作文-评分对进行训练的方法在获得提示泛化的作文表示时面临挑战。在本工作中，我们提出了一种语法感知的跨提示特性评分（GAPS），它内部捕获与提示无关的句法方面，以学习通用的作文表示。我们通过语法错误纠正技术获取作文中的语法错误修正信息，并设计AES模型以无缝集成此类信息。通过内部同时参考修正后的和原始的作文，模型可以在训练过程中专注于通用特征。实验验证了该方法的泛化能力，展示了在提示无关和语法相关的特性上显著的改进。此外，GAPS 在最具挑战性的跨提示场景中实现了显著的QWK提升，突显了其在评估未见过的提示方面的强大能力。', 'title_zh': '面向提示泛化的语法意识跨提示自动作文评分'}
{'arxiv_id': 'arXiv:2502.08417', 'title': 'Handwritten Text Recognition: A Survey', 'authors': 'Carlos Garrido-Munoz, Antonio Rios-Vila, Jorge Calvo-Zaragoza', 'link': 'https://arxiv.org/abs/2502.08417', 'abstract': 'Handwritten Text Recognition (HTR) has become an essential field within pattern recognition and machine learning, with applications spanning historical document preservation to modern data entry and accessibility solutions. The complexity of HTR lies in the high variability of handwriting, which makes it challenging to develop robust recognition systems. This survey examines the evolution of HTR models, tracing their progression from early heuristic-based approaches to contemporary state-of-the-art neural models, which leverage deep learning techniques. The scope of the field has also expanded, with models initially capable of recognizing only word-level content progressing to recent end-to-end document-level approaches. Our paper categorizes existing work into two primary levels of recognition: (1) \\emph{up to line-level}, encompassing word and line recognition, and (2) \\emph{beyond line-level}, addressing paragraph- and document-level challenges. We provide a unified framework that examines research methodologies, recent advances in benchmarking, key datasets in the field, and a discussion of the results reported in the literature. Finally, we identify pressing research challenges and outline promising future directions, aiming to equip researchers and practitioners with a roadmap for advancing the field.', 'abstract_zh': '手写文本识别（HTR）已成为模式识别和机器学习领域中的一个重要研究方向，其应用范围从历史文献保护扩展到现代数据录入和无障碍解决方案。手写文本识别的复杂性在于手写高度变化，这使得开发稳健的识别系统极具挑战性。本文综述了手写文本识别模型的发展历程，从早期基于启发式的方法逐步演进到当前利用深度学习技术的先进神经网络模型。该领域的研究范围也得到了扩展，从仅能识别单词级别的内容逐步发展到最近的端到端文档级方法。本文将现有工作分为两个主要的识别级别：(1) 线性级别以下，包括单词和行的识别；(2) 线性级别以上，解决段落级和文档级的挑战。我们提供了一个统一的框架来审视研究方法、最近的 benchmarking 进展、领域内的关键数据集以及文献中报告的结果。最后，我们指出了亟待解决的研究挑战，并概述了有潜力的未来发展方向，旨在为研究者和实践者提供一个推动该领域发展的路线图。', 'title_zh': '手写文本识别：一种综述'}
{'arxiv_id': 'arXiv:2502.08365', 'title': 'Towards Principled Multi-Agent Task Agnostic Exploration', 'authors': 'Riccardo Zamboni, Mirco Mutti, Marcello Restelli', 'link': 'https://arxiv.org/abs/2502.08365', 'abstract': "In reinforcement learning, we typically refer to task-agnostic exploration when we aim to explore the environment without access to the task specification a priori. In a single-agent setting the problem has been extensively studied and mostly understood. A popular approach cast the task-agnostic objective as maximizing the entropy of the state distribution induced by the agent's policy, from which principles and methods follows. In contrast, little is known about task-agnostic exploration in multi-agent settings, which are ubiquitous in the real world. How should different agents explore in the presence of others? In this paper, we address this question through a generalization to multiple agents of the problem of maximizing the state distribution entropy. First, we investigate alternative formulations, highlighting respective positives and negatives. Then, we present a scalable, decentralized, trust-region policy search algorithm to address the problem in practical settings. Finally, we provide proof of concept experiments to both corroborate the theoretical findings and pave the way for task-agnostic exploration in challenging multi-agent settings.", 'abstract_zh': '在强化学习中，当我们旨在在无法预先获取任务规范的情况下探索环境时，通常称之为任务无关探索。在单-agent设置中，该问题已被广泛研究并基本理解。一种流行的方法是将任务无关目标定义为最大化由代理策略诱导的状态分布熵，从而得出相应的原则和方法。相比之下，在多-agent设置中，这种情况在现实世界中普遍存在，但对其了解甚少。面对其他代理时，不同代理应该如何探索？在本文中，我们通过将最大化状态分布熵的问题推广到多代理场景来回答这个问题。首先，我们探讨了不同的问题表述，并突出各自的优点和缺点。然后，我们提出了一种可扩展的、去中心化的、信任区域策略搜索算法，以解决实际场景中的问题。最后，我们提供了概念验证实验，既验证了理论发现，也为具有挑战性的多-agent场景中的任务无关探索铺平了道路。', 'title_zh': '面向原则的多智能体任务无关探索'}
{'arxiv_id': 'arXiv:2502.08346', 'title': 'Graph Foundation Models for Recommendation: A Comprehensive Survey', 'authors': 'Bin Wu, Yihang Wang, Yuanhao Zeng, Jiawei Liu, Jiashu Zhao, Cheng Yang, Yawen Li, Long Xia, Dawei Yin, Chuan Shi', 'link': 'https://arxiv.org/abs/2502.08346', 'abstract': 'Recommender systems (RS) serve as a fundamental tool for navigating the vast expanse of online information, with deep learning advancements playing an increasingly important role in improving ranking accuracy. Among these, graph neural networks (GNNs) excel at extracting higher-order structural information, while large language models (LLMs) are designed to process and comprehend natural language, making both approaches highly effective and widely adopted. Recent research has focused on graph foundation models (GFMs), which integrate the strengths of GNNs and LLMs to model complex RS problems more efficiently by leveraging the graph-based structure of user-item relationships alongside textual understanding. In this survey, we provide a comprehensive overview of GFM-based RS technologies by introducing a clear taxonomy of current approaches, diving into methodological details, and highlighting key challenges and future directions. By synthesizing recent advancements, we aim to offer valuable insights into the evolving landscape of GFM-based recommender systems.', 'abstract_zh': '基于图的推荐系统：利用图神经网络和大规模语言模型的技术综述', 'title_zh': '图基础模型推荐：一项综合综述'}
{'arxiv_id': 'arXiv:2502.08340', 'title': 'Hierarchical Learning-based Graph Partition for Large-scale Vehicle Routing Problems', 'authors': 'Yuxin Pan, Ruohong Liu, Yize Chen, Zhiguang Cao, Fangzhen Lin', 'link': 'https://arxiv.org/abs/2502.08340', 'abstract': 'Neural solvers based on the divide-and-conquer approach for Vehicle Routing Problems (VRPs) in general, and capacitated VRP (CVRP) in particular, integrates the global partition of an instance with local constructions for each subproblem to enhance generalization. However, during the global partition phase, misclusterings within subgraphs have a tendency to progressively compound throughout the multi-step decoding process of the learning-based partition policy. This suboptimal behavior in the global partition phase, in turn, may lead to a dramatic deterioration in the performance of the overall decomposition-based system, despite using optimal local constructions. To address these challenges, we propose a versatile Hierarchical Learning-based Graph Partition (HLGP) framework, which is tailored to benefit the partition of CVRP instances by synergistically integrating global and local partition policies. Specifically, the global partition policy is tasked with creating the coarse multi-way partition to generate the sequence of simpler two-way partition subtasks. These subtasks mark the initiation of the subsequent K local partition levels. At each local partition level, subtasks exclusive for this level are assigned to the local partition policy which benefits from the insensitive local topological features to incrementally alleviate the compounded errors. This framework is versatile in the sense that it optimizes the involved partition policies towards a unified objective harmoniously compatible with both reinforcement learning (RL) and supervised learning (SL). (*Due to the notification of arXiv "The Abstract field cannot be longer than 1,920 characters", the appeared Abstract is shortened. For the full Abstract, please download the Article.)', 'abstract_zh': '基于分而治之方法的神经网络求解器在一般车辆路线问题（VRP）及容量受限车辆路线问题（CVRP）中的全局划分与局部构建结合增强泛化能力的研究', 'title_zh': '基于层级学习的图划分方法解决大规模车辆路径问题'}
{'arxiv_id': 'arXiv:2502.08337', 'title': 'Hierarchical Multi-Agent Framework for Carbon-Efficient Liquid-Cooled Data Center Clusters', 'authors': 'Soumyendu Sarkar, Avisek Naug, Antonio Guillen, Vineet Gundecha, Ricardo Luna Gutierrez, Sahand Ghorbanpour, Sajad Mousavi, Ashwin Ramesh Babu, Desik Rengarajan, Cullen Bash', 'link': 'https://arxiv.org/abs/2502.08337', 'abstract': 'Reducing the environmental impact of cloud computing requires efficient workload distribution across geographically dispersed Data Center Clusters (DCCs) and simultaneously optimizing liquid and air (HVAC) cooling with time shift of workloads within individual data centers (DC). This paper introduces Green-DCC, which proposes a Reinforcement Learning (RL) based hierarchical controller to optimize both workload and liquid cooling dynamically in a DCC. By incorporating factors such as weather, carbon intensity, and resource availability, Green-DCC addresses realistic constraints and interdependencies. We demonstrate how the system optimizes multiple data centers synchronously, enabling the scope of digital twins, and compare the performance of various RL approaches based on carbon emissions and sustainability metrics while also offering a framework and benchmark simulation for broader ML research in sustainability.', 'abstract_zh': '减少云 computing 对环境的影响需要高效地在地理分散的数据中心群组（DCCs）之间分配工作负载，并同时优化液体冷却和空气（HVAC）冷却，通过个体数据中心内部工作负载的时间转移来实现。本文介绍了 Green-DCC，该方法提出了一种基于强化学习（RL）的分层控制器，以动态优化数据中心群组中的工作负载和液体冷却。通过纳入天气、碳强度和资源可用性等因素，Green-DCC 应对了现实中的约束和相互依赖性。我们展示了系统如何同步优化多个数据中心，扩展数字孪生的应用范围，并基于碳排放和可持续性指标比较了不同 RL 方法的性能，同时提供了一个针对可持续性研究更广泛 ML 研究的框架和基准仿真。', 'title_zh': '面向低碳液冷数据中心集群的分层多Agent框架'}
{'arxiv_id': 'arXiv:2502.08302', 'title': 'HDT: Hierarchical Discrete Transformer for Multivariate Time Series Forecasting', 'authors': 'Shibo Feng, Peilin Zhao, Liu Liu, Pengcheng Wu, Zhiqi Shen', 'link': 'https://arxiv.org/abs/2502.08302', 'abstract': 'Generative models have gained significant attention in multivariate time series forecasting (MTS), particularly due to their ability to generate high-fidelity samples. Forecasting the probability distribution of multivariate time series is a challenging yet practical task. Although some recent attempts have been made to handle this task, two major challenges persist: 1) some existing generative methods underperform in high-dimensional multivariate time series forecasting, which is hard to scale to higher dimensions; 2) the inherent high-dimensional multivariate attributes constrain the forecasting lengths of existing generative models. In this paper, we point out that discrete token representations can model high-dimensional MTS with faster inference time, and forecasting the target with long-term trends of itself can extend the forecasting length with high accuracy. Motivated by this, we propose a vector quantized framework called Hierarchical Discrete Transformer (HDT) that models time series into discrete token representations with l2 normalization enhanced vector quantized strategy, in which we transform the MTS forecasting into discrete tokens generation. To address the limitations of generative models in long-term forecasting, we propose a hierarchical discrete Transformer. This model captures the discrete long-term trend of the target at the low level and leverages this trend as a condition to generate the discrete representation of the target at the high level that introduces the features of the target itself to extend the forecasting length in high-dimensional MTS. Extensive experiments on five popular MTS datasets verify the effectiveness of our proposed method.', 'abstract_zh': '生成模型在多变量时间序列预测中的应用取得了显著关注，尤其是在生成高保真样本方面。预测多变量时间序列的概率分布是一项具有挑战性但实际意义重大的任务。尽管最近已经有一些尝试处理这个问题，但仍然存在两大挑战：1）一些现有的生成方法在高维多变量时间序列预测中表现不佳，难以扩展到更高维度；2）固有的高维多变量属性限制了现有生成模型的预测长度。在本文中，我们指出，离散标记表示可以在加快推理时间的同时表示高维多变量时间序列，并通过长期趋势预测目标可以大大提高预测长度的准确性。受此启发，我们提出了一种称为分层离散变换器（HDT）的向量量化框架，该框架使用L2归一化的增强向量量化策略将时间序列转换为离散标记表示，从而将多变量时间序列预测转化为离散标记生成。为了克服生成模型在长期预测中的局限性，我们提出了一种分层离散变换器模型。该模型在低层捕捉目标的离散长期趋势，并利用这种趋势作为条件生成高层的目标离散表示，从而在高维多变量时间序列中引入目标自身的特征来延长预测长度。广泛的实验在五个流行的多变量时间序列数据集上验证了我们提出方法的有效性。', 'title_zh': '多层次离散变换器多变量时间序列预测'}
{'arxiv_id': 'arXiv:2502.08282', 'title': 'Individualised Treatment Effects Estimation with Composite Treatments and Composite Outcomes', 'authors': 'Vinod Kumar Chauhan, Lei Clifton, Gaurav Nigam, David A. Clifton', 'link': 'https://arxiv.org/abs/2502.08282', 'abstract': 'Estimating individualised treatment effect (ITE) -- that is the causal effect of a set of variables (also called exposures, treatments, actions, policies, or interventions), referred to as \\textit{composite treatments}, on a set of outcome variables of interest, referred to as \\textit{composite outcomes}, for a unit from observational data -- remains a fundamental problem in causal inference with applications across disciplines, such as healthcare, economics, education, social science, marketing, and computer science. Previous work in causal machine learning for ITE estimation is limited to simple settings, like single treatments and single outcomes. This hinders their use in complex real-world scenarios; for example, consider studying the effect of different ICU interventions, such as beta-blockers and statins for a patient admitted for heart surgery, on different outcomes of interest such as atrial fibrillation and in-hospital mortality. The limited research into composite treatments and outcomes is primarily due to data scarcity for all treatments and outcomes. To address the above challenges, we propose a novel and innovative hypernetwork-based approach, called \\emph{H-Learner}, to solve ITE estimation under composite treatments and composite outcomes, which tackles the data scarcity issue by dynamically sharing information across treatments and outcomes. Our empirical analysis with binary and arbitrary composite treatments and outcomes demonstrates the effectiveness of the proposed approach compared to existing methods.', 'abstract_zh': '基于超网络的复合治疗与复合结果个体化治疗效果估计（H-Learner）', 'title_zh': '个体化治疗效果估计中的复合治疗与复合结局'}
{'arxiv_id': 'arXiv:2502.08266', 'title': 'Dealing with Annotator Disagreement in Hate Speech Classification', 'authors': 'Somaiyeh Dehghan, Mehmet Umut Sen, Berrin Yanikoglu', 'link': 'https://arxiv.org/abs/2502.08266', 'abstract': 'Hate speech detection is a crucial task, especially on social media, where harmful content can spread quickly. Implementing machine learning models to automatically identify and address hate speech is essential for mitigating its impact and preventing its proliferation. The first step in developing an effective hate speech detection model is to acquire a high-quality dataset for training. Labeled data is foundational for most natural language processing tasks, but categorizing hate speech is difficult due to the diverse and often subjective nature of hate speech, which can lead to varying interpretations and disagreements among annotators. This paper examines strategies for addressing annotator disagreement, an issue that has been largely overlooked. In particular, we evaluate different approaches to deal with annotator disagreement regarding hate speech classification in Turkish tweets, based on a fine-tuned BERT model. Our work highlights the importance of the problem and provides state-of-art benchmark results for detection and understanding of hate speech in online discourse.', 'abstract_zh': '仇恨言论检测是一项关键任务，特别是在社交媒体上，有害内容可以迅速传播。开发有效的仇恨言论检测模型的第一步是获取高质量的数据集进行训练。标注数据对于大多数自然语言处理任务至关重要，但由于仇恨言论的多样性和主观性，对其进行分类会导致标注者之间存在不同的解释和分歧。本文探讨了应对标注者分歧的策略，这是一个长期被忽视的问题。特别是，我们根据微调的BERT模型，评估了处理土耳其推文中仇恨言论分类标注者分歧的不同方法。我们的研究突显了该问题的重要性，并提供了在线话语中仇恨言论检测和理解的最新基准结果。', 'title_zh': '处理仇恨言论分类中的注释者分歧'}
{'arxiv_id': 'arXiv:2502.08259', 'title': 'Balancing optimism and pessimism in offline-to-online learning', 'authors': 'Sentenac Flore, Lee Albin, Szepesvari Csaba', 'link': 'https://arxiv.org/abs/2502.08259', 'abstract': 'We consider what we call the offline-to-online learning setting, focusing on stochastic finite-armed bandit problems. In offline-to-online learning, a learner starts with offline data collected from interactions with an unknown environment in a way that is not under the learner\'s control. Given this data, the learner begins interacting with the environment, gradually improving its initial strategy as it collects more data to maximize its total reward. The learner in this setting faces a fundamental dilemma: if the policy is deployed for only a short period, a suitable strategy (in a number of senses) is the Lower Confidence Bound (LCB) algorithm, which is based on pessimism. LCB can effectively compete with any policy that is sufficiently "covered" by the offline data. However, for longer time horizons, a preferred strategy is the Upper Confidence Bound (UCB) algorithm, which is based on optimism. Over time, UCB converges to the performance of the optimal policy at a rate that is nearly the best possible among all online algorithms. In offline-to-online learning, however, UCB initially explores excessively, leading to worse short-term performance compared to LCB. This suggests that a learner not in control of how long its policy will be in use should start with LCB for short horizons and gradually transition to a UCB-like strategy as more rounds are played. This article explores how and why this transition should occur. Our main result shows that our new algorithm performs nearly as well as the better of LCB and UCB at any point in time. The core idea behind our algorithm is broadly applicable, and we anticipate that our results will extend beyond the multi-armed bandit setting.', 'abstract_zh': '我们考虑一种我们称为离线到在线学习的设置，关注随机有限臂bandit问题。在离线到在线学习中，学习者从不受其控制的方式与未知环境交互收集离线数据。基于这些数据，学习者开始与环境交互，逐渐改进其初始策略，以最大化其总奖励。在这种设置中，学习者面临一个根本性的困境：如果策略仅部署较短时间，一种合适的战略是基于悲观原则的Lower Confidence Bound (LCB)算法。LCB能够有效与任何被离线数据充分“覆盖”的策略竞争。然而，对于更长的时间范围，更佳的战略是基于乐观原则的Upper Confidence Bound (UCB)算法。随着时间推移，UCB以几乎是最优的在线算法中最快的速率收敛到最优策略的表现。然而，在离线到在线学习中，UCB最初过度探索，导致短期内性能较差，相较于LCB。这表明，在无法控制策略使用时间的学习者应开始使用LCB策略进行较短期限，并随着回合数的增加逐渐过渡到类似UCB的战略。本文探讨了这种过渡应该如何发生及其原因。我们的主要结果表明，我们的新算法在任何时候的表现几乎与LCB和UCB中较好的算法表现一样优秀。我们的算法背后的核心思想具有广泛的应用性，我们预期我们的结果将超出多臂bandit环境的范围。', 'title_zh': '离线到在线学习中乐观与 pessimism 的平衡'}
{'arxiv_id': 'arXiv:2502.08211', 'title': 'Quality over Quantity: Boosting Data Efficiency Through Ensembled Multimodal Data Curation', 'authors': 'Jinda Xu, Yuhao Song, Daming Wang, Weiwei Zhao, Minghua Chen, Kangliang Chen, Qinya Li', 'link': 'https://arxiv.org/abs/2502.08211', 'abstract': 'In an era overwhelmed by vast amounts of data, the effective curation of web-crawl datasets is essential for optimizing model performance. This paper tackles the challenges associated with the unstructured and heterogeneous nature of such datasets. Traditional heuristic curation methods often inadequately capture complex features, resulting in biases and the exclusion of relevant data. We introduce an advanced, learning-driven approach, Ensemble Curation Of DAta ThroUgh Multimodal Operators (EcoDatum), incorporating a novel quality-guided deduplication method to ensure balanced feature distributions. EcoDatum strategically integrates various unimodal and multimodal data curation operators within a weak supervision ensemble framework, utilizing automated optimization to score each data point effectively. EcoDatum, which significantly improves the data curation quality and efficiency, outperforms existing state-of-the-art (SOTA) techniques, ranked 1st on the DataComp leaderboard, with an average performance score of 0.182 across 38 diverse evaluation datasets. This represents a 28% improvement over the DataComp baseline method, demonstrating its effectiveness in improving dataset curation and model training efficiency.', 'abstract_zh': '在大数据泛滥的时代，优化网络抓取数据集的编目对于提升模型性能至关重要。本文应对这类数据集无结构性和异质性的挑战。传统的启发式编目方法往往难以捕捉复杂特征，导致偏见并排除相关数据。我们提出了一种先进的学习驱动方法，即多模态操作员驱动的数据集中编目（EcoDatum），结合了一种新颖的质量导向去重方法，确保特征分布的均衡。EcoDatum 在弱监督集成框架内战略性地整合多种单模态和多模态数据编目操作，并利用自动化优化有效评分每个数据点。EcoDatum 显著提升数据编目质量和效率，超越现有先进方法（SOTA），在 DataComp 领先榜上排名第 1，平均性能得分为 0.182，涵盖 38 个不同评估数据集。相比于 DataComp 基线方法，性能提升 28%，证明了其在提高数据集编目和模型训练效率方面的有效性。', 'title_zh': '质胜于量：通过集成多模态数据整理提升数据效率'}
{'arxiv_id': 'arXiv:2502.08209', 'title': 'Equivariant Masked Position Prediction for Efficient Molecular Representation', 'authors': 'Junyi An, Chao Qu, Yun-Fei Shi, XinHao Liu, Qianwei Tang, Fenglei Cao, Yuan Qi', 'link': 'https://arxiv.org/abs/2502.08209', 'abstract': "Graph neural networks (GNNs) have shown considerable promise in computational chemistry. However, the limited availability of molecular data raises concerns regarding GNNs' ability to effectively capture the fundamental principles of physics and chemistry, which constrains their generalization capabilities. To address this challenge, we introduce a novel self-supervised approach termed Equivariant Masked Position Prediction (EMPP), grounded in intramolecular potential and force theory. Unlike conventional attribute masking techniques, EMPP formulates a nuanced position prediction task that is more well-defined and enhances the learning of quantum mechanical features. EMPP also bypasses the approximation of the Gaussian mixture distribution commonly used in denoising methods, allowing for more accurate acquisition of physical properties. Experimental results indicate that EMPP significantly enhances performance of advanced molecular architectures, surpassing state-of-the-art self-supervised approaches. Our code is released in this https URL.", 'abstract_zh': 'Graph神经网络（GNNs）在计算化学中展示了显著的潜力。然而，分子数据的有限可用性引发了关于GNNs是否能够有效捕捉物理学和化学基本原理的担忧，这限制了它们的泛化能力。为应对这一挑战，我们提出了一种基于分子内势和力理论的新颖自监督方法，称为等变遮掩位置预测（EMPP）。与传统的属性遮掩技术不同，EMPP 形成了一个更明确的位置预测任务，有助于增强量子力学特征的学习。此外，EMPP 跳过了去噪方法中常用的高斯混合分布近似，从而更准确地获取物理性质。实验结果表明，EMPP 显著提升了高级分子架构的表现，超越了当前最好的自监督方法。我们的代码在此 https://释放。', 'title_zh': '对称掩码位置预测用于高效分子表示'}
{'arxiv_id': 'arXiv:2502.08181', 'title': 'Latest Advancements Towards Catastrophic Forgetting under Data Scarcity: A Comprehensive Survey on Few-Shot Class Incremental Learning', 'authors': "M. Anwar Ma'sum, Mahardhika Pratama, Igor Skrjanc", 'link': 'https://arxiv.org/abs/2502.08181', 'abstract': 'Data scarcity significantly complicates the continual learning problem, i.e., how a deep neural network learns in dynamic environments with very few samples. However, the latest progress of few-shot class incremental learning (FSCIL) methods and related studies show insightful knowledge on how to tackle the problem. This paper presents a comprehensive survey on FSCIL that highlights several important aspects i.e. comprehensive and formal objectives of FSCIL approaches, the importance of prototype rectifications, the new learning paradigms based on pre-trained model and language-guided mechanism, the deeper analysis of FSCIL performance metrics and evaluation, and the practical contexts of FSCIL in various areas. Our extensive discussion presents the open challenges, potential solutions, and future directions of FSCIL.', 'abstract_zh': '数据稀缺显著 complicates 持续学习问题，即如何在样本极少的动态环境中使深度神经网络进行学习。然而，最新的少样本类别增量学习（FSCIL）方法及相关研究展示了应对这一问题的有价值的见解。本文对 FSCIL 进行了全面综述，强调了几方面的重要内容，包括 FSCIL 方法的全面和正式目标、原型校正的重要性、基于预训练模型和语言引导机制的新学习范式、FSCIL 性能指标和评估的深入分析，以及 FSCIL 在各个领域的实际应用背景。我们广泛的讨论提出了 FSCIL 的开放挑战、潜在解决方案和未来方向。', 'title_zh': '在数据稀缺下的灾难性遗忘最新进展：少量样本分类增量学习综述'}
{'arxiv_id': 'arXiv:2502.08161', 'title': 'MixDec Sampling: A Soft Link-based Sampling Method of Graph Neural Network for Recommendation', 'authors': 'Xiangjin Xie, Yuxin Chen, Ruipeng Wang, Kai Ouyang, Zihan Zhang, Hai-Tao Zheng, Buyue Qian, Hansen Zheng, Bo Hu, Chengxiang Zhuo, Zang Li', 'link': 'https://arxiv.org/abs/2502.08161', 'abstract': 'Graph neural networks have been widely used in recent recommender systems, where negative sampling plays an important role. Existing negative sampling methods restrict the relationship between nodes as either hard positive pairs or hard negative pairs. This leads to the loss of structural information, and lacks the mechanism to generate positive pairs for nodes with few neighbors. To overcome limitations, we propose a novel soft link-based sampling method, namely MixDec Sampling, which consists of Mixup Sampling module and Decay Sampling module. The Mixup Sampling augments node features by synthesizing new nodes and soft links, which provides sufficient number of samples for nodes with few neighbors. The Decay Sampling strengthens the digestion of graph structure information by generating soft links for node embedding learning. To the best of our knowledge, we are the first to model sampling relationships between nodes by soft links in GNN-based recommender systems. Extensive experiments demonstrate that the proposed MixDec Sampling can significantly and consistently improve the recommendation performance of several representative GNN-based models on various recommendation benchmarks.', 'abstract_zh': '基于软链接的MixDec采样方法在图神经网络推荐系统中的应用', 'title_zh': 'MixDec采样：图神经网络推荐中的软链接基采样方法'}
{'arxiv_id': 'arXiv:2502.08160', 'title': 'Vertical Federated Learning in Practice: The Good, the Bad, and the Ugly', 'authors': 'Zhaomin Wu, Zhen Qin, Junyi Hou, Haodong Zhao, Qinbin Li, Bingsheng He, Lixin Fan', 'link': 'https://arxiv.org/abs/2502.08160', 'abstract': 'Vertical Federated Learning (VFL) is a privacy-preserving collaborative learning paradigm that enables multiple parties with distinct feature sets to jointly train machine learning models without sharing their raw data. Despite its potential to facilitate cross-organizational collaborations, the deployment of VFL systems in real-world applications remains limited. To investigate the gap between existing VFL research and practical deployment, this survey analyzes the real-world data distributions in potential VFL applications and identifies four key findings that highlight this gap. We propose a novel data-oriented taxonomy of VFL algorithms based on real VFL data distributions. Our comprehensive review of existing VFL algorithms reveals that some common practical VFL scenarios have few or no viable solutions. Based on these observations, we outline key research directions aimed at bridging the gap between current VFL research and real-world applications.', 'abstract_zh': '垂直 Federated Learning (VFL) 是一种隐私保护的合作学习范式，使多个具有不同特征集的参与方能够在不共享其原始数据的情况下共同训练机器学习模型。尽管 VFL 有促进组织间合作的潜力，但在实际应用中的部署仍然受到限制。为了探究现有 VFL 研究与实际部署之间的差距，本文分析了潜在 VFL 应用中的现实数据分布，并识别出四个关键发现，突显了这一差距。我们基于实际的 VFL 数据分布提出了一种新颖的数据导向型 VFL 算法分类框架。通过全面回顾现有 VFL 算法，我们发现一些常见的实际 VFL 场景缺乏可行的解决方案。基于这些观察，我们概述了关键的研究方向，旨在弥合当前 VFL 研究与实际应用之间的差距。', 'title_zh': '垂直联邦学习在实践中的优缺点及挑战'}
{'arxiv_id': 'arXiv:2502.08155', 'title': 'DGSense: A Domain Generalization Framework for Wireless Sensing', 'authors': 'Rui Zhou, Yu Cheng, Songlin Li, Hongwang Zhang, Chenxu Liu', 'link': 'https://arxiv.org/abs/2502.08155', 'abstract': 'Wireless sensing is of great benefits to our daily lives. However, wireless signals are sensitive to the surroundings. Various factors, e.g. environments, locations, and individuals, may induce extra impact on wireless propagation. Such a change can be regarded as a domain, in which the data distribution shifts. A vast majority of the sensing schemes are learning-based. They are dependent on the training domains, resulting in performance degradation in unseen domains. Researchers have proposed various solutions to address this issue. But these solutions leverage either semi-supervised or unsupervised domain adaptation techniques. They still require some data in the target domains and do not perform well in unseen domains. In this paper, we propose a domain generalization framework DGSense, to eliminate the domain dependence problem in wireless sensing. The framework is a general solution working across diverse sensing tasks and wireless technologies. Once the sensing model is built, it can generalize to unseen domains without any data from the target domain. To achieve the goal, we first increase the diversity of the training set by a virtual data generator, and then extract the domain independent features via episodic training between the main feature extractor and the domain feature extractors. The feature extractors employ a pre-trained Residual Network (ResNet) with an attention mechanism for spatial features, and a 1D Convolutional Neural Network (1DCNN) for temporal features. To demonstrate the effectiveness and generality of DGSense, we evaluated on WiFi gesture recognition, Millimeter Wave (mmWave) activity recognition, and acoustic fall detection. All the systems exhibited high generalization capability to unseen domains, including new users, locations, and environments, free of new data and retraining.', 'abstract_zh': '无线传感极大地改善了我们的日常生活。然而，无线信号对周围环境非常敏感。各种因素，如环境、位置和个体，都可能对无线传播产生额外影响。这种变化可以被视为一个领域，在这个领域中，数据分布发生偏移。大多数传感方案都是基于学习的，依赖于训练领域，导致在未见领域中的性能下降。研究人员提出了各种解决方案来解决这一问题。但这些解决方案依赖于半监督或无监督领域的适应技术，仍然需要一些目标领域的数据，并且在未见领域中的表现不佳。在本文中，我们提出了一种域泛化框架DGSense，以消除无线传感中的域依赖问题。该框架可以适用于多种传感任务和无线技术。一旦构建了传感模型，它可以在没有任何目标领域数据的情况下泛化到未见领域。为了实现这一目标，我们首先通过虚拟数据生成器增加训练集的多样性，然后通过主特征提取器和域特征提取器之间的 episodic 训练提取域独立特征。特征提取器采用带有注意力机制的预训练残差网络（ResNet）提取空间特征，并使用一维卷积神经网络（1DCNN）提取时序特征。为了证明DGSense的有效性和普适性，我们在WiFi手势识别、毫米波（mmWave）活动识别以及声学跌倒检测系统中进行了评估。所有系统都展示了对未见领域的高泛化能力，包括新用户、新地点和新环境，无需新的数据和重新训练。', 'title_zh': 'DGSense: 无线传感的领域泛化框架'}
{'arxiv_id': 'arXiv:2502.08150', 'title': 'Force Matching with Relativistic Constraints: A Physics-Inspired Approach to Stable and Efficient Generative Modeling', 'authors': 'Yang Cao, Bo Chen, Xiaoyu Li, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song, Mingda Wan', 'link': 'https://arxiv.org/abs/2502.08150', 'abstract': 'This paper introduces Force Matching (ForM), a novel framework for generative modeling that represents an initial exploration into leveraging special relativistic mechanics to enhance the stability of the sampling process. By incorporating the Lorentz factor, ForM imposes a velocity constraint, ensuring that sample velocities remain bounded within a constant limit. This constraint serves as a fundamental mechanism for stabilizing the generative dynamics, leading to a more robust and controlled sampling process. We provide a rigorous theoretical analysis demonstrating that the velocity constraint is preserved throughout the sampling procedure within the ForM framework. To validate the effectiveness of our approach, we conduct extensive empirical evaluations. On the \\textit{half-moons} dataset, ForM significantly outperforms baseline methods, achieving the lowest Euclidean distance loss of \\textbf{0.714}, in contrast to vanilla first-order flow matching (5.853) and first- and second-order flow matching (5.793). Additionally, we perform an ablation study to further investigate the impact of our velocity constraint, reaffirming the superiority of ForM in stabilizing the generative process. The theoretical guarantees and empirical results underscore the potential of integrating special relativity principles into generative modeling. Our findings suggest that ForM provides a promising pathway toward achieving stable, efficient, and flexible generative processes. This work lays the foundation for future advancements in high-dimensional generative modeling, opening new avenues for the application of physical principles in machine learning.', 'abstract_zh': '力匹配（Force Matching），一种利用特殊相对论力学增强生成模型采样过程稳定性的新型框架', 'title_zh': '基于相对论约束的力匹配方法：一种物理启发的生成建模稳定高效方法'}
{'arxiv_id': 'arXiv:2502.08122', 'title': 'Hookpad Aria: A Copilot for Songwriters', 'authors': 'Chris Donahue, Shih-Lun Wu, Yewon Kim, Dave Carlton, Ryan Miyakawa, John Thickstun', 'link': 'https://arxiv.org/abs/2502.08122', 'abstract': 'We present Hookpad Aria, a generative AI system designed to assist musicians in writing Western pop songs. Our system is seamlessly integrated into Hookpad, a web-based editor designed for the composition of lead sheets: symbolic music scores that describe melody and harmony. Hookpad Aria has numerous generation capabilities designed to assist users in non-sequential composition workflows, including: (1) generating left-to-right continuations of existing material, (2) filling in missing spans in the middle of existing material, and (3) generating harmony from melody and vice versa. Hookpad Aria is also a scalable data flywheel for music co-creation -- since its release in March 2024, Aria has generated 318k suggestions for 3k users who have accepted 74k into their songs.\nMore information about Hookpad Aria is available at this https URL', 'abstract_zh': 'Hookpad Aria：一个用于创作西方流行歌曲的生成型AI系统', 'title_zh': 'Hookpad Aria：作曲人的副驾助手'}
{'arxiv_id': 'arXiv:2502.08108', 'title': 'Generative AI and Empirical Software Engineering: A Paradigm Shift', 'authors': 'Christoph Treude, Margaret-Anne Storey', 'link': 'https://arxiv.org/abs/2502.08108', 'abstract': 'The widespread adoption of generative AI in software engineering marks a paradigm shift, offering new opportunities to design and utilize software engineering tools while influencing both developers and the artifacts they create. Traditional empirical methods in software engineering, including quantitative, qualitative, and mixed-method approaches, are well established. However, this paradigm shift introduces novel data types and redefines many concepts in the software engineering process. The roles of developers, users, agents, and researchers increasingly overlap, blurring the distinctions between these social and technical actors within the field.\nThis paper examines how integrating AI into software engineering challenges traditional research paradigms. It focuses on the research phenomena that we investigate, the methods and theories that we employ, the data we analyze, and the threats to validity that emerge in this new context. Through this exploration, our goal is to understand how AI adoption disrupts established software development practices that creates new opportunities for empirical software engineering research.', 'abstract_zh': '生成式AI在软件工程中的广泛应用标志着范式的转变，为设计和利用软件工程工具提供了新的机会，同时影响着开发者及其创造的产物。传统的软件工程实证方法，包括定量、定性以及混合方法，已经十分成熟。然而，这种范式的转变引入了新的数据类型，并重新定义了软件工程过程中的许多概念。开发者、用户、代理和研究人员的角色日益重叠，模糊了该领域社会和技术行为者之间的界限。\n\n这篇论文探讨了将AI integrates into软件工程如何挑战传统的研究范式。它关注我们调查的研究现象、采用的方法和理论、分析的数据以及在这种新背景下出现的效度威胁。通过这一探索，我们的目标是理解AI的采用如何扰乱了已有的软件开发实践，从而为实证软件工程研究创造新的机遇。', 'title_zh': '生成式AI与实证软件工程： paradigmhift'}
{'arxiv_id': 'arXiv:2502.08106', 'title': 'PoGDiff: Product-of-Gaussians Diffusion Models for Imbalanced Text-to-Image Generation', 'authors': 'Ziyan Wang, Sizhe Wei, Xiaoming Huo, Hao Wang', 'link': 'https://arxiv.org/abs/2502.08106', 'abstract': 'Diffusion models have made significant advancements in recent years. However, their performance often deteriorates when trained or fine-tuned on imbalanced datasets. This degradation is largely due to the disproportionate representation of majority and minority data in image-text pairs. In this paper, we propose a general fine-tuning approach, dubbed PoGDiff, to address this challenge. Rather than directly minimizing the KL divergence between the predicted and ground-truth distributions, PoGDiff replaces the ground-truth distribution with a Product of Gaussians (PoG), which is constructed by combining the original ground-truth targets with the predicted distribution conditioned on a neighboring text embedding. Experiments on real-world datasets demonstrate that our method effectively addresses the imbalance problem in diffusion models, improving both generation accuracy and quality.', 'abstract_zh': '扩散模型在 recent years 取得了显著进展，但它们在训练或微调于不平衡数据集时性能往往会下降。这种下降主要是由于图像-文本对中主流数据和少数数据的不成比例的表示。本文提出了一种通用的微调方法，称为 PoGDiff，以应对这一挑战。PoGDiff 不是直接最小化预测分布与真实分布的 KL 散度，而是用一个由原始真实目标与基于邻近文本嵌入条件下预测分布组合而成的乘积高斯（PoG）替代真实分布。实验证明，我们的方法有效地解决了扩散模型中的不平衡问题，提高了生成的准确性和质量。', 'title_zh': 'PoGDiff: 均值漂移扩散模型在不平衡文本到图像生成中的应用'}
{'arxiv_id': 'arXiv:2502.08101', 'title': 'Rethinking Tokenized Graph Transformers for Node Classification', 'authors': 'Jinsong Chen, Chenyang Li, GaiChao Li, John E. Hopcroft, Kun He', 'link': 'https://arxiv.org/abs/2502.08101', 'abstract': 'Node tokenized graph Transformers (GTs) have shown promising performance in node classification. The generation of token sequences is the key module in existing tokenized GTs which transforms the input graph into token sequences, facilitating the node representation learning via Transformer. In this paper, we observe that the generations of token sequences in existing GTs only focus on the first-order neighbors on the constructed similarity graphs, which leads to the limited usage of nodes to generate diverse token sequences, further restricting the potential of tokenized GTs for node classification. To this end, we propose a new method termed SwapGT. SwapGT first introduces a novel token swapping operation based on the characteristics of token sequences that fully leverages the semantic relevance of nodes to generate more informative token sequences. Then, SwapGT leverages a Transformer-based backbone to learn node representations from the generated token sequences. Moreover, SwapGT develops a center alignment loss to constrain the representation learning from multiple token sequences, further enhancing the model performance. Extensive empirical results on various datasets showcase the superiority of SwapGT for node classification.', 'abstract_zh': '基于节点切片的图变换器（SwapGT）在节点分类中的应用', 'title_zh': '重新思考token化图变换器在节点分类中的应用'}
{'arxiv_id': 'arXiv:2502.08092', 'title': 'GCoT: Chain-of-Thought Prompt Learning for Graphs', 'authors': 'Xingtong Yu, Chang Zhou, Zhongwei Kuai, Xinming Zhang, Yuan Fang', 'link': 'https://arxiv.org/abs/2502.08092', 'abstract': "Chain-of-thought (CoT) prompting has achieved remarkable success in natural language processing (NLP). However, its vast potential remains largely unexplored for graphs. This raises an interesting question: How can we design CoT prompting for graphs to guide graph models to learn step by step? On one hand, unlike natural languages, graphs are non-linear and characterized by complex topological structures. On the other hand, many graphs lack textual data, making it difficult to formulate language-based CoT prompting. In this work, we propose the first CoT prompt learning framework for text-free graphs, GCoT. Specifically, we decompose the adaptation process for each downstream task into a series of inference steps, with each step consisting of prompt-based inference, ``thought'' generation, and thought-conditioned prompt learning. While the steps mimic CoT prompting in NLP, the exact mechanism differs significantly. Specifically, at each step, an input graph, along with a prompt, is first fed into a pre-trained graph encoder for prompt-based inference. We then aggregate the hidden layers of the encoder to construct a ``thought'', which captures the working state of each node in the current step. Conditioned on this thought, we learn a prompt specific to each node based on the current state. These prompts are fed into the next inference step, repeating the cycle. To evaluate and analyze the effectiveness of GCoT, we conduct comprehensive experiments on eight public datasets, which demonstrate the advantage of our approach.", 'abstract_zh': '无文本图的链式思维提示学习框架：GCoT', 'title_zh': 'GCoT: 图的链式思考提示学习'}
{'arxiv_id': 'arXiv:2502.08021', 'title': 'Model Selection for Off-policy Evaluation: New Algorithms and Experimental Protocol', 'authors': 'Pai Liu, Lingfeng Zhao, Shivangi Agarwal, Jinghan Liu, Audrey Huang, Philip Amortila, Nan Jiang', 'link': 'https://arxiv.org/abs/2502.08021', 'abstract': 'Holdout validation and hyperparameter tuning from data is a long-standing problem in offline reinforcement learning (RL). A standard framework is to use off-policy evaluation (OPE) methods to evaluate and select the policies, but OPE either incurs exponential variance (e.g., importance sampling) or has hyperparameters on their own (e.g., FQE and model-based). In this work we focus on hyperparameter tuning for OPE itself, which is even more under-investigated. Concretely, we select among candidate value functions ("model-free") or dynamics ("model-based") to best assess the performance of a target policy. Our contributions are two fold. We develop: (1) new model-free and model-based selectors with theoretical guarantees, and (2) a new experimental protocol for empirically evaluating them. Compared to the model-free protocol in prior works, our new protocol allows for more stable generation of candidate value functions, better control of misspecification, and evaluation of model-free and model-based methods alike. We exemplify the protocol on a Gym environment, and find that our new model-free selector, LSTD-Tournament, demonstrates promising empirical performance.', 'abstract_zh': '从数据中进行保留集验证和超参数调优是离线强化学习（RL）中的一个长期问题。我们专注于对针对保留集验证自身的超参数进行调优，这甚至更少被研究。具体来说，我们在候选值函数（“无模型”）或动力学（“有模型”）中选择最优者以评估目标策略的表现。我们的贡献包括：（1）开发具有理论保证的新“无模型”和“有模型”选择器，以及（2）开发新的实验协议以实证评估它们。与先前工作的“无模型”协议相比，我们的新协议允许更稳定的候选值函数生成、更好的误指定控制，并能够评估“无模型”和“有模型”方法。我们在Gym环境中展示了该协议的应用，并发现我们的新“无模型”选择器LSTD-Tournament表现出有希望的实证性能。', 'title_zh': '离线政策评估的模型选择：新算法及实验协议'}
{'arxiv_id': 'arXiv:2502.08006', 'title': 'Greed is Good: Guided Generation from a Greedy Perspective', 'authors': 'Zander W. Blasingame, Chen Liu', 'link': 'https://arxiv.org/abs/2502.08006', 'abstract': 'Training-free guided generation is a widely used and powerful technique that allows the end user to exert further control over the generative process of diffusion models. In this work, we explore the guided generation from the perspective of optimizing the solution trajectory of a neural differential equation in a greedy manner. We present such a strategy as a unifying view on training-free guidance by showing that the greedy strategy is a first-order discretization of end-to-end optimization techniques. We show that a greedy guidance strategy makes good decisions and compare it to a guidance strategy using the ideal gradients found via the continuous adjoint equations. We then show how other popular training-free guidance strategies can be viewed in a unified manner from this perspective.', 'abstract_zh': '无训练引导生成是一种广泛使用且强大的技术，允许最终用户进一步控制扩散模型的生成过程。在本文中，我们从神经微分方程解轨迹的贪心优化视角探索引导生成。我们通过展示贪心策略是端到端优化技术的一阶离散化来阐述这种策略作为一个无训练引导的统一视图。我们展示了贪心引导策略能做出良好的决策，并将其与通过连续伴随方程找到的理想梯度来实现的引导策略进行了比较。然后，我们展示了从这一视角来看，其他流行的无训练引导策略可以统一地被理解。', 'title_zh': '贪婪亦可取：从贪婪视角指导生成'}
{'arxiv_id': 'arXiv:2502.07972', 'title': 'Training Sparse Mixture Of Experts Text Embedding Models', 'authors': 'Zach Nussbaum, Brandon Duderstadt', 'link': 'https://arxiv.org/abs/2502.07972', 'abstract': "Transformer-based text embedding models have improved their performance on benchmarks like MIRACL and BEIR by increasing their parameter counts. However, this scaling approach introduces significant deployment challenges, including increased inference latency and memory usage. These challenges are particularly severe in retrieval-augmented generation (RAG) applications, where large models' increased memory requirements constrain dataset ingestion capacity, and their higher latency directly impacts query-time performance. While causal language models have addressed similar efficiency challenges using Mixture of Experts (MoE) architectures, this approach hasn't been successfully adapted to the general text embedding setting. In this paper, we introduce Nomic Embed v2, the first general purpose MoE text embedding model. Our model outperforms models in the same parameter class on both monolingual and multilingual benchmarks while also maintaining competitive performance with models twice its size. We open-source all code, models, and evaluation data to ensure full reproducibility of our training pipeline.", 'abstract_zh': '基于Transformer的文本嵌入模型通过增加参数量在MIRACL和BEIR等基准测试上的性能有所提升，但这种扩展方法引入了显著的部署挑战，包括推理延迟增加和内存使用量增加。这些挑战在检索增强生成（RAG）应用中尤为严重，其中大型模型增加的内存需求限制了数据集的摄入能力，而更高的延迟直接影响查询时的性能。虽然因果语言模型通过专家混合（MoE）架构解决了类似的有效性挑战，但这种方法尚未成功应用于通用文本嵌入设置中。在本文中，我们引入了Nomic Embed v2，这是首个通用目的MoE文本嵌入模型。我们的模型在单调语言和多语言基准测试中均表现出色，同时与两倍参数量的模型保持竞争力。我们开源了所有代码、模型和评估数据，以确保训练流程的完全可再现性。', 'title_zh': '训练稀疏专家混合文本嵌入模型'}
{'arxiv_id': 'arXiv:2502.07971', 'title': 'ReTreever: Tree-based Coarse-to-Fine Representations for Retrieval', 'authors': 'Shubham Gupta, Zichao Li, Tianyi Chen, Cem Subakan, Siva Reddy, Perouz Taslakian, Valentina Zantedeschi', 'link': 'https://arxiv.org/abs/2502.07971', 'abstract': 'Document retrieval is a core component of question-answering systems, as it enables conditioning answer generation on new and large-scale corpora. While effective, the standard practice of encoding documents into high-dimensional embeddings for similarity search entails large memory and compute footprints, and also makes it hard to inspect the inner workings of the system. In this paper, we propose a tree-based method for organizing and representing reference documents at various granular levels, which offers the flexibility to balance cost and utility, and eases the inspection of the corpus content and retrieval operations. Our method, called ReTreever, jointly learns a routing function per internal node of a binary tree such that query and reference documents are assigned to similar tree branches, hence directly optimizing for retrieval performance. Our evaluations show that ReTreever generally preserves full representation accuracy. Its hierarchical structure further provides strong coarse representations and enhances transparency by indirectly learning meaningful semantic groupings. Among hierarchical retrieval methods, ReTreever achieves the best retrieval accuracy at the lowest latency, proving that this family of techniques can be viable in practical applications.', 'abstract_zh': '基于树的方法在文档检索中的应用：ReTreever提高了检索性能并增强了透明度', 'title_zh': 'ReTreever: 基于树的粗细粒度表示检索'}
{'arxiv_id': 'arXiv:2502.07968', 'title': 'Generative Risk Minimization for Out-of-Distribution Generalization on Graphs', 'authors': 'Song Wang, Zhen Tan, Yaochen Zhu, Chuxu Zhang, Jundong Li', 'link': 'https://arxiv.org/abs/2502.07968', 'abstract': 'Out-of-distribution (OOD) generalization on graphs aims at dealing with scenarios where the test graph distribution differs from the training graph distributions. Compared to i.i.d. data like images, the OOD generalization problem on graph-structured data remains challenging due to the non-i.i.d. property and complex structural information on graphs. Recently, several works on graph OOD generalization have explored extracting invariant subgraphs that share crucial classification information across different distributions. Nevertheless, such a strategy could be suboptimal for entirely capturing the invariant information, as the extraction of discrete structures could potentially lead to the loss of invariant information or the involvement of spurious information. In this paper, we propose an innovative framework, named Generative Risk Minimization (GRM), designed to generate an invariant subgraph for each input graph to be classified, instead of extraction. To address the challenge of optimization in the absence of optimal invariant subgraphs (i.e., ground truths), we derive a tractable form of the proposed GRM objective by introducing a latent causal variable, and its effectiveness is validated by our theoretical analysis. We further conduct extensive experiments across a variety of real-world graph datasets for both node-level and graph-level OOD generalization, and the results demonstrate the superiority of our framework GRM.', 'abstract_zh': '图结构数据离分布外泛化的生成风险最小化方法', 'title_zh': '图中离分布泛化的生成风险最小化'}
{'arxiv_id': 'arXiv:2502.07943', 'title': 'CREDAL: Close Reading of Data Models', 'authors': 'George Fletcher, Olha Nahurna, Matvii Prytula, Julia Stoyanovich', 'link': 'https://arxiv.org/abs/2502.07943', 'abstract': 'Data models are necessary for the birth of data and of any data-driven system. Indeed, every algorithm, every machine learning model, every statistical model, and every database has an underlying data model without which the system would not be usable. Hence, data models are excellent sites for interrogating the (material, social, political, ...) conditions giving rise to a data system. Towards this, drawing inspiration from literary criticism, we propose to closely read data models in the same spirit as we closely read literary artifacts. Close readings of data models reconnect us with, among other things, the materiality, the genealogies, the techne, the closed nature, and the design of technical systems.\nWhile recognizing from literary theory that there is no one correct way to read, it is nonetheless critical to have systematic guidance for those unfamiliar with close readings. This is especially true for those trained in the computing and data sciences, who too often are enculturated to set aside the socio-political aspects of data work. A systematic methodology for reading data models currently does not exist. To fill this gap, we present the CREDAL methodology for close readings of data models. We detail our iterative development process and present results of a qualitative evaluation of CREDAL demonstrating its usability, usefulness, and effectiveness in the critical study of data.', 'abstract_zh': '数据模型是数据及其任何数据驱动系统的诞生基础。事实上，每个算法、每种机器学习模型、每种统计模型和每个数据库都基于一个底层的数据模型，缺乏这一模型系统将无法使用。因此，数据模型是探究（物质的、社会的、政治的……）产生数据系统的条件的极佳场所。借鉴文学批评的方法，我们建议以同样的精神仔细解读数据模型。这种对数据模型的仔细解读使我们重新聚焦于其物质性、源流、技术技艺、封闭性质以及技术系统的构建。\n\n尽管从文学理论中认识到没有一种阅读方式是完全正确的，但为初学者提供系统的指导依然至关重要。特别是对于那些训练背景在计算机和数据科学领域的人来说，他们往往被培养成忽视数据工作中的社会政治方面。当前尚不存在系统的方法来解读数据模型。为填补这一空白，我们提出了CREDAL方法论，用于仔细解读数据模型。我们详细描述了CREDAL方法论的迭代开发过程，并呈现了对CREDAL的定性评估结果，展示了其在批判性研究数据方面的适用性、有用性和有效性。', 'title_zh': 'CREDAL: 数据模型的细致解读'}
{'arxiv_id': 'arXiv:2502.07924', 'title': 'NDAI Agreements', 'authors': 'Matthew Stephenson, Andrew Miller, Xyn Sun, Bhargav Annem, Rohan Parikh', 'link': 'https://arxiv.org/abs/2502.07924', 'abstract': 'We study a fundamental challenge in the economics of innovation: an inventor must reveal details of a new idea to secure compensation or funding, yet such disclosure risks expropriation. We present a model in which a seller (inventor) and buyer (investor) bargain over an information good under the threat of hold-up. In the classical setting, the seller withholds disclosure to avoid misappropriation, leading to inefficiency. We show that trusted execution environments (TEEs) combined with AI agents can mitigate and even fully eliminate this hold-up problem. By delegating the disclosure and payment decisions to tamper-proof programs, the seller can safely reveal the invention without risking expropriation, achieving full disclosure and an efficient ex post transfer. Moreover, even if the invention\'s value exceeds a threshold that TEEs can fully secure, partial disclosure still improves outcomes compared to no disclosure. Recognizing that real AI agents are imperfect, we model "agent errors" in payments or disclosures and demonstrate that budget caps and acceptance thresholds suffice to preserve most of the efficiency gains.\nOur results imply that cryptographic or hardware-based solutions can function as an "ironclad NDA," substantially mitigating the fundamental disclosure-appropriation paradox first identified by Arrow (1962) and Nelson (1959). This has far-reaching policy implications for fostering R&D, technology transfer, and collaboration.', 'abstract_zh': '我们研究创新经济学中的一个基本挑战：发明者必须披露新想法的细节以获得补偿或融资，但这种披露又面临被剥夺的风险。在经典的讨价还价框架中，卖方（发明者）会出于避免滥用的目的而保留披露，导致效率低下。我们展示了一种结合可信执行环境（TEE）和AI代理的模型，可以缓解甚至完全消除这种剥夺问题。通过将披露和支付决策委托给防篡改程序，卖方可以在不冒被剥夺风险的情况下安全地披露发明，实现完全披露和事后高效转移。即使发明的价值超过TEE可以完全保障的门槛，部分披露依然优于完全不披露。鉴于实际的AI代理可能存在缺陷，我们考虑了支付或披露中的“代理错误”，并证明预算限制和接受阈值足以保留大部分效率提升。我们的结果表明，加密或基于硬件的解决方案可以充当“坚不可摧的保密协议”，大幅缓解Arrow（1962）和Nelson（1959）首次识别的基本披露-滥用悖论。这对促进R&D、技术转移和合作具有深远的政策意义。', 'title_zh': 'NDAI协议'}
{'arxiv_id': 'arXiv:2502.07857', 'title': 'SNAP: Sequential Non-Ancestor Pruning for Targeted Causal Effect Estimation With an Unknown Graph', 'authors': 'Mátyás Schubert, Tom Claassen, Sara Magliacane', 'link': 'https://arxiv.org/abs/2502.07857', 'abstract': 'Causal discovery can be computationally demanding for large numbers of variables. If we only wish to estimate the causal effects on a small subset of target variables, we might not need to learn the causal graph for all variables, but only a small subgraph that includes the targets and their adjustment sets. In this paper, we focus on identifying causal effects between target variables in a computationally and statistically efficient way. This task combines causal discovery and effect estimation, aligning the discovery objective with the effects to be estimated. We show that definite non-ancestors of the targets are unnecessary to learn causal relations between the targets and to identify efficient adjustments sets. We sequentially identify and prune these definite non-ancestors with our Sequential Non-Ancestor Pruning (SNAP) framework, which can be used either as a preprocessing step to standard causal discovery methods, or as a standalone sound and complete causal discovery algorithm. Our results on synthetic and real data show that both approaches substantially reduce the number of independence tests and the computation time without compromising the quality of causal effect estimations.', 'abstract_zh': '因果发现对于大量变量可能是计算上 demanding 的。如果我们只想估计目标变量子集的因果效应，我们可能不需要学习所有变量的因果图，而只需要包括目标变量及其调整集的较小子图。本文旨在以计算和统计效率的方式识别目标变量之间的因果效应。该任务结合了因果发现和效应估计，将发现目标与效果估计对齐。我们表明，目标的确定非祖先对于识别目标间的因果关系及其有效的调整集是不必要的。我们通过顺序非祖先剪枝（SNAP）框架识别并移除这些确定非祖先，该框架可以作为标准因果发现方法的预处理步骤，或作为独立的完整因果发现算法。我们在合成和真实数据上的结果表明，两种方法都能大幅减少独立性检验的数量和计算时间，而不牺牲因果效应估计的质量。', 'title_zh': 'SNAP: 序列非祖先修剪以估计未知图下的靶向因果效应'}
{'arxiv_id': 'arXiv:2502.07856', 'title': 'MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers', 'authors': 'Ao Li, Wei Fang, Hongbo Zhao, Le Lu, Ge Yang, Minfeng Xu', 'link': 'https://arxiv.org/abs/2502.07856', 'abstract': 'In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation.', 'abstract_zh': 'MR Sampler: Reducing Sampling NFEs for MR Diffusion in Controllable Generation', 'title_zh': 'MRS：基于常微分方程和随机微分方程求解器的快速均值回复扩散抽样器'}
{'arxiv_id': 'arXiv:2502.07849', 'title': 'Understanding Classifier-Free Guidance: High-Dimensional Theory and Non-Linear Generalizations', 'authors': 'Krunoslav Lehman Pavasovic, Jakob Verbeek, Giulio Biroli, Marc Mezard', 'link': 'https://arxiv.org/abs/2502.07849', 'abstract': 'Recent studies have raised concerns about the effectiveness of Classifier-Free Guidance (CFG), indicating that in low-dimensional settings, it can lead to overshooting the target distribution and reducing sample diversity. In this work, we demonstrate that in infinite and sufficiently high-dimensional contexts CFG effectively reproduces the target distribution, revealing a blessing-of-dimensionality result. Additionally, we explore finite-dimensional effects, precisely characterizing overshoot and variance reduction. Based on our analysis, we introduce non-linear generalizations of CFG. Through numerical simulations on Gaussian mixtures and experiments on class-conditional and text-to-image diffusion models, we validate our analysis and show that our non-linear CFG offers improved flexibility and generation quality without additional computation cost.', 'abstract_zh': '近年来的研究对Classifier-Free Guidance (CFG)的有效性提出了担忧，表明在低维设置中，它可能导致过度拟合目标分布并降低样本多样性。在本工作中，我们展示了在无穷维和充分高维的背景下，CFG能够有效地再现目标分布，揭示了维度优势的结果。此外，我们探讨了有限维度的影响，精确刻画了过度拟合和方差减少。基于我们的分析，我们提出了非线性化的CFG一般化方法。通过高斯混合模型的数值模拟以及类条件和文本到图像扩散模型的实验，我们验证了我们的分析，并展示了我们的非线性CFG能够在不增加计算成本的情况下提供更好的灵活性和生成质量。', 'title_zh': '无分类器指导的理解：高维理论与非线性推广'}
{'arxiv_id': 'arXiv:2502.07845', 'title': 'Spread them Apart: Towards Robust Watermarking of Generated Content', 'authors': 'Mikhail Pautov, Danil Ivanov, Andrey V. Galichin, Oleg Rogov, Ivan Oseledets', 'link': 'https://arxiv.org/abs/2502.07845', 'abstract': 'Generative models that can produce realistic images have improved significantly in recent years. The quality of the generated content has increased drastically, so sometimes it is very difficult to distinguish between the real images and the generated ones. Such an improvement comes at a price of ethical concerns about the usage of the generative models: the users of generative models can improperly claim ownership of the generated content protected by a license. In this paper, we propose an approach to embed watermarks into the generated content to allow future detection of the generated content and identification of the user who generated it. The watermark is embedded during the inference of the model, so the proposed approach does not require the retraining of the latter. We prove that watermarks embedded are guaranteed to be robust against additive perturbations of a bounded magnitude. We apply our method to watermark diffusion models and show that it matches state-of-the-art watermarking schemes in terms of robustness to different types of synthetic watermark removal attacks.', 'abstract_zh': '生成模型在 recent years 中已大幅提高生成逼真图像的能力。生成内容的质量大幅提升，有时很难区分真实的图像和生成的图像。这种进步伴随着生成模型使用中伦理问题的担忧：生成模型的使用者可能会不当声称受版权保护的生成内容的所有权。本文提出了一种将水印嵌入生成内容的方法，以允许未来检测生成的内容并识别生成内容的用户。水印在模型推理过程中嵌入，因此不需要重新训练模型。我们证明嵌入的水印对幅度有限的附加扰动具有鲁棒性。我们将该方法应用于水印扩散模型，并显示其在不同类型的合成水印去除攻击下的鲁棒性与现有最佳水印方案相当。', 'title_zh': '分散开来：面向生成内容的稳健水印技术探索'}
{'arxiv_id': 'arXiv:2502.07842', 'title': 'Column-wise Quantization of Weights and Partial Sums for Accurate and Efficient Compute-In-Memory Accelerators', 'authors': 'Jiyoon Kim, Kang Eun Jeon, Yulhwa Kim, Jong Hwan Ko', 'link': 'https://arxiv.org/abs/2502.07842', 'abstract': 'Compute-in-memory (CIM) is an efficient method for implementing deep neural networks (DNNs) but suffers from substantial overhead from analog-to-digital converters (ADCs), especially as ADC precision increases. Low-precision ADCs can re- duce this overhead but introduce partial-sum quantization errors degrading accuracy. Additionally, low-bit weight constraints, im- posed by cell limitations and the need for multiple cells for higher- bit weights, present further challenges. While fine-grained partial- sum quantization has been studied to lower ADC resolution effectively, weight granularity, which limits overall partial-sum quantized accuracy, remains underexplored. This work addresses these challenges by aligning weight and partial-sum quantization granularities at the column-wise level. Our method improves accuracy while maintaining dequantization overhead, simplifies training by removing two-stage processes, and ensures robustness to memory cell variations via independent column-wise scale factors. We also propose an open-source CIM-oriented convolution framework to handle fine-grained weights and partial-sums effi- ciently, incorporating a novel tiling method and group convolution. Experimental results on ResNet-20 (CIFAR-10, CIFAR-100) and ResNet-18 (ImageNet) show accuracy improvements of 0.99%, 2.69%, and 1.01%, respectively, compared to the best-performing related works. Additionally, variation analysis reveals the robust- ness of our method against memory cell variations. These findings highlight the effectiveness of our quantization scheme in enhancing accuracy and robustness while maintaining hardware efficiency in CIM-based DNN implementations. Our code is available at this https URL.', 'abstract_zh': '基于计算存储器的深度神经网络低精度权重和部分和量化方法', 'title_zh': '列方向权重和部分和的量化解码以实现准确高效的计算在内存加速器'}
{'arxiv_id': 'arXiv:2502.07834', 'title': 'MEMHD: Memory-Efficient Multi-Centroid Hyperdimensional Computing for Fully-Utilized In-Memory Computing Architectures', 'authors': 'Do Yeong Kang, Yeong Hwan Oh, Chanwook Hwang, Jinhee Kim, Kang Eun Jeon, Jong Hwan Ko', 'link': 'https://arxiv.org/abs/2502.07834', 'abstract': 'The implementation of Hyperdimensional Computing (HDC) on In-Memory Computing (IMC) architectures faces significant challenges due to the mismatch between highdimensional vectors and IMC array sizes, leading to inefficient memory utilization and increased computation cycles. This paper presents MEMHD, a Memory-Efficient Multi-centroid HDC framework designed to address these challenges. MEMHD introduces a clustering-based initialization method and quantization aware iterative learning for multi-centroid associative memory. Through these approaches and its overall architecture, MEMHD achieves a significant reduction in memory requirements while maintaining or improving classification accuracy. Our approach achieves full utilization of IMC arrays and enables one-shot (or few-shot) associative search. Experimental results demonstrate that MEMHD outperforms state-of-the-art binary HDC models, achieving up to 13.69% higher accuracy with the same memory usage, or 13.25x more memory efficiency at the same accuracy level. Moreover, MEMHD reduces computation cycles by up to 80x and array usage by up to 71x compared to baseline IMC mapping methods when mapped to 128x128 IMC arrays, while significantly improving energy and computation cycle efficiency.', 'abstract_zh': 'Memory-Efficient Multi-centroid Hyperdimensional Computing Framework for In-Memory Computing Architectures', 'title_zh': 'MEMHD: 高效内存多中心超维计算以充分利用内存计算架构'}
{'arxiv_id': 'arXiv:2502.07830', 'title': 'Captured by Captions: On Memorization and its Mitigation in CLIP Models', 'authors': 'Wenhao Wang, Adam Dziedzic, Grace C. Kim, Michael Backes, Franziska Boenisch', 'link': 'https://arxiv.org/abs/2502.07830', 'abstract': 'Multi-modal models, such as CLIP, have demonstrated strong performance in aligning visual and textual representations, excelling in tasks like image retrieval and zero-shot classification. Despite this success, the mechanisms by which these models utilize training data, particularly the role of memorization, remain unclear. In uni-modal models, both supervised and self-supervised, memorization has been shown to be essential for generalization. However, it is not well understood how these findings would apply to CLIP, which incorporates elements from both supervised learning via captions that provide a supervisory signal similar to labels, and from self-supervised learning via the contrastive objective. To bridge this gap in understanding, we propose a formal definition of memorization in CLIP (CLIPMem) and use it to quantify memorization in CLIP models. Our results indicate that CLIP\'s memorization behavior falls between the supervised and self-supervised paradigms, with "mis-captioned" samples exhibiting highest levels of memorization. Additionally, we find that the text encoder contributes more to memorization than the image encoder, suggesting that mitigation strategies should focus on the text domain. Building on these insights, we propose multiple strategies to reduce memorization while at the same time improving utility--something that had not been shown before for traditional learning paradigms where reducing memorization typically results in utility decrease.', 'abstract_zh': 'CLIP模型中的记忆机制正式定义及其量化', 'title_zh': '被描述所困：关于CLIP模型中记忆化及其缓解方法的研究'}
{'arxiv_id': 'arXiv:2502.07823', 'title': 'Runtime Tunable Tsetlin Machines for Edge Inference on eFPGAs', 'authors': 'Tousif Rahman, Gang Mao, Bob Pattison, Sidharth Maheshwari, Marcos Sartori, Adrian Wheeldon, Rishad Shafik, Alex Yakovlev', 'link': 'https://arxiv.org/abs/2502.07823', 'abstract': 'Embedded Field-Programmable Gate Arrays (eFPGAs) allow for the design of hardware accelerators of edge Machine Learning (ML) applications at a lower power budget compared with traditional FPGA platforms. However, the limited eFPGA logic and memory significantly constrain compute capabilities and model size. As such, ML application deployment on eFPGAs is in direct contrast with the most recent FPGA approaches developing architecture-specific implementations and maximizing throughput over resource frugality. This paper focuses on the opposite side of this trade-off: the proposed eFPGA accelerator focuses on minimizing resource usage and allowing flexibility for on-field recalibration over throughput. This allows for runtime changes in model size, architecture, and input data dimensionality without offline resynthesis. This is made possible through the use of a bitwise compressed inference architecture of the Tsetlin Machine (TM) algorithm. TM compute does not require any multiplication operations, being limited to only bitwise AND, OR, NOT, summations and additions. Additionally, TM model compression allows the entire model to fit within the on-chip block RAM of the eFPGA. The paper uses this accelerator to propose a strategy for runtime model tuning in the field. The proposed approach uses 2.5x fewer Look-up-Tables (LUTs) and 3.38x fewer registers than the current most resource-fugal design and achieves up to 129x energy reduction compared with low-power microcontrollers running the same ML application.', 'abstract_zh': '嵌入式可编程门阵列（eFPGAs）允许以较低的能耗设计边缘机器学习（ML）应用的硬件加速器，相比传统的FPGA平台具有优势。然而，有限的eFPGA逻辑和内存显著限制了计算能力和模型规模。因此，eFPGA上的ML应用部署与最新的FPGA方法形成了直接对比，后者旨在通过特定架构实现最大吞吐量，而不是资源节约。本文关注这一权衡的另一面：提出的eFPGA加速器旨在最小化资源使用并允许在现场重新校准以灵活性超过吞吐量。这使得可以在运行时改变模型大小、架构和输入数据维度而无需离线重新综合成为可能。这通过使用Tsetlin机器（TM）算法的位级压缩推理架构得以实现。TM计算仅限于位级AND、OR、NOT、求和和加法操作，无需任何乘法操作。此外，TM模型压缩使整个模型能够fits入eFPGA片上块RAM中。本文利用该加速器提出了现场模型调整的策略。所提出的方法在查找表（LUTs）和寄存器的使用上分别比当前最资源节约的设计少2.5倍和3.38倍，并且与低功耗微控制器上运行相同ML应用相比，能耗最多可降低129倍。', 'title_zh': '用于eFPGAs的运行时可调Tsetlin机器的边缘推理'}
{'arxiv_id': 'arXiv:2502.07820', 'title': 'Low-Rank Compression for IMC Arrays', 'authors': 'Kang Eun Jeon, Johnny Rhe, Jong Hwan Ko', 'link': 'https://arxiv.org/abs/2502.07820', 'abstract': 'In this study, we address the challenge of low-rank model compression in the context of in-memory computing (IMC) architectures. Traditional pruning approaches, while effective in model size reduction, necessitate additional peripheral circuitry to manage complex dataflows and mitigate dislocation issues, leading to increased area and energy overheads. To circumvent these drawbacks, we propose leveraging low-rank compression techniques, which, unlike pruning, streamline the dataflow and seamlessly integrate with IMC architectures. However, low-rank compression presents its own set of challenges, namely i) suboptimal IMC array utilization and ii) compromised accuracy. To address these issues, we introduce a novel approach i) employing shift and duplicate kernel (SDK) mapping technique, which exploits idle IMC columns for parallel processing, and ii) group low-rank convolution, which mitigates the information imbalance in the decomposed matrices. Our experimental results demonstrate that our proposed method achieves up to 2.5x speedup or +20.9% accuracy boost over existing pruning techniques.', 'abstract_zh': '本研究在内存计算架构（IMC）背景下解决低秩模型压缩的挑战。', 'title_zh': 'IMC阵列的低秩压缩'}
{'arxiv_id': 'arXiv:2502.07815', 'title': 'Decoding Complexity: Intelligent Pattern Exploration with CHPDA (Context Aware Hybrid Pattern Detection Algorithm)', 'authors': 'Lokesh Koli, Shubham Kalra, Karanpreet Singh', 'link': 'https://arxiv.org/abs/2502.07815', 'abstract': 'Detecting sensitive data such as Personally Identifiable Information (PII) and Protected Health Information (PHI) is critical for data security platforms. This study evaluates regex-based pattern matching algorithms and exact-match search techniques to optimize detection speed, accuracy, and scalability. Our benchmarking results indicate that Google RE2 provides the best balance of speed (10-15 ms/MB), memory efficiency (8-16 MB), and accuracy (99.5%) among regex engines, outperforming PCRE while maintaining broader hardware compatibility than Hyperscan. For exact matching, Aho-Corasick demonstrated superior performance (8 ms/MB) and scalability for large datasets. Performance analysis revealed that regex processing time scales linearly with dataset size and pattern complexity. A hybrid AI + Regex approach achieved the highest F1 score (91. 6%) by improving recall and minimizing false positives. Device benchmarking confirmed that our solution maintains efficient CPU and memory usage on both high-performance and mid-range systems. Despite its effectiveness, challenges remain, such as limited multilingual support and the need for regular pattern updates. Future work should focus on expanding language coverage, integrating data security and privacy management (DSPM) with data loss prevention (DLP) tools, and enhancing regulatory compliance for broader global adoption.', 'abstract_zh': '检测个人可识别信息（PII）和受保护的健康信息（PHI）对于数据安全平台至关重要。本研究评估了基于正则表达式的模式匹配算法和精确匹配搜索技术，以优化检测速度、准确性和可扩展性。基准测试结果显示，Google RE2在速度（每兆字节10-15毫秒）、内存效率（8-16兆字节）和准确性（99.5%）方面提供了最优平衡，优于PCRE并在硬件兼容性上优于Hyperscan。对于精确匹配，Aho-Corasick在大容量数据集上表现出更优的性能（每兆字节8毫秒）和可扩展性。性能分析表明，正则表达式处理时间随着数据集大小和模式复杂性的线性增长。通过结合人工智能与正则表达式的混合方法实现了最高的F1分数（91.6%），通过提高召回率和减少假阳性来实现。设备基准测试证实，我们的解决方案能够在高性能和中端系统上维持高效的CPU和内存使用。尽管取得了有效成果，仍存在限制多语言支持和定期更新模式等方面的挑战。未来的工作应侧重于扩展语言覆盖面，将数据安全和隐私管理（DSPM）与数据丢失防护（DLP）工具集成，并增强国际合规性以实现更广泛的全球应用。', 'title_zh': '解码复杂性：基于CHPDA（情境aware混合模式检测算法）的智能模式探索'}
{'arxiv_id': 'arXiv:2502.07814', 'title': 'Satellite Observations Guided Diffusion Model for Accurate Meteorological States at Arbitrary Resolution', 'authors': 'Siwei Tu, Ben Fei, Weidong Yang, Fenghua Ling, Hao Chen, Zili Liu, Kun Chen, Hang Fan, Wanli Ouyang, Lei Bai', 'link': 'https://arxiv.org/abs/2502.07814', 'abstract': 'Accurate acquisition of surface meteorological conditions at arbitrary locations holds significant importance for weather forecasting and climate simulation. Due to the fact that meteorological states derived from satellite observations are often provided in the form of low-resolution grid fields, the direct application of spatial interpolation to obtain meteorological states for specific locations often results in significant discrepancies when compared to actual observations. Existing downscaling methods for acquiring meteorological state information at higher resolutions commonly overlook the correlation with satellite observations. To bridge the gap, we propose Satellite-observations Guided Diffusion Model (SGD), a conditional diffusion model pre-trained on ERA5 reanalysis data with satellite observations (GridSat) as conditions, which is employed for sampling downscaled meteorological states through a zero-shot guided sampling strategy and patch-based methods. During the training process, we propose to fuse the information from GridSat satellite observations into ERA5 maps via the attention mechanism, enabling SGD to generate atmospheric states that align more accurately with actual conditions. In the sampling, we employed optimizable convolutional kernels to simulate the upscale process, thereby generating high-resolution ERA5 maps using low-resolution ERA5 maps as well as observations from weather stations as guidance. Moreover, our devised patch-based method promotes SGD to generate meteorological states at arbitrary resolutions. Experiments demonstrate SGD fulfills accurate meteorological states downscaling to 6.25km.', 'abstract_zh': '基于卫星观测指导的扩散模型：从ERA5再分析数据中获取高分辨率气象状态', 'title_zh': '基于卫星观测引导的扩散模型高精度任意分辨率气象状态估算'}
{'arxiv_id': 'arXiv:2502.07807', 'title': 'CP-Guard+: A New Paradigm for Malicious Agent Detection and Defense in Collaborative Perception', 'authors': 'Senkang Hu, Yihang Tao, Zihan Fang, Guowen Xu, Yiqin Deng, Sam Kwong, Yuguang Fang', 'link': 'https://arxiv.org/abs/2502.07807', 'abstract': 'Collaborative perception (CP) is a promising method for safe connected and autonomous driving, which enables multiple vehicles to share sensing information to enhance perception performance. However, compared with single-vehicle perception, the openness of a CP system makes it more vulnerable to malicious attacks that can inject malicious information to mislead the perception of an ego vehicle, resulting in severe risks for safe driving. To mitigate such vulnerability, we first propose a new paradigm for malicious agent detection that effectively identifies malicious agents at the feature level without requiring verification of final perception results, significantly reducing computational overhead. Building on this paradigm, we introduce CP-GuardBench, the first comprehensive dataset provided to train and evaluate various malicious agent detection methods for CP systems. Furthermore, we develop a robust defense method called CP-Guard+, which enhances the margin between the representations of benign and malicious features through a carefully designed Dual-Centered Contrastive Loss (DCCLoss). Finally, we conduct extensive experiments on both CP-GuardBench and V2X-Sim, and demonstrate the superiority of CP-Guard+.', 'abstract_zh': '协作感知(CP)是一种有潜力的安全自动驾驶方法，能够使多辆车辆共享感知信息以提升感知性能。然而，与单 vehicle 感知相比，CP 系统的开放性使其更容易受到恶意攻击，这些攻击可以注入恶意信息误导自主车辆的感知，从而带来严重的安全风险。为了减轻这种脆弱性，我们首先提出了一种新的恶意代理检测范式，能够在特征层面有效地识别恶意代理，无需验证最终的感知结果，从而显著减少计算开销。在此基础上，我们引入了 CP-GuardBench，这是第一个用于训练和评估各种恶意代理检测方法的全面数据集。此外，我们开发了一种 robust 的防御方法 CP-Guard+，通过精心设计的双中心对比损失（DCCLoss）增强良性特征和恶意特征的表示之间的差距。最后，我们在 CP-GuardBench 和 V2X-Sim 上进行了广泛的实验，并展示了 CP-Guard+ 的优越性。', 'title_zh': 'CP-Guard+: 一种协作感知中恶意代理检测与防御的新范式'}
{'arxiv_id': 'arXiv:2502.07806', 'title': 'Quantum Powered Credit Risk Assessment: A Novel Approach using hybrid Quantum-Classical Deep Neural Network for Row-Type Dependent Predictive Analysis', 'authors': 'Rath Minati, Date Hema', 'link': 'https://arxiv.org/abs/2502.07806', 'abstract': 'The integration of Quantum Deep Learning (QDL) techniques into the landscape of financial risk analysis presents a promising avenue for innovation. This study introduces a framework for credit risk assessment in the banking sector, combining quantum deep learning techniques with adaptive modeling for Row-Type Dependent Predictive Analysis (RTDPA). By leveraging RTDPA, the proposed approach tailors predictive models to different loan categories, aiming to enhance the accuracy and efficiency of credit risk evaluation. While this work explores the potential of integrating quantum methods with classical deep learning for risk assessment, it focuses on the feasibility and performance of this hybrid framework rather than claiming transformative industry-wide impacts. The findings offer insights into how quantum techniques can complement traditional financial analysis, paving the way for further advancements in predictive modeling for credit risk.', 'abstract_zh': '量子深度学习技术在金融风险分析领域的整合为创新提供了前景。本文介绍了一种结合量子深度学习技术和自适应建模的信用风险评估框架，以行依赖预测分析（RTDPA）为基础。通过利用RTDPA，所提出的方法针对不同的贷款类别定制预测模型，旨在提高信用风险评估的准确性和效率。虽然本研究探索了将量子方法与经典深度学习结合进行风险评估的可能性，但它主要关注这种混合框架的可行性和性能，而非宣称具有行业颠覆性的影响。研究结果为量子技术如何补充传统金融分析提供了洞见，并为信用风险预测建模的进一步进展铺平了道路。', 'title_zh': '量子驱动的信用风险评估：一种基于混合量子-经典深度神经网络的行依赖预测分析新方法'}
{'arxiv_id': 'arXiv:2502.07789', 'title': 'Do AI assistants help students write formal specifications? A study with ChatGPT and the B-Method', 'authors': 'Alfredo Capozucca, Daniil Yampolskyi, Alexander Goldberg, Maximiliano Cristiá', 'link': 'https://arxiv.org/abs/2502.07789', 'abstract': "This paper investigates the role of AI assistants, specifically OpenAI's ChatGPT, in teaching formal methods (FM) to undergraduate students, using the B-method as a formal specification technique. While existing studies demonstrate the effectiveness of AI in coding tasks, no study reports on its impact on formal specifications. We examine whether ChatGPT provides an advantage when writing B-specifications and analyse student trust in its outputs. Our findings indicate that the AI does not help students to enhance the correctness of their specifications, with low trust correlating to better outcomes. Additionally, we identify a behavioural pattern with which to interact with ChatGPT which may influence the correctness of B-specifications.", 'abstract_zh': '本研究 investigate 了 AI 辅助工具，特别是 OpenAI 的 ChatGPT，在教学正式方法（FM）给本科生时的作用，使用 B 方法作为形式化规范技术。尽管现有研究证明了 AI 在编程任务中的有效性，但尚未有研究报道其对形式化规范的影响。我们探讨了 ChatGPT 在编写 B 规范时是否提供优势，并分析了学生对其输出的信任度。研究结果表明，AI 并未帮助学生提高规范的正确性，低信任度与更好的结果相关。此外，我们还识别出一种与 ChatGPT 交互的行为模式，这可能影响 B 规范的正确性。', 'title_zh': 'AI助手中小学生编写正式规范的帮助：基于ChatGPT和B-方法的研究'}
{'arxiv_id': 'arXiv:2410.21339', 'title': 'Machine Learning and Quantum Intelligence for Health Data Scenarios', 'authors': 'Sanjeev Naguleswaran', 'link': 'https://arxiv.org/abs/2410.21339', 'abstract': "The advent of quantum computing has opened new possibilities in data science, offering unique capabilities for addressing complex, data-intensive problems. Traditional machine learning algorithms often face challenges in high-dimensional or limited-quality datasets, which are common in healthcare. Quantum Machine Learning leverages quantum properties, such as superposition and entanglement, to enhance pattern recognition and classification, potentially surpassing classical approaches. This paper explores QML's application in healthcare, focusing on quantum kernel methods and hybrid quantum-classical networks for heart disease prediction and COVID-19 detection, assessing their feasibility and performance.", 'abstract_zh': '量子计算的兴起为数据科学开辟了新的可能性，提供了解决复杂、数据密集型问题的独特能力。传统机器学习算法在高维或质量有限的数据集面前常常面临挑战，这在医疗保健领域尤为常见。量子机器学习利用量子特性，如超position和缠绵，增强模式识别和分类能力，有可能超越经典方法。本文探讨了量子机器学习在医疗保健领域的应用，重点是量子核方法和混合量子经典网络在心脏病预测和COVID-19检测中的应用，评估其可行性和性能。', 'title_zh': '机器学习与量子智能在健康数据场景中的应用'}
