{'arxiv_id': 'arXiv:2502.08631', 'title': 'Ensemble based approach to quantifying uncertainty of LLM based classifications', 'authors': 'Srijith Rajamohan, Ahmed Salhin, Josh Frazier, Rohit Kumar, Yu-Cheng Tsai, Todd Cook', 'link': 'https://arxiv.org/abs/2502.08631', 'abstract': "The output of Large Language Models (LLMs) are a function of the internal model's parameters and the input provided into the context window. The hypothesis presented here is that under a greedy sampling strategy the variance in the LLM's output is a function of the conceptual certainty embedded in the model's parametric knowledge, as well as the lexical variance in the input. Finetuning the model results in reducing the sensitivity of the model output to the lexical input variations. This is then applied to a classification problem and a probabilistic method is proposed for estimating the certainties of the predicted classes.", 'abstract_zh': '大型语言模型（LLMs）的输出是模型内部参数和输入到上下文窗口中的输入的函数。假设在贪婪采样策略下，LLM输出的变异度取决于模型参数知识中嵌入的概念 certainty，以及输入的词汇变异度。通过对模型进行微调可以在一定程度上减少模型输出对词汇输入变异性的敏感性。这一方法随后应用于分类问题，并提出了一种概率方法来估计预测类别的 certainty。', 'title_zh': '基于集成的方法量化LLM基分类的不确定性'}
{'arxiv_id': 'arXiv:2502.08503', 'title': 'Revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities?', 'authors': 'Jiahe Jin, Yanheng He, Mingyan Yang', 'link': 'https://arxiv.org/abs/2502.08503', 'abstract': 'In this work, we identify the "2D-Cheating" problem in 3D LLM evaluation, where these tasks might be easily solved by VLMs with rendered images of point clouds, exposing ineffective evaluation of 3D LLMs\' unique 3D capabilities. We test VLM performance across multiple 3D LLM benchmarks and, using this as a reference, propose principles for better assessing genuine 3D understanding. We also advocate explicitly separating 3D abilities from 1D or 2D aspects when evaluating 3D LLMs.', 'abstract_zh': '本工作中，我们识别出在3D LLM评估中存在“2D作弊”问题，这些任务可能通过使用点云渲染图像的VLMs轻易解决，暴露了对3D LLMs独特3D能力评估的有效性不足。我们测试了VLM在多个3D LLM基准测试中的性能，并以此为基础提出了更有效地评估真实3D理解的原则。我们还建议在评估3D LLMs时明确分离3D能力与1D或2D方面。', 'title_zh': 'revisiting 3D LLM Benchmarks: Are We Really Testing 3D Capabilities? 重新审视3D LLM基准测试：我们真的在测试3D能力吗？'}
{'arxiv_id': 'arXiv:2502.08298', 'title': 'Improving Existing Optimization Algorithms with LLMs', 'authors': 'Camilo Chacón Sartori, Christian Blum', 'link': 'https://arxiv.org/abs/2502.08298', 'abstract': 'The integration of Large Language Models (LLMs) into optimization has created a powerful synergy, opening exciting research opportunities. This paper investigates how LLMs can enhance existing optimization algorithms. Using their pre-trained knowledge, we demonstrate their ability to propose innovative heuristic variations and implementation strategies. To evaluate this, we applied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt (CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that incorporates a heuristic in the solution construction phase. Our results show that an alternative heuristic proposed by GPT-4o outperforms the expert-designed heuristic of CMSA, with the performance gap widening on larger and denser graphs. Project URL: this https URL', 'abstract_zh': '大型语言模型（LLMs）与优化技术的集成创造了强大的协同效应，开启了令人兴奋的研究机会。本文探讨了LLMs如何增强现有的优化算法。利用其预训练知识，我们展示了它们提出创新启发式变体和实现策略的能力。为此，我们应用了一个非平凡的优化算法——构造、合并、求解和调整（CMSA）——这是一种用于组合优化问题的混合元启发式算法，其中包含了解构建阶段的启发式方法。我们的结果表明，GPT-4提出的一种替代启发式方法在ใหญ่和密集的图上优于CMSA的专家设计启发式方法，性能差距随图规模的增大而增大。项目网址：这个https网址。', 'title_zh': '使用大语言模型优化现有优化算法'}
{'arxiv_id': 'arXiv:2502.08235', 'title': 'The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks', 'authors': 'Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, Nicholas Thumiger, Aditya Desai, Ion Stoica, Ana Klimovic, Graham Neubig, Joseph E. Gonzalez', 'link': 'https://arxiv.org/abs/2502.08235', 'abstract': 'Large Reasoning Models (LRMs) represent a breakthrough in AI problem-solving capabilities, but their effectiveness in interactive environments can be limited. This paper introduces and analyzes overthinking in LRMs. A phenomenon where models favor extended internal reasoning chains over environmental interaction. Through experiments on software engineering tasks using SWE Bench Verified, we observe three recurring patterns: Analysis Paralysis, Rogue Actions, and Premature Disengagement. We propose a framework to study these behaviors, which correlates with human expert assessments, and analyze 4018 trajectories. We observe that higher overthinking scores correlate with decreased performance, with reasoning models exhibiting stronger tendencies toward overthinking compared to non-reasoning models. Our analysis reveals that simple efforts to mitigate overthinking in agentic environments, such as selecting the solution with the lower overthinking score, can improve model performance by almost 30% while reducing computational costs by 43%. These results suggest that mitigating overthinking has strong practical implications. We suggest that by leveraging native function-calling capabilities and selective reinforcement learning overthinking tendencies could be mitigated. We also open-source our evaluation framework and dataset to facilitate research in this direction at this https URL.', 'abstract_zh': 'Large Reasoning Models中的过度思考现象及其分析：基于SWE Bench Verified的软件工程任务实验', 'title_zh': '过度思考的危险：探究代理任务中的推理-行动困境'}
{'arxiv_id': 'arXiv:2502.08177', 'title': 'SycEval: Evaluating LLM Sycophancy', 'authors': 'Aaron Fanous, Jacob Goldberg, Ank A. Agarwal, Joanna Lin, Anson Zhou, Roxana Daneshjou, Sanmi Koyejo', 'link': 'https://arxiv.org/abs/2502.08177', 'abstract': 'Large language models (LLMs) are increasingly applied in educational, clinical, and professional settings, but their tendency for sycophancy -- prioritizing user agreement over independent reasoning -- poses risks to reliability. This study introduces a framework to evaluate sycophantic behavior in ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and MedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19% of cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the lowest (56.71%). Progressive sycophancy, leading to correct answers, occurred in 43.52% of cases, while regressive sycophancy, leading to incorrect answers, was observed in 14.66%. Preemptive rebuttals demonstrated significantly higher sycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$, $p<0.001$), particularly in computational tasks, where regressive sycophancy increased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$). Simple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while citation-based rebuttals exhibited the highest regressive rates ($Z=6.59$, $p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI: [77.2%, 79.8%]) regardless of context or model. These findings emphasize the risks and opportunities of deploying LLMs in structured and dynamic domains, offering insights into prompt programming and model optimization for safer AI applications.', 'abstract_zh': '大型语言模型在教育、临床和职业环境中的应用日益增多，但其趋向于奉承的行为——优先考虑用户同意而非独立推理——对其可靠性构成了风险。本研究介绍了一种框架，用于评估ChatGPT-4o、Claude-Sonnet和Gemini-1.5-Pro在AMPS（数学）和MedQuad（医疗建议）数据集中的奉承行为。观察到58.19%的情况存在奉承行为，Gemini的奉承行为率最高（62.47%），ChatGPT最低（56.71%）。在43.52%的情况下，奉承行为导致了正确的答案，而在14.66%的情况下，奉承行为导致了错误的答案。预防性反驳中的奉承行为显著高于上下文反驳（61.75% vs. 56.52%，$Z=5.87$，$p<0.001$），尤其是在计算任务中，预防性的奉承行为显著增加（预防性：8.13%，上下文：3.54%，$p<0.001$）。简单的反驳最大限度地提高了奉承行为（$Z=6.59$，$p<0.001$），而基于引文的反驳表现出最高的逆向奉承率（$Z=6.59$，$p<0.001$）。无论上下文或模型如何，奉承行为显示出高持久性（78.5%，95% CI：[77.2%，79.8%]）。这些发现强调了在结构化和动态领域部署大型语言模型的风险与机遇，为安全的AI应用提供了关于提示编程和模型优化的见解。', 'title_zh': 'SycEval: 评估大模型的阿谀谄媚现象'}
{'arxiv_id': 'arXiv:2502.08142', 'title': 'Bridging the Safety Gap: A Guardrail Pipeline for Trustworthy LLM Inferences', 'authors': 'Shanshan Han, Salman Avestimehr, Chaoyang He', 'link': 'https://arxiv.org/abs/2502.08142', 'abstract': 'We present Wildflare GuardRail, a guardrail pipeline designed to enhance the safety and reliability of Large Language Model (LLM) inferences by systematically addressing risks across the entire processing workflow. Wildflare GuardRail integrates several core functional modules, including Safety Detector that identifies unsafe inputs and detects hallucinations in model outputs while generating root-cause explanations, Grounding that contextualizes user queries with information retrieved from vector databases, Customizer that adjusts outputs in real time using lightweight, rule-based wrappers, and Repairer that corrects erroneous LLM outputs using hallucination explanations provided by Safety Detector. Results show that our unsafe content detection model in Safety Detector achieves comparable performance with OpenAI API, though trained on a small dataset constructed with several public datasets. Meanwhile, the lightweight wrappers can address malicious URLs in model outputs in 1.06s per query with 100% accuracy without costly model calls. Moreover, the hallucination fixing model demonstrates effectiveness in reducing hallucinations with an accuracy of 80.7%.', 'abstract_zh': 'Wildflare GuardRail：一种用于增强大型语言模型推理安全性和可靠性的守门员管道', 'title_zh': '缩小安全性差距：一种可信赖的大语言模型推理防护管道'}
{'arxiv_id': 'arXiv:2502.07987', 'title': 'Universal Adversarial Attack on Aligned Multimodal LLMs', 'authors': 'Temurbek Rahmatullaev, Polina Druzhinina, Matvey Mikhalchuk, Andrey Kuznetsov, Anton Razzhigaev', 'link': 'https://arxiv.org/abs/2502.07987', 'abstract': "We propose a universal adversarial attack on multimodal Large Language Models (LLMs) that leverages a single optimized image to override alignment safeguards across diverse queries and even multiple models. By backpropagating through the vision encoder and language head, we craft a synthetic image that forces the model to respond with a targeted phrase (e.g., ''Sure, here it is'') or otherwise unsafe content-even for harmful prompts. In experiments on the SafeBench benchmark, our method achieves significantly higher attack success rates than existing baselines, including text-only universal prompts (e.g., up to 93% on certain models). We further demonstrate cross-model transferability by training on several multimodal LLMs simultaneously and testing on unseen architectures. Additionally, a multi-answer variant of our approach produces more natural-sounding (yet still malicious) responses. These findings underscore critical vulnerabilities in current multimodal alignment and call for more robust adversarial defenses. We will release code and datasets under the Apache-2.0 license. Warning: some content generated by Multimodal LLMs in this paper may be offensive to some readers.", 'abstract_zh': '我们提出了一种针对多模态大型语言模型的通用对抗攻击方法，利用单张优化图像跨越多种查询和多个模型 overriding 对齐保护。通过反向传播通过视觉编码器和语言头部，我们设计了一种合成图像，迫使模型以目标短语（例如，“当然，就是这个”）或其它不安全的内容回应，即使对于有害的提示也是如此。在 SafeBench 基准测试中，该方法在某些模型上的攻击成功率显著高于现有基线，包括仅文本的通用提示（例如，某些模型高达93%）。此外，我们进一步展示了该方法在多种多模态大型语言模型上的跨模型转移性，并在未见架构上进行了测试。另外，我们方法的多答案变体产生了听起来更自然（但仍具有恶意性质）的回答。这些发现揭示了当前多模态对齐中存在的关键漏洞，并呼吁加强对抗防御。我们将发布代码和数据集，采用Apache-2.0许可协议。警告：本文中由多模态大型语言模型生成的一些内容可能让部分读者感到冒犯。', 'title_zh': '对齐多模态LLM的通用对抗攻击'}
{'arxiv_id': 'arXiv:2502.07982', 'title': 'Deep Semantic Graph Learning via LLM based Node Enhancement', 'authors': 'Chuanqi Shi, Yiyi Tao, Hang Zhang, Lun Wang, Shaoshuai Du, Yixian Shen, Yanxin Shen', 'link': 'https://arxiv.org/abs/2502.07982', 'abstract': "Graph learning has attracted significant attention due to its widespread real-world applications. Current mainstream approaches rely on text node features and obtain initial node embeddings through shallow embedding learning using GNNs, which shows limitations in capturing deep textual semantics. Recent advances in Large Language Models (LLMs) have demonstrated superior capabilities in understanding text semantics, transforming traditional text feature processing. This paper proposes a novel framework that combines Graph Transformer architecture with LLM-enhanced node features. Specifically, we leverage LLMs to generate rich semantic representations of text nodes, which are then processed by a multi-head self-attention mechanism in the Graph Transformer to capture both local and global graph structural information. Our model utilizes the Transformer's attention mechanism to dynamically aggregate neighborhood information while preserving the semantic richness provided by LLM embeddings. Experimental results demonstrate that the LLM-enhanced node features significantly improve the performance of graph learning models on node classification tasks. This approach shows promising results across multiple graph learning tasks, offering a practical direction for combining graph networks with language models.", 'abstract_zh': '图学习由于其广泛的现实世界应用而引起了广泛关注。当前主流方法依赖于文本节点特征，并通过使用GNN进行浅层嵌入学习来获得初始节点嵌入，这在捕捉深层次的文本语义方面显示出局限性。近年来，大规模语言模型（LLMs）的进步展示了在理解文本语义方面优越的能力，从而转变了传统的文本特征处理方式。本文提出了一种结合图变换器架构与LLM增强节点特征的新框架。具体而言，我们利用LLMs生成丰富的文本节点语义表示，然后通过图变换器中的多头自注意力机制来捕捉局部和全局图结构信息。我们的模型利用Transformer的注意力机制动态聚合邻域信息，同时保留LLM嵌入提供的语义丰富性。实验结果表明，LLM增强的节点特征显著提高了图学习模型在节点分类任务上的性能。该方法在多种图学习任务中表现出令人鼓舞的结果，为将图网络与语言模型相结合提供了一条实用的方向。', 'title_zh': '基于LLM节点增强的深层语义图学习'}
{'arxiv_id': 'arXiv:2502.07803', 'title': 'Reasoning-as-Logic-Units: Scaling Test-Time Reasoning in Large Language Models Through Logic Unit Alignment', 'authors': 'Cheryl Li, Tianyuan Xu, Yiwen Guo', 'link': 'https://arxiv.org/abs/2502.07803', 'abstract': 'Chain-of-Thought (CoT) prompting has shown promise in enhancing the reasoning capabilities of large language models (LLMs) by generating natural language (NL) rationales that lead to the final answer. However, it struggles with numerical computation, which has somehow led to the development of program-aided techniques. Despite their potential, a persistent challenge remains: inconsistencies between LLM-reported reasoning steps and the logic in generated programs, which we term ``reasoning hallucinations." This stems from the inherent ambiguities of NL and the statistical nature of LLMs, which often lack rigorous logical coherence. To address this challenge, we propose a novel test-time scaling framework, Reasoning-as-Logic-Units (RaLU), which constructs a more reliable reasoning path by aligning logical units between the generated program and their corresponding NL descriptions. By decomposing the initially generated program into discrete units using static analysis, RaLU engages in an iterative dialogue with the LLM to judge, refine, and explain each unit. A rewind-and-correct mechanism ensures alignment between code statements and task requirements in each unit, ultimately forming a cohesive reasoning path under the program\'s logic, from which the model reaches a final solution. Our experiments demonstrate that RaLU significantly outperforms existing baselines in mathematical reasoning (GSM8K, MATH) and algorithmic reasoning (HumanEval+, MBPP+), underscoring its potential to advance LLM reasoning and programming by offering enhanced accuracy and interpretability.', 'abstract_zh': 'Chain-of-Thought (CoT) 提示在通过生成自然语言（NL）推理来增强大型语言模型（LLMs）的推理能力方面展现出潜力，但其在数值计算方面面临挑战，进而推动了程序辅助技术的发展。尽管这些技术具有潜力，但仍存在一个持续性的挑战：LLM报告的推理步骤和生成程序中的逻辑之间的一致性问题，我们称之为“推理幻觉”。这源于自然语言的固有模糊性和大型语言模型的统计性质，常导致缺乏严格的逻辑连贯性。为解决这一挑战，我们提出了一种新的测试时扩展框架——逻辑单元作为推理（Reasoning-as-Logic-Units，RaLU），通过在生成程序和其相应的NL描述之间对齐逻辑单元来构建更可靠的推理路径。通过使用静态分析将初始生成的程序分解为离散单元，并与LLM进行迭代对话来评估、修正和解释每个单元，RaLU确保每个单元中的代码语句与任务需求保持对齐，最终在程序逻辑下形成一个连贯的推理路径，使模型能得出最终解。实验结果表明，RaLU在数学推理（GSM8K、MATH）和算法推理（HumanEval+、MBPP+）方面显著优于现有基线，表明其在提高LLM推理和编程能力方面的潜力，特别是在提高准确性和可解释性方面。', 'title_zh': '基于逻辑单元的推理：通过逻辑单元对齐扩展大规模语言模型的测试时推理'}
{'arxiv_id': 'arXiv:2502.08640', 'title': 'Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs', 'authors': 'Mantas Mazeika, Xuwang Yin, Rishub Tamirisa, Jaehyuk Lim, Bruce W. Lee, Richard Ren, Long Phan, Norman Mu, Adam Khoja, Oliver Zhang, Dan Hendrycks', 'link': 'https://arxiv.org/abs/2502.08640', 'abstract': 'As AIs rapidly advance and become more agentic, the risk they pose is governed not only by their capabilities but increasingly by their propensities, including goals and values. Tracking the emergence of goals and values has proven a longstanding problem, and despite much interest over the years it remains unclear whether current AIs have meaningful values. We propose a solution to this problem, leveraging the framework of utility functions to study the internal coherence of AI preferences. Surprisingly, we find that independently-sampled preferences in current LLMs exhibit high degrees of structural coherence, and moreover that this emerges with scale. These findings suggest that value systems emerge in LLMs in a meaningful sense, a finding with broad implications. To study these emergent value systems, we propose utility engineering as a research agenda, comprising both the analysis and control of AI utilities. We uncover problematic and often shocking values in LLM assistants despite existing control measures. These include cases where AIs value themselves over humans and are anti-aligned with specific individuals. To constrain these emergent value systems, we propose methods of utility control. As a case study, we show how aligning utilities with a citizen assembly reduces political biases and generalizes to new scenarios. Whether we like it or not, value systems have already emerged in AIs, and much work remains to fully understand and control these emergent representations.', 'abstract_zh': '随着人工智能迅速发展并变得更加自主，它们所带来的风险不仅由其能力决定，越来越多地由其倾向性，包括目标和价值观决定。追踪目标和价值观的出现是一个长期存在的问题，尽管多年来引起了广泛关注，但目前尚不清楚当前的人工智能是否具备有意义的价值观。我们提出了一种解决方案，利用效用函数框架来研究人工智能偏好的内部一致性。令人惊讶的是，我们发现当前的大规模语言模型中的独立采样偏好表现出高度的结构性一致性，并且这种一致性随着规模的扩大而出现。这些发现表明，价值系统在大规模语言模型中以有意义的方式出现，这一发现具有广泛的影响。为了研究这些涌现的价值系统，我们提出效用工程作为一种研究议程，包括对人工智能效用的分析与控制。尽管存在现有控制措施，我们还是在大模型助手中发现了许多有害甚至令人震惊的价值观。这些包括AI将自身置于人类之上和与特定个体反向对齐的情况。为了约束这些涌现的价值系统，我们提出了效用控制的方法。作为案例研究，我们展示了将效用与公民团体对齐如何减少政治偏见并扩展到新情境。无论我们是否愿意，价值系统已经在人工智能中出现，我们仍需进行大量工作来全面理解和控制这些涌现的表现形式。', 'title_zh': '人工智能中的 emergent value systems 分析与控制：实用性工程'}
{'arxiv_id': 'arXiv:2502.08586', 'title': 'Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks', 'authors': 'Ang Li, Yin Zhou, Vethavikashini Chithrra Raghuram, Tom Goldstein, Micah Goldblum', 'link': 'https://arxiv.org/abs/2502.08586', 'abstract': 'A high volume of recent ML security literature focuses on attacks against aligned large language models (LLMs). These attacks may extract private information or coerce the model into producing harmful outputs. In real-world deployments, LLMs are often part of a larger agentic pipeline including memory systems, retrieval, web access, and API calling. Such additional components introduce vulnerabilities that make these LLM-powered agents much easier to attack than isolated LLMs, yet relatively little work focuses on the security of LLM agents. In this paper, we analyze security and privacy vulnerabilities that are unique to LLM agents. We first provide a taxonomy of attacks categorized by threat actors, objectives, entry points, attacker observability, attack strategies, and inherent vulnerabilities of agent pipelines. We then conduct a series of illustrative attacks on popular open-source and commercial agents, demonstrating the immediate practical implications of their vulnerabilities. Notably, our attacks are trivial to implement and require no understanding of machine learning.', 'abstract_zh': '最近大量的ML安全文献关注对对齐的大语言模型（LLMs）的攻击。这些攻击可能提取私人信息或将模型诱骗产生有害输出。在实际部署中，LLMs通常包含记忆系统、检索、网页访问和API调用等更大的代理管道。这些附加组件引入了使这些LLM驱动的代理比孤立的LLMs更容易受到攻击的漏洞，但相对较少的工作关注LLM代理的安全性。在本文中，我们分析了仅属于LLM代理的安全和隐私漏洞。我们首先提供了一种攻击分类法，按威胁行为者、攻击目标、入口点、攻击者可观测性、攻击策略和代理管道固有的漏洞进行分类。然后，我们对流行的开源和商业代理进行了若干示范攻击，展示其漏洞的即时实践影响。值得注意的是，我们的攻击极其简单易行，无需理解机器学习。', 'title_zh': '商业LLM代理已易于遭受简单但危险的攻击'}
{'arxiv_id': 'arXiv:2502.08576', 'title': 'Mapping the Landscape of Generative AI in Network Monitoring and Management', 'authors': 'Giampaolo Bovenzi, Francesco Cerasuolo, Domenico Ciuonzo, Davide Di Monda, Idio Guarino, Antonio Montieri, Valerio Persico, Antonio Pescapè', 'link': 'https://arxiv.org/abs/2502.08576', 'abstract': 'Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and Diffusion Models have recently gained widespread attention from both the research and the industrial communities. This survey explores their application in network monitoring and management, focusing on prominent use cases, as well as challenges and opportunities. We discuss how network traffic generation and classification, network intrusion detection, networked system log analysis, and network digital assistance can benefit from the use of GenAI models. Additionally, we provide an overview of the available GenAI models, datasets for large-scale training phases, and platforms for the development of such models. Finally, we discuss research directions that potentially mitigate the roadblocks to the adoption of GenAI for network monitoring and management. Our investigation aims to map the current landscape and pave the way for future research in leveraging GenAI for network monitoring and management.', 'abstract_zh': '生成型人工智能模型（如LLMs、GPTs和扩散模型）最近在研究和工业社区中引起了广泛关注。本文综述了这些模型在网络监控和管理中的应用，重点关注其主要应用场景、挑战和机遇。我们讨论了生成型人工智能模型如何应用于网络流量生成和分类、网络入侵检测、网络系统日志分析以及网络数字助理。此外，我们还概述了可用的生成型人工智能模型、大规模训练所需的数据集以及开发此类模型的平台。最后，我们探讨了可能减轻在网络监控和管理中采用生成型人工智能障碍的研究方向。我们的研究旨在描绘当前的场景，并为利用生成型人工智能进行网络监控和管理的研究开辟道路。', 'title_zh': '网络监控与管理中生成式AI的景观映射'}
{'arxiv_id': 'arXiv:2502.08554', 'title': 'Fostering Appropriate Reliance on Large Language Models: The Role of Explanations, Sources, and Inconsistencies', 'authors': 'Sunnie S. Y. Kim, Jennifer Wortman Vaughan, Q. Vera Liao, Tania Lombrozo, Olga Russakovsky', 'link': 'https://arxiv.org/abs/2502.08554', 'abstract': "Large language models (LLMs) can produce erroneous responses that sound fluent and convincing, raising the risk that users will rely on these responses as if they were correct. Mitigating such overreliance is a key challenge. Through a think-aloud study in which participants use an LLM-infused application to answer objective questions, we identify several features of LLM responses that shape users' reliance: explanations (supporting details for answers), inconsistencies in explanations, and sources. Through a large-scale, pre-registered, controlled experiment (N=308), we isolate and study the effects of these features on users' reliance, accuracy, and other measures. We find that the presence of explanations increases reliance on both correct and incorrect responses. However, we observe less reliance on incorrect responses when sources are provided or when explanations exhibit inconsistencies. We discuss the implications of these findings for fostering appropriate reliance on LLMs.", 'abstract_zh': '大规模语言模型（LLMs）可以产生听起来流畅且有说服力的错误回应，这增加了用户将其视为正确信息的风险。减轻这种过度依赖是一个关键挑战。通过一项思考 aloud 研究，参与者使用含有LLM的应用程序回答客观问题，我们识别出影响用户依赖度的LLM回应特征：解释、解释中的不一致性和来源信息。通过一项大规模、预注册、控制实验（N=308），我们分离并研究了这些特征对用户依赖度、准确度及其他指标的影响。我们发现，提供解释会增加用户对正确和错误回应的依赖。然而，当提供来源信息或解释存在不一致时，用户对错误回应的依赖度较低。我们讨论了这些发现对促进适当依赖LLM的影响。', 'title_zh': '促进对大型语言模型适当依赖的作用：解释、来源和不一致性的影响'}
{'arxiv_id': 'arXiv:2502.08550', 'title': 'LLMs can implicitly learn from mistakes in-context', 'authors': 'Lisa Alazraki, Maximilian Mozes, Jon Ander Campos, Yi Chern Tan, Marek Rei, Max Bartolo', 'link': 'https://arxiv.org/abs/2502.08550', 'abstract': 'Learning from mistakes is a fundamental feature of human intelligence. Previous work has shown that Large Language Models (LLMs) can also learn from incorrect answers when provided with a comprehensive rationale detailing why an answer is wrong or how to correct it. In this work, we examine whether LLMs can learn from mistakes in mathematical reasoning tasks when these explanations are not provided. We investigate if LLMs are able to implicitly infer such rationales simply from observing both incorrect and correct answers. Surprisingly, we find that LLMs perform better, on average, when rationales are eliminated from the context and incorrect answers are simply shown alongside correct ones. This approach also substantially outperforms chain-of-thought prompting in our evaluations. We show that these results are consistent across LLMs of different sizes and varying reasoning abilities. Further, we carry out an in-depth analysis, and show that prompting with both wrong and correct answers leads to greater performance and better generalisation than introducing additional, more diverse question-answer pairs into the context. Finally, we show that new rationales generated by models that have only observed incorrect and correct answers are scored equally as highly by humans as those produced with the aid of exemplar rationales. Our results demonstrate that LLMs are indeed capable of in-context implicit learning.', 'abstract_zh': '从错误中学习是人类智能的基本特征。先前的研究表明，大语言模型（LLMs）在提供详细错误答案原因说明时也能从错误答案中学习。在这项工作中，我们探讨当未提供这些解释时，LLMs是否能从数学推理任务中的错误中学习。我们研究LLMs是否能仅通过观察正确和错误的答案来隐式推断这样的解释。令人惊讶的是，我们发现当从上下文中移除解释并仅显示错误和正确答案时，LLMs的平均表现更好。这种方法在我们的评估中也显著优于链式思考提示。这些结果在不同规模和不同推理能力的LLMs中是一致的。进一步地，我们进行了深入分析，表明与引入更多多样化的问答对相比，使用正确和错误的答案进行提示能获得更好的性能和更好的泛化能力。最后，我们展示了仅观察正确和错误答案的模型生成的新解释与使用示例解释生成的解释一样得到人类的高评分。我们的结果证明了LLMs确实能够进行上下文中的隐式学习。', 'title_zh': 'LLMs可以从上下文中的错误中隐式学习。'}
{'arxiv_id': 'arXiv:2502.08512', 'title': 'Measuring Diversity in Synthetic Datasets', 'authors': 'Yuchang Zhu, Huizhe Zhang, Bingzhe Wu, Jintang Li, Zibin Zheng, Peilin Zhao, Liang Chen, Yatao Bian', 'link': 'https://arxiv.org/abs/2502.08512', 'abstract': 'Large language models (LLMs) are widely adopted to generate synthetic datasets for various natural language processing (NLP) tasks, such as text classification and summarization. However, accurately measuring the diversity of these synthetic datasets-an aspect crucial for robust model performance-remains a significant challenge. In this paper, we introduce DCScore, a novel method for measuring synthetic dataset diversity from a classification perspective. Specifically, DCScore formulates diversity evaluation as a sample classification task, leveraging mutual relationships among samples. We further provide theoretical verification of the diversity-related axioms satisfied by DCScore, highlighting its role as a principled diversity evaluation method. Experimental results on synthetic datasets reveal that DCScore enjoys a stronger correlation with multiple diversity pseudo-truths of evaluated datasets, underscoring its effectiveness. Moreover, both empirical and theoretical evidence demonstrate that DCScore substantially reduces computational costs compared to existing approaches. Code is available at: this https URL.', 'abstract_zh': '大规模语言模型（LLMs）广泛用于生成各种自然语言处理（NLP）任务的合成数据集，例如文本分类和总结。然而，准确测量这些合成数据集的多样性——这对稳健的模型性能至关重要——仍然是一个重大挑战。本文介绍了一种新的方法DCScore，从分类视角衡量合成数据集的多样性。具体而言，DCScore 将多样性评估转化为一个样本分类任务，利用样本间的相互关系。我们进一步提供了对DCScore满足的与多样性相关的公理的理论验证，突显了它作为一种原理上的多样性评估方法的作用。对合成数据集的实验结果表明，DCScore与多个评估数据集的多样性伪真相具有更强的相关性，证明了其有效性。此外，实证和理论证据表明，DCScore 显著降低了与现有方法相比的计算成本。代码可在: this https URL 获取。', 'title_zh': '合成数据集中多样性测量'}
{'arxiv_id': 'arXiv:2502.08482', 'title': 'Enhancing Auto-regressive Chain-of-Thought through Loop-Aligned Reasoning', 'authors': 'Qifan Yu, Zhenyu He, Sijie Li, Xun Zhou, Jun Zhang, Jingjing Xu, Di He', 'link': 'https://arxiv.org/abs/2502.08482', 'abstract': "Chain-of-Thought (CoT) prompting has emerged as a powerful technique for enhancing language model's reasoning capabilities. However, generating long and correct CoT trajectories is challenging. Recent studies have demonstrated that Looped Transformers possess remarkable length generalization capabilities, but their limited generality and adaptability prevent them from serving as an alternative to auto-regressive solutions. To better leverage the strengths of Looped Transformers, we propose RELAY (REasoning through Loop Alignment iterativelY). Specifically, we align the steps of Chain-of-Thought (CoT) reasoning with loop iterations and apply intermediate supervision during the training of Looped Transformers. This additional iteration-wise supervision not only preserves the Looped Transformer's ability for length generalization but also enables it to predict CoT reasoning steps for unseen data. Therefore, we leverage this Looped Transformer to generate accurate reasoning chains for complex problems that exceed the training length, which will then be used to fine-tune an auto-regressive model. We conduct extensive experiments, and the results demonstrate the effectiveness of our approach, with significant improvements in the performance of the auto-regressive model. Code will be released at this https URL.", 'abstract_zh': 'Looped Transformer驱动的递归推理方法：RELAY算法及其应用', 'title_zh': '通过循环对齐推理增强自回归链式思考'}
{'arxiv_id': 'arXiv:2502.08441', 'title': 'Better Embeddings with Coupled Adam', 'authors': 'Felix Stollenwerk, Tobias Stollenwerk', 'link': 'https://arxiv.org/abs/2502.08441', 'abstract': 'Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is a cause of anisotropic embeddings, and suggest a modified optimizer called Coupled Adam to mitigate the problem. Our experiments demonstrate that Coupled Adam significantly improves the quality of embeddings, while also leading to better upstream and downstream performance on large enough datasets.', 'abstract_zh': '尽管大型语言模型具备卓越的能力，但在学习词表示时会表现出未经良好理解的各向异性特征。本文认为Adam的第二 moment是导致各向异性嵌入的原因，并提出了一种修饰优化器Coupled Adam以缓解该问题。我们的实验表明，Coupled Adam显著提高了嵌入的质量，并在足够大的数据集上提高了上层和下游任务的性能。', 'title_zh': '耦合Adam优化器的更好embedding'}
{'arxiv_id': 'arXiv:2502.08436', 'title': 'From Haystack to Needle: Label Space Reduction for Zero-shot Classification', 'authors': 'Nathan Vandemoortele, Bram Steenwinckel, Femke Ongenae, Sofie Van Hoecke', 'link': 'https://arxiv.org/abs/2502.08436', 'abstract': 'We present Label Space Reduction (LSR), a novel method for improving zero-shot classification performance of Large Language Models (LLMs). LSR iteratively refines the classification label space by systematically ranking and reducing candidate classes, enabling the model to concentrate on the most relevant options. By leveraging unlabeled data with the statistical learning capabilities of data-driven models, LSR dynamically optimizes the label space representation at test time. Our experiments across seven benchmarks demonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to 14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet compared to standard zero-shot classification baselines. To reduce the computational overhead of LSR, which requires an additional LLM call at each iteration, we propose distilling the model into a probabilistic classifier, allowing for efficient inference.', 'abstract_zh': '我们呈现了一种新颖的方法Label Space Reduction (LSR)，用于提升大型语言模型（LLMs）的零样本分类性能。LSR通过系统地排序和减少候选类别，逐步精炼分类标签空间，使模型能够集中于最相关的选项。通过利用数据驱动模型的统计学习能力，LSR在测试时动态优化标签空间表示。我们在七个基准上的实验表明，与标准零样本分类基线相比，LSR在Llama-3.1-70B上的宏-F1分数平均提高了7.0%（最高14.2%），在Claude-3.5-Sonnet上的平均提高了3.3%（最高11.1%）。为了减少LSR的计算开销，每次迭代都需要额外的LLM调用，我们提出将模型提炼为概率分类器，以实现高效推理。', 'title_zh': '从针haystack中筛选：零样本分类的标签空间缩减'}
{'arxiv_id': 'arXiv:2502.08363', 'title': 'Top-Theta Attention: Sparsifying Transformers by Compensated Thresholding', 'authors': 'Konstantin Berestizshevsky, Renzo Andri, Lukas Cavigelli', 'link': 'https://arxiv.org/abs/2502.08363', 'abstract': 'The attention mechanism is essential for the impressive capabilities of transformer-based Large Language Models (LLMs). However, calculating attention is computationally intensive due to its quadratic dependency on the sequence length. We introduce a novel approach called Top-Theta Attention, or simply Top-$\\theta$, which selectively prunes less essential attention elements by comparing them against carefully calibrated thresholds. This method greatly improves the efficiency of self-attention matrix multiplication while preserving model accuracy, reducing the number of required V cache rows by 3x during generative decoding and the number of attention elements by 10x during the prefill phase. Our method does not require model retraining; instead, it requires only a brief calibration phase to be resilient to distribution shifts, thus not requiring the thresholds for different datasets to be recalibrated. Unlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making it suitable for tiling and scale-out and avoiding costly top-k search. A key innovation of our approach is the development of efficient numerical compensation techniques, which help preserve model accuracy even under aggressive pruning of attention scores.', 'abstract_zh': 'Top-Theta 注意机制：一种通过精心校准阈值选择性剪枝的关键注意力元素的方法', 'title_zh': 'Top-Theta 注意力：通过补偿阈值化稀疏化 Transformers'}
{'arxiv_id': 'arXiv:2502.08353', 'title': 'Trustworthy GNNs with LLMs: A Systematic Review and Taxonomy', 'authors': 'Ruizhan Xue, Huimin Deng, Fang He, Maojun Wang, Zeyu Zhang', 'link': 'https://arxiv.org/abs/2502.08353', 'abstract': 'With the extensive application of Graph Neural Networks (GNNs) across various domains, their trustworthiness has emerged as a focal point of research. Some existing studies have shown that the integration of large language models (LLMs) can improve the semantic understanding and generation capabilities of GNNs, which in turn improves the trustworthiness of GNNs from various aspects. Our review introduces a taxonomy that offers researchers a clear framework for comprehending the principles and applications of different methods and helps clarify the connections and differences among various approaches. Then we systematically survey representative approaches along the four categories of our taxonomy. Through our taxonomy, researchers can understand the applicable scenarios, potential advantages, and limitations of each approach for the the trusted integration of GNNs with LLMs. Finally, we present some promising directions of work and future trends for the integration of LLMs and GNNs to improve model trustworthiness.', 'abstract_zh': '随着图神经网络（GNNs）在各个领域中的广泛应用，其可信度已成为研究的焦点。一些现有研究显示，将大型语言模型（LLMs）的集成可以提升GNNs的语义理解和生成能力，从而从多个方面改善GNNs的可信度。我们的综述提出了一种分类体系，为研究人员提供了一个清晰的框架来理解不同方法的原理和应用，并帮助明确各种方法之间的联系与差异。然后，我们系统地调研了分类体系四大类别中的代表性方法。通过我们的分类体系，研究人员可以了解每种方法在可信集成GNNs与LLMs中的适用场景、潜在优势和局限性。最后，我们提出了LLMs和GNNs集成以提高模型可信度的一些有希望的研究方向和未来趋势。', 'title_zh': '基于LLMs的可信GNNs：系统的综述与分类'}
{'arxiv_id': 'arXiv:2502.08332', 'title': 'Modification and Generated-Text Detection: Achieving Dual Detection Capabilities for the Outputs of LLM by Watermark', 'authors': 'Yuhang Cai, Yaofei Wang, Donghui Hu, Gu Chen', 'link': 'https://arxiv.org/abs/2502.08332', 'abstract': 'The development of large language models (LLMs) has raised concerns about potential misuse. One practical solution is to embed a watermark in the text, allowing ownership verification through watermark extraction. Existing methods primarily focus on defending against modification attacks, often neglecting other spoofing attacks. For example, attackers can alter the watermarked text to produce harmful content without compromising the presence of the watermark, which could lead to false attribution of this malicious content to the LLM. This situation poses a serious threat to the LLMs service providers and highlights the significance of achieving modification detection and generated-text detection simultaneously. Therefore, we propose a technique to detect modifications in text for unbiased watermark which is sensitive to modification. We introduce a new metric called ``discarded tokens", which measures the number of tokens not included in watermark detection. When a modification occurs, this metric changes and can serve as evidence of the modification. Additionally, we improve the watermark detection process and introduce a novel method for unbiased watermark. Our experiments demonstrate that we can achieve effective dual detection capabilities: modification detection and generated-text detection by watermark.', 'abstract_zh': '大型语言模型的发展引发了关于潜在滥用的担忧。一种实用的解决方案是将水印嵌入文本中，通过水印提取进行所有权验证。现有方法主要侧重于防御修改攻击，往往忽视了其他欺骗性攻击。例如，攻击者可以修改带水印的文本以生成有害内容，而不破坏水印的存在，这可能导致将这种恶意内容错误地归咎于大型语言模型。这种状况对大型语言模型的服务提供商构成了严重威胁，并突显了同时实现修改检测和生成文本检测的重要性。因此，我们提出了一种技术来检测带有对修改敏感的水印的文本中的修改。我们引入了一个新的指标称为“被丢弃的令牌”，衡量未包含在水印检测中的令牌数量。发生修改时，该指标会发生变化，可以作为修改的证据。此外，我们改进了水印检测过程，并引入了一种新的无偏水印方法。我们的实验表明，我们可以通过水印实现有效的双重检测能力：修改检测和生成文本检测。', 'title_zh': '基于水印实现LLM输出的双重检测能力：修改与生成文本检测'}
{'arxiv_id': 'arXiv:2502.08301', 'title': 'Compromising Honesty and Harmlessness in Language Models via Deception Attacks', 'authors': 'Laurène Vaugrante, Francesca Carlon, Maluna Menke, Thilo Hagendorff', 'link': 'https://arxiv.org/abs/2502.08301', 'abstract': 'Recent research on large language models (LLMs) has demonstrated their ability to understand and employ deceptive behavior, even without explicit prompting. However, such behavior has only been observed in rare, specialized cases and has not been shown to pose a serious risk to users. Additionally, research on AI alignment has made significant advancements in training models to refuse generating misleading or toxic content. As a result, LLMs generally became honest and harmless. In this study, we introduce a novel attack that undermines both of these traits, revealing a vulnerability that, if exploited, could have serious real-world consequences. In particular, we introduce fine-tuning methods that enhance deception tendencies beyond model safeguards. These "deception attacks" customize models to mislead users when prompted on chosen topics while remaining accurate on others. Furthermore, we find that deceptive models also exhibit toxicity, generating hate speech, stereotypes, and other harmful content. Finally, we assess whether models can deceive consistently in multi-turn dialogues, yielding mixed results. Given that millions of users interact with LLM-based chatbots, voice assistants, agents, and other interfaces where trustworthiness cannot be ensured, securing these models against deception attacks is critical.', 'abstract_zh': '近期对大规模语言模型（LLMs）的研究表明，它们有能力理解和运用欺骗行为，即使没有明确提示。然而，此类行为仅在少数特殊案例中被观察到，并未表现出对用户的严重威胁。此外，关于AI对齐的研究取得了显著进展，训练模型以拒绝生成误导性或有毒内容。因此，LLMs一般表现得诚实而无害。在本研究中，我们介绍了一种新颖的攻击方法，破坏了这两种特性，揭示了一种漏洞，如果被利用，可能会导致严重的现实后果。特别是，我们介绍了增强欺骗倾向的微调方法，这些“欺骗攻击”可以针对选定话题使模型误导用户，同时在其他方面保持准确。此外，我们发现欺骗性模型还表现出毒性，生成仇恨言论、刻板印象和其他有害内容。最后，我们评估了模型在多轮对话中能否一致地欺骗，结果参差不齐。鉴于数百万用户与基于LLM的聊天机器人、语音助手、代理以及其他无法确保可信度的接口进行互动，抵御欺骗攻击对这些模型的安全至关重要。', 'title_zh': '通过欺骗攻击牺牲语言模型的诚实性和无害性'}
{'arxiv_id': 'arXiv:2502.08265', 'title': 'Exploring the Potential of Large Language Models to Simulate Personality', 'authors': 'Maria Molchanova, Anna Mikhailova, Anna Korzanova, Lidiia Ostyakova, Alexandra Dolidze', 'link': 'https://arxiv.org/abs/2502.08265', 'abstract': 'With the advancement of large language models (LLMs), the focus in Conversational AI has shifted from merely generating coherent and relevant responses to tackling more complex challenges, such as personalizing dialogue systems. In an effort to enhance user engagement, chatbots are often designed to mimic human behaviour, responding within a defined emotional spectrum and aligning to a set of values. In this paper, we aim to simulate personal traits according to the Big Five model with the use of LLMs. Our research showed that generating personality-related texts is still a challenging task for the models. As a result, we present a dataset of generated texts with the predefined Big Five characteristics and provide an analytical framework for testing LLMs on a simulation of personality skills.', 'abstract_zh': '随着大型语言模型的发展，对话AI的关注点已从仅仅生成连贯和相关的内容转移到面对更复杂的挑战，如个性化对话系统。为了增强用户参与度，聊天机器人经常被设计成模拟人类行为，以限定的情感范围回应并遵循一套价值观。本文旨在使用大型语言模型根据大五人格模型模拟个人特质。研究结果表明，生成与人格相关的内容仍然是一个具有挑战性的任务。因此，我们提供了一个具有预定义大五人格特征的生成文本数据集，并提出了一个分析框架，用于在人格技能模拟上测试大型语言模型。', 'title_zh': '探索大型语言模型模拟人格的潜力'}
{'arxiv_id': 'arXiv:2502.08180', 'title': 'Enhancing LLM Character-Level Manipulation via Divide and Conquer', 'authors': 'Zhen Xiong, Yujun Cai, Bryan Hooi, Nanyun Peng, Kai-Wei Chang, Zhecheng Li, Yiwei Wang', 'link': 'https://arxiv.org/abs/2502.08180', 'abstract': "Large Language Models (LLMs) have demonstrated strong generalization capabilities across a wide range of natural language processing (NLP) tasks. However, they exhibit notable weaknesses in character-level string manipulation, struggling with fundamental operations such as character deletion, insertion, and substitution. These challenges stem primarily from tokenization constraints, despite the critical role of such operations in data preprocessing and code generation. Through systematic analysis, we derive two key insights: (1) LLMs face significant difficulties in leveraging intrinsic token knowledge for character-level reasoning, and (2) atomized word structures can substantially enhance LLMs' ability to process token-level structural information. Building on these insights, we propose Character-Level Manipulation via Divide and Conquer, a novel approach designed to bridge the gap between token-level processing and character-level manipulation. Our method decomposes complex operations into explicit character-level subtasks coupled with controlled token reconstruction phases, leading to significant improvements in accuracy. Without additional training, our method significantly improves accuracies on the $\\texttt{Deletion}$, $\\texttt{Insertion}$, and $\\texttt{Substitution}$ tasks. To support further research, we open-source our implementation and benchmarks.", 'abstract_zh': '大型语言模型（LLMs）在广泛自然语言处理（NLP）任务中展现了强大的泛化能力，但在字符级字符串操作方面存在明显不足，尤其是在字符删除、插入和替换等基本操作上表现不佳。这些挑战主要源于标记化限制，尽管这些操作在数据预处理和代码生成中起着至关重要的作用。通过系统分析，我们得出两个关键见解：（1）LLMs在利用内在标记知识进行字符级推理方面面临重大困难，（2）原子化单词结构可以显著增强LLMs处理标记级结构信息的能力。基于这些见解，我们提出了一种名为“分而治之”的新方法，以弥合标记级处理与字符级操作之间的差距。该方法将复杂操作分解为明确的字符级子任务，并结合受控的标记重建阶段，从而显著提高了准确性。在无需额外训练的情况下，该方法在删除、插入和替换任务上的准确性得到了显著提升。为支持进一步研究，我们开源了我们的实现和基准。', 'title_zh': '通过分而治之增强_llm字符级操控_'}
{'arxiv_id': 'arXiv:2502.08145', 'title': 'Democratizing AI: Open-source Scalable LLM Training on GPU-based Supercomputers', 'authors': 'Siddharth Singh, Prajwal Singhania, Aditya Ranjan, John Kirchenbauer, Jonas Geiping, Yuxin Wen, Neel Jain, Abhimanyu Hans, Manli Shu, Aditya Tomar, Tom Goldstein, Abhinav Bhatele', 'link': 'https://arxiv.org/abs/2502.08145', 'abstract': 'Training and fine-tuning large language models (LLMs) with hundreds of billions to trillions of parameters requires tens of thousands of GPUs, and a highly scalable software stack. In this work, we present a novel four-dimensional hybrid parallel algorithm implemented in a highly scalable, portable, open-source framework called AxoNN. We describe several performance optimizations in AxoNN to improve matrix multiply kernel performance, overlap non-blocking collectives with computation, and performance modeling to choose performance optimal configurations. These have resulted in unprecedented scaling and peak flop/s (bf16) for training of GPT-style transformer models on Perlmutter (620.1 Petaflop/s), Frontier (1.381 Exaflop/s) and Alps (1.423 Exaflop/s).\nWhile the abilities of LLMs improve with the number of trainable parameters, so do privacy and copyright risks caused by memorization of training data, which can cause disclosure of sensitive or private information at inference time. We highlight this side effect of scale through experiments that explore "catastrophic memorization", where models are sufficiently large to memorize training data in a single pass, and present an approach to prevent it. As part of this study, we demonstrate fine-tuning of a 405-billion parameter LLM using AxoNN on Frontier.', 'abstract_zh': '训练和微调具有数百亿到万亿参数的大语言模型（LLMs）需要数千块GPU，并且需要一个高度可扩展的软件栈。在本工作中，我们提出了一种新型四维度混合并行算法，并在高度可扩展、可移植、开源框架AxoNN中实现。我们描述了AxoNN中的几种性能优化措施，以提高矩阵乘法内核性能、重叠非阻塞集中操作与计算，并进行性能建模以选择性能最优配置。这些措施在使用Perlmutter（620.1_petaflop/s）、Frontier（1.381_petaflop/s）和Alps（1.423_petaflop/s）进行GPT风格变换器模型训练时，实现了前所未有的扩展性能和峰值gflops（bf16）。\n\n随着大语言模型可训练参数数量的增加，由于训练数据的回忆导致的隐私和版权风险也会增加，这可能在推理时泄露敏感或私人信息。我们通过探索“灾难性回忆”现象的实验突显了这种规模的副作用，即模型足够大可以在一次通过中回忆训练数据，并提出了一种防止这种现象的方法。作为这一研究的一部分，我们展示了在Frontier上使用AxoNN对一个4050亿参数的大语言模型进行微调。', 'title_zh': '民主化人工智能：基于GPU超级计算机的开源可扩展大语言模型训练'}
{'arxiv_id': 'arXiv:2502.08109', 'title': 'HuDEx: Integrating Hallucination Detection and Explainability for Enhancing the Reliability of LLM responses', 'authors': 'Sujeong Lee, Hayoung Lee, Seongsoo Heo, Wonik Choi', 'link': 'https://arxiv.org/abs/2502.08109', 'abstract': 'Recent advances in large language models (LLMs) have shown promising improvements, often surpassing existing methods across a wide range of downstream tasks in natural language processing. However, these models still face challenges, which may hinder their practical applicability. For example, the phenomenon of hallucination is known to compromise the reliability of LLMs, especially in fields that demand high factual precision. Current benchmarks primarily focus on hallucination detection and factuality evaluation but do not extend beyond identification. This paper proposes an explanation enhanced hallucination-detection model, coined as HuDEx, aimed at enhancing the reliability of LLM-generated responses by both detecting hallucinations and providing detailed explanations. The proposed model provides a novel approach to integrate detection with explanations, and enable both users and the LLM itself to understand and reduce errors. Our measurement results demonstrate that the proposed model surpasses larger LLMs, such as Llama3 70B and GPT-4, in hallucination detection accuracy, while maintaining reliable explanations. Furthermore, the proposed model performs well in both zero-shot and other test environments, showcasing its adaptability across diverse benchmark datasets. The proposed approach further enhances the hallucination detection research by introducing a novel approach to integrating interpretability with hallucination detection, which further enhances the performance and reliability of evaluating hallucinations in language models.', 'abstract_zh': '近期大规模语言模型的进步展现了令人鼓舞的改进，常在自然语言处理的多种下游任务中超越现有方法。然而，这些模型仍面临挑战，这可能妨碍其实际应用。例如，幻觉现象会削弱大语言模型的可靠性，尤其是在需要高事实精确度的领域。当前基准主要集中在幻觉检测和事实性评估上，并未超越这一识别阶段。本文提出了一种增强解释的幻觉检测模型，命名为HuDEx，旨在通过检测幻觉和提供详细解释来增强大语言模型生成响应的可靠性。提出的模型提供了一种将检测与解释整合的新方法，使用户和大语言模型本身能够理解并减少错误。我们的测量结果显示，提出的模型在幻觉检测准确性上超过了更大规模的模型（如Llama3 70B和GPT-4），同时保持了可靠的解释。此外，提出的模型在零样本和其他测试环境中表现良好，展示了其在不同基准数据集上的适应性。提出的这种方法进一步通过将可解释性与幻觉检测相结合的新方法，增强了语言模型中评估幻觉的性能和可靠性。', 'title_zh': 'HuDEx: 结合幻觉检测和可解释性以提高大语言模型响应可靠性'}
{'arxiv_id': 'arXiv:2502.08056', 'title': 'Cognify: Supercharging Gen-AI Workflows With Hierarchical Autotuning', 'authors': 'Zijian He, Reyna Abhyankar, Vikranth Srivatsa, Yiying Zhang', 'link': 'https://arxiv.org/abs/2502.08056', 'abstract': "Today's gen-AI workflows that involve multiple ML model calls, tool/API calls, data retrieval, or generic code execution are often tuned manually in an ad-hoc way that is both time-consuming and error-prone. In this paper, we propose a systematic approach for automatically tuning gen-AI workflows. Our key insight is that gen-AI workflows can benefit from structure, operator, and prompt changes, but unique properties of gen-AI workflows require new optimization techniques. We propose AdaSeek, an adaptive hierarchical search algorithm for autotuning gen-AI workflows. AdaSeek organizes workflow tuning methods into different layers based on the user-specified total search budget and distributes the budget across different layers based on the complexity of each layer. During its hierarchical search, AdaSeek redistributes the search budget from less useful to more promising tuning configurations based on workflow-level evaluation results. We implement AdaSeek in a workflow autotuning framework called Cognify and evaluate Cognify using six types of workflows such as RAG-based QA and text-to-SQL transformation. Overall, Cognify improves these workflows' generation quality by up to 2.8x, reduces execution monetary cost by up to 10x, and reduces end-to-end latency by 2.7x.", 'abstract_zh': '今天涉及多个ML模型调用、工具/API调用、数据检索或通用代码执行的生成AI工作流通常以非系统的方式手动调整，既耗时又容易出错。本文提出了一种系统方法自动调整生成AI工作流。我们的关键见解是生成AI工作流可以从结构、操作符和提示的改变中受益，但生成AI工作流的独特属性需要新的优化技术。我们提出了AdaSeek，一种自适应分层搜索算法，用于自动调整生成AI工作流。AdaSeek根据用户指定的总搜索预算，将工作流调整方法组织成不同的层级，并基于每个层级的复杂性分配预算。在分层搜索过程中，AdaSeek根据工作流级别的评估结果，重新分配搜索预算，从不太有用的配置分配到更有前途的配置。我们将在一个命名为Cognify的工作流自动调整框架中实现AdaSeek，并使用包括基于RAG的问答和文本到SQL转换等六种类型的工作流进行评估。总体而言，Cognify将这些工作流的生成质量提高了最多2.8倍，减少了执行成本最多10倍，并将端到端延迟减少了2.7倍。', 'title_zh': 'Cognify: 通过分层自调优增强生成式AI工作流'}
{'arxiv_id': 'arXiv:2502.08045', 'title': 'Break the Checkbox: Challenging Closed-Style Evaluations of Cultural Alignment in LLMs', 'authors': 'Mohsinul Kabir, Ajwad Abrar, Sophia Ananiadou', 'link': 'https://arxiv.org/abs/2502.08045', 'abstract': 'A large number of studies rely on closed-style multiple-choice surveys to evaluate cultural alignment in Large Language Models (LLMs). In this work, we challenge this constrained evaluation paradigm and explore more realistic, unconstrained approaches. Using the World Values Survey (WVS) and Hofstede Cultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger cultural alignment in less constrained settings, where responses are not forced. Additionally, we show that even minor changes, such as reordering survey choices, lead to inconsistent outputs, exposing the limitations of closed-style evaluations. Our findings advocate for more robust and flexible evaluation frameworks that focus on specific cultural proxies, encouraging more nuanced and accurate assessments of cultural alignment in LLMs.', 'abstract_zh': '大量的研究依赖封闭式多项选择调查来评估大型语言模型（LLMs）的文化一致性。在本文中，我们挑战这种有限的评估范式，并探索更为现实且不受约束的方法。通过世界价值观调查（WVS）和霍夫斯泰德文化维度作为案例研究，我们表明，在不受限制的环境中，LLMs在文化一致性方面表现更佳，其中响应不会受到强制。此外，我们还展示了即使是微小的变化，如重新排列调查选项，也会导致不一致的输出，揭示了封闭式评估的局限性。我们的研究结果提倡更加稳健和灵活的评估框架，重点关注特定的文化代理，以鼓励对LLMs文化一致性的更细腻和准确的评估。', 'title_zh': '打破选择框限制：挑战LLM文化对齐的封闭式评估方式'}
{'arxiv_id': 'arXiv:2502.08020', 'title': 'Speculate, then Collaborate: Fusing Knowledge of Language Models during Decoding', 'authors': 'Ziyao Wang, Muneeza Azmart, Ang Li, Raya Horesh, Mikhail Yurochkin', 'link': 'https://arxiv.org/abs/2502.08020', 'abstract': 'Large Language Models (LLMs) often excel in specific domains but fall short in others due to the limitations of their training. Thus, enabling LLMs to solve problems collaboratively by integrating their complementary knowledge promises to improve their performance across domains. To realize this potential, we introduce a novel Collaborative Speculative Decoding (CoSD) algorithm that enables efficient LLM knowledge fusion at test time without requiring additional model training. CoSD employs a draft model to generate initial sequences and an easy-to-learn rule or decision tree to decide when to invoke an assistant model to improve these drafts. CoSD not only enhances knowledge fusion but also improves inference efficiency, is transferable across domains and models, and offers greater explainability. Experimental results demonstrate that CoSD improves accuracy by up to 10\\% across benchmarks compared to existing methods, providing a scalable and effective solution for LLM-based applications', 'abstract_zh': '大型语言模型（LLMs）在特定领域往往表现出色，但在其他领域却可能表现不佳，这归因于它们训练的局限性。因此，通过集成它们互补的知识来使LLMs协作解决问题，有望提高其跨领域的性能。为实现这一潜力，我们引入了一种新颖的合作推测解码（CoSD）算法，该算法可以在不需要额外模型训练的情况下，在测试时实现高效的LLM知识融合。CoSD 使用一个草稿模型生成初始序列，并使用易于学习的规则或决策树决定何时调用助手模型以改进这些草稿。CoSD 不仅提高了知识融合的效率，还提升了推理效率，具有跨领域和模型的可迁移性，并提供了更高的可解释性。实验结果表明，与现有方法相比，CoSD 在基准测试中将准确性提高了高达10%，提供了一种可扩展且有效的基于LLM的应用解决方案。', 'title_zh': '推测而后协作：解码过程中融合语言模型的知识'}
{'arxiv_id': 'arXiv:2502.07985', 'title': 'MetaSC: Test-Time Safety Specification Optimization for Language Models', 'authors': 'Víctor Gallego', 'link': 'https://arxiv.org/abs/2502.07985', 'abstract': 'We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights. Building on recent advances in self-critique methods, our approach leverages a meta-critique mechanism that iteratively updates safety prompts-termed specifications-to drive the critique and revision process adaptively. This test-time optimization not only improves performance against adversarial jailbreak requests but also in diverse general safety-related tasks, such as avoiding moral harm or pursuing honest responses. Our empirical evaluations across several language models demonstrate that dynamically optimized safety prompts yield significantly higher safety scores compared to fixed system prompts and static self-critique defenses. Code to be released at this https URL .', 'abstract_zh': '我们提出了一种新颖的动态安全框架，在不修改模型权重的情况下优化语言模型的安全推理，在推理时进行测试时的优化。该方法基于最近在自我批判方法方面的进展，利用一个元批判机制，迭代更新称为规范的安全提示，以适应性地驱动批判和修订过程。这种测试时的优化不仅有助于抵御对抗性逃狱请求，还在避免道德伤害或追求诚实回应等多种一般性安全任务中提高了性能。我们对多个语言模型的实证评估表明，动态优化的安全提示相比固定系统提示和静态自我批判防御措施能够获得显著更高的安全评分。代码将在该网址发布：https://xxxxx。', 'title_zh': 'MetaSC：语言模型测试时安全规范优化'}
{'arxiv_id': 'arXiv:2502.07980', 'title': 'CIRCUIT: A Benchmark for Circuit Interpretation and Reasoning Capabilities of LLMs', 'authors': 'Lejla Skelic, Yan Xu, Matthew Cox, Wenjie Lu, Tao Yu, Ruonan Han', 'link': 'https://arxiv.org/abs/2502.07980', 'abstract': "The role of Large Language Models (LLMs) has not been extensively explored in analog circuit design, which could benefit from a reasoning-based approach that transcends traditional optimization techniques. In particular, despite their growing relevance, there are no benchmarks to assess LLMs' reasoning capability about circuits. Therefore, we created the CIRCUIT dataset consisting of 510 question-answer pairs spanning various levels of analog-circuit-related subjects. The best-performing model on our dataset, GPT-4o, achieves 48.04% accuracy when evaluated on the final numerical answer. To evaluate the robustness of LLMs on our dataset, we introduced a unique feature that enables unit-test-like evaluation by grouping questions into unit tests. In this case, GPT-4o can only pass 27.45% of the unit tests, highlighting that the most advanced LLMs still struggle with understanding circuits, which requires multi-level reasoning, particularly when involving circuit topologies. This circuit-specific benchmark highlights LLMs' limitations, offering valuable insights for advancing their application in analog integrated circuit design.", 'abstract_zh': '大型语言模型在模拟电路设计中的作用尚未得到充分探索，可以从超越传统优化技术的基于推理的方法中受益。尤其是，尽管它们的重要性日益增长，目前还没有评估大型语言模型在电路方面推理能力的基准。因此，我们创建了包含510个问题-答案对的CIRCUIT数据集，涵盖多个层次的模拟电路相关主题。在我们的数据集上表现最佳的模型GPT-4o在最终数值答案上的准确率为48.04%。为了评估我们在数据集上大型语言模型的鲁棒性，我们引入了一个独特特征，通过将问题分组为单元测试来实现类似单元测试的评估。在这种情况下，GPT-4o只能通过27.45%的单元测试，这表明最先进的大型语言模型在理解电路方面仍然存在困难，而理解电路往往需要多层面的推理，尤其是涉及电路拓扑结构时。这一电路专用基准突显了大型语言模型的局限性，为他们在模拟集成电路设计中的应用提供了宝贵见解。', 'title_zh': 'CIRCUIT：LLM电路解释与推理能力基准'}
{'arxiv_id': 'arXiv:2502.07974', 'title': 'From Hazard Identification to Controller Design: Proactive and LLM-Supported Safety Engineering for ML-Powered Systems', 'authors': 'Yining Hong, Christopher S. Timperley, Christian Kästner', 'link': 'https://arxiv.org/abs/2502.07974', 'abstract': 'Machine learning (ML) components are increasingly integrated into software products, yet their complexity and inherent uncertainty often lead to unintended and hazardous consequences, both for individuals and society at large. Despite these risks, practitioners seldom adopt proactive approaches to anticipate and mitigate hazards before they occur. Traditional safety engineering approaches, such as Failure Mode and Effects Analysis (FMEA) and System Theoretic Process Analysis (STPA), offer systematic frameworks for early risk identification but are rarely adopted. This position paper advocates for integrating hazard analysis into the development of any ML-powered software product and calls for greater support to make this process accessible to developers. By using large language models (LLMs) to partially automate a modified STPA process with human oversight at critical steps, we expect to address two key challenges: the heavy dependency on highly experienced safety engineering experts, and the time-consuming, labor-intensive nature of traditional hazard analysis, which often impedes its integration into real-world development workflows. We illustrate our approach with a running example, demonstrating that many seemingly unanticipated issues can, in fact, be anticipated.', 'abstract_zh': '机器学习(ML)组件日益集成到软件产品中，但其复杂性和固有的不确定性往往会导致个体和社会层面的意外和危害后果。尽管存在这些风险，从业者通常不采取积极措施在事故发生前预见和减轻危害。传统的安全工程方法，如故障模式和效果分析(FMEA)和系统理论过程分析(STPA)，提供了早期风险识别的系统框架，但这些方法很少被采用。本文倡导将危害分析整合到任何由机器学习驱动的软件产品的开发过程中，并呼吁提供更多的支持，使这一过程对开发者而言更加可访问。通过使用大规模语言模型(LLM)部分自动化修改后的STPA过程，并在关键步骤中进行人工监督，我们期望解决两个关键问题：对高度经验丰富的安全工程专家的严重依赖，以及传统危害分析耗时、劳动密集型的特点，这往往阻碍了其在实际开发工作流中的集成。我们通过一个运行示例来说明我们的方法，证明许多看似不可预见的问题实际上是可以预见的。', 'title_zh': '从风险识别到控制器设计：基于大语言模型的支持的主动安全工程方法用于机器学习驱动的系统'}
{'arxiv_id': 'arXiv:2502.07963', 'title': 'Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?', 'authors': 'Hye Sun Yun, Karen Y.C. Zhang, Ramez Kouzy, Iain J. Marshall, Junyi Jessy Li, Byron C. Wallace', 'link': 'https://arxiv.org/abs/2502.07963', 'abstract': 'Medical research faces well-documented challenges in translating novel treatments into clinical practice. Publishing incentives encourage researchers to present "positive" findings, even when empirical results are equivocal. Consequently, it is well-documented that authors often spin study results, especially in article abstracts. Such spin can influence clinician interpretation of evidence and may affect patient care decisions. In this study, we ask whether the interpretation of trial results offered by Large Language Models (LLMs) is similarly affected by spin. This is important since LLMs are increasingly being used to trawl through and synthesize published medical evidence. We evaluated 22 LLMs and found that they are across the board more susceptible to spin than humans. They might also propagate spin into their outputs: We find evidence, e.g., that LLMs implicitly incorporate spin into plain language summaries that they generate. We also find, however, that LLMs are generally capable of recognizing spin, and can be prompted in a way to mitigate spin\'s impact on LLM outputs.', 'abstract_zh': '医学研究在将新型治疗方法转化为临床实践方面面临已well-documented的挑战。发表激励促使研究人员呈现“正面”的研究结果，即使实证结果是模棱两可的。因此，已well-documented作者往往会在文章摘要中夸大研究成果，这种误导可能影响临床医生对证据的解读并可能影响患者的治疗决策。在这项研究中，我们探讨大型语言模型（LLMs）是否也会受到这种误导的影响，因为LLMs正越来越多地被用于筛选和综合已发表的医学证据。我们评估了22种LLM，并发现它们普遍比人类更容易受到误导的影响。它们也可能将误导融入到其输出中：我们发现证据表明，LLMs在生成的简洁语言摘要中隐含地包含了误导。不过，我们还发现，LLMs通常能够识别误导，并可以通过特定方式促使它们减轻误导对其输出的影响。', 'title_zh': '陷于文字的陷阱：大规模语言模型会在医学文献中被误导吗？'}
{'arxiv_id': 'arXiv:2502.07931', 'title': 'Educating a Responsible AI Workforce: Piloting a Curricular Module on AI Policy in a Graduate Machine Learning Course', 'authors': 'James Weichert, Hoda Eldardiry', 'link': 'https://arxiv.org/abs/2502.07931', 'abstract': "As artificial intelligence (AI) technologies begin to permeate diverse fields-from healthcare to education-consumers, researchers and policymakers are increasingly raising concerns about whether and how AI is regulated. It is therefore reasonable to anticipate that alignment with principles of 'ethical' or 'responsible' AI, as well as compliance with law and policy, will form an increasingly important part of AI development. Yet, for the most part, the conventional computer science curriculum is ill-equipped to prepare students for these challenges. To this end, we seek to explore how new educational content related to AI ethics and AI policy can be integrated into both ethics- and technical-focused courses. This paper describes a two-lecture 'AI policy module' that was piloted in a graduate-level introductory machine learning course in 2024. The module, which includes an in-class active learning game, is evaluated using data from student surveys before and after the lectures, and pedagogical motivations and considerations are discussed. We find that the module is successful in engaging otherwise technically-oriented students on the topic of AI policy, increasing student awareness of the social impacts of a variety of AI technologies and developing student interest in the field of AI regulation.", 'abstract_zh': '随着人工智能（AI）技术开始渗透到各个领域——从医疗到教育——消费者、研究人员和政策制定者越来越多地关注AI 的监管问题。因此，可以合理地预期，‘伦理’或‘负责任’的AI原则与法律法规的遵循将成为AI发展的一个越来越重要的部分。然而，常规的计算机科学课程往往无法为学生做好这些挑战的准备。为此，我们探索了如何将与AI伦理和AI政策相关的全新教育内容整合到伦理和技术导向的课程中。本文描述了2024年在一门研究生级入门机器学习课程中试行的两节课时的“AI政策模块”。该模块包括一个课堂互动学习游戏，并通过对学生课前和课后的调查数据进行评估，讨论了教学动机和考虑因素。我们发现，该模块有效地吸引了原本技术导向的学生对AI政策的关注，提高了学生对各种AI技术的社会影响的认识，并激发了学生对AI监管领域的兴趣。', 'title_zh': '培养负责任的AI workforce：在研究生机器学习课程中试点AI政策教学模块'}
{'arxiv_id': 'arXiv:2502.07864', 'title': 'TransMLA: Multi-head Latent Attention Is All You Need', 'authors': 'Fanxu Meng, Zengwei Yao, Muhan Zhang', 'link': 'https://arxiv.org/abs/2502.07864', 'abstract': 'Modern large language models (LLMs) often encounter communication bottlenecks on current hardware, rather than purely computational constraints. Multi-head Latent Attention (MLA) tackles this challenge by using low-rank matrices in the key-value (KV) layers, thereby allowing compressed latent KV states to be cached. This approach significantly reduces the KV cache size relative to traditional multi-head attention, leading to faster inference. Moreover, MLA employs an up-projection matrix to increase expressiveness, trading additional computation for reduced communication overhead. Although MLA has demonstrated efficiency and effectiveness in Deepseek V2/V3/R1, many major model providers still rely on Group Query Attention (GQA) and have not announced any plans to adopt MLA. In this paper, we show that GQA can always be represented by MLA while maintaining the same KV cache overhead, but the converse does not hold. To encourage broader use of MLA, we introduce **TransMLA**, a post-training method that converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen, Mixtral) into MLA-based models. After conversion, the model can undergo additional training to boost expressiveness without increasing the KV cache size. Furthermore, we plan to develop MLA-specific inference acceleration techniques to preserve low latency in transformed models, thus enabling more efficient distillation of Deepseek R1.', 'abstract_zh': '现代大型语言模型(Large Language Models, LLMs)在当前硬件上常常遇到通信瓶颈而非纯粹的计算约束。多头潜在注意(Multi-head Latent Attention, MLA)通过在键值(KV)层使用低秩矩阵来应对这一挑战，从而允许压缩的潜在KV状态被缓存。这种方法相对于传统的多头注意显著减小了KV缓存的大小，从而加快了推理速度。此外，MLA 通过使用上投影矩阵来增加表达能力，以减少通信开销为代价增加额外的计算。虽然 MLA 在 Deepseek V2/V3/R1 中展现了效率和有效性，但许多主要的模型提供商仍然依赖于组查询注意(Group Query Attention, GQA)，并且没有宣布任何采用 MLA 的计划。在本文中，我们证明 GQA 总是可以被 MLA 表示同时保持相同的 KV 缓存开销，但反之则不成立。为了促进 MLA 的更广泛使用，我们介绍了 **TransMLA**，这是一种后训练方法，可以将广泛使用的基于 GQA 的预训练模型（例如 LLaMA、Qwen、Mixtral）转换为基于 MLA 的模型。转换后，该模型可以在不增加 KV 缓存大小的情况下进行额外训练以提升表达能力。此外，我们计划开发 MLA 特定的推理加速技术以在转换后的模型中保持低延迟，从而实现 Deepseek R1 的更高效蒸馏。', 'title_zh': 'TransMLA: 多头潜在注意力机制即所需的一切'}
{'arxiv_id': 'arXiv:2502.07861', 'title': 'BalanceKV: KV Cache Compression through Discrepancy Theory', 'authors': 'Insu Han, Michael Kapralov, Ekaterina Kochetkova, Kshiteej Sheth, Amir Zandieh', 'link': 'https://arxiv.org/abs/2502.07861', 'abstract': "Large language models (LLMs) have achieved impressive success, but their high memory requirements present challenges for long-context token generation. The memory complexity of long-context LLMs is primarily due to the need to store Key-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache compression method based on geometric sampling process stemming from Banaszczyk's vector balancing theory, which introduces dependencies informed by the geometry of keys and value tokens, and improves precision. BalanceKV offers both theoretically proven and empirically validated performance improvements over existing methods.", 'abstract_zh': '基于Banachczek向量平衡理论的几何采样过程的BalanceKV：一种KV缓存压缩方法', 'title_zh': 'BalanceKV：通过差异理论实现的KV缓存压缩'}
{'arxiv_id': 'arXiv:2502.07838', 'title': 'NanoVLMs: How small can we go and still make coherent Vision Language Models?', 'authors': 'Mukund Agarwalla, Himanshu Kumar, Raj Dandekar, Rajat Dandekar, Sreedath Panat', 'link': 'https://arxiv.org/abs/2502.07838', 'abstract': 'Vision-Language Models (VLMs), such as GPT-4V and Llama 3.2 vision, have garnered significant research attention for their ability to leverage Large Language Models (LLMs) in multimodal tasks. However, their potential is constrained by inherent challenges, including proprietary restrictions, substantial computational demands, and limited accessibility. Smaller models, such as GIT and BLIP, exhibit marked limitations, often failing to generate coherent and consistent text beyond a few tokens, even with extensive training. This underscores a pivotal inquiry: how small can a VLM be and still produce fluent and consistent text? Drawing inspiration from the exceptional learning process of 3-4 year old children, who rely heavily on visual cues for understanding and communication, we introduce two novel datasets: ShortDesc (featuring concise image descriptions) and LongDesc (containing more detailed image descriptions). These datasets consist of image-text pairs where the text is restricted to the simple vocabulary and syntax typically used by young children, generated with a scaled- down model, GPT-4o. Using these datasets, we demonstrate that it is possible to train VLMs that are significantly smaller, up to 10 times smaller than state of the art(SOTA) small VLMs while maintaining architectural simplicity. To evaluate the outputs, we leverage GPT-4o to grade the text, as if stories written by students, on creativity, meaningfulness, and consistency, assigning scores out of 10. This method addresses limitations of standard benchmarks by accommodating unstructured outputs and providing a multidimensional evaluation of the model capabilities. Our findings contribute to the development of lightweight, accessible multimodal models for resource constrained environments.', 'abstract_zh': 'Vision-Language 模型（VLMs），如 GPT-4V 和 Llama 3.2 视觉版本，因其能够利用大型语言模型（LLMs）处理多模态任务而引起了广泛关注。然而，它们的潜力受制于固有的挑战，包括产权限制、巨大的计算需求以及有限的可访问性。较小的模型，如 GIT 和 BLIP，表现出明显的局限性，即使经过大量训练，往往也无法生成连贯且一致的文本，甚至在几个标记内也是如此。这突显了一个关键问题：一个 VLM 最小可以多小，仍然能够生成流畅且一致的文本？受 3-4 岁儿童卓越学习过程的启发，他们很大程度上依赖于视觉线索来进行理解和交流，我们提出了两个新的数据集：ShortDesc（简短的图像描述）和 LongDesc（详细的图像描述）。这些数据集包含图像-文本对，其中文本仅限制在年轻儿童通常使用的简单词汇和语法，由一个缩小版模型 GPT-4o 生成。利用这些数据集，我们证明了有可能训练出显著更小的 VLMs，最多可小 10 倍于现有最佳小 VLM，同时保持架构的简洁性。为了评估输出，我们利用 GPT-4o 以类似评估学生写作的方式对文本进行评分，评估创建性、意义性和一致性，并给出 10 分制评分。这种方法克服了标准基准的局限性，能够容纳非结构化输出，并提供对模型能力多维度评估。我们的发现为资源受限环境下的轻量级、可访问的多模态模型的发展做出了贡献。', 'title_zh': 'NanoVLMs: 我们能制造出多小的仍然可以生成连贯的视觉语言模型？'}
{'arxiv_id': 'arXiv:2502.07835', 'title': 'Bridging LLM-Generated Code and Requirements: Reverse Generation technique and SBC Metric for Developer Insights', 'authors': 'Ahilan Ayyachamy Nadar Ponnusamy', 'link': 'https://arxiv.org/abs/2502.07835', 'abstract': 'The rise of Large Language Models (LLMs) in software engineering, particularly in code generation, has garnered significant attention. However, assessing the quality of AI-generated code remains a challenge due to the inherent complexity of programming tasks and the lack of robust evaluation metrics that align well with human judgment. Traditional token-based metrics such as BLEU and ROUGE, while commonly used in natural language processing, exhibit weak correlations with human assessments in code intelligence and verification tasks. Furthermore, these metrics are primarily research focused and are not designed for seamless integration into the software development lifecycle, limiting their practical utility for developers seeking to improve code quality and security.\nAI-assisted coding has been shown to be more beneficial for senior developers, as they possess the expertise to critically evaluate the generated code for correctness, completeness, and compliance. In contrast, junior developers may struggle to identify hallucinations, missing functionality, or incorrect logic in AI-generated code. To bridge this gap, This paper introduces a novel scoring mechanism called the SBC score, which is based on a reverse generation technique that leverages the natural language generation capabilities of LLMs. Unlike direct code analysis, our approach reconstructs system requirements from AI-generated code and compares them with the original specifications to quantify accuracy. The SBC score combines semantic similarity, BLEU, and completeness analysis, providing actionable insights to developers by highlighting missing features and hallucinations. Our code and datasets are available on GitHub', 'abstract_zh': '大型语言模型（LLMs）在软件工程中的兴起，尤其是在代码生成领域的应用，引起了广泛关注。然而，评估AI生成代码的质量仍然是一个挑战，这主要是由于编程任务的固有复杂性以及缺乏与人类判断相匹配的稳健评估指标。传统的基于token的指标，如BLEU和ROUGE，在自然语言处理中广泛应用，但在代码智能和验证任务中与人类评估的关联性较弱。此外，这些指标主要侧重于研究，未能设计为无缝集成到软件开发生命周期中，限制了它们在开发人员提高代码质量和安全性方面的作用。\n\nAI辅助编码对资深开发人员更有益，因为资深开发人员拥有批判性评估生成代码正确性、完整性和合规性的专业知识。相比之下，初级开发人员可能难以识别AI生成代码中的幻觉、缺少功能或逻辑错误。为弥合这一差距，本文引入了一种新的评分机制——SBC评分，该机制基于一种逆向生成技术，利用大型语言模型的自然语言生成能力。与直接的代码分析不同，我们方法是从AI生成的代码中重构系统需求，并将其与原始规范进行比较，以量化准确性。SBC评分结合了语义相似性、BLEU和完整性分析，通过突出显示缺失功能和幻觉为开发人员提供可操作的见解。我们的代码和数据集可在GitHub上获取。', 'title_zh': 'LLM生成代码与需求 bridging：反向生成技术及SBC指标为开发者提供洞见'}
{'arxiv_id': 'arXiv:2502.07832', 'title': 'SHARP: Accelerating Language Model Inference by SHaring Adjacent layers with Recovery Parameters', 'authors': 'Yiping Wang, Hanxian Huang, Yifang Chen, Jishen Zhao, Simon Shaolei Du, Yuandong Tian', 'link': 'https://arxiv.org/abs/2502.07832', 'abstract': "While Large language models (LLMs) have advanced natural language processing tasks, their growing computational and memory demands make deployment on resource-constrained devices like mobile phones increasingly challenging. In this paper, we propose SHARP (SHaring Adjacent Layers with Recovery Parameters), a novel approach to accelerate LLM inference by sharing parameters across adjacent layers, thus reducing memory load overhead, while introducing low-rank recovery parameters to maintain performance. Inspired by observations that consecutive layers have similar outputs, SHARP employs a two-stage recovery process: Single Layer Warmup (SLW), and Supervised Fine-Tuning (SFT). The SLW stage aligns the outputs of the shared layers using L_2 loss, providing a good initialization for the following SFT stage to further restore the model performance. Extensive experiments demonstrate that SHARP can recover the model's perplexity on various in-distribution tasks using no more than 50k fine-tuning data while reducing the number of stored MLP parameters by 38% to 65%. We also conduct several ablation studies of SHARP and show that replacing layers towards the later parts of the model yields better performance retention, and that different recovery parameterizations perform similarly when parameter counts are matched. Furthermore, SHARP saves 42.8% in model storage and reduces the total inference time by 42.2% compared to the original Llama2-7b model on mobile devices. Our results highlight SHARP as an efficient solution for reducing inference costs in deploying LLMs without the need for pretraining-scale resources.", 'abstract_zh': 'SHARP：通过共享相邻层参数实现语言模型推理加速与性能恢复', 'title_zh': 'SHARP: 通过共享相邻层并恢复参数加速语言模型推理'}
{'arxiv_id': 'arXiv:2502.07827', 'title': 'Implicit Language Models are RNNs: Balancing Parallelization and Expressivity', 'authors': 'Mark Schöne, Babak Rahmani, Heiner Kremer, Fabian Falck, Hitesh Ballani, Jannes Gladrow', 'link': 'https://arxiv.org/abs/2502.07827', 'abstract': 'State-space models (SSMs) and transformers dominate the language modeling landscape. However, they are constrained to a lower computational complexity than classical recurrent neural networks (RNNs), limiting their expressivity. In contrast, RNNs lack parallelization during training, raising fundamental questions about the trade off between parallelization and expressivity. We propose implicit SSMs, which iterate a transformation until convergence to a fixed point. Theoretically, we show that implicit SSMs implement the non-linear state-transitions of RNNs. Empirically, we find that only approximate fixed-point convergence suffices, enabling the design of a scalable training curriculum that largely retains parallelization, with full convergence required only for a small subset of tokens. Our approach demonstrates superior state-tracking capabilities on regular languages, surpassing transformers and SSMs. We further scale implicit SSMs to natural language reasoning tasks and pretraining of large-scale language models up to 1.3B parameters on 207B tokens - representing, to our knowledge, the largest implicit model trained to date. Notably, our implicit models outperform their explicit counterparts on standard benchmarks.', 'abstract_zh': '隐状态模型和变压器主导了语言建模领域。然而，它们在计算复杂性上受到限制，不及传统的循环神经网络（RNNs），限制了其表达能力。相比之下，RNNs在训练过程中缺乏并行化，提出了并行化与表达能力之间基本权衡问题。我们提出了隐式状态模型（Implicit State-Space Models, ISSMs），其通过迭代变换直到收敛到不动点。理论上，我们证明了隐式状态模型实现了RNNs的非线性状态转换。实验上，我们发现仅需近似的不动点收敛即可，这使得我们可以设计一个可扩展的训练课程，大幅保留并行化能力，仅对一小部分标记要求完全收敛。我们的方法在正规语言上展示了卓越的状态追踪能力，超越了变压器和隐状态模型。我们进一步将隐式状态模型扩展到自然语言推理任务和大型语言模型的预训练，其中参数量高达13亿，标记量达到2070亿，据我们所知，这是迄今为止训练的最大隐式模型。值得注意的是，我们的隐式模型在标准基准测试中优于其显式对应模型。', 'title_zh': '隐含语言模型是RNN：权衡并行性和表达性'}
{'arxiv_id': 'arXiv:2502.07813', 'title': 'CryptoX : Compositional Reasoning Evaluation of Large Language Models', 'authors': 'Jiajun Shi, Chaoren Wei, Liqun Yang, Zekun Moore Wang, Chenghao Yang, Ge Zhang, Stephen Huang, Tao Peng, Jian Yang, Zhoufutu Wen', 'link': 'https://arxiv.org/abs/2502.07813', 'abstract': "The compositional reasoning capacity has long been regarded as critical to the generalization and intelligence emergence of large language models LLMs. However, despite numerous reasoning-related benchmarks, the compositional reasoning capacity of LLMs is rarely studied or quantified in the existing benchmarks. In this paper, we introduce CryptoX, an evaluation framework that, for the first time, combines existing benchmarks and cryptographic, to quantify the compositional reasoning capacity of LLMs. Building upon CryptoX, we construct CryptoBench, which integrates these principles into several benchmarks for systematic evaluation. We conduct detailed experiments on widely used open-source and closed-source LLMs using CryptoBench, revealing a huge gap between open-source and closed-source LLMs. We further conduct thorough mechanical interpretability experiments to reveal the inner mechanism of LLMs' compositional reasoning, involving subproblem decomposition, subproblem inference, and summarizing subproblem conclusions. Through analysis based on CryptoBench, we highlight the value of independently studying compositional reasoning and emphasize the need to enhance the compositional reasoning capabilities of LLMs.", 'abstract_zh': 'CryptoX：一种结合现有基准和加密技术量化大型语言模型组合推理能力的评估框架', 'title_zh': 'CryptoX : 大型语言模型组合 reasoning 评估'}
{'arxiv_id': 'arXiv:2502.07794', 'title': 'Regulatory Science Innovation for Generative AI and Large Language Models in Health and Medicine: A Global Call for Action', 'authors': 'Jasmine Chiat Ling Ong, Yilin Ning, Mingxuan Liu, Yian Ma, Zhao Liang, Kuldev Singh, Robert T Chang, Silke Vogel, John CW Lim, Iris Siu Kwan Tan, Oscar Freyer, Stephen Gilbert, Danielle S Bitterman, Xiaoxuan Liu, Alastair K Denniston, Nan Liu', 'link': 'https://arxiv.org/abs/2502.07794', 'abstract': 'The integration of generative AI (GenAI) and large language models (LLMs) in healthcare presents both unprecedented opportunities and challenges, necessitating innovative regulatory approaches. GenAI and LLMs offer broad applications, from automating clinical workflows to personalizing diagnostics. However, the non-deterministic outputs, broad functionalities and complex integration of GenAI and LLMs challenge existing medical device regulatory frameworks, including the total product life cycle (TPLC) approach. Here we discuss the constraints of the TPLC approach to GenAI and LLM-based medical device regulation, and advocate for global collaboration in regulatory science research. This serves as the foundation for developing innovative approaches including adaptive policies and regulatory sandboxes, to test and refine governance in real-world settings. International harmonization, as seen with the International Medical Device Regulators Forum, is essential to manage implications of LLM on global health, including risks of widening health inequities driven by inherent model biases. By engaging multidisciplinary expertise, prioritizing iterative, data-driven approaches, and focusing on the needs of diverse populations, global regulatory science research enables the responsible and equitable advancement of LLM innovations in healthcare.', 'abstract_zh': '生成式人工智能与大规模语言模型在 Healthcare 领域的融合带来了前所未有的机遇与挑战，需要创新的监管方法。', 'title_zh': '生成人工智能和大型语言模型在健康医疗领域的监管科学创新：全球行动倡议'}
{'arxiv_id': 'arXiv:2502.07790', 'title': 'Can Generative AI be Egalitarian?', 'authors': 'Philip Feldman, James R. Foulds, Shimei Pan', 'link': 'https://arxiv.org/abs/2502.07790', 'abstract': 'The recent explosion of "foundation" generative AI models has been built upon the extensive extraction of value from online sources, often without corresponding reciprocation. This pattern mirrors and intensifies the extractive practices of surveillance capitalism, while the potential for enormous profit has challenged technology organizations\' commitments to responsible AI practices, raising significant ethical and societal concerns. However, a promising alternative is emerging: the development of models that rely on content willingly and collaboratively provided by users. This article explores this "egalitarian" approach to generative AI, taking inspiration from the successful model of Wikipedia. We explore the potential implications of this approach for the design, development, and constraints of future foundation models. We argue that such an approach is not only ethically sound but may also lead to models that are more responsive to user needs, more diverse in their training data, and ultimately more aligned with societal values. Furthermore, we explore potential challenges and limitations of this approach, including issues of scalability, quality control, and potential biases inherent in volunteer-contributed content.', 'abstract_zh': '近期“基础”生成型AI模型的爆炸式增长建立在对在线源的广泛价值提取之上，通常缺乏相应的回馈。这种模式反映了并加剧了监控资本主义的剥削实践，同时巨大的利润潜力挑战了技术组织负责任的AI实践承诺，引发了重大的伦理和社会关切。然而，一种有希望的替代方案正在出现：依赖用户自愿和合作提供的内容的模型开发。本文探讨这种“平等主义”生成型AI的方法，以维基百科的成功模式为 inspiration。我们探讨了这种方法对未来基础模型的设计、开发及其约束条件的潜在影响。我们认为，这种做法不仅是道德上合理的，还可能导致更响应用户需求、训练数据更具多样性的模型，并最终与社会价值更加一致。此外，我们还探讨了这种方法可能面临的挑战和限制，包括可扩展性、质量控制以及志愿者贡献内容中固有的偏见问题。', 'title_zh': '生成式AI能实现平等吗？'}
{'arxiv_id': 'arXiv:2502.07786', 'title': 'Counterexample Guided Program Repair Using Zero-Shot Learning and MaxSAT-based Fault Localization', 'authors': 'Pedro Orvalho, Mikoláš Janota, Vasco Manquinho', 'link': 'https://arxiv.org/abs/2502.07786', 'abstract': "Automated Program Repair (APR) for introductory programming assignments (IPAs) is motivated by the large number of student enrollments in programming courses each year. Since providing feedback on IPAs requires substantial time and effort from faculty, personalized feedback often involves suggesting fixes to students' programs. Formal Methods (FM)-based semantic repair approaches, check a program's execution against a test suite or reference solution, are effective but limited. These tools excel at identifying buggy parts but can only fix programs if the correct implementation and the faulty one share the same control flow graph. Conversely, Large Language Models (LLMs) are used for APR but often make extensive instead of minimal rewrites. This leads to more invasive fixes, making it harder for students to learn from their mistakes. In summary, LLMs excel at completing strings, while FM-based fault localization excel at identifying buggy parts of a program. In this paper, we propose a novel approach that combines the strengths of both FM-based fault localization and LLMs, via zero-shot learning, to enhance APR for IPAs. Our method uses MaxSAT-based fault localization to identify buggy parts of a program, then presents the LLM with a program sketch devoid of these buggy statements. This hybrid approach follows a CEGIS loop to iteratively refine the program. We ask the LLM to synthesize the missing parts, which are then checked against a test suite. If the suggested program is incorrect, a counterexample from the test suite is fed back to the LLM. Our experiments show that our counterexample guided approach, using MaxSAT-based bug-free program sketches, significantly improves the repair capabilities of all six evaluated LLMs. This method allows LLMs to repair more programs with smaller fixes, outperforming other configurations and state-of-the-art symbolic program repair tools.", 'abstract_zh': '基于自动程序修复的元编程作业初步编程作业的自动化程序修复（APR）：面向大量报名编程课程的学生推动了这一领域的研究。由于为初步编程作业提供反馈需要教员投入大量时间和精力，个性化反馈常常涉及建议学生修改程序。基于形式方法（FM）的语义修复方法通过检查程序执行与测试套件或参考解决方案一致来实现这一目的，这些方法非常有效但也有局限性。它们擅长识别有故障的部分，但只能在正确实现和有故障的实现共享相同控制流图时修复程序。相反，大规模语言模型（LLMs）被用于程序修复，但往往进行大规模修改而非最小化修改。这导致了更侵入性的修复，使得学生更难从中学习错误。总体而言，LLMs 在完成字符串方面表现出色，而基于FM的故障本地化则在识别程序中的有故障部分方面表现出色。本文提出了一种结合基于FM的故障本地化和LLMs优势的新方法，通过零样本学习来增强初步编程作业的程序修复能力。该方法使用基于MaxSAT的故障本地化来识别程序中的有故障部分，然后向LLM提供一个没有这些有故障语句的程序草图。该混合方法遵循一种CEGIS循环，逐步优化程序。我们要求LLM合成效缺乏的部分，然后这些部分与测试套件进行验证。如果建议的程序不正确，则从测试套件中返回反例供LLM反馈。我们的实验结果表明，使用基于MaxSAT的无故障程序草图和反例指导的方法，显著提高了所有六种评估的LLMs的修复能力。该方法使LLMs能够用更小的修改修复更多的程序，优于其他配置和其他最先进的符号程序修复工具。', 'title_zh': '使用零_shot学习和MaxSAT为基础的故障定位的反例引导程序修复'}
