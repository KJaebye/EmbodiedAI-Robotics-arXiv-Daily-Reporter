# AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset 

**Title (ZH)**: AIMO-2 获胜方案：基于 OpenMathReasoning 数据集构建领先数学推理模型 

**Authors**: Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schifferer, Wei Du, Igor Gitman  

**Link**: [PDF](https://arxiv.org/pdf/2504.16891)  

**Abstract**: This paper presents our winning submission to the AI Mathematical Olympiad - Progress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art mathematical reasoning models relies on three key pillars. First, we create a large-scale dataset comprising 540K unique high-quality math problems, including olympiad-level problems, and their 3.2M long-reasoning solutions. Second, we develop a novel method to integrate code execution with long reasoning models through iterative training, generation, and quality filtering, resulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we create a pipeline to train models to select the most promising solution from many candidates. We show that such generative solution selection (GenSelect) can significantly improve upon majority voting baseline. Combining these ideas, we train a series of models that achieve state-of-the-art results on mathematical reasoning benchmarks. To facilitate further research, we release our code, models, and the complete OpenMathReasoning dataset under a commercially permissive license. 

**Abstract (ZH)**: 本文呈现了我们参加AI数学奥林匹克竞赛-进展奖（AIMO-2）的获奖提交。构建最先进的数学推理模型的关键在于三个方面。首先，我们构建了一个包含54万个高质量数学问题的大规模数据集，其中包括奥林匹克级别的问题及其320万字长推理解决方案。其次，我们开发了一种新颖方法，通过迭代训练、生成和质量过滤，将代码执行与长推理模型集成，生成170万高质量工具集成推理解决方案。第三，我们创建了一个流水线，用于训练模型从众多候选方案中选择最有潜力的方案。我们表明，生成性解决方案选择（GenSelect）可以显著优于多数投票基准。结合这些想法，我们训练了一系列模型，在数学推理基准测试中达到了最先进的成果。为了促进进一步的研究，我们以商业友好的许可证发布了我们的代码、模型和完整的OpenMathReasoning数据集。 

---
# Lightweight Latent Verifiers for Efficient Meta-Generation Strategies 

**Title (ZH)**: 轻量级潜在验证器以实现高效的元生成策略 

**Authors**: Bartosz Piotrowski, Witold Drzewakowski, Konrad Staniszewski, Piotr Miłoś  

**Link**: [PDF](https://arxiv.org/pdf/2504.16760)  

**Abstract**: Verifiers are auxiliary models that assess the correctness of outputs generated by base large language models (LLMs). They play a crucial role in many strategies for solving reasoning-intensive problems with LLMs. Typically, verifiers are LLMs themselves, often as large (or larger) than the base model they support, making them computationally expensive. In this work, we introduce a novel lightweight verification approach, LiLaVe, which reliably extracts correctness signals from the hidden states of the base LLM. A key advantage of LiLaVe is its ability to operate with only a small fraction of the computational budget required by traditional LLM-based verifiers. To demonstrate its practicality, we couple LiLaVe with popular meta-generation strategies, like best-of-n or self-consistency. Moreover, we design novel LiLaVe-based approaches, like conditional self-correction or conditional majority voting, that significantly improve both accuracy and efficiency in generation tasks with smaller LLMs. Our work demonstrates the fruitfulness of extracting latent information from the hidden states of LLMs, and opens the door to scalable and resource-efficient solutions for reasoning-intensive applications. 

**Abstract (ZH)**: 验证器是辅助模型，用以评估基础大语言模型（LLMs）生成输出的正确性。它们在使用LLMs解决推理密集型问题的各种策略中扮演着至关重要的角色。通常，验证器本身就是LLMs，往往比它们支持的基础模型更大（或更大），使其在计算上非常昂贵。在本文中，我们介绍了一种新颖的轻量级验证方法LiLaVe，它可以可靠地从基础LLM的隐藏状态中提取正确性信号。LiLaVe的一个关键优势是，它可以仅使用传统基于LLM的验证器所需计算预算的一小部分来运行。为了证明其实用性，我们将LiLaVe与流行的元生成策略（如最佳n选一或自一致性）结合使用。此外，我们设计了基于LiLaVe的新颖方法，如条件自我纠正或条件多数表决，这些方法在使用小型LLM进行生成任务时显著提高了准确性和效率。我们的工作证明了从LLM的隐藏状态中提取潜在信息的有效性，并为推理密集型应用提供了可扩展且资源高效的解决方案。 

---
# A Survey of AI Agent Protocols 

**Title (ZH)**: AI代理协议综述 

**Authors**: Yingxuan Yang, Huacan Chai, Yuanyi Song, Siyuan Qi, Muning Wen, Ning Li, Junwei Liao, Haoyi Hu, Jianghao Lin, Gaowei Chang, Weiwen Liu, Ying Wen, Yong Yu, Weinan Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2504.16736)  

**Abstract**: The rapid development of large language models (LLMs) has led to the widespread deployment of LLM agents across diverse industries, including customer service, content generation, data analysis, and even healthcare. However, as more LLM agents are deployed, a major issue has emerged: there is no standard way for these agents to communicate with external tools or data sources. This lack of standardized protocols makes it difficult for agents to work together or scale effectively, and it limits their ability to tackle complex, real-world tasks. A unified communication protocol for LLM agents could change this. It would allow agents and tools to interact more smoothly, encourage collaboration, and triggering the formation of collective intelligence. In this paper, we provide a systematic overview of existing communication protocols for LLM agents. We classify them into four main categories and make an analysis to help users and developers select the most suitable protocols for specific applications. Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency. Finally, we explore future challenges, such as how protocols can adapt and survive in fast-evolving environments, and what qualities future protocols might need to support the next generation of LLM agent ecosystems. We expect this work to serve as a practical reference for both researchers and engineers seeking to design, evaluate, or integrate robust communication infrastructures for intelligent agents. 

**Abstract (ZH)**: 大型语言模型快速发展的背景下，这些模型的代理已在客户服务、内容生成、数据分析乃至医疗保健等多个行业得到了广泛应用。然而，随着更多代理的部署，出现了一个主要问题：这些代理没有标准化的方式与外部工具或数据源进行通信。缺乏标准化的协议使得代理难以协同工作或有效扩展，并限制了它们解决复杂现实任务的能力。为LLM代理制定统一的通信协议可以改变这一状况。该协议将使代理和工具能够更顺畅地交互，促进合作，并促使集体智能的形成。本文系统地概述了现有的LLM代理通信协议，将其分为四大类，并进行分析以帮助用户和开发人员选择最适合特定应用的协议。此外，我们还从安全性、可扩展性和延迟等关键维度对这些协议进行了比较性能分析。最后，我们探讨了未来挑战，如协议如何适应和在快速变化的环境中生存，以及未来协议可能需要具备哪些特性以支持新一代LLM代理生态系统。我们期望这项工作能够为研究人员和工程师提供一个实用的参考，用于设计、评估或集成用于智能代理的强大通信基础设施。 

---
# IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery 

**Title (ZH)**: IRIS: 交互式研究思路系统，用于加速科学发现 

**Authors**: Aniketh Garikaparthi, Manasi Patwardhan, Lovekesh Vig, Arman Cohan  

**Link**: [PDF](https://arxiv.org/pdf/2504.16728)  

**Abstract**: The rapid advancement in capabilities of large language models (LLMs) raises a pivotal question: How can LLMs accelerate scientific discovery? This work tackles the crucial first stage of research, generating novel hypotheses. While recent work on automated hypothesis generation focuses on multi-agent frameworks and extending test-time compute, none of the approaches effectively incorporate transparency and steerability through a synergistic Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS: Interactive Research Ideation System, an open-source platform designed for researchers to leverage LLM-assisted scientific ideation. IRIS incorporates innovative features to enhance ideation, including adaptive test-time compute expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism, and query-based literature synthesis. Designed to empower researchers with greater control and insight throughout the ideation process. We additionally conduct a user study with researchers across diverse disciplines, validating the effectiveness of our system in enhancing ideation. We open-source our code at this https URL 

**Abstract (ZH)**: 大型语言模型（LLMs）能力的 rapid advancement 产生了关键问题：LLMs 如何度量加速科学发现？本文解决研究的关键第一阶段，即生成新颖假设。尽管 recent work 在自动假设生成方面侧重于多智能体框架和扩展测试时计算资源，但 none of these approaches 有效通过协同的人机协同（Human-in-the-loop, HITL）方式整合透明度和可控性。为填补这一空白，我们提出 IRIS：交互式研究构思系统，一个旨在供研究人员利用 LLM 辅助的科学研究构思的开源平台。IRIS 结合了增强构思的创新功能，包括通过蒙特卡洛树搜索（MCTS）实现自适应测试时计算资源扩展、细粒度反馈机制以及基于查询的文献整合。旨在赋予研究人员在整个构思过程中更大的控制力和洞察力。我们还对来自不同学科的研究人员进行了用户研究，验证了系统的有效性，能够在构思过程中提高效率。我们的代码在以下链接开源：this https URL 

---
# Bridging Econometrics and AI: VaR Estimation via Reinforcement Learning and GARCH Models 

**Title (ZH)**: 经济计量学与人工智能的桥梁：基于强化学习和GARCH模型的VaR估计 

**Authors**: Fredy Pokou, Jules Sadefo Kamdem, François Benhmad  

**Link**: [PDF](https://arxiv.org/pdf/2504.16635)  

**Abstract**: In an environment of increasingly volatile financial markets, the accurate estimation of risk remains a major challenge. Traditional econometric models, such as GARCH and its variants, are based on assumptions that are often too rigid to adapt to the complexity of the current market dynamics. To overcome these limitations, we propose a hybrid framework for Value-at-Risk (VaR) estimation, combining GARCH volatility models with deep reinforcement learning. Our approach incorporates directional market forecasting using the Double Deep Q-Network (DDQN) model, treating the task as an imbalanced classification problem. This architecture enables the dynamic adjustment of risk-level forecasts according to market conditions. Empirical validation on daily Eurostoxx 50 data covering periods of crisis and high volatility shows a significant improvement in the accuracy of VaR estimates, as well as a reduction in the number of breaches and also in capital requirements, while respecting regulatory risk thresholds. The ability of the model to adjust risk levels in real time reinforces its relevance to modern and proactive risk management. 

**Abstract (ZH)**: 在日益波动的金融市场环境中，风险准确估计仍然是一个重大挑战。传统的经济计量模型，如GARCH及其变体，基于的假设往往过于僵化，难以适应当前市场动态的复杂性。为克服这些局限，我们提出了一个VaR估计的混合框架，结合了GARCH波动模型与深度强化学习。该方法使用双深Q网络（DDQN）模型进行市场方向预测，将任务视为不平衡分类问题。该架构能够根据市场条件动态调整风险水平预测。实证结果表明，该方法在危机时期和高波动期的日Eurostoxx 50数据上的VaR估计准确性有显著提高，减少了违约次数和资本要求，同时遵守监管风险阈值。模型能够实时调整风险水平，增强了其对现代和前瞻风险管理的相关性。 

---
# Cognitive Silicon: An Architectural Blueprint for Post-Industrial Computing Systems 

**Title (ZH)**: 认知硅片：后工业计算系统架构设计蓝图 

**Authors**: Christoforus Yoga Haryanto, Emily Lomempow  

**Link**: [PDF](https://arxiv.org/pdf/2504.16622)  

**Abstract**: Autonomous AI systems reveal foundational limitations in deterministic, human-authored computing architectures. This paper presents Cognitive Silicon: a hypothetical full-stack architectural framework projected toward 2035, exploring a possible trajectory for cognitive computing system design. The proposed architecture would integrate symbolic scaffolding, governed memory, runtime moral coherence, and alignment-aware execution across silicon-to-semantics layers. Our design grammar has emerged from dialectical co-design with LLMs under asymmetric epistemic conditions--creating structured friction to expose blind spots and trade-offs. The envisioned framework would establish mortality as a natural consequence of physical constraints, non-copyable tacit knowledge, and non-cloneable identity keys as cognitive-embodiment primitives. Core tensions (trust/agency, scaffolding/emergence, execution/governance) would function as central architectural pressures rather than edge cases. The architecture theoretically converges with the Free Energy Principle, potentially offering a formal account of how cognitive systems could maintain identity through prediction error minimization across physical and computational boundaries. The resulting framework aims to deliver a morally tractable cognitive infrastructure that could maintain human-alignment through irreversible hardware constraints and identity-bound epistemic mechanisms resistant to replication or subversion. 

**Abstract (ZH)**: 自主人工智能系统揭示了确定性、由人类编写的计算架构的基本局限性。本文提出了认知硅：一种面向2035年的设想全栈架构框架，探索可能的认知计算系统设计轨迹。提出的架构将结合符号支撑、管理记忆、运行时道德一致性以及对齐感知执行，贯穿硅至语义层。我们的设计语法源自与LLMs在不对称认知条件下进行的辩证协同设计——创造结构化的摩擦力以揭示盲点和权衡。所设想的框架将死亡视为物理约束的自然结果，非可复制的暗知识以及非克隆身份密钥作为认知-实体的基础。核心张力（信任/自主性、支撑/涌现、执行/治理）将作为主要的架构压力，而非边缘案例。该架构理论上与自由能量原理一致，可能提供一种形式化的解释，说明认知系统如何通过最小化预测误差在物理和计算边界上维持身份。最终框架旨在提供一种道德上可处理的认知基础设施，能够通过不可逆的硬件约束和基于身份的认知机制维持人类对齐，这些机制对复制或操纵具有抵抗力。 

---
# Investigating LLMs in Clinical Triage: Promising Capabilities, Persistent Intersectional Biases 

**Title (ZH)**: 探究大型语言模型在临床分流中的应用：有潜力的能力与持续存在的交叉偏见 

**Authors**: Joseph Lee, Tianqi Shang, Jae Young Baik, Duy Duong-Tran, Shu Yang, Lingyao Li, Li Shen  

**Link**: [PDF](https://arxiv.org/pdf/2504.16273)  

**Abstract**: Large Language Models (LLMs) have shown promise in clinical decision support, yet their application to triage remains underexplored. We systematically investigate the capabilities of LLMs in emergency department triage through two key dimensions: (1) robustness to distribution shifts and missing data, and (2) counterfactual analysis of intersectional biases across sex and race. We assess multiple LLM-based approaches, ranging from continued pre-training to in-context learning, as well as machine learning approaches. Our results indicate that LLMs exhibit superior robustness, and we investigate the key factors contributing to the promising LLM-based approaches. Furthermore, in this setting, we identify gaps in LLM preferences that emerge in particular intersections of sex and race. LLMs generally exhibit sex-based differences, but they are most pronounced in certain racial groups. These findings suggest that LLMs encode demographic preferences that may emerge in specific clinical contexts or particular combinations of characteristics. 

**Abstract (ZH)**: 大型语言模型（LLMs）在临床决策支持方面展现了潜力，但在急诊分诊应用方面仍有待探索。我们通过两个关键维度系统地研究了LLMs在急诊分诊中的能力：（1）在分布偏移和缺失数据情况下的鲁棒性，以及（2）性别和种族交叉偏见的反事实分析。我们评估了从持续预训练到情景学习的多种LLM方法，以及机器学习方法。我们的结果表明，LLMs在鲁棒性方面表现更为优越，并探讨了促成有前景的LLM方法的关键因素。此外，在此情境下，我们识别出了性别和种族交叉重叠中出现的LLMs偏好差距。LLMs通常表现出性别差异，但在某些种族群体中尤为明显。这些发现表明，LLMs可能在特定临床情境或特定特征组合中编码了人口统计学偏好。 

---
# HTN Plan Repair Algorithms Compared: Strengths and Weaknesses of Different Methods 

**Title (ZH)**: HTN规划修复算法比较：不同方法的优势与弱点 

**Authors**: Paul Zaidins, Robert P. Goldman, Ugur Kuter, Dana Nau, Mark Roberts  

**Link**: [PDF](https://arxiv.org/pdf/2504.16209)  

**Abstract**: This paper provides theoretical and empirical comparisons of three recent hierarchical plan repair algorithms: SHOPFixer, IPyHOPPER, and Rewrite. Our theoretical results show that the three algorithms correspond to three different definitions of the plan repair problem, leading to differences in the algorithms' search spaces, the repair problems they can solve, and the kinds of repairs they can make. Understanding these distinctions is important when choosing a repair method for any given application.
Building on the theoretical results, we evaluate the algorithms empirically in a series of benchmark planning problems. Our empirical results provide more detailed insight into the runtime repair performance of these systems and the coverage of the repair problems solved, based on algorithmic properties such as replanning, chronological backtracking, and backjumping over plan trees. 

**Abstract (ZH)**: 本文提供了三种近期层次计划修复算法的理论和实证比较：SHOPFixer、IPyHOPPER和Rewrite。我们的理论结果表明，这三种算法对应于计划修复问题的三种不同定义，导致它们的搜索空间、可解决的修复问题类型以及可施行的修复类型存在差异。在选择适用于特定应用的修复方法时，理解这些区别非常重要。基于算法特性（如重新规划、时间顺序回溯和计划树上的跨越回溯），我们进一步通过一系列基准规划问题对这些算法进行了实证评估，提供了关于这些系统运行时修复性能和解决的修复问题覆盖范围的更详细见解。 

---
# A Framework for Objective-Driven Dynamical Stochastic Fields 

**Title (ZH)**: 基于目标导向的动力学随机场框架 

**Authors**: Yibo Jacky Zhang, Sanmi Koyejo  

**Link**: [PDF](https://arxiv.org/pdf/2504.16115)  

**Abstract**: Fields offer a versatile approach for describing complex systems composed of interacting and dynamic components. In particular, some of these dynamical and stochastic systems may exhibit goal-directed behaviors aimed at achieving specific objectives, which we refer to as $\textit{intelligent fields}$. However, due to their inherent complexity, it remains challenging to develop a formal theoretical description of such systems and to effectively translate these descriptions into practical applications. In this paper, we propose three fundamental principles -- complete configuration, locality, and purposefulness -- to establish a theoretical framework for understanding intelligent fields. Moreover, we explore methodologies for designing such fields from the perspective of artificial intelligence applications. This initial investigation aims to lay the groundwork for future theoretical developments and practical advances in understanding and harnessing the potential of such objective-driven dynamical stochastic fields. 

**Abstract (ZH)**: 领域为描述由互动和动态组件组成的复杂系统提供了一种灵活的方法。特别是，这些动力学和随机系统中的一些可能表现出旨在实现特定目标的有目标导向的行为，我们称之为智能领域。然而，由于它们固有的复杂性，开发这些系统的正式理论描述并有效将其翻译为实际应用仍然具有挑战性。在本文中，我们提出三条基本原理——完整配置、局部性、和目的性——来建立理解智能领域的理论框架。此外，我们从人工智能应用的角度探讨了设计此类领域的方法论。这项初步研究旨在为未来理论发展和理解及利用此类目标导向的动力学随机领域的能力奠定基础。 

---
# I-Con: A Unifying Framework for Representation Learning 

**Title (ZH)**: I-Con：一种统一的表示学习框架 

**Authors**: Shaden Alshammari, John Hershey, Axel Feldmann, William T. Freeman, Mark Hamilton  

**Link**: [PDF](https://arxiv.org/pdf/2504.16929)  

**Abstract**: As the field of representation learning grows, there has been a proliferation of different loss functions to solve different classes of problems. We introduce a single information-theoretic equation that generalizes a large collection of modern loss functions in machine learning. In particular, we introduce a framework that shows that several broad classes of machine learning methods are precisely minimizing an integrated KL divergence between two conditional distributions: the supervisory and learned representations. This viewpoint exposes a hidden information geometry underlying clustering, spectral methods, dimensionality reduction, contrastive learning, and supervised learning. This framework enables the development of new loss functions by combining successful techniques from across the literature. We not only present a wide array of proofs, connecting over 23 different approaches, but we also leverage these theoretical results to create state-of-the-art unsupervised image classifiers that achieve a +8% improvement over the prior state-of-the-art on unsupervised classification on ImageNet-1K. We also demonstrate that I-Con can be used to derive principled debiasing methods which improve contrastive representation learners. 

**Abstract (ZH)**: 随着表示学习领域的发展，出现了多种不同的损失函数以解决不同类别的问题。我们引入了一个信息论方程来泛化机器学习中的多种现代损失函数。特别地，我们提出了一种框架，表明多种机器学习方法实际上是精确地最小化两个条件分布之间的集成KL散度：监督表示和学习到的表示。这一观点揭示了聚类、谱方法、降维、对比学习和监督学习背后的隐藏信息几何结构。该框架通过跨文献的成功技术结合，使开发新的损失函数成为可能。我们不仅呈现了将超过23种不同方法联系起来的广泛证明，还利用这些理论结果创建了在ImageNet-1K的无监督分类中比 previous state-of-the-art 提高了8% 的无监督图像分类器。我们还展示了I-Con可以用来推导出有效的去偏差方法，从而改进对比表示学习器。 

---
# Latent Diffusion Planning for Imitation Learning 

**Title (ZH)**: 潜在扩散规划用于模仿学习 

**Authors**: Amber Xie, Oleh Rybkin, Dorsa Sadigh, Chelsea Finn  

**Link**: [PDF](https://arxiv.org/pdf/2504.16925)  

**Abstract**: Recent progress in imitation learning has been enabled by policy architectures that scale to complex visuomotor tasks, multimodal distributions, and large datasets. However, these methods often rely on learning from large amount of expert demonstrations. To address these shortcomings, we propose Latent Diffusion Planning (LDP), a modular approach consisting of a planner which can leverage action-free demonstrations, and an inverse dynamics model which can leverage suboptimal data, that both operate over a learned latent space. First, we learn a compact latent space through a variational autoencoder, enabling effective forecasting of future states in image-based domains. Then, we train a planner and an inverse dynamics model with diffusion objectives. By separating planning from action prediction, LDP can benefit from the denser supervision signals of suboptimal and action-free data. On simulated visual robotic manipulation tasks, LDP outperforms state-of-the-art imitation learning approaches, as they cannot leverage such additional data. 

**Abstract (ZH)**: 近期模仿学习的进步得益于能够处理复杂视觉运动任务、多模态分布和大数据集的策略架构。然而，这些方法通常依赖于从大量专家演示中学习。为了解决这些不足，我们提出了一种模块化方法——潜在扩散规划（LDP），它由一个可以利用无动作演示的规划器和一个可以利用非最优数据的动力学逆模型组成，两者均在学习得到的潜在空间中运行。首先，通过变分自编码器学习一个紧凑的潜在空间，从而使基于图像的域中未来的状态预测变得有效。然后，我们使用扩散目标训练规划器和动力学逆模型。通过分离规划与动作预测，LDP可以从非最优和无动作数据的更密集监督信号中受益。在模拟的视觉机器人 manipulation 任务中，LDP 比最先进的模仿学习方法表现更优，因为后者无法利用此类额外数据。 

---
# Generalized Neighborhood Attention: Multi-dimensional Sparse Attention at the Speed of Light 

**Title (ZH)**: 广义邻域注意力：光速下的多维稀疏注意力 

**Authors**: Ali Hassani, Fengzhe Zhou, Aditya Kane, Jiannan Huang, Chieh-Yun Chen, Min Shi, Steven Walton, Markus Hoehnerbach, Vijay Thakkar, Michael Isaev, Qinsheng Zhang, Bing Xu, Haicheng Wu, Wen-mei Hwu, Ming-Yu Liu, Humphrey Shi  

**Link**: [PDF](https://arxiv.org/pdf/2504.16922)  

**Abstract**: Many sparse attention mechanisms such as Neighborhood Attention have typically failed to consistently deliver speedup over the self attention baseline. This is largely due to the level of complexity in attention infrastructure, and the rapid evolution of AI hardware architecture. At the same time, many state-of-the-art foundational models, particularly in computer vision, are heavily bound by attention, and need reliable sparsity to escape the O(n^2) complexity. In this paper, we study a class of promising sparse attention mechanisms that focus on locality, and aim to develop a better analytical model of their performance improvements. We first introduce Generalized Neighborhood Attention (GNA), which can describe sliding window, strided sliding window, and blocked attention. We then consider possible design choices in implementing these approaches, and create a simulator that can provide much more realistic speedup upper bounds for any given setting. Finally, we implement GNA on top of a state-of-the-art fused multi-headed attention (FMHA) kernel designed for the NVIDIA Blackwell architecture in CUTLASS. Our implementation can fully realize the maximum speedup theoretically possible in many perfectly block-sparse cases, and achieves an effective utilization of 1.3 petaFLOPs/second in FP16. In addition, we plug various GNA configurations into off-the-shelf generative models, such as Cosmos-7B, HunyuanVideo, and FLUX, and show that it can deliver 28% to 46% end-to-end speedup on B200 without any fine-tuning. We will open source our simulator and Blackwell kernels directly through the NATTEN project. 

**Abstract (ZH)**: 一种新的局部稀疏注意力机制及其性能分析与实现：以Generalized Neighborhood Attention为例 

---
# OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents 

**Title (ZH)**: OptimAI: 从自然语言进行优化的LLM驱动AI代理 

**Authors**: Raghav Thind, Youran Sun, Ling Liang, Haizhao Yang  

**Link**: [PDF](https://arxiv.org/pdf/2504.16918)  

**Abstract**: Optimization plays a vital role in scientific research and practical applications, but formulating a concrete optimization problem described in natural language into a mathematical form and selecting a suitable solver to solve the problem requires substantial domain expertise. We introduce \textbf{OptimAI}, a framework for solving \underline{Optim}ization problems described in natural language by leveraging LLM-powered \underline{AI} agents, achieving superior performance over current state-of-the-art methods. Our framework is built upon four key roles: (1) a \emph{formulator} that translates natural language problem descriptions into precise mathematical formulations; (2) a \emph{planner} that constructs a high-level solution strategy prior to execution; and (3) a \emph{coder} and a \emph{code critic} capable of interacting with the environment and reflecting on outcomes to refine future actions. Ablation studies confirm that all roles are essential; removing the planner or code critic results in $5.8\times$ and $3.1\times$ drops in productivity, respectively. Furthermore, we introduce UCB-based debug scheduling to dynamically switch between alternative plans, yielding an additional $3.3\times$ productivity gain. Our design emphasizes multi-agent collaboration, allowing us to conveniently explore the synergistic effect of combining diverse models within a unified system. Our approach attains 88.1\% accuracy on the NLP4LP dataset and 71.2\% on the Optibench (non-linear w/o table) subset, reducing error rates by 58\% and 50\% respectively over prior best results. 

**Abstract (ZH)**: OptimAI：基于LLM的自然语言描述优化问题求解框架 

---
# Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text 

**Title (ZH)**: 追踪思维：使用链式推理识别生成文本的大型语言模型 

**Authors**: Shifali Agrahari, Sanasam Ranbir Singh  

**Link**: [PDF](https://arxiv.org/pdf/2504.16913)  

**Abstract**: In recent years, the detection of AI-generated text has become a critical area of research due to concerns about academic integrity, misinformation, and ethical AI deployment. This paper presents COT Fine-tuned, a novel framework for detecting AI-generated text and identifying the specific language model. responsible for generating the text. We propose a dual-task approach, where Task A involves classifying text as AI-generated or human-written, and Task B identifies the specific LLM behind the text. The key innovation of our method lies in the use of Chain-of-Thought reasoning, which enables the model to generate explanations for its predictions, enhancing transparency and interpretability. Our experiments demonstrate that COT Fine-tuned achieves high accuracy in both tasks, with strong performance in LLM identification and human-AI classification. We also show that the CoT reasoning process contributes significantly to the models effectiveness and interpretability. 

**Abstract (ZH)**: 近年来，AI生成文本的检测已成为一项关键研究领域，由于学术诚信、虚假信息和伦理AI部署的担忧。本文提出了一种新型框架COT Fine-tuned，用于检测AI生成文本并识别其背后的特定语言模型。我们提出了一种双任务方法，其中任务A涉及将文本分类为AI生成或人类编写，任务B识别文本背后的特定大型语言模型。我们方法的关键创新在于使用链式推理，这使模型能够为其预测生成解释，从而提高透明度和可解释性。我们的实验显示，COT Fine-tuned在两个任务上均取得了高准确性，特别是在大型语言模型识别和人类-AI分类方面表现优异。我们还展示了链式推理过程对模型的 effectiveness 和可解释性有显著贡献。 

---
# BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation 

**Title (ZH)**: BadVideo：针对文本生成视频的隐蔽后门攻击 

**Authors**: Ruotong Wang, Mingli Zhu, Jiarong Ou, Rui Chen, Xin Tao, Pengfei Wan, Baoyuan Wu  

**Link**: [PDF](https://arxiv.org/pdf/2504.16907)  

**Abstract**: Text-to-video (T2V) generative models have rapidly advanced and found widespread applications across fields like entertainment, education, and marketing. However, the adversarial vulnerabilities of these models remain rarely explored. We observe that in T2V generation tasks, the generated videos often contain substantial redundant information not explicitly specified in the text prompts, such as environmental elements, secondary objects, and additional details, providing opportunities for malicious attackers to embed hidden harmful content. Exploiting this inherent redundancy, we introduce BadVideo, the first backdoor attack framework tailored for T2V generation. Our attack focuses on designing target adversarial outputs through two key strategies: (1) Spatio-Temporal Composition, which combines different spatiotemporal features to encode malicious information; (2) Dynamic Element Transformation, which introduces transformations in redundant elements over time to convey malicious information. Based on these strategies, the attacker's malicious target seamlessly integrates with the user's textual instructions, providing high stealthiness. Moreover, by exploiting the temporal dimension of videos, our attack successfully evades traditional content moderation systems that primarily analyze spatial information within individual frames. Extensive experiments demonstrate that BadVideo achieves high attack success rates while preserving original semantics and maintaining excellent performance on clean inputs. Overall, our work reveals the adversarial vulnerability of T2V models, calling attention to potential risks and misuse. Our project page is at this https URL. 

**Abstract (ZH)**: Text-to-video (T2V) 生成模型已迅速发展并在娱乐、教育和营销等领域找到了广泛的应用。然而，这些模型的对抗性漏洞仍然很少被探索。我们观察到，在T2V生成任务中，生成的视频中常常包含大量的冗余信息，这些信息并未明确地出现在文本提示中，如环境元素、次要对象和额外细节，为恶意攻击者提供了嵌入隐藏有害内容的机会。利用这种固有的冗余性，我们引入了BadVideo，这是首个针对T2V生成的后门攻击框架。我们的攻击主要通过两种关键策略设计目标对抗输出：(1) 空间-时间合成，将不同的空间-时间特征结合起来编码恶意信息；(2) 动态元素转变，通过在冗余元素上引入时间上的变换来传达恶意信息。基于这些策略，攻击者的恶意目标能够无缝地与用户的文本指令整合，提供极高的隐蔽性。此外，通过利用视频的时间维度，我们的攻击成功规避了主要分析单帧内空间信息的传统内容审核系统。广泛实验结果表明，BadVideo 在保持原始语义的同时，对抗成功率高且在干净输入上维持了优异的性能。整体而言，我们的工作揭示了T2V模型的对抗性漏洞，引起了潜在风险和滥用的注意。我们的项目页面在此 [链接]。 

---
# Building A Secure Agentic AI Application Leveraging A2A Protocol 

**Title (ZH)**: 基于A2A协议构建一个安全自主的人工智能应用 

**Authors**: Idan Habler, Ken Huang, Vineeth Sai Narajala, Prashant Kulkarni  

**Link**: [PDF](https://arxiv.org/pdf/2504.16902)  

**Abstract**: As Agentic AI systems evolve from basic workflows to complex multi agent collaboration, robust protocols such as Google's Agent2Agent (A2A) become essential enablers. To foster secure adoption and ensure the reliability of these complex interactions, understanding the secure implementation of A2A is essential. This paper addresses this goal by providing a comprehensive security analysis centered on the A2A protocol. We examine its fundamental elements and operational dynamics, situating it within the framework of agent communication development. Utilizing the MAESTRO framework, specifically designed for AI risks, we apply proactive threat modeling to assess potential security issues in A2A deployments, focusing on aspects such as Agent Card management, task execution integrity, and authentication methodologies.
Based on these insights, we recommend practical secure development methodologies and architectural best practices designed to build resilient and effective A2A systems. Our analysis also explores how the synergy between A2A and the Model Context Protocol (MCP) can further enhance secure interoperability. This paper equips developers and architects with the knowledge and practical guidance needed to confidently leverage the A2A protocol for building robust and secure next generation agentic applications. 

**Abstract (ZH)**: 随着代理型人工智能系统从基本的工作流发展到复杂的多代理协作，像Google的Agent2Agent (A2A)这样的 robust 协议变得至关重要。为了促进安全采用并确保这些复杂交互的可靠性，理解A2A的安全实现是必不可少的。本文通过提供以A2A协议为中心的全面安全分析来实现这一目标，我们考察其基本要素和操作动态，并将其置于代理通信发展的框架中。利用专门设计用于AI风险的MAESTRO框架，我们应用前瞻性的威胁建模来评估A2A部署中的潜在安全问题，重点关注代理卡管理、任务执行完整性和认证方法等方面。

基于这些见解，我们建议实用的安全开发方法和架构最佳实践，以构建坚固且有效的A2A系统。我们的分析还探讨了A2A与模型上下文协议（MCP）之间的协同作用如何进一步增强安全互操作性。本文为开发者和架构师提供了所需的知识和实用指导，使他们能够自信地利用A2A协议构建坚固且安全的下一代代理型应用。 

---
# Approximating Optimal Labelings for Temporal Connectivity 

**Title (ZH)**: 接近最优标记以实现临时连通性 

**Authors**: Daniele Carnevale, Gianlorenzo D'Angelo, Martin Olsen  

**Link**: [PDF](https://arxiv.org/pdf/2504.16837)  

**Abstract**: In a temporal graph the edge set dynamically changes over time according to a set of time-labels associated with each edge that indicates at which time-steps the edge is available. Two vertices are connected if there is a path connecting them in which the edges are traversed in increasing order of their labels. We study the problem of scheduling the availability time of the edges of a temporal graph in such a way that all pairs of vertices are connected within a given maximum allowed time $a$ and the overall number of labels is minimized.
The problem, known as \emph{Minimum Aged Labeling} (MAL), has several applications in logistics, distribution scheduling, and information spreading in social networks, where carefully choosing the time-labels can significantly reduce infrastructure costs, fuel consumption, or greenhouse gases.
The problem MAL has previously been proved to be NP-complete on undirected graphs and \APX-hard on directed graphs. In this paper, we extend our knowledge on the complexity and approximability of MAL in several directions. We first show that the problem cannot be approximated within a factor better than $O(\log n)$ when $a\geq 2$, unless $\text{P} = \text{NP}$, and a factor better than $2^{\log ^{1-\epsilon} n}$ when $a\geq 3$, unless $\text{NP}\subseteq \text{DTIME}(2^{\text{polylog}(n)})$, where $n$ is the number of vertices in the graph. Then we give a set of approximation algorithms that, under some conditions, almost match these lower bounds. In particular, we show that the approximation depends on a relation between $a$ and the diameter of the input graph.
We further establish a connection with a foundational optimization problem on static graphs called \emph{Diameter Constrained Spanning Subgraph} (DCSS) and show that our hardness results also apply to DCSS. 

**Abstract (ZH)**: 最小年龄标签问题的调度 

---
# Improving Significant Wave Height Prediction Using Chronos Models 

**Title (ZH)**: 使用Chronos模型提高显著波高预测精度 

**Authors**: Yilin Zhai, Hongyuan Shi, Chao Zhan, Qing Wang, Zaijin You, Nan Wang  

**Link**: [PDF](https://arxiv.org/pdf/2504.16834)  

**Abstract**: Accurate wave height prediction is critical for maritime safety and coastal resilience, yet conventional physics-based models and traditional machine learning methods face challenges in computational efficiency and nonlinear dynamics modeling. This study introduces Chronos, the first implementation of a large language model (LLM)-powered temporal architecture (Chronos) optimized for wave forecasting. Through advanced temporal pattern recognition applied to historical wave data from three strategically chosen marine zones in the Northwest Pacific basin, our framework achieves multimodal improvements: (1) 14.3% reduction in training time with 2.5x faster inference speed compared to PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units; (2) superior short-term forecasting (1-24h) across comprehensive metrics; (3) sustained predictive leadership in extended-range forecasts (1-120h); and (4) demonstrated zero-shot capability maintaining median performance (rank 4/12) against specialized operational models. This LLM-enhanced temporal modeling paradigm establishes a new standard in wave prediction, offering both computationally efficient solutions and a transferable framework for complex geophysical systems modeling. 

**Abstract (ZH)**: 基于大型语言模型的Chronos时空架构：精确波高预测的新标准 

---
# Process Reward Models That Think 

**Title (ZH)**: 思考的进程奖励模型 

**Authors**: Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, Lu Wang  

**Link**: [PDF](https://arxiv.org/pdf/2504.16828)  

**Abstract**: Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models will be released at this https URL. 

**Abstract (ZH)**: 逐步验证器——也称为过程奖励模型（PRMs）——是测试时扩展的关键成分。PRMs需要步骤级监督，使其训练成本高昂。本文旨在构建数据高效的PRMs，即基于逐步奖励模式的语言模型，通过生成验证链式思维（CoT）来验证每一步。我们提出ThinkPRM，这是一种长CoT验证器，在比判别性PRMs所需的工艺标签少多个数量级的条件下进行微调。我们的方法利用长CoT模型固有的推理能力，在PRM800K的1%工艺标签下，在多个挑战性基准上优于LLM-as-a-Judge和判别性验证器。具体而言，在ProcessBench、MATH-500和AIME '24的最佳选择和奖励引导搜索下，ThinkPRM优于基线。在GPQA-Diamond和LiveCodeBench的子集的域外评估中，我们的PRM在判别性验证器在PRM800K上进行完整训练的基础上分别优于它们8%和4.5%。最后，与LLM-as-a-Judge相比，在相同令牌预算下，ThinkPRM更有效地扩展验证计算能力，在ProcessBench的子集上优于LLM-as-a-Judge 7.2%。我们的工作突显了生成性长CoT PRMs的价值，这些模型可以在需要最少监督的情况下扩展验证时的计算能力。我们的代码、数据和模型将在以下链接发布：this https URL。 

---
# Random Long-Context Access for Mamba via Hardware-aligned Hierarchical Sparse Attention 

**Title (ZH)**: Mamba的硬件对齐分层稀疏注意机制下的随机长上下文访问 

**Authors**: Xiang Hu, Jiaqi Leng, Jun Zhao, Kewei Tu, Wei Wu  

**Link**: [PDF](https://arxiv.org/pdf/2504.16795)  

**Abstract**: A key advantage of Recurrent Neural Networks (RNNs) over Transformers is their linear computational and space complexity enables faster training and inference for long sequences. However, RNNs are fundamentally unable to randomly access historical context, and simply integrating attention mechanisms may undermine their efficiency advantages. To overcome this limitation, we propose \textbf{H}ierarchical \textbf{S}parse \textbf{A}ttention (HSA), a novel attention mechanism that enhances RNNs with long-range random access flexibility while preserving their merits in efficiency and length generalization. HSA divides inputs into chunks, selecting the top-$k$ chunks and hierarchically aggregates information. The core innovation lies in learning token-to-chunk relevance based on fine-grained token-level information inside each chunk. This approach enhances the precision of chunk selection across both in-domain and out-of-domain context lengths. To make HSA efficient, we further introduce a hardware-aligned kernel design. By combining HSA with Mamba, we introduce RAMba, which achieves perfect accuracy in passkey retrieval across 64 million contexts despite pre-training on only 4K-length contexts, and significant improvements on various downstream tasks, with nearly constant memory footprint. These results show RAMba's huge potential in long-context modeling. 

**Abstract (ZH)**: Recurrent Neural Networks with Hierarchical Sparse Attention for Efficient Long-Sequence Modeling 

---
# Radiometer Calibration using Machine Learning 

**Title (ZH)**: 使用机器学习进行辐射计校准 

**Authors**: S. A. K. Leeney, H. T. J. Bevins, E. de Lera Acedo, W. J. Handley, C. Kirkham, R. S. Patel, J. Zhu, D. Molnar, J. Cumner, D. Anstey, K. Artuc, G. Bernardi, M. Bucher, S. Carey, J. Cavillot, R. Chiello, W. Croukamp, D. I. L. de Villiers, J. A. Ely, A. Fialkov, T. Gessey-Jones, G. Kulkarni, A. Magro, P. D. Meerburg, S. Mittal, J. H. N. Pattison, S. Pegwal, C. M. Pieterse, J. R. Pritchard, E. Puchwein, N. Razavi-Ghods, I. L. V. Roque, A. Saxena, K. H. Scheutwinkel, P. Scott, E. Shen, P. H. Sims, M. Spinelli  

**Link**: [PDF](https://arxiv.org/pdf/2504.16791)  

**Abstract**: Radiometers are crucial instruments in radio astronomy, forming the primary component of nearly all radio telescopes. They measure the intensity of electromagnetic radiation, converting this radiation into electrical signals. A radiometer's primary components are an antenna and a Low Noise Amplifier (LNA), which is the core of the ``receiver'' chain. Instrumental effects introduced by the receiver are typically corrected or removed during calibration. However, impedance mismatches between the antenna and receiver can introduce unwanted signal reflections and distortions. Traditional calibration methods, such as Dicke switching, alternate the receiver input between the antenna and a well-characterised reference source to mitigate errors by comparison. Recent advances in Machine Learning (ML) offer promising alternatives. Neural networks, which are trained using known signal sources, provide a powerful means to model and calibrate complex systems where traditional analytical approaches struggle. These methods are especially relevant for detecting the faint sky-averaged 21-cm signal from atomic hydrogen at high redshifts. This is one of the main challenges in observational Cosmology today. Here, for the first time, we introduce and test a machine learning-based calibration framework capable of achieving the precision required for radiometric experiments aiming to detect the 21-cm line. 

**Abstract (ZH)**: 射电望远镜是射电天文学中至关重要的仪器，构成了几乎全部射电望远镜的主要组件。它们测量电磁辐射的强度，将这种辐射转换为电信号。射电望远镜的主要组件是天线和低噪声放大器（LNA），它是“接收器”链的核心。由接收器引入的仪器效应通常在校准过程中被修正或移除。然而，天线和接收器之间的阻抗失配可能会引入不需要的信号反射和失真。传统的校准方法，如狄克开关，通过交替接收入射天线和一个具有良好表征的参考源来减轻比较方法中的错误。近年来，机器学习的进步提出了有希望的替代方案。使用已知信号源训练的神经网络为建模和校准传统分析方法难以处理的复杂系统提供了强大手段。这些方法特别适用于检测高红移时平均化后的稀弱21厘米线信号。这是当今观测 cosmology 中的主要挑战之一。在这里，我们首次引入并测试了一种基于机器学习的校准框架，能够实现检测21厘米线所需的高度精确性。 

---
# Towards Explainable AI: Multi-Modal Transformer for Video-based Image Description Generation 

**Title (ZH)**: 面向可解释人工智能：基于视频的图像描述生成的多模态变换器 

**Authors**: Lakshita Agarwal, Bindu Verma  

**Link**: [PDF](https://arxiv.org/pdf/2504.16788)  

**Abstract**: Understanding and analyzing video actions are essential for producing insightful and contextualized descriptions, especially for video-based applications like intelligent monitoring and autonomous systems. The proposed work introduces a novel framework for generating natural language descriptions from video datasets by combining textual and visual modalities. The suggested architecture makes use of ResNet50 to extract visual features from video frames that are taken from the Microsoft Research Video Description Corpus (MSVD), and Berkeley DeepDrive eXplanation (BDD-X) datasets. The extracted visual characteristics are converted into patch embeddings and then run through an encoder-decoder model based on Generative Pre-trained Transformer-2 (GPT-2). In order to align textual and visual representations and guarantee high-quality description production, the system uses multi-head self-attention and cross-attention techniques. The model's efficacy is demonstrated by performance evaluation using BLEU (1-4), CIDEr, METEOR, and ROUGE-L. The suggested framework outperforms traditional methods with BLEU-4 scores of 0.755 (BDD-X) and 0.778 (MSVD), CIDEr scores of 1.235 (BDD-X) and 1.315 (MSVD), METEOR scores of 0.312 (BDD-X) and 0.329 (MSVD), and ROUGE-L scores of 0.782 (BDD-X) and 0.795 (MSVD). By producing human-like, contextually relevant descriptions, strengthening interpretability, and improving real-world applications, this research advances explainable AI. 

**Abstract (ZH)**: 理解并分析视频动作对于生成洞察性和上下文相关描述至关重要，尤其是在基于视频的应用如智能监控和自主系统中。本文提出了一种结合文本和视觉模态的新框架，用于从视频数据集中生成自然语言描述。该建议架构利用ResNet50从来自Microsoft Research Video Description Corpus (MSVD) 和Berkeley DeepDrive eXplanation (BDD-X) 数据集的视频帧中提取视觉特征。提取的视觉特征被转换为斑块嵌入，并通过基于生成预训练变压器-2 (GPT-2) 的编码器-解码器模型进行处理。为了对齐文本和视觉表示并确保高质量描述的生成，系统使用了多头自注意力和交叉注意力技术。通过使用BLEU (1-4), CIDEr, METEOR, 和 ROUGE-L 进行性能评估，展示了该模型的有效性。与传统的传统方法相比，所提出的框架在BLEU-4上的得分为0.755 (BDD-X) 和0.778 (MSVD)，CIDEr分数为1.235 (BDD-X) 和1.315 (MSVD)，METEOR分为0.312 (BDD-X) 和0.329 (MSVD)，ROUGE-L分为0.782 (BDD-X) 和0.795 (MSVD)。通过生成人类like、上下文相关描述，增强可解释性和改进实际应用，本文推进了可解释人工智能的发展。 

---
# Credible plan-driven RAG method for Multi-hop Question Answering 

**Title (ZH)**: 基于计划驱动的可信多跳问答RAG方法 

**Authors**: Ningning Zhang, Chi Zhang, Zhizhong Tan, Xingxing Yang, Weiping Deng, Wenyong Wang  

**Link**: [PDF](https://arxiv.org/pdf/2504.16787)  

**Abstract**: Multi-hop question answering (QA) presents a considerable challenge for Retrieval-Augmented Generation (RAG), requiring the structured decomposition of complex queries into logical reasoning paths and the generation of dependable intermediate results. However, deviations in reasoning paths or errors in intermediate results, which are common in current RAG methods, may propagate and accumulate throughout the reasoning process, diminishing the accuracy of the answer to complex queries. To address this challenge, we propose the Plan-then-Act-and-Review (PAR RAG) framework, which is organized into three key stages: planning, act, and review, and aims to offer an interpretable and incremental reasoning paradigm for accurate and reliable multi-hop question answering by mitigating error this http URL RAG initially applies a top-down problem decomposition strategy, formulating a comprehensive plan that integrates multiple executable steps from a holistic viewpoint. This approach avoids the pitfalls of local optima common in traditional RAG methods, ensuring the accuracy of the entire reasoning path. Subsequently, PAR RAG incorporates a plan execution mechanism based on multi-granularity verification. By utilizing both coarse-grained similarity information and fine-grained relevant data, the framework thoroughly checks and adjusts intermediate results, ensuring process accuracy while effectively managing error propagation and amplification. Experimental results on multi-hop QA datasets demonstrate that the PAR RAG framework substantially outperforms existing state-of-the-art methods in key metrics, including EM and F1 scores. 

**Abstract (ZH)**: 多跳问答（多跳QA）对检索增强生成（RAG）构成了显著挑战，需要将复杂查询结构化分解为逻辑推理路径，并生成可靠的中间结果。然而，当前RAG方法中推理路径中的偏差或中间结果中的错误可能在整个推理过程中传递和累积，降低复杂查询答案的准确性。为应对这一挑战，我们提出了计划-执行-回顾（PAR RAG）框架，该框架分为计划、执行和回顾三个关键阶段，旨在通过缓解错误提供可解释的逐步推理范式，以实现准确可靠的多跳问答。RAG最初采用自上而下的问题分解策略，从全局视角综合多个可执行步骤，避免传统RAG方法中常见的局部最优解问题，确保整体推理路径的准确性。随后，PAR RAG结合多粒度验证机制，通过利用粗粒度相似性和细粒度相关数据，彻底检查和调整中间结果，确保过程准确同时有效管理错误传播和放大。实验结果表明，PAR RAG框架在关键指标（包括EM和F1分数）上显著优于现有最先进的方法。 

---
# Evaluation Framework for AI Systems in "the Wild" 

**Title (ZH)**: “野生”环境中AI系统的评估框架 

**Authors**: Sarah Jabbour, Trenton Chang, Anindya Das Antar, Joseph Peper, Insu Jang, Jiachen Liu, Jae-Won Chung, Shiqi He, Michael Wellman, Bryan Goodman, Elizabeth Bondi-Kelly, Kevin Samy, Rada Mihalcea, Mosharaf Chowhury, David Jurgens, Lu Wang  

**Link**: [PDF](https://arxiv.org/pdf/2504.16778)  

**Abstract**: Generative AI (GenAI) models have become vital across industries, yet current evaluation methods have not adapted to their widespread use. Traditional evaluations often rely on benchmarks and fixed datasets, frequently failing to reflect real-world performance, which creates a gap between lab-tested outcomes and practical applications. This white paper proposes a comprehensive framework for how we should evaluate real-world GenAI systems, emphasizing diverse, evolving inputs and holistic, dynamic, and ongoing assessment approaches. The paper offers guidance for practitioners on how to design evaluation methods that accurately reflect real-time capabilities, and provides policymakers with recommendations for crafting GenAI policies focused on societal impacts, rather than fixed performance numbers or parameter sizes. We advocate for holistic frameworks that integrate performance, fairness, and ethics and the use of continuous, outcome-oriented methods that combine human and automated assessments while also being transparent to foster trust among stakeholders. Implementing these strategies ensures GenAI models are not only technically proficient but also ethically responsible and impactful. 

**Abstract (ZH)**: 面向现实应用的生成式AI系统评估框架 

---
# How Effective are Generative Large Language Models in Performing Requirements Classification? 

**Title (ZH)**: 生成型大型语言模型在执行需求分类任务中效果如何？ 

**Authors**: Waad Alhoshan, Alessio Ferrari, Liping Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2504.16768)  

**Abstract**: In recent years, transformer-based large language models (LLMs) have revolutionised natural language processing (NLP), with generative models opening new possibilities for tasks that require context-aware text generation. Requirements engineering (RE) has also seen a surge in the experimentation of LLMs for different tasks, including trace-link detection, regulatory compliance, and others. Requirements classification is a common task in RE. While non-generative LLMs like BERT have been successfully applied to this task, there has been limited exploration of generative LLMs. This gap raises an important question: how well can generative LLMs, which produce context-aware outputs, perform in requirements classification? In this study, we explore the effectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing both binary and multi-class requirements classification. We design an extensive experimental study involving over 400 experiments across three widely used datasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes that while factors like prompt design and LLM architecture are universally important, others-such as dataset variations-have a more situational impact, depending on the complexity of the classification task. This insight can guide future model development and deployment strategies, focusing on optimising prompt structures and aligning model architectures with task-specific needs for improved performance. 

**Abstract (ZH)**: 基于变压器的大语言模型近年来革新了自然语言处理（NLP），生成模型为需要上下文感知文本生成的任务开启了新的可能性。在需求工程（RE）领域，大语言模型也被用于不同任务的实验，包括跟踪链接检测、法规合规性等。需求分类是RE中的一个常见任务。尽管像BERT这样的非生成模型已经在该任务上取得了成功应用，但生成大语言模型的应用却相对有限。这一差距提出了一个重要问题：上下文感知的生成大语言模型在需求分类任务中的表现如何？本研究探讨了三个生成大语言模型——Bloom、Gemma 和 Llama 在执行二分类和多分类需求分类任务中的有效性。我们设计了一个广泛的实验研究，涉及超过400个实验，涵盖了三个广泛使用的数据集（PROMISE NFR、Functional-Quality 和 SecReq）。研究结果表明，尽管提示设计和大语言模型架构等因素普遍重要，但其他因素如数据集的差异，会根据不同分类任务的复杂度而产生更情境化的影响力。这一见解可以指导未来的模型开发和部署策略，专注于优化提示结构并使模型架构与其特定任务需求相匹配，以提高性能。 

---
# Noise-Tolerant Coreset-Based Class Incremental Continual Learning 

**Title (ZH)**: 噪声鲁棒核子集基于类增量连续学习 

**Authors**: Edison Mucllari, Aswin Raghavan, Zachary Alan Daniels  

**Link**: [PDF](https://arxiv.org/pdf/2504.16763)  

**Abstract**: Many applications of computer vision require the ability to adapt to novel data distributions after deployment. Adaptation requires algorithms capable of continual learning (CL). Continual learners must be plastic to adapt to novel tasks while minimizing forgetting of previous this http URL, CL opens up avenues for noise to enter the training pipeline and disrupt the CL. This work focuses on label noise and instance noise in the context of class-incremental learning (CIL), where new classes are added to a classifier over time, and there is no access to external data from past classes. We aim to understand the sensitivity of CL methods that work by replaying items from a memory constructed using the idea of Coresets. We derive a new bound for the robustness of such a method to uncorrelated instance noise under a general additive noise threat model, revealing several insights. Putting the theory into practice, we create two continual learning algorithms to construct noise-tolerant replay buffers. We empirically compare the effectiveness of prior memory-based continual learners and the proposed algorithms under label and uncorrelated instance noise on five diverse datasets. We show that existing memory-based CL are not robust whereas the proposed methods exhibit significant improvements in maximizing classification accuracy and minimizing forgetting in the noisy CIL setting. 

**Abstract (ZH)**: 许多计算机视觉应用要求在部署后能够适应新的数据分布。适应性需要具备持续学习能力的算法。持续学习者必须具有可塑性，以便适应新任务的同时尽量减少对先前任务的遗忘。持续学习可能为噪声进入训练管道并扰乱持续学习打开途径。本工作专注于类别增量学习（CIL）中的标签噪声和实例噪声，其中随着时间的推移，新的类别被添加到分类器中，且无法获取过去类别的外部数据。我们旨在理解通过使用Coreset思想构建记忆库并重复播放其中项的持续学习方法在无关联实例噪声情况下的鲁棒性。我们推导出该方法在一般加性噪声威胁模型下的新鲁棒性界限，揭示了若干洞察。将理论付诸实践，我们创建了两个持续学习算法以构建噪声容忍的重复播放缓存。我们在标签噪声和无关联实例噪声环境下，通过五个不同的数据集评估了现有记忆型持续学习方法和所提出算法的有效性。我们表明，现有记忆型持续学习方法不具有鲁棒性，而所提出的方法在噪声类别增量学习环境中显著提升了分类准确率并减少了遗忘。 

---
# HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations 

**Title (ZH)**: HEMA：一种受海马体启发的扩展记忆架构用于长语境AI对话 

**Authors**: Kwangseob Ahn  

**Link**: [PDF](https://arxiv.org/pdf/2504.16754)  

**Abstract**: Large language models (LLMs) struggle with maintaining coherence in extended conversations spanning hundreds of turns, despite performing well within their context windows. This paper introduces HEMA (Hippocampus-Inspired Extended Memory Architecture), a dual-memory system inspired by human cognitive processes. HEMA combines Compact Memory - a continuously updated one-sentence summary preserving global narrative coherence, and Vector Memory - an episodic store of chunk embeddings queried via cosine similarity. When integrated with a 6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns while keeping prompt length under 3,500 tokens. Experimental results show substantial improvements: factual recall accuracy increases from 41% to 87%, and human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K indexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling the area under the precision-recall curve compared to summarization-only approaches. Ablation studies reveal two key insights: semantic forgetting through age-weighted pruning reduces retrieval latency by 34% with minimal recall loss, and a two-level summary hierarchy prevents cascade errors in ultra-long conversations exceeding 1,000 turns. HEMA demonstrates that combining verbatim recall with semantic continuity provides a practical solution for privacy-aware conversational AI capable of month-long dialogues without model retraining. 

**Abstract (ZH)**: HEMA：受海马体启发的扩展记忆架构在大规模语言模型中的应用 

---
# MOSAIC: A Skill-Centric Algorithmic Framework for Long-Horizon Manipulation Planning 

**Title (ZH)**: MOSAIC: 一种专注于技能的长期 manipulation 规划算法框架 

**Authors**: Itamar Mishani, Yorai Shaoul, Maxim Likhachev  

**Link**: [PDF](https://arxiv.org/pdf/2504.16738)  

**Abstract**: Planning long-horizon motions using a set of predefined skills is a key challenge in robotics and AI. Addressing this challenge requires methods that systematically explore skill combinations to uncover task-solving sequences, harness generic, easy-to-learn skills (e.g., pushing, grasping) to generalize across unseen tasks, and bypass reliance on symbolic world representations that demand extensive domain and task-specific knowledge. Despite significant progress, these elements remain largely disjoint in existing approaches, leaving a critical gap in achieving robust, scalable solutions for complex, long-horizon problems. In this work, we present MOSAIC, a skill-centric framework that unifies these elements by using the skills themselves to guide the planning process. MOSAIC uses two families of skills: Generators compute executable trajectories and world configurations, and Connectors link these independently generated skill trajectories by solving boundary value problems, enabling progress toward completing the overall task. By breaking away from the conventional paradigm of incrementally discovering skills from predefined start or goal states--a limitation that significantly restricts exploration--MOSAIC focuses planning efforts on regions where skills are inherently effective. We demonstrate the efficacy of MOSAIC in both simulated and real-world robotic manipulation tasks, showcasing its ability to solve complex long-horizon planning problems using a diverse set of skills incorporating generative diffusion models, motion planning algorithms, and manipulation-specific models. Visit this https URL for demonstrations and examples. 

**Abstract (ZH)**: 基于预定义技能规划长期 horizon 动作是机器人技术和人工智能领域的关键挑战。解决这一挑战需要系统探索技能组合的方法，以发现任务解决序列，利用通用且易于学习的技能（如推拉、抓取）来泛化到未见过的任务，并避免依赖要求广泛领域和任务特定知识的符号世界表示。尽管取得了显著进展，但这些元素在现有方法中仍然基本分离，留下了实现鲁棒性和可扩展性解决方案的重大缺口。在本文中，我们提出了MOSAIC，这是一种以技能为中心的框架，通过使用技能本身来指导规划过程来统一这些元素。MOSAIC 使用两类技能：生成器计算可执行轨迹和世界配置，连接器通过解决边界值问题将这些独立生成的技能轨迹链接起来，使任务完成取得进展。通过脱离从预定义的起始或目标状态增量发现技能的传统范式——这一局限显著限制了探索范围——MOSAIC 将规划努力集中在技能本身天然有效的区域。我们在模拟和实际机器人操作任务中展示了MOSAIC 的有效性，展示了它使用生成扩散模型、运动规划算法和操作特定模型解决复杂长期 horizon 规划问题的能力。更多演示和示例请访问：https://... 

---
# V$^2$R-Bench: Holistically Evaluating LVLM Robustness to Fundamental Visual Variations 

**Title (ZH)**: V$^2$R-Bench: 综合评估LVLM 对基本视觉变化的鲁棒性 

**Authors**: Zhiyuan Fan, Yumeng Wang, Sandeep Polisetty, Yi R., Fung  

**Link**: [PDF](https://arxiv.org/pdf/2504.16727)  

**Abstract**: Large Vision Language Models (LVLMs) excel in various vision-language tasks. Yet, their robustness to visual variations in position, scale, orientation, and context that objects in natural scenes inevitably exhibit due to changes in viewpoint and environment remains largely underexplored. To bridge this gap, we introduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating Visual Variation Robustness of LVLMs, which encompasses automated evaluation dataset generation and principled metrics for thorough robustness assessment. Through extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability to visual variations, in which even advanced models that excel at complex vision-language tasks significantly underperform on simple tasks such as object recognition. Interestingly, these models exhibit a distinct visual position bias that contradicts theories of effective receptive fields, and demonstrate a human-like visual acuity threshold. To identify the source of these vulnerabilities, we present a systematic framework for component-level analysis, featuring a novel visualization approach for aligned visual features. Results show that these vulnerabilities stem from error accumulation in the pipeline architecture and inadequate multimodal alignment. Complementary experiments with synthetic data further demonstrate that these limitations are fundamentally architectural deficiencies, scoring the need for architectural innovations in future LVLM designs. 

**Abstract (ZH)**: 大型视觉语言模型（LVLMs）在各种视觉语言任务中表现出色。然而，它们对自然场景中物体由于视角和环境变化而不可避免地表现出的位置、尺度、方向和上下文视觉变异的鲁棒性研究仍然不足。为填补这一空白，我们提出了V$^2$R-Bench，一个全面的基准框架，用于评估LVLMs的视觉变异鲁棒性，该框架包含自动化评估数据集生成和原则性的度量标准，以便进行全面的鲁棒性评估。通过在21个LVLMs上的广泛评估，我们揭示了一个令人惊讶的视觉变异脆弱性，即使在复杂视觉语言任务中表现出色的模型在简单的物体识别任务中也会显著表现不佳。有趣的是，这些模型表现出与有效感受野理论相悖的视觉位置偏见，并展示了类似人类的视觉敏锐度阈值。为了识别这些脆弱性的来源，我们提出了一个系统的组件级分析框架，并引入了一种新的对齐视觉特征的可视化方法。结果显示，这些脆弱性源于流水线架构中的错误累积和不足的多模态对齐。补充实验进一步证明了这些局限性是根本性的架构缺陷，强调了未来LVLM设计中架构创新的必要性。 

---
# Detecting and Understanding Hateful Contents in Memes Through Captioning and Visual Question-Answering 

**Title (ZH)**: 通过描述和视觉问答检测和理解 meme 中的 hateful 内容 

**Authors**: Ali Anaissi, Junaid Akram, Kunal Chaturvedi, Ali Braytee  

**Link**: [PDF](https://arxiv.org/pdf/2504.16723)  

**Abstract**: Memes are widely used for humor and cultural commentary, but they are increasingly exploited to spread hateful content. Due to their multimodal nature, hateful memes often evade traditional text-only or image-only detection systems, particularly when they employ subtle or coded references. To address these challenges, we propose a multimodal hate detection framework that integrates key components: OCR to extract embedded text, captioning to describe visual content neutrally, sub-label classification for granular categorization of hateful content, RAG for contextually relevant retrieval, and VQA for iterative analysis of symbolic and contextual cues. This enables the framework to uncover latent signals that simpler pipelines fail to detect. Experimental results on the Facebook Hateful Memes dataset reveal that the proposed framework exceeds the performance of unimodal and conventional multimodal models in both accuracy and AUC-ROC. 

**Abstract (ZH)**: 表情包广泛用于幽默和文化评论，但它们越来越多地被用于传播仇恨内容。由于其多模态性质，仇恨表情包常常规避传统只针对文本或图像的检测系统，尤其是在它们使用含蓄或编码的引用时。为应对这些挑战，我们提出了一种多模态仇恨内容检测框架，该框架整合了关键组件：OCR用于提取嵌入文本、字幕描述视觉内容、子标签分类进行仇恨内容的细粒度分类、基于检索的生成（RAG）以实现上下文相关的内容检索，以及视觉问答（VQA）进行迭代分析象征性和上下文线索。这使得该框架能够发现简单流水线未能检测到的潜在信号。实验结果表明，在Facebook仇恨表情包数据集上，所提框架在准确率和AUC-ROC方面均超过了单模态和传统多模态模型。 

---
# PMG: Progressive Motion Generation via Sparse Anchor Postures Curriculum Learning 

**Title (ZH)**: PMG: 基于稀疏锚姿势 curriculum 学习的渐进运动生成 

**Authors**: Yingjie Xi, Jian Jun Zhang, Xiaosong Yang  

**Link**: [PDF](https://arxiv.org/pdf/2504.16722)  

**Abstract**: In computer animation, game design, and human-computer interaction, synthesizing human motion that aligns with user intent remains a significant challenge. Existing methods have notable limitations: textual approaches offer high-level semantic guidance but struggle to describe complex actions accurately; trajectory-based techniques provide intuitive global motion direction yet often fall short in generating precise or customized character movements; and anchor poses-guided methods are typically confined to synthesize only simple motion patterns. To generate more controllable and precise human motions, we propose \textbf{ProMoGen (Progressive Motion Generation)}, a novel framework that integrates trajectory guidance with sparse anchor motion control. Global trajectories ensure consistency in spatial direction and displacement, while sparse anchor motions only deliver precise action guidance without displacement. This decoupling enables independent refinement of both aspects, resulting in a more controllable, high-fidelity, and sophisticated motion synthesis. ProMoGen supports both dual and single control paradigms within a unified training process. Moreover, we recognize that direct learning from sparse motions is inherently unstable, we introduce \textbf{SAP-CL (Sparse Anchor Posture Curriculum Learning)}, a curriculum learning strategy that progressively adjusts the number of anchors used for guidance, thereby enabling more precise and stable convergence. Extensive experiments demonstrate that ProMoGen excels in synthesizing vivid and diverse motions guided by predefined trajectory and arbitrary anchor frames. Our approach seamlessly integrates personalized motion with structured guidance, significantly outperforming state-of-the-art methods across multiple control scenarios. 

**Abstract (ZH)**: 面向用户意图的渐进式motion生成：轨迹引导与稀疏关键动作控制 

---
# Offline Robotic World Model: Learning Robotic Policies without a Physics Simulator 

**Title (ZH)**: 离线机器人世界模型：无需物理模拟器学习机器人策略 

**Authors**: Chenhao Li, Andreas Krause, Marco Hutter  

**Link**: [PDF](https://arxiv.org/pdf/2504.16680)  

**Abstract**: Reinforcement Learning (RL) has demonstrated impressive capabilities in robotic control but remains challenging due to high sample complexity, safety concerns, and the sim-to-real gap. While offline RL eliminates the need for risky real-world exploration by learning from pre-collected data, it suffers from distributional shift, limiting policy generalization. Model-Based RL (MBRL) addresses this by leveraging predictive models for synthetic rollouts, yet existing approaches often lack robust uncertainty estimation, leading to compounding errors in offline settings. We introduce Offline Robotic World Model (RWM-O), a model-based approach that explicitly estimates epistemic uncertainty to improve policy learning without reliance on a physics simulator. By integrating these uncertainty estimates into policy optimization, our approach penalizes unreliable transitions, reducing overfitting to model errors and enhancing stability. Experimental results show that RWM-O improves generalization and safety, enabling policy learning purely from real-world data and advancing scalable, data-efficient RL for robotics. 

**Abstract (ZH)**: 基于模型的机器人世界模型（RWM-O）：通过显式估计知识性不确定性提高政策学习 

---
# A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics 

**Title (ZH)**: 多语言训练数据后训练指南：揭示跨语言迁移动态 

**Authors**: Luisa Shimabucoro, Ahmet Ustun, Marzieh Fadaee, Sebastian Ruder  

**Link**: [PDF](https://arxiv.org/pdf/2504.16677)  

**Abstract**: In order for large language models to be useful across the globe, they are fine-tuned to follow instructions on multilingual data. Despite the ubiquity of such post-training, a clear understanding of the dynamics that enable cross-lingual transfer remains elusive. This study examines cross-lingual transfer (CLT) dynamics in realistic post-training settings. We study two model families of up to 35B parameters in size trained on carefully controlled mixtures of multilingual data on three generative tasks with varying levels of complexity (summarization, instruction following, and mathematical reasoning) in both single-task and multi-task instruction tuning settings. Overall, we find that the dynamics of cross-lingual transfer and multilingual performance cannot be explained by isolated variables, varying depending on the combination of post-training settings. Finally, we identify the conditions that lead to effective cross-lingual transfer in practice. 

**Abstract (ZH)**: 为了使大型语言模型在全球范围内具有实用性，它们在多语言数据上进行了微调。尽管这种后训练是普遍存在的，但关于使跨语言迁移成为可能的动力学机制仍不清楚。本研究探讨了在现实后训练设置中跨语言迁移（CLT）的动力学。我们研究了在严格控制的多语言数据混合训练下，在三个不同复杂程度的生成任务（总结、指令遵循和数学推理）上训练的两个模型家族，这些任务分别在单任务和多任务指令微调设置中进行。总体而言，我们发现跨语言迁移和多语言性能的动力学机制不能仅由孤立的变量解释，而是取决于后训练设置的组合。最后，我们确定了在实践中实现有效跨语言迁移的条件。 

---
# Representation Learning via Non-Contrastive Mutual Information 

**Title (ZH)**: 非对比 Mutual Information 的表示学习 

**Authors**: Zhaohan Daniel Guo, Bernardo Avila Pires, Khimya Khetarpal, Dale Schuurmans, Bo Dai  

**Link**: [PDF](https://arxiv.org/pdf/2504.16667)  

**Abstract**: Labeling data is often very time consuming and expensive, leaving us with a majority of unlabeled data. Self-supervised representation learning methods such as SimCLR (Chen et al., 2020) or BYOL (Grill et al., 2020) have been very successful at learning meaningful latent representations from unlabeled image data, resulting in much more general and transferable representations for downstream tasks. Broadly, self-supervised methods fall into two types: 1) Contrastive methods, such as SimCLR; and 2) Non-Contrastive methods, such as BYOL. Contrastive methods are generally trying to maximize mutual information between related data points, so they need to compare every data point to every other data point, resulting in high variance, and thus requiring large batch sizes to work well. Non-contrastive methods like BYOL have much lower variance as they do not need to make pairwise comparisons, but are much trickier to implement as they have the possibility of collapsing to a constant vector. In this paper, we aim to develop a self-supervised objective that combines the strength of both types. We start with a particular contrastive method called the Spectral Contrastive Loss (HaoChen et al., 2021; Lu et al., 2024), and we convert it into a more general non-contrastive form; this removes the pairwise comparisons resulting in lower variance, but keeps the mutual information formulation of the contrastive method preventing collapse. We call our new objective the Mutual Information Non-Contrastive (MINC) loss. We test MINC by learning image representations on ImageNet (similar to SimCLR and BYOL) and show that it consistently improves upon the Spectral Contrastive loss baseline. 

**Abstract (ZH)**: 无监督标签学习既耗时又昂贵，导致大量数据未标注。SimCLR（Chen等，2020）或BYOL（Grill等，2020）等自监督表示学习方法能够从未标注图像数据中学习到有意义的潜在表示，从而为下游任务提供了更加通用和可迁移的表示。自监督方法大致可分为两大类：1）对比方法，如SimCLR；2）非对比方法，如BYOL。对比方法通常试图最大化相关数据点之间的互信息，因此需要将每一对数据点与其他所有数据点进行比较，导致高方差，从而需要大批次大小才能有效工作。非对比方法如BYOL的方差更低，因为它们不需要进行成对比较，但实现起来更为棘手，因为存在退化为固定向量的可能性。在本文中，我们旨在开发一种结合了这两种方法优势的自监督目标函数。我们以特定的对比方法谱对比损失（Spectral Contrastive Loss，HaoChen等，2021；Lu等，2024）为起点，将其转换为一种更通用的非对比形式；这消除了成对比较，从而降低了方差，但保留了对比方法的互信息形式，防止退化。我们称这一新目标函数为互信息非对比损失（Mutual Information Non-Contrastive，MINC）损失。我们通过在ImageNet上学习图像表示（类似于SimCLR和BYOL）来测试MINC，并证明它可以持续改进谱对比损失的基线效果。 

---
# MAYA: Addressing Inconsistencies in Generative Password Guessing through a Unified Benchmark 

**Title (ZH)**: MAYA: 通过统一基准解决生成密码猜测中的一致性问题 

**Authors**: William Corrias, Fabio De Gaspari, Dorjan Hitaj, Luigi V. Mancini  

**Link**: [PDF](https://arxiv.org/pdf/2504.16651)  

**Abstract**: The rapid evolution of generative models has led to their integration across various fields, including password guessing, aiming to generate passwords that resemble human-created ones in complexity, structure, and patterns. Despite generative model's promise, inconsistencies in prior research and a lack of rigorous evaluation have hindered a comprehensive understanding of their true potential. In this paper, we introduce MAYA, a unified, customizable, plug-and-play password benchmarking framework. MAYA provides a standardized approach for evaluating generative password-guessing models through a rigorous set of advanced testing scenarios and a collection of eight real-life password datasets. Using MAYA, we comprehensively evaluate six state-of-the-art approaches, which have been re-implemented and adapted to ensure standardization, for a total of over 15,000 hours of computation. Our findings indicate that these models effectively capture different aspects of human password distribution and exhibit strong generalization capabilities. However, their effectiveness varies significantly with long and complex passwords. Through our evaluation, sequential models consistently outperform other generative architectures and traditional password-guessing tools, demonstrating unique capabilities in generating accurate and complex guesses. Moreover, models learn and generate different password distributions, enabling a multi-model attack that outperforms the best individual model. By releasing MAYA, we aim to foster further research, providing the community with a new tool to consistently and reliably benchmark password-generation techniques. Our framework is publicly available at this https URL 

**Abstract (ZH)**: 生成模型的快速演进已将其整合到各种领域中，包括密码猜测，旨在生成与人类创建的密码在复杂性、结构和模式上相似的密码。尽管生成模型具有巨大的潜力，但由于先前研究中的不一致性以及缺乏严格的评估，阻碍了对其真正潜力的全面理解。在本文中，我们介绍了MAYA，一个统一、可定制、即插即用的密码基准测试框架。MAYA提供了一种通过一系列严格的高级测试场景和八个真实密码数据集来评估生成式密码猜测模型的标准方法。使用MAYA，我们全面评估了六种最先进的方法，这些方法已重新实现和调整以确保标准化，总共进行了超过15,000小时的计算。我们的研究发现，这些模型能有效捕捉人类密码分布的不同方面，并展现出强大的泛化能力。然而，它们在长而复杂的密码上的效果差异显著。通过我们的评估，序列模型普遍优于其他生成架构和传统密码猜测工具，展示了在生成准确且复杂的猜测方面的独特能力。此外，模型学习并生成不同的密码分布，使多模型攻击优于单一最佳模型。通过发布MAYA，我们旨在促进进一步的研究，为社区提供一个新的工具来一致且可靠地评估密码生成技术。我们的框架可在以下网址公开获取：这个 https URL。 

---
# SSLR: A Semi-Supervised Learning Method for Isolated Sign Language Recognition 

**Title (ZH)**: SSLR：孤立手语识别的半监督学习方法 

**Authors**: Hasan Algafri, Hamzah Luqman, Sarah Alyami, Issam Laradji  

**Link**: [PDF](https://arxiv.org/pdf/2504.16640)  

**Abstract**: Sign language is the primary communication language for people with disabling hearing loss. Sign language recognition (SLR) systems aim to recognize sign gestures and translate them into spoken language. One of the main challenges in SLR is the scarcity of annotated datasets. To address this issue, we propose a semi-supervised learning (SSL) approach for SLR (SSLR), employing a pseudo-label method to annotate unlabeled samples. The sign gestures are represented using pose information that encodes the signer's skeletal joint points. This information is used as input for the Transformer backbone model utilized in the proposed approach. To demonstrate the learning capabilities of SSL across various labeled data sizes, several experiments were conducted using different percentages of labeled data with varying numbers of classes. The performance of the SSL approach was compared with a fully supervised learning-based model on the WLASL-100 dataset. The obtained results of the SSL model outperformed the supervised learning-based model with less labeled data in many cases. 

**Abstract (ZH)**: 基于半监督学习的 sign 语言识别方法 

---
# Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories 

**Title (ZH)**: 用对话澄清谬误？探究AI生成的反驳言论挑战阴谋理论的研究 

**Authors**: Mareike Lisker, Christina Gottschalk, Helena Mihaljević  

**Link**: [PDF](https://arxiv.org/pdf/2504.16604)  

**Abstract**: Counterspeech is a key strategy against harmful online content, but scaling expert-driven efforts is challenging. Large Language Models (LLMs) present a potential solution, though their use in countering conspiracy theories is under-researched. Unlike for hate speech, no datasets exist that pair conspiracy theory comments with expert-crafted counterspeech. We address this gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively apply counterspeech strategies derived from psychological research provided through structured prompts. Our results show that the models often generate generic, repetitive, or superficial results. Additionally, they over-acknowledge fear and frequently hallucinate facts, sources, or figures, making their prompt-based use in practical applications problematic. 

**Abstract (ZH)**: 专家驱动的反制言论是应对有害网络内容的关键策略，但扩大此类努力具有挑战性。大型语言模型（LLMs）可能提供一种解决方案，尽管它们在反制阴谋论方面的研究不足。与仇恨言论不同，目前没有任何数据集将阴谋论评论与其由专家精心设计的反制言论配对。我们通过评估GPT-4o、Llama 3和Mistral在这方面的能力，填补了这一空白，这些能力来自于通过结构化提示提供的心理学研究中的反制策略。结果显示，这些模型经常生成通用、重复或表面化的结果，还过度承认恐惧并频繁地虚构事实、来源或数字，使其基于提示的实际应用具有问题性。 

---
# Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study 

**Title (ZH)**: 比较大型语言模型与传统机器翻译工具翻译医疗咨询摘要的效果：一项初步研究 

**Authors**: Andy Li, Wei Zhou, Rashina Hoda, Chris Bain, Peter Poon  

**Link**: [PDF](https://arxiv.org/pdf/2504.16601)  

**Abstract**: This study evaluates how well large language models (LLMs) and traditional machine translation (MT) tools translate medical consultation summaries from English into Arabic, Chinese, and Vietnamese. It assesses both patient, friendly and clinician, focused texts using standard automated metrics. Results showed that traditional MT tools generally performed better, especially for complex texts, while LLMs showed promise, particularly in Vietnamese and Chinese, when translating simpler summaries. Arabic translations improved with complexity due to the language's morphology. Overall, while LLMs offer contextual flexibility, they remain inconsistent, and current evaluation metrics fail to capture clinical relevance. The study highlights the need for domain-specific training, improved evaluation methods, and human oversight in medical translation. 

**Abstract (ZH)**: 本研究评估了大型语言模型（LLM）和传统机器翻译（MT）工具将英语的医疗咨询总结翻译成阿拉伯语、中文和越南语的质量，使用标准自动化评估指标对面向患者和面向临床人员的文本进行了评估。结果显示，传统MT工具在复杂文本上的表现优于LLM，而LLM在翻译简单总结时特别是在越南语和中文方面表现出潜力。阿拉伯语翻译随着文本复杂性的增加而改善，这是由于阿拉伯语的形态学特性。总体而言，虽然LLM提供了上下文灵活性，但它们仍存在不一致性，当前的评估指标未能捕捉到临床相关性。该研究强调了在医疗翻译中需要进行领域特定训练、改进评估方法以及增加人工监督的重要性。 

---
# Case Study: Fine-tuning Small Language Models for Accurate and Private CWE Detection in Python Code 

**Title (ZH)**: 案例研究：针对Python代码中的准确且私有的CWE检测微调小型语言模型 

**Authors**: Md. Azizul Hakim Bappy, Hossen A Mustafa, Prottoy Saha, Rajinus Salehat  

**Link**: [PDF](https://arxiv.org/pdf/2504.16584)  

**Abstract**: Large Language Models (LLMs) have demonstrated significant capabilities in understanding and analyzing code for security vulnerabilities, such as Common Weakness Enumerations (CWEs). However, their reliance on cloud infrastructure and substantial computational requirements pose challenges for analyzing sensitive or proprietary codebases due to privacy concerns and inference costs. This work explores the potential of Small Language Models (SLMs) as a viable alternative for accurate, on-premise vulnerability detection. We investigated whether a 350-million parameter pre-trained code model (codegen-mono) could be effectively fine-tuned to detect the MITRE Top 25 CWEs specifically within Python code. To facilitate this, we developed a targeted dataset of 500 examples using a semi-supervised approach involving LLM-driven synthetic data generation coupled with meticulous human review. Initial tests confirmed that the base codegen-mono model completely failed to identify CWEs in our samples. However, after applying instruction-following fine-tuning, the specialized SLM achieved remarkable performance on our test set, yielding approximately 99% accuracy, 98.08% precision, 100% recall, and a 99.04% F1-score. These results strongly suggest that fine-tuned SLMs can serve as highly accurate and efficient tools for CWE detection, offering a practical and privacy-preserving solution for integrating advanced security analysis directly into development workflows. 

**Abstract (ZH)**: 小型语言模型（SLMs）在精准、本地漏洞检测中的潜力 

---
# MMHCL: Multi-Modal Hypergraph Contrastive Learning for Recommendation 

**Title (ZH)**: MMHCL：多模态超图对比学习推荐 

**Authors**: Xu Guo, Tong Zhang, Fuyun Wang, Xudong Wang, Xiaoya Zhang, Xin Liu, Zhen Cui  

**Link**: [PDF](https://arxiv.org/pdf/2504.16576)  

**Abstract**: The burgeoning presence of multimodal content-sharing platforms propels the development of personalized recommender systems. Previous works usually suffer from data sparsity and cold-start problems, and may fail to adequately explore semantic user-product associations from multimodal data. To address these issues, we propose a novel Multi-Modal Hypergraph Contrastive Learning (MMHCL) framework for user recommendation. For a comprehensive information exploration from user-product relations, we construct two hypergraphs, i.e. a user-to-user (u2u) hypergraph and an item-to-item (i2i) hypergraph, to mine shared preferences among users and intricate multimodal semantic resemblance among items, respectively. This process yields denser second-order semantics that are fused with first-order user-item interaction as complementary to alleviate the data sparsity issue. Then, we design a contrastive feature enhancement paradigm by applying synergistic contrastive learning. By maximizing/minimizing the mutual information between second-order (e.g. shared preference pattern for users) and first-order (information of selected items for users) embeddings of the same/different users and items, the feature distinguishability can be effectively enhanced. Compared with using sparse primary user-item interaction only, our MMHCL obtains denser second-order hypergraphs and excavates more abundant shared attributes to explore the user-product associations, which to a certain extent alleviates the problems of data sparsity and cold-start. Extensive experiments have comprehensively demonstrated the effectiveness of our method. Our code is publicly available at: this https URL. 

**Abstract (ZH)**: 多模态超图对比学习的用户推荐框架 

---
# PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression 

**Title (ZH)**: PIS：连接重要性采样与注意力机制以实现高效的提示压缩 

**Authors**: Lizhe Chen, Binjia Zhou, Yuyao Ge, Jiayi Chen, Shiguang NI  

**Link**: [PDF](https://arxiv.org/pdf/2504.16574)  

**Abstract**: Large language models (LLMs) have achieved remarkable progress, demonstrating unprecedented capabilities across various natural language processing tasks. However, the high costs associated with such exceptional performance limit the widespread adoption of LLMs, highlighting the need for prompt compression. Existing prompt compression methods primarily rely on heuristic truncation or abstractive summarization techniques, which fundamentally overlook the intrinsic mechanisms of LLMs and lack a systematic evaluation of token importance for generation. In this work, we introduce Prompt Importance Sampling (PIS), a novel compression framework that dynamically compresses prompts by sampling important tokens based on the analysis of attention scores of hidden states. PIS employs a dual-level compression mechanism: 1) at the token level, we quantify saliency using LLM-native attention scores and implement adaptive compression through a lightweight 9-layer reinforcement learning (RL) network; 2) at the semantic level, we propose a Russian roulette sampling strategy for sentence-level importance sampling. Comprehensive evaluations across multiple domain benchmarks demonstrate that our method achieves state-of-the-art compression performance. Notably, our framework serendipitously enhances reasoning efficiency through optimized context structuring. This work advances prompt engineering by offering both theoretical grounding and practical efficiency in context management for LLMs. 

**Abstract (ZH)**: 大型语言模型（LLMs）取得了显著进展，在各种自然语言处理任务中展现了前所未有的能力。然而，与其出色的性能相伴的高昂成本限制了LLMs的广泛应用，突显了提示压缩的必要性。现有提示压缩方法主要依赖启发式截断或提取性总结技术，从根本上忽视了LLMs的内在机制，并缺乏对生成中token重要性的系统评估。在本工作中，我们提出了提示重要性采样（PIS），这是一种新颖的压缩框架，通过分析隐藏状态的注意力分数来动态压缩提示，选择重要token进行采样。PIS采用双重压缩机制：1）在token级别，使用LLM内置的注意力分数量化显著性，并通过一个轻量级的9层强化学习（RL）网络实现自适应压缩；2）在语义级别，我们提出了俄式轮盘赌采样策略进行句级重要性采样。全面的评估表明，我们的方法达到了最先进的压缩性能。值得注意的是，我们的框架通过优化上下文结构意外地提升了推理效率。本工作通过提供理论基础和上下文管理的实际效率促进了提示工程的发展。 

---
# PsyCounAssist: A Full-Cycle AI-Powered Psychological Counseling Assistant System 

**Title (ZH)**: PsyCounAssist: 一个全程AI驱动的心理咨询助理系统 

**Authors**: Xianghe Liu, Jiaqi Xu, Tao Sun  

**Link**: [PDF](https://arxiv.org/pdf/2504.16573)  

**Abstract**: Psychological counseling is a highly personalized and dynamic process that requires therapists to continuously monitor emotional changes, document session insights, and maintain therapeutic continuity. In this paper, we introduce PsyCounAssist, a comprehensive AI-powered counseling assistant system specifically designed to augment psychological counseling practices. PsyCounAssist integrates multimodal emotion recognition combining speech and photoplethysmography (PPG) signals for accurate real-time affective analysis, automated structured session reporting using large language models (LLMs), and personalized AI-generated follow-up support. Deployed on Android-based tablet devices, the system demonstrates practical applicability and flexibility in real-world counseling scenarios. Experimental evaluation confirms the reliability of PPG-based emotional classification and highlights the system's potential for non-intrusive, privacy-aware emotional support. PsyCounAssist represents a novel approach to ethically and effectively integrating AI into psychological counseling workflows. 

**Abstract (ZH)**: 心理辅导是一个高度个性化和动态的过程，要求治疗师持续监控情绪变化、记录会话洞见并维持治疗连续性。本文介绍了一种综合AI赋能的心理辅导助理系统PsyCounAssist，该系统旨在增强心理辅导实践。PsyCounAssist结合使用语音和光电容积描记法（PPG）信号进行多模态情绪识别，实现准确的实时情绪分析，采用大型语言模型（LLMs）自动生成结构化会话报告，并提供个性化AI生成的后续支持。该系统在基于Android的平板设备上部署，展示了在实际心理辅导场景中的实用性和灵活性。实验评估证实了基于PPG的情绪分类的可靠性，并强调了系统在非侵入性和隐私意识情绪支持方面的潜力。PsyCounAssist代表了将AI伦理有效地整合到心理辅导工作流中的新方法。 

---
# A Vision for AI-Driven Adaptation of Dynamic AR Content to Users and Environments 

**Title (ZH)**: AI驱动的动态AR内容适应用户和环境的愿景 

**Authors**: Julian Rasch, Florian Müller, Francesco Chiossi  

**Link**: [PDF](https://arxiv.org/pdf/2504.16562)  

**Abstract**: Augmented Reality (AR) is transforming the way we interact with virtual information in the physical world. By overlaying digital content in real-world environments, AR enables new forms of immersive and engaging experiences. However, existing AR systems often struggle to effectively manage the many interactive possibilities that AR presents. This vision paper speculates on AI-driven approaches for adaptive AR content placement, dynamically adjusting to user movement and environmental changes. By leveraging machine learning methods, such a system would intelligently manage content distribution between AR projections integrated into the external environment and fixed static content, enabling seamless UI layout and potentially reducing users' cognitive load. By exploring the possibilities of AI-driven dynamic AR content placement, we aim to envision new opportunities for innovation and improvement in various industries, from urban navigation and workplace productivity to immersive learning and beyond. This paper outlines a vision for the development of more intuitive, engaging, and effective AI-powered AR experiences. 

**Abstract (ZH)**: augmented reality (ar) 是以全新方式在物理世界中与虚拟信息交互的革命。通过在现实世界环境中叠加数字内容，ar 启用了新的沉浸式和互动体验形式。然而，现有的 ar 系统经常难以有效地管理 ar 呈现的众多交互可能性。本文推测了基于人工智能的方法，以实现自适应的 ar 内容放置，动态适应用户运动和环境变化。通过利用机器学习方法，这样的系统将智能地管理 ar 投影与外部环境中的固定静态内容之间的内容分布，从而实现无感的 ui 布局，并可能减轻用户的认知负荷。通过探索基于 ai 的动态 ar 内容放置的可能性，我们旨在构想在各个行业，从城市导航和工作效率提升到沉浸式学习等领域的创新和改进机会。本文勾勒了一种更具直觉性、互动性和有效性的人工智能增强现实体验的发展愿景。 

---
# Exploring human-SAV interaction using large language models: The impact of psychological ownership and anthropomorphism on user experience 

**Title (ZH)**: 使用大型语言模型探索人类与智能代理系统的互动：心理所有权和拟人化对用户体验的影响 

**Authors**: Lirui Guo, Michael G. Burke, Wynita M. Griggs  

**Link**: [PDF](https://arxiv.org/pdf/2504.16548)  

**Abstract**: There has been extensive prior work exploring how psychological factors such as anthropomorphism affect the adoption of shared autonomous vehicles (SAVs). However, limited research has been conducted on how prompt strategies in large language model (LLM)-powered SAV User Interfaces (UIs) affect users' perceptions, experiences, and intentions to adopt such technology. In this work, we investigate how conversational UIs powered by LLMs drive these psychological factors and psychological ownership, the sense of possession a user may come to feel towards an entity or object they may not legally own. We designed four SAV UIs with varying levels of anthropomorphic characteristics and psychological ownership triggers. Quantitative measures of psychological ownership, anthropomorphism, quality of service, disclosure tendency, sentiment of SAV responses, and overall acceptance were collected after participants interacted with each SAV. Qualitative feedback was also gathered regarding the experience of psychological ownership during the interactions. The results indicate that an SAV conversational UI designed to be more anthropomorphic and to induce psychological ownership improved users' perceptions of the SAV's human-like qualities and improved the sentiment of responses compared to a control condition. These findings provide practical guidance for designing LLM-based conversational UIs that enhance user experience and adoption of SAVs. 

**Abstract (ZH)**: 大规模语言模型powered共享自主车辆用户界面中提示策略如何影响用户感知、体验和采纳意图的研究：基于拟人化特征和心理占有感的对话界面设计与效果分析 

---
# Transformers for Complex Query Answering over Knowledge Hypergraphs 

**Title (ZH)**: Transformer在知识超图上的复杂查询回答中应用 

**Authors**: Hong Ting Tsang, Zihao Wang, Yangqiu Song  

**Link**: [PDF](https://arxiv.org/pdf/2504.16537)  

**Abstract**: Complex Query Answering (CQA) has been extensively studied in recent years. In order to model data that is closer to real-world distribution, knowledge graphs with different modalities have been introduced. Triple KGs, as the classic KGs composed of entities and relations of arity 2, have limited representation of real-world facts. Real-world data is more sophisticated. While hyper-relational graphs have been introduced, there are limitations in representing relationships of varying arity that contain entities with equal contributions. To address this gap, we sampled new CQA datasets: JF17k-HCQA and M-FB15k-HCQA. Each dataset contains various query types that include logical operations such as projection, negation, conjunction, and disjunction. In order to answer knowledge hypergraph (KHG) existential first-order queries, we propose a two-stage transformer model, the Logical Knowledge Hypergraph Transformer (LKHGT), which consists of a Projection Encoder for atomic projection and a Logical Encoder for complex logical operations. Both encoders are equipped with Type Aware Bias (TAB) for capturing token interactions. Experimental results on CQA datasets show that LKHGT is a state-of-the-art CQA method over KHG and is able to generalize to out-of-distribution query types. 

**Abstract (ZH)**: 复杂查询回答（CQA）近年来受到了广泛研究。为了更好地模拟现实世界的分布，引入了不同模态的知识图谱。三元组KG作为经典的二目关联实体和关系的知识图谱，对现实世界的事实表示能力有限。现实世界的数据更为复杂。尽管引入了超关系图，但在表示包含等贡献实体的多种 arity 关系时仍有限制。为解决这一问题，我们采样了新的CQA数据集：JF17k-HCQA和M-FB15k-HCQA。每个数据集包含多种查询类型，包括逻辑操作如投影、否定、合取和析取。为了回答知识超图（KHG）的存在性一阶查询，我们提出了一种两阶段的变压器模型，即逻辑知识超图变换器（LKHGT），该模型包括用于原子投影的投影编码器和用于复杂逻辑操作的逻辑编码器。两个编码器均配备了类型感知偏差（TAB）以捕捉 token 交互。在CQA数据集上的实验结果表明，LKHGT 是KHG上最先进的CQA方法，并且能够泛化到未出分布的查询类型。 

---
# Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion and Reasoning for Vision-and-Language Navigation 

**Title (ZH)**: 层次化思考，动态行动：面向视觉语言导航的层次化多模态融合与推理 

**Authors**: Junrong Yue, Yifan Zhang, Chuan Qin, Bo Li, Xiaomin Lie, Xinlei Yu, Wenxin Zhang, Zhendong Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2504.16516)  

**Abstract**: Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow natural language instructions and reach target locations in real-world environments. While prior methods often rely on either global scene representations or object-level features, these approaches are insufficient for capturing the complex interactions across modalities required for accurate navigation. In this paper, we propose a Multi-level Fusion and Reasoning Architecture (MFRA) to enhance the agent's ability to reason over visual observations, language instructions and navigation history. Specifically, MFRA introduces a hierarchical fusion mechanism that aggregates multi-level features-ranging from low-level visual cues to high-level semantic concepts-across multiple modalities. We further design a reasoning module that leverages fused representations to infer navigation actions through instruction-guided attention and dynamic context integration. By selectively capturing and combining relevant visual, linguistic, and temporal signals, MFRA improves decision-making accuracy in complex navigation scenarios. Extensive experiments on benchmark VLN datasets including REVERIE, R2R, and SOON demonstrate that MFRA achieves superior performance compared to state-of-the-art methods, validating the effectiveness of multi-level modal fusion for embodied navigation. 

**Abstract (ZH)**: 视觉-语言导航（VLN）旨在使具身代理能够遵循自然语言指令并在真实环境中的目标位置。尽管先前的方法通常依赖于全局场景表示或物体级特征，这些方法不足以捕捉准确导航所需的多种模态间复杂交互。在本文中，我们提出了一种多层次融合与推理架构（MFRA）以增强代理在视觉观察、语言指令和导航历史上的推理能力。具体而言，MFRA 引入了一种层次融合机制，该机制在多种模态间聚合从低级视觉线索到高级语义概念的多级特征。我们进一步设计了一个推理模块，该模块通过指令引导的注意力和动态上下文集成来推断导航动作。通过选择性地捕获和结合相关的视觉、语言和时空信号，MFRA 在复杂导航场景中的决策准确性得到提高。在基准 VLN 数据集 REVERIE、R2R 和 SOON 上的广泛实验表明，MFRA 达到了比现有最佳方法更好的性能，验证了多层次模态融合在具身导航中的有效性。 

---
# Federated Learning of Low-Rank One-Shot Image Detection Models in Edge Devices with Scalable Accuracy and Compute Complexity 

**Title (ZH)**: 边缘设备上具有可扩展准确度和计算复杂度的低秩单次图像检测模型的联邦学习 

**Authors**: Abdul Hannaan, Zubair Shah, Aiman Erbad, Amr Mohamed, Ali Safa  

**Link**: [PDF](https://arxiv.org/pdf/2504.16515)  

**Abstract**: This paper introduces a novel federated learning framework termed LoRa-FL designed for training low-rank one-shot image detection models deployed on edge devices. By incorporating low-rank adaptation techniques into one-shot detection architectures, our method significantly reduces both computational and communication overhead while maintaining scalable accuracy. The proposed framework leverages federated learning to collaboratively train lightweight image recognition models, enabling rapid adaptation and efficient deployment across heterogeneous, resource-constrained devices. Experimental evaluations on the MNIST and CIFAR10 benchmark datasets, both in an independent-and-identically-distributed (IID) and non-IID setting, demonstrate that our approach achieves competitive detection performance while significantly reducing communication bandwidth and compute complexity. This makes it a promising solution for adaptively reducing the communication and compute power overheads, while not sacrificing model accuracy. 

**Abstract (ZH)**: 一种用于边缘设备上训练低秩单-shot图像检测模型的新型联邦学习框架：LoRa-FL 

---
# Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate 

**Title (ZH)**: 放大后的漏洞：基于LLM的多Agent辩论结构化 Jailbreak 攻击 

**Authors**: Senmao Qi, Yifei Zou, Peng Li, Ziyi Lin, Xiuzhen Cheng, Dongxiao Yu  

**Link**: [PDF](https://arxiv.org/pdf/2504.16489)  

**Abstract**: Multi-Agent Debate (MAD), leveraging collaborative interactions among Large Language Models (LLMs), aim to enhance reasoning capabilities in complex tasks. However, the security implications of their iterative dialogues and role-playing characteristics, particularly susceptibility to jailbreak attacks eliciting harmful content, remain critically underexplored. This paper systematically investigates the jailbreak vulnerabilities of four prominent MAD frameworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo, and DeepSeek) without compromising internal agents. We introduce a novel structured prompt-rewriting framework specifically designed to exploit MAD dynamics via narrative encapsulation, role-driven escalation, iterative refinement, and rhetorical obfuscation. Our extensive experiments demonstrate that MAD systems are inherently more vulnerable than single-agent setups. Crucially, our proposed attack methodology significantly amplifies this fragility, increasing average harmfulness from 28.14% to 80.34% and achieving attack success rates as high as 80% in certain scenarios. These findings reveal intrinsic vulnerabilities in MAD architectures and underscore the urgent need for robust, specialized defenses prior to real-world deployment. 

**Abstract (ZH)**: 多层次语言模型辩论系统的脱监攻击脆弱性研究：基于四大主流框架的系统性探究 

---
# On Developers' Self-Declaration of AI-Generated Code: An Analysis of Practices 

**Title (ZH)**: 开发者对自己声明的AI生成代码的研究：实践分析 

**Authors**: Syed Mohammad Kashif, Peng Liang, Amjed Tahir  

**Link**: [PDF](https://arxiv.org/pdf/2504.16485)  

**Abstract**: AI code generation tools have gained significant popularity among developers, who use them to assist in software development due to their capability to generate code. Existing studies mainly explored the quality, e.g., correctness and security, of AI-generated code, while in real-world software development, the prerequisite is to distinguish AI-generated code from human-written code, which emphasizes the need to explicitly declare AI-generated code by developers. To this end, this study intends to understand the ways developers use to self-declare AI-generated code and explore the reasons why developers choose to self-declare or not. We conducted a mixed-methods study consisting of two phases. In the first phase, we mined GitHub repositories and collected 613 instances of AI-generated code snippets. In the second phase, we conducted a follow-up industrial survey, which received 111 valid responses. Our research revealed the practices followed by developers to self-declare AI-generated code. Most practitioners (76.6%) always or sometimes self-declare AI-generated code. In contrast, other practitioners (23.4%) noted that they never self-declare AI-generated code. The reasons for self-declaring AI-generated code include the need to track and monitor the code for future review and debugging, and ethical considerations. The reasons for not self-declaring AI-generated code include extensive modifications to AI-generated code and the developers' perception that self-declaration is an unnecessary activity. We finally provided guidelines for practitioners to self-declare AI-generated code, addressing ethical and code quality concerns. 

**Abstract (ZH)**: AI代码生成工具在开发者中的应用及其自声明方式的研究 

---
# The Dance of Atoms-De Novo Protein Design with Diffusion Model 

**Title (ZH)**: 原子之舞-基于扩散模型的从头蛋白质设计 

**Authors**: Yujie Qin, Ming He, Changyong Yu, Ming Ni, Xian Liu, Xiaochen Bo  

**Link**: [PDF](https://arxiv.org/pdf/2504.16479)  

**Abstract**: The de novo design of proteins refers to creating proteins with specific structures and functions that do not naturally exist. In recent years, the accumulation of high-quality protein structure and sequence data and technological advancements have paved the way for the successful application of generative artificial intelligence (AI) models in protein design. These models have surpassed traditional approaches that rely on fragments and bioinformatics. They have significantly enhanced the success rate of de novo protein design, and reduced experimental costs, leading to breakthroughs in the field. Among various generative AI models, diffusion models have yielded the most promising results in protein design. In the past two to three years, more than ten protein design models based on diffusion models have emerged. Among them, the representative model, RFDiffusion, has demonstrated success rates in 25 protein design tasks that far exceed those of traditional methods, and other AI-based approaches like RFjoint and hallucination. This review will systematically examine the application of diffusion models in generating protein backbones and sequences. We will explore the strengths and limitations of different models, summarize successful cases of protein design using diffusion models, and discuss future development directions. 

**Abstract (ZH)**: 从头设计蛋白质是指设计具有特定结构和功能且自然界中不存在的蛋白质。近年来，高质量的蛋白质结构和序列数据的积累及技术的进步为生成性人工智能（AI）模型在蛋白质设计中的成功应用铺平了道路。这些模型超越了依赖片段和生物信息学的传统方法，显著提高了从头设计蛋白质的成功率，降低了实验成本，推动了该领域的突破。在各种生成性AI模型中，扩散模型在蛋白质设计领域取得了最令人鼓舞的结果。在过去两到三年中，基于扩散模型的蛋白质设计模型超过了十个。其中，代表性的模型RFDiffusion在25项蛋白质设计任务中的成功率远超传统方法及其他基于AI的方法（如RFjoint和hallucination）。本文将系统地探讨扩散模型在生成蛋白质主链和序列中的应用。我们将分析不同模型的优势和局限性，总结使用扩散模型进行蛋白质设计的成功案例，并讨论未来的发展方向。 

---
# Harden and Catch for Just-in-Time Assured LLM-Based Software Testing: Open Research Challenges 

**Title (ZH)**: Just-in-Time Assured LLM-Based Software Testing: 坚韧与捕获在即时保证的软件测试中的应用：开放的研究挑战 

**Authors**: Mark Harman, Peter O'Hearn, Shubho Sengupta  

**Link**: [PDF](https://arxiv.org/pdf/2504.16472)  

**Abstract**: Despite decades of research and practice in automated software testing, several fundamental concepts remain ill-defined and under-explored, yet offer enormous potential real-world impact. We show that these concepts raise exciting new challenges in the context of Large Language Models for software test generation. More specifically, we formally define and investigate the properties of hardening and catching tests. A hardening test is one that seeks to protect against future regressions, while a catching test is one that catches such a regression or a fault in new functionality introduced by a code change. Hardening tests can be generated at any time and may become catching tests when a future regression is caught. We also define and motivate the Catching `Just-in-Time' (JiTTest) Challenge, in which tests are generated `just-in-time' to catch new faults before they land into production. We show that any solution to Catching JiTTest generation can also be repurposed to catch latent faults in legacy code. We enumerate possible outcomes for hardening and catching tests and JiTTests, and discuss open research problems, deployment options, and initial results from our work on automated LLM-based hardening at Meta. This paper\footnote{Author order is alphabetical. The corresponding author is Mark Harman.} was written to accompany the keynote by the authors at the ACM International Conference on the Foundations of Software Engineering (FSE) 2025. 

**Abstract (ZH)**: 尽管在自动化软件测试方面进行了数十年的研究和实践，但仍有一些基本概念界定不清且研究不足，但具有巨大的实际影响潜力。我们展示了这些概念在大型语言模型用于软件测试生成的背景下提出了新的挑战。更具体地，我们正式定义并探讨了强化测试和捕捉测试的属性。强化测试旨在防止未来的回归，而捕捉测试则是捕获重构或新功能引入的代码变更中出现的回归或故障。强化测试可以在任何时候生成，当未来回归被捕获时，它们可能成为捕捉测试。我们还定义并提出了捕捉即时生成（Catching Just-in-Time，JiTTest）挑战，在此挑战中，测试在新故障进入生产之前即时生成。我们证明，任何解决JiTTest生成问题的方法也可以用于捕获遗留代码中的潜伏故障。我们列出了强化测试、捕捉测试和JiTTest可能的结果，并讨论了开放的研究问题、部署选项以及我们在Meta使用自动化大型语言模型进行强化测试的初步结果。本文作者的文章将在2025年ACM国际软件工程基础会议（FSE）上作者的主旨演讲中发表。 

---
# ManipDreamer: Boosting Robotic Manipulation World Model with Action Tree and Visual Guidance 

**Title (ZH)**: ManipDreamer: 通过动作树和视觉引导提升机器人 manipulation 世界模型 

**Authors**: Ying Li, Xiaobao Wei, Xiaowei Chi, Yuming Li, Zhongyu Zhao, Hao Wang, Ningning Ma, Ming Lu, Shanghang Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2504.16464)  

**Abstract**: While recent advancements in robotic manipulation video synthesis have shown promise, significant challenges persist in ensuring effective instruction-following and achieving high visual quality. Recent methods, like RoboDreamer, utilize linguistic decomposition to divide instructions into separate lower-level primitives, conditioning the world model on these primitives to achieve compositional instruction-following. However, these separate primitives do not consider the relationships that exist between them. Furthermore, recent methods neglect valuable visual guidance, including depth and semantic guidance, both crucial for enhancing visual quality. This paper introduces ManipDreamer, an advanced world model based on the action tree and visual guidance. To better learn the relationships between instruction primitives, we represent the instruction as the action tree and assign embeddings to tree nodes, each instruction can acquire its embeddings by navigating through the action tree. The instruction embeddings can be used to guide the world model. To enhance visual quality, we combine depth and semantic guidance by introducing a visual guidance adapter compatible with the world model. This visual adapter enhances both the temporal and physical consistency of video generation. Based on the action tree and visual guidance, ManipDreamer significantly boosts the instruction-following ability and visual quality. Comprehensive evaluations on robotic manipulation benchmarks reveal that ManipDreamer achieves large improvements in video quality metrics in both seen and unseen tasks, with PSNR improved from 19.55 to 21.05, SSIM improved from 0.7474 to 0.7982 and reduced Flow Error from 3.506 to 3.201 in unseen tasks, compared to the recent RoboDreamer model. Additionally, our method increases the success rate of robotic manipulation tasks by 2.5% in 6 RLbench tasks on average. 

**Abstract (ZH)**: 基于动作树和视觉引导的ManipDreamer：提升机器人操作视频合成的有效指令跟随和视觉质量 

---
# T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic Understanding via Deep Triplet Loss Fine-Tuning 

**Title (ZH)**: T-VEC：一种通过深度三重损失微调以增强语义理解的电信专用向量化模型 

**Authors**: Vignesh Ethiraj, Sidhanth Menon, Divya Vijay  

**Link**: [PDF](https://arxiv.org/pdf/2504.16460)  

**Abstract**: The specialized vocabulary and complex concepts of the telecommunications industry present significant challenges for standard Natural Language Processing models. Generic text embeddings often fail to capture telecom-specific semantics, hindering downstream task performance. We introduce T-VEC (Telecom Vectorization Model), a novel embedding model tailored for the telecom domain through deep fine-tuning. Developed by NetoAI, T-VEC is created by adapting the state-of-the-art gte-Qwen2-1.5B-instruct model using a triplet loss objective on a meticulously curated, large-scale dataset of telecom-specific data. Crucially, this process involved substantial modification of weights across 338 layers of the base model, ensuring deep integration of domain knowledge, far exceeding superficial adaptation techniques. We quantify this deep change via weight difference analysis. A key contribution is the development and open-sourcing (MIT License) of the first dedicated telecom-specific tokenizer, enhancing the handling of industry jargon. T-VEC achieves a leading average MTEB score (0.825) compared to established models and demonstrates vastly superior performance (0.9380 vs. less than 0.07) on our internal telecom-specific triplet evaluation benchmark, indicating an exceptional grasp of domain-specific nuances, visually confirmed by improved embedding separation. This work positions NetoAI at the forefront of telecom AI innovation, providing the community with a powerful, deeply adapted, open-source tool. 

**Abstract (ZH)**: 电信领域的专用词汇和复杂概念给标准自然语言处理模型带来了显著挑战。通用文本嵌入往往无法捕捉电信特定的语义，影响下游任务性能。我们介绍了T-VEC（电信向量化模型），这是一种通过深度微调为电信领域量身定制的新嵌入模型。由NetoAI开发的T-VEC通过在精心策划的大型电信特定数据集上使用三重损失目标适应了最新的gte-Qwen2-1.5B-instruct模型。这个过程涉及对基模型338层的权重进行重大修改，确保了领域知识的深度集成，远超表面适应技术。我们通过权重差异分析量化了这种深度变化。一个关键贡献是开发并公开发布了首个专门针对电信领域的分词器（MIT许可证），增强了对行业术语的处理能力。T-VEC在MTEB评分上取得了领先平均得分（0.825），并在我们的内部电信特定三重评估基准测试中展现出了远超现有模型的性能（0.9380 vs. 小于0.07），显示了对领域特定细微差别的出色掌握，可视化分析进一步证实了这一点。这项工作将NetoAI定位为电信AI创新的前沿地带，为社区提供了强大且深度定制的开源工具。 

---
# EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records 

**Title (ZH)**: EMRModel: 一个将医疗咨询对话提取为结构化医疗记录的大语言模型 

**Authors**: Shuguang Zhao, Qiangzhong Feng, Zhiyang He, Peipei Sun, Yingying Wang, Xiaodong Tao, Xiaoliang Lu, Mei Cheng, Xinyue Wu, Yanyan Wang, Wei Liang  

**Link**: [PDF](https://arxiv.org/pdf/2504.16448)  

**Abstract**: Medical consultation dialogues contain critical clinical information, yet their unstructured nature hinders effective utilization in diagnosis and treatment. Traditional methods, relying on rule-based or shallow machine learning techniques, struggle to capture deep and implicit semantics. Recently, large pre-trained language models and Low-Rank Adaptation (LoRA), a lightweight fine-tuning method, have shown promise for structured information extraction. We propose EMRModel, a novel approach that integrates LoRA-based fine-tuning with code-style prompt design, aiming to efficiently convert medical consultation dialogues into structured electronic medical records (EMRs). Additionally, we construct a high-quality, realistically grounded dataset of medical consultation dialogues with detailed annotations. Furthermore, we introduce a fine-grained evaluation benchmark for medical consultation information extraction and provide a systematic evaluation methodology, advancing the optimization of medical natural language processing (NLP) models. Experimental results show EMRModel achieves an F1 score of 88.1%, improving by49.5% over standard pre-trained models. Compared to traditional LoRA fine-tuning methods, our model shows superior performance, highlighting its effectiveness in structured medical record extraction tasks. 

**Abstract (ZH)**: 医疗咨询对话包含关键临床信息，但由于其非结构化特性，在诊断和治疗中的有效利用受到阻碍。传统方法依赖于基于规则或浅层机器学习技术，难以捕捉深层次和隐含的语义。近年来，大型预训练语言模型和基于低秩适应（LoRA）的轻量化微调方法显示出提取结构化信息的潜力。我们提出了一种名为EMRModel的新方法，该方法结合了基于LoRA的微调与代码风格的提示设计，旨在高效地将医疗咨询对话转换为结构化的电子医疗记录（EMRs）。此外，我们构建了一个高质量、具有实际基础的医疗咨询对话数据集，附有详细的注释。进一步地，我们引入了针对医疗咨询信息抽取的细粒度评估基准，并提供了系统的评估方法，推动了医疗自然语言处理（NLP）模型的优化。实验结果显示，EMRModel的F1得分为88.1%，相较于标准预训练模型提高了49.5%。与传统的LoRA微调方法相比，我们的模型显示出更优的性能，强调其在结构化医疗记录提取任务中的有效性。 

---
# Private Federated Learning using Preference-Optimized Synthetic Data 

**Title (ZH)**: 基于偏好优化合成数据的隐私联邦学习 

**Authors**: Charlie Hou, Mei-Yu Wang, Yige Zhu, Daniel Lazar, Giulia Fanti  

**Link**: [PDF](https://arxiv.org/pdf/2504.16438)  

**Abstract**: In practical settings, differentially private Federated learning (DP-FL) is the dominant method for training models from private, on-device client data. Recent work has suggested that DP-FL may be enhanced or outperformed by methods that use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary algorithms for generating DP synthetic data for FL applications require careful prompt engineering based on public information and/or iterative private client feedback. Our key insight is that the private client feedback collected by prior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be viewed as a preference ranking. Our algorithm, Preference Optimization for Private Client Data (POPri) harnesses client feedback using preference optimization algorithms such as Direct Preference Optimization (DPO) to fine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri, we release LargeFedBench, a new federated text benchmark for uncontaminated LLM evaluations on federated client data. POPri substantially improves the utility of DP synthetic data relative to prior work on LargeFedBench datasets and an existing benchmark from Xie et al. (2024). POPri closes the gap between next-token prediction accuracy in the fully-private and non-private settings by up to 68%, compared to 52% for prior synthetic data methods, and 10% for state-of-the-art DP federated learning methods. The code and data are available at this https URL. 

**Abstract (ZH)**: 差分隐私联邦学习（DP-FL）在实际应用中是训练设备端私有数据模型的主要方法。近年来的研究表明，使用差分隐私合成数据的方法可能会增强或超越DP-FL（Wu et al., 2024；Hou et al., 2024）。用于生成差分隐私合成数据的主要算法需要基于公共信息和/或迭代的私有客户端反馈进行精心的设计。我们的关键见解是，先前使用差分隐私合成数据方法收集的客户端反馈可以被视为偏好排名。我们的算法，基于偏好优化的私有客户端数据（POPri），利用偏好优化算法（如直接偏好优化DPO）来微调大语言模型（LLMs），以生成高质量的差分隐私合成数据。为了评估POPri，我们发布了LargeFedBench，一个新的联邦文本基准，用于评估联邦客户端数据上的大语言模型。与LargeFedBench数据集及Xie et al.（2024）的现有基准相比，POPri显著提高了差分隐私合成数据的实用性。与先前的合成数据方法相比，POPri在完全私有和非私有设置下的下一个词预测准确性差距缩小了高达68%，先前的方法为52%，最先进的差分隐私联邦学习方法为10%。代码和数据可在以下链接获取。 

---
# iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold Network 

**Title (ZH)**: iTFKAN：可解释的时间序列预测与柯尔莫戈罗夫-阿诺尔德网络 

**Authors**: Ziran Liang, Rui An, Wenqi Fan, Yanghui Rao, Yuxuan Liang  

**Link**: [PDF](https://arxiv.org/pdf/2504.16432)  

**Abstract**: As time evolves, data within specific domains exhibit predictability that motivates time series forecasting to predict future trends from historical data. However, current deep forecasting methods can achieve promising performance but generally lack interpretability, hindering trustworthiness and practical deployment in safety-critical applications such as auto-driving and healthcare. In this paper, we propose a novel interpretable model, iTFKAN, for credible time series forecasting. iTFKAN enables further exploration of model decision rationales and underlying data patterns due to its interpretability achieved through model symbolization. Besides, iTFKAN develops two strategies, prior knowledge injection, and time-frequency synergy learning, to effectively guide model learning under complex intertwined time series data. Extensive experimental results demonstrated that iTFKAN can achieve promising forecasting performance while simultaneously possessing high interpretive capabilities. 

**Abstract (ZH)**: 随时间演进，特定领域内的数据展现出可预测性，这促使时间序列预测从历史数据中预测未来趋势。然而，当前的深度预测方法虽然能够取得显著性能，但在可解释性方面 generally 缺乏，这妨碍了其在自动驾驶和医疗健康等关键安全应用中的信任度和实际部署。在本文中，我们提出了一种新颖的可解释模型 iTFKAN，以实现可信的时间序列预测。iTFKAN 通过模型符号化实现的可解释性，能够进一步探索模型决策依据和数据的基本模式。此外，iTFKAN 开发了先验知识注入和时频协同学习两种策略，以有效引导在复杂交织的时间序列数据下的模型学习。广泛实验结果表明，iTFKAN 可同时实现优异的预测性能和高可解释能力。 

---
# Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark 

**Title (ZH)**: 大语言模型能辅助多模态语言分析吗？MMLA：一项全面的基准 

**Authors**: Hanlei Zhang, Zhuohang Li, Yeshuang Zhu, Hua Xu, Peiwu Wang, Jinchao Zhang, Jie Zhou, Haige Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2504.16427)  

**Abstract**: Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA comprises over 61K multimodal utterances drawn from both staged and real-world scenarios, covering six core dimensions of multimodal semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. We evaluate eight mainstream branches of LLMs and MLLMs using three methods: zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive experiments reveal that even fine-tuned models achieve only about 60%~70% accuracy, underscoring the limitations of current MLLMs in understanding complex human language. We believe that MMLA will serve as a solid foundation for exploring the potential of large language models in multimodal language analysis and provide valuable resources to advance this field. The datasets and code are open-sourced at this https URL. 

**Abstract (ZH)**: 多模态语言分析是一个迅速发展的领域，它利用多种模态来增强对人类对话表达高级语义的理解。尽管其重要性不言而喻，但鲜有研究探讨多模态大规模语言模型（MLLMs）在理解认知层次语义方面的能力。本文我们引入了MMLA，这是一个专门为此空白设计的综合基准。MMLA 包含超过61,000条多模态对话数据，涵盖了六种核心的多模态语义维度：意图、情绪、对话行为、情感、说话风格和沟通行为。我们使用三种方法评估了八种主流的LLM和MLLM分支：零样本推理、监督微调和指令微调。广泛的实验表明，即使是微调后的模型也只能达到约60%~70%的准确率，突显了当前MLLMs在理解复杂人类语言方面的局限性。我们认为MMLA 将为探索大规模语言模型在多模态语言分析中的潜力提供坚实的基础，并提供有价值的数据资源来推进这一领域的发展。数据集和代码已开源于此链接：this https URL。 

---
# A Survey of Foundation Model-Powered Recommender Systems: From Feature-Based, Generative to Agentic Paradigms 

**Title (ZH)**: 基于基础模型的推荐系统综述：从特征导向、生成导向到能动导向 paradigm 

**Authors**: Chengkai Huang, Hongtao Huang, Tong Yu, Kaige Xie, Junda Wu, Shuai Zhang, Julian Mcauley, Dietmar Jannach, Lina Yao  

**Link**: [PDF](https://arxiv.org/pdf/2504.16420)  

**Abstract**: Recommender systems (RS) have become essential in filtering information and personalizing content for users. RS techniques have traditionally relied on modeling interactions between users and items as well as the features of content using models specific to each task. The emergence of foundation models (FMs), large scale models trained on vast amounts of data such as GPT, LLaMA and CLIP, is reshaping the recommendation paradigm. This survey provides a comprehensive overview of the Foundation Models for Recommender Systems (FM4RecSys), covering their integration in three paradigms: (1) Feature-Based augmentation of representations, (2) Generative recommendation approaches, and (3) Agentic interactive systems. We first review the data foundations of RS, from traditional explicit or implicit feedback to multimodal content sources. We then introduce FMs and their capabilities for representation learning, natural language understanding, and multi-modal reasoning in RS contexts. The core of the survey discusses how FMs enhance RS under different paradigms. Afterward, we examine FM applications in various recommendation tasks. Through an analysis of recent research, we highlight key opportunities that have been realized as well as challenges encountered. Finally, we outline open research directions and technical challenges for next-generation FM4RecSys. This survey not only reviews the state-of-the-art methods but also provides a critical analysis of the trade-offs among the feature-based, the generative, and the agentic paradigms, outlining key open issues and future research directions. 

**Abstract (ZH)**: 基础模型在推荐系统中的应用：一种涵盖特征增强表示、生成推荐方法和自主交互系统的全面概述 

---
# PixelWeb: The First Web GUI Dataset with Pixel-Wise Labels 

**Title (ZH)**: PixelWeb: 首个具有像素级标签的Web GUI数据集 

**Authors**: Qi Yang, Weichen Bi, Haiyang Shen, Yaoqi Guo, Yun Ma  

**Link**: [PDF](https://arxiv.org/pdf/2504.16419)  

**Abstract**: Graphical User Interface (GUI) datasets are crucial for various downstream tasks. However, GUI datasets often generate annotation information through automatic labeling, which commonly results in inaccurate GUI element BBox annotations, including missing, duplicate, or meaningless BBoxes. These issues can degrade the performance of models trained on these datasets, limiting their effectiveness in real-world applications. Additionally, existing GUI datasets only provide BBox annotations visually, which restricts the development of visually related GUI downstream tasks. To address these issues, we introduce PixelWeb, a large-scale GUI dataset containing over 100,000 annotated web pages. PixelWeb is constructed using a novel automatic annotation approach that integrates visual feature extraction and Document Object Model (DOM) structure analysis through two core modules: channel derivation and layer analysis. Channel derivation ensures accurate localization of GUI elements in cases of occlusion and overlapping elements by extracting BGRA four-channel bitmap annotations. Layer analysis uses the DOM to determine the visibility and stacking order of elements, providing precise BBox annotations. Additionally, PixelWeb includes comprehensive metadata such as element images, contours, and mask annotations. Manual verification by three independent annotators confirms the high quality and accuracy of PixelWeb annotations. Experimental results on GUI element detection tasks show that PixelWeb achieves performance on the mAP95 metric that is 3-7 times better than existing datasets. We believe that PixelWeb has great potential for performance improvement in downstream tasks such as GUI generation and automated user interaction. 

**Abstract (ZH)**: 图形用户界面（GUI）数据集对于各种下游任务至关重要。然而，GUI数据集往往通过自动标注生成注释信息，这通常会导致GUI元素边界框（BBox）注释不准确，包括缺失、重复或无意义的边界框。这些问题会降低基于这些数据集训练的模型的性能，限制其在实际应用中的效果。此外，现有的GUI数据集仅提供可视化的边界框注释，这限制了与GUI相关的视觉下游任务的发展。为了解决这些问题，我们引入了PixelWeb，这是一个包含超过100,000个标注网页的大规模GUI数据集。PixelWeb利用一种新颖的自动标注方法构建，该方法结合了视觉特征提取和文档对象模型（DOM）结构分析，通过两个核心模块：通道衍生和层分析来实现。通道衍生通过提取BGRA四通道位图注释确保GUI元素的准确定位，尤其是在遮挡和重叠的情况下。层分析利用DOM来确定元素的可见性和堆叠顺序，提供精确的边界框注释。此外，PixelWeb还包括元素图像、轮廓和掩码注释等全面的元数据。三名独立注释者的手动验证证实了PixelWeb注释的高质量和准确性。在GUI元素检测任务上的实验结果表明，PixelWeb在mAP95指标上的性能比现有数据集高出3到7倍。我们相信，PixelWeb在GUI生成和自动用户交互等下游任务中的性能改进方面具有巨大潜力。 

---
# FeedQUAC: Quick Unobtrusive AI-Generated Commentary 

**Title (ZH)**: FeedQUAC: 快速不显干预的AI生成评论 

**Authors**: Tao Long, Kendra Wannamaker, Jo Vermeulen, George Fitzmaurice, Justin Matejka  

**Link**: [PDF](https://arxiv.org/pdf/2504.16416)  

**Abstract**: Design thrives on feedback. However, gathering constant feedback throughout the design process can be labor-intensive and disruptive. We explore how AI can bridge this gap by providing effortless, ambient feedback. We introduce FeedQUAC, a design companion that delivers real-time AI-generated commentary from a variety of perspectives through different personas. A design probe study with eight participants highlights how designers can leverage quick yet ambient AI feedback to enhance their creative workflows. Participants highlight benefits such as convenience, playfulness, confidence boost, and inspiration from this lightweight feedback agent, while suggesting additional features, like chat interaction and context curation. We discuss the role of AI feedback, its strengths and limitations, and how to integrate it into existing design workflows while balancing user involvement. Our findings also suggest that ambient interaction is a valuable consideration for both the design and evaluation of future creativity support systems. 

**Abstract (ZH)**: 设计依赖于反馈。然而，在设计过程中不断收集反馈可能是劳动密集型且具有干扰性的。我们探索AI如何通过提供轻松且无感知的反馈来弥合这一差距。我们介绍了FeedQUAC，这是一种设计伴侣，能够通过不同的角色提供实时的AI生成的多视角评论。一项涉及八名参与者的探索性设计实验突显了设计者如何利用快速且无感知的AI反馈来增强其创意工作流程。参与者强调了这种轻量级反馈代理的便利性、趣味性、信心提升和灵感，并建议增加诸如聊天交互和内容筛选等功能。我们讨论了AI反馈的作用、优势和局限性，以及如何在平衡用户参与的情况下将其整合到现有的设计工作流程中。研究结果还表明，无感知交互对于未来创意支持系统的构思和评估均是值得考虑的重要因素。 

---
# Assessing the Feasibility of Internet-Sourced Video for Automatic Cattle Lameness Detection 

**Title (ZH)**: 评估互联网来源视频在自动检测奶牛跛行中的可行性 

**Authors**: Md Fahimuzzman Sohan  

**Link**: [PDF](https://arxiv.org/pdf/2504.16404)  

**Abstract**: Cattle lameness is often caused by hoof injuries or interdigital dermatitis, leads to pain and significantly impacts essential physiological activities such as walking, feeding, and drinking. This study presents a deep learning-based model for detecting cattle lameness, sickness, or gait abnormalities using publicly available video data. The dataset consists of 50 unique videos from 40 individual cattle, recorded from various angles in both indoor and outdoor environments. Half of the dataset represents naturally walking (normal/non-lame) cattle, while the other half consists of cattle exhibiting gait abnormalities (lame). To enhance model robustness and generalizability, data augmentation was applied to the training data. The pre-processed videos were then classified using two deep learning models: ConvLSTM2D and 3D CNN. A comparative analysis of the results demonstrates strong classification performance. Specifically, the 3D CNN model achieved a video-level classification accuracy of 90%, with precision, recall, and f1-score of 90.9%, 90.9%, and 90.91% respectively. The ConvLSTM2D model exhibited a slightly lower accuracy of 85%. This study highlights the effectiveness of directly applying classification models to learn spatiotemporal features from video data, offering an alternative to traditional multi-stage approaches that typically involve object detection, pose estimation, and feature extraction. Besides, the findings demonstrate that the proposed deep learning models, particularly the 3D CNN, effectively classify and detect lameness in cattle while simplifying the processing pipeline. 

**Abstract (ZH)**: 基于深度学习的牛只跛行、疾病或行态异常检测模型研究 

---
# ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs 

**Title (ZH)**: ConTextual: 在上下文保留的 Tokens 过滤和知识图谱基础上提升临床文本总结的大模型性能 

**Authors**: Fahmida Liza Piya, Rahmatollah Beheshti  

**Link**: [PDF](https://arxiv.org/pdf/2504.16394)  

**Abstract**: Unstructured clinical data can serve as a unique and rich source of information that can meaningfully inform clinical practice. Extracting the most pertinent context from such data is critical for exploiting its true potential toward optimal and timely decision-making in patient care. While prior research has explored various methods for clinical text summarization, most prior studies either process all input tokens uniformly or rely on heuristic-based filters, which can overlook nuanced clinical cues and fail to prioritize information critical for decision-making. In this study, we propose Contextual, a novel framework that integrates a Context-Preserving Token Filtering method with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By preserving context-specific important tokens and enriching them with structured knowledge, ConTextual improves both linguistic coherence and clinical fidelity. Our extensive empirical evaluations on two public benchmark datasets demonstrate that ConTextual consistently outperforms other baselines. Our proposed approach highlights the complementary role of token-level filtering and structured retrieval in enhancing both linguistic and clinical integrity, as well as offering a scalable solution for improving precision in clinical text generation. 

**Abstract (ZH)**: 无结构化临床数据可以作为独特且丰富的信息来源，对临床实践具有重要意义。从此类数据中提取最相关的上下文对于充分发挥其潜力、实现最优和及时的决策至关重要。尽管先前研究探索了各种临床文本总结方法，但大多数前期研究要么以统一方式处理所有输入词，要么依赖于基于启发式的过滤器，这些方法可能会忽略精细的临床线索，无法优先处理对决策至关重要的信息。在本研究中，我们提出了一种名为Contextual的新框架，该框架结合了上下文保留的 token 过滤方法和领域特定知识图谱（KG）进行上下文增强。通过保留上下文特定的重要 token 并通过结构化知识进行丰富，ConTextual提高了语言连贯性和临床准确性。我们在两个公开基准数据集上的广泛实证评估结果显示，ConTextual始终优于其他基线方法。我们提出的方法突显了 token 级别过滤和结构化检索在增强语言和临床完整性方面的作用，并提供了提高临床文本生成精确性的可扩展解决方案。 

---
# PINN-MEP: Continuous Neural Representations for Minimum-Energy Path Discovery in Molecular Systems 

**Title (ZH)**: PINN-MEP：分子系统中最小能量路径发现的连续神经表示方法 

**Authors**: Magnus Petersen, Roberto Covino  

**Link**: [PDF](https://arxiv.org/pdf/2504.16381)  

**Abstract**: Characterizing conformational transitions in physical systems remains a fundamental challenge in the computational sciences. Traditional sampling methods like molecular dynamics (MD) or MCMC often struggle with the high-dimensional nature of molecular systems and the high energy barriers of transitions between stable states. While these transitions are rare events in simulation timescales, they often represent the most biologically significant processes - for example, the conformational change of an ion channel protein from its closed to open state, which controls cellular ion flow and is crucial for neural signaling. Such transitions in real systems may take milliseconds to seconds but could require months or years of continuous simulation to observe even once. We present a method that reformulates transition path generation as a continuous optimization problem solved through physics-informed neural networks (PINNs) inspired by string methods for minimum-energy path (MEP) generation. By representing transition paths as implicit neural functions and leveraging automatic differentiation with differentiable molecular dynamics force fields, our method enables the efficient discovery of physically realistic transition pathways without requiring expensive path sampling. We demonstrate our method's effectiveness on two proteins, including an explicitly hydrated bovine pancreatic trypsin inhibitor (BPTI) system with over 8,300 atoms. 

**Abstract (ZH)**: 物理系统构象转变的特性描述仍然是计算科学中的一个基础挑战。传统的采样方法，如分子动力学（MD）或MCMC，在处理分子系统的高维性质以及稳定状态间高能垒的过渡时常常力不从心。虽然这些过渡事件在模拟时间尺度上是罕见的，但它们往往代表了生物上最显著的过程——例如离子通道蛋白从关闭状态到打开状态的构象变化，这控制着细胞离子流，并对于神经信号传导至关重要。这类过程在实际系统中可能需要毫秒到秒的时间，但在长时间连续模拟中观测到这些过程却可能需要数月至数年。我们提出了一种方法，将过渡路径生成重新表述为通过物理启发式神经网络（PINNs）求解的连续优化问题，该方法受最低能量路径（MEP）生成的串行方法启发。通过将过渡路径表示为隐式神经函数，并利用自动微分和可微分子动力学势场，我们的方法能够在无需昂贵路径采样的情况下高效发现物理上可行的过渡路径。我们在两个蛋白质上展示了该方法的有效性，包括含有超过8,300个原子的明确水化的牛胰糜蛋白酶抑制剂（BPTI）系统。 

---
# Cyberoception: Finding a Painlessly-Measurable New Sense in the Cyberworld Towards Emotion-Awareness in Computing 

**Title (ZH)**: 网络感知：在网络世界中寻找一种无痛可测的新感觉，以实现计算中的情感意识 

**Authors**: Tadashi Okoshi, Zexiong Gao, Tan Yi Zhen, Takumi Karasawa, Takeshi Miki, Wataru Sasaki, Rajesh K. Balan  

**Link**: [PDF](https://arxiv.org/pdf/2504.16378)  

**Abstract**: In Affective computing, recognizing users' emotions accurately is the basis of affective human-computer interaction. Understanding users' interoception contributes to a better understanding of individually different emotional abilities, which is essential for achieving inter-individually accurate emotion estimation. However, existing interoception measurement methods, such as the heart rate discrimination task, have several limitations, including their dependence on a well-controlled laboratory environment and precision apparatus, making monitoring users' interoception challenging. This study aims to determine other forms of data that can explain users' interoceptive or similar states in their real-world lives and propose a novel hypothetical concept "cyberoception," a new sense (1) which has properties similar to interoception in terms of the correlation with other emotion-related abilities, and (2) which can be measured only by the sensors embedded inside commodity smartphone devices in users' daily lives. Results from a 10-day-long in-lab/in-the-wild hybrid experiment reveal a specific cyberoception type "Turn On" (users' subjective sensory perception about the frequency of turning-on behavior on their smartphones), significantly related to participants' emotional valence. We anticipate that cyberoception to serve as a fundamental building block for developing more "emotion-aware", user-friendly applications and services. 

**Abstract (ZH)**: 在情感计算中，准确识别用户的情绪是实现情感人机交互的基础。理解用户的内感受有助于更好地理解个体之间不同的情感能力，这对于实现个体之间准确的情绪估计至关重要。然而，现有的内感受测量方法，如心率辨别任务，存在一些局限性，包括依赖于受控实验室环境和精确的测量仪器，这使得监测用户的内感受具有挑战性。本研究旨在确定其他形式的数据，以解释用户在现实生活中的内感受或类似状态，并提出一个全新的假设概念“赛博感受”，这是一种新的感觉（1）其与与情绪相关的其他能力的相关性类似于内感受的属性；（2）仅通过用户日常生活中内置在智能手机设备中的传感器进行测量。一项为期10天的室内/室外混合实验表明，“开启”（用户对其智能手机开机行为频率的主观感知感受）这一赛博感受类型与参与者的情绪效价显著相关。我们期待赛博感受成为开发更多“情绪感知”、用户友好型应用和服务的基础构建块。 

---
# CLPSTNet: A Progressive Multi-Scale Convolutional Steganography Model Integrating Curriculum Learning 

**Title (ZH)**: CLPSTNet：一种融合分级学习的 Progressive 多尺度卷积隐写模型 

**Authors**: Fengchun Liu, Tong Zhang, Chunying Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2504.16364)  

**Abstract**: In recent years, a large number of works have introduced Convolutional Neural Networks (CNNs) into image steganography, which transform traditional steganography methods such as hand-crafted features and prior knowledge design into steganography methods that neural networks autonomically learn information embedding. However, due to the inherent complexity of digital images, issues of invisibility and security persist when using CNN models for information embedding. In this paper, we propose Curriculum Learning Progressive Steganophy Network (CLPSTNet). The network consists of multiple progressive multi-scale convolutional modules that integrate Inception structures and dilated convolutions. The module contains multiple branching pathways, starting from a smaller convolutional kernel and dilatation rate, extracting the basic, local feature information from the feature map, and gradually expanding to the convolution with a larger convolutional kernel and dilatation rate for perceiving the feature information of a larger receptive field, so as to realize the multi-scale feature extraction from shallow to deep, and from fine to coarse, allowing the shallow secret information features to be refined in different fusion stages. The experimental results show that the proposed CLPSTNet not only has high PSNR , SSIM metrics and decoding accuracy on three large public datasets, ALASKA2, VOC2012 and ImageNet, but also the steganographic images generated by CLPSTNet have low steganalysis this http URL can find our code at \href{this https URL}{this https URL}. 

**Abstract (ZH)**: 基于逐层学习的多尺度卷积秘密嵌入网络（CLPSTNet） 

---
# DP2FL: Dual Prompt Personalized Federated Learning in Foundation Models 

**Title (ZH)**: DP2FL: 基础模型中的双重提示个性化联邦学习 

**Authors**: Ying Chang, Xiaohu Shi, Xiaohui Zhao, Zhaohuang Chen, Deyin Ma  

**Link**: [PDF](https://arxiv.org/pdf/2504.16357)  

**Abstract**: Personalized federated learning (PFL) has garnered significant attention for its ability to address heterogeneous client data distributions while preserving data privacy. However, when local client data is limited, deep learning models often suffer from insufficient training, leading to suboptimal performance. Foundation models, such as CLIP (Contrastive Language-Image Pretraining), exhibit strong feature extraction capabilities and can alleviate this issue by fine-tuning on limited local data. Despite their potential, foundation models are rarely utilized in federated learning scenarios, and challenges related to integrating new clients remain largely unresolved. To address these challenges, we propose the Dual Prompt Personalized Federated Learning (DP2FL) framework, which introduces dual prompts and an adaptive aggregation strategy. DP2FL combines global task awareness with local data-driven insights, enabling local models to achieve effective generalization while remaining adaptable to specific data distributions. Moreover, DP2FL introduces a global model that enables prediction on new data sources and seamlessly integrates newly added clients without requiring retraining. Experimental results in highly heterogeneous environments validate the effectiveness of DP2FL's prompt design and aggregation strategy, underscoring the advantages of prediction on novel data sources and demonstrating the seamless integration of new clients into the federated learning framework. 

**Abstract (ZH)**: 双提示个性化联邦学习（DP2FL）框架 

---
# Transformer-Based Extraction of Statutory Definitions from the U.S. Code 

**Title (ZH)**: 基于变压器的美国法典法定定义提取 

**Authors**: Arpana Hosabettu, Harsh Shah  

**Link**: [PDF](https://arxiv.org/pdf/2504.16353)  

**Abstract**: Automatic extraction of definitions from legal texts is critical for enhancing the comprehension and clarity of complex legal corpora such as the United States Code (U.S.C.). We present an advanced NLP system leveraging transformer-based architectures to automatically extract defined terms, their definitions, and their scope from the U.S.C. We address the challenges of automatically identifying legal definitions, extracting defined terms, and determining their scope within this complex corpus of over 200,000 pages of federal statutory law. Building upon previous feature-based machine learning methods, our updated model employs domain-specific transformers (Legal-BERT) fine-tuned specifically for statutory texts, significantly improving extraction accuracy. Our work implements a multi-stage pipeline that combines document structure analysis with state-of-the-art language models to process legal text from the XML version of the U.S. Code. Each paragraph is first classified using a fine-tuned legal domain BERT model to determine if it contains a definition. Our system then aggregates related paragraphs into coherent definitional units and applies a combination of attention mechanisms and rule-based patterns to extract defined terms and their jurisdictional scope. The definition extraction system is evaluated on multiple titles of the U.S. Code containing thousands of definitions, demonstrating significant improvements over previous approaches. Our best model achieves 96.8% precision and 98.9% recall (98.2% F1-score), substantially outperforming traditional machine learning classifiers. This work contributes to improving accessibility and understanding of legal information while establishing a foundation for downstream legal reasoning tasks. 

**Abstract (ZH)**: 自动从法律文本中提取定义对于增强如美国法典（U.S.C.）等复杂法律文集的可理解性和清晰度至关重要。我们提出了一种基于变换器架构的高级自然语言处理系统，用于自动从美国法典中提取定义术语、其定义及其适用范围。我们解决了自动识别法律定义、提取定义术语以及确定其在复杂文集中的适用范围的挑战，该文集包含超过20万页的联邦立法法条。基于以前的基于特征的机器学习方法，我们的更新模型采用特定领域变换器（Legal-BERT）进行微调，专为立法文本设计，显著提高了提取准确性。我们的工作实施了一个多阶段管道，结合文档结构分析和最先进的语言模型来处理来自美国法典XML版本的法律文本。每个段落首先使用微调的法律领域BERT模型进行分类，以确定是否包含定义。系统随后将相关段落聚合为连贯的定义单元，并应用注意力机制和基于规则的模式组合来提取定义术语及其管辖范围。定义提取系统在包含数千个定义的美国法典多个标题上进行了评估，展示了与之前方法相比的重大改进。我们的最佳模型实现96.8%的精确率和98.9%的召回率（F1分数为98.2%），显著优于传统机器学习分类器。本工作有助于提高法律信息的可访问性和理解，同时为后续法律推理任务奠定基础。 

---
# Disentangling and Generating Modalities for Recommendation in Missing Modality Scenarios 

**Title (ZH)**: 在缺失模态场景中解耦和生成模态进行推荐 

**Authors**: Jiwan Kim, Hongseok Kang, Sein Kim, Kibum Kim, Chanyoung Park  

**Link**: [PDF](https://arxiv.org/pdf/2504.16352)  

**Abstract**: Multi-modal recommender systems (MRSs) have achieved notable success in improving personalization by leveraging diverse modalities such as images, text, and audio. However, two key challenges remain insufficiently addressed: (1) Insufficient consideration of missing modality scenarios and (2) the overlooking of unique characteristics of modality features. These challenges result in significant performance degradation in realistic situations where modalities are missing. To address these issues, we propose Disentangling and Generating Modality Recommender (DGMRec), a novel framework tailored for missing modality scenarios. DGMRec disentangles modality features into general and specific modality features from an information-based perspective, enabling richer representations for recommendation. Building on this, it generates missing modality features by integrating aligned features from other modalities and leveraging user modality preferences. Extensive experiments show that DGMRec consistently outperforms state-of-the-art MRSs in challenging scenarios, including missing modalities and new item settings as well as diverse missing ratios and varying levels of missing modalities. Moreover, DGMRec's generation-based approach enables cross-modal retrieval, a task inapplicable for existing MRSs, highlighting its adaptability and potential for real-world applications. Our code is available at this https URL. 

**Abstract (ZH)**: 多模态推荐系统（MRSs）通过利用图像、文本和音频等多种模态取得了显著的个性化提升。然而，仍存在两个关键挑战：（1）对缺失模态场景考虑不足；（2）忽视了模态特征的独特性。这些挑战导致在实际情况下模态缺失时性能显著下降。为解决这些问题，我们提出了基于信息视角解耦和生成模态推荐（DGMRec）的新框架，该框架专门针对缺失模态场景。DGMRec从信息视角将模态特征解耦为通用和特定的模态特征，从而为推荐提供更丰富的表示。在此基础上，它通过整合其他模态的对齐特征并利用用户的模态偏好生成缺失模态特征。广泛的实验表明，DGMRec在包含缺失模态和新项目设置等各种具有挑战性的场景中，以及在不同的缺失比例和不同的模态缺失程度下，均优于现有的MRSs。此外，DGMRec的生成方法使其能够进行跨模态检索，这是现有MRSs无法实现的任务，凸显了其适应性和在实际应用中的潜力。我们的代码可在以下链接获取：this https URL。 

---
# QAOA-GPT: Efficient Generation of Adaptive and Regular Quantum Approximate Optimization Algorithm Circuits 

**Title (ZH)**: QAOA-GPT: 高效生成自适应和规则量子近似优化算法电路 

**Authors**: Ilya Tyagin, Marwa H. Farag, Kyle Sherbert, Karunya Shirali, Yuri Alexeev, Ilya Safro  

**Link**: [PDF](https://arxiv.org/pdf/2504.16350)  

**Abstract**: Quantum computing has the potential to improve our ability to solve certain optimization problems that are computationally difficult for classical computers, by offering new algorithmic approaches that may provide speedups under specific conditions. In this work, we introduce QAOA-GPT, a generative framework that leverages Generative Pretrained Transformers (GPT) to directly synthesize quantum circuits for solving quadratic unconstrained binary optimization problems, and demonstrate it on the MaxCut problem on graphs. To diversify the training circuits and ensure their quality, we have generated a synthetic dataset using the adaptive QAOA approach, a method that incrementally builds and optimizes problem-specific circuits. The experiments conducted on a curated set of graph instances demonstrate that QAOA-GPT, generates high quality quantum circuits for new problem instances unseen in the training as well as successfully parametrizes QAOA. Our results show that using QAOA-GPT to generate quantum circuits will significantly decrease both the computational overhead of classical QAOA and adaptive approaches that often use gradient evaluation to generate the circuit and the classical optimization of the circuit parameters. Our work shows that generative AI could be a promising avenue to generate compact quantum circuits in a scalable way. 

**Abstract (ZH)**: 量子计算有望通过提供在特定条件下可能加速的新算法方法，提高我们解决某些计算上具有挑战性的优化问题的能力。在此工作中，我们介绍了QAOA-GPT，这是一种利用生成预训练变换器（GPT）直接合成求解无约束二次二元优化问题的量子电路的生成框架，并在图的MaxCut问题上进行了演示。为确保训练电路的质量并增加多样性，我们使用自适应QAOA方法生成了一个合成数据集，该方法能够逐步构建和优化针对特定问题的电路。针对精心挑选的图实例进行的实验表明，QAOA-GPT能够生成高质量的量子电路，适用于未在训练中出现的新问题实例，并成功参数化了QAOA。我们的结果表明，使用QAOA-GPT生成量子电路将大幅减少经典QAOA及其通常使用梯度评估生成电路和经典优化电路参数的自适应方法的计算开销。我们的工作表明，生成式AI可能是以可扩展的方式生成紧凑量子电路的一个有前途的方向。 

---
# Mining Software Repositories for Expert Recommendation 

**Title (ZH)**: 从软件仓库中挖掘专家推荐 

**Authors**: Chad Marshall, Andrew Barovic, Armin Moin  

**Link**: [PDF](https://arxiv.org/pdf/2504.16343)  

**Abstract**: We propose an automated approach to bug assignment to developers in large open-source software projects. This way, we assist human bug triagers who are in charge of finding the best developer with the right level of expertise in a particular area to be assigned to a newly reported issue. Our approach is based on the history of software development as documented in the issue tracking systems. We deploy BERTopic and techniques from TopicMiner. Our approach works based on the bug reports' features, such as the corresponding products and components, as well as their priority and severity levels. We sort developers based on their experience with specific combinations of new reports. The evaluation is performed using Top-k accuracy, and the results are compared with the reported results in prior work, namely TopicMiner MTM, BUGZIE, Bug triaging via deep Reinforcement Learning BT-RL, and LDA-SVM. The evaluation data come from various Eclipse and Mozilla projects, such as JDT, Firefox, and Thunderbird. 

**Abstract (ZH)**: 我们提出了一种自动化的方法，用于在大型开源软件项目中自动分配错误给开发者。通过这种方法，我们协助管理人员错误的人员找到最适合解决特定问题的具有适当专业知识水平的开发者。我们的方法基于问题跟踪系统中记录的软件开发历史。我们部署了BERTopic和TopicMiner的技术。我们的方法基于错误报告的特征，如相关的产物和组件，以及它们的优先级和严重程度。我们将开发者根据其对新报告的特定组合的经验进行排序。评估使用Top-k准确率进行，结果与先前的工作，即TopicMiner MTM、BUGZIE、基于深度强化学习的错误处理BT-RL以及LDA-SVM的比较。评估数据来自多个Eclipse和Mozilla项目，如JDT、Firefox和Thunderbird。 

---
# On the Consistency of GNN Explanations for Malware Detection 

**Title (ZH)**: 基于图神经网络恶意软件检测解释的一致性研究 

**Authors**: Hossein Shokouhinejad, Griffin Higgins, Roozbeh Razavi-Far, Hesamodin Mohammadian, Ali A. Ghorbani  

**Link**: [PDF](https://arxiv.org/pdf/2504.16316)  

**Abstract**: Control Flow Graphs (CFGs) are critical for analyzing program execution and characterizing malware behavior. With the growing adoption of Graph Neural Networks (GNNs), CFG-based representations have proven highly effective for malware detection. This study proposes a novel framework that dynamically constructs CFGs and embeds node features using a hybrid approach combining rule-based encoding and autoencoder-based embedding. A GNN-based classifier is then constructed to detect malicious behavior from the resulting graph representations. To improve model interpretability, we apply state-of-the-art explainability techniques, including GNNExplainer, PGExplainer, and CaptumExplainer, the latter is utilized three attribution methods: Integrated Gradients, Guided Backpropagation, and Saliency. In addition, we introduce a novel aggregation method, called RankFusion, that integrates the outputs of the top-performing explainers to enhance the explanation quality. We also evaluate explanations using two subgraph extraction strategies, including the proposed Greedy Edge-wise Composition (GEC) method for improved structural coherence. A comprehensive evaluation using accuracy, fidelity, and consistency metrics demonstrates the effectiveness of the proposed framework in terms of accurate identification of malware samples and generating reliable and interpretable explanations. 

**Abstract (ZH)**: 基于控制流图的图神经网络新型框架：结合规则编码和自编码嵌入的恶意软件检测与解释 

---
# DataS^3: Dataset Subset Selection for Specialization 

**Title (ZH)**: DataS³: 数据子集选择以实现专业化 

**Authors**: Neha Hulkund, Alaa Maalouf, Levi Cai, Daniel Yang, Tsun-Hsuan Wang, Abigail O'Neil, Timm Haucke, Sandeep Mukherjee, Vikram Ramaswamy, Judy Hansen Shen, Gabriel Tseng, Mike Walmsley, Daniela Rus, Ken Goldberg, Hannah Kerner, Irene Chen, Yogesh Girdhar, Sara Beery  

**Link**: [PDF](https://arxiv.org/pdf/2504.16277)  

**Abstract**: In many real-world machine learning (ML) applications (e.g. detecting broken bones in x-ray images, detecting species in camera traps), in practice models need to perform well on specific deployments (e.g. a specific hospital, a specific national park) rather than the domain broadly. However, deployments often have imbalanced, unique data distributions. Discrepancy between the training distribution and the deployment distribution can lead to suboptimal performance, highlighting the need to select deployment-specialized subsets from the available training data. We formalize dataset subset selection for specialization (DS3): given a training set drawn from a general distribution and a (potentially unlabeled) query set drawn from the desired deployment-specific distribution, the goal is to select a subset of the training data that optimizes deployment performance.
We introduce DataS^3; the first dataset and benchmark designed specifically for the DS3 problem. DataS^3 encompasses diverse real-world application domains, each with a set of distinct deployments to specialize in. We conduct a comprehensive study evaluating algorithms from various families--including coresets, data filtering, and data curation--on DataS^3, and find that general-distribution methods consistently fail on deployment-specific tasks. Additionally, we demonstrate the existence of manually curated (deployment-specific) expert subsets that outperform training on all available data with accuracy gains up to 51.3 percent. Our benchmark highlights the critical role of tailored dataset curation in enhancing performance and training efficiency on deployment-specific distributions, which we posit will only become more important as global, public datasets become available across domains and ML models are deployed in the real world. 

**Abstract (ZH)**: 在实际应用场景中（例如，X射线图像中的骨折检测，相机陷阱中的物种检测），模型需要在特定部署（如特定医院，特定国家公园）上表现良好，而不是在广泛的领域中。然而，部署往往具有不平衡且独特的数据分布。训练分布与部署分布之间的差异会导致性能不佳，从而强调需要从可用的训练数据中选择部署专门化的子集。我们形式化了数据子集选择以专门化（Dataset Subset Selection for Specialization, DS3）：给定一个来自一般分布的训练集和一个（可能未标注的）来自目标部署特定分布的查询集，目标是选择一个优化部署性能的训练数据子集。

DataS^3：第一个专门为DS3问题设计的数据集和基准。DataS^3涵盖了多种多样的实际应用场景领域，每个领域都有专门化的部署集。我们在DataS^3上对来自不同家族的算法（包括核集、数据过滤和数据整理）进行了全面评估，发现一般分布方法在特定部署任务上表现不佳。此外，我们证明了手动整理的（部署特定的）专家子集优于在所有可用数据上进行训练，准确率可提高多达51.3%。我们的基准突显了针对特定部署分布进行精心数据整理在提高性能和训练效率方面的重要作用，我们认为随着全球公共数据集在各领域中的普及以及ML模型在现实世界的部署，这一作用将变得更加重要。 

---
# An Automated Pipeline for Few-Shot Bird Call Classification: A Case Study with the Tooth-Billed Pigeon 

**Title (ZH)**: 基于Few-Shot鸟类叫声分类的自动化管道：以齿aileronto鸽为例的研究案例 

**Authors**: Abhishek Jana, Moeumu Uili, James Atherton, Mark O'Brien, Joe Wood, Leandra Brickson  

**Link**: [PDF](https://arxiv.org/pdf/2504.16276)  

**Abstract**: This paper presents an automated one-shot bird call classification pipeline designed for rare species absent from large publicly available classifiers like BirdNET and Perch. While these models excel at detecting common birds with abundant training data, they lack options for species with only 1-3 known recordings-a critical limitation for conservationists monitoring the last remaining individuals of endangered birds. To address this, we leverage the embedding space of large bird classification networks and develop a classifier using cosine similarity, combined with filtering and denoising preprocessing techniques, to optimize detection with minimal training data. We evaluate various embedding spaces using clustering metrics and validate our approach in both a simulated scenario with Xeno-Canto recordings and a real-world test on the critically endangered tooth-billed pigeon (Didunculus strigirostris), which has no existing classifiers and only three confirmed recordings. The final model achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon calls, making it practical for use in the field. This open-source system provides a practical tool for conservationists seeking to detect and monitor rare species on the brink of extinction. 

**Abstract (ZH)**: 一种针对大型公开分类器（如BirdNET和Perch）未包含的稀有鸟类设计的自动化单次鸟类叫声分类管道的研究 

---
# Quantum Doubly Stochastic Transformers 

**Title (ZH)**: 量子双随机变换器 

**Authors**: Jannis Born, Filip Skogh, Kahn Rhrissorrakrai, Filippo Utro, Nico Wagner, Aleksandros Sobczyk  

**Link**: [PDF](https://arxiv.org/pdf/2504.16275)  

**Abstract**: At the core of the Transformer, the Softmax normalizes the attention matrix to be right stochastic. Previous research has shown that this often destabilizes training and that enforcing the attention matrix to be doubly stochastic (through Sinkhorn's algorithm) consistently improves performance across different tasks, domains and Transformer flavors. However, Sinkhorn's algorithm is iterative, approximative, non-parametric and thus inflexible w.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been proven that DSMs can be obtained with a parametric quantum circuit, yielding a novel quantum inductive bias for DSMs with no known classical analogue. Motivated by this, we demonstrate the feasibility of a hybrid classical-quantum doubly stochastic Transformer (QDSFormer) that replaces the Softmax in the self-attention layer with a variational quantum circuit. We study the expressive power of the circuit and find that it yields more diverse DSMs that better preserve information than classical operators. Across multiple small-scale object recognition tasks, we find that our QDSFormer consistently surpasses both a standard Vision Transformer and other doubly stochastic Transformers. Beyond the established Sinkformer, this comparison includes a novel quantum-inspired doubly stochastic Transformer (based on QR decomposition) that can be of independent interest. The QDSFormer also shows improved training stability and lower performance variation suggesting that it may mitigate the notoriously unstable training of ViTs on small-scale data. 

**Abstract (ZH)**: 基于变换器的软-max归一化可将注意力矩阵标准化为正确的随机矩阵。前期研究显示，这经常导致训练不稳定，而通过Sinkhorn算法强制注意力矩阵成为双随机矩阵（DSM）则在不同任务、领域和变换器版本中均能一致地提升性能。然而，Sinkhorn算法是迭代的、近似的、非参数化的，因此在获得双随机矩阵方面不够灵活。最近的研究证明，可以通过参数化的量子电路获得DSM，这为DSM提供了一种新型的量子归纳偏置，而这种偏置在经典的类比中尚不存在。受此启发，我们证明了一种混合经典-量子双随机变换器（QDSFormer）的可行性，其中自注意力层中的Softmax被变分量子电路替代。我们研究了该电路的表达能力，发现其产生的DSM更加多样，更能保留信息，而不同于经典的算子。在多个小型对象识别任务中，我们发现我们的QDSFormer在性能上始终优于标准视觉变换器和其他双随机变换器。除了现有的Sinkformer之外，此比较还包括一种基于QR分解的新型量子启发双随机变换器，该变换器也具有独立的研究价值。QDSFormer还显示出改进的训练稳定性和较低的性能变异性，表明它可能缓解视觉变换器在小规模数据上的 notoriously 不稳定训练问题。 

---
# Boosting Classifier Performance with Opposition-Based Data Transformation 

**Title (ZH)**: 基于反对面数据转换的分类器性能提升方法 

**Authors**: Abdesslem Layeb  

**Link**: [PDF](https://arxiv.org/pdf/2504.16268)  

**Abstract**: In this paper, we introduce a novel data transformation framework based on Opposition-Based Learning (OBL) to boost the performance of traditional classification algorithms. Originally developed to accelerate convergence in optimization tasks, OBL is leveraged here to generate synthetic opposite samples that replace the acutely training data and improve decision boundary formation. We explore three OBL variants; Global OBL, Class-Wise OBL, and Localized Class-Wise OBL; and integrate them with several widely used classifiers, including K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Logistic Regression (LR), and Decision Tree (DT). Extensive experiments conducted on 26 heterogeneous and high-dimensional datasets demonstrate that OBL-enhanced classifiers consistently outperform their standard counterparts in terms of accuracy and F1-score, frequently achieving near-perfect or perfect classification. Furthermore, OBL contributes to improved computational efficiency, particularly in SVM and LR. These findings underscore the potential of OBL as a lightweight yet powerful data transformation strategy for enhancing classification performance, especially in complex or sparse learning environments. 

**Abstract (ZH)**: 基于反对学习的数据转换框架在提升传统分类算法性能中的应用 

---
# Gradient-Optimized Fuzzy Classifier: A Benchmark Study Against State-of-the-Art Models 

**Title (ZH)**: 基于梯度优化的模糊分类器：与先进模型的基准研究 

**Authors**: Magnus Sieverding, Nathan Steffen, Kelly Cohen  

**Link**: [PDF](https://arxiv.org/pdf/2504.16263)  

**Abstract**: This paper presents a performance benchmarking study of a Gradient-Optimized Fuzzy Inference System (GF) classifier against several state-of-the-art machine learning models, including Random Forest, XGBoost, Logistic Regression, Support Vector Machines, and Neural Networks. The evaluation was conducted across five datasets from the UCI Machine Learning Repository, each chosen for their diversity in input types, class distributions, and classification complexity. Unlike traditional Fuzzy Inference Systems that rely on derivative-free optimization methods, the GF leverages gradient descent to significantly improving training efficiency and predictive performance. Results demonstrate that the GF model achieved competitive, and in several cases superior, classification accuracy while maintaining high precision and exceptionally low training times. In particular, the GF exhibited strong consistency across folds and datasets, underscoring its robustness in handling noisy data and variable feature sets. These findings support the potential of gradient optimized fuzzy systems as interpretable, efficient, and adaptable alternatives to more complex deep learning models in supervised learning tasks. 

**Abstract (ZH)**: 基于梯度优化的模糊推理系统(GF)分类器与多种先进机器学习模型的性能基准研究 

---
# Blockchain Meets Adaptive Honeypots: A Trust-Aware Approach to Next-Gen IoT Security 

**Title (ZH)**: 区块链与自适应蜜罐相结合：一种基于信任的物联网安全下一代方案 

**Authors**: Yazan Otoum, Arghavan Asad, Amiya Nayak  

**Link**: [PDF](https://arxiv.org/pdf/2504.16226)  

**Abstract**: Edge computing-based Next-Generation Wireless Networks (NGWN)-IoT offer enhanced bandwidth capacity for large-scale service provisioning but remain vulnerable to evolving cyber threats. Existing intrusion detection and prevention methods provide limited security as adversaries continually adapt their attack strategies. We propose a dynamic attack detection and prevention approach to address this challenge. First, blockchain-based authentication uses the Deoxys Authentication Algorithm (DAA) to verify IoT device legitimacy before data transmission. Next, a bi-stage intrusion detection system is introduced: the first stage uses signature-based detection via an Improved Random Forest (IRF) algorithm. In contrast, the second stage applies feature-based anomaly detection using a Diffusion Convolution Recurrent Neural Network (DCRNN). To ensure Quality of Service (QoS) and maintain Service Level Agreements (SLA), trust-aware service migration is performed using Heap-Based Optimization (HBO). Additionally, on-demand virtual High-Interaction honeypots deceive attackers and extract attack patterns, which are securely stored using the Bimodal Lattice Signature Scheme (BLISS) to enhance signature-based Intrusion Detection Systems (IDS). The proposed framework is implemented in the NS3 simulation environment and evaluated against existing methods across multiple performance metrics, including accuracy, attack detection rate, false negative rate, precision, recall, ROC curve, memory usage, CPU usage, and execution time. Experimental results demonstrate that the framework significantly outperforms existing approaches, reinforcing the security of NGWN-enabled IoT ecosystems 

**Abstract (ZH)**: 基于边缘计算的下一代无线网络（NGWN）-IoT提供大规模服务供应的增强带宽容量，但仍易受不断演变的网络安全威胁。现有的入侵检测与预防方法提供有限的安全性，因为攻击者不断调整其攻击策略。我们提出了一种动态攻击检测与预防方法以应对这一挑战。首先，基于区块链的认证使用Deoxys认证算法（DAA）在数据传输前验证物联网设备的合法性。其次，引入了一种两阶段入侵检测系统：第一阶段使用改进的随机森林（IRF）算法进行签名检测；第二阶段采用扩散卷积循环神经网络（DCRNN）进行基于特征的异常检测。为确保服务质量（QoS）并维持服务级别协议（SLA），使用堆基优化（HBO）进行信任感知的服务迁移。此外，基于需求的虚拟高互动蜜罐欺骗攻击者并提取攻击模式，这些模式通过二模格签名方案（BLISS）安全存储，以增强基于签名的入侵检测系统。所提出的框架在NS3仿真环境中实现，并通过多个性能指标与现有方法进行评估，包括准确性、攻击检测率、假阴性率、精确率、召回率、ROC曲线、内存使用率、CPU使用率和执行时间。实验结果表明，该框架显著优于现有方法，增强了NGWN驱动的物联网生态系统安全性。 

---
# Hexcute: A Tile-based Programming Language with Automatic Layout and Task-Mapping Synthesis 

**Title (ZH)**: Hexcute: 基于瓷砖的编程语言，自动布局与任务映射合成 

**Authors**: Xiao Zhang, Yaoyao Ding, Yang Hu, Gennady Pekhimenko  

**Link**: [PDF](https://arxiv.org/pdf/2504.16214)  

**Abstract**: Deep learning (DL) workloads mainly run on accelerators like GPUs. Recent DL quantization techniques demand a new matrix multiplication operator with mixed input data types, further complicating GPU optimization. Prior high-level compilers like Triton lack the expressiveness to implement key optimizations like fine-grained data pipelines and hardware-friendly memory layouts for these operators, while low-level programming models, such as Hidet, Graphene, and CUTLASS, require significant programming efforts. To balance expressiveness with engineering effort, we propose Hexcute, a tile-based programming language that exposes shared memory and register abstractions to enable fine-grained optimization for these operators. Additionally, Hexcute leverages task mapping to schedule the GPU program, and to reduce programming efforts, it automates layout and task mapping synthesis with a novel type-inference-based algorithm. Our evaluation shows that Hexcute generalizes to a wide range of DL operators, achieves 1.7-11.28$\times$ speedup over existing DL compilers for mixed-type operators, and brings up to 2.91$\times$ speedup in the end-to-end evaluation. 

**Abstract (ZH)**: 基于 Tiles 的 Hexcute 编程语言：一种细粒度优化的深度学习矩阵乘法操作符编程方法 

---
# TinyML for Speech Recognition 

**Title (ZH)**: TinyML for Speech Recognition 

**Authors**: Andrew Barovic, Armin Moin  

**Link**: [PDF](https://arxiv.org/pdf/2504.16213)  

**Abstract**: We train and deploy a quantized 1D convolutional neural network model to conduct speech recognition on a highly resource-constrained IoT edge device. This can be useful in various Internet of Things (IoT) applications, such as smart homes and ambient assisted living for the elderly and people with disabilities, just to name a few examples. In this paper, we first create a new dataset with over one hour of audio data that enables our research and will be useful to future studies in this field. Second, we utilize the technologies provided by Edge Impulse to enhance our model's performance and achieve a high Accuracy of up to 97% on our dataset. For the validation, we implement our prototype using the Arduino Nano 33 BLE Sense microcontroller board. This microcontroller board is specifically designed for IoT and AI applications, making it an ideal choice for our target use case scenarios. While most existing research focuses on a limited set of keywords, our model can process 23 different keywords, enabling complex commands. 

**Abstract (ZH)**: 我们训练并部署了一个量化的一维卷积神经网络模型，以在资源极度受限的物联网边缘设备上进行语音识别。这在智能家居、辅助生活等物联网应用中具有实用价值。在本文中，我们首先创建了一个包含超过一小时音频数据的新数据集，以支持我们的研究，并对未来的研究具有参考价值。其次，我们利用Edge Impulse提供的技术优化了模型性能，并在数据集上达到了高达97%的准确率。在验证过程中，我们使用Arduino Nano 33 BLE Sense微控制器板实现我们的原型。该微控制器板专门设计用于物联网和人工智能应用，使其成为我们的目标应用场景的理想选择。与现有研究主要关注有限的关键词集不同，我们的模型可以处理23个不同的关键词，从而实现复杂的命令处理。 

---
# Reflexive Prompt Engineering: A Framework for Responsible Prompt Engineering and Interaction Design 

**Title (ZH)**: 反思性提示工程：负责任的提示工程与交互设计框架 

**Authors**: Christian Djeffal  

**Link**: [PDF](https://arxiv.org/pdf/2504.16204)  

**Abstract**: Responsible prompt engineering has emerged as a critical framework for ensuring that generative artificial intelligence (AI) systems serve society's needs while minimizing potential harms. As generative AI applications become increasingly powerful and ubiquitous, the way we instruct and interact with them through prompts has profound implications for fairness, accountability, and transparency. This article examines how strategic prompt engineering can embed ethical and legal considerations and societal values directly into AI interactions, moving beyond mere technical optimization for functionality. This article proposes a comprehensive framework for responsible prompt engineering that encompasses five interconnected components: prompt design, system selection, system configuration, performance evaluation, and prompt management. Drawing from empirical evidence, the paper demonstrates how each component can be leveraged to promote improved societal outcomes while mitigating potential risks. The analysis reveals that effective prompt engineering requires a delicate balance between technical precision and ethical consciousness, combining the systematic rigor and focus on functionality with the nuanced understanding of social impact. Through examination of real-world and emerging practices, the article illustrates how responsible prompt engineering serves as a crucial bridge between AI development and deployment, enabling organizations to fine-tune AI outputs without modifying underlying model architectures. This approach aligns with broader "Responsibility by Design" principles, embedding ethical considerations directly into the implementation process rather than treating them as post-hoc additions. The article concludes by identifying key research directions and practical guidelines for advancing the field of responsible prompt engineering. 

**Abstract (ZH)**: 负责任的提示工程已 emerges as a critical framework for ensuring that generative artificial intelligence (AI) systems serve society's needs while minimizing potential harms. 

---
# Quality of explanation of xAI from the prespective of Italian end-users: Italian version of System Causability Scale (SCS) 

**Title (ZH)**: 从意大利终端用户视角解释xAI的质量：System Causability Scale (SCS)的意大利版本 

**Authors**: Carmine Attanasio, Alireza Mortezapour  

**Link**: [PDF](https://arxiv.org/pdf/2504.16193)  

**Abstract**: Background and aim: Considering the scope of the application of artificial intelligence beyond the field of computer science, one of the concerns of researchers is to provide quality explanations about the functioning of algorithms based on artificial intelligence and the data extracted from it. The purpose of the present study is to validate the Italian version of system causability scale (I-SCS) to measure the quality of explanations provided in a xAI.
Method: For this purpose, the English version, initially provided in 2020 in coordination with the main developer, was utilized. The forward-backward translation method was applied to ensure accuracy. Finally, these nine steps were completed by calculating the content validity index/ratio and conducting cognitive interviews with representative end users.
Results: The original version of the questionnaire consisted of 10 questions. However, based on the obtained indexes (CVR below 0.49), one question (Question 8) was entirely removed. After completing the aforementioned steps, the Italian version contained 9 questions. The representative sample of Italian end users fully comprehended the meaning and content of the questions in the Italian version.
Conclusion: The Italian version obtained in this study can be used in future research studies as well as in the field by xAI developers. This tool can be used to measure the quality of explanations provided for an xAI system in Italian culture. 

**Abstract (ZH)**: 背景与目的：考虑到人工智能在计算机科学领域之外的应用范围，研究人员的一个关切是如何提供高质量的解释，这些解释基于人工智能算法及其提取的数据。本研究的目的在于验证系统因果量表的意大利语版本（I-SCS），以衡量xAI提供的解释质量。方法：为此目的，使用了2020年与主要开发者协调提供的英语版本，并采用了正向与反向翻译的方法以确保准确性。最终通过计算内容效度指数/比率并进行认知访谈，完成了这九个步骤。结果：原问卷包含10个问题。然而，根据获得的指数（CVR低于0.49），完全删除了一个问题（第8题）。完成上述步骤后，意大利语版本包含9个问题。代表性意大利终端用户完全理解了意大利语版本中问题的意义和内容。结论：本研究获得的意大利语版本可以在未来的研究中以及xAI开发者领域使用。此工具可以用于衡量意大利文化背景下xAI系统提供的解释质量。 

---
# FinNLI: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking 

**Title (ZH)**: FinNLI: 新颖的数据集用于多文体金融自然语言推理基准测试 

**Authors**: Jabez Magomere, Elena Kochkina, Samuel Mensah, Simerjot Kaur, Charese H. Smiley  

**Link**: [PDF](https://arxiv.org/pdf/2504.16188)  

**Abstract**: We introduce FinNLI, a benchmark dataset for Financial Natural Language Inference (FinNLI) across diverse financial texts like SEC Filings, Annual Reports, and Earnings Call transcripts. Our dataset framework ensures diverse premise-hypothesis pairs while minimizing spurious correlations. FinNLI comprises 21,304 pairs, including a high-quality test set of 3,304 instances annotated by finance experts. Evaluations show that domain shift significantly degrades general-domain NLI performance. The highest Macro F1 scores for pre-trained (PLMs) and large language models (LLMs) baselines are 74.57% and 78.62%, respectively, highlighting the dataset's difficulty. Surprisingly, instruction-tuned financial LLMs perform poorly, suggesting limited generalizability. FinNLI exposes weaknesses in current LLMs for financial reasoning, indicating room for improvement. 

**Abstract (ZH)**: Financial Natural Language Inference Benchmark Dataset (FinNLI) Across Diverse Financial Texts 

---
# FPGA-Based Neural Network Accelerators for Space Applications: A Survey 

**Title (ZH)**: 基于FPGA的太空应用神经网络加速器：一个综述 

**Authors**: Pedro Antunes, Artur Podobas  

**Link**: [PDF](https://arxiv.org/pdf/2504.16173)  

**Abstract**: Space missions are becoming increasingly ambitious, necessitating high-performance onboard spacecraft computing systems. In response, field-programmable gate arrays (FPGAs) have garnered significant interest due to their flexibility, cost-effectiveness, and radiation tolerance potential. Concurrently, neural networks (NNs) are being recognized for their capability to execute space mission tasks such as autonomous operations, sensor data analysis, and data compression. This survey serves as a valuable resource for researchers aiming to implement FPGA-based NN accelerators in space applications. By analyzing existing literature, identifying trends and gaps, and proposing future research directions, this work highlights the potential of these accelerators to enhance onboard computing systems. 

**Abstract (ZH)**: 空间任务变得越来越具雄心，需要高性能的机载航天器计算系统。为响应这一需求，现场可编程门阵列（FPGAs）因其灵活性、成本效益以及辐射耐受潜力而引起广泛关注。同时，神经网络（NNs）正被认可为执行空间任务的关键技术，例如自主操作、传感器数据处理和数据压缩。本文综述旨在为研究人员实施基于FPGA的NN加速器提供有价值的资源。通过分析现有文献、识别趋势和空白，以及提出未来的研究方向，本文突显了这些加速器增强机载计算系统的潜力。 

---
# Physics-Informed Inference Time Scaling via Simulation-Calibrated Scientific Machine Learning 

**Title (ZH)**: 基于物理信息的推断时间缩放通过仿真校准的科学机器学习 

**Authors**: Zexi Fan, Yan Sun, Shihao Yang, Yiping Lu  

**Link**: [PDF](https://arxiv.org/pdf/2504.16172)  

**Abstract**: High-dimensional partial differential equations (PDEs) pose significant computational challenges across fields ranging from quantum chemistry to economics and finance. Although scientific machine learning (SciML) techniques offer approximate solutions, they often suffer from bias and neglect crucial physical insights. Inspired by inference-time scaling strategies in language models, we propose Simulation-Calibrated Scientific Machine Learning (SCaSML), a physics-informed framework that dynamically refines and debiases the SCiML predictions during inference by enforcing the physical laws. SCaSML leverages derived new physical laws that quantifies systematic errors and employs Monte Carlo solvers based on the Feynman-Kac and Elworthy-Bismut-Li formulas to dynamically correct the prediction. Both numerical and theoretical analysis confirms enhanced convergence rates via compute-optimal inference methods. Our numerical experiments demonstrate that SCaSML reduces errors by 20-50% compared to the base surrogate model, establishing it as the first algorithm to refine approximated solutions to high-dimensional PDE during inference. Code of SCaSML is available at this https URL. 

**Abstract (ZH)**: 高维偏微分方程的物理校准科学机器学习（SCaSML） 

---
# A detection-task-specific deep-learning method to improve the quality of sparse-view myocardial perfusion SPECT images 

**Title (ZH)**: 针对检测任务的深度学习方法以提高稀视角心肌灌注SPECT图像质量 

**Authors**: Zezhang Yang, Zitong Yu, Nuri Choi, Abhinav K. Jha  

**Link**: [PDF](https://arxiv.org/pdf/2504.16171)  

**Abstract**: Myocardial perfusion imaging (MPI) with single-photon emission computed tomography (SPECT) is a widely used and cost-effective diagnostic tool for coronary artery disease. However, the lengthy scanning time in this imaging procedure can cause patient discomfort, motion artifacts, and potentially inaccurate diagnoses due to misalignment between the SPECT scans and the CT-scans which are acquired for attenuation compensation. Reducing projection angles is a potential way to shorten scanning time, but this can adversely impact the quality of the reconstructed images. To address this issue, we propose a detection-task-specific deep-learning method for sparse-view MPI SPECT images. This method integrates an observer loss term that penalizes the loss of anthropomorphic channel features with the goal of improving performance in perfusion defect-detection task. We observed that, on the task of detecting myocardial perfusion defects, the proposed method yielded an area under the receiver operating characteristic (ROC) curve (AUC) significantly larger than the sparse-view protocol. Further, the proposed method was observed to be able to restore the structure of the left ventricle wall, demonstrating ability to overcome sparse-sampling artifacts. Our preliminary results motivate further evaluations of the method. 

**Abstract (ZH)**: 单光子发射计算机断层成像(SPECT) myocardial perfusion imaging (MPI)中特定检测任务的深度学习方法研究：稀疏视图下的心脏病灶检测与结构恢复 

---
# Leveraging Social Media Analytics for Sustainability Trend Detection in Saudi Arabias Evolving Market 

**Title (ZH)**: 利用社交媒体分析检测Saudi Arabia evolving市场可持续性趋势 

**Authors**: Kanwal Aalijah  

**Link**: [PDF](https://arxiv.org/pdf/2504.16153)  

**Abstract**: Saudi Arabias rapid economic growth and social evolution under Vision 2030 present a unique opportunity to track emerging trends in real time. Uncovering trends in real time can open up new avenues for business and investment opportunities. This paper explores how AI and social media analytics can uncover and monitor these trends across sectors like sustainability, construction, food beverages industry, tourism, technology, and entertainment. This paper focus on use of AI-driven methodology to identify sustainability trends across Saudi Arabia. We processed millions of social media posts, news, blogs in order to understand sustainability trends in the region. The paper presents an AI approach that can help economists, businesses, government to understand sustainability trends and make better decisions around them. This approach offers both sector-specific and cross-sector insights, giving decision-makers a reliable, up to date snapshot of Saudi Arabias market shifts. Beyond Saudi Arabia, this framework also shows potential for adapting to other regions. Overall, our findings highlight how by using AI-methodologies, give decision makers a reliable method to understand how initiatives are perceived and adopted by the public and understand growth of trends. 

**Abstract (ZH)**: 沙特阿拉伯在Vision 2030愿景下的快速经济成长与社会进化提供了实时追踪新兴趋势的独特机会。实时发现趋势可以开辟新的商业和投资机会。本文探讨了如何通过人工智能和社会媒体分析在可持续性、建筑、食品饮料行业、旅游业、技术和娱乐等领域发现和监控这些趋势。本文重点探讨了利用人工智能驱动的方法在沙特阿拉伯识别可持续性趋势。我们处理了数百万条社交媒体帖子、新闻和博客，以了解该地区的可持续性趋势。本文提出了一种人工智能方法，可以帮助经济学家、企业、政府了解可持续性趋势，并围绕这些趋势做出更好的决策。该方法提供了领域特定和跨领域的洞察，为决策者提供了可靠的、及时的市场转变快照。此外，该框架还展示了适应其他地区的潜力。总体而言，我们的研究结果强调，通过使用人工智能方法，决策者可以获得一种可靠的方法来理解公众对倡议的看法和接受度，以及趋势的增长。 

---
# Heterogeneous networks in drug-target interaction prediction 

**Title (ZH)**: 药物-靶标相互作用预测中的异质网络模型 

**Authors**: Mohammad Molaee, Nasrollah Moghadam Charkari  

**Link**: [PDF](https://arxiv.org/pdf/2504.16152)  

**Abstract**: Drug discovery requires a tremendous amount of time and cost. Computational drug-target interaction prediction, a significant part of this process, can reduce these requirements by narrowing the search space for wet lab experiments. In this survey, we provide comprehensive details of graph machine learning-based methods in predicting drug-target interaction, as they have shown promising results in this field. These details include the overall framework, main contribution, datasets, and their source codes. The selected papers were mainly published from 2020 to 2024. Prior to discussing papers, we briefly introduce the datasets commonly used with these methods and measurements to assess their performance. Finally, future challenges and some crucial areas that need to be explored are discussed. 

**Abstract (ZH)**: 药物发现需要大量的时间和成本。基于图机器学习的药物-靶点相互作用预测，在这一过程中占据显著位置，可以通过缩小湿实验室实验的搜索空间来减少这些需求。在本综述中，我们提供了基于图机器学习方法预测药物-靶点相互作用的全面细节，因为这些方法在该领域展现出了令人鼓舞的结果。这些细节包括整体框架、主要贡献、数据集及其源代码。所选择的论文主要发表于2020年至2024年。在讨论论文之前，我们简要介绍了这些方法常使用的数据集和评估性能的度量标准。最后，讨论了未来的研究挑战和需要探索的关键领域。 

---
# Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room 

**Title (ZH)**: 面向教育的负责任AI：人机融合共克教育领域的顽疾 

**Authors**: Danial Hooshyar, Gustav Šír, Yeongwook Yang, Eve Kikas, Raija Hämäläinen, Tommi Kärkkäinen, Dragan Gašević, Roger Azevedo  

**Link**: [PDF](https://arxiv.org/pdf/2504.16148)  

**Abstract**: Despite significant advancements in AI-driven educational systems and ongoing calls for responsible AI for education, several critical issues remain unresolved -- acting as the elephant in the room within AI in education, learning analytics, educational data mining, learning sciences, and educational psychology communities. This critical analysis identifies and examines nine persistent challenges that continue to undermine the fairness, transparency, and effectiveness of current AI methods and applications in education. These include: (1) the lack of clarity around what AI for education truly means -- often ignoring the distinct purposes, strengths, and limitations of different AI families -- and the trend of equating it with domain-agnostic, company-driven large language models; (2) the widespread neglect of essential learning processes such as motivation, emotion, and (meta)cognition in AI-driven learner modelling and their contextual nature; (3) limited integration of domain knowledge and lack of stakeholder involvement in AI design and development; (4) continued use of non-sequential machine learning models on temporal educational data; (5) misuse of non-sequential metrics to evaluate sequential models; (6) use of unreliable explainable AI methods to provide explanations for black-box models; (7) ignoring ethical guidelines in addressing data inconsistencies during model training; (8) use of mainstream AI methods for pattern discovery and learning analytics without systematic benchmarking; and (9) overemphasis on global prescriptions while overlooking localised, student-specific recommendations. Supported by theoretical and empirical research, we demonstrate how hybrid AI methods -- specifically neural-symbolic AI -- can address the elephant in the room and serve as the foundation for responsible, trustworthy AI systems in education. 

**Abstract (ZH)**: 尽管在AI驱动教育系统方面取得了显著进展，并不断呼吁负责任的AI应用于教育，但仍存在若干关键问题——这些问题在教育AI、学习分析、教育数据挖掘、学习科学和教育心理学社区中如同未解决的象てしま停滞不前。本文批判性地识别并分析了九个持续存在的挑战，这些挑战继续削弱当前AI方法和在教育中的有效性。这些挑战包括：（1）对教育AI的真正含义缺乏明确性——往往忽视不同AI家庭的独特目的、优势和局限性，并将其与领域无关、企业驱动的大语言模型相提并论；（2）广泛忽视动机、情绪和元认知等基本学习过程在其情境性；（3）领域知识的有限集成和利益相关者在AI设计和开发中的参与不足；（4）继续在时间序列教育数据上使用非顺序机器学习模型；（5）使用非顺序指标评估顺序模型；（6）使用不可靠的解释性AI方法为黑盒模型提供解释；（7）在模型训练过程中忽视伦理准则以解决数据不一致性；（8）未经系统基准测试就使用主流AI方法进行模式发现和学习分析；（9）过分强调全局处方而忽视本地化的、针对学生的具体建议。我们通过理论和实证研究证明，特定类型的混合AI方法——神经符号AI——可以解决上述问题，并为教育中负责任和可信赖的AI系统奠定基础。 

---
# Progressive Language-guided Visual Learning for Multi-Task Visual Grounding 

**Title (ZH)**: 渐进式语言引导的视觉学习在多任务视觉定位中的应用 

**Authors**: Jingchao Wang, Hong Wang, Wenlong Zhang, Kunhua Ji, Dingjiang Huang, Yefeng Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2504.16145)  

**Abstract**: Multi-task visual grounding (MTVG) includes two sub-tasks, i.e., Referring Expression Comprehension (REC) and Referring Expression Segmentation (RES). The existing representative approaches generally follow the research pipeline which mainly consists of three core procedures, including independent feature extraction for visual and linguistic modalities, respectively, cross-modal interaction module, and independent prediction heads for different sub-tasks. Albeit achieving remarkable performance, this research line has two limitations: 1) The linguistic content has not been fully injected into the entire visual backbone for boosting more effective visual feature extraction and it needs an extra cross-modal interaction module; 2) The relationship between REC and RES tasks is not effectively exploited to help the collaborative prediction for more accurate output. To deal with these problems, in this paper, we propose a Progressive Language-guided Visual Learning framework for multi-task visual grounding, called PLVL, which not only finely mine the inherent feature expression of the visual modality itself but also progressively inject the language information to help learn linguistic-related visual features. In this manner, our PLVL does not need additional cross-modal fusion module while fully introducing the language guidance. Furthermore, we analyze that the localization center for REC would help identify the to-be-segmented object region for RES to some extent. Inspired by this investigation, we design a multi-task head to accomplish collaborative predictions for these two sub-tasks. Extensive experiments conducted on several benchmark datasets comprehensively substantiate that our PLVL obviously outperforms the representative methods in both REC and RES tasks. this https URL 

**Abstract (ZH)**: 多任务视觉定位（MTVG）包括两项子任务，即引用表达理解（REC）和引用表达分割（RES）。现有代表性方法通常遵循主要由三个核心步骤组成的研究所采用的pipeline，包括分别对视觉和语言模态进行独立特征提取、跨模态交互模块，以及为不同子任务提供的独立预测头。尽管取得了显著性能，这条研究线仍存在两个局限性：1）语言内容尚未完全注入整个视觉骨干以增强更有效的视觉特征提取，需要额外的跨模态交互模块；2）REC和RES任务之间的关系没有得到有效利用，以帮助协作预测以获得更准确的输出。为解决这些问题，在本文中，我们提出了一种渐进的语言引导视觉学习框架，称为PLVL，该框架不仅细致挖掘视觉模态自身固有的特征表达，还逐步注入语言信息以帮助学习与语言相关的视觉特征。这样一来，我们不需要额外的跨模态融合模块，同时完全引入语言指导。此外，我们分析认为，REC的定位中心在一定程度上有助于识别RES待分割的对象区域。受这一研究的启发，我们设计了一个多任务头来完成这两项子任务的协作预测。在几个基准数据集上进行的广泛实验全面证实，我们的PLVL在REC和RES任务上明显优于代表方法。 

---
# Detecting Actionable Requests and Offers on Social Media During Crises Using LLMs 

**Title (ZH)**: 使用大语言模型在危机期间检测社交媒体上的可行动请求和报价 

**Authors**: Ahmed El Fekih Zguir, Ferda Ofli, Muhammad Imran  

**Link**: [PDF](https://arxiv.org/pdf/2504.16144)  

**Abstract**: Natural disasters often result in a surge of social media activity, including requests for assistance, offers of help, sentiments, and general updates. To enable humanitarian organizations to respond more efficiently, we propose a fine-grained hierarchical taxonomy to systematically organize crisis-related information about requests and offers into three critical dimensions: supplies, emergency personnel, and actions. Leveraging the capabilities of Large Language Models (LLMs), we introduce Query-Specific Few-shot Learning (QSF Learning) that retrieves class-specific labeled examples from an embedding database to enhance the model's performance in detecting and classifying posts. Beyond classification, we assess the actionability of messages to prioritize posts requiring immediate attention. Extensive experiments demonstrate that our approach outperforms baseline prompting strategies, effectively identifying and prioritizing actionable requests and offers. 

**Abstract (ZH)**: 自然灾害往往会导致社交媒体活动激增，包括求助请求、帮助提议、情绪表达和一般更新。为了使人道主义组织能够更有效地响应，我们提出一种精细层次的-taxonomy分类体系，系统地将危机相关的求助和帮助信息组织到三个关键维度中：物资、应急管理人力和行动。利用大型语言模型的能力，我们引入了查询特定的小样本学习（QSF学习），从嵌入数据库中检索特定类别的标注示例，以增强模型在检测和分类帖子方面的性能。除了分类之外，我们还评估了消息的实际操作性，以优先处理需要立即关注的帖子。 extensive实验表明，我们的方法在识别和优先处理可操作的求助和帮助方面优于基线提示策略。 

---
# A Non-Invasive Load Monitoring Method for Edge Computing Based on MobileNetV3 and Dynamic Time Regulation 

**Title (ZH)**: 基于MobileNetV3和动态时间规整的边缘计算非侵入式负荷监测方法 

**Authors**: Hangxu Liu, Yaojie Sun, Yu Wang  

**Link**: [PDF](https://arxiv.org/pdf/2504.16142)  

**Abstract**: In recent years, non-intrusive load monitoring (NILM) technology has attracted much attention in the related research field by virtue of its unique advantage of utilizing single meter data to achieve accurate decomposition of device-level energy consumption. Cutting-edge methods based on machine learning and deep learning have achieved remarkable results in load decomposition accuracy by fusing time-frequency domain features. However, these methods generally suffer from high computational costs and huge memory requirements, which become the main obstacles for their deployment on resource-constrained microcontroller units (MCUs). To address these challenges, this study proposes an innovative Dynamic Time Warping (DTW) algorithm in the time-frequency domain and systematically compares and analyzes the performance of six machine learning techniques in home electricity scenarios. Through complete experimental validation on edge MCUs, this scheme successfully achieves a recognition accuracy of 95%. Meanwhile, this study deeply optimizes the frequency domain feature extraction process, which effectively reduces the running time by 55.55% and the storage overhead by about 34.6%. The algorithm performance will be further optimized in future research work. Considering that the elimination of voltage transformer design can significantly reduce the cost, the subsequent research will focus on this direction, and is committed to providing more cost-effective solutions for the practical application of NILM, and providing a solid theoretical foundation and feasible technical paths for the design of efficient NILM systems in edge computing environments. 

**Abstract (ZH)**: 近年来，非侵入式负荷监测（NILM）技术因其利用单个电表数据实现设备级能源消耗精确分解的独特优势，在相关研究领域引起了广泛关注。基于机器学习和深度学习的先进方法通过时频域特征融合，在负荷分解准确性方面取得了显著成果。然而，这些方法通常面临高计算成本和巨大内存需求的问题，成为其在资源受限微控制器（MCUs）上部署的主要障碍。为应对这些挑战，本研究提出了一种创新的时频域动态时间 warped（DTW）算法，并系统比较和分析了六种机器学习技术在家庭电力场景中的性能。通过在边缘MCUs上进行全面实验验证，该方案成功实现了95%的识别准确率。同时，本研究深入优化了频域特征提取过程，有效减少了55.55%的运行时间并降低了约34.6%的存储开销。未来研究将进一步优化算法性能。鉴于消除电压互感器设计可显著降低成本，后续研究将侧重于此方向，致力于提供更经济有效的非侵入式负荷监测解决方案，并为边缘计算环境中高效非侵入式负荷监测系统的设计提供坚实的理论基础和可行的技术路径。 

---
# SparseJEPA: Sparse Representation Learning of Joint Embedding Predictive Architectures 

**Title (ZH)**: SparseJEPA: 联合嵌入预测架构的稀疏表示学习 

**Authors**: Max Hartman, Lav Varshney  

**Link**: [PDF](https://arxiv.org/pdf/2504.16140)  

**Abstract**: Joint Embedding Predictive Architectures (JEPA) have emerged as a powerful framework for learning general-purpose representations. However, these models often lack interpretability and suffer from inefficiencies due to dense embedding representations. We propose SparseJEPA, an extension that integrates sparse representation learning into the JEPA framework to enhance the quality of learned representations. SparseJEPA employs a penalty method that encourages latent space variables to be shared among data features with strong semantic relationships, while maintaining predictive performance. We demonstrate the effectiveness of SparseJEPA by training on the CIFAR-100 dataset and pre-training a lightweight Vision Transformer. The improved embeddings are utilized in linear-probe transfer learning for both image classification and low-level tasks, showcasing the architecture's versatility across different transfer tasks. Furthermore, we provide a theoretical proof that demonstrates that the grouping mechanism enhances representation quality. This was done by displaying that grouping reduces Multiinformation among latent-variables, including proofing the Data Processing Inequality for Multiinformation. Our results indicate that incorporating sparsity not only refines the latent space but also facilitates the learning of more meaningful and interpretable representations. In further work, hope to further extend this method by finding new ways to leverage the grouping mechanism through object-centric representation learning. 

**Abstract (ZH)**: SparseJEPA：将稀疏表示学习整合到联合嵌入预测架构中以提高表示质量 

---
# Enhancing Trust Through Standards: A Comparative Risk-Impact Framework for Aligning ISO AI Standards with Global Ethical and Regulatory Contexts 

**Title (ZH)**: 通过标准提升信任：一种将ISO AI标准与全球伦理和监管 contexts 对齐的比较风险影响框架 

**Authors**: Sridharan Sankaran  

**Link**: [PDF](https://arxiv.org/pdf/2504.16139)  

**Abstract**: As artificial intelligence (AI) reshapes industries and societies, ensuring its trustworthiness-through mitigating ethical risks like bias, opacity, and accountability deficits-remains a global challenge. International Organization for Standardization (ISO) AI standards, such as ISO/IEC 24027 and 24368, aim to foster responsible development by embedding fairness, transparency, and risk management into AI systems. However, their effectiveness varies across diverse regulatory landscapes, from the EU's risk-based AI Act to China's stability-focused measures and the U.S.'s fragmented state-led initiatives. This paper introduces a novel Comparative Risk-Impact Assessment Framework to evaluate how well ISO standards address ethical risks within these contexts, proposing enhancements to strengthen their global applicability. By mapping ISO standards to the EU AI Act and surveying regulatory frameworks in ten regions-including the UK, Canada, India, Japan, Singapore, South Korea, and Brazil-we establish a baseline for ethical alignment. The framework, applied to case studies in the EU, US-Colorado, and China, reveals gaps: voluntary ISO standards falter in enforcement (e.g., Colorado) and undervalue region-specific risks like privacy (China). We recommend mandatory risk audits, region-specific annexes, and a privacy-focused module to enhance ISO's adaptability. This approach not only synthesizes global trends but also offers a replicable tool for aligning standardization with ethical imperatives, fostering interoperability and trust in AI worldwide. Policymakers and standards bodies can leverage these insights to evolve AI governance, ensuring it meets diverse societal needs as the technology advances. 

**Abstract (ZH)**: 随着人工智能（AI）重塑产业和社會，確保其可信度——通過減輕偏見、不透明和 Accountability 缺陷等道德風險——依然是一個全球性的挑戰。國際标准化組織（ISO）的AI標準，如ISO/IEC 24027和24368，旨在通過將公平、透明和風險管理嵌入AI系統而促進負責開發。然而，這些標準在全球不同的監管環境中的有效性各不相同，從歐盟的風險基輔助AI法到中國的重点穩定措施和美國的碎片化州主导Initiatives。本文引入了一種新的比較風險影響評估框架，以評估ISO標準如何在這些環境中應對道德風險，並提出改進措施以增強其全球適用性。通過將ISO標準映射到歐盟AI法，并對包括英國、加拿大、印度、日本、新加坡、韓國和巴西在內的十個地區的規制框架進行調研，我們建立了符合道德標準的基線。該框架應用於歐盟、美國丹佛和中國的案例研究，揭示了缺口：自愿ISO標準在執行情況下 crumbling（如丹佛）並低估地方特定風險（如中國的隱私）。我們建議實行強制風險審核、地區特定附錄和隱私Focus模塊，以增強ISO的靈活性。該方法不僅綜合了全球趨勢，還提供了一種可複製工具，以使標準化與道德要求對接，促進人工智能的互操作性和可信度。政策制定者和標準機構可以借助這些洞察來進化人工智能治理，確保其滿足技術進步帶來的多樣化社會需求。 

---
# Trends in Frontier AI Model Count: A Forecast to 2028 

**Title (ZH)**: 前沿AI模型数量的发展趋势：至2028年的预测 

**Authors**: Iyngkarran Kumar, Sam Manning  

**Link**: [PDF](https://arxiv.org/pdf/2504.16138)  

**Abstract**: Governments are starting to impose requirements on AI models based on how much compute was used to train them. For example, the EU AI Act imposes requirements on providers of general-purpose AI with systemic risk, which includes systems trained using greater than $10^{25}$ floating point operations (FLOP). In the United States' AI Diffusion Framework, a training compute threshold of $10^{26}$ FLOP is used to identify "controlled models" which face a number of requirements. We explore how many models such training compute thresholds will capture over time. We estimate that by the end of 2028, there will be between 103-306 foundation models exceeding the $10^{25}$ FLOP threshold put forward in the EU AI Act (90% CI), and 45-148 models exceeding the $10^{26}$ FLOP threshold that defines controlled models in the AI Diffusion Framework (90% CI). We also find that the number of models exceeding these absolute compute thresholds each year will increase superlinearly -- that is, each successive year will see more new models captured within the threshold than the year before. Thresholds that are defined with respect to the largest training run to date (for example, such that all models within one order of magnitude of the largest training run to date are captured by the threshold) see a more stable trend, with a median forecast of 14-16 models being captured by this definition annually from 2025-2028. 

**Abstract (ZH)**: 政府开始基于训练时所使用的计算量对AI模型提出要求。例如，欧盟AI法案对具有系统性风险的一般目的AI提出了要求，其中包括使用超过$10^{25}$浮点运算（FLOP）训练的系统。在美国AI扩散框架中，使用$10^{26}$ FLOP的训练计算阈值来识别“受控模型”，这些模型需要满足一系列要求。我们探究了随着时间的推移，将达到这些训练计算阈值的模型数量。我们估计到2028年底，将达到欧盟AI法案提出$10^{25}$ FLOP阈值的103-306个基础模型（90%置信区间），将达到美国AI扩散框架中“受控模型”定义的$10^{26}$ FLOP阈值的45-148个模型（90%置信区间）。我们也发现，每年超过这些绝对计算阈值的模型数量将呈现超线性增长——即每年捕获到阈值内的新模型数量将超过前一年。以迄今为止最大的训练运行为基础定义的阈值（例如，所有在迄今为止最大训练运行数量的一个数量级范围内的模型都被捕获到）呈现出更稳定的趋势，从2025年到2028年，每年平均预测将有14-16个模型被这种定义捕获。 

---
# A Conceptual Framework for AI-based Decision Systems in Critical Infrastructures 

**Title (ZH)**: 基于人工智能的决策系统在关键基础设施中的概念框架 

**Authors**: Milad Leyli-abadi, Ricardo J. Bessa, Jan Viebahn, Daniel Boos, Clark Borst, Alberto Castagna, Ricardo Chavarriaga, Mohamed Hassouna, Bruno Lemetayer, Giulia Leto, Antoine Marot, Maroua Meddeb, Manuel Meyer, Viola Schiaffonati, Manuel Schneider, Toni Waefler  

**Link**: [PDF](https://arxiv.org/pdf/2504.16133)  

**Abstract**: The interaction between humans and AI in safety-critical systems presents a unique set of challenges that remain partially addressed by existing frameworks. These challenges stem from the complex interplay of requirements for transparency, trust, and explainability, coupled with the necessity for robust and safe decision-making. A framework that holistically integrates human and AI capabilities while addressing these concerns is notably required, bridging the critical gaps in designing, deploying, and maintaining safe and effective systems. This paper proposes a holistic conceptual framework for critical infrastructures by adopting an interdisciplinary approach. It integrates traditionally distinct fields such as mathematics, decision theory, computer science, philosophy, psychology, and cognitive engineering and draws on specialized engineering domains, particularly energy, mobility, and aeronautics. The flexibility in its adoption is also demonstrated through its instantiation on an already existing framework. 

**Abstract (ZH)**: 人类与AI在安全关键系统中的交互提出了一个独特的挑战集，现有框架在部分方面仍然未能有效应对。这些挑战源于透明性、信任和可解释性要求与稳健安全决策需求之间的复杂交互。一个能全面整合人类和AI能力并解决这些问题的框架尤为必要，以弥合设计、部署和维护安全有效系统的关键缺口。本文通过多学科方法提出了一种全方位的概念框架，将传统上独立的数学、决策理论、计算机科学、哲学、心理学和认知工程等领域进行综合，并借鉴了能源、移动性、航空等专门工程领域。同时，通过将其应用于一个现有的框架，展示了其采用的灵活性。 

---
# Efficacy of a Computer Tutor that Models Expert Human Tutors 

**Title (ZH)**: 基于专家人类导师模型的计算机导师的效果研究 

**Authors**: Andrew M. Olney, Sidney K. D'Mello, Natalie Person, Whitney Cade, Patrick Hays, Claire W. Dempsey, Blair Lehman, Betsy Williams, Art Graesser  

**Link**: [PDF](https://arxiv.org/pdf/2504.16132)  

**Abstract**: Tutoring is highly effective for promoting learning. However, the contribution of expertise to tutoring effectiveness is unclear and continues to be debated. We conducted a 9-week learning efficacy study of an intelligent tutoring system (ITS) for biology modeled on expert human tutors with two control conditions: human tutors who were experts in the domain but not in tutoring and a no-tutoring condition. All conditions were supplemental to classroom instruction, and students took learning tests immediately before and after tutoring sessions as well as delayed tests 1-2 weeks later. Analysis using logistic mixed-effects modeling indicates significant positive effects on the immediate post-test for the ITS (d =.71) and human tutors (d =.66) which are in the 99th percentile of meta-analytic effects, as well as significant positive effects on the delayed post-test for the ITS (d =.36) and human tutors (d =.39). We discuss implications for the role of expertise in tutoring and the design of future studies. 

**Abstract (ZH)**: 智能辅导系统在促进学习中的有效性研究：基于专家的人工智能辅导和控制条件下的专家人工辅导对比分析及其对未来研究设计的影响 

---
# Introduction to Quantum Machine Learning and Quantum Architecture Search 

**Title (ZH)**: 量子机器学习与量子架构搜索导论 

**Authors**: Samuel Yen-Chi Chen, Zhiding Liang  

**Link**: [PDF](https://arxiv.org/pdf/2504.16131)  

**Abstract**: Recent advancements in quantum computing (QC) and machine learning (ML) have fueled significant research efforts aimed at integrating these two transformative technologies. Quantum machine learning (QML), an emerging interdisciplinary field, leverages quantum principles to enhance the performance of ML algorithms. Concurrently, the exploration of systematic and automated approaches for designing high-performance quantum circuit architectures for QML tasks has gained prominence, as these methods empower researchers outside the quantum computing domain to effectively utilize quantum-enhanced tools. This tutorial will provide an in-depth overview of recent breakthroughs in both areas, highlighting their potential to expand the application landscape of QML across diverse fields. 

**Abstract (ZH)**: 最近在量子计算和机器学习领域的进展推动了将这两种变革性技术整合的研究努力。量子机器学习作为一种新兴的跨学科领域，利用量子原理来提高机器学习算法的性能。同时，系统化和自动化设计高性能量子电路架构以用于量子机器学习任务的方法也逐渐受到关注，这些方法使非量子计算领域的研究者能够有效利用量子增强的工具。本教程将提供这两个领域的最新突破的详细概述，并强调它们在扩展量子机器学习在多个领域的应用前景。 

---
# A Self-supervised Learning Method for Raman Spectroscopy based on Masked Autoencoders 

**Title (ZH)**: 基于遮蔽自动编码器的自监督学习方法用于拉曼光谱分析 

**Authors**: Pengju Ren, Ri-gui Zhou, Yaochong Li  

**Link**: [PDF](https://arxiv.org/pdf/2504.16130)  

**Abstract**: Raman spectroscopy serves as a powerful and reliable tool for analyzing the chemical information of substances. The integration of Raman spectroscopy with deep learning methods enables rapid qualitative and quantitative analysis of materials. Most existing approaches adopt supervised learning methods. Although supervised learning has achieved satisfactory accuracy in spectral analysis, it is still constrained by costly and limited well-annotated spectral datasets for training. When spectral annotation is challenging or the amount of annotated data is insufficient, the performance of supervised learning in spectral material identification declines. In order to address the challenge of feature extraction from unannotated spectra, we propose a self-supervised learning paradigm for Raman Spectroscopy based on a Masked AutoEncoder, termed SMAE. SMAE does not require any spectral annotations during pre-training. By randomly masking and then reconstructing the spectral information, the model learns essential spectral features. The reconstructed spectra exhibit certain denoising properties, improving the signal-to-noise ratio (SNR) by more than twofold. Utilizing the network weights obtained from masked pre-training, SMAE achieves clustering accuracy of over 80% for 30 classes of isolated bacteria in a pathogenic bacterial dataset, demonstrating significant improvements compared to classical unsupervised methods and other state-of-the-art deep clustering methods. After fine-tuning the network with a limited amount of annotated data, SMAE achieves an identification accuracy of 83.90% on the test set, presenting competitive performance against the supervised ResNet (83.40%). 

**Abstract (ZH)**: 基于_MASKED AUTOENCODER_的自我监督学习在拉曼光谱中的应用 

---
# MARFT: Multi-Agent Reinforcement Fine-Tuning 

**Title (ZH)**: MARFT：多代理强化学习微调 

**Authors**: Junwei Liao, Muning Wen, Jun Wang, Weinan Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2504.16129)  

**Abstract**: LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in addressing complex, agentic tasks requiring multifaceted reasoning and collaboration, from generating high-quality presentation slides to conducting sophisticated scientific research. Meanwhile, RL has been widely recognized for its effectiveness in enhancing agent intelligence, but limited research has investigated the fine-tuning of LaMAS using foundational RL techniques. Moreover, the direct application of MARL methodologies to LaMAS introduces significant challenges, stemming from the unique characteristics and mechanisms inherent to LaMAS. To address these challenges, this article presents a comprehensive study of LLM-based MARL and proposes a novel paradigm termed Multi-Agent Reinforcement Fine-Tuning (MARFT). We introduce a universal algorithmic framework tailored for LaMAS, outlining the conceptual foundations, key distinctions, and practical implementation strategies. We begin by reviewing the evolution from RL to Reinforcement Fine-Tuning, setting the stage for a parallel analysis in the multi-agent domain. In the context of LaMAS, we elucidate critical differences between MARL and MARFT. These differences motivate a transition toward a novel, LaMAS-oriented formulation of RFT. Central to this work is the presentation of a robust and scalable MARFT framework. We detail the core algorithm and provide a complete, open-source implementation to facilitate adoption and further research. The latter sections of the paper explore real-world application perspectives and opening challenges in MARFT. By bridging theoretical underpinnings with practical methodologies, this work aims to serve as a roadmap for researchers seeking to advance MARFT toward resilient and adaptive solutions in agentic systems. Our implementation of the proposed framework is publicly available at: this https URL. 

**Abstract (ZH)**: 基于LLM的多Agent系统在处理需要多方面推理和协作的复杂任务方面展现了显著的能力，从生成高质量的演示文稿到进行复杂的科学研究。同时，强化学习因其在提升智能体能力方面的有效性而得到广泛应用，但对于使用基础强化学习技术微调LaMAS的有限研究引起了关注。此外，将MARL方法直接应用于LaMAS引入了重大挑战，源于LaMAS特有的特性和机制。为应对这些挑战，本文对基于LLM的MARL进行了全面研究，并提出了一种新型范式——多Agent强化微调（MARFT）。我们介绍了适用于LaMAS的通用算法框架，概述了概念基础、关键区别和实际实施策略。我们首先回顾了从RL到强化微调的演变，为多Agent领域提供了并行分析的背景。在LaMAS的背景下，我们详细阐述了MARL和MARFT之间的关键区别，并据此推动了一种新形式的RFT。本文的核心在于展示了一个稳健且可扩展的MARFT框架。我们详细描述了核心算法，并提供了完整的开源实现，以促进应用和进一步研究。文章后部分探讨了MARFT在实际应用中的前景和挑战。通过结合理论基础和实用方法论，本文旨在为研究人员提供一条路线图，以推进MARFT向具有韧性与适应性的智能系统解决方案的发展。我们提出的框架的实现已在以下链接公开：this https URL。 

---
# Hybrid Knowledge Transfer through Attention and Logit Distillation for On-Device Vision Systems in Agricultural IoT 

**Title (ZH)**: 基于注意力和逻辑斯蒂_distillation的农业物联网设备上视觉系统混合知识迁移 

**Authors**: Stanley Mugisha, Rashid Kisitu, Florence Tushabe  

**Link**: [PDF](https://arxiv.org/pdf/2504.16128)  

**Abstract**: Integrating deep learning applications into agricultural IoT systems faces a serious challenge of balancing the high accuracy of Vision Transformers (ViTs) with the efficiency demands of resource-constrained edge devices. Large transformer models like the Swin Transformers excel in plant disease classification by capturing global-local dependencies. However, their computational complexity (34.1 GFLOPs) limits applications and renders them impractical for real-time on-device inference. Lightweight models such as MobileNetV3 and TinyML would be suitable for on-device inference but lack the required spatial reasoning for fine-grained disease detection. To bridge this gap, we propose a hybrid knowledge distillation framework that synergistically transfers logit and attention knowledge from a Swin Transformer teacher to a MobileNetV3 student model. Our method includes the introduction of adaptive attention alignment to resolve cross-architecture mismatch (resolution, channels) and a dual-loss function optimizing both class probabilities and spatial focus. On the lantVillage-Tomato dataset (18,160 images), the distilled MobileNetV3 attains 92.4% accuracy relative to 95.9% for Swin-L but at an 95% reduction on PC and < 82% in inference latency on IoT devices. (23ms on PC CPU and 86ms/image on smartphone CPUs). Key innovations include IoT-centric validation metrics (13 MB memory, 0.22 GFLOPs) and dynamic resolution-matching attention maps. Comparative experiments show significant improvements over standalone CNNs and prior distillation methods, with a 3.5% accuracy gain over MobileNetV3 baselines. Significantly, this work advances real-time, energy-efficient crop monitoring in precision agriculture and demonstrates how we can attain ViT-level diagnostic precision on edge devices. Code and models will be made available for replication after acceptance. 

**Abstract (ZH)**: 将深度学习应用集成到农业物联网系统中面临严重挑战，即在保持视觉变换器（ViTs）的高准确性与边缘受限设备的效率需求之间取得平衡。Swin变换器等大型变换器模型在植物疾病分类中表现优异，通过捕捉全局-局部依赖性。然而，其计算复杂度（34.1 GFLOPs）限制了其应用，并使其无法满足实时设备推理的需求。MobileNetV3和TinyML等轻量级模型适合设备推理，但缺乏细粒度疾病检测所需的空间推理能力。为此，我们提出了一种混合知识蒸馏框架，以协同方式将Swin Transformer教师的logit和注意力知识传递给MobileNetV3学生模型。该方法包括自适应注意力对齐以解决跨架构不匹配问题（分辨率、通道），以及双损失函数优化类概率和空间集中度。在lantVillage-Tomato数据集（18,160张图像）上，蒸馏后的MobileNetV3的准确率为92.4%，比Swin-L高95%，但在PC上的计算复杂度降低至<95%， inference延迟降低至<82%（PC CPU为23ms，智能手机CPU为86ms/张图像）。关键创新包括物联网中心化的验证指标（13 MB内存，0.22 GFLOPs）和动态分辨率匹配注意力图。与单独的卷积神经网络和之前的蒸馏方法相比，实验结果显示显著改进，MobileNetV3基线准确率提升3.5%。这项工作推进了精准农业中实时、能效高的作物监测，并展示了如何在边缘设备上实现ViT级别的诊断精度。接受后，代码和模型将供 replication 使用。 

---
# SOTOPIA-S4: a user-friendly system for flexible, customizable, and large-scale social simulation 

**Title (ZH)**: SOTOPIA-S4：一种用户友好的灵活、可定制和大规模社会仿真系统 

**Authors**: Xuhui Zhou, Zhe Su, Sophie Feng, Jiaxu Zhou, Jen-tse Huang, Hsien-Te Kao, Spencer Lynch, Svitlana Volkova, Tongshuang Sherry Wu, Anita Woolley, Hao Zhu, Maarten Sap  

**Link**: [PDF](https://arxiv.org/pdf/2504.16122)  

**Abstract**: Social simulation through large language model (LLM) agents is a promising approach to explore and validate hypotheses related to social science questions and LLM agents behavior. We present SOTOPIA-S4, a fast, flexible, and scalable social simulation system that addresses the technical barriers of current frameworks while enabling practitioners to generate multi-turn and multi-party LLM-based interactions with customizable evaluation metrics for hypothesis testing. SOTOPIA-S4 comes as a pip package that contains a simulation engine, an API server with flexible RESTful APIs for simulation management, and a web interface that enables both technical and non-technical users to design, run, and analyze simulations without programming. We demonstrate the usefulness of SOTOPIA-S4 with two use cases involving dyadic hiring negotiation and multi-party planning scenarios. 

**Abstract (ZH)**: 通过大型语言模型（LLM）代理的社会仿真是一种探索和验证与社会科学问题和LLM代理行为相关假设的有前途的方法。我们提出了SOTOPIA-S4，一个快速、灵活、可扩展的社会仿真系统，该系统解决了当前框架的技术障碍，同时使 practitioners 能够生成多轮和多参与者的基于LLM的交互，并使用可定制的评估指标进行假设检验。SOTOPIA-S4 作为一个 pip 包提供，包含仿真引擎、具有灵活 RESTful API 的 API 服务器以及使技术用户和非技术用户无需编程即可设计、运行和分析仿真功能的 Web 界面。我们通过涉及二元招聘谈判和多参与规划场景的两个用例展示了 SOTOPIA-S4 的实用性。 

---
# A Data-Centric Approach for Safe and Secure Large Language Models against Threatening and Toxic Content 

**Title (ZH)**: 一种针对威胁性和有毒内容的安全可靠的大规模语言模型以数据为中心的方法 

**Authors**: Chaima Njeh, Haïfa Nakouri, Fehmi Jaafar  

**Link**: [PDF](https://arxiv.org/pdf/2504.16120)  

**Abstract**: Large Language Models (LLM) have made remarkable progress, but concerns about potential biases and harmful content persist. To address these apprehensions, we introduce a practical solution for ensuring LLM's safe and ethical use. Our novel approach focuses on a post-generation correction mechanism, the BART-Corrective Model, which adjusts generated content to ensure safety and security. Unlike relying solely on model fine-tuning or prompt engineering, our method provides a robust data-centric alternative for mitigating harmful content. We demonstrate the effectiveness of our approach through experiments on multiple toxic datasets, which show a significant reduction in mean toxicity and jail-breaking scores after integration. Specifically, our results show a reduction of 15% and 21% in mean toxicity and jail-breaking scores with GPT-4, a substantial reduction of 28% and 5% with PaLM2, a reduction of approximately 26% and 23% with Mistral-7B, and a reduction of 11.1% and 19% with Gemma-2b-it. These results demonstrate the potential of our approach to improve the safety and security of LLM, making them more suitable for real-world applications. 

**Abstract (ZH)**: 大型语言模型（LLM）取得了显著进步，但对其潜在偏见和有害内容的担忧依然存在。为应对这些担忧，我们提出了一种实用解决方案，确保LLM的安全和负责任使用。我们的新方法侧重于一种后生成修正机制——BART-Corrective模型，该模型调整生成的内容以确保安全和安全性。不同于仅依赖模型微调或提示工程，我们的方法提供了 robust 数据为中心的替代方案，以减少有害内容。通过在多个有毒数据集上的实验，我们证明了该方法的有效性，结果显示集成后显著降低了平均毒性和越狱评分。具体而言，我们的结果表明，与GPT-4相比，平均毒性和越狱评分分别减少了15%和21%；与PaLM2相比，这两项分别减少了28%和5%；与Mistral-7B相比，平均毒性和越狱评分分别减少了约26%和23%；与Gemma-2b-it相比，这两项分别减少了11.1%和19%。这些结果展示了我们方法提高LLM安全性和安全性的潜力，使它们更适合实际应用。 

---
# Towards Explainable and Lightweight AI for Real-Time Cyber Threat Hunting in Edge Networks 

**Title (ZH)**: 面向边缘网络实时网络威胁狩猎的可解释和轻量级AI方法 

**Authors**: Milad Rahmati  

**Link**: [PDF](https://arxiv.org/pdf/2504.16118)  

**Abstract**: As cyber threats continue to evolve, securing edge networks has become increasingly challenging due to their distributed nature and resource limitations. Many AI-driven threat detection systems rely on complex deep learning models, which, despite their high accuracy, suffer from two major drawbacks: lack of interpretability and high computational cost. Black-box AI models make it difficult for security analysts to understand the reasoning behind their predictions, limiting their practical deployment. Moreover, conventional deep learning techniques demand significant computational resources, rendering them unsuitable for edge devices with limited processing power. To address these issues, this study introduces an Explainable and Lightweight AI (ELAI) framework designed for real-time cyber threat detection in edge networks. Our approach integrates interpretable machine learning algorithms with optimized lightweight deep learning techniques, ensuring both transparency and computational efficiency. The proposed system leverages decision trees, attention-based deep learning, and federated learning to enhance detection accuracy while maintaining explainability. We evaluate ELAI using benchmark cybersecurity datasets, such as CICIDS and UNSW-NB15, assessing its performance across diverse cyberattack scenarios. Experimental results demonstrate that the proposed framework achieves high detection rates with minimal false positives, all while significantly reducing computational demands compared to traditional deep learning methods. The key contributions of this work include: (1) a novel interpretable AI-based cybersecurity model tailored for edge computing environments, (2) an optimized lightweight deep learning approach for real-time cyber threat detection, and (3) a comprehensive analysis of explainability techniques in AI-driven cybersecurity applications. 

**Abstract (ZH)**: 随着网络威胁不断演变，由于其分布式特性和资源限制，确保边缘网络的安全变得越来越具挑战性。许多基于AI的威胁检测系统依赖于复杂的深度学习模型，尽管这些模型具有高度的准确性，但也存在两大主要缺点：缺乏可解释性和高计算成本。黑盒AI模型使得安全分析师难以理解其预测背后的推理，从而限制了其实用部署。此外，传统的深度学习技术需要大量的计算资源，这对于处理能力有限的边缘设备来说是不合适的。为了解决这些问题，本研究提出了一种适用于边缘网络实时网络威胁检测的可解释轻量级AI（ELAI）框架。我们的方法结合了可解释的机器学习算法和优化的轻量级深度学习技术，确保了透明性和计算效率。所提出系统利用决策树、基于注意力的深度学习和联邦学习来提高检测准确性的同时保持可解释性。我们使用CICIDS和UNSW-NB15等基准网络安全数据集评估了ELAI，评估了其在不同网络攻击场景下的性能。实验结果表明，与传统的深度学习方法相比，所提出框架能够以较低的计算需求实现高检测率，同时将误报率降至最低。本项研究的主要贡献包括：（1）一种针对边缘计算环境的新型可解释AI网络安全模型；（2）用于实时网络威胁检测的优化轻量级深度学习方法；（3）对AI驱动网络安全应用中可解释性技术的全面分析。 

---
# Context-Awareness and Interpretability of Rare Occurrences for Discovery and Formalization of Critical Failure Modes 

**Title (ZH)**: 基于上下文感知和少见发生事件解释性的关键失败模式发现与形式化 

**Authors**: Sridevi Polavaram, Xin Zhou, Meenu Ravi, Mohammad Zarei, Anmol Srivastava  

**Link**: [PDF](https://arxiv.org/pdf/2504.16117)  

**Abstract**: Vision systems are increasingly deployed in critical domains such as surveillance, law enforcement, and transportation. However, their vulnerabilities to rare or unforeseen scenarios pose significant safety risks. To address these challenges, we introduce Context-Awareness and Interpretability of Rare Occurrences (CAIRO), an ontology-based human-assistive discovery framework for failure cases (or CP - Critical Phenomena) detection and formalization. CAIRO by design incentivizes human-in-the-loop for testing and evaluation of criticality that arises from misdetections, adversarial attacks, and hallucinations in AI black-box models. Our robust analysis of object detection model(s) failures in automated driving systems (ADS) showcases scalable and interpretable ways of formalizing the observed gaps between camera perception and real-world contexts, resulting in test cases stored as explicit knowledge graphs (in OWL/XML format) amenable for sharing, downstream analysis, logical reasoning, and accountability. 

**Abstract (ZH)**: 基于上下文意识和稀有事件可解释性的失败案例（或CP-关键现象）检测与 formalization 框架（CAIRO） 

---
# DMind Benchmark: The First Comprehensive Benchmark for LLM Evaluation in the Web3 Domain 

**Title (ZH)**: DMind基准：Web3领域首个全面的LLM评估基准 

**Authors**: Miracle Master, Rainy Sun, Anya Reese, Joey Ouyang, Alex Chen, Winter Dong, Frank Li, James Yi, Garry Zhao, Tony Ling, Hobert Wong, Lowes Yang  

**Link**: [PDF](https://arxiv.org/pdf/2504.16116)  

**Abstract**: Recent advances in Large Language Models (LLMs) have led to significant progress on a wide range of natural language processing tasks. However, their effectiveness in specialized and rapidly evolving domains such as Web3 remains underexplored. In this paper, we introduce DMind Benchmark, a novel framework that systematically tests LLMs across nine key categories encompassing blockchain fundamentals, infrastructure, smart contract analysis, decentralized finance (DeFi), decentralized autonomous organizations (DAOs), non-fungible tokens (NFTs), token economics, meme concepts, and security vulnerabilities.
DMind Benchmark goes beyond conventional multiple-choice questions by incorporating domain-specific subjective tasks (e.g., smart contract code auditing and repair, numeric reasoning on on-chain data, and fill-in assessments), thereby capturing real-world complexities and stress-testing model adaptability. We evaluate fifteen popular LLMs (from ChatGPT, DeepSeek, Claude, and Gemini series) on DMind Benchmark, uncovering performance gaps in Web3-specific reasoning and application, particularly in emerging areas like token economics and meme concepts. Even the strongest models face significant challenges in identifying subtle security vulnerabilities and analyzing complex DeFi mechanisms. To foster progress in this area, we publicly release our benchmark dataset, evaluation pipeline, and annotated results at this http URL, offering a valuable resource for advancing specialized domain adaptation and the development of more robust Web3-enabled LLMs. 

**Abstract (ZH)**: 最近大型语言模型（LLMs）的进展在多种自然语言处理任务中取得了显著进步。然而，在如Web3这样的专业且快速演化的领域，其有效性仍待深入探索。本文引入了DMind基准框架，这是一个系统测试LLMs的新型框架，涵盖了区块链基础、基础设施、智能合约分析、去中心化金融（DeFi）、去中心化自治组织（DAOs）、不可替代代币（NFTs）、代币经济、 meme概念和安全漏洞等九大关键类别。 

---
# AI-Based Vulnerability Analysis of NFT Smart Contracts 

**Title (ZH)**: 基于AI的NFT智能合约漏洞分析 

**Authors**: Xin Wang, Xiaoqi Li  

**Link**: [PDF](https://arxiv.org/pdf/2504.16113)  

**Abstract**: In the research experiment of this article, our research work is divided into several stages. Firstly, we collected a large number of smart contract codes and classified them, identifying several common defects, including Risky Mutably Porxy, ERC-721 Recentrancy, Unlimited Mining, Missing Requirements, and Public Burns. Secondly, we used Python to process the smart contracts. On the one hand, we modified the file names, and on the other hand, we batched the process of the content for analysis and application. Next, we built a model of the decision tree. Firstly, we carried out the feature extraction. We selected the algorithm and divided the data. After comparing and processing, we chose the CART classification tree to process. By gene coefficient, we analyzed and sorted the data, and got the initial model of the decision tree. Then, we introduced the random forest model on the basis of the decision tree. From abstracting the same amount of samples to selecting features this http URL adjusting and optimizing parameters to completing the construction of the forest model. Finally, we compared and analyzed the decision tree, random forest, and self-built model in the paper and drew general conclusions. 

**Abstract (ZH)**: 在本文的研究实验中，我们的研究工作分为几个阶段。首先，我们收集了大量的智能合约代码并进行分类，识别出几种常见的缺陷，包括Risky Mutably Proxy、ERC-721 Reentrancy、Unlimited Mining、Missing Requirements和Public Burns。其次，我们使用Python处理智能合约。一方面，我们修改了文件名；另一方面，我们批量处理内容进行分析和应用。接着，我们构建了决策树模型。首先，我们进行了特征提取，选择了算法并划分了数据。在比较和处理后，我们选择了CART分类树进行处理。通过基因系数分析和排序数据，获得了决策树的初步模型。然后，我们在决策树的基础上引入了随机森林模型。从提取相同数量的样本到选择特征，再到调整和优化参数，最终完成森林模型的构建。最后，我们在文章中比较和分析了决策树、随机森林和自建模型，并得出了总体结论。 

---
# HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing 

**Title (ZH)**: 高带宽处理单元：通过GPU协处理实现可扩展、成本效益高的LLM推理 

**Authors**: Myunghyun Rhee, Joonseop Sim, Taeyoung Ahn, Seungyong Lee, Daegun Yoon, Euiseok Kim, Kyoung Park, Youngpyo Joo, Hosik Kim  

**Link**: [PDF](https://arxiv.org/pdf/2504.16112)  

**Abstract**: The attention layer, a core component of Transformer-based LLMs, brings out inefficiencies in current GPU systems due to its low operational intensity and the substantial memory requirements of KV caches. We propose a High-bandwidth Processing Unit (HPU), a memoryintensive co-processor that enhances GPU resource utilization during large-batched LLM inference. By offloading memory-bound operations, the HPU allows the GPU to focus on compute-intensive tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales out to accommodate surging memory demands driven by large batch sizes and extended sequence lengths. In this paper, we show the HPU prototype implemented with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy efficiency improvements over a GPUonly system, providing scalability without increasing the number of GPUs. 

**Abstract (ZH)**: 基于Transformer的大语言模型注意力层在当前GPU系统中存在效率低下问题，主要是由于其低运算强度和大量KV缓存的内存需求。我们提出了一种高带宽处理单元（HPU），这是一种内存密集型协处理器，能够在大规模批次的大语言模型推理过程中提高GPU资源利用率。通过卸载内存受限操作，HPU使GPU能够专注于计算密集型任务，从而提高整体效率。此外，作为附加卡，HPU可扩展以应对由大批次大小和长序列长度驱动的内存需求激增。本文展示了使用基于PCIe的FPGA卡实现的HPU原型，安装在GPU系统上。我们的新型GPU-HPU异构系统在仅GPU系统上实现了高达4.1倍的性能提升和4.6倍的能量效率提升，提供了无须增加GPU数量的扩展能力。 

---
# Security-First AI: Foundations for Robust and Trustworthy Systems 

**Title (ZH)**: 安全优先的人工智能：坚实可靠和值得信赖系统的基础 

**Authors**: Krti Tallam  

**Link**: [PDF](https://arxiv.org/pdf/2504.16110)  

**Abstract**: The conversation around artificial intelligence (AI) often focuses on safety, transparency, accountability, alignment, and responsibility. However, AI security (i.e., the safeguarding of data, models, and pipelines from adversarial manipulation) underpins all of these efforts. This manuscript posits that AI security must be prioritized as a foundational layer. We present a hierarchical view of AI challenges, distinguishing security from safety, and argue for a security-first approach to enable trustworthy and resilient AI systems. We discuss core threat models, key attack vectors, and emerging defense mechanisms, concluding that a metric-driven approach to AI security is essential for robust AI safety, transparency, and accountability. 

**Abstract (ZH)**: 关于人工智能（AI）的讨论常常聚焦于安全、透明性、问责制、契合度和责任。然而，AI安全（即保护数据、模型和管道免受敌对操纵）是这一切努力的基础。本文认为，AI安全必须作为基础层得到优先考虑。我们提出了一个分层的AI挑战视图，区分安全与安全，主张采取以安全为主的方法以实现可信赖和健壮的AI系统。我们讨论了核心威胁模型、关键攻击向量和新兴防御机制，认为以度量驱动的方法对于实现稳健的AI安全、透明性和问责制是必不可少的。 

---
# xLSTM-ECG: Multi-label ECG Classification via Feature Fusion with xLSTM 

**Title (ZH)**: xLSTM-ECG：基于xLSTM的特征融合多标签心电图分类 

**Authors**: Lei Kang, Xuanshuo Fu, Javier Vazquez-Corral, Ernest Valveny, Dimosthenis Karatzas  

**Link**: [PDF](https://arxiv.org/pdf/2504.16101)  

**Abstract**: Cardiovascular diseases (CVDs) remain the leading cause of mortality worldwide, highlighting the critical need for efficient and accurate diagnostic tools. Electrocardiograms (ECGs) are indispensable in diagnosing various heart conditions; however, their manual interpretation is time-consuming and error-prone. In this paper, we propose xLSTM-ECG, a novel approach that leverages an extended Long Short-Term Memory (xLSTM) network for multi-label classification of ECG signals, using the PTB-XL dataset. To the best of our knowledge, this work represents the first design and application of xLSTM modules specifically adapted for multi-label ECG classification. Our method employs a Short-Time Fourier Transform (STFT) to convert time-series ECG waveforms into the frequency domain, thereby enhancing feature extraction. The xLSTM architecture is specifically tailored to address the complexities of 12-lead ECG recordings by capturing both local and global signal features. Comprehensive experiments on the PTB-XL dataset reveal that our model achieves strong multi-label classification performance, while additional tests on the Georgia 12-Lead dataset underscore its robustness and efficiency. This approach significantly improves ECG classification accuracy, thereby advancing clinical diagnostics and patient care. The code will be publicly available upon acceptance. 

**Abstract (ZH)**: 心血管疾病(CVDs)仍然是全球死亡的主要原因，强调了高效准确诊断工具的迫切需求。心电图(ECGs)在诊断各种心脏状况中不可或缺；然而，其手动解读耗时且易出错。本文提出了一种新的方法xLSTM-ECG，该方法利用扩展长短期记忆(xLSTM)网络进行心电图信号的多标签分类，并使用PTB-XL数据集。据我们所知，这是首次设计和应用专门适应多标签ECG分类的xLSTM模块。该方法采用短时傅里叶变换(STFT)将时间序列心电图波形转换到频域，从而增强特征提取。xLSTM架构特别针对12导联心电图记录的复杂性进行了优化，以捕获局部和全局信号特征。在PTB-XL数据集上的全面实验表明，我们的模型在多标签分类性能上表现出色，而额外的Georgia 12导联数据集测试进一步证明了其稳健性和效率。该方法显著提高了心电图分类准确性，从而推动临床诊断和患者护理的进步。代码将在接受后公开。 

---
# Towards Accurate Forecasting of Renewable Energy : Building Datasets and Benchmarking Machine Learning Models for Solar and Wind Power in France 

**Title (ZH)**: 面向可再生能源准确预测的研究：构建数据集和评估机器学习模型在法国太阳能和风能电力上的标杆研究 

**Authors**: Eloi Lindas, Yannig Goude, Philippe Ciais  

**Link**: [PDF](https://arxiv.org/pdf/2504.16100)  

**Abstract**: Accurate prediction of non-dispatchable renewable energy sources is essential for grid stability and price prediction. Regional power supply forecasts are usually indirect through a bottom-up approach of plant-level forecasts, incorporate lagged power values, and do not use the potential of spatially resolved data. This study presents a comprehensive methodology for predicting solar and wind power production at country scale in France using machine learning models trained with spatially explicit weather data combined with spatial information about production sites capacity. A dataset is built spanning from 2012 to 2023, using daily power production data from RTE (the national grid operator) as the target variable, with daily weather data from ERA5, production sites capacity and location, and electricity prices as input features. Three modeling approaches are explored to handle spatially resolved weather data: spatial averaging over the country, dimension reduction through principal component analysis, and a computer vision architecture to exploit complex spatial relationships. The study benchmarks state-of-the-art machine learning models as well as hyperparameter tuning approaches based on cross-validation methods on daily power production data. Results indicate that cross-validation tailored to time series is best suited to reach low error. We found that neural networks tend to outperform traditional tree-based models, which face challenges in extrapolation due to the increasing renewable capacity over time. Model performance ranges from 4% to 10% in nRMSE for midterm horizon, achieving similar error metrics to local models established at a single-plant level, highlighting the potential of these methods for regional power supply forecasting. 

**Abstract (ZH)**: 准确预测不可调度的可再生能源对于电网稳定性和价格预测至关重要。基于下自上的机组级预测方法，区域电力供应预测通常间接进行，并未充分利用空间解析数据的潜力。本研究提出了一种全面的方法，通过使用结合生产站点空间信息的空间显式气象数据训练的机器学习模型，在法国尺度上预测太阳能和风能生产。研究构建了一个从2012年到2023年的数据集，以法国国家电网运营商RTE的日电力生产数据为目标变量，输入特征包括ERA5的日气象数据、生产站点的产能和位置以及电价。研究探索了三种处理空间解析气象数据的方法：国家尺度的空间平均、主成分分析降维以及利用复杂空间关系的计算机视觉架构。这项研究在每日电力生产数据上基准了先进的机器学习模型及基于交叉验证方法的超参数调优方法。结果表明，针对时间序列进行定制的交叉验证最适合达到较低的误差。研究发现，神经网络倾向于优于传统的树基模型，后者由于可再生能源产能的增加而面临外推难题。在中期展望下，模型性能范围为4%至10%的nRMSE，表明这些方法在区域电力供应预测中的潜力。 

---
# Two-Timescale Joint Transmit and Pinching Beamforming for Pinching-Antenna Systems 

**Title (ZH)**: 两时标联合传输与压缩波束形成算法for压缩天线系统 

**Authors**: Luyuan Zhang, Xidong Mu, An Liu, Yuanwei Liu  

**Link**: [PDF](https://arxiv.org/pdf/2504.16099)  

**Abstract**: Pinching antenna systems (PASS) have been proposed as a revolutionary flexible antenna technology which facilitates line-of-sight links via numerous low-cost pinching antennas with adjustable activation positions over waveguides. This letter proposes a two-timescale joint transmit and pinching beamforming design for the maximization of sum rate of a PASS-based downlink multi-user multiple input single output system. A primal dual decomposition method is developed to decouple the two-timescale problem into two sub-problems: 1) A Karush-Kuhn-Tucker-guided dual learning-based approach is proposed to solve the short-term transmit beamforming design sub-problem; 2) The long-term pinching beamforming design sub-problem is tackled by adopting a stochastic successive convex approximation method. Simulation results demonstrate that the proposed two-timescale algorithm achieves a significant performance gain compared to other baselines. 

**Abstract (ZH)**: 基于PASS的下行多用户单输入多输出系统两时标联合传输与针状波束形成设计 

---
# A CNN-based Local-Global Self-Attention via Averaged Window Embeddings for Hierarchical ECG Analysis 

**Title (ZH)**: 基于CNN的平均窗口嵌入局部-全局自注意力用于心脏电图的层级分析 

**Authors**: Arthur Buzelin, Pedro Robles Dutenhefner, Turi Rezende, Luisa G. Porfirio, Pedro Bento, Yan Aquino, Jose Fernandes, Caio Santana, Gabriela Miana, Gisele L. Pappa, Antonio Ribeiro, Wagner Meira Jr  

**Link**: [PDF](https://arxiv.org/pdf/2504.16097)  

**Abstract**: Cardiovascular diseases remain the leading cause of global mortality, emphasizing the critical need for efficient diagnostic tools such as electrocardiograms (ECGs). Recent advancements in deep learning, particularly transformers, have revolutionized ECG analysis by capturing detailed waveform features as well as global rhythm patterns. However, traditional transformers struggle to effectively capture local morphological features that are critical for accurate ECG interpretation. We propose a novel Local-Global Attention ECG model (LGA-ECG) to address this limitation, integrating convolutional inductive biases with global self-attention mechanisms. Our approach extracts queries by averaging embeddings obtained from overlapping convolutional windows, enabling fine-grained morphological analysis, while simultaneously modeling global context through attention to keys and values derived from the entire sequence. Experiments conducted on the CODE-15 dataset demonstrate that LGA-ECG outperforms state-of-the-art models and ablation studies validate the effectiveness of the local-global attention strategy. By capturing the hierarchical temporal dependencies and morphological patterns in ECG signals, this new design showcases its potential for clinical deployment with robust automated ECG classification. 

**Abstract (ZH)**: 心血管疾病仍然是全球 mortality 的首要原因，强调了高效诊断工具如心电图（ECGs）的重要性。最近深度学习的进步，尤其是 transformer 的应用，已经通过捕捉详细的心电波形特征及整体节律模式重塑了心电图分析。然而，传统的 transformer 在捕捉对于准确心电图解释至关重要的局部形态特征方面存在困难。我们提出了一种新型局部-全局注意力心电图模型（LGA-ECG），结合卷积诱导偏差与全局自我注意机制。我们的方法通过从重叠卷积窗口中获得的嵌入平均来提取查询，从而实现精细的形态学分析，同时通过注意力机制处理整个序列得到的键和值来建模全局上下文。在 CODE-15 数据集上进行的实验表明，LGA-ECG 超过了现有最好的模型，并且消融研究验证了局部-全局注意力策略的有效性。通过捕捉心电图信号中的分层时间依赖性和形态学模式，此新设计展示了其在具有稳健自动心电图分类能力的临床部署中的潜力。 

---
# BrainPrompt: Multi-Level Brain Prompt Enhancement for Neurological Condition Identification 

**Title (ZH)**: 脑提示：多层脑提示增强用于神经疾病识别 

**Authors**: Jiaxing Xu, Kai He, Yue Tang, Wei Li, Mengcheng Lan, Xia Dong, Yiping Ke, Mengling Feng  

**Link**: [PDF](https://arxiv.org/pdf/2504.16096)  

**Abstract**: Neurological conditions, such as Alzheimer's Disease, are challenging to diagnose, particularly in the early stages where symptoms closely resemble healthy controls. Existing brain network analysis methods primarily focus on graph-based models that rely solely on imaging data, which may overlook important non-imaging factors and limit the model's predictive power and interpretability. In this paper, we present BrainPrompt, an innovative framework that enhances Graph Neural Networks (GNNs) by integrating Large Language Models (LLMs) with knowledge-driven prompts, enabling more effective capture of complex, non-imaging information and external knowledge for neurological disease identification. BrainPrompt integrates three types of knowledge-driven prompts: (1) ROI-level prompts to encode the identity and function of each brain region, (2) subject-level prompts that incorporate demographic information, and (3) disease-level prompts to capture the temporal progression of disease. By leveraging these multi-level prompts, BrainPrompt effectively harnesses knowledge-enhanced multi-modal information from LLMs, enhancing the model's capability to predict neurological disease stages and meanwhile offers more interpretable results. We evaluate BrainPrompt on two resting-state functional Magnetic Resonance Imaging (fMRI) datasets from neurological disorders, showing its superiority over state-of-the-art methods. Additionally, a biomarker study demonstrates the framework's ability to extract valuable and interpretable information aligned with domain knowledge in neuroscience. 

**Abstract (ZH)**: 神经学条件，如阿尔茨海默病，诊断起来颇具挑战性，尤其是在早期阶段，因为其症状与健康对照组极为相似。现有的脑网络分析方法主要依赖于基于图的模型，这些模型仅依靠影像数据，可能会忽略重要的非影像因素，从而限制了模型的预测能力和可解释性。本文提出了BrainPrompt，这是一种创新框架，通过将大型语言模型（LLMs）与知识驱动的提示结合起来，增强图神经网络（GNNs），从而使模型更有效地捕捉到复杂的非影像信息和外部知识，以识别神经性疾病。BrainPrompt整合了三种类型的知识驱动提示：（1）ROI级提示，用于编码每个大脑区域的身份和功能；（2）受试者级提示，结合人口统计学信息；（3）疾病级提示，捕捉疾病的纵向进展。通过利用这些多级提示，BrainPrompt有效地利用了LLMs提供的知识增强的多模态信息，增强了模型预测神经性疾病阶段的能力，并提供更具可解释性的结果。我们在两种静息态功能性磁共振成像（fMRI）神经障碍数据集上评估了BrainPrompt，显示其在最先进的方法之上具有优越性。此外，生物标志物研究还表明，该框架能够提取与神经科学领域知识相一致的有价值和可解释的信息。 

---
# Efficient Portfolio Selection through Preference Aggregation with Quicksort and the Bradley--Terry Model 

**Title (ZH)**: 基于快速排序和布雷德利-泰尔模型的偏好聚合的有效投资组合选择 

**Authors**: Yurun Ge, Lucas Böttcher, Tom Chou, Maria R. D'Orsogna  

**Link**: [PDF](https://arxiv.org/pdf/2504.16093)  

**Abstract**: How to allocate limited resources to projects that will yield the greatest long-term benefits is a problem that often arises in decision-making under uncertainty. For example, organizations may need to evaluate and select innovation projects with risky returns. Similarly, when allocating resources to research projects, funding agencies are tasked with identifying the most promising proposals based on idiosyncratic criteria. Finally, in participatory budgeting, a local community may need to select a subset of public projects to fund. Regardless of context, agents must estimate the uncertain values of a potentially large number of projects. Developing parsimonious methods to compare these projects, and aggregating agent evaluations so that the overall benefit is maximized, are critical in assembling the best project portfolio. Unlike in standard sorting algorithms, evaluating projects on the basis of uncertain long-term benefits introduces additional complexities. We propose comparison rules based on Quicksort and the Bradley--Terry model, which connects rankings to pairwise "win" probabilities. In our model, each agent determines win probabilities of a pair of projects based on his or her specific evaluation of the projects' long-term benefit. The win probabilities are then appropriately aggregated and used to rank projects. Several of the methods we propose perform better than the two most effective aggregation methods currently available. Additionally, our methods can be combined with sampling techniques to significantly reduce the number of pairwise comparisons. We also discuss how the Bradley--Terry portfolio selection approach can be implemented in practice. 

**Abstract (ZH)**: 如何将有限资源分配给预期能带来最大长期效益的项目是一个在不确定性条件下做决策时常遇到的问题。例如，组织可能需要评估和选择具有风险回报的研发项目。同样，在为研究项目分配资源时，资助机构需根据特定标准识别最有前景的提案。最后，在参与预算制中，当地社区可能需要选择一部分公共项目进行资助。无论在何种情境下，决策者都必须估算大量项目的不确定性价值。开发简洁的方法来比较这些项目，并综合决策者评估以最大化总体效益，对于构建最佳项目组合至关重要。与标准排序算法不同，基于不确定长期效益评估项目引入了额外的复杂性。我们提出了基于Quicksort和Bradley-Terry模型的比较规则，将排名与“胜负”概率联系起来。在我们的模型中，每个决策者根据对项目长期效益的特定评估来确定项目对之间的胜率。然后对这些胜率进行适当综合，并用于项目排序。我们提出的一些方法在综合效果方面优于目前最有效的两种方法。此外，我们的方法可以与采样技术结合使用，显著减少项目对的比较次数。我们还讨论了如何在实践中实现Bradley-Terry项目选择方法。 

---
