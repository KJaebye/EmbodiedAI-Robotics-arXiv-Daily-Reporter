{'arxiv_id': 'arXiv:2504.16916', 'title': 'Zero-shot Sim-to-Real Transfer for Reinforcement Learning-based Visual Servoing of Soft Continuum Arms', 'authors': 'Hsin-Jung Yang, Mahsa Khosravi, Benjamin Walt, Girish Krishnan, Soumik Sarkar', 'link': 'https://arxiv.org/abs/2504.16916', 'abstract': 'Soft continuum arms (SCAs) soft and deformable nature presents challenges in modeling and control due to their infinite degrees of freedom and non-linear behavior. This work introduces a reinforcement learning (RL)-based framework for visual servoing tasks on SCAs with zero-shot sim-to-real transfer capabilities, demonstrated on a single section pneumatic manipulator capable of bending and twisting. The framework decouples kinematics from mechanical properties using an RL kinematic controller for motion planning and a local controller for actuation refinement, leveraging minimal sensing with visual feedback. Trained entirely in simulation, the RL controller achieved a 99.8% success rate. When deployed on hardware, it achieved a 67% success rate in zero-shot sim-to-real transfer, demonstrating robustness and adaptability. This approach offers a scalable solution for SCAs in 3D visual servoing, with potential for further refinement and expanded applications.', 'abstract_zh': '基于强化学习的软连续臂视觉伺服方法：零样本模拟到现实的转移能力', 'title_zh': '基于强化学习的软连续臂视觉伺服零样本模拟到现实转移'}
{'arxiv_id': 'arXiv:2504.16129', 'title': 'MARFT: Multi-Agent Reinforcement Fine-Tuning', 'authors': 'Junwei Liao, Muning Wen, Jun Wang, Weinan Zhang', 'link': 'https://arxiv.org/abs/2504.16129', 'abstract': 'LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in addressing complex, agentic tasks requiring multifaceted reasoning and collaboration, from generating high-quality presentation slides to conducting sophisticated scientific research. Meanwhile, RL has been widely recognized for its effectiveness in enhancing agent intelligence, but limited research has investigated the fine-tuning of LaMAS using foundational RL techniques. Moreover, the direct application of MARL methodologies to LaMAS introduces significant challenges, stemming from the unique characteristics and mechanisms inherent to LaMAS. To address these challenges, this article presents a comprehensive study of LLM-based MARL and proposes a novel paradigm termed Multi-Agent Reinforcement Fine-Tuning (MARFT). We introduce a universal algorithmic framework tailored for LaMAS, outlining the conceptual foundations, key distinctions, and practical implementation strategies. We begin by reviewing the evolution from RL to Reinforcement Fine-Tuning, setting the stage for a parallel analysis in the multi-agent domain. In the context of LaMAS, we elucidate critical differences between MARL and MARFT. These differences motivate a transition toward a novel, LaMAS-oriented formulation of RFT. Central to this work is the presentation of a robust and scalable MARFT framework. We detail the core algorithm and provide a complete, open-source implementation to facilitate adoption and further research. The latter sections of the paper explore real-world application perspectives and opening challenges in MARFT. By bridging theoretical underpinnings with practical methodologies, this work aims to serve as a roadmap for researchers seeking to advance MARFT toward resilient and adaptive solutions in agentic systems. Our implementation of the proposed framework is publicly available at: this https URL.', 'abstract_zh': '基于LLM的多Agent系统在处理需要多方面推理和协作的复杂任务方面展现了显著的能力，从生成高质量的演示文稿到进行复杂的科学研究。同时，强化学习因其在提升智能体能力方面的有效性而得到广泛应用，但对于使用基础强化学习技术微调LaMAS的有限研究引起了关注。此外，将MARL方法直接应用于LaMAS引入了重大挑战，源于LaMAS特有的特性和机制。为应对这些挑战，本文对基于LLM的MARL进行了全面研究，并提出了一种新型范式——多Agent强化微调（MARFT）。我们介绍了适用于LaMAS的通用算法框架，概述了概念基础、关键区别和实际实施策略。我们首先回顾了从RL到强化微调的演变，为多Agent领域提供了并行分析的背景。在LaMAS的背景下，我们详细阐述了MARL和MARFT之间的关键区别，并据此推动了一种新形式的RFT。本文的核心在于展示了一个稳健且可扩展的MARFT框架。我们详细描述了核心算法，并提供了完整的开源实现，以促进应用和进一步研究。文章后部分探讨了MARFT在实际应用中的前景和挑战。通过结合理论基础和实用方法论，本文旨在为研究人员提供一条路线图，以推进MARFT向具有韧性与适应性的智能系统解决方案的发展。我们提出的框架的实现已在以下链接公开：this https URL。', 'title_zh': 'MARFT：多代理强化学习微调'}
{'arxiv_id': 'arXiv:2504.16760', 'title': 'Lightweight Latent Verifiers for Efficient Meta-Generation Strategies', 'authors': 'Bartosz Piotrowski, Witold Drzewakowski, Konrad Staniszewski, Piotr Miłoś', 'link': 'https://arxiv.org/abs/2504.16760', 'abstract': 'Verifiers are auxiliary models that assess the correctness of outputs generated by base large language models (LLMs). They play a crucial role in many strategies for solving reasoning-intensive problems with LLMs. Typically, verifiers are LLMs themselves, often as large (or larger) than the base model they support, making them computationally expensive. In this work, we introduce a novel lightweight verification approach, LiLaVe, which reliably extracts correctness signals from the hidden states of the base LLM. A key advantage of LiLaVe is its ability to operate with only a small fraction of the computational budget required by traditional LLM-based verifiers. To demonstrate its practicality, we couple LiLaVe with popular meta-generation strategies, like best-of-n or self-consistency. Moreover, we design novel LiLaVe-based approaches, like conditional self-correction or conditional majority voting, that significantly improve both accuracy and efficiency in generation tasks with smaller LLMs. Our work demonstrates the fruitfulness of extracting latent information from the hidden states of LLMs, and opens the door to scalable and resource-efficient solutions for reasoning-intensive applications.', 'abstract_zh': '验证器是辅助模型，用以评估基础大语言模型（LLMs）生成输出的正确性。它们在使用LLMs解决推理密集型问题的各种策略中扮演着至关重要的角色。通常，验证器本身就是LLMs，往往比它们支持的基础模型更大（或更大），使其在计算上非常昂贵。在本文中，我们介绍了一种新颖的轻量级验证方法LiLaVe，它可以可靠地从基础LLM的隐藏状态中提取正确性信号。LiLaVe的一个关键优势是，它可以仅使用传统基于LLM的验证器所需计算预算的一小部分来运行。为了证明其实用性，我们将LiLaVe与流行的元生成策略（如最佳n选一或自一致性）结合使用。此外，我们设计了基于LiLaVe的新颖方法，如条件自我纠正或条件多数表决，这些方法在使用小型LLM进行生成任务时显著提高了准确性和效率。我们的工作证明了从LLM的隐藏状态中提取潜在信息的有效性，并为推理密集型应用提供了可扩展且资源高效的解决方案。', 'title_zh': '轻量级潜在验证器以实现高效的元生成策略'}
{'arxiv_id': 'arXiv:2504.16736', 'title': 'A Survey of AI Agent Protocols', 'authors': 'Yingxuan Yang, Huacan Chai, Yuanyi Song, Siyuan Qi, Muning Wen, Ning Li, Junwei Liao, Haoyi Hu, Jianghao Lin, Gaowei Chang, Weiwen Liu, Ying Wen, Yong Yu, Weinan Zhang', 'link': 'https://arxiv.org/abs/2504.16736', 'abstract': 'The rapid development of large language models (LLMs) has led to the widespread deployment of LLM agents across diverse industries, including customer service, content generation, data analysis, and even healthcare. However, as more LLM agents are deployed, a major issue has emerged: there is no standard way for these agents to communicate with external tools or data sources. This lack of standardized protocols makes it difficult for agents to work together or scale effectively, and it limits their ability to tackle complex, real-world tasks. A unified communication protocol for LLM agents could change this. It would allow agents and tools to interact more smoothly, encourage collaboration, and triggering the formation of collective intelligence. In this paper, we provide a systematic overview of existing communication protocols for LLM agents. We classify them into four main categories and make an analysis to help users and developers select the most suitable protocols for specific applications. Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency. Finally, we explore future challenges, such as how protocols can adapt and survive in fast-evolving environments, and what qualities future protocols might need to support the next generation of LLM agent ecosystems. We expect this work to serve as a practical reference for both researchers and engineers seeking to design, evaluate, or integrate robust communication infrastructures for intelligent agents.', 'abstract_zh': '大型语言模型快速发展的背景下，这些模型的代理已在客户服务、内容生成、数据分析乃至医疗保健等多个行业得到了广泛应用。然而，随着更多代理的部署，出现了一个主要问题：这些代理没有标准化的方式与外部工具或数据源进行通信。缺乏标准化的协议使得代理难以协同工作或有效扩展，并限制了它们解决复杂现实任务的能力。为LLM代理制定统一的通信协议可以改变这一状况。该协议将使代理和工具能够更顺畅地交互，促进合作，并促使集体智能的形成。本文系统地概述了现有的LLM代理通信协议，将其分为四大类，并进行分析以帮助用户和开发人员选择最适合特定应用的协议。此外，我们还从安全性、可扩展性和延迟等关键维度对这些协议进行了比较性能分析。最后，我们探讨了未来挑战，如协议如何适应和在快速变化的环境中生存，以及未来协议可能需要具备哪些特性以支持新一代LLM代理生态系统。我们期望这项工作能够为研究人员和工程师提供一个实用的参考，用于设计、评估或集成用于智能代理的强大通信基础设施。', 'title_zh': 'AI代理协议综述'}
{'arxiv_id': 'arXiv:2504.16728', 'title': 'IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery', 'authors': 'Aniketh Garikaparthi, Manasi Patwardhan, Lovekesh Vig, Arman Cohan', 'link': 'https://arxiv.org/abs/2504.16728', 'abstract': 'The rapid advancement in capabilities of large language models (LLMs) raises a pivotal question: How can LLMs accelerate scientific discovery? This work tackles the crucial first stage of research, generating novel hypotheses. While recent work on automated hypothesis generation focuses on multi-agent frameworks and extending test-time compute, none of the approaches effectively incorporate transparency and steerability through a synergistic Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS: Interactive Research Ideation System, an open-source platform designed for researchers to leverage LLM-assisted scientific ideation. IRIS incorporates innovative features to enhance ideation, including adaptive test-time compute expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism, and query-based literature synthesis. Designed to empower researchers with greater control and insight throughout the ideation process. We additionally conduct a user study with researchers across diverse disciplines, validating the effectiveness of our system in enhancing ideation. We open-source our code at this https URL', 'abstract_zh': '大型语言模型（LLMs）能力的 rapid advancement 产生了关键问题：LLMs 如何度量加速科学发现？本文解决研究的关键第一阶段，即生成新颖假设。尽管 recent work 在自动假设生成方面侧重于多智能体框架和扩展测试时计算资源，但 none of these approaches 有效通过协同的人机协同（Human-in-the-loop, HITL）方式整合透明度和可控性。为填补这一空白，我们提出 IRIS：交互式研究构思系统，一个旨在供研究人员利用 LLM 辅助的科学研究构思的开源平台。IRIS 结合了增强构思的创新功能，包括通过蒙特卡洛树搜索（MCTS）实现自适应测试时计算资源扩展、细粒度反馈机制以及基于查询的文献整合。旨在赋予研究人员在整个构思过程中更大的控制力和洞察力。我们还对来自不同学科的研究人员进行了用户研究，验证了系统的有效性，能够在构思过程中提高效率。我们的代码在以下链接开源：this https URL', 'title_zh': 'IRIS: 交互式研究思路系统，用于加速科学发现'}
{'arxiv_id': 'arXiv:2504.16273', 'title': 'Investigating LLMs in Clinical Triage: Promising Capabilities, Persistent Intersectional Biases', 'authors': 'Joseph Lee, Tianqi Shang, Jae Young Baik, Duy Duong-Tran, Shu Yang, Lingyao Li, Li Shen', 'link': 'https://arxiv.org/abs/2504.16273', 'abstract': 'Large Language Models (LLMs) have shown promise in clinical decision support, yet their application to triage remains underexplored. We systematically investigate the capabilities of LLMs in emergency department triage through two key dimensions: (1) robustness to distribution shifts and missing data, and (2) counterfactual analysis of intersectional biases across sex and race. We assess multiple LLM-based approaches, ranging from continued pre-training to in-context learning, as well as machine learning approaches. Our results indicate that LLMs exhibit superior robustness, and we investigate the key factors contributing to the promising LLM-based approaches. Furthermore, in this setting, we identify gaps in LLM preferences that emerge in particular intersections of sex and race. LLMs generally exhibit sex-based differences, but they are most pronounced in certain racial groups. These findings suggest that LLMs encode demographic preferences that may emerge in specific clinical contexts or particular combinations of characteristics.', 'abstract_zh': '大型语言模型（LLMs）在临床决策支持方面展现了潜力，但在急诊分诊应用方面仍有待探索。我们通过两个关键维度系统地研究了LLMs在急诊分诊中的能力：（1）在分布偏移和缺失数据情况下的鲁棒性，以及（2）性别和种族交叉偏见的反事实分析。我们评估了从持续预训练到情景学习的多种LLM方法，以及机器学习方法。我们的结果表明，LLMs在鲁棒性方面表现更为优越，并探讨了促成有前景的LLM方法的关键因素。此外，在此情境下，我们识别出了性别和种族交叉重叠中出现的LLMs偏好差距。LLMs通常表现出性别差异，但在某些种族群体中尤为明显。这些发现表明，LLMs可能在特定临床情境或特定特征组合中编码了人口统计学偏好。', 'title_zh': '探究大型语言模型在临床分流中的应用：有潜力的能力与持续存在的交叉偏见'}
{'arxiv_id': 'arXiv:2504.16918', 'title': 'OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents', 'authors': 'Raghav Thind, Youran Sun, Ling Liang, Haizhao Yang', 'link': 'https://arxiv.org/abs/2504.16918', 'abstract': 'Optimization plays a vital role in scientific research and practical applications, but formulating a concrete optimization problem described in natural language into a mathematical form and selecting a suitable solver to solve the problem requires substantial domain expertise. We introduce \\textbf{OptimAI}, a framework for solving \\underline{Optim}ization problems described in natural language by leveraging LLM-powered \\underline{AI} agents, achieving superior performance over current state-of-the-art methods. Our framework is built upon four key roles: (1) a \\emph{formulator} that translates natural language problem descriptions into precise mathematical formulations; (2) a \\emph{planner} that constructs a high-level solution strategy prior to execution; and (3) a \\emph{coder} and a \\emph{code critic} capable of interacting with the environment and reflecting on outcomes to refine future actions. Ablation studies confirm that all roles are essential; removing the planner or code critic results in $5.8\\times$ and $3.1\\times$ drops in productivity, respectively. Furthermore, we introduce UCB-based debug scheduling to dynamically switch between alternative plans, yielding an additional $3.3\\times$ productivity gain. Our design emphasizes multi-agent collaboration, allowing us to conveniently explore the synergistic effect of combining diverse models within a unified system. Our approach attains 88.1\\% accuracy on the NLP4LP dataset and 71.2\\% on the Optibench (non-linear w/o table) subset, reducing error rates by 58\\% and 50\\% respectively over prior best results.', 'abstract_zh': 'OptimAI：基于LLM的自然语言描述优化问题求解框架', 'title_zh': 'OptimAI: 从自然语言进行优化的LLM驱动AI代理'}
{'arxiv_id': 'arXiv:2504.16913', 'title': 'Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text', 'authors': 'Shifali Agrahari, Sanasam Ranbir Singh', 'link': 'https://arxiv.org/abs/2504.16913', 'abstract': 'In recent years, the detection of AI-generated text has become a critical area of research due to concerns about academic integrity, misinformation, and ethical AI deployment. This paper presents COT Fine-tuned, a novel framework for detecting AI-generated text and identifying the specific language model. responsible for generating the text. We propose a dual-task approach, where Task A involves classifying text as AI-generated or human-written, and Task B identifies the specific LLM behind the text. The key innovation of our method lies in the use of Chain-of-Thought reasoning, which enables the model to generate explanations for its predictions, enhancing transparency and interpretability. Our experiments demonstrate that COT Fine-tuned achieves high accuracy in both tasks, with strong performance in LLM identification and human-AI classification. We also show that the CoT reasoning process contributes significantly to the models effectiveness and interpretability.', 'abstract_zh': '近年来，AI生成文本的检测已成为一项关键研究领域，由于学术诚信、虚假信息和伦理AI部署的担忧。本文提出了一种新型框架COT Fine-tuned，用于检测AI生成文本并识别其背后的特定语言模型。我们提出了一种双任务方法，其中任务A涉及将文本分类为AI生成或人类编写，任务B识别文本背后的特定大型语言模型。我们方法的关键创新在于使用链式推理，这使模型能够为其预测生成解释，从而提高透明度和可解释性。我们的实验显示，COT Fine-tuned在两个任务上均取得了高准确性，特别是在大型语言模型识别和人类-AI分类方面表现优异。我们还展示了链式推理过程对模型的 effectiveness 和可解释性有显著贡献。', 'title_zh': '追踪思维：使用链式推理识别生成文本的大型语言模型'}
{'arxiv_id': 'arXiv:2504.16834', 'title': 'Improving Significant Wave Height Prediction Using Chronos Models', 'authors': 'Yilin Zhai, Hongyuan Shi, Chao Zhan, Qing Wang, Zaijin You, Nan Wang', 'link': 'https://arxiv.org/abs/2504.16834', 'abstract': 'Accurate wave height prediction is critical for maritime safety and coastal resilience, yet conventional physics-based models and traditional machine learning methods face challenges in computational efficiency and nonlinear dynamics modeling. This study introduces Chronos, the first implementation of a large language model (LLM)-powered temporal architecture (Chronos) optimized for wave forecasting. Through advanced temporal pattern recognition applied to historical wave data from three strategically chosen marine zones in the Northwest Pacific basin, our framework achieves multimodal improvements: (1) 14.3% reduction in training time with 2.5x faster inference speed compared to PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units; (2) superior short-term forecasting (1-24h) across comprehensive metrics; (3) sustained predictive leadership in extended-range forecasts (1-120h); and (4) demonstrated zero-shot capability maintaining median performance (rank 4/12) against specialized operational models. This LLM-enhanced temporal modeling paradigm establishes a new standard in wave prediction, offering both computationally efficient solutions and a transferable framework for complex geophysical systems modeling.', 'abstract_zh': '基于大型语言模型的Chronos时空架构：精确波高预测的新标准', 'title_zh': '使用Chronos模型提高显著波高预测精度'}
{'arxiv_id': 'arXiv:2504.16828', 'title': 'Process Reward Models That Think', 'authors': 'Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, Lu Wang', 'link': 'https://arxiv.org/abs/2504.16828', 'abstract': "Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models will be released at this https URL.", 'abstract_zh': "逐步验证器——也称为过程奖励模型（PRMs）——是测试时扩展的关键成分。PRMs需要步骤级监督，使其训练成本高昂。本文旨在构建数据高效的PRMs，即基于逐步奖励模式的语言模型，通过生成验证链式思维（CoT）来验证每一步。我们提出ThinkPRM，这是一种长CoT验证器，在比判别性PRMs所需的工艺标签少多个数量级的条件下进行微调。我们的方法利用长CoT模型固有的推理能力，在PRM800K的1%工艺标签下，在多个挑战性基准上优于LLM-as-a-Judge和判别性验证器。具体而言，在ProcessBench、MATH-500和AIME '24的最佳选择和奖励引导搜索下，ThinkPRM优于基线。在GPQA-Diamond和LiveCodeBench的子集的域外评估中，我们的PRM在判别性验证器在PRM800K上进行完整训练的基础上分别优于它们8%和4.5%。最后，与LLM-as-a-Judge相比，在相同令牌预算下，ThinkPRM更有效地扩展验证计算能力，在ProcessBench的子集上优于LLM-as-a-Judge 7.2%。我们的工作突显了生成性长CoT PRMs的价值，这些模型可以在需要最少监督的情况下扩展验证时的计算能力。我们的代码、数据和模型将在以下链接发布：this https URL。", 'title_zh': '思考的进程奖励模型'}
{'arxiv_id': 'arXiv:2504.16795', 'title': 'Random Long-Context Access for Mamba via Hardware-aligned Hierarchical Sparse Attention', 'authors': 'Xiang Hu, Jiaqi Leng, Jun Zhao, Kewei Tu, Wei Wu', 'link': 'https://arxiv.org/abs/2504.16795', 'abstract': "A key advantage of Recurrent Neural Networks (RNNs) over Transformers is their linear computational and space complexity enables faster training and inference for long sequences. However, RNNs are fundamentally unable to randomly access historical context, and simply integrating attention mechanisms may undermine their efficiency advantages. To overcome this limitation, we propose \\textbf{H}ierarchical \\textbf{S}parse \\textbf{A}ttention (HSA), a novel attention mechanism that enhances RNNs with long-range random access flexibility while preserving their merits in efficiency and length generalization. HSA divides inputs into chunks, selecting the top-$k$ chunks and hierarchically aggregates information. The core innovation lies in learning token-to-chunk relevance based on fine-grained token-level information inside each chunk. This approach enhances the precision of chunk selection across both in-domain and out-of-domain context lengths. To make HSA efficient, we further introduce a hardware-aligned kernel design. By combining HSA with Mamba, we introduce RAMba, which achieves perfect accuracy in passkey retrieval across 64 million contexts despite pre-training on only 4K-length contexts, and significant improvements on various downstream tasks, with nearly constant memory footprint. These results show RAMba's huge potential in long-context modeling.", 'abstract_zh': 'Recurrent Neural Networks with Hierarchical Sparse Attention for Efficient Long-Sequence Modeling', 'title_zh': 'Mamba的硬件对齐分层稀疏注意机制下的随机长上下文访问'}
{'arxiv_id': 'arXiv:2504.16768', 'title': 'How Effective are Generative Large Language Models in Performing Requirements Classification?', 'authors': 'Waad Alhoshan, Alessio Ferrari, Liping Zhao', 'link': 'https://arxiv.org/abs/2504.16768', 'abstract': 'In recent years, transformer-based large language models (LLMs) have revolutionised natural language processing (NLP), with generative models opening new possibilities for tasks that require context-aware text generation. Requirements engineering (RE) has also seen a surge in the experimentation of LLMs for different tasks, including trace-link detection, regulatory compliance, and others. Requirements classification is a common task in RE. While non-generative LLMs like BERT have been successfully applied to this task, there has been limited exploration of generative LLMs. This gap raises an important question: how well can generative LLMs, which produce context-aware outputs, perform in requirements classification? In this study, we explore the effectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing both binary and multi-class requirements classification. We design an extensive experimental study involving over 400 experiments across three widely used datasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes that while factors like prompt design and LLM architecture are universally important, others-such as dataset variations-have a more situational impact, depending on the complexity of the classification task. This insight can guide future model development and deployment strategies, focusing on optimising prompt structures and aligning model architectures with task-specific needs for improved performance.', 'abstract_zh': '基于变压器的大语言模型近年来革新了自然语言处理（NLP），生成模型为需要上下文感知文本生成的任务开启了新的可能性。在需求工程（RE）领域，大语言模型也被用于不同任务的实验，包括跟踪链接检测、法规合规性等。需求分类是RE中的一个常见任务。尽管像BERT这样的非生成模型已经在该任务上取得了成功应用，但生成大语言模型的应用却相对有限。这一差距提出了一个重要问题：上下文感知的生成大语言模型在需求分类任务中的表现如何？本研究探讨了三个生成大语言模型——Bloom、Gemma 和 Llama 在执行二分类和多分类需求分类任务中的有效性。我们设计了一个广泛的实验研究，涉及超过400个实验，涵盖了三个广泛使用的数据集（PROMISE NFR、Functional-Quality 和 SecReq）。研究结果表明，尽管提示设计和大语言模型架构等因素普遍重要，但其他因素如数据集的差异，会根据不同分类任务的复杂度而产生更情境化的影响力。这一见解可以指导未来的模型开发和部署策略，专注于优化提示结构并使模型架构与其特定任务需求相匹配，以提高性能。', 'title_zh': '生成型大型语言模型在执行需求分类任务中效果如何？'}
{'arxiv_id': 'arXiv:2504.16754', 'title': 'HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations', 'authors': 'Kwangseob Ahn', 'link': 'https://arxiv.org/abs/2504.16754', 'abstract': 'Large language models (LLMs) struggle with maintaining coherence in extended conversations spanning hundreds of turns, despite performing well within their context windows. This paper introduces HEMA (Hippocampus-Inspired Extended Memory Architecture), a dual-memory system inspired by human cognitive processes. HEMA combines Compact Memory - a continuously updated one-sentence summary preserving global narrative coherence, and Vector Memory - an episodic store of chunk embeddings queried via cosine similarity. When integrated with a 6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns while keeping prompt length under 3,500 tokens. Experimental results show substantial improvements: factual recall accuracy increases from 41% to 87%, and human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K indexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling the area under the precision-recall curve compared to summarization-only approaches. Ablation studies reveal two key insights: semantic forgetting through age-weighted pruning reduces retrieval latency by 34% with minimal recall loss, and a two-level summary hierarchy prevents cascade errors in ultra-long conversations exceeding 1,000 turns. HEMA demonstrates that combining verbatim recall with semantic continuity provides a practical solution for privacy-aware conversational AI capable of month-long dialogues without model retraining.', 'abstract_zh': 'HEMA：受海马体启发的扩展记忆架构在大规模语言模型中的应用', 'title_zh': 'HEMA：一种受海马体启发的扩展记忆架构用于长语境AI对话'}
{'arxiv_id': 'arXiv:2504.16727', 'title': 'V$^2$R-Bench: Holistically Evaluating LVLM Robustness to Fundamental Visual Variations', 'authors': 'Zhiyuan Fan, Yumeng Wang, Sandeep Polisetty, Yi R., Fung', 'link': 'https://arxiv.org/abs/2504.16727', 'abstract': 'Large Vision Language Models (LVLMs) excel in various vision-language tasks. Yet, their robustness to visual variations in position, scale, orientation, and context that objects in natural scenes inevitably exhibit due to changes in viewpoint and environment remains largely underexplored. To bridge this gap, we introduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating Visual Variation Robustness of LVLMs, which encompasses automated evaluation dataset generation and principled metrics for thorough robustness assessment. Through extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability to visual variations, in which even advanced models that excel at complex vision-language tasks significantly underperform on simple tasks such as object recognition. Interestingly, these models exhibit a distinct visual position bias that contradicts theories of effective receptive fields, and demonstrate a human-like visual acuity threshold. To identify the source of these vulnerabilities, we present a systematic framework for component-level analysis, featuring a novel visualization approach for aligned visual features. Results show that these vulnerabilities stem from error accumulation in the pipeline architecture and inadequate multimodal alignment. Complementary experiments with synthetic data further demonstrate that these limitations are fundamentally architectural deficiencies, scoring the need for architectural innovations in future LVLM designs.', 'abstract_zh': '大型视觉语言模型（LVLMs）在各种视觉语言任务中表现出色。然而，它们对自然场景中物体由于视角和环境变化而不可避免地表现出的位置、尺度、方向和上下文视觉变异的鲁棒性研究仍然不足。为填补这一空白，我们提出了V$^2$R-Bench，一个全面的基准框架，用于评估LVLMs的视觉变异鲁棒性，该框架包含自动化评估数据集生成和原则性的度量标准，以便进行全面的鲁棒性评估。通过在21个LVLMs上的广泛评估，我们揭示了一个令人惊讶的视觉变异脆弱性，即使在复杂视觉语言任务中表现出色的模型在简单的物体识别任务中也会显著表现不佳。有趣的是，这些模型表现出与有效感受野理论相悖的视觉位置偏见，并展示了类似人类的视觉敏锐度阈值。为了识别这些脆弱性的来源，我们提出了一个系统的组件级分析框架，并引入了一种新的对齐视觉特征的可视化方法。结果显示，这些脆弱性源于流水线架构中的错误累积和不足的多模态对齐。补充实验进一步证明了这些局限性是根本性的架构缺陷，强调了未来LVLM设计中架构创新的必要性。', 'title_zh': 'V$^2$R-Bench: 综合评估LVLM 对基本视觉变化的鲁棒性'}
{'arxiv_id': 'arXiv:2504.16677', 'title': "A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics", 'authors': 'Luisa Shimabucoro, Ahmet Ustun, Marzieh Fadaee, Sebastian Ruder', 'link': 'https://arxiv.org/abs/2504.16677', 'abstract': 'In order for large language models to be useful across the globe, they are fine-tuned to follow instructions on multilingual data. Despite the ubiquity of such post-training, a clear understanding of the dynamics that enable cross-lingual transfer remains elusive. This study examines cross-lingual transfer (CLT) dynamics in realistic post-training settings. We study two model families of up to 35B parameters in size trained on carefully controlled mixtures of multilingual data on three generative tasks with varying levels of complexity (summarization, instruction following, and mathematical reasoning) in both single-task and multi-task instruction tuning settings. Overall, we find that the dynamics of cross-lingual transfer and multilingual performance cannot be explained by isolated variables, varying depending on the combination of post-training settings. Finally, we identify the conditions that lead to effective cross-lingual transfer in practice.', 'abstract_zh': '为了使大型语言模型在全球范围内具有实用性，它们在多语言数据上进行了微调。尽管这种后训练是普遍存在的，但关于使跨语言迁移成为可能的动力学机制仍不清楚。本研究探讨了在现实后训练设置中跨语言迁移（CLT）的动力学。我们研究了在严格控制的多语言数据混合训练下，在三个不同复杂程度的生成任务（总结、指令遵循和数学推理）上训练的两个模型家族，这些任务分别在单任务和多任务指令微调设置中进行。总体而言，我们发现跨语言迁移和多语言性能的动力学机制不能仅由孤立的变量解释，而是取决于后训练设置的组合。最后，我们确定了在实践中实现有效跨语言迁移的条件。', 'title_zh': '多语言训练数据后训练指南：揭示跨语言迁移动态'}
{'arxiv_id': 'arXiv:2504.16604', 'title': 'Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories', 'authors': 'Mareike Lisker, Christina Gottschalk, Helena Mihaljević', 'link': 'https://arxiv.org/abs/2504.16604', 'abstract': 'Counterspeech is a key strategy against harmful online content, but scaling expert-driven efforts is challenging. Large Language Models (LLMs) present a potential solution, though their use in countering conspiracy theories is under-researched. Unlike for hate speech, no datasets exist that pair conspiracy theory comments with expert-crafted counterspeech. We address this gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively apply counterspeech strategies derived from psychological research provided through structured prompts. Our results show that the models often generate generic, repetitive, or superficial results. Additionally, they over-acknowledge fear and frequently hallucinate facts, sources, or figures, making their prompt-based use in practical applications problematic.', 'abstract_zh': '专家驱动的反制言论是应对有害网络内容的关键策略，但扩大此类努力具有挑战性。大型语言模型（LLMs）可能提供一种解决方案，尽管它们在反制阴谋论方面的研究不足。与仇恨言论不同，目前没有任何数据集将阴谋论评论与其由专家精心设计的反制言论配对。我们通过评估GPT-4o、Llama 3和Mistral在这方面的能力，填补了这一空白，这些能力来自于通过结构化提示提供的心理学研究中的反制策略。结果显示，这些模型经常生成通用、重复或表面化的结果，还过度承认恐惧并频繁地虚构事实、来源或数字，使其基于提示的实际应用具有问题性。', 'title_zh': '用对话澄清谬误？探究AI生成的反驳言论挑战阴谋理论的研究'}
{'arxiv_id': 'arXiv:2504.16601', 'title': 'Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study', 'authors': 'Andy Li, Wei Zhou, Rashina Hoda, Chris Bain, Peter Poon', 'link': 'https://arxiv.org/abs/2504.16601', 'abstract': "This study evaluates how well large language models (LLMs) and traditional machine translation (MT) tools translate medical consultation summaries from English into Arabic, Chinese, and Vietnamese. It assesses both patient, friendly and clinician, focused texts using standard automated metrics. Results showed that traditional MT tools generally performed better, especially for complex texts, while LLMs showed promise, particularly in Vietnamese and Chinese, when translating simpler summaries. Arabic translations improved with complexity due to the language's morphology. Overall, while LLMs offer contextual flexibility, they remain inconsistent, and current evaluation metrics fail to capture clinical relevance. The study highlights the need for domain-specific training, improved evaluation methods, and human oversight in medical translation.", 'abstract_zh': '本研究评估了大型语言模型（LLM）和传统机器翻译（MT）工具将英语的医疗咨询总结翻译成阿拉伯语、中文和越南语的质量，使用标准自动化评估指标对面向患者和面向临床人员的文本进行了评估。结果显示，传统MT工具在复杂文本上的表现优于LLM，而LLM在翻译简单总结时特别是在越南语和中文方面表现出潜力。阿拉伯语翻译随着文本复杂性的增加而改善，这是由于阿拉伯语的形态学特性。总体而言，虽然LLM提供了上下文灵活性，但它们仍存在不一致性，当前的评估指标未能捕捉到临床相关性。该研究强调了在医疗翻译中需要进行领域特定训练、改进评估方法以及增加人工监督的重要性。', 'title_zh': '比较大型语言模型与传统机器翻译工具翻译医疗咨询摘要的效果：一项初步研究'}
{'arxiv_id': 'arXiv:2504.16584', 'title': 'Case Study: Fine-tuning Small Language Models for Accurate and Private CWE Detection in Python Code', 'authors': 'Md. Azizul Hakim Bappy, Hossen A Mustafa, Prottoy Saha, Rajinus Salehat', 'link': 'https://arxiv.org/abs/2504.16584', 'abstract': 'Large Language Models (LLMs) have demonstrated significant capabilities in understanding and analyzing code for security vulnerabilities, such as Common Weakness Enumerations (CWEs). However, their reliance on cloud infrastructure and substantial computational requirements pose challenges for analyzing sensitive or proprietary codebases due to privacy concerns and inference costs. This work explores the potential of Small Language Models (SLMs) as a viable alternative for accurate, on-premise vulnerability detection. We investigated whether a 350-million parameter pre-trained code model (codegen-mono) could be effectively fine-tuned to detect the MITRE Top 25 CWEs specifically within Python code. To facilitate this, we developed a targeted dataset of 500 examples using a semi-supervised approach involving LLM-driven synthetic data generation coupled with meticulous human review. Initial tests confirmed that the base codegen-mono model completely failed to identify CWEs in our samples. However, after applying instruction-following fine-tuning, the specialized SLM achieved remarkable performance on our test set, yielding approximately 99% accuracy, 98.08% precision, 100% recall, and a 99.04% F1-score. These results strongly suggest that fine-tuned SLMs can serve as highly accurate and efficient tools for CWE detection, offering a practical and privacy-preserving solution for integrating advanced security analysis directly into development workflows.', 'abstract_zh': '小型语言模型（SLMs）在精准、本地漏洞检测中的潜力', 'title_zh': '案例研究：针对Python代码中的准确且私有的CWE检测微调小型语言模型'}
{'arxiv_id': 'arXiv:2504.16574', 'title': 'PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression', 'authors': 'Lizhe Chen, Binjia Zhou, Yuyao Ge, Jiayi Chen, Shiguang NI', 'link': 'https://arxiv.org/abs/2504.16574', 'abstract': 'Large language models (LLMs) have achieved remarkable progress, demonstrating unprecedented capabilities across various natural language processing tasks. However, the high costs associated with such exceptional performance limit the widespread adoption of LLMs, highlighting the need for prompt compression. Existing prompt compression methods primarily rely on heuristic truncation or abstractive summarization techniques, which fundamentally overlook the intrinsic mechanisms of LLMs and lack a systematic evaluation of token importance for generation. In this work, we introduce Prompt Importance Sampling (PIS), a novel compression framework that dynamically compresses prompts by sampling important tokens based on the analysis of attention scores of hidden states. PIS employs a dual-level compression mechanism: 1) at the token level, we quantify saliency using LLM-native attention scores and implement adaptive compression through a lightweight 9-layer reinforcement learning (RL) network; 2) at the semantic level, we propose a Russian roulette sampling strategy for sentence-level importance sampling. Comprehensive evaluations across multiple domain benchmarks demonstrate that our method achieves state-of-the-art compression performance. Notably, our framework serendipitously enhances reasoning efficiency through optimized context structuring. This work advances prompt engineering by offering both theoretical grounding and practical efficiency in context management for LLMs.', 'abstract_zh': '大型语言模型（LLMs）取得了显著进展，在各种自然语言处理任务中展现了前所未有的能力。然而，与其出色的性能相伴的高昂成本限制了LLMs的广泛应用，突显了提示压缩的必要性。现有提示压缩方法主要依赖启发式截断或提取性总结技术，从根本上忽视了LLMs的内在机制，并缺乏对生成中token重要性的系统评估。在本工作中，我们提出了提示重要性采样（PIS），这是一种新颖的压缩框架，通过分析隐藏状态的注意力分数来动态压缩提示，选择重要token进行采样。PIS采用双重压缩机制：1）在token级别，使用LLM内置的注意力分数量化显著性，并通过一个轻量级的9层强化学习（RL）网络实现自适应压缩；2）在语义级别，我们提出了俄式轮盘赌采样策略进行句级重要性采样。全面的评估表明，我们的方法达到了最先进的压缩性能。值得注意的是，我们的框架通过优化上下文结构意外地提升了推理效率。本工作通过提供理论基础和上下文管理的实际效率促进了提示工程的发展。', 'title_zh': 'PIS：连接重要性采样与注意力机制以实现高效的提示压缩'}
{'arxiv_id': 'arXiv:2504.16548', 'title': 'Exploring human-SAV interaction using large language models: The impact of psychological ownership and anthropomorphism on user experience', 'authors': 'Lirui Guo, Michael G. Burke, Wynita M. Griggs', 'link': 'https://arxiv.org/abs/2504.16548', 'abstract': "There has been extensive prior work exploring how psychological factors such as anthropomorphism affect the adoption of shared autonomous vehicles (SAVs). However, limited research has been conducted on how prompt strategies in large language model (LLM)-powered SAV User Interfaces (UIs) affect users' perceptions, experiences, and intentions to adopt such technology. In this work, we investigate how conversational UIs powered by LLMs drive these psychological factors and psychological ownership, the sense of possession a user may come to feel towards an entity or object they may not legally own. We designed four SAV UIs with varying levels of anthropomorphic characteristics and psychological ownership triggers. Quantitative measures of psychological ownership, anthropomorphism, quality of service, disclosure tendency, sentiment of SAV responses, and overall acceptance were collected after participants interacted with each SAV. Qualitative feedback was also gathered regarding the experience of psychological ownership during the interactions. The results indicate that an SAV conversational UI designed to be more anthropomorphic and to induce psychological ownership improved users' perceptions of the SAV's human-like qualities and improved the sentiment of responses compared to a control condition. These findings provide practical guidance for designing LLM-based conversational UIs that enhance user experience and adoption of SAVs.", 'abstract_zh': '大规模语言模型powered共享自主车辆用户界面中提示策略如何影响用户感知、体验和采纳意图的研究：基于拟人化特征和心理占有感的对话界面设计与效果分析', 'title_zh': '使用大型语言模型探索人类与智能代理系统的互动：心理所有权和拟人化对用户体验的影响'}
{'arxiv_id': 'arXiv:2504.16489', 'title': 'Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate', 'authors': 'Senmao Qi, Yifei Zou, Peng Li, Ziyi Lin, Xiuzhen Cheng, Dongxiao Yu', 'link': 'https://arxiv.org/abs/2504.16489', 'abstract': 'Multi-Agent Debate (MAD), leveraging collaborative interactions among Large Language Models (LLMs), aim to enhance reasoning capabilities in complex tasks. However, the security implications of their iterative dialogues and role-playing characteristics, particularly susceptibility to jailbreak attacks eliciting harmful content, remain critically underexplored. This paper systematically investigates the jailbreak vulnerabilities of four prominent MAD frameworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo, and DeepSeek) without compromising internal agents. We introduce a novel structured prompt-rewriting framework specifically designed to exploit MAD dynamics via narrative encapsulation, role-driven escalation, iterative refinement, and rhetorical obfuscation. Our extensive experiments demonstrate that MAD systems are inherently more vulnerable than single-agent setups. Crucially, our proposed attack methodology significantly amplifies this fragility, increasing average harmfulness from 28.14% to 80.34% and achieving attack success rates as high as 80% in certain scenarios. These findings reveal intrinsic vulnerabilities in MAD architectures and underscore the urgent need for robust, specialized defenses prior to real-world deployment.', 'abstract_zh': '多层次语言模型辩论系统的脱监攻击脆弱性研究：基于四大主流框架的系统性探究', 'title_zh': '放大后的漏洞：基于LLM的多Agent辩论结构化 Jailbreak 攻击'}
{'arxiv_id': 'arXiv:2504.16472', 'title': 'Harden and Catch for Just-in-Time Assured LLM-Based Software Testing: Open Research Challenges', 'authors': "Mark Harman, Peter O'Hearn, Shubho Sengupta", 'link': 'https://arxiv.org/abs/2504.16472', 'abstract': "Despite decades of research and practice in automated software testing, several fundamental concepts remain ill-defined and under-explored, yet offer enormous potential real-world impact. We show that these concepts raise exciting new challenges in the context of Large Language Models for software test generation. More specifically, we formally define and investigate the properties of hardening and catching tests. A hardening test is one that seeks to protect against future regressions, while a catching test is one that catches such a regression or a fault in new functionality introduced by a code change. Hardening tests can be generated at any time and may become catching tests when a future regression is caught. We also define and motivate the Catching `Just-in-Time' (JiTTest) Challenge, in which tests are generated `just-in-time' to catch new faults before they land into production. We show that any solution to Catching JiTTest generation can also be repurposed to catch latent faults in legacy code. We enumerate possible outcomes for hardening and catching tests and JiTTests, and discuss open research problems, deployment options, and initial results from our work on automated LLM-based hardening at Meta. This paper\\footnote{Author order is alphabetical. The corresponding author is Mark Harman.} was written to accompany the keynote by the authors at the ACM International Conference on the Foundations of Software Engineering (FSE) 2025.", 'abstract_zh': '尽管在自动化软件测试方面进行了数十年的研究和实践，但仍有一些基本概念界定不清且研究不足，但具有巨大的实际影响潜力。我们展示了这些概念在大型语言模型用于软件测试生成的背景下提出了新的挑战。更具体地，我们正式定义并探讨了强化测试和捕捉测试的属性。强化测试旨在防止未来的回归，而捕捉测试则是捕获重构或新功能引入的代码变更中出现的回归或故障。强化测试可以在任何时候生成，当未来回归被捕获时，它们可能成为捕捉测试。我们还定义并提出了捕捉即时生成（Catching Just-in-Time，JiTTest）挑战，在此挑战中，测试在新故障进入生产之前即时生成。我们证明，任何解决JiTTest生成问题的方法也可以用于捕获遗留代码中的潜伏故障。我们列出了强化测试、捕捉测试和JiTTest可能的结果，并讨论了开放的研究问题、部署选项以及我们在Meta使用自动化大型语言模型进行强化测试的初步结果。本文作者的文章将在2025年ACM国际软件工程基础会议（FSE）上作者的主旨演讲中发表。', 'title_zh': 'Just-in-Time Assured LLM-Based Software Testing: 坚韧与捕获在即时保证的软件测试中的应用：开放的研究挑战'}
{'arxiv_id': 'arXiv:2504.16460', 'title': 'T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic Understanding via Deep Triplet Loss Fine-Tuning', 'authors': 'Vignesh Ethiraj, Sidhanth Menon, Divya Vijay', 'link': 'https://arxiv.org/abs/2504.16460', 'abstract': 'The specialized vocabulary and complex concepts of the telecommunications industry present significant challenges for standard Natural Language Processing models. Generic text embeddings often fail to capture telecom-specific semantics, hindering downstream task performance. We introduce T-VEC (Telecom Vectorization Model), a novel embedding model tailored for the telecom domain through deep fine-tuning. Developed by NetoAI, T-VEC is created by adapting the state-of-the-art gte-Qwen2-1.5B-instruct model using a triplet loss objective on a meticulously curated, large-scale dataset of telecom-specific data. Crucially, this process involved substantial modification of weights across 338 layers of the base model, ensuring deep integration of domain knowledge, far exceeding superficial adaptation techniques. We quantify this deep change via weight difference analysis. A key contribution is the development and open-sourcing (MIT License) of the first dedicated telecom-specific tokenizer, enhancing the handling of industry jargon. T-VEC achieves a leading average MTEB score (0.825) compared to established models and demonstrates vastly superior performance (0.9380 vs. less than 0.07) on our internal telecom-specific triplet evaluation benchmark, indicating an exceptional grasp of domain-specific nuances, visually confirmed by improved embedding separation. This work positions NetoAI at the forefront of telecom AI innovation, providing the community with a powerful, deeply adapted, open-source tool.', 'abstract_zh': '电信领域的专用词汇和复杂概念给标准自然语言处理模型带来了显著挑战。通用文本嵌入往往无法捕捉电信特定的语义，影响下游任务性能。我们介绍了T-VEC（电信向量化模型），这是一种通过深度微调为电信领域量身定制的新嵌入模型。由NetoAI开发的T-VEC通过在精心策划的大型电信特定数据集上使用三重损失目标适应了最新的gte-Qwen2-1.5B-instruct模型。这个过程涉及对基模型338层的权重进行重大修改，确保了领域知识的深度集成，远超表面适应技术。我们通过权重差异分析量化了这种深度变化。一个关键贡献是开发并公开发布了首个专门针对电信领域的分词器（MIT许可证），增强了对行业术语的处理能力。T-VEC在MTEB评分上取得了领先平均得分（0.825），并在我们的内部电信特定三重评估基准测试中展现出了远超现有模型的性能（0.9380 vs. 小于0.07），显示了对领域特定细微差别的出色掌握，可视化分析进一步证实了这一点。这项工作将NetoAI定位为电信AI创新的前沿地带，为社区提供了强大且深度定制的开源工具。', 'title_zh': 'T-VEC：一种通过深度三重损失微调以增强语义理解的电信专用向量化模型'}
{'arxiv_id': 'arXiv:2504.16448', 'title': 'EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records', 'authors': 'Shuguang Zhao, Qiangzhong Feng, Zhiyang He, Peipei Sun, Yingying Wang, Xiaodong Tao, Xiaoliang Lu, Mei Cheng, Xinyue Wu, Yanyan Wang, Wei Liang', 'link': 'https://arxiv.org/abs/2504.16448', 'abstract': 'Medical consultation dialogues contain critical clinical information, yet their unstructured nature hinders effective utilization in diagnosis and treatment. Traditional methods, relying on rule-based or shallow machine learning techniques, struggle to capture deep and implicit semantics. Recently, large pre-trained language models and Low-Rank Adaptation (LoRA), a lightweight fine-tuning method, have shown promise for structured information extraction. We propose EMRModel, a novel approach that integrates LoRA-based fine-tuning with code-style prompt design, aiming to efficiently convert medical consultation dialogues into structured electronic medical records (EMRs). Additionally, we construct a high-quality, realistically grounded dataset of medical consultation dialogues with detailed annotations. Furthermore, we introduce a fine-grained evaluation benchmark for medical consultation information extraction and provide a systematic evaluation methodology, advancing the optimization of medical natural language processing (NLP) models. Experimental results show EMRModel achieves an F1 score of 88.1%, improving by49.5% over standard pre-trained models. Compared to traditional LoRA fine-tuning methods, our model shows superior performance, highlighting its effectiveness in structured medical record extraction tasks.', 'abstract_zh': '医疗咨询对话包含关键临床信息，但由于其非结构化特性，在诊断和治疗中的有效利用受到阻碍。传统方法依赖于基于规则或浅层机器学习技术，难以捕捉深层次和隐含的语义。近年来，大型预训练语言模型和基于低秩适应（LoRA）的轻量化微调方法显示出提取结构化信息的潜力。我们提出了一种名为EMRModel的新方法，该方法结合了基于LoRA的微调与代码风格的提示设计，旨在高效地将医疗咨询对话转换为结构化的电子医疗记录（EMRs）。此外，我们构建了一个高质量、具有实际基础的医疗咨询对话数据集，附有详细的注释。进一步地，我们引入了针对医疗咨询信息抽取的细粒度评估基准，并提供了系统的评估方法，推动了医疗自然语言处理（NLP）模型的优化。实验结果显示，EMRModel的F1得分为88.1%，相较于标准预训练模型提高了49.5%。与传统的LoRA微调方法相比，我们的模型显示出更优的性能，强调其在结构化医疗记录提取任务中的有效性。', 'title_zh': 'EMRModel: 一个将医疗咨询对话提取为结构化医疗记录的大语言模型'}
{'arxiv_id': 'arXiv:2504.16394', 'title': 'ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs', 'authors': 'Fahmida Liza Piya, Rahmatollah Beheshti', 'link': 'https://arxiv.org/abs/2504.16394', 'abstract': 'Unstructured clinical data can serve as a unique and rich source of information that can meaningfully inform clinical practice. Extracting the most pertinent context from such data is critical for exploiting its true potential toward optimal and timely decision-making in patient care. While prior research has explored various methods for clinical text summarization, most prior studies either process all input tokens uniformly or rely on heuristic-based filters, which can overlook nuanced clinical cues and fail to prioritize information critical for decision-making. In this study, we propose Contextual, a novel framework that integrates a Context-Preserving Token Filtering method with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By preserving context-specific important tokens and enriching them with structured knowledge, ConTextual improves both linguistic coherence and clinical fidelity. Our extensive empirical evaluations on two public benchmark datasets demonstrate that ConTextual consistently outperforms other baselines. Our proposed approach highlights the complementary role of token-level filtering and structured retrieval in enhancing both linguistic and clinical integrity, as well as offering a scalable solution for improving precision in clinical text generation.', 'abstract_zh': '无结构化临床数据可以作为独特且丰富的信息来源，对临床实践具有重要意义。从此类数据中提取最相关的上下文对于充分发挥其潜力、实现最优和及时的决策至关重要。尽管先前研究探索了各种临床文本总结方法，但大多数前期研究要么以统一方式处理所有输入词，要么依赖于基于启发式的过滤器，这些方法可能会忽略精细的临床线索，无法优先处理对决策至关重要的信息。在本研究中，我们提出了一种名为Contextual的新框架，该框架结合了上下文保留的 token 过滤方法和领域特定知识图谱（KG）进行上下文增强。通过保留上下文特定的重要 token 并通过结构化知识进行丰富，ConTextual提高了语言连贯性和临床准确性。我们在两个公开基准数据集上的广泛实证评估结果显示，ConTextual始终优于其他基线方法。我们提出的方法突显了 token 级别过滤和结构化检索在增强语言和临床完整性方面的作用，并提供了提高临床文本生成精确性的可扩展解决方案。', 'title_zh': 'ConTextual: 在上下文保留的 Tokens 过滤和知识图谱基础上提升临床文本总结的大模型性能'}
{'arxiv_id': 'arXiv:2504.16144', 'title': 'Detecting Actionable Requests and Offers on Social Media During Crises Using LLMs', 'authors': 'Ahmed El Fekih Zguir, Ferda Ofli, Muhammad Imran', 'link': 'https://arxiv.org/abs/2504.16144', 'abstract': "Natural disasters often result in a surge of social media activity, including requests for assistance, offers of help, sentiments, and general updates. To enable humanitarian organizations to respond more efficiently, we propose a fine-grained hierarchical taxonomy to systematically organize crisis-related information about requests and offers into three critical dimensions: supplies, emergency personnel, and actions. Leveraging the capabilities of Large Language Models (LLMs), we introduce Query-Specific Few-shot Learning (QSF Learning) that retrieves class-specific labeled examples from an embedding database to enhance the model's performance in detecting and classifying posts. Beyond classification, we assess the actionability of messages to prioritize posts requiring immediate attention. Extensive experiments demonstrate that our approach outperforms baseline prompting strategies, effectively identifying and prioritizing actionable requests and offers.", 'abstract_zh': '自然灾害往往会导致社交媒体活动激增，包括求助请求、帮助提议、情绪表达和一般更新。为了使人道主义组织能够更有效地响应，我们提出一种精细层次的-taxonomy分类体系，系统地将危机相关的求助和帮助信息组织到三个关键维度中：物资、应急管理人力和行动。利用大型语言模型的能力，我们引入了查询特定的小样本学习（QSF学习），从嵌入数据库中检索特定类别的标注示例，以增强模型在检测和分类帖子方面的性能。除了分类之外，我们还评估了消息的实际操作性，以优先处理需要立即关注的帖子。 extensive实验表明，我们的方法在识别和优先处理可操作的求助和帮助方面优于基线提示策略。', 'title_zh': '使用大语言模型在危机期间检测社交媒体上的可行动请求和报价'}
{'arxiv_id': 'arXiv:2504.16120', 'title': 'A Data-Centric Approach for Safe and Secure Large Language Models against Threatening and Toxic Content', 'authors': 'Chaima Njeh, Haïfa Nakouri, Fehmi Jaafar', 'link': 'https://arxiv.org/abs/2504.16120', 'abstract': "Large Language Models (LLM) have made remarkable progress, but concerns about potential biases and harmful content persist. To address these apprehensions, we introduce a practical solution for ensuring LLM's safe and ethical use. Our novel approach focuses on a post-generation correction mechanism, the BART-Corrective Model, which adjusts generated content to ensure safety and security. Unlike relying solely on model fine-tuning or prompt engineering, our method provides a robust data-centric alternative for mitigating harmful content. We demonstrate the effectiveness of our approach through experiments on multiple toxic datasets, which show a significant reduction in mean toxicity and jail-breaking scores after integration. Specifically, our results show a reduction of 15% and 21% in mean toxicity and jail-breaking scores with GPT-4, a substantial reduction of 28% and 5% with PaLM2, a reduction of approximately 26% and 23% with Mistral-7B, and a reduction of 11.1% and 19% with Gemma-2b-it. These results demonstrate the potential of our approach to improve the safety and security of LLM, making them more suitable for real-world applications.", 'abstract_zh': '大型语言模型（LLM）取得了显著进步，但对其潜在偏见和有害内容的担忧依然存在。为应对这些担忧，我们提出了一种实用解决方案，确保LLM的安全和负责任使用。我们的新方法侧重于一种后生成修正机制——BART-Corrective模型，该模型调整生成的内容以确保安全和安全性。不同于仅依赖模型微调或提示工程，我们的方法提供了 robust 数据为中心的替代方案，以减少有害内容。通过在多个有毒数据集上的实验，我们证明了该方法的有效性，结果显示集成后显著降低了平均毒性和越狱评分。具体而言，我们的结果表明，与GPT-4相比，平均毒性和越狱评分分别减少了15%和21%；与PaLM2相比，这两项分别减少了28%和5%；与Mistral-7B相比，平均毒性和越狱评分分别减少了约26%和23%；与Gemma-2b-it相比，这两项分别减少了11.1%和19%。这些结果展示了我们方法提高LLM安全性和安全性的潜力，使它们更适合实际应用。', 'title_zh': '一种针对威胁性和有毒内容的安全可靠的大规模语言模型以数据为中心的方法'}
{'arxiv_id': 'arXiv:2504.16116', 'title': 'DMind Benchmark: The First Comprehensive Benchmark for LLM Evaluation in the Web3 Domain', 'authors': 'Miracle Master, Rainy Sun, Anya Reese, Joey Ouyang, Alex Chen, Winter Dong, Frank Li, James Yi, Garry Zhao, Tony Ling, Hobert Wong, Lowes Yang', 'link': 'https://arxiv.org/abs/2504.16116', 'abstract': 'Recent advances in Large Language Models (LLMs) have led to significant progress on a wide range of natural language processing tasks. However, their effectiveness in specialized and rapidly evolving domains such as Web3 remains underexplored. In this paper, we introduce DMind Benchmark, a novel framework that systematically tests LLMs across nine key categories encompassing blockchain fundamentals, infrastructure, smart contract analysis, decentralized finance (DeFi), decentralized autonomous organizations (DAOs), non-fungible tokens (NFTs), token economics, meme concepts, and security vulnerabilities.\nDMind Benchmark goes beyond conventional multiple-choice questions by incorporating domain-specific subjective tasks (e.g., smart contract code auditing and repair, numeric reasoning on on-chain data, and fill-in assessments), thereby capturing real-world complexities and stress-testing model adaptability. We evaluate fifteen popular LLMs (from ChatGPT, DeepSeek, Claude, and Gemini series) on DMind Benchmark, uncovering performance gaps in Web3-specific reasoning and application, particularly in emerging areas like token economics and meme concepts. Even the strongest models face significant challenges in identifying subtle security vulnerabilities and analyzing complex DeFi mechanisms. To foster progress in this area, we publicly release our benchmark dataset, evaluation pipeline, and annotated results at this http URL, offering a valuable resource for advancing specialized domain adaptation and the development of more robust Web3-enabled LLMs.', 'abstract_zh': '最近大型语言模型（LLMs）的进展在多种自然语言处理任务中取得了显著进步。然而，在如Web3这样的专业且快速演化的领域，其有效性仍待深入探索。本文引入了DMind基准框架，这是一个系统测试LLMs的新型框架，涵盖了区块链基础、基础设施、智能合约分析、去中心化金融（DeFi）、去中心化自治组织（DAOs）、不可替代代币（NFTs）、代币经济、 meme概念和安全漏洞等九大关键类别。', 'title_zh': 'DMind基准：Web3领域首个全面的LLM评估基准'}
{'arxiv_id': 'arXiv:2504.16112', 'title': 'HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing', 'authors': 'Myunghyun Rhee, Joonseop Sim, Taeyoung Ahn, Seungyong Lee, Daegun Yoon, Euiseok Kim, Kyoung Park, Youngpyo Joo, Hosik Kim', 'link': 'https://arxiv.org/abs/2504.16112', 'abstract': 'The attention layer, a core component of Transformer-based LLMs, brings out inefficiencies in current GPU systems due to its low operational intensity and the substantial memory requirements of KV caches. We propose a High-bandwidth Processing Unit (HPU), a memoryintensive co-processor that enhances GPU resource utilization during large-batched LLM inference. By offloading memory-bound operations, the HPU allows the GPU to focus on compute-intensive tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales out to accommodate surging memory demands driven by large batch sizes and extended sequence lengths. In this paper, we show the HPU prototype implemented with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy efficiency improvements over a GPUonly system, providing scalability without increasing the number of GPUs.', 'abstract_zh': '基于Transformer的大语言模型注意力层在当前GPU系统中存在效率低下问题，主要是由于其低运算强度和大量KV缓存的内存需求。我们提出了一种高带宽处理单元（HPU），这是一种内存密集型协处理器，能够在大规模批次的大语言模型推理过程中提高GPU资源利用率。通过卸载内存受限操作，HPU使GPU能够专注于计算密集型任务，从而提高整体效率。此外，作为附加卡，HPU可扩展以应对由大批次大小和长序列长度驱动的内存需求激增。本文展示了使用基于PCIe的FPGA卡实现的HPU原型，安装在GPU系统上。我们的新型GPU-HPU异构系统在仅GPU系统上实现了高达4.1倍的性能提升和4.6倍的能量效率提升，提供了无须增加GPU数量的扩展能力。', 'title_zh': '高带宽处理单元：通过GPU协处理实现可扩展、成本效益高的LLM推理'}
