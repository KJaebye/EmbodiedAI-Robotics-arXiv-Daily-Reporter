{'arxiv_id': 'arXiv:2504.16406', 'title': 'Long Exposure Localization in Darkness Using Consumer Cameras', 'authors': 'Michael Milford, Ian Turner, Peter Corke', 'link': 'https://arxiv.org/abs/2504.16406', 'abstract': 'In this paper we evaluate performance of the SeqSLAM algorithm for passive vision-based localization in very dark environments with low-cost cameras that result in massively blurred images. We evaluate the effect of motion blur from exposure times up to 10,000 ms from a moving car, and the performance of localization in day time from routes learned at night in two different environments. Finally we perform a statistical analysis that compares the baseline performance of matching unprocessed grayscale images to using patch normalization and local neighborhood normalization - the two key SeqSLAM components. Our results and analysis show for the first time why the SeqSLAM algorithm is effective, and demonstrate the potential for cheap camera-based localization systems that function despite extreme appearance change.', 'abstract_zh': '本文评估了SeqSLAM算法在极暗环境中使用低成本相机进行被动视觉定位的表现，这些相机产生的图像极度模糊。我们评估了移动汽车在曝光时间长达10,000毫秒时的运动模糊对定位效果的影响，并研究了在夜间学习路径后白天定位的性能在两种不同环境中的表现。最后，我们进行了一项统计分析，比较了直接匹配未处理的灰度图像与使用局部补丁规范化和局部邻域规范化之间的基线性能——SeqSLAM算法的两个关键组件。我们的结果和分析首次揭示了SeqSLAM算法为何有效，并展示了尽管存在极端的外观变化，低成本相机定位系统仍有应用潜力。', 'title_zh': '在黑暗中使用消费级相机进行长时间曝光定位'}
{'arxiv_id': 'arXiv:2504.16377', 'title': 'SILM: A Subjective Intent Based Low-Latency Framework for Multiple Traffic Participants Joint Trajectory Prediction', 'authors': 'Qu Weiming, Wang Jia, Du Jiawei, Zhu Yuanhao, Yu Jianfeng, Xia Rui, Cao Song, Wu Xihong, Luo Dingsheng', 'link': 'https://arxiv.org/abs/2504.16377', 'abstract': 'Trajectory prediction is a fundamental technology for advanced autonomous driving systems and represents one of the most challenging problems in the field of cognitive intelligence. Accurately predicting the future trajectories of each traffic participant is a prerequisite for building high safety and high reliability decision-making, planning, and control capabilities in autonomous driving. However, existing methods often focus solely on the motion of other traffic participants without considering the underlying intent behind that motion, which increases the uncertainty in trajectory prediction. Autonomous vehicles operate in real-time environments, meaning that trajectory prediction algorithms must be able to process data and generate predictions in real-time. While many existing methods achieve high accuracy, they often struggle to effectively handle heterogeneous traffic scenarios. In this paper, we propose a Subjective Intent-based Low-latency framework for Multiple traffic participants joint trajectory prediction. Our method explicitly incorporates the subjective intent of traffic participants based on their key points, and predicts the future trajectories jointly without map, which ensures promising performance while significantly reducing the prediction latency. Additionally, we introduce a novel dataset designed specifically for trajectory prediction. Related code and dataset will be available soon.', 'abstract_zh': '基于主体意图的低延迟多交通参与者联合轨迹预测框架', 'title_zh': 'SILM：一种基于主观意图的低延迟多交通参与者联合轨迹预测框架'}
{'arxiv_id': 'arXiv:2504.16374', 'title': 'DPGP: A Hybrid 2D-3D Dual Path Potential Ghost Probe Zone Prediction Framework for Safe Autonomous Driving', 'authors': 'Weiming Qu, Jiawei Du, Shenghai Yuan, Jia Wang, Yang Sun, Shengyi Liu, Yuanhao Zhu, Jianfeng Yu, Song Cao, Rui Xia, Xiaoyu Tang, Xihong Wu, Dingsheng Luo', 'link': 'https://arxiv.org/abs/2504.16374', 'abstract': 'Modern robots must coexist with humans in dense urban environments. A key challenge is the ghost probe problem, where pedestrians or objects unexpectedly rush into traffic paths. This issue affects both autonomous vehicles and human drivers. Existing works propose vehicle-to-everything (V2X) strategies and non-line-of-sight (NLOS) imaging for ghost probe zone detection. However, most require high computational power or specialized hardware, limiting real-world feasibility. Additionally, many methods do not explicitly address this issue. To tackle this, we propose DPGP, a hybrid 2D-3D fusion framework for ghost probe zone prediction using only a monocular camera during training and inference. With unsupervised depth prediction, we observe ghost probe zones align with depth discontinuities, but different depth representations offer varying robustness. To exploit this, we fuse multiple feature embeddings to improve prediction. To validate our approach, we created a 12K-image dataset annotated with ghost probe zones, carefully sourced and cross-checked for accuracy. Experimental results show our framework outperforms existing methods while remaining cost-effective. To our knowledge, this is the first work extending ghost probe zone prediction beyond vehicles, addressing diverse non-vehicle objects. We will open-source our code and dataset for community benefit.', 'abstract_zh': '现代机器人必须在稠密的城市环境中与人类共存。一个关键挑战是幽灵探测问题，其中行人或物体意外冲入交通路径。这个问题影响自动驾驶车辆和人类驾驶员。现有工作提出了车对外界通信（V2X）策略和非视线（NLOS）成像方法用于幽灵探测区检测。然而，大多数方法需要高计算能力或专用硬件，限制了其实用性。此外，许多方法并没有明确解决这一问题。为了解决这一问题，我们提出了一种基于单一摄像头的混合2D-3D融合框架DPGP，用于幽灵探测区预测。通过无监督深度预测，我们发现幽灵探测区与深度不连续处对齐，但不同的深度表示具有不同的鲁棒性。为充分利用这一点，我们融合多个特征嵌入以改进预测。为了验证我们的方法，我们创建了一个包含12000张标注有幽灵探测区的图像数据集，精心选择并交叉核对以确保准确性。实验结果表明，我们的框架在保持成本效益的同时优于现有方法。据我们所知，这是首项将幽灵探测区预测扩展到车辆之外，解决多样化非车辆对象的工作。我们将开源我们的代码和数据集以惠及社区。', 'title_zh': 'DPGP：一种用于安全自动驾驶的二维-三维双路径潜在鬼探头区域预测混合框架'}
{'arxiv_id': 'arXiv:2504.16320', 'title': 'PCF-Grasp: Converting Point Completion to Geometry Feature to Enhance 6-DoF Grasp', 'authors': 'Yaofeng Cheng, Fusheng Zha, Wei Guo, Pengfei Wang, Chao Zeng, Lining Sun, Chenguang Yang', 'link': 'https://arxiv.org/abs/2504.16320', 'abstract': 'The 6-Degree of Freedom (DoF) grasp method based on point clouds has shown significant potential in enabling robots to grasp target objects. However, most existing methods are based on the point clouds (2.5D points) generated from single-view depth images. These point clouds only have one surface side of the object providing incomplete geometry information, which mislead the grasping algorithm to judge the shape of the target object, resulting in low grasping accuracy. Humans can accurately grasp objects from a single view by leveraging their geometry experience to estimate object shapes. Inspired by humans, we propose a novel 6-DoF grasping framework that converts the point completion results as object shape features to train the 6-DoF grasp network. Here, point completion can generate approximate complete points from the 2.5D points similar to the human geometry experience, and converting it as shape features is the way to utilize it to improve grasp efficiency. Furthermore, due to the gap between the network generation and actual execution, we integrate a score filter into our framework to select more executable grasp proposals for the real robot. This enables our method to maintain a high grasp quality in any camera viewpoint. Extensive experiments demonstrate that utilizing complete point features enables the generation of significantly more accurate grasp proposals and the inclusion of a score filter greatly enhances the credibility of real-world robot grasping. Our method achieves a 17.8\\% success rate higher than the state-of-the-art method in real-world experiments.', 'abstract_zh': '基于点云的6自由度抓取方法在使机器人抓取目标物体方面展现出了显著潜力。然而，现有大多数方法基于单视角深度图像生成的2.5D点云。这些点云仅提供物体一面的几何信息，导致抓取算法错误判断目标物体的形状，从而降低了抓取准确性。受到人类单视角准确抓取物体的经验启发，我们提出了一种新颖的6自由度抓取框架，通过将点云补全结果转换为物体形状特征来训练6自由度抓取网络。点云补全可以产生与人类几何经验类似的近似完整点，将其转换为形状特征是利用其提升抓取效率的一种方式。此外，由于网络生成与实际执行之间的差距，我们在框架中整合了一个评分滤波器，以选择更适合实际机器人执行的抓取方案。这使得我们方法在任何相机视角下都能维持较高的抓取质量。大量实验表明，利用完整点特征生成的抓取提案更加准确，同时加入评分滤波器极大地提升了实际机器人抓取的可信度。在实际实验中，我们的方法实现了比现有最佳方法高出17.8%的成功率。', 'title_zh': 'PCF-抓取：将点完成转换为几何特征以增强6自由度抓取'}
{'arxiv_id': 'arXiv:2504.16183', 'title': 'Measuring Uncertainty in Shape Completion to Improve Grasp Quality', 'authors': 'Nuno Ferreira Duarte, Seyed S. Mohammadi, Plinio Moreno, Alessio Del Bue, Jose Santos-Victor', 'link': 'https://arxiv.org/abs/2504.16183', 'abstract': 'Shape completion networks have been used recently in real-world robotic experiments to complete the missing/hidden information in environments where objects are only observed in one or few instances where self-occlusions are bound to occur. Nowadays, most approaches rely on deep neural networks that handle rich 3D point cloud data that lead to more precise and realistic object geometries. However, these models still suffer from inaccuracies due to its nondeterministic/stochastic inferences which could lead to poor performance in grasping scenarios where these errors compound to unsuccessful grasps. We present an approach to calculate the uncertainty of a 3D shape completion model during inference of single view point clouds of an object on a table top. In addition, we propose an update to grasp pose algorithms quality score by introducing the uncertainty of the completed point cloud present in the grasp candidates. To test our full pipeline we perform real world grasping with a 7dof robotic arm with a 2 finger gripper on a large set of household objects and compare against previous approaches that do not measure uncertainty. Our approach ranks the grasp quality better, leading to higher grasp success rate for the rank 5 grasp candidates compared to state of the art.', 'abstract_zh': '基于单视角点云的3D形状完成模型的不确定性评估及抓取质量分数更新方法', 'title_zh': '基于形状补全测量不确定性以提高抓取质量'}
{'arxiv_id': 'arXiv:2504.16103', 'title': 'Shape Your Ground: Refining Road Surfaces Beyond Planar Representations', 'authors': 'Oussema Dhaouadi, Johannes Meier, Jacques Kaiser, Daniel Cremers', 'link': 'https://arxiv.org/abs/2504.16103', 'abstract': 'Road surface reconstruction from aerial images is fundamental for autonomous driving, urban planning, and virtual simulation, where smoothness, compactness, and accuracy are critical quality factors. Existing reconstruction methods often produce artifacts and inconsistencies that limit usability, while downstream tasks have a tendency to represent roads as planes for simplicity but at the cost of accuracy. We introduce FlexRoad, the first framework to directly address road surface smoothing by fitting Non-Uniform Rational B-Splines (NURBS) surfaces to 3D road points obtained from photogrammetric reconstructions or geodata providers. Our method at its core utilizes the Elevation-Constrained Spatial Road Clustering (ECSRC) algorithm for robust anomaly correction, significantly reducing surface roughness and fitting errors. To facilitate quantitative comparison between road surface reconstruction methods, we present GeoRoad Dataset (GeRoD), a diverse collection of road surface and terrain profiles derived from openly accessible geodata. Experiments on GeRoD and the photogrammetry-based DeepScenario Open 3D Dataset (DSC3D) demonstrate that FlexRoad considerably surpasses commonly used road surface representations across various metrics while being insensitive to various input sources, terrains, and noise types. By performing ablation studies, we identify the key role of each component towards high-quality reconstruction performance, making FlexRoad a generic method for realistic road surface modeling.', 'abstract_zh': '基于航拍图像的道路表面重建对于自动驾驶、城市规划和虚拟 simulation 至关重要，平滑度、紧凑性和准确性是关键质量因素。现有的重建方法常常产生伪影和不一致性，限制了其可用性，而下游任务倾向于简化地将道路表示为平面以提高效率，但会牺牲准确性。我们引入了 FlexRoad，这是首个通过使用非均匀有理B样条（NURBS）表面拟合三维道路点来直接解决道路表面平滑问题的框架，这些点来自光达重建或地理数据服务商。该方法的核心采用了高斯约束空间道路聚类（ECSRC）算法进行稳健的异常值修正，显著减少了表面粗糙度和拟合误差。为了便于道路表面重建方法的定量比较，我们提出了 GeoRoad 数据集（GeRoD），该数据集包含来自开放获取地理数据的地表和地形剖面。实验表明，FlexRoad 在各种指标上显著超越了常用的道路表面表示方法，且对不同输入源、地形和噪声类型不敏感。通过进行消融研究，我们确定了每个组件在高质量重建性能中的关键作用，使 FlexRoad 成为一种通用的道路表面建模方法。', 'title_zh': '塑造地面形态：超越平面表示的道路表面精Refining 改进'}
{'arxiv_id': 'arXiv:2504.16922', 'title': 'Generalized Neighborhood Attention: Multi-dimensional Sparse Attention at the Speed of Light', 'authors': 'Ali Hassani, Fengzhe Zhou, Aditya Kane, Jiannan Huang, Chieh-Yun Chen, Min Shi, Steven Walton, Markus Hoehnerbach, Vijay Thakkar, Michael Isaev, Qinsheng Zhang, Bing Xu, Haicheng Wu, Wen-mei Hwu, Ming-Yu Liu, Humphrey Shi', 'link': 'https://arxiv.org/abs/2504.16922', 'abstract': 'Many sparse attention mechanisms such as Neighborhood Attention have typically failed to consistently deliver speedup over the self attention baseline. This is largely due to the level of complexity in attention infrastructure, and the rapid evolution of AI hardware architecture. At the same time, many state-of-the-art foundational models, particularly in computer vision, are heavily bound by attention, and need reliable sparsity to escape the O(n^2) complexity. In this paper, we study a class of promising sparse attention mechanisms that focus on locality, and aim to develop a better analytical model of their performance improvements. We first introduce Generalized Neighborhood Attention (GNA), which can describe sliding window, strided sliding window, and blocked attention. We then consider possible design choices in implementing these approaches, and create a simulator that can provide much more realistic speedup upper bounds for any given setting. Finally, we implement GNA on top of a state-of-the-art fused multi-headed attention (FMHA) kernel designed for the NVIDIA Blackwell architecture in CUTLASS. Our implementation can fully realize the maximum speedup theoretically possible in many perfectly block-sparse cases, and achieves an effective utilization of 1.3 petaFLOPs/second in FP16. In addition, we plug various GNA configurations into off-the-shelf generative models, such as Cosmos-7B, HunyuanVideo, and FLUX, and show that it can deliver 28% to 46% end-to-end speedup on B200 without any fine-tuning. We will open source our simulator and Blackwell kernels directly through the NATTEN project.', 'abstract_zh': '一种新的局部稀疏注意力机制及其性能分析与实现：以Generalized Neighborhood Attention为例', 'title_zh': '广义邻域注意力：光速下的多维稀疏注意力'}
{'arxiv_id': 'arXiv:2504.16722', 'title': 'PMG: Progressive Motion Generation via Sparse Anchor Postures Curriculum Learning', 'authors': 'Yingjie Xi, Jian Jun Zhang, Xiaosong Yang', 'link': 'https://arxiv.org/abs/2504.16722', 'abstract': 'In computer animation, game design, and human-computer interaction, synthesizing human motion that aligns with user intent remains a significant challenge. Existing methods have notable limitations: textual approaches offer high-level semantic guidance but struggle to describe complex actions accurately; trajectory-based techniques provide intuitive global motion direction yet often fall short in generating precise or customized character movements; and anchor poses-guided methods are typically confined to synthesize only simple motion patterns. To generate more controllable and precise human motions, we propose \\textbf{ProMoGen (Progressive Motion Generation)}, a novel framework that integrates trajectory guidance with sparse anchor motion control. Global trajectories ensure consistency in spatial direction and displacement, while sparse anchor motions only deliver precise action guidance without displacement. This decoupling enables independent refinement of both aspects, resulting in a more controllable, high-fidelity, and sophisticated motion synthesis. ProMoGen supports both dual and single control paradigms within a unified training process. Moreover, we recognize that direct learning from sparse motions is inherently unstable, we introduce \\textbf{SAP-CL (Sparse Anchor Posture Curriculum Learning)}, a curriculum learning strategy that progressively adjusts the number of anchors used for guidance, thereby enabling more precise and stable convergence. Extensive experiments demonstrate that ProMoGen excels in synthesizing vivid and diverse motions guided by predefined trajectory and arbitrary anchor frames. Our approach seamlessly integrates personalized motion with structured guidance, significantly outperforming state-of-the-art methods across multiple control scenarios.', 'abstract_zh': '面向用户意图的渐进式motion生成：轨迹引导与稀疏关键动作控制', 'title_zh': 'PMG: 基于稀疏锚姿势 curriculum 学习的渐进运动生成'}
{'arxiv_id': 'arXiv:2504.16562', 'title': 'A Vision for AI-Driven Adaptation of Dynamic AR Content to Users and Environments', 'authors': 'Julian Rasch, Florian Müller, Francesco Chiossi', 'link': 'https://arxiv.org/abs/2504.16562', 'abstract': "Augmented Reality (AR) is transforming the way we interact with virtual information in the physical world. By overlaying digital content in real-world environments, AR enables new forms of immersive and engaging experiences. However, existing AR systems often struggle to effectively manage the many interactive possibilities that AR presents. This vision paper speculates on AI-driven approaches for adaptive AR content placement, dynamically adjusting to user movement and environmental changes. By leveraging machine learning methods, such a system would intelligently manage content distribution between AR projections integrated into the external environment and fixed static content, enabling seamless UI layout and potentially reducing users' cognitive load. By exploring the possibilities of AI-driven dynamic AR content placement, we aim to envision new opportunities for innovation and improvement in various industries, from urban navigation and workplace productivity to immersive learning and beyond. This paper outlines a vision for the development of more intuitive, engaging, and effective AI-powered AR experiences.", 'abstract_zh': 'augmented reality (ar) 是以全新方式在物理世界中与虚拟信息交互的革命。通过在现实世界环境中叠加数字内容，ar 启用了新的沉浸式和互动体验形式。然而，现有的 ar 系统经常难以有效地管理 ar 呈现的众多交互可能性。本文推测了基于人工智能的方法，以实现自适应的 ar 内容放置，动态适应用户运动和环境变化。通过利用机器学习方法，这样的系统将智能地管理 ar 投影与外部环境中的固定静态内容之间的内容分布，从而实现无感的 ui 布局，并可能减轻用户的认知负荷。通过探索基于 ai 的动态 ar 内容放置的可能性，我们旨在构想在各个行业，从城市导航和工作效率提升到沉浸式学习等领域的创新和改进机会。本文勾勒了一种更具直觉性、互动性和有效性的人工智能增强现实体验的发展愿景。', 'title_zh': 'AI驱动的动态AR内容适应用户和环境的愿景'}
{'arxiv_id': 'arXiv:2504.16404', 'title': 'Assessing the Feasibility of Internet-Sourced Video for Automatic Cattle Lameness Detection', 'authors': 'Md Fahimuzzman Sohan', 'link': 'https://arxiv.org/abs/2504.16404', 'abstract': 'Cattle lameness is often caused by hoof injuries or interdigital dermatitis, leads to pain and significantly impacts essential physiological activities such as walking, feeding, and drinking. This study presents a deep learning-based model for detecting cattle lameness, sickness, or gait abnormalities using publicly available video data. The dataset consists of 50 unique videos from 40 individual cattle, recorded from various angles in both indoor and outdoor environments. Half of the dataset represents naturally walking (normal/non-lame) cattle, while the other half consists of cattle exhibiting gait abnormalities (lame). To enhance model robustness and generalizability, data augmentation was applied to the training data. The pre-processed videos were then classified using two deep learning models: ConvLSTM2D and 3D CNN. A comparative analysis of the results demonstrates strong classification performance. Specifically, the 3D CNN model achieved a video-level classification accuracy of 90%, with precision, recall, and f1-score of 90.9%, 90.9%, and 90.91% respectively. The ConvLSTM2D model exhibited a slightly lower accuracy of 85%. This study highlights the effectiveness of directly applying classification models to learn spatiotemporal features from video data, offering an alternative to traditional multi-stage approaches that typically involve object detection, pose estimation, and feature extraction. Besides, the findings demonstrate that the proposed deep learning models, particularly the 3D CNN, effectively classify and detect lameness in cattle while simplifying the processing pipeline.', 'abstract_zh': '基于深度学习的牛只跛行、疾病或行态异常检测模型研究', 'title_zh': '评估互联网来源视频在自动检测奶牛跛行中的可行性'}
{'arxiv_id': 'arXiv:2504.16171', 'title': 'A detection-task-specific deep-learning method to improve the quality of sparse-view myocardial perfusion SPECT images', 'authors': 'Zezhang Yang, Zitong Yu, Nuri Choi, Abhinav K. Jha', 'link': 'https://arxiv.org/abs/2504.16171', 'abstract': 'Myocardial perfusion imaging (MPI) with single-photon emission computed tomography (SPECT) is a widely used and cost-effective diagnostic tool for coronary artery disease. However, the lengthy scanning time in this imaging procedure can cause patient discomfort, motion artifacts, and potentially inaccurate diagnoses due to misalignment between the SPECT scans and the CT-scans which are acquired for attenuation compensation. Reducing projection angles is a potential way to shorten scanning time, but this can adversely impact the quality of the reconstructed images. To address this issue, we propose a detection-task-specific deep-learning method for sparse-view MPI SPECT images. This method integrates an observer loss term that penalizes the loss of anthropomorphic channel features with the goal of improving performance in perfusion defect-detection task. We observed that, on the task of detecting myocardial perfusion defects, the proposed method yielded an area under the receiver operating characteristic (ROC) curve (AUC) significantly larger than the sparse-view protocol. Further, the proposed method was observed to be able to restore the structure of the left ventricle wall, demonstrating ability to overcome sparse-sampling artifacts. Our preliminary results motivate further evaluations of the method.', 'abstract_zh': '单光子发射计算机断层成像(SPECT) myocardial perfusion imaging (MPI)中特定检测任务的深度学习方法研究：稀疏视图下的心脏病灶检测与结构恢复', 'title_zh': '针对检测任务的深度学习方法以提高稀视角心肌灌注SPECT图像质量'}
{'arxiv_id': 'arXiv:2504.16145', 'title': 'Progressive Language-guided Visual Learning for Multi-Task Visual Grounding', 'authors': 'Jingchao Wang, Hong Wang, Wenlong Zhang, Kunhua Ji, Dingjiang Huang, Yefeng Zheng', 'link': 'https://arxiv.org/abs/2504.16145', 'abstract': 'Multi-task visual grounding (MTVG) includes two sub-tasks, i.e., Referring Expression Comprehension (REC) and Referring Expression Segmentation (RES). The existing representative approaches generally follow the research pipeline which mainly consists of three core procedures, including independent feature extraction for visual and linguistic modalities, respectively, cross-modal interaction module, and independent prediction heads for different sub-tasks. Albeit achieving remarkable performance, this research line has two limitations: 1) The linguistic content has not been fully injected into the entire visual backbone for boosting more effective visual feature extraction and it needs an extra cross-modal interaction module; 2) The relationship between REC and RES tasks is not effectively exploited to help the collaborative prediction for more accurate output. To deal with these problems, in this paper, we propose a Progressive Language-guided Visual Learning framework for multi-task visual grounding, called PLVL, which not only finely mine the inherent feature expression of the visual modality itself but also progressively inject the language information to help learn linguistic-related visual features. In this manner, our PLVL does not need additional cross-modal fusion module while fully introducing the language guidance. Furthermore, we analyze that the localization center for REC would help identify the to-be-segmented object region for RES to some extent. Inspired by this investigation, we design a multi-task head to accomplish collaborative predictions for these two sub-tasks. Extensive experiments conducted on several benchmark datasets comprehensively substantiate that our PLVL obviously outperforms the representative methods in both REC and RES tasks. this https URL', 'abstract_zh': '多任务视觉定位（MTVG）包括两项子任务，即引用表达理解（REC）和引用表达分割（RES）。现有代表性方法通常遵循主要由三个核心步骤组成的研究所采用的pipeline，包括分别对视觉和语言模态进行独立特征提取、跨模态交互模块，以及为不同子任务提供的独立预测头。尽管取得了显著性能，这条研究线仍存在两个局限性：1）语言内容尚未完全注入整个视觉骨干以增强更有效的视觉特征提取，需要额外的跨模态交互模块；2）REC和RES任务之间的关系没有得到有效利用，以帮助协作预测以获得更准确的输出。为解决这些问题，在本文中，我们提出了一种渐进的语言引导视觉学习框架，称为PLVL，该框架不仅细致挖掘视觉模态自身固有的特征表达，还逐步注入语言信息以帮助学习与语言相关的视觉特征。这样一来，我们不需要额外的跨模态融合模块，同时完全引入语言指导。此外，我们分析认为，REC的定位中心在一定程度上有助于识别RES待分割的对象区域。受这一研究的启发，我们设计了一个多任务头来完成这两项子任务的协作预测。在几个基准数据集上进行的广泛实验全面证实，我们的PLVL在REC和RES任务上明显优于代表方法。', 'title_zh': '渐进式语言引导的视觉学习在多任务视觉定位中的应用'}
{'arxiv_id': 'arXiv:2504.16128', 'title': 'Hybrid Knowledge Transfer through Attention and Logit Distillation for On-Device Vision Systems in Agricultural IoT', 'authors': 'Stanley Mugisha, Rashid Kisitu, Florence Tushabe', 'link': 'https://arxiv.org/abs/2504.16128', 'abstract': 'Integrating deep learning applications into agricultural IoT systems faces a serious challenge of balancing the high accuracy of Vision Transformers (ViTs) with the efficiency demands of resource-constrained edge devices. Large transformer models like the Swin Transformers excel in plant disease classification by capturing global-local dependencies. However, their computational complexity (34.1 GFLOPs) limits applications and renders them impractical for real-time on-device inference. Lightweight models such as MobileNetV3 and TinyML would be suitable for on-device inference but lack the required spatial reasoning for fine-grained disease detection. To bridge this gap, we propose a hybrid knowledge distillation framework that synergistically transfers logit and attention knowledge from a Swin Transformer teacher to a MobileNetV3 student model. Our method includes the introduction of adaptive attention alignment to resolve cross-architecture mismatch (resolution, channels) and a dual-loss function optimizing both class probabilities and spatial focus. On the lantVillage-Tomato dataset (18,160 images), the distilled MobileNetV3 attains 92.4% accuracy relative to 95.9% for Swin-L but at an 95% reduction on PC and < 82% in inference latency on IoT devices. (23ms on PC CPU and 86ms/image on smartphone CPUs). Key innovations include IoT-centric validation metrics (13 MB memory, 0.22 GFLOPs) and dynamic resolution-matching attention maps. Comparative experiments show significant improvements over standalone CNNs and prior distillation methods, with a 3.5% accuracy gain over MobileNetV3 baselines. Significantly, this work advances real-time, energy-efficient crop monitoring in precision agriculture and demonstrates how we can attain ViT-level diagnostic precision on edge devices. Code and models will be made available for replication after acceptance.', 'abstract_zh': '将深度学习应用集成到农业物联网系统中面临严重挑战，即在保持视觉变换器（ViTs）的高准确性与边缘受限设备的效率需求之间取得平衡。Swin变换器等大型变换器模型在植物疾病分类中表现优异，通过捕捉全局-局部依赖性。然而，其计算复杂度（34.1 GFLOPs）限制了其应用，并使其无法满足实时设备推理的需求。MobileNetV3和TinyML等轻量级模型适合设备推理，但缺乏细粒度疾病检测所需的空间推理能力。为此，我们提出了一种混合知识蒸馏框架，以协同方式将Swin Transformer教师的logit和注意力知识传递给MobileNetV3学生模型。该方法包括自适应注意力对齐以解决跨架构不匹配问题（分辨率、通道），以及双损失函数优化类概率和空间集中度。在lantVillage-Tomato数据集（18,160张图像）上，蒸馏后的MobileNetV3的准确率为92.4%，比Swin-L高95%，但在PC上的计算复杂度降低至<95%， inference延迟降低至<82%（PC CPU为23ms，智能手机CPU为86ms/张图像）。关键创新包括物联网中心化的验证指标（13 MB内存，0.22 GFLOPs）和动态分辨率匹配注意力图。与单独的卷积神经网络和之前的蒸馏方法相比，实验结果显示显著改进，MobileNetV3基线准确率提升3.5%。这项工作推进了精准农业中实时、能效高的作物监测，并展示了如何在边缘设备上实现ViT级别的诊断精度。接受后，代码和模型将供 replication 使用。', 'title_zh': '基于注意力和逻辑斯蒂_distillation的农业物联网设备上视觉系统混合知识迁移'}
{'arxiv_id': 'arXiv:2504.16097', 'title': 'A CNN-based Local-Global Self-Attention via Averaged Window Embeddings for Hierarchical ECG Analysis', 'authors': 'Arthur Buzelin, Pedro Robles Dutenhefner, Turi Rezende, Luisa G. Porfirio, Pedro Bento, Yan Aquino, Jose Fernandes, Caio Santana, Gabriela Miana, Gisele L. Pappa, Antonio Ribeiro, Wagner Meira Jr', 'link': 'https://arxiv.org/abs/2504.16097', 'abstract': 'Cardiovascular diseases remain the leading cause of global mortality, emphasizing the critical need for efficient diagnostic tools such as electrocardiograms (ECGs). Recent advancements in deep learning, particularly transformers, have revolutionized ECG analysis by capturing detailed waveform features as well as global rhythm patterns. However, traditional transformers struggle to effectively capture local morphological features that are critical for accurate ECG interpretation. We propose a novel Local-Global Attention ECG model (LGA-ECG) to address this limitation, integrating convolutional inductive biases with global self-attention mechanisms. Our approach extracts queries by averaging embeddings obtained from overlapping convolutional windows, enabling fine-grained morphological analysis, while simultaneously modeling global context through attention to keys and values derived from the entire sequence. Experiments conducted on the CODE-15 dataset demonstrate that LGA-ECG outperforms state-of-the-art models and ablation studies validate the effectiveness of the local-global attention strategy. By capturing the hierarchical temporal dependencies and morphological patterns in ECG signals, this new design showcases its potential for clinical deployment with robust automated ECG classification.', 'abstract_zh': '心血管疾病仍然是全球 mortality 的首要原因，强调了高效诊断工具如心电图（ECGs）的重要性。最近深度学习的进步，尤其是 transformer 的应用，已经通过捕捉详细的心电波形特征及整体节律模式重塑了心电图分析。然而，传统的 transformer 在捕捉对于准确心电图解释至关重要的局部形态特征方面存在困难。我们提出了一种新型局部-全局注意力心电图模型（LGA-ECG），结合卷积诱导偏差与全局自我注意机制。我们的方法通过从重叠卷积窗口中获得的嵌入平均来提取查询，从而实现精细的形态学分析，同时通过注意力机制处理整个序列得到的键和值来建模全局上下文。在 CODE-15 数据集上进行的实验表明，LGA-ECG 超过了现有最好的模型，并且消融研究验证了局部-全局注意力策略的有效性。通过捕捉心电图信号中的分层时间依赖性和形态学模式，此新设计展示了其在具有稳健自动心电图分类能力的临床部署中的潜力。', 'title_zh': '基于CNN的平均窗口嵌入局部-全局自注意力用于心脏电图的层级分析'}
