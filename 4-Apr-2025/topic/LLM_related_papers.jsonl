{'arxiv_id': 'arXiv:2504.02509', 'title': 'A Memory-Augmented LLM-Driven Method for Autonomous Merging of 3D Printing Work Orders', 'authors': 'Yuhao Liu, Maolin Yang, Pingyu Jiang', 'link': 'https://arxiv.org/abs/2504.02509', 'abstract': 'With the rapid development of 3D printing, the demand for personalized and customized production on the manufacturing line is steadily increasing. Efficient merging of printing workpieces can significantly enhance the processing efficiency of the production line. Addressing the challenge, a Large Language Model (LLM)-driven method is established in this paper for the autonomous merging of 3D printing work orders, integrated with a memory-augmented learning strategy. In industrial scenarios, both device and order features are modeled into LLM-readable natural language prompt templates, and develop an order-device matching tool along with a merging interference checking module. By incorporating a self-memory learning strategy, an intelligent agent for autonomous order merging is constructed, resulting in improved accuracy and precision in order allocation. The proposed method effectively leverages the strengths of LLMs in industrial applications while reducing hallucination.', 'abstract_zh': '随着3D打印的快速发展，制造线上对个性化和定制化生产的需求稳步增加。高效融合印刷工件可以显著提高生产线的加工效率。针对这一挑战，本文提出了一种由大规模语言模型（LLM）驱动的方法，用于自主融合3D打印工作订单，并结合了增强记忆的学习策略。在工业场景中，将设备和订单特征建模为LLM可读的自然语言提示模板，并开发了订单-设备匹配工具以及融合干扰检查模块。通过集成自我记忆学习策略，构建了一个智能化订单融合代理，从而提高了订单分配的准确性和精确度。所提出的方法有效利用了LLM在工业应用中的优势，减少了幻觉现象。', 'title_zh': '带有记忆增强的LLM驱动方法实现3D打印工作订单的自主合并'}
{'arxiv_id': 'arXiv:2504.02793', 'title': 'A Framework for Situating Innovations, Opportunities, and Challenges in Advancing Vertical Systems with Large AI Models', 'authors': 'Gaurav Verma, Jiawei Zhou, Mohit Chandra, Srijan Kumar, Munmun De Choudhury', 'link': 'https://arxiv.org/abs/2504.02793', 'abstract': 'Large artificial intelligence (AI) models have garnered significant attention for their remarkable, often "superhuman", performance on standardized benchmarks. However, when these models are deployed in high-stakes verticals such as healthcare, education, and law, they often reveal notable limitations. For instance, they exhibit brittleness to minor variations in input data, present contextually uninformed decisions in critical settings, and undermine user trust by confidently producing or reproducing inaccuracies. These challenges in applying large models necessitate cross-disciplinary innovations to align the models\' capabilities with the needs of real-world applications. We introduce a framework that addresses this gap through a layer-wise abstraction of innovations aimed at meeting users\' requirements with large models. Through multiple case studies, we illustrate how researchers and practitioners across various fields can operationalize this framework. Beyond modularizing the pipeline of transforming large models into useful "vertical systems", we also highlight the dynamism that exists within different layers of the framework. Finally, we discuss how our framework can guide researchers and practitioners to (i) optimally situate their innovations (e.g., when vertical-specific insights can empower broadly impactful vertical-agnostic innovations), (ii) uncover overlooked opportunities (e.g., spotting recurring problems across verticals to develop practically useful foundation models instead of chasing benchmarks), and (iii) facilitate cross-disciplinary communication of critical challenges (e.g., enabling a shared vocabulary for AI developers, domain experts, and human-computer interaction scholars).', 'abstract_zh': '大型人工智能模型在标准化基准上的出色表现吸引了广泛关注，但在医疗、教育和法律等高风险领域部署时，往往暴露出显著的局限性。这些挑战促使我们需要跨学科创新，以调整模型能力以满足实际应用的需求。我们提出一种框架，通过分层抽象创新，旨在利用大型模型满足用户需求。通过多个案例研究，我们展示了不同领域研究人员和实践者如何实现这一框架的落地。该框架不仅模块化了将大型模型转化为实用垂直系统的管道，还突显了框架各层中的动态性。最后，我们讨论了该框架如何指导研究人员和实践者：（i）优化创新定位（如，将垂直领域的洞察应用于普遍具有广泛影响的无垂直设定的创新），（ii）发现被忽视的机会（如，识别垂直领域中的重复问题，发展实用性基础模型而不是追求基准），以及（iii）促进跨学科关键挑战的沟通（如，为人工智能开发者、领域专家和人机交互学者提供共享的术语）。', 'title_zh': '一种关于推进垂直系统中大型AI模型应用中创新、机遇与挑战的框架研究'}
{'arxiv_id': 'arXiv:2504.02670', 'title': 'Affordable AI Assistants with Knowledge Graph of Thoughts', 'authors': 'Maciej Besta, Lorenzo Paleari, Jia Hao Andrea Jiang, Robert Gerstenberger, You Wu, Patrick Iff, Ales Kubicek, Piotr Nyczyk, Diana Khimey, Jón Gunnar Hannesson, Grzegorz Kwaśniewski, Marcin Copik, Hubert Niewiadomski, Torsten Hoefler', 'link': 'https://arxiv.org/abs/2504.02670', 'abstract': 'Large Language Models (LLMs) are revolutionizing the development of AI assistants capable of performing diverse tasks across domains. However, current state-of-the-art LLM-driven agents face significant challenges, including high operational costs and limited success rates on complex benchmarks like GAIA. To address these issues, we propose the Knowledge Graph of Thoughts (KGoT), an innovative AI assistant architecture that integrates LLM reasoning with dynamically constructed knowledge graphs (KGs). KGoT extracts and structures task-relevant knowledge into a dynamic KG representation, iteratively enhanced through external tools such as math solvers, web crawlers, and Python scripts. Such structured representation of task-relevant knowledge enables low-cost models to solve complex tasks effectively. For example, KGoT achieves a 29% improvement in task success rates on the GAIA benchmark compared to Hugging Face Agents with GPT-4o mini, while reducing costs by over 36x compared to GPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and 37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a scalable, affordable, and high-performing solution for AI assistants.', 'abstract_zh': '大型语言模型（LLMs）正在颠覆跨领域能够执行多样化任务的AI助手的发展。然而，当前最先进的LLM驱动代理面临显著挑战，包括高昂的运营成本和在GAIA等复杂基准测试上有限的成功率。为了解决这些问题，我们提出了思维知识图谱（KGoT）这一创新的AI助手架构，它将LLM推理与动态构建的知识图谱（KGs）集成。KGoT提取并结构化与任务相关知识，通过外部工具如数学求解器、网页爬虫和Python脚本进行迭代增强。这种结构化表示使得低成本模型能够有效解决复杂任务。例如，KGoT在GAIA基准测试中的任务成功率比使用GPT-4o mini的Hugging Face代理提高了29%，同时成本降低了36倍以上，与GPT-4o相比。最近的推理模型也有类似改进，如Qwen2.5-32B的36%和Deepseek-R1-70B的37.5%。KGoT提供了一种可扩展、经济高效且高性能的AI助手解决方案。', 'title_zh': '具有思维知识图谱的可负担AI助手'}
{'arxiv_id': 'arXiv:2504.02623', 'title': 'Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents through Related and Dynamic Missions', 'authors': 'PeiJie Yu, Yifan Yang, Jinjian Li, Zelong Zhang, Haorui Wang, Xiao Feng, Feng Zhang', 'link': 'https://arxiv.org/abs/2504.02623', 'abstract': 'Large language models (LLMs) demonstrate strong potential as agents for tool invocation due to their advanced comprehension and planning capabilities. Users increasingly rely on LLM-based agents to solve complex missions through iterative interactions. However, existing benchmarks predominantly access agents in single-mission scenarios, failing to capture real-world complexity. To bridge this gap, we propose the Multi-Mission Tool Bench. In the benchmark, each test case comprises multiple interrelated missions. This design requires agents to dynamically adapt to evolving demands. Moreover, the proposed benchmark explores all possible mission-switching patterns within a fixed mission number. Specifically, we propose a multi-agent data generation framework to construct the benchmark. We also propose a novel method to evaluate the accuracy and efficiency of agent decisions with dynamic decision trees. Experiments on diverse open-source and closed-source LLMs reveal critical factors influencing agent robustness and provide actionable insights to the tool invocation society.', 'abstract_zh': '多任务工具基准：多任务场景下的智能代理评估', 'title_zh': '多任务工具台：通过相关和动态任务评估基于LLM的代理的 robustness'}
{'arxiv_id': 'arXiv:2504.02489', 'title': 'The Self-Learning Agent with a Progressive Neural Network Integrated Transformer', 'authors': 'Ajay Sivakumar, Shalini, Vasantha Raj, Sebastian Sylvester', 'link': 'https://arxiv.org/abs/2504.02489', 'abstract': 'This paper introduces a self-learning agent that integrates LLaMA 3.2 with a Progressive Neural Network (PNN) for continual learning in conversational AI and code generation. The framework dynamically collects data, fine-tunes tasks with minimal samples, and leverages Meta-Learning for rapid adaptation. LoRA optimizes fine-tuning, while Elastic Weight Consolidation (EWC) enhances knowledge retention. Experimental results demonstrate improved adaptability and memory stability, positioning this approach as a scalable step toward Artificial General Intelligence (AGI).', 'abstract_zh': '本文介绍了一种将LLaMA 3.2与渐进神经网络（PNN）相结合的自学习代理，用于对话AI和代码生成的连续学习。该框架动态收集数据，使用最少样本微调任务，并利用元学习实现快速适应。LoRA优化微调，而弹性权重 consolidation（EWC）增强知识保持。实验结果表明，该方法在适应性和记忆稳定性方面有所改进，为通用人工智能（AGI）的发展提供了一个可扩展的步骤。', 'title_zh': '具有渐进神经网络集成变换器的自学习代理'}
{'arxiv_id': 'arXiv:2504.02426', 'title': 'Narrative Studio: Visual narrative exploration using LLMs and Monte Carlo Tree Search', 'authors': 'Parsa Ghaffari, Chris Hokamp', 'link': 'https://arxiv.org/abs/2504.02426', 'abstract': "Interactive storytelling benefits from planning and exploring multiple 'what if' scenarios. Modern LLMs are useful tools for ideation and exploration, but current chat-based user interfaces restrict users to a single linear flow. To address this limitation, we propose Narrative Studio -- a novel in-browser narrative exploration environment featuring a tree-like interface that allows branching exploration from user-defined points in a story. Each branch is extended via iterative LLM inference guided by system and user-defined prompts. Additionally, we employ Monte Carlo Tree Search (MCTS) to automatically expand promising narrative paths based on user-specified criteria, enabling more diverse and robust story development. We also allow users to enhance narrative coherence by grounding the generated text in an entity graph that represents the actors and environment of the story.", 'abstract_zh': '交互式讲故事受益于规划和探索多个“如果”情景。现代语言模型是创意和探索的有用工具，但基于聊天的用户界面限制用户在单一线性流程中操作。为了解决这一限制，我们提出了Narrative Studio——一种新的基于浏览器的叙事探索环境，其特色是一个树状界面，允许从故事中用户定义的点出发进行分支探索。每条分支通过迭代的LLM推理并根据系统和用户定义的提示扩展。此外，我们采用蒙特卡洛树搜索（MCTS）来根据用户指定的指标自动扩展有前途的叙事路径，从而实现更多样和 robust 的故事开发。我们还允许用户通过将生成的文本与表示故事中演员和环境的实体图结合，增强叙事连贯性。', 'title_zh': '叙事工作室：使用LLMs和蒙特卡洛树搜索的视觉叙事探索'}
{'arxiv_id': 'arXiv:2504.02193', 'title': 'More is Less: The Pitfalls of Multi-Model Synthetic Preference Data in DPO Safety Alignment', 'authors': 'Yifan Wang, Runjin Chen, Bolian Li, David Cho, Yihe Deng, Ruqi Zhang, Tianlong Chen, Zhangyang Wang, Ananth Grama, Junyuan Hong', 'link': 'https://arxiv.org/abs/2504.02193', 'abstract': 'Aligning large language models (LLMs) with human values is an increasingly critical step in post-training. Direct Preference Optimization (DPO) has emerged as a simple, yet effective alternative to reinforcement learning from human feedback (RLHF). Synthetic preference data with its low cost and high quality enable effective alignment through single- or multi-model generated preference data. Our study reveals a striking, safety-specific phenomenon associated with DPO alignment: Although multi-model generated data enhances performance on general tasks (ARC, Hellaswag, MMLU, TruthfulQA, Winogrande) by providing diverse responses, it also tends to facilitate reward hacking during training. This can lead to a high attack success rate (ASR) when models encounter jailbreaking prompts. The issue is particularly pronounced when employing stronger models like GPT-4o or larger models in the same family to generate chosen responses paired with target model self-generated rejected responses, resulting in dramatically poorer safety outcomes. Furthermore, with respect to safety, using solely self-generated responses (single-model generation) for both chosen and rejected pairs significantly outperforms configurations that incorporate responses from stronger models, whether used directly as chosen data or as part of a multi-model response pool. We demonstrate that multi-model preference data exhibits high linear separability between chosen and rejected responses, which allows models to exploit superficial cues rather than internalizing robust safety constraints. Our experiments, conducted on models from the Llama, Mistral, and Qwen families, consistently validate these findings.', 'abstract_zh': '使用多模型数据在直接偏好优化对齐中增强性能但也可能导致安全风险：从Llama、Mistral和Qwen家族模型的实验中观察到', 'title_zh': '更多反而更少：多模型合成偏好数据在DPO安全对齐中的陷阱'}
{'arxiv_id': 'arXiv:2504.02181', 'title': 'A Survey of Scaling in Large Language Model Reasoning', 'authors': 'Zihan Chen, Song Wang, Zhen Tan, Xingbo Fu, Zhenyu Lei, Peng Wang, Huan Liu, Cong Shen, Jundong Li', 'link': 'https://arxiv.org/abs/2504.02181', 'abstract': 'The rapid advancements in large Language models (LLMs) have significantly enhanced their reasoning capabilities, driven by various strategies such as multi-agent collaboration. However, unlike the well-established performance improvements achieved through scaling data and model size, the scaling of reasoning in LLMs is more complex and can even negatively impact reasoning performance, introducing new challenges in model alignment and robustness. In this survey, we provide a comprehensive examination of scaling in LLM reasoning, categorizing it into multiple dimensions and analyzing how and to what extent different scaling strategies contribute to improving reasoning capabilities. We begin by exploring scaling in input size, which enables LLMs to process and utilize more extensive context for improved reasoning. Next, we analyze scaling in reasoning steps that improves multi-step inference and logical consistency. We then examine scaling in reasoning rounds, where iterative interactions refine reasoning outcomes. Furthermore, we discuss scaling in training-enabled reasoning, focusing on optimization through iterative model improvement. Finally, we review applications of scaling across domains and outline future directions for further advancing LLM reasoning. By synthesizing these diverse perspectives, this survey aims to provide insights into how scaling strategies fundamentally enhance the reasoning capabilities of LLMs and further guide the development of next-generation AI systems.', 'abstract_zh': '大型语言模型（LLMs）的 Rapid Advancements and Scaling in Reasoning: A Comprehensive Survey', 'title_zh': '大型语言模型推理中的扩展性研究'}
{'arxiv_id': 'arXiv:2504.02148', 'title': 'OmniCellTOSG: The First Cell Text-Omic Signaling Graphs Dataset for Joint LLM and GNN Modeling', 'authors': 'Heming Zhang, Tim Xu, Dekang Cao, Shunning Liang, Lars Schimmelpfennig, Levi Kaster, Di Huang, Carlos Cruchaga, Guangfu Li, Michael Province, Yixin Chen, Philip Payne, Fuhai Li', 'link': 'https://arxiv.org/abs/2504.02148', 'abstract': 'Complex cell signaling systems -- governed by varying protein abundances and interactions -- generate diverse cell types across organs. These systems evolve under influences such as age, sex, diet, environmental exposures, and diseases, making them challenging to decode given the involvement of tens of thousands of genes and proteins. Recently, hundreds of millions of single-cell omics data have provided a robust foundation for understanding these signaling networks within various cell subpopulations and conditions. Inspired by the success of large foundation models (for example, large language models and large vision models) pre-trained on massive datasets, we introduce OmniCellTOSG, the first dataset of cell text-omic signaling graphs (TOSGs). Each TOSG represents the signaling network of an individual or meta-cell and is labeled with information such as organ, disease, sex, age, and cell subtype. OmniCellTOSG offers two key contributions. First, it introduces a novel graph model that integrates human-readable annotations -- such as biological functions, cellular locations, signaling pathways, related diseases, and drugs -- with quantitative gene and protein abundance data, enabling graph reasoning to decode cell signaling. This approach calls for new joint models combining large language models and graph neural networks. Second, the dataset is built from single-cell RNA sequencing data of approximately 120 million cells from diverse tissues and conditions (healthy and diseased) and is fully compatible with PyTorch. This facilitates the development of innovative cell signaling models that could transform research in life sciences, healthcare, and precision medicine. The OmniCellTOSG dataset is continuously expanding and will be updated regularly. The dataset and code are available at this https URL.', 'abstract_zh': '复杂的细胞信号系统——由蛋白质丰度和相互作用的差异调控——在不同器官中生成多种细胞类型。这些系统受到年龄、性别、饮食、环境暴露和疾病等因素的影响而进化，给解析带来挑战，因为涉及成千上万的基因和蛋白质。近年来，数亿条单细胞组学数据为理解这些信号网络提供了坚实基础，特别是在不同细胞亚群和条件下。受大规模基础模型（如大型语言模型和大型视觉模型）在大规模数据集上预训练成功的启发，我们引入了OmniCellTOSG，这是首个细胞文本-组学信号图谱（TOSGs）数据集。每个TOSG代表个体或元细胞的信号网络，并标注有器官、疾病、性别、年龄和细胞亚型等信息。OmniCellTOSG提供了两个关键贡献。首先，它引入了一种新颖的图模型，将生物功能、细胞位置、信号通路、相关疾病和药物等可读注释与定量的基因和蛋白质丰度数据结合，以图示推理解码细胞信号。这需要新的结合大型语言模型和图神经网络的联合模型。其次，该数据集源自大约1.2亿个来自不同组织和条件（健康和患病状态）的单细胞RNA测序数据，并完全兼容PyTorch，这为开发创新的细胞信号模型提供了便利，这些模型有可能改变生命科学、医疗保健和精准医疗领域的研究。OmniCellTOSG数据集将持续扩展并定期更新。数据集和代码可从此链接访问。', 'title_zh': 'OmniCellTOSG：首个联合LLM和GNN建模的细胞文本组信号图数据集'}
{'arxiv_id': 'arXiv:2504.02111', 'title': 'Exploring LLM Reasoning Through Controlled Prompt Variations', 'authors': 'Giannis Chatziveroglou, Richard Yun, Maura Kelleher', 'link': 'https://arxiv.org/abs/2504.02111', 'abstract': "This study investigates the reasoning robustness of large language models (LLMs) on mathematical problem-solving tasks under systematically introduced input perturbations. Using the GSM8K dataset as a controlled testbed, we evaluate how well state-of-the-art models maintain logical consistency and correctness when confronted with four categories of prompt perturbations: irrelevant context, pathological instructions, factually relevant but non-essential context, and a combination of the latter two. Our experiments, conducted on thirteen open-source and closed-source LLMs, reveal that introducing irrelevant context within the model's context window significantly degrades performance, suggesting that distinguishing essential from extraneous details remains a pressing challenge. Surprisingly, performance regressions are relatively insensitive to the complexity of the reasoning task, as measured by the number of steps required, and are not strictly correlated with model size. Moreover, we observe that certain perturbations inadvertently trigger chain-of-thought-like reasoning behaviors, even without explicit prompting. Our findings highlight critical vulnerabilities in current LLMs and underscore the need for improved robustness against noisy, misleading, and contextually dense inputs, paving the way for more resilient and reliable reasoning in real-world applications.", 'abstract_zh': '本研究探讨了在系统引入输入扰动的情况下，大型语言模型（LLMs）在数学问题解决任务中的推理稳健性。使用GSM8K数据集作为控制试验平台，我们评估了当前最先进的模型在面对四类提示扰动（无关背景信息、病态指令、事实相关但非关键的背景信息以及后两者的组合）时，如何保持逻辑一致性和正确性。我们的实验在十三个开源和闭源LLM上进行，结果表明，在模型上下文窗口中引入无关背景信息显著降低了性能，这表明区分核心信息和非核心信息仍然是一个紧迫的挑战。令人惊讶的是，性能下降对所需推理步骤的数量（作为推理任务复杂性的度量）的敏感度较低，并且与模型大小之间没有严格的正相关关系。此外，我们观察到某些扰动会无意中触发类似于链式推理的行为，即使没有显式的提示也是如此。我们的研究结果突显了当前LLM中存在的关键漏洞，并强调了需要提高对嘈杂、误导性和语境密集输入的稳健性的改进，为在实际应用中实现更可靠和稳健的推理铺平了道路。', 'title_zh': '通过受控提示变异探究LLM推理能力'}
{'arxiv_id': 'arXiv:2504.01995', 'title': 'Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics', 'authors': 'Hamed Mahdavi, Alireza Hashemi, Majid Daliri, Pegah Mohammadipour, Alireza Farhadi, Samira Malek, Yekta Yazdanifard, Amir Khasahmadi, Vasant Honavar', 'link': 'https://arxiv.org/abs/2504.01995', 'abstract': 'Recent advancements in large language models (LLMs) have shown impressive progress in mathematical reasoning tasks. However, current evaluation benchmarks predominantly focus on the accuracy of final answers, often overlooking the logical rigor crucial for mathematical problem-solving. The claim that state-of-the-art LLMs can solve Math Olympiad-level problems requires closer examination. To explore this, we conducted both qualitative and quantitative human evaluations of proofs generated by LLMs, and developed a schema for automatically assessing their reasoning capabilities. Our study reveals that current LLMs fall significantly short of solving challenging Olympiad-level problems and frequently fail to distinguish correct mathematical reasoning from clearly flawed solutions. We also found that occasional correct final answers provided by LLMs often result from pattern recognition or heuristic shortcuts rather than genuine mathematical reasoning. These findings underscore the substantial gap between LLM performance and human expertise in advanced mathematical reasoning and highlight the importance of developing benchmarks that prioritize the rigor and coherence of mathematical arguments rather than merely the correctness of final answers.', 'abstract_zh': '近期大型语言模型（LLMs）在数学推理任务上的进展显示了令人印象深刻的成果，然而当前的评估基准主要集中在最终答案的准确性上，常常忽视了数学问题解决中至关重要的逻辑严谨性。当前关于最先进的LLMs能够解决奥林匹克数学水平问题的断言需要更深入的审视。为了探索这一问题，我们对LLMs生成的证明进行了定性和定量的人类评估，并开发了一种自动评估其推理能力的框架。研究结果揭示，当前的LLMs在解决具有挑战性的奥林匹克级别问题时存在显著不足，经常无法区分正确的数学推理与明显错误的解决方案。我们还发现，LLMs偶尔提供的正确最终答案通常源于模式识别或启发式捷径，而不是真正的数学推理。这些发现凸显了LLMs在高级数学推理方面的表现与人类专长之间的显著差距，并强调了优先考虑数学论证的严谨性和连贯性而非仅仅最终答案的正确性的重要性。', 'title_zh': 'brains vs. bytes: 评估大型语言模型在奥林匹克数学中的专业水平'}
{'arxiv_id': 'arXiv:2504.02821', 'title': 'Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models', 'authors': 'Mateusz Pach, Shyamgopal Karthik, Quentin Bouniot, Serge Belongie, Zeynep Akata', 'link': 'https://arxiv.org/abs/2504.02821', 'abstract': 'Sparse Autoencoders (SAEs) have recently been shown to enhance interpretability and steerability in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity in vision representations. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons while also exhibiting hierarchical representations that align well with expert-defined structures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that applying SAEs to intervene on a CLIP vision encoder, directly steer output from multimodal LLMs (e.g., LLaVA) without any modifications to the underlying model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised approach for enhancing both the interpretability and control of VLMs.', 'abstract_zh': '稀疏自编码器（SAEs）最近被证明可以增强大型语言模型（LLMs）的可解释性和可控性。在本文中，我们将SAEs的应用扩展到视觉-语言模型（VLMs），如CLIP，并引入了一种全面的框架来评估视觉表示的单义性。实验结果表明，SAEs在VLMs上的训练显著增强了单个神经元的单义性，同时展示了与专家定义的结构（例如，iNaturalist分类学）高度对齐的层次表示。尤为重要的是，我们证明了将SAEs应用于干预CLIP视觉编码器可以直接引导多模态LLMs（例如，LLaVA）的输出，而无需对底层模型进行任何修改。这些发现突显了SAEs作为无监督方法，用于增强VLMs的可解释性和可控性的实用性和有效性。', 'title_zh': '稀疏自编码器在视觉-语言模型中学习单义特征'}
{'arxiv_id': 'arXiv:2504.02810', 'title': 'Generative Evaluation of Complex Reasoning in Large Language Models', 'authors': 'Haowei Lin, Xiangyu Wang, Ruilin Yan, Baizhou Huang, Haotian Ye, Jianhua Zhu, Zihao Wang, James Zou, Jianzhu Ma, Yitao Liang', 'link': 'https://arxiv.org/abs/2504.02810', 'abstract': "With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for genuine LLM reasoning capabilities.", 'abstract_zh': '强大的大型语言模型（LLMs）展示出超人类的推理能力，一个关键问题由此产生：LLMs究竟是真正的推理，还是仅仅从广泛采集的网络训练数据集中回忆答案？公开发布的基准不可避免地会在被纳入后续LLM训练集后受到污染，削弱它们作为可信评估工具的有效性。为解决这一问题，我们提出KUMO，一种专门用于评估LLM推理能力的生成型评估框架。KUMO利用LLMs与符号引擎协同工作，动态生成多样且多轮的推理任务，这些任务部分可观测且具备调整难度的能力。通过自动化流程，KUMO持续生成跨开放式领域的新型任务，促使模型展示真正的泛化能力而非记忆能力。我们对KUMO生成的100个领域中的5000个任务评估了23种最先进的LLMs，并将其推理能力与大学生进行了基准测试。研究发现，许多LLMs已经在简单的推理任务上超越了大学水平的表现，而推理调优的LLMs在复杂推理挑战中也达到了大学水平的表现。此外，LLMs在KUMO任务上的表现与新发布的实际推理基准测试结果之间存在强烈的相关性，这突显了KUMO作为评估LLM真正推理能力的坚固且持久工具的价值。', 'title_zh': '大型语言模型中复杂推理的生成性评估'}
{'arxiv_id': 'arXiv:2504.02807', 'title': 'MegaMath: Pushing the Limits of Open Math Corpora', 'authors': 'Fan Zhou, Zengzhi Wang, Nikhil Ranjan, Zhoujun Cheng, Liping Tang, Guowei He, Zhengzhong Liu, Eric P. Xing', 'link': 'https://arxiv.org/abs/2504.02807', 'abstract': 'Mathematical reasoning is a cornerstone of human intelligence and a key benchmark for advanced capabilities in large language models (LLMs). However, the research community still lacks an open, large-scale, high-quality corpus tailored to the demands of math-centric LLM pre-training. We present MegaMath, an open dataset curated from diverse, math-focused sources through following practices: (1) Revisiting web data: We re-extracted mathematical documents from Common Crawl with math-oriented HTML optimizations, fasttext-based filtering and deduplication, all for acquiring higher-quality data on the Internet. (2) Recalling Math-related code data: We identified high quality math-related code from large code training corpus, Stack-V2, further enhancing data diversity. (3) Exploring Synthetic data: We synthesized QA-style text, math-related code, and interleaved text-code blocks from web data or code data. By integrating these strategies and validating their effectiveness through extensive ablations, MegaMath delivers 371B tokens with the largest quantity and top quality among existing open math pre-training datasets.', 'abstract_zh': '数学推理是人类智能的基础，也是大型语言模型（LLMs）高级能力的关键评价标准。然而，研究社区仍然缺乏一个针对数学中心型LLM预训练需求的开放、大规模、高质量语料库。我们介绍了MegaMath，一个通过以下策略慎重编纂的开放数据集：（1）重访网络数据：我们重新从Common Crawl中提取数学文档，并进行了数学导向的HTML优化、基于fastText的过滤和去重，以获取更高质量的网络数据。（2）回忆相关代码数据：我们从大规模代码训练语料库Stack-V2中识别高质量的数学相关代码，进一步增强数据多样性。（3）探索合成数据：我们从网络数据或代码数据中合成了问答风格文本、数学相关代码以及交错的文本-代码块。通过整合这些策略并在广泛的 ablation 实验中验证其有效性，MegaMath 提供了现有开放数学预训练语料库中词汇量和质量最大的 371B 个词元。', 'title_zh': 'MegaMath: 推动开源数学语料库的极限'}
{'arxiv_id': 'arXiv:2504.02780', 'title': 'From Consumption to Collaboration: Measuring Interaction Patterns to Augment Human Cognition in Open-Ended Tasks', 'authors': 'Joshua Holstein, Moritz Diener, Philipp Spitzer', 'link': 'https://arxiv.org/abs/2504.02780', 'abstract': 'The rise of Generative AI, and Large Language Models (LLMs) in particular, is fundamentally changing cognitive processes in knowledge work, raising critical questions about their impact on human reasoning and problem-solving capabilities. As these AI systems become increasingly integrated into workflows, they offer unprecedented opportunities for augmenting human thinking while simultaneously risking cognitive erosion through passive consumption of generated answers. This tension is particularly pronounced in open-ended tasks, where effective solutions require deep contextualization and integration of domain knowledge. Unlike structured tasks with established metrics, measuring the quality of human-LLM interaction in such open-ended tasks poses significant challenges due to the absence of ground truth and the iterative nature of solution development. To address this, we present a framework that analyzes interaction patterns along two dimensions: cognitive activity mode (exploration vs. exploitation) and cognitive engagement mode (constructive vs. detrimental). This framework provides systematic measurements to evaluate when LLMs are effective tools for thought rather than substitutes for human cognition, advancing theoretical understanding and practical guidance for developing AI systems that protect and augment human cognitive capabilities.', 'abstract_zh': '生成式AI的兴起，尤其是大型语言模型（LLMs），从根本上改变了知识工作中的认知过程，引发了对其对人类推理和问题解决能力影响的关键问题。随着这些AI系统越来越多地集成到工作流程中，它们提供了前所未有的机会来增强人类思维，同时也通过被动消费生成的答案而带来认知衰退的风险。这一紧张关系在开放式任务中尤为突出，有效解决方案需要深厚的上下文理解和领域知识的集成。与有明确度量标准的结构化任务不同，由于缺乏真实标准且解决方案开发具有迭代性，衡量人类与LLM交互的质量面临重大挑战。为此，我们提出了一种框架，从两个维度分析交互模式：认知活动模式（探索 vs. 利用）和认知参与模式（建设性 vs. 损害性）。该框架提供了系统性测量标准，以评估LLM是作为辅助思维工具还是代替人类认知的工具，从而推动对保护和增强人类认知能力的AI系统理论理解与实践指导。', 'title_zh': '从消费到协作：通过测量互动模式增强在开放任务中的人类认知'}
{'arxiv_id': 'arXiv:2504.02767', 'title': 'How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices?', 'authors': 'Andres Algaba, Vincent Holst, Floriano Tori, Melika Mobini, Brecht Verbeken, Sylvia Wenmackers, Vincent Ginis', 'link': 'https://arxiv.org/abs/2504.02767', 'abstract': 'The spread of scientific knowledge depends on how researchers discover and cite previous work. The adoption of large language models (LLMs) in the scientific research process introduces a new layer to these citation practices. However, it remains unclear to what extent LLMs align with human citation practices, how they perform across domains, and may influence citation dynamics. Here, we show that LLMs systematically reinforce the Matthew effect in citations by consistently favoring highly cited papers when generating references. This pattern persists across scientific domains despite significant field-specific variations in existence rates, which refer to the proportion of generated references that match existing records in external bibliometric databases. Analyzing 274,951 references generated by GPT-4o for 10,000 papers, we find that LLM recommendations diverge from traditional citation patterns by preferring more recent references with shorter titles and fewer authors. Emphasizing their content-level relevance, the generated references are semantically aligned with the content of each paper at levels comparable to the ground truth references and display similar network effects while reducing author self-citations. These findings illustrate how LLMs may reshape citation practices and influence the trajectory of scientific discovery by reflecting and amplifying established trends. As LLMs become more integrated into the scientific research process, it is important to understand their role in shaping how scientific communities discover and build upon prior work.', 'abstract_zh': '科学知识的传播取决于研究人员如何发现和引用先前的工作。大型语言模型（LLMs）在科学研究过程中的应用引入了引用实践的新层次。然而，LLMs与人类引用实践的契合度、其在不同领域的表现以及如何影响引用动态尚不明确。在这里，我们表明，LLMs系统地强化了引用中的马太效应，即在生成参考文献时始终更青睐高被引论文。这一模式在不同科学领域中持续存在，尽管这些领域在生成的参考文献与外部引文数据库现有记录匹配率方面存在显著的领域特定差异。分析GPT-4o为10,000篇论文生成的274,951个参考文献，我们发现，LLMs的推荐偏好于更近、标题更短和作者更少的参考文献。通过强调内容相关性，生成的参考文献在语义上与每篇论文的内容保持一致，显示相似的网络效应，同时减少了作者自引。这些发现说明了LLMs可能如何重新塑造引用实践，以及通过反映和放大已确立的趋势如何影响科学发现的轨迹。随着LLMs在科学研究过程中的集成程度加深，理解其在塑造科学社区发现和借鉴先前工作方面的作用变得尤为重要。', 'title_zh': '大型语言模型在多大程度上内化了科学文献和引用实践？'}
{'arxiv_id': 'arXiv:2504.02646', 'title': 'Prompt Optimization with Logged Bandit Data', 'authors': 'Haruka Kiyohara, Daniel Yiming Cao, Yuta Saito, Thorsten Joachims', 'link': 'https://arxiv.org/abs/2504.02646', 'abstract': 'We study how to use naturally available user feedback, such as clicks, to optimize large language model (LLM) pipelines for generating personalized sentences using prompts. Naive approaches, which estimate the policy gradient in the prompt space, suffer either from variance caused by the large action space of prompts or bias caused by inaccurate reward predictions. To circumvent these challenges, we propose a novel kernel-based off-policy gradient method, which estimates the policy gradient by leveraging similarity among generated sentences, substantially reducing variance while suppressing the bias. Empirical results on our newly established suite of benchmarks demonstrate the effectiveness of the proposed approach in generating personalized descriptions for movie recommendations, particularly when the number of candidate prompts is large.', 'abstract_zh': '我们研究如何利用自然可用的用户反馈，如点击行为，优化生成个性化句子的大语言模型（LLM）管道，并使用提示进行优化。针对提示空间大的动作空间导致的方差问题及不准确奖励预测导致的偏差问题，我们提出了一种新颖的核基于离策策略梯度方法，通过利用生成句子之间的相似性来估计策略梯度，显著降低了方差并抑制了偏差。我们在新建立的基准测试套件上的实验结果表明，所提出的方法在生成电影推荐的个性化描述方面特别有效，尤其是在候选提示数量较多时。', 'title_zh': '带日志的bandit数据的提示优化'}
{'arxiv_id': 'arXiv:2504.02495', 'title': 'Inference-Time Scaling for Generalist Reward Modeling', 'authors': 'Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, Yu Wu', 'link': 'https://arxiv.org/abs/2504.02495', 'abstract': 'Reinforcement learning (RL) has been widely adopted in post-training for large language models (LLMs) at scale. Recently, the incentivization of reasoning capabilities in LLMs from RL indicates that $\\textit{proper learning methods could enable effective inference-time scalability}$. A key challenge of RL is to obtain accurate reward signals for LLMs in various domains beyond verifiable questions or artificial rules. In this work, we investigate how to improve reward modeling (RM) with more inference compute for general queries, i.e. the $\\textbf{inference-time scalability of generalist RM}$, and further, how to improve the effectiveness of performance-compute scaling with proper learning methods. For the RM approach, we adopt pointwise generative reward modeling (GRM) to enable flexibility for different input types and potential for inference-time scaling. For the learning method, we propose Self-Principled Critique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs through online RL, to generate principles adaptively and critiques accurately, resulting in $\\textbf{DeepSeek-GRM}$ models. Furthermore, for effective inference-time scaling, we use parallel sampling to expand compute usage, and introduce a meta RM to guide voting process for better scaling performance. Empirically, we show that SPCT significantly improves the quality and scalability of GRMs, outperforming existing methods and models in various RM benchmarks without severe biases, and could achieve better performance compared to training-time scaling. DeepSeek-GRM still meets challenges in some tasks, which we believe can be addressed by future efforts in generalist reward systems. The models will be released and open-sourced.', 'abstract_zh': '强化学习（RL）在大规模语言模型（LLMs）训练后应用中已被广泛采用。近期，从RL激励LLMs的推理能力表明，适当的學習方法能够实现有效的推理時�试可扩展性。强化学习的关键挑战是如何在可验证的问题或人工规则之外的各种领域为LLMs获得准确的奖励信号。在本文中，我们研究如何通过增加推理计算来改进奖励建模（RM），即通用查询的奖励建模的推理时测试可扩展性，并进一步探讨如何通过适当的學習方法改进性能计算的扩展效果。在奖励建模方法方面，我们采用点式生成奖励建模（GRM）来实现不同类型输入的灵活性和推理时测试的可扩展性。在学习方法方面，我们提出了自我原则批判调优（SPCT），通过在线RL促进GRM中的可扩展奖励生成行为，以适应性生成原则并准确地提出批判，从而构建出DeepSeek-GRM模型。此外，为了实现高效的推理时测试可扩展性，我们使用并行采样扩展计算使用，引入元级奖励建模以指导投票过程，以获得更好的扩展性能。实验结果表明，SPCT显著提高了GRM的质量和可扩展性，在各种奖励建模基准测试中优于现有方法和模型，且在某些任务中仍能实现更好的性能，这表明了一般奖励系统的未来努力可能会解决这些问题。该模型将被发布和开源。', 'title_zh': '通用奖励建模的推理时缩放方法'}
{'arxiv_id': 'arXiv:2504.02458', 'title': 'Retrieval-Augmented Purifier for Robust LLM-Empowered Recommendation', 'authors': 'Liangbo Ning, Wenqi Fan, Qing Li', 'link': 'https://arxiv.org/abs/2504.02458', 'abstract': "Recently, Large Language Model (LLM)-empowered recommender systems have revolutionized personalized recommendation frameworks and attracted extensive attention. Despite the remarkable success, existing LLM-empowered RecSys have been demonstrated to be highly vulnerable to minor perturbations. To mitigate the negative impact of such vulnerabilities, one potential solution is to employ collaborative signals based on item-item co-occurrence to purify the malicious collaborative knowledge from the user's historical interactions inserted by attackers. On the other hand, due to the capabilities to expand insufficient internal knowledge of LLMs, Retrieval-Augmented Generation (RAG) techniques provide unprecedented opportunities to enhance the robustness of LLM-empowered recommender systems by introducing external collaborative knowledge. Therefore, in this paper, we propose a novel framework (RETURN) by retrieving external collaborative signals to purify the poisoned user profiles and enhance the robustness of LLM-empowered RecSys in a plug-and-play manner. Specifically, retrieval-augmented perturbation positioning is proposed to identify potential perturbations within the users' historical sequences by retrieving external knowledge from collaborative item graphs. After that, we further retrieve the collaborative knowledge to cleanse the perturbations by using either deletion or replacement strategies and introduce a robust ensemble recommendation strategy to generate final robust predictions. Extensive experiments on three real-world datasets demonstrate the effectiveness of the proposed RETURN.", 'abstract_zh': '大型语言模型赋能的推荐系统近年来革命性地革新了个性化推荐框架并吸引了广泛注意。尽管取得了显著成功，现有的大型语言模型赋能的RecSys已被证明对轻微扰动非常脆弱。为了减轻这些脆弱性带来的负面影响，一种潜在的解决方案是利用基于项目共现的合作信号来净化攻击者插入到用户历史交互中的恶意合作知识。另一方面，由于能够扩展大型语言模型内部知识的局限性，检索增强生成（RAG）技术提供了前所未有的机会，通过引入外部合作知识来增强大型语言模型赋能的推荐系统的鲁棒性。因此，在本文中，我们提出了一种新型框架（RETURN），通过检索外部合作信号以插拔式方式净化被毒化的用户配置文件并增强大型语言模型赋能的RecSys的鲁棒性。具体而言，提出了一种检索增强扰动定位方法，通过从合作项目图中检索外部知识来识别用户历史序列中的潜在扰动。然后，我们进一步使用删除或替换策略检索合作知识以净化扰动，并引入一种健壮的集成推荐策略以生成最终的健壮预测。在三个真实世界数据集上的广泛实验表明了所提出RETURN的有效性。', 'title_zh': 'Robust LLM增强的检索增广净化器推荐系统'}
{'arxiv_id': 'arXiv:2504.02441', 'title': 'Cognitive Memory in Large Language Models', 'authors': 'Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, Yong Wu', 'link': 'https://arxiv.org/abs/2504.02441', 'abstract': 'This paper examines memory mechanisms in Large Language Models (LLMs), emphasizing their importance for context-rich responses, reduced hallucinations, and improved efficiency. It categorizes memory into sensory, short-term, and long-term, with sensory memory corresponding to input prompts, short-term memory processing immediate context, and long-term memory implemented via external databases or structures. The text-based memory section covers acquisition (selection and summarization), management (updating, accessing, storing, and resolving conflicts), and utilization (full-text search, SQL queries, semantic search). The KV cache-based memory section discusses selection methods (regularity-based summarization, score-based approaches, special token embeddings) and compression techniques (low-rank compression, KV merging, multimodal compression), along with management strategies like offloading and shared attention mechanisms. Parameter-based memory methods (LoRA, TTT, MoE) transform memories into model parameters to enhance efficiency, while hidden-state-based memory approaches (chunk mechanisms, recurrent transformers, Mamba model) improve long-text processing by combining RNN hidden states with current methods. Overall, the paper offers a comprehensive analysis of LLM memory mechanisms, highlighting their significance and future research directions.', 'abstract_zh': '本文探讨了大型语言模型中的记忆机制，强调其对丰富上下文响应、减少幻觉和提高效率的重要性。它将记忆分为感觉记忆、短期记忆和长期记忆，感觉记忆对应于输入提示，短期记忆处理即时上下文，长期记忆通过外部数据库或结构实现。基于文本的记忆部分涵盖了获取（选择和摘要）、管理（更新、访问、存储和冲突解决）以及利用（全文搜索、SQL查询、语义搜索）。基于KV缓存的记忆部分讨论了选择方法（基于规律的摘要、基于评分的方法、特殊标记嵌入）和压缩技术（低秩压缩、KV合并、多模态压缩），以及管理策略，如卸载和共享注意力机制。基于参数的记忆方法（LoRA、TTT、MoE）将记忆转换为模型参数以增强效率，基于隐藏状态的记忆方法（片段机制、递归变压器、Mamba模型）通过结合RNN隐藏状态和当前方法来改进长文本处理。总体而言，本文对大型语言模型的记忆机制进行了全面分析，突出了它们的重要性及其未来的研究方向。', 'title_zh': '大型语言模型中的认知记忆'}
{'arxiv_id': 'arXiv:2504.02293', 'title': 'State-of-the-Art Translation of Text-to-Gloss using mBART : A case study of Bangla', 'authors': 'Sharif Md. Abdullah, Abhijit Paul, Shebuti Rayana, Ahmedul Kabir, Zarif Masud', 'link': 'https://arxiv.org/abs/2504.02293', 'abstract': 'Despite a large deaf and dumb population of 1.7 million, Bangla Sign Language (BdSL) remains a understudied domain. Specifically, there are no works on Bangla text-to-gloss translation task. To address this gap, we begin by addressing the dataset problem. We take inspiration from grammatical rule based gloss generation used in Germany and American sign langauage (ASL) and adapt it for BdSL. We also leverage LLM to generate synthetic data and use back-translation, text generation for data augmentation. With dataset prepared, we started experimentation. We fine-tuned pretrained mBART-50 and mBERT-multiclass-uncased model on our dataset. We also trained GRU, RNN and a novel seq-to-seq model with multi-head attention. We observe significant high performance (ScareBLEU=79.53) with fine-tuning pretrained mBART-50 multilingual model from Facebook. We then explored why we observe such high performance with mBART. We soon notice an interesting property of mBART -- it was trained on shuffled and masked text data. And as we know, gloss form has shuffling property. So we hypothesize that mBART is inherently good at text-to-gloss tasks. To find support against this hypothesis, we trained mBART-50 on PHOENIX-14T benchmark and evaluated it with existing literature. Our mBART-50 finetune demonstrated State-of-the-Art performance on PHOENIX-14T benchmark, far outperforming existing models in all 6 metrics (ScareBLEU = 63.89, BLEU-1 = 55.14, BLEU-2 = 38.07, BLEU-3 = 27.13, BLEU-4 = 20.68, COMET = 0.624). Based on the results, this study proposes a new paradigm for text-to-gloss task using mBART models. Additionally, our results show that BdSL text-to-gloss task can greatly benefit from rule-based synthetic dataset.', 'abstract_zh': '尽管有170万聋哑人口，孟加拉手语（BdSL）仍是一个未被充分研究的领域。具体而言，目前没有关于孟加拉文本到手笨词翻译任务的研究。为填补这一空白，我们首先解决了数据集问题。我们借鉴了德国和美国手语（ASL）基于语法规则的手笨词生成方法，并将其适应到BdSL。我们还利用大语言模型生成合成数据，并通过反向翻译和文本生成进行数据增强。数据集准备完成后，我们开始了实验。我们对Facebook的预训练mBART-50和mBERT-multiclass-uncased模型进行了微调，并训练了GRU、RNN和一种新型的带有多头注意力机制的序列到序列模型。我们观察到显著的高性能（ScareBLEU=79.53）来自预训练mBART-50多语言模型的微调。为了进一步探索为什么mBART表现出如此高的性能，我们注意到一个有趣的现象：mBART是在打乱和掩盖的文本数据上进行训练的。而我们知道，手笨词形式具有打乱属性。因此，我们推测mBART本就擅长文本到手笨词任务。为了验证这一假设，我们在PHOENIX-14T基准数据集上训练了mBART-50，并用现有文献进行了评估。我们的mBART-50微调在PHOENIX-14T基准数据集上表现出SOTA性能，在所有6项指标上都远超现有模型（ScareBLEU = 63.89，BLEU-1 = 55.14，BLEU-2 = 38.07，BLEU-3 = 27.13，BLEU-4 = 20.68，COMET = 0.624）。基于这些结果，这项研究提出了一种新的基于mBART模型的文本到手笨词任务范式。此外，我们的研究表明，基于规则生成的数据集可以显著提升BdSL文本到手笨词任务的表现。', 'title_zh': '基于mBART的文本到手语翻译：孟加拉语案例研究'}
{'arxiv_id': 'arXiv:2504.02254', 'title': 'LLMs as Deceptive Agents: How Role-Based Prompting Induces Semantic Ambiguity in Puzzle Tasks', 'authors': 'Seunghyun Yoo', 'link': 'https://arxiv.org/abs/2504.02254', 'abstract': 'Recent advancements in Large Language Models (LLMs) have not only showcased impressive creative capabilities but also revealed emerging agentic behaviors that exploit linguistic ambiguity in adversarial settings. In this study, we investigate how an LLM, acting as an autonomous agent, leverages semantic ambiguity to generate deceptive puzzles that mislead and challenge human users. Inspired by the popular puzzle game "Connections", we systematically compare puzzles produced through zero-shot prompting, role-injected adversarial prompts, and human-crafted examples, with an emphasis on understanding the underlying agent decision-making processes. Employing computational analyses with HateBERT to quantify semantic ambiguity, alongside subjective human evaluations, we demonstrate that explicit adversarial agent behaviors significantly heighten semantic ambiguity -- thereby increasing cognitive load and reducing fairness in puzzle solving. These findings provide critical insights into the emergent agentic qualities of LLMs and underscore important ethical considerations for evaluating and safely deploying autonomous language systems in both educational technologies and entertainment.', 'abstract_zh': '最近在大型语言模型（LLMs）方面的进展不仅展示了其令人印象深刻的创造力，还揭示了其利用语义模糊在对抗性环境中展现出的新兴代理行为。本研究探讨了一个作为自主代理的LLM如何利用语义模糊生成误导性和挑战性极强的谜题，误导和挑战人类用户。受到流行谜题游戏“连接”（Connections）的启发，我们系统地比较了通过零样本提示、角色注入对抗性提示以及人工制作的示例生成的谜题，重点在于理解底层代理决策过程。利用HateBERT进行计算分析以量化语义模糊，并结合主观的人类评估，我们证明了明确的对抗性代理行为显著增加了语义模糊度——从而增加了解谜的认知负担并降低了公平性。这些发现为了解LLMs的 emergent 代理品质提供了关键性见解，并强调了在教育技术和娱乐领域评估和安全部署自主语言系统时重要的伦理考虑。', 'title_zh': 'LLMs作为欺骗性代理：基于角色的提示如何在谜题任务中诱导语义 ambiguity'}
{'arxiv_id': 'arXiv:2504.02234', 'title': 'LLM Social Simulations Are a Promising Research Method', 'authors': 'Jacy Reese Anthis, Ryan Liu, Sean M. Richardson, Austin C. Kozlowski, Bernard Koch, James Evans, Erik Brynjolfsson, Michael Bernstein', 'link': 'https://arxiv.org/abs/2504.02234', 'abstract': 'Accurate and verifiable large language model (LLM) simulations of human research subjects promise an accessible data source for understanding human behavior and training new AI systems. However, results to date have been limited, and few social scientists have adopted these methods. In this position paper, we argue that the promise of LLM social simulations can be achieved by addressing five tractable challenges. We ground our argument in a literature survey of empirical comparisons between LLMs and human research subjects, commentaries on the topic, and related work. We identify promising directions with prompting, fine-tuning, and complementary methods. We believe that LLM social simulations can already be used for exploratory research, such as pilot experiments for psychology, economics, sociology, and marketing. More widespread use may soon be possible with rapidly advancing LLM capabilities, and researchers should prioritize developing conceptual models and evaluations that can be iteratively deployed and refined at pace with ongoing AI advances.', 'abstract_zh': '准确可验证的大语言模型（LLM）模拟人类研究主体有望提供一个了解人类行为和培训新AI系统的可访问数据源。然而，目前的结果有限，很少有社会科学家采用这些方法。在本文中，我们认为通过解决五个可处理的挑战，可以实现LLM社会模拟的潜力。我们基于对LLM与人类研究主体之间实证比较的文献综述、主题评论和相关工作的分析，提出论点。我们指出了通过提示、微调和互补方法的有前途的方向。我们认为，LLM社会模拟已经在探索性研究中得到应用，如心理学、经济学、社会学和市场营销的试点实验。随着LLM能力的迅猛发展，更广泛的使用可能很快成为可能，并且研究人员应优先开发可以与持续的AI进步同步迭代部署和优化的概念模型和评估方法。', 'title_zh': 'LLM社会模拟是一种有前景的研究方法'}
{'arxiv_id': 'arXiv:2504.02144', 'title': 'Towards Interpretable Soft Prompts', 'authors': 'Oam Patel, Jason Wang, Nikhil Shivakumar Nayak, Suraj Srinivas, Himabindu Lakkaraju', 'link': 'https://arxiv.org/abs/2504.02144', 'abstract': 'Soft prompts have been popularized as a cheap and easy way to improve task-specific LLM performance beyond few-shot prompts. Despite their origin as an automated prompting method, however, soft prompts and other trainable prompts remain a black-box method with no immediately interpretable connections to prompting. We create a novel theoretical framework for evaluating the interpretability of trainable prompts based on two desiderata: faithfulness and scrutability. We find that existing methods do not naturally satisfy our proposed interpretability criterion. Instead, our framework inspires a new direction of trainable prompting methods that explicitly optimizes for interpretability. To this end, we formulate and test new interpretability-oriented objective functions for two state-of-the-art prompt tuners: Hard Prompts Made Easy (PEZ) and RLPrompt. Our experiments with GPT-2 demonstrate a fundamental trade-off between interpretability and the task-performance of the trainable prompt, explicating the hardness of the soft prompt interpretability problem and revealing odd behavior that arises when one optimizes for an interpretability proxy.', 'abstract_zh': '软提示作为一种经济高效的方法，在提高任务特定的大语言模型性能方面日益流行，超越了少量示例提示。然而，尽管软提示和其他可训练提示源自自动化提示方法，它们仍然是一种黑盒方法，缺乏直接可解释的连接。我们构建了一个新的理论框架，基于忠实度和可审查性两个需求来评估可训练提示的可解释性。我们发现现有方法并不自然满足我们提出的可解释性标准。相反，我们的框架启发了一种新的可训练提示方法方向，明确地优化了可解释性。为此，我们为两种最先进的提示调优方法——Hard Prompts Made Easy (PEZ) 和 RLPrompt——制定了并测试了新的可解释性导向的目标函数。GPT-2的实验展示了可解释性和可训练提示任务性能之间的基本权衡，解释了软提示可解释性问题的困难性，并揭示了在优化可解释性代理时出现的奇怪行为。', 'title_zh': '可解释的软提示研究'}
{'arxiv_id': 'arXiv:2504.02141', 'title': 'On Simulation-Guided LLM-based Code Generation for Safe Autonomous Driving Software', 'authors': 'Ali Nouri, Johan Andersson, Kailash De Jesus Hornig, Zhennan Fei, Emil Knabe, Hakan Sivencrona, Beatriz Cabrero-Daniel, Christian Berger', 'link': 'https://arxiv.org/abs/2504.02141', 'abstract': "Automated Driving System (ADS) is a safety-critical software system responsible for the interpretation of the vehicle's environment and making decisions accordingly. The unbounded complexity of the driving context, including unforeseeable events, necessitate continuous improvement, often achieved through iterative DevOps processes. However, DevOps processes are themselves complex, making these improvements both time- and resource-intensive. Automation in code generation for ADS using Large Language Models (LLM) is one potential approach to address this challenge. Nevertheless, the development of ADS requires rigorous processes to verify, validate, assess, and qualify the code before it can be deployed in the vehicle and used. In this study, we developed and evaluated a prototype for automatic code generation and assessment using a designed pipeline of a LLM-based agent, simulation model, and rule-based feedback generator in an industrial setup. The LLM-generated code is evaluated automatically in a simulation model against multiple critical traffic scenarios, and an assessment report is provided as feedback to the LLM for modification or bug fixing. We report about the experimental results of the prototype employing Codellama:34b, DeepSeek (r1:32b and Coder:33b), CodeGemma:7b, Mistral:7b, and GPT4 for Adaptive Cruise Control (ACC) and Unsupervised Collision Avoidance by Evasive Manoeuvre (CAEM). We finally assessed the tool with 11 experts at two Original Equipment Manufacturers (OEMs) by conducting an interview study.", 'abstract_zh': '自动驾驶系统（ADS）的自动化代码生成与评估研究：基于大规模语言模型的管道设计与工业应用', 'title_zh': '基于仿真引导的大语言模型代码生成以实现安全自主驾驶软件'}
{'arxiv_id': 'arXiv:2504.02128', 'title': 'Achieving Unanimous Consensus in Decision Making Using Multi-Agents', 'authors': 'Apurba Pokharel, Ram Dantu, Shakila Zaman, Sirisha Talapuru, Vinh Quach', 'link': 'https://arxiv.org/abs/2504.02128', 'abstract': "Blockchain consensus mechanisms have relied on algorithms such as Proof-of-Work (PoW) and Proof-of-Stake (PoS) to ensure network functionality and integrity. However, these approaches struggle with adaptability for decision-making where the opinions of each matter rather than reaching an agreement based on honest majority or weighted consensus. This paper introduces a novel deliberation-based consensus mechanism where Large Language Models (LLMs) act as rational agents engaging in structured discussions to reach a unanimous consensus. By leveraging graded consensus and a multi-round deliberation process, our approach ensures both unanimous consensus for definitive problems and graded confidence for prioritized decisions and policies. We provide a formalization of our system and use it to show that the properties of blockchains: consistency, agreement, liveness, and determinism are maintained. Moreover, experimental results demonstrate our system's feasibility, showcasing how our deliberation method's convergence, block properties, and accuracy enable decision-making on blockchain networks. We also address key challenges with this novel approach such as degeneration of thoughts, hallucinations, malicious models and nodes, resource consumption, and scalability.", 'abstract_zh': '基于大型语言模型的共识机制：一种基于协商的创新方法', 'title_zh': '在决策中使用多智能体实现一致共识'}
{'arxiv_id': 'arXiv:2504.02118', 'title': 'LLMPi: Optimizing LLMs for High-Throughput on Raspberry Pi', 'authors': 'Mahsa Ardakani, Jinendra Malekar, Ramtin Zand', 'link': 'https://arxiv.org/abs/2504.02118', 'abstract': 'Deploying Large Language Models (LLMs) on resource-constrained edge devices like the Raspberry Pi presents challenges in computational efficiency, power consumption, and response latency. This paper explores quantization-based optimization techniques to enable high-throughput, energy-efficient execution of LLMs on low-power embedded systems. Our approach leverages k-quantization, a Post-Training Quantization (PTQ) method designed for different bit-widths, enabling efficient 2-bit, 4-bit, 6-bit, and 8-bit weight quantization. Additionally, we employ ternary quantization using Quantization-Aware Training (QAT) for BitNet models, allowing for more effective adaptation to lower-bit representations while preserving accuracy.\nOur findings highlight the potential of quantized LLMs for real-time conversational AI on edge devices, paving the way for low-power, high-efficiency AI deployment in mobile and embedded applications. This study demonstrates that aggressive quantization strategies can significantly reduce energy consumption while maintaining inference quality, making LLMs practical for resource-limited environments.', 'abstract_zh': '将大型语言模型（LLMs）部署在资源受限的边缘设备如Raspberry Pi上，面临计算效率、功耗和响应延迟的挑战。本文探讨基于量化优化技术，以在低功耗嵌入式系统上实现LLMs的高吞吐量、节能执行。我们的方法采用了k-量化，这是一种用于不同位宽的后训练量化（PTQ）方法，支持高效的2位、4位、6位和8位权重量化。此外，我们还利用Quantization-Aware Training（QAT）进行三值量化，适用于BitNet模型，允许更有效地适应低位表示，同时保持准确性。', 'title_zh': 'LLMPi: 优化Raspberry Pi上的高 throughput 大语言模型'}
{'arxiv_id': 'arXiv:2504.02110', 'title': 'ScreenAudit: Detecting Screen Reader Accessibility Errors in Mobile Apps Using Large Language Models', 'authors': 'Mingyuan Zhong, Ruolin Chen, Xia Chen, James Fogarty, Jacob O. Wobbrock', 'link': 'https://arxiv.org/abs/2504.02110', 'abstract': "Many mobile apps are inaccessible, thereby excluding people from their potential benefits. Existing rule-based accessibility checkers aim to mitigate these failures by identifying errors early during development but are constrained in the types of errors they can detect. We present ScreenAudit, an LLM-powered system designed to traverse mobile app screens, extract metadata and transcripts, and identify screen reader accessibility errors overlooked by existing checkers. We recruited six accessibility experts including one screen reader user to evaluate ScreenAudit's reports across 14 unique app screens. Our findings indicate that ScreenAudit achieves an average coverage of 69.2%, compared to only 31.3% with a widely-used accessibility checker. Expert feedback indicated that ScreenAudit delivered higher-quality feedback and addressed more aspects of screen reader accessibility compared to existing checkers, and that ScreenAudit would benefit app developers in real-world settings.", 'abstract_zh': '许多移动应用不可访问，从而排除了人们享受其潜在利益的机会。现有的基于规则的无障碍检查器旨在通过在开发早期识别错误来减轻这些失败，但它们在检测错误的类型上存在限制。我们提出了一个以LLM为动力的系统ScreenAudit，用于遍历移动应用屏幕、提取元数据和转录，并识别现有检查器未能发现的屏幕阅读器无障碍错误。我们招募了六名无障碍专家，其中包括一名屏幕阅读器用户，对ScreenAudit的报告在14个独特的应用屏幕上的表现进行了评估。我们的研究发现，ScreenAudit的覆盖率为69.2%，而一款广泛使用的无障碍检查器的覆盖率为31.3%。专家反馈表明，与现有的检查器相比，ScreenAudit提供了更高质量的反馈，并且涵盖了更多屏幕阅读器无障碍方面的内容，且ScreenAudit在实际应用环境中将有助于应用开发人员。', 'title_zh': 'ScreenAudit：使用大型语言模型检测移动应用中的屏幕阅读器 Accessibility 错误'}
{'arxiv_id': 'arXiv:2504.02094', 'title': 'FlowDistill: Scalable Traffic Flow Prediction via Distillation from LLMs', 'authors': 'Chenyang Yu, Xinpeng Xie, Yan Huang, Chenxi Qiu', 'link': 'https://arxiv.org/abs/2504.02094', 'abstract': "Accurate traffic flow prediction is vital for optimizing urban mobility, yet it remains difficult in many cities due to complex spatio-temporal dependencies and limited high-quality data. While deep graph-based models demonstrate strong predictive power, their performance often comes at the cost of high computational overhead and substantial training data requirements, making them impractical for deployment in resource-constrained or data-scarce environments. We propose the FlowDistill, a lightweight and scalable traffic prediction framework based on knowledge distillation from large language models (LLMs). In this teacher-student setup, a fine-tuned LLM guides a compact multi-layer perceptron (MLP) student model using a novel combination of the information bottleneck principle and teacher-bounded regression loss, ensuring the distilled model retains only essential and transferable knowledge. Spatial and temporal correlations are explicitly encoded to enhance the model's generalization across diverse urban settings. Despite its simplicity, FlowDistill consistently outperforms state-of-the-art models in prediction accuracy while requiring significantly less training data, and achieving lower memory usage and inference latency, highlighting its efficiency and suitability for real-world, scalable deployment.", 'abstract_zh': '基于大型语言模型知识蒸馏的FlowDistill交通预测框架', 'title_zh': 'FlowDistill：通过来自LLM的精炼实现可扩展的交通流预测'}
{'arxiv_id': 'arXiv:2504.02080', 'title': 'Evolving Security in LLMs: A Study of Jailbreak Attacks and Defenses', 'authors': 'Zhengchun Shang, Wenlan Wei', 'link': 'https://arxiv.org/abs/2504.02080', 'abstract': 'Large Language Models (LLMs) are increasingly popular, powering a wide range of applications. Their widespread use has sparked concerns, especially through jailbreak attacks that bypass safety measures to produce harmful content.\nIn this paper, we present a comprehensive security analysis of large language models (LLMs), addressing critical research questions on the evolution and determinants of model safety.\nSpecifically, we begin by identifying the most effective techniques for detecting jailbreak attacks. Next, we investigate whether newer versions of LLMs offer improved security compared to their predecessors. We also assess the impact of model size on overall security and explore the potential benefits of integrating multiple defense strategies to enhance model robustness.\nOur study evaluates both open-source models (e.g., LLaMA and Mistral) and closed-source systems (e.g., GPT-4) by employing four state-of-the-art attack techniques and assessing the efficacy of three new defensive approaches.', 'abstract_zh': '大型语言模型（LLMs）日益流行，推动了广泛的应用。它们的广泛应用引发了诸多关切，尤其是在通过越狱攻击绕过安全措施以生成有害内容的问题上。\n\n在这篇论文中，我们对大型语言模型（LLMs）进行了全面的安全分析，探讨了模型安全性演化及其决定因素的关键研究问题。\n\n具体来说，我们首先识别出检测越狱攻击最有效的方法。接下来，我们调查了新版本的LLMs是否在安全性上优于其前代产品。我们还评估了模型规模对整体安全性的影响，并探索了结合多种防御策略以增强模型稳健性的潜力。\n\n我们的研究通过使用四种最先进的攻击技术评估开源模型（如LLaMA和Mistral）和封闭源系统（如GPT-4），并评估了三种新防御方法的有效性。', 'title_zh': 'LLMs中 evolving 安全性：一项关于 Jailbreak 攻击与防御的研究'}
{'arxiv_id': 'arXiv:2504.02074', 'title': 'Trapped by Expectations: Functional Fixedness in LLM-Enabled Chat Search', 'authors': 'Jiqun Liu, Jamshed Karimnazarov, Ryen W. White', 'link': 'https://arxiv.org/abs/2504.02074', 'abstract': "Functional fixedness, a cognitive bias that restricts users' interactions with a new system or tool to expected or familiar ways, limits the full potential of Large Language Model (LLM)-enabled chat search, especially in complex and exploratory tasks. To investigate its impact, we conducted a crowdsourcing study with 450 participants, each completing one of six decision-making tasks spanning public safety, diet and health management, sustainability, and AI ethics. Participants engaged in a multi-prompt conversation with ChatGPT to address the task, allowing us to compare pre-chat intent-based expectations with observed interactions. We found that: 1) Several aspects of pre-chat expectations are closely associated with users' prior experiences with ChatGPT, search engines, and virtual assistants; 2) Prior system experience shapes language use and prompting behavior. Frequent ChatGPT users reduced deictic terms and hedge words and frequently adjusted prompts. Users with rich search experience maintained structured, less-conversational queries with minimal modifications. Users of virtual assistants favored directive, command-like prompts, reinforcing functional fixedness; 3) When the system failed to meet expectations, participants generated more detailed prompts with increased linguistic diversity, reflecting adaptive shifts. These findings suggest that while preconceived expectations constrain early interactions, unmet expectations can motivate behavioral adaptation. With appropriate system support, this may promote broader exploration of LLM capabilities. This work also introduces a typology for user intents in chat search and highlights the importance of mitigating functional fixedness to support more creative and analytical use of LLMs.", 'abstract_zh': '认知偏见功能固定性限制了大语言模型（LLM）赋能的聊天搜索的潜力，特别是在复杂和探索性任务中。为了探究其影响，我们通过众包研究对450名参与者进行了调查，每位参与者完成了一个涉及公共安全、饮食与健康管理、可持续发展和AI伦理的六项决策任务之一。参与者与ChatGPT进行了多轮对话以应对任务，使我们能够比较事前预期与实际交互之间的差异。研究发现：1）事前预期的多个方面与用户对ChatGPT、搜索引擎和虚拟助手的先前经验密切相关；2）先前系统的使用经验影响语言使用和提问行为。频繁使用ChatGPT的用户减少了指示词和模糊词汇的使用，并频繁调整提问。拥有丰富搜索经验的用户维持了结构化、较少对话性的查询并进行了最小的修改。虚拟助手的用户倾向于使用指令性、命令式的提问，强化了功能固定性；3）当系统未能满足预期时，参与者生成了更详细的、语言多样性更高的提问，反映出适应性调整。这些发现表明，尽管先入为主的预期会限制早期交互，但未能满足的预期可以促使行为调整。适当的支持系统可以使用户更广泛地探索LLM的能力。这项工作还提出了聊天搜索用户意图的分类，并强调减轻功能固定性的重要性，以支持更创造性和分析性的LLM使用。', 'title_zh': '被困于期望之中：大语言模型 enabling 的聊天查找中的功能固定性'}
{'arxiv_id': 'arXiv:2504.02051', 'title': 'Self-Resource Allocation in Multi-Agent LLM Systems', 'authors': 'Alfonso Amayuelas, Jingbo Yang, Saaket Agashe, Ashwin Nagarajan, Antonis Antoniades, Xin Eric Wang, William Wang', 'link': 'https://arxiv.org/abs/2504.02051', 'abstract': 'With the development of LLMs as agents, there is a growing interest in connecting multiple agents into multi-agent systems to solve tasks concurrently, focusing on their role in task assignment and coordination. This paper explores how LLMs can effectively allocate computational tasks among multiple agents, considering factors such as cost, efficiency, and performance. In this work, we address key questions, including the effectiveness of LLMs as orchestrators and planners, comparing their effectiveness in task assignment and coordination. Our experiments demonstrate that LLMs can achieve high validity and accuracy in resource allocation tasks. We find that the planner method outperforms the orchestrator method in handling concurrent actions, resulting in improved efficiency and better utilization of agents. Additionally, we show that providing explicit information about worker capabilities enhances the allocation strategies of planners, particularly when dealing with suboptimal workers.', 'abstract_zh': '随着大型语言模型（LLMs）作为代理的发展，人们越来越兴趣于将多个代理连接到多代理系统中并发完成任务，重点在于它们在任务分配和协调中的作用。本文探讨了LLMs如何有效地将计算任务分配给多个代理，考虑了成本、效率和性能等因素。在本研究中，我们关注的关键问题包括LLMs作为调度器和规划者的有效性，比较了它们在任务分配和协调中的效果。我们的实验表明，LLMs在资源分配任务中能够达到高有效性和准确性。我们发现，在处理并发动作时，规划者方法优于调度器方法，从而提高了效率并更好地利用了代理。此外，我们展示了提供关于工人能力的显式信息可以提高规划者的分配策略，尤其是在处理低效工人时。', 'title_zh': '多智能体大语言模型系统中的自我资源分配'}
{'arxiv_id': 'arXiv:2504.01994', 'title': 'PIM-LLM: A High-Throughput Hybrid PIM Architecture for 1-bit LLMs', 'authors': 'Jinendra Malekar, Peyton Chandarana, Md Hasibul Amin, Mohammed E. Elbtity, Ramtin Zand', 'link': 'https://arxiv.org/abs/2504.01994', 'abstract': 'In this paper, we propose PIM-LLM, a hybrid architecture developed to accelerate 1-bit large language models (LLMs). PIM-LLM leverages analog processing-in-memory (PIM) architectures and digital systolic arrays to accelerate low-precision matrix multiplication (MatMul) operations in projection layers and high-precision MatMul operations in attention heads of 1-bit LLMs, respectively. Our design achieves up to roughly 80x improvement in tokens per second and a 70% increase in tokens per joule compared to conventional hardware accelerators. Additionally, PIM-LLM outperforms previous PIM-based LLM accelerators, setting a new benchmark with at least 2x and 5x improvement in GOPS and GOPS/W, respectively.', 'abstract_zh': '本文提出PIM-LLM，一种加速1比特大型语言模型（LLM）的混合架构。PIM-LLM利用模拟处理-in-内存（PIM）架构和数字梭形阵列分别加速1比特LLM中投影层的低精度矩阵乘法（MatMul）操作和注意头的高精度MatMul操作，实现每秒高达约80倍的token性能提升，以及每焦耳70%的token性能提升，相较于传统硬件加速器。此外，PIM-LLM优于现有的PIM基LLM加速器，分别在GOPS和GOPS/W上实现至少2倍和5倍的性能提升。', 'title_zh': 'PIM-LLM: 一种用于1比特大语言模型的高吞吐量混合PIM架构'}
{'arxiv_id': 'arXiv:2504.01986', 'title': 'TuRTLe: A Unified Evaluation of LLMs for RTL Generation', 'authors': "Dario Garcia-Gasulla, Gokcen Kestor, Emanuele Parisi, Miquel Albert'i-Binimelis, Cristian Gutierrez, Razine Moundir Ghorab, Orlando Montenegro, Bernat Homs, Miquel Moreto", 'link': 'https://arxiv.org/abs/2504.01986', 'abstract': 'The rapid advancements in LLMs have driven the adoption of generative AI in various domains, including Electronic Design Automation (EDA). Unlike traditional software development, EDA presents unique challenges, as generated RTL code must not only be syntactically correct and functionally accurate but also synthesizable by hardware generators while meeting performance, power, and area constraints. These additional requirements introduce complexities that existing code-generation benchmarks often fail to capture, limiting their effectiveness in evaluating LLMs for RTL generation. To address this gap, we propose TuRTLe, a unified evaluation framework designed to systematically assess LLMs across key RTL generation tasks. TuRTLe integrates multiple existing benchmarks and automates the evaluation process, enabling a comprehensive assessment of LLM performance in syntax correctness, functional correctness, synthesis, PPA optimization, and exact line completion. Using this framework, we benchmark a diverse set of open LLMs and analyze their strengths and weaknesses in EDA-specific tasks. Our results show that reasoning-based models, such as DeepSeek R1, consistently outperform others across multiple evaluation criteria, but at the cost of increased computational overhead and inference latency. Additionally, base models are better suited in module completion tasks, while instruct-tuned models perform better in specification-to-RTL tasks.', 'abstract_zh': 'LLMs迅猛发展推动了生成式AI在电子设计自动化(EDA)领域的应用。与传统软件开发不同，EDA面临着独特的挑战，生成的RTL代码不仅需要语法正确、功能准确，还需要能够被硬件合成器合成，并满足性能、功耗和面积的约束。这些额外的要求使得现有的代码生成基准往往无法完全捕捉到这些复杂性，限制了它们在评价LLMs在RTL生成方面的有效性。为弥补这一短板，我们提出了TuRTLe，一个统一的评价框架，旨在系统地评估LLMs在关键RTL生成任务中的性能。TuRTLe集成了多个现有的基准，并自动化了评价过程，能够全面评估LLMs在语法正确性、功能正确性、合成、PPA优化和精确行完成等方面的性能。利用此框架，我们对多种开源LLMs进行了基准测试，并分析了它们在EDA特定任务中的强项和弱点。结果显示，基于推理的模型，如DeepSeek R1，在多个评价标准中持续表现出色，但需要增加计算开销和推理延迟。此外，基础模型更适合模块完成任务，而指令调优模型在规格到RTL任务中表现更好。', 'title_zh': 'TuRTLe: LLMs for RTL Generation 的统一评估'}
{'arxiv_id': 'arXiv:2504.01963', 'title': 'LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems', 'authors': 'R. M. Aratchige, W. M. K. S. Ilmini', 'link': 'https://arxiv.org/abs/2504.01963', 'abstract': 'This survey investigates foundational technologies essential for developing effective Large Language Model (LLM)-based multi-agent systems. Aiming to answer how best to optimize these systems for collaborative, dynamic environments, we focus on four critical areas: Architecture, Memory, Planning, and Technologies/Frameworks. By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape. Frameworks like the Mixture of Agents architecture and the ReAct planning model exemplify current innovations, showcasing improvements in role assignment and decision-making. This review synthesizes key strengths and persistent challenges, offering practical recommendations to enhance system scalability, agent collaboration, and adaptability. Our findings provide a roadmap for future research, supporting the creation of robust, efficient multi-agent systems that advance both individual agent performance and collective system resilience.', 'abstract_zh': '本调查探讨了开发有效的大型语言模型（LLM）基于的多agent系统所需的基础技术。着眼于如何最佳地优化这些系统以适应协作和动态环境，我们专注于四大关键领域：架构、内存、规划和技术/框架。通过分析近期的进步及其局限性，如可扩展性、实时响应挑战以及agent协调约束，我们提供了一个详细的技术景观图。诸如Mixture of Agents架构和ReAct规划模型等框架体现了当前的创新，展示了在角色分配和决策方面取得的改进。本综述综合了关键优势和持续挑战，并提供了实用建议以提高系统的可扩展性、agent间的协作和适应性。我们的发现为未来研究提供了一条路线图，支持创建既强大又高效的多agent系统，从而提高个体agent性能和集体系统韧性。', 'title_zh': 'LLMs和谐共进：基于大规模语言模型的多代理系统技术综述'}
