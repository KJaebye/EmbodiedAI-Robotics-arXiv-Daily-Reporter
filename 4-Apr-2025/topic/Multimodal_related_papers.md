# VEGAS: Towards Visually Explainable and Grounded Artificial Social Intelligence 

**Title (ZH)**: VEGAS:向具有视觉可解释性和grounded人工社会智能迈进 

**Authors**: Hao Li, Hao Fei, Zechao Hu, Zhengwei Yang, Zheng Wang  

**Link**: [PDF](https://arxiv.org/pdf/2504.02227)  

**Abstract**: Social Intelligence Queries (Social-IQ) serve as the primary multimodal benchmark for evaluating a model's social intelligence level. While impressive multiple-choice question(MCQ) accuracy is achieved by current solutions, increasing evidence shows that they are largely, and in some cases entirely, dependent on language modality, overlooking visual context. Additionally, the closed-set nature further prevents the exploration of whether and to what extent the reasoning path behind selection is correct. To address these limitations, we propose the Visually Explainable and Grounded Artificial Social Intelligence (VEGAS) model. As a generative multimodal model, VEGAS leverages open-ended answering to provide explainable responses, which enhances the clarity and evaluation of reasoning paths. To enable visually grounded answering, we propose a novel sampling strategy to provide the model with more relevant visual frames. We then enhance the model's interpretation of these frames through Generalist Instruction Fine-Tuning (GIFT), which aims to: i) learn multimodal-language transformations for fundamental emotional social traits, and ii) establish multimodal joint reasoning capabilities. Extensive experiments, comprising modality ablation, open-ended assessments, and supervised MCQ evaluations, consistently show that VEGAS effectively utilizes visual information in reasoning to produce correct and also credible answers. We expect this work to of fer a new perspective on Social-IQ and advance the development of human-like social AI. 

**Abstract (ZH)**: Visually Explainable and Grounded Artificial Social Intelligence (VEGAS)：一种可解释且视觉导向的人工社会智能模型 

---
# OmniCam: Unified Multimodal Video Generation via Camera Control 

**Title (ZH)**: OmniCam：通过摄像头控制实现统一多模态视频生成 

**Authors**: Xiaoda Yang, Jiayang Xu, Kaixuan Luan, Xinyu Zhan, Hongshun Qiu, Shijun Shi, Hao Li, Shuai Yang, Li Zhang, Checheng Yu, Cewu Lu, Lixin Yang  

**Link**: [PDF](https://arxiv.org/pdf/2504.02312)  

**Abstract**: Camera control, which achieves diverse visual effects by changing camera position and pose, has attracted widespread attention. However, existing methods face challenges such as complex interaction and limited control capabilities. To address these issues, we present OmniCam, a unified multimodal camera control framework. Leveraging large language models and video diffusion models, OmniCam generates spatio-temporally consistent videos. It supports various combinations of input modalities: the user can provide text or video with expected trajectory as camera path guidance, and image or video as content reference, enabling precise control over camera motion. To facilitate the training of OmniCam, we introduce the OmniTr dataset, which contains a large collection of high-quality long-sequence trajectories, videos, and corresponding descriptions. Experimental results demonstrate that our model achieves state-of-the-art performance in high-quality camera-controlled video generation across various metrics. 

**Abstract (ZH)**: 相机控制：一种通过改变相机位置和姿态实现多样视觉效果的统一多模态相机控制框架 

---
