{'arxiv_id': 'arXiv:2505.02543', 'title': 'Data-Driven Energy Modeling of Industrial IoT Systems: A Benchmarking Approach', 'authors': 'Dimitris Kallis, Moysis Symeonides, Marios D. Dikaiakos', 'link': 'https://arxiv.org/abs/2505.02543', 'abstract': 'The widespread adoption of IoT has driven the development of cyber-physical systems (CPS) in industrial environments, leveraging Industrial IoTs (IIoTs) to automate manufacturing processes and enhance productivity. The transition to autonomous systems introduces significant operational costs, particularly in terms of energy consumption. Accurate modeling and prediction of IIoT energy requirements are critical, but traditional physics- and engineering-based approaches often fall short in addressing these challenges comprehensively. In this paper, we propose a novel methodology for benchmarking and analyzing IIoT devices and applications to uncover insights into their power demands, energy consumption, and performance. To demonstrate this methodology, we develop a comprehensive framework and apply it to study an industrial CPS comprising an educational robotic arm, a conveyor belt, a smart camera, and a compute node. By creating micro-benchmarks and an end-to-end application within this framework, we create an extensive performance and power consumption dataset, which we use to train and analyze ML models for predicting energy usage from features of the application and the CPS system. The proposed methodology and framework provide valuable insights into the energy dynamics of industrial CPS, offering practical implications for researchers and practitioners aiming to enhance the efficiency and sustainability of IIoT-driven automation.', 'abstract_zh': '物联网的广泛应用推动了工业环境中的赛博物理系统（CPS）的发展，利用工业物联网（IIoTs）自动化工厂流程并提高生产效率。向自主系统过渡带来了显著的运营成本，尤其是在能源消耗方面。准确建模和预测IIoT的能源需求至关重要，但传统基于物理和工程的方法往往未能全面解决这些挑战。在本文中，我们提出了一种新的方法论，用于基准测试和分析IIoT设备和应用，以揭示其功率需求、能源消耗和性能的见解。为了验证此方法论，我们开发了一个综合框架，并将其应用于一个工业CPS的研究，该系统包括一个教育型机器人臂、传送带、智能相机和计算节点。通过在这个框架中创建微基准和端到端应用，我们生成了一个广泛的性能和功耗数据集，用于训练和分析机器学习模型，以从应用程序和CPS系统的特征预测能源消耗。所提出的框架和方法论为工业CPS中的能源动态提供了有价值的认识，对旨在提高IIoT驱动自动化效率和可持续性的研究者和实践者具有实际意义。', 'title_zh': '基于数据驱动的工业物联网系统能量建模：一个基准分析方法'}
{'arxiv_id': 'arXiv:2505.01893', 'title': 'DriveNetBench: An Affordable and Configurable Single-Camera Benchmarking System for Autonomous Driving Networks', 'authors': 'Ali Al-Bustami, Humberto Ruiz-Ochoa, Jaerock Kwon', 'link': 'https://arxiv.org/abs/2505.01893', 'abstract': 'Validating autonomous driving neural networks often demands expensive equipment and complex setups, limiting accessibility for researchers and educators. We introduce DriveNetBench, an affordable and configurable benchmarking system designed to evaluate autonomous driving networks using a single-camera setup. Leveraging low-cost, off-the-shelf hardware, and a flexible software stack, DriveNetBench enables easy integration of various driving models, such as object detection and lane following, while ensuring standardized evaluation in real-world scenarios. Our system replicates common driving conditions and provides consistent, repeatable metrics for comparing network performance. Through preliminary experiments with representative vision models, we illustrate how DriveNetBench effectively measures inference speed and accuracy within a controlled test environment. The key contributions of this work include its affordability, its replicability through open-source software, and its seamless integration into existing workflows, making autonomous vehicle research more accessible.', 'abstract_zh': 'DriveNetBench：一种基于单摄像头配置的经济高效且可配置的自动驾驶神经网络评估系统', 'title_zh': 'DriveNetBench: 一种可配置的单摄像头自动驾驶网络基准测试系统'}
{'arxiv_id': 'arXiv:2505.02274', 'title': 'On the Need for a Statistical Foundation in Scenario-Based Testing of Autonomous Vehicles', 'authors': 'Xingyu Zhao, Robab Aghazadeh-Chakherlou, Chih-Hong Cheng, Peter Popov, Lorenzo Strigini', 'link': 'https://arxiv.org/abs/2505.02274', 'abstract': 'Scenario-based testing has emerged as a common method for autonomous vehicles (AVs) safety, offering a more efficient alternative to mile-based testing by focusing on high-risk scenarios. However, fundamental questions persist regarding its stopping rules, residual risk estimation, debug effectiveness, and the impact of simulation fidelity on safety claims. This paper argues that a rigorous statistical foundation is essential to address these challenges and enable rigorous safety assurance. By drawing parallels between AV testing and traditional software testing methodologies, we identify shared research gaps and reusable solutions. We propose proof-of-concept models to quantify the probability of failure per scenario (pfs) and evaluate testing effectiveness under varying conditions. Our analysis reveals that neither scenario-based nor mile-based testing universally outperforms the other. Furthermore, we introduce Risk Estimation Fidelity (REF), a novel metric to certify the alignment of synthetic and real-world testing outcomes, ensuring simulation-based safety claims are statistically defensible.', 'abstract_zh': '基于场景的测试已成为自动驾驶车辆（AVs）安全评估的常见方法，作为一种更高效的替代方案，它侧重于高风险场景，而不是基于里程的测试。然而，其停止规则、剩余风险估算、调试效果以及仿真保真度对安全声明的影响等根本性问题仍存在。本文认为，严谨的统计基础对于解决这些挑战并实现严格的安全保证是必不可少的。通过将AV测试与传统的软件测试方法学相联系，我们识别出共享的研究空白并提出可重用的解决方案。我们提出概念验证模型来量化每个场景的故障概率（pfs），并在不同条件下评估测试效果。我们的分析表明，基于场景的测试和基于里程的测试均无一能普遍优于另一种。此外，我们引入了风险管理保真度（REF）这一新型指标，以确保合成测试和实际测试结果的一致性，确保基于仿真的安全声明具有统计可信度。', 'title_zh': '基于场景的自动驾驶车辆测试中统计基础的必要性'}
{'arxiv_id': 'arXiv:2505.02050', 'title': 'Enhancing Safety Standards in Automated Systems Using Dynamic Bayesian Networks', 'authors': 'Kranthi Kumar Talluri, Anders L. Madsen, Galia Weidl', 'link': 'https://arxiv.org/abs/2505.02050', 'abstract': "Cut-in maneuvers in high-speed traffic pose critical challenges that can lead to abrupt braking and collisions, necessitating safe and efficient lane change strategies. We propose a Dynamic Bayesian Network (DBN) framework to integrate lateral evidence with safety assessment models, thereby predicting lane changes and ensuring safe cut-in maneuvers effectively. Our proposed framework comprises three key probabilistic hypotheses (lateral evidence, lateral safety, and longitudinal safety) that facilitate the decision-making process through dynamic data processing and assessments of vehicle positions, lateral velocities, relative distance, and Time-to-Collision (TTC) computations. The DBN model's performance compared with other conventional approaches demonstrates superior performance in crash reduction, especially in critical high-speed scenarios, while maintaining a competitive performance in low-speed scenarios. This paves the way for robust, scalable, and efficient safety validation in automated driving systems.", 'abstract_zh': '高速交通中的切线 maneuvers 命令提出了严峻挑战，需要有效的换道策略以避免突然刹车和碰撞。我们提出了一种动态贝叶斯网络（DBN）框架，将侧向证据与安全评估模型集成，从而预测换道行为并有效确保安全的切线 maneuvers。我们的框架包含三个关键的概率假设（侧向证据、侧向安全和纵向安全），通过动态数据处理和对车辆位置、侧向速度、相对距离和碰撞时间（TTC）的计算评估来促进决策过程。与其它传统方法相比，DBN模型在碰撞减少方面表现出更优的性能，尤其是在关键的高速场景中，同时在低速场景中也保持了竞争力。这为自动驾驶系统的稳健、可扩展和高效的安全验证铺平了道路。', 'title_zh': '使用动态贝叶斯网络提升自动系统安全性标准'}
{'arxiv_id': 'arXiv:2505.01453', 'title': 'Safe and Efficient CAV Lane Changing using Decentralised Safety Shields', 'authors': 'Bharathkumar Hegde, Melanie Bouroche', 'link': 'https://arxiv.org/abs/2505.01453', 'abstract': 'Lane changing is a complex decision-making problem for Connected and Autonomous Vehicles (CAVs) as it requires balancing traffic efficiency with safety. Although traffic efficiency can be improved by using vehicular communication for training lane change controllers using Multi-Agent Reinforcement Learning (MARL), ensuring safety is difficult. To address this issue, we propose a decentralised Hybrid Safety Shield (HSS) that combines optimisation and a rule-based approach to guarantee safety. Our method applies control barrier functions to constrain longitudinal and lateral control inputs of a CAV to ensure safe manoeuvres. Additionally, we present an architecture to integrate HSS with MARL, called MARL-HSS, to improve traffic efficiency while ensuring safety. We evaluate MARL-HSS using a gym-like environment that simulates an on-ramp merging scenario with two levels of traffic densities, such as light and moderate densities. The results show that HSS provides a safety guarantee by strictly enforcing a dynamic safety constraint defined on a time headway, even in moderate traffic density that offers challenging lane change scenarios. Moreover, the proposed method learns stable policies compared to the baseline, a state-of-the-art MARL lane change controller without a safety shield. Further policy evaluation shows that our method achieves a balance between safety and traffic efficiency with zero crashes and comparable average speeds in light and moderate traffic densities.', 'abstract_zh': '基于分散式混合安全盾的多代理强化学习在自动驾驶车辆变道中的安全效率平衡', 'title_zh': '基于去中心化安全罩的Safe和高效CAV变道技术'}
{'arxiv_id': 'arXiv:2505.02829', 'title': 'LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery', 'authors': 'Jerome Quenum, Wen-Han Hsieh, Tsung-Han Wu, Ritwik Gupta, Trevor Darrell, David M. Chan', 'link': 'https://arxiv.org/abs/2505.02829', 'abstract': 'Segmentation models can recognize a pre-defined set of objects in images. However, models that can reason over complex user queries that implicitly refer to multiple objects of interest are still in their infancy. Recent advances in reasoning segmentation--generating segmentation masks from complex, implicit query text--demonstrate that vision-language models can operate across an open domain and produce reasonable outputs. However, our experiments show that such models struggle with complex remote-sensing imagery. In this work, we introduce LISAt, a vision-language model designed to describe complex remote-sensing scenes, answer questions about them, and segment objects of interest. We trained LISAt on a new curated geospatial reasoning-segmentation dataset, GRES, with 27,615 annotations over 9,205 images, and a multimodal pretraining dataset, PreGRES, containing over 1 million question-answer pairs. LISAt outperforms existing geospatial foundation models such as RS-GPT4V by over 10.04 % (BLEU-4) on remote-sensing description tasks, and surpasses state-of-the-art open-domain models on reasoning segmentation tasks by 143.36 % (gIoU). Our model, datasets, and code are available at this https URL', 'abstract_zh': '基于视觉-语言模型的复杂遥感场景分割与描述：LISAt模型的研究', 'title_zh': 'LISAT：语言指导下的卫星影像分割助手'}
{'arxiv_id': 'arXiv:2505.02828', 'title': 'Privacy Risks and Preservation Methods in Explainable Artificial Intelligence: A Scoping Review', 'authors': 'Sonal Allana, Mohan Kankanhalli, Rozita Dara', 'link': 'https://arxiv.org/abs/2505.02828', 'abstract': 'Explainable Artificial Intelligence (XAI) has emerged as a pillar of Trustworthy AI and aims to bring transparency in complex models that are opaque by nature. Despite the benefits of incorporating explanations in models, an urgent need is found in addressing the privacy concerns of providing this additional information to end users. In this article, we conduct a scoping review of existing literature to elicit details on the conflict between privacy and explainability. Using the standard methodology for scoping review, we extracted 57 articles from 1,943 studies published from January 2019 to December 2024. The review addresses 3 research questions to present readers with more understanding of the topic: (1) what are the privacy risks of releasing explanations in AI systems? (2) what current methods have researchers employed to achieve privacy preservation in XAI systems? (3) what constitutes a privacy preserving explanation? Based on the knowledge synthesized from the selected studies, we categorize the privacy risks and preservation methods in XAI and propose the characteristics of privacy preserving explanations to aid researchers and practitioners in understanding the requirements of XAI that is privacy compliant. Lastly, we identify the challenges in balancing privacy with other system desiderata and provide recommendations for achieving privacy preserving XAI. We expect that this review will shed light on the complex relationship of privacy and explainability, both being the fundamental principles of Trustworthy AI.', 'abstract_zh': '可解释的人工智能（XAI）已成为值得信赖的人工智能的支柱，旨在为本就难以理解的复杂模型带来透明度。尽管将解释纳入模型具有诸多益处，但在向终端用户提供这些额外信息时，迫切需要解决隐私问题。本文通过范围性回顾现有文献，探讨隐私与可解释性之间的冲突。采用范围性回顾的标准方法，我们从2019年1月至2024年12月期间发表的1,943项研究中提取了57篇文章。本文围绕三个研究问题展开，以帮助读者更深入地了解该主题：（1）在人工智能系统中释放解释的隐私风险是什么？（2）研究人员采用了哪些现有方法来实现XAI系统的隐私保护？（3）什么是隐私保护的解释？基于所选研究的知识综合，我们将XAI中的隐私风险和保护方法进行分类，并提出隐私保护解释的特征，以帮助研究人员和实践者理解符合隐私要求的XAI的要求。最后，我们指出现实中在平衡隐私与其他系统需求时所面临的挑战，并提出实现隐私保护XAI的建议。我们期望本文能揭示隐私与可解释性之间的复杂关系，这两者都是值得信赖的人工智能的基本原则。', 'title_zh': '可解释人工智能中的隐私风险与保存方法：一项范围性回顾'}
{'arxiv_id': 'arXiv:2505.02820', 'title': 'AutoLibra: Agent Metric Induction from Open-Ended Feedback', 'authors': 'Hao Zhu, Phil Cuvin, Xinkai Yu, Charlotte Ka Yee Yan, Jason Zhang, Diyi Yang', 'link': 'https://arxiv.org/abs/2505.02820', 'abstract': 'Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback, e.g., "If you find that the button is disabled, don\'t click it again", or "This agent has too much autonomy to decide what to do on its own", into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent\'s behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: "coverage" and "redundancy". Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra\'s ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.', 'abstract_zh': '基于自动评价框架AutoLibra的智能体评估方法：从开放反馈到细粒度行为评估', 'title_zh': 'AutoLibra：从开放反馈中诱导代理指标'}
{'arxiv_id': 'arXiv:2505.02811', 'title': "Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing", 'authors': 'Diji Yang, Linda Zeng, Jinmeng Rao, Yi Zhang', 'link': 'https://arxiv.org/abs/2505.02811', 'abstract': "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing language models' knowledge and reducing AI generative hallucinations, driving its widespread use. However, complex tasks requiring multi-round retrieval remain challenging, and early attempts tend to be overly optimistic without a good sense of self-skepticism. Current multi-round RAG systems may continue searching even when enough information has already been retrieved, or they may provide incorrect answers without having sufficient information or knowledge. Existing solutions either require large amounts of expensive human-labeled process supervision data or lead to subpar performance.\nThis paper aims to address these limitations by introducing a new framework, \\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and multi-round retrieval capabilities. To train SIM-RAG, we first let a RAG system self-practice multi-round retrieval, augmenting existing question-answer pairs with intermediate inner monologue reasoning steps to generate synthetic training data. For each pair, the system may explore multiple retrieval paths, which are labeled as successful if they reach the correct answer and unsuccessful otherwise. Using this data, we train a lightweight information sufficiency Critic. At inference time, the Critic evaluates whether the RAG system has retrieved sufficient information at each round, guiding retrieval decisions and improving system-level self-awareness through in-context reinforcement learning.\nExperiments across multiple prominent RAG benchmarks show that SIM-RAG is an effective multi-round RAG solution. Furthermore, this framework is system-efficient, adding a lightweight component to RAG without requiring modifications to existing LLMs or search engines, and data-efficient, eliminating the need for costly human-annotated mid-step retrieval process supervision data.", 'abstract_zh': 'Retrieval Augmented Generation with Self-aware Information Management (SIM-RAG): Enhancing Multi-round Retrieval Capabilities', 'title_zh': '了解你不知道的：在多轮RAG中通过自我实践学习何时继续搜索'}
{'arxiv_id': 'arXiv:2505.02781', 'title': 'Local Markov Equivalence and Local Causal Discovery for Identifying Controlled Direct Effects', 'authors': 'Timothée Loranchet, Charles K. Assaad', 'link': 'https://arxiv.org/abs/2505.02781', 'abstract': 'Understanding and identifying controlled direct effects (CDEs) is crucial across numerous scientific domains, including public health. While existing methods can identify these effects from causal directed acyclic graphs (DAGs), the true underlying structure is often unknown in practice. Essential graphs, which represent a Markov equivalence class of DAGs characterized by the same set of d-separations, provide a more practical and realistic alternative. However, learning the full essential graph is computationally intensive and typically depends on strong, untestable assumptions. In this work, we characterize a local class of graphs, defined relative to a target variable, that share a specific subset of d-separations, and introduce a graphical representation of this class, called the local essential graph (LEG). We then present LocPC, a novel algorithm designed to recover the LEG from an observed distribution using only local conditional independence tests. Building on LocPC, we propose LocPC-CDE, an algorithm that discovers the portion of the LEG that is sufficient to identify a CDE, bypassing the need of retrieving the full essential graph. Compared to global methods, our algorithms require less conditional independence tests and operate under weaker assumptions while maintaining theoretical guarantees.', 'abstract_zh': '理解并识别控制直接效应（CDEs）对于公共卫生等多个科学领域而言至关重要。现有的方法可以从因果有向无环图（DAGs）中识别这些效应，但在实践中，真实的底层结构往往未知。本质图代表了一组具有相同d-分离集的DAGs的马尔可夫等价类，提供了更加实际和现实的选择。然而，学习完整的本质图在计算上非常耗时，并且通常依赖于难以验证的强假设。本文我们定义一个相对于目标变量的局部类图，该类图共享特定的d-分离集子集，并引入了这种类图的图形表示，称为局部本质图（LEG）。随后，我们介绍了一种名为LocPC的新算法，利用仅有的局部条件独立性检验从观察分布中恢复LEG。在此基础上，我们提出了LocPC-CDE算法，可以直接发现识别CDE所需的LEG的部分，从而避免检索完整本质图。相较于全局方法，我们的算法需要更少的条件独立性检验，适用的假设条件更弱，同时保持理论保证。', 'title_zh': '局部马尔可夫等价类和局部因果发现识别可控直接效应'}
{'arxiv_id': 'arXiv:2505.02747', 'title': 'The use of Artificial Intelligence for Intervention and Assessment in Individuals with ASD', 'authors': 'Aggeliki Sideraki, Christos-Nikolaos Anagnostopoulos', 'link': 'https://arxiv.org/abs/2505.02747', 'abstract': "This paper explores the use of Artificial Intelligence (AI) as a tool for diagnosis, assessment, and intervention for individuals with Autism Spectrum Disorder (ASD). It focuses particularly on AI's role in early diagnosis, utilizing advanced machine learning techniques and data analysis. Recent studies demonstrate that deep learning algorithms can identify behavioral patterns through biometric data analysis, video-based interaction assessments, and linguistic feature extraction, providing a more accurate and timely diagnosis compared to traditional methods. Additionally, AI automates diagnostic tools, reducing subjective biases and enabling the development of personalized assessment protocols for ASD monitoring. At the same time, the paper examines AI-powered intervention technologies, emphasizing educational robots and adaptive communication tools. Social robotic assistants, such as NAO and Kaspar, have been shown to enhance social skills in children by offering structured, repetitive interactions that reinforce learning. Furthermore, AI-driven Augmentative and Alternative Communication (AAC) systems allow children with ASD to express themselves more effectively, while machine-learning chatbots provide language development support through personalized responses. The study presents research findings supporting the effectiveness of these AI applications while addressing challenges such as long-term evaluation and customization to individual needs. In conclusion, the paper highlights the significance of AI as an innovative tool in ASD diagnosis and intervention, advocating for further research to assess its long-term impact.", 'abstract_zh': '本文探讨了人工智能（AI）作为自闭症谱系障碍（ASD）患者诊断、评估和干预工具的应用。它特别关注AI在早期诊断中的作用，利用先进的机器学习技术和数据分析。近期研究表明，深度学习算法可以通过生物特征数据分析、基于视频的交互评估和语言特征提取来识别行为模式，提供比传统方法更准确和及时的诊断。此外，AI自动化诊断工具，减少了主观偏见，并促进了自闭症谱系障碍监测的个性化评估协议的发展。同时，本文还探讨了基于AI的干预技术，强调了教育机器人和自适应沟通工具。社会机器人辅助者，如NAO和Kaspar，已被证明通过提供结构化、重复的互动来增强儿童的社会技能。此外，基于AI的辅助和替代沟通（AAC）系统使自闭症谱系障碍患者能够更有效地表达自己，而机器学习聊天机器人则通过个性化响应提供语言发展支持。研究呈现了支持这些AI应用有效性的研究成果，同时也指出了长期评估和针对个体需求定制等方面的挑战。最终，本文强调了AI在自闭症谱系障碍诊断和干预中的创新工具意义，并倡导进一步研究以评估其长期影响。', 'title_zh': '使用人工智能进行自闭症谱系障碍个体的干预和评估'}
{'arxiv_id': 'arXiv:2505.02609', 'title': 'Study of the influence of a biased database on the prediction of standard algorithms for selecting the best candidate for an interview', 'authors': 'Shuyu Wang, Angélique Saillet, Philomène Le Gall, Alain Lacroux, Christelle Martin-Lacroux, Vincent Brault', 'link': 'https://arxiv.org/abs/2505.02609', 'abstract': 'Artificial intelligence is used at various stages of the recruitment process to automatically select the best candidate for a position, with companies guaranteeing unbiased recruitment. However, the algorithms used are either trained by humans or are based on learning from past experiences that were biased. In this article, we propose to generate data mimicking external (discrimination) and internal biases (self-censorship) in order to train five classic algorithms and to study the extent to which they do or do not find the best candidates according to objective criteria. In addition, we study the influence of the anonymisation of files on the quality of predictions.', 'abstract_zh': '人工智能在招聘过程中用于自动筛选最佳候选人，企业保证招聘过程无偏见。然而，使用的算法要么由人类训练，要么基于有偏见的过往经验学习。本文提出生成模拟外部（歧视）和内部偏见（自我审查）的数据来训练五种经典算法，并研究它们根据客观标准是否能够找到最佳候选人的程度。此外，我们研究文件匿名化对预测质量的影响。', 'title_zh': '带偏见的数据集对标准算法选拔面试最佳候选人的预测影响研究'}
{'arxiv_id': 'arXiv:2505.02581', 'title': 'Agentic Neurodivergence as a Contingent Solution to the AI Alignment Problem', 'authors': 'Alberto Hernández-Espinosa, Felipe S. Abrahão, Olaf Witkowski, Hector Zenil', 'link': 'https://arxiv.org/abs/2505.02581', 'abstract': "The AI alignment problem, which focusses on ensuring that artificial intelligence (AI), including AGI and ASI, systems act according to human values, presents profound challenges. With the progression from narrow AI to Artificial General Intelligence (AGI) and Superintelligence, fears about control and existential risk have escalated. This paper demonstrates that achieving complete alignment is inherently unattainable due to mathematical principles rooted in the foundations of predicate logic and computability, in particular Turing's computational universality, Gödel's incompleteness and Chaitin's randomness. Instead, we argue that embracing AI misalignment or agent's `neurodivergence' as a contingent strategy, defined as fostering a dynamic ecosystem of competing, partially aligned agents, is a possible only viable path to mitigate risks. Through mathematical proofs and an experimental design, we explore how misalignment may serve and should be promoted as a counterbalancing mechanism to team up with whichever agents are most aligned AI to human values, ensuring that no single system dominates destructively. The main premise of our contribution is that misalignment is inevitable because full AI-human alignment is a mathematical impossibility from Turing-complete systems which we also prove in this paper, a feature then inherited to AGI and ASI systems. We introduce and test `change-of-opinion' attacks based on this kind of perturbation and intervention analysis to study how agents may neutralise friendly or unfriendly AIs through cooperation, competition or malice.", 'abstract_zh': '人工智能对齐问题：由于图灵完备性、哥德尔不完备性和查特林随机性基础逻辑原理的限制，从狭窄人工智能到人工通用智能和超级智能的转变带来了根本性的挑战。我们将证明实现完全对齐是先天不可行的，转而建议将人工智能不对齐或代理的“神经多样性”作为一种临时策略，培养竞争性的部分对齐代理生态系统，作为降低风险的可能唯一途径。通过数学证明和实验设计，我们探讨了不对齐如何作为一种平衡机制发挥作用，与最符合人类价值观的代理合作，防止单一系统破坏性地占主导地位。我们的主要贡献前提是，由于图灵完备系统的数学不可能性，完整的人工智能-人类对齐是不可避免的，这一特性也扩展到人工通用智能和超级智能系统。我们提出了基于这种类型扰动和干预分析的“意见改变”攻击，并测试了它们，以研究代理如何通过合作、竞争或恶意途径中和友好或敌对的人工智能。', 'title_zh': '代理神经多样性作为AI一致性问题的条件性解决方案'}
{'arxiv_id': 'arXiv:2505.02462', 'title': 'Incentivizing Inclusive Contributions in Model Sharing Markets', 'authors': 'Enpei Zhang, Jingyi Chai, Rui Ye, Yanfeng Wang, Siheng Chen', 'link': 'https://arxiv.org/abs/2505.02462', 'abstract': "While data plays a crucial role in training contemporary AI models, it is acknowledged that valuable public data will be exhausted in a few years, directing the world's attention towards the massive decentralized private data. However, the privacy-sensitive nature of raw data and lack of incentive mechanism prevent these valuable data from being fully exploited. Addressing these challenges, this paper proposes inclusive and incentivized personalized federated learning (iPFL), which incentivizes data holders with diverse purposes to collaboratively train personalized models without revealing raw data. iPFL constructs a model-sharing market by solving a graph-based training optimization and incorporates an incentive mechanism based on game theory principles. Theoretical analysis shows that iPFL adheres to two key incentive properties: individual rationality and truthfulness. Empirical studies on eleven AI tasks (e.g., large language models' instruction-following tasks) demonstrate that iPFL consistently achieves the highest economic utility, and better or comparable model performance compared to baseline methods. We anticipate that our iPFL can serve as a valuable technique for boosting future AI models on decentralized private data while making everyone satisfied.", 'abstract_zh': '包容性和激励性的个性化联邦学习（iPFL）', 'title_zh': '激励模型共享市场中的包容性贡献'}
{'arxiv_id': 'arXiv:2505.02443', 'title': 'Investigating the Impact of Personalized AI Tutors on Language Learning Performance', 'authors': 'Simon Suh', 'link': 'https://arxiv.org/abs/2505.02443', 'abstract': 'Driven by the global shift towards online learning prompted by the COVID 19 pandemic, Artificial Intelligence has emerged as a pivotal player in the field of education. Intelligent Tutoring Systems offer a new method of personalized teaching, replacing the limitations of traditional teaching methods. However, concerns arise about the ability of AI tutors to address skill development and engagement during the learning process. In this paper, I will conduct a quasi experiment with paired sample t test on 34 students pre and post use of AI tutors in language learning platforms like Santa and Duolingo to examine the relationship between students engagement, academic performance, and students satisfaction during a personalized language learning experience.', 'abstract_zh': '受COVID-19疫情推动的全球在线学习转型影响，人工智能在教育领域 emerged as a pivotal player. 智能辅导系统提供了个性化教学的新方法，取代了传统教学方法的限制。然而，人们对AI辅导能否在学习过程中有效促进技能发展和提升学生参与度表示担忧。本文通过在语言学习平台Santa和Duolingo上对学生进行配对样本t检验的准实验研究，探讨个性化语言学习体验中学生参与度、学术表现和满意度之间的关系。', 'title_zh': '探究个性化AI导师对语言学习绩效的影响'}
{'arxiv_id': 'arXiv:2505.02439', 'title': 'ReeM: Ensemble Building Thermodynamics Model for Efficient HVAC Control via Hierarchical Reinforcement Learning', 'authors': 'Yang Deng, Yaohui Liu, Rui Liang, Dafang Zhao, Donghua Xie, Ittetsu Taniguchi, Dan Wang', 'link': 'https://arxiv.org/abs/2505.02439', 'abstract': 'The building thermodynamics model, which predicts real-time indoor temperature changes under potential HVAC (Heating, Ventilation, and Air Conditioning) control operations, is crucial for optimizing HVAC control in buildings. While pioneering studies have attempted to develop such models for various building environments, these models often require extensive data collection periods and rely heavily on expert knowledge, making the modeling process inefficient and limiting the reusability of the models. This paper explores a model ensemble perspective that utilizes existing developed models as base models to serve a target building environment, thereby providing accurate predictions while reducing the associated efforts. Given that building data streams are non-stationary and the number of base models may increase, we propose a Hierarchical Reinforcement Learning (HRL) approach to dynamically select and weight the base models. Our approach employs a two-tiered decision-making process: the high-level focuses on model selection, while the low-level determines the weights of the selected models. We thoroughly evaluate the proposed approach through offline experiments and an on-site case study, and the experimental results demonstrate the effectiveness of our method.', 'abstract_zh': '基于现有模型的层次强化学习构建热力学模型以优化建筑HVAC控制', 'title_zh': 'ReeM: 基于层次强化学习的高效 HVAC 控制热力学模型集成方法'}
{'arxiv_id': 'arXiv:2505.02413', 'title': 'Task-Oriented Semantic Communication in Large Multimodal Models-based Vehicle Networks', 'authors': 'Baoxia Du, Hongyang Du, Dusit Niyato, Ruidong Li', 'link': 'https://arxiv.org/abs/2505.02413', 'abstract': "Task-oriented semantic communication has emerged as a fundamental approach for enhancing performance in various communication scenarios. While recent advances in Generative Artificial Intelligence (GenAI), such as Large Language Models (LLMs), have been applied to semantic communication designs, the potential of Large Multimodal Models (LMMs) remains largely unexplored. In this paper, we investigate an LMM-based vehicle AI assistant using a Large Language and Vision Assistant (LLaVA) and propose a task-oriented semantic communication framework to facilitate efficient interaction between users and cloud servers. To reduce computational demands and shorten response time, we optimize LLaVA's image slicing to selectively focus on areas of utmost interest to users. Additionally, we assess the importance of image patches by combining objective and subjective user attention, adjusting energy usage for transmitting semantic information. This strategy optimizes resource utilization, ensuring precise transmission of critical information. We construct a Visual Question Answering (VQA) dataset for traffic scenarios to evaluate effectiveness. Experimental results show that our semantic communication framework significantly increases accuracy in answering questions under the same channel conditions, performing particularly well in environments with poor Signal-to-Noise Ratios (SNR). Accuracy can be improved by 13.4% at an SNR of 12dB and 33.1% at 10dB, respectively.", 'abstract_zh': '基于Large Multimodal模型的任务导向语义通信：以LLaVA为Vehicle AI助理的框架探究', 'title_zh': '基于大型多模态模型的车辆网络中的任务导向语义通信'}
{'arxiv_id': 'arXiv:2505.02279', 'title': 'A survey of agent interoperability protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP)', 'authors': 'Abul Ehtesham, Aditi Singh, Gaurav Kumar Gupta, Saket Kumar', 'link': 'https://arxiv.org/abs/2505.02279', 'abstract': 'Large language model (LLM)-powered autonomous agents demand robust, standardized protocols to integrate tools, share contextual data, and coordinate tasks across heterogeneous systems. Ad-hoc integrations are difficult to scale, secure, and generalize across domains. This survey examines four emerging agent communication protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP), each addressing interoperability in distinct deployment contexts. MCP provides a JSON-RPC client-server interface for secure tool invocation and typed data exchange. ACP introduces REST-native messaging via multi-part messages and asynchronous streaming to support multimodal agent responses. A2A enables peer-to-peer task outsourcing through capability-based Agent Cards, facilitating enterprise-scale workflows. ANP supports open-network agent discovery and secure collaboration using decentralized identifiers (DIDs) and JSON-LD graphs. The protocols are compared across multiple dimensions, including interaction modes, discovery mechanisms, communication patterns, and security models. Based on the comparative analysis, a phased adoption roadmap is proposed: beginning with MCP for tool access, followed by ACP for multimodal messaging, A2A for collaborative task execution, and extending to ANP for decentralized agent marketplaces. This work provides a comprehensive foundation for designing secure, interoperable, and scalable ecosystems of LLM-powered agents.', 'abstract_zh': '大型语言模型（LLM）驱动的自主代理需要稳健且标准化的协议来集成工具、共享上下文数据并跨异构系统协调任务。临时集成难以扩展、安全且跨域通用。本文综述了四种新兴的代理通信协议：模型上下文协议（MCP）、代理通信协议（ACP）、代理到代理协议（A2A）和代理网络协议（ANP），它们各自针对不同的部署情景解决互操作性问题。MCP 提供了一个安全的工具调用和类型化数据交换的 JSON-RPC 客户端-服务器接口。ACP 通过多部分消息和异步流式传输引入 REST 本机消息传递，以支持多模态代理响应。A2A 通过能力基础的代理卡片实现端到端任务外包，促进企业规模的工作流。ANP 使用去中心化标识符（DIDs）和 JSON-LD 图表支持开放网络的代理发现和安全协作。本文从交互模式、发现机制、通信模式和安全模型等多个维度对这些协议进行了比较，并基于比较分析提出了分阶段采用路线图：从 MCP 开始用于工具访问，随后是 ACP 用于多模态消息传递，接着是 A2A 用于协作任务执行，最终扩展到 ANP 用于去中心化的代理市场。本文为设计安全、互操作且可扩展的 LLM 驱动代理生态系统提供了全面的基础。', 'title_zh': '代理互操作协议综述：模型-context协议（MCP）、代理通信协议（ACP）、代理到代理协议（A2A）和代理网络协议（ANP）'}
{'arxiv_id': 'arXiv:2505.02215', 'title': 'Interpretable Emergent Language Using Inter-Agent Transformers', 'authors': 'Mannan Bhardwaj', 'link': 'https://arxiv.org/abs/2505.02215', 'abstract': 'This paper explores the emergence of language in multi-agent reinforcement learning (MARL) using transformers. Existing methods such as RIAL, DIAL, and CommNet enable agent communication but lack interpretability. We propose Differentiable Inter-Agent Transformers (DIAT), which leverage self-attention to learn symbolic, human-understandable communication protocols. Through experiments, DIAT demonstrates the ability to encode observations into interpretable vocabularies and meaningful embeddings, effectively solving cooperative tasks. These results highlight the potential of DIAT for interpretable communication in complex multi-agent environments.', 'abstract_zh': '基于变压器的可解释多智能体强化学习中的语言涌现：不同可微跨智能体变压器的研究', 'title_zh': '可解释的 emergence 语言通过代理间变换器'}
{'arxiv_id': 'arXiv:2505.02121', 'title': 'Overview of AI Grading of Physics Olympiad Exams', 'authors': 'Lachlan McGinness', 'link': 'https://arxiv.org/abs/2505.02121', 'abstract': "Automatically grading the diverse range of question types in high school physics problem is a challenge that requires automated grading techniques from different fields. We report the findings of a Systematic Literature Review of potential physics grading techniques. We propose a multi-modal AI grading framework to address these challenges and examine our framework in light of Australia's AI Ethical Principles.", 'abstract_zh': '自动评估高中物理问题中多样化题型的技术是一个挑战，需要不同领域的自动评估技术。我们报告了一项系统文献审查中潜在物理评估技术的研究成果。我们提出了一个多元模态AI评估框架以应对这些挑战，并基于澳大利亚的AI伦理原则对其进行审查。', 'title_zh': '物理奥林匹克竞赛中AI评阅概览'}
{'arxiv_id': 'arXiv:2505.02110', 'title': 'Eterna is Solved', 'authors': 'Tristan Cazenave', 'link': 'https://arxiv.org/abs/2505.02110', 'abstract': 'RNA design consists of discovering a nucleotide sequence that folds into a target secondary structure. It is useful for synthetic biology, medicine, and nanotechnology. We propose Montparnasse, a Multi Objective Generalized Nested Rollout Policy Adaptation with Limited Repetition (MOGNRPALR) RNA design algorithm. It solves the Eterna benchmark.', 'abstract_zh': 'RNA设计涉及发现一个核苷酸序列，使其折叠成目标二级结构。它在合成生物学、医学和纳米技术中非常有用。我们提出了一种多目标广义嵌套卷出策略适应有限重复（MOGNRPALR）RNA设计算法，它解决了Eterna基准问题。', 'title_zh': 'Eterna被破解'}
{'arxiv_id': 'arXiv:2505.02062', 'title': 'Ethical AI in the Healthcare Sector: Investigating Key Drivers of Adoption through the Multi-Dimensional Ethical AI Adoption Model (MEAAM)', 'authors': 'Prathamesh Muzumdar, Apoorva Muley, Kuldeep Singh, Sumanth Cheemalapati', 'link': 'https://arxiv.org/abs/2505.02062', 'abstract': 'The adoption of Artificial Intelligence (AI) in the healthcare service industry presents numerous ethical challenges, yet current frameworks often fail to offer a comprehensive, empirical understanding of the multidimensional factors influencing ethical AI integration. Addressing this critical research gap, this study introduces the Multi-Dimensional Ethical AI Adoption Model (MEAAM), a novel theoretical framework that categorizes 13 critical ethical variables across four foundational dimensions of Ethical AI Fair AI, Responsible AI, Explainable AI, and Sustainable AI. These dimensions are further analyzed through three core ethical lenses: epistemic concerns (related to knowledge, transparency, and system trustworthiness), normative concerns (focused on justice, autonomy, dignity, and moral obligations), and overarching concerns (highlighting global, systemic, and long-term ethical implications). This study adopts a quantitative, cross-sectional research design using survey data collected from healthcare professionals and analyzed via Partial Least Squares Structural Equation Modeling (PLS-SEM). Employing PLS-SEM, this study empirically investigates the influence of these ethical constructs on two outcomes Operational AI Adoption and Systemic AI Adoption. Results indicate that normative concerns most significantly drive operational adoption decisions, while overarching concerns predominantly shape systemic adoption strategies and governance frameworks. Epistemic concerns play a facilitative role, enhancing the impact of ethical design principles on trust and transparency in AI systems. By validating the MEAAM framework, this research advances a holistic, actionable approach to ethical AI adoption in healthcare and provides critical insights for policymakers, technologists, and healthcare administrators striving to implement ethically grounded AI solutions.', 'abstract_zh': '人工智能在医疗服务体系中的采用面临诸多伦理挑战，现有框架往往未能提供对影响伦理AI集成的多维因素的全面、经验性理解。本研究通过引入多维度伦理AI采用模型（MEAAM），提出一个新颖的理论框架，该框架将13个关键伦理变量分类为四个基础维度：公平AI、负责任AI、可解释AI和可持续AI。进一步通过三个核心伦理视角进行分析：知识论关切（知识、透明度和系统可信性）、规范关切（公正、自主、尊严和道德义务），以及总体关切（强调全球、系统性和长期伦理影响）。本研究采用定量的横截面研究设计，通过向医疗专业人员收集问卷数据，并利用偏最小二乘结构方程建模（PLS-SEM）进行分析。本研究通过PLS-SEM，实证研究这些伦理构念对操作性AI采用和系统性AI采用的影响。研究结果表明，规范关切对操作性采用决策影响最大，而总体关切则主要塑造系统性采用策略和治理框架。知识论关切则起着促进作用，增强伦理设计原则对AI系统的信任和透明度的影响。通过验证MEAAM框架，本研究为医疗领域伦理AI采用提供了一个综合性、可操作的途径，并为寻求实施伦理导向AI解决方案的政策制定者、技术专家和医疗管理人员提供了关键见解。', 'title_zh': '医疗保健领域中的伦理AI：通过多维度伦理AI采纳模型（MEAAM）探究关键驱动因素'}
{'arxiv_id': 'arXiv:2505.02052', 'title': 'TxP: Reciprocal Generation of Ground Pressure Dynamics and Activity Descriptions for Improving Human Activity Recognition', 'authors': 'Lala Shakti Swarup Ray, Lars Krupp, Vitor Fortes Rey, Bo Zhou, Sungho Suh, Paul Lukowicz', 'link': 'https://arxiv.org/abs/2505.02052', 'abstract': 'Sensor-based human activity recognition (HAR) has predominantly focused on Inertial Measurement Units and vision data, often overlooking the capabilities unique to pressure sensors, which capture subtle body dynamics and shifts in the center of mass. Despite their potential for postural and balance-based activities, pressure sensors remain underutilized in the HAR domain due to limited datasets. To bridge this gap, we propose to exploit generative foundation models with pressure-specific HAR techniques. Specifically, we present a bidirectional Text$\\times$Pressure model that uses generative foundation models to interpret pressure data as natural language. TxP accomplishes two tasks: (1) Text2Pressure, converting activity text descriptions into pressure sequences, and (2) Pressure2Text, generating activity descriptions and classifications from dynamic pressure maps. Leveraging pre-trained models like CLIP and LLaMA 2 13B Chat, TxP is trained on our synthetic PressLang dataset, containing over 81,100 text-pressure pairs. Validated on real-world data for activities such as yoga and daily tasks, TxP provides novel approaches to data augmentation and classification grounded in atomic actions. This consequently improved HAR performance by up to 12.4\\% in macro F1 score compared to the state-of-the-art, advancing pressure-based HAR with broader applications and deeper insights into human movement.', 'abstract_zh': '基于传感器的人体活动识别（HAR）主要关注惯性测量单元和视觉数据，往往忽视了压力传感器的独特能力，后者能够捕获细微的体质动态和质心转移。尽管压力传感器在姿势和平衡相关的活动上具有潜力，但在HAR领域中仍因数据集有限而未得到充分利用。为解决这一问题，我们提议利用生成基础模型与压力特定的HAR技术相结合。具体而言，我们提出了一种双向Text×Pressure模型，该模型使用生成基础模型将压力数据解释为自然语言。TxP完成两项任务：（1）Text2Pressure，将活动文本描述转换为压力序列；（2）Pressure2Text，从动态压力图生成活动描述和分类。利用预训练模型如CLIP和LLaMA 2 13B Chat进行训练，TxP在包含超过81,100个文本-压力对的合成PressLang数据集上进行训练。在瑜伽和日常任务等真实世界数据上验证，TxP提供了基于原子动作的数据增强和分类的新方法。这进而使HAR性能相对于最先进的方法在宏F1得分上提高了12.4%，推动了基于压力的HAR在更广泛的应用和更深入的人体运动理解方面的发展。', 'title_zh': 'TxP: 相互生成地面压力动力学和活动描述以提高人类活动识别'}
{'arxiv_id': 'arXiv:2505.02003', 'title': 'Closed-loop control of seizure activity via real-time seizure forecasting by reservoir neuromorphic computing', 'authors': 'Maryam Sadeghi, Darío Fernández Khatiboun, Yasser Rezaeiyan, Saima Rizwan, Alessandro Barcellona, Andrea Merello, Marco Crepaldi, Gabriella Panuccio, Farshad Moradi', 'link': 'https://arxiv.org/abs/2505.02003', 'abstract': 'Closed-loop brain stimulation holds potential as personalized treatment for drug-resistant epilepsy (DRE) but still suffers from limitations that result in highly variable efficacy. First, stimulation is typically delivered upon detection of the seizure to abort rather than prevent it; second, the stimulation parameters are established by trial and error, requiring lengthy rounds of fine-tuning, which delay steady-state therapeutic efficacy. Here, we address these limitations by leveraging the potential of neuromorphic computing. We present a system capable of driving personalized free-run stimulations based on seizure forecasting, wherein each forecast triggers an electrical pulse rather than an arbitrarily predefined fixed-frequency stimulus train. We validate the system against hippocampal spheroids coupled to 3D microelectrode array as a simplified testbed, showing that it can achieve seizure reduction >97% while primarily using instantaneous stimulation frequencies within 20 Hz, well below what typically used in clinical settings. Our work demonstrates the potential of neuromorphic systems as a next-generation neuromodulation strategy for personalized DRE treatment.', 'abstract_zh': '闭合回路脑刺激有潜力成为难治性癫痫的个性化治疗方案，但仍然受到限制，导致疗效高度变异。首先，刺激通常在检测到发作时进行以中止而非预防；其次，刺激参数是通过试错方法确定的，需要长时间的精细调整，这延缓了稳定状态的治疗效果。为此，我们通过利用类脑计算的潜力来解决这些限制。我们提出了一种基于发作预报的个性化自由运行刺激系统，其中每次预报触发一个电脉冲而非预先定义的固定频率刺激串。我们使用与三维微电极阵列耦合的海马体球状体作为简化测试平台进行了验证，结果显示该系统可在主要使用瞬时刺激频率低于20 Hz的情况下实现>97%的发作减少。我们的研究展示了类脑系统作为下一代个性化难治性癫痫治疗的神经调制策略的潜力。', 'title_zh': '通过水库神经形态计算实现的实时癫痫发作预测控制'}
{'arxiv_id': 'arXiv:2505.01712', 'title': 'World Model-Based Learning for Long-Term Age of Information Minimization in Vehicular Networks', 'authors': 'Lingyi Wang, Rashed Shelim, Walid Saad, Naren Ramakrishnan', 'link': 'https://arxiv.org/abs/2505.01712', 'abstract': 'Traditional reinforcement learning (RL)-based learning approaches for wireless networks rely on expensive trial-and-error mechanisms and real-time feedback based on extensive environment interactions, which leads to low data efficiency and short-sighted policies. These limitations become particularly problematic in complex, dynamic networks with high uncertainty and long-term planning requirements. To address these limitations, in this paper, a novel world model-based learning framework is proposed to minimize packet-completeness-aware age of information (CAoI) in a vehicular network. Particularly, a challenging representative scenario is considered pertaining to a millimeter-wave (mmWave) vehicle-to-everything (V2X) communication network, which is characterized by high mobility, frequent signal blockages, and extremely short coherence time. Then, a world model framework is proposed to jointly learn a dynamic model of the mmWave V2X environment and use it to imagine trajectories for learning how to perform link scheduling. In particular, the long-term policy is learned in differentiable imagined trajectories instead of environment interactions. Moreover, owing to its imagination abilities, the world model can jointly predict time-varying wireless data and optimize link scheduling in real-world wireless and V2X networks. Thus, during intervals without actual observations, the world model remains capable of making efficient decisions. Extensive experiments are performed on a realistic simulator based on Sionna that integrates physics-based end-to-end channel modeling, ray-tracing, and scene geometries with material properties. Simulation results show that the proposed world model achieves a significant improvement in data efficiency, and achieves 26% improvement and 16% improvement in CAoI, respectively, compared to the model-based RL (MBRL) method and the model-free RL (MFRL) method.', 'abstract_zh': '基于世界模型的学习框架在车联网中最小化包完成感知的信息老化（CAoI）', 'title_zh': '基于世界模型的学习在vehicular网络中长期信息老化最小化'}
{'arxiv_id': 'arXiv:2505.01651', 'title': 'Human-AI Governance (HAIG): A Trust-Utility Approach', 'authors': 'Zeynep Engin', 'link': 'https://arxiv.org/abs/2505.01651', 'abstract': 'This paper introduces the HAIG framework for analysing trust dynamics across evolving human-AI relationships. Current categorical frameworks (e.g., "human-in-the-loop" models) inadequately capture how AI systems evolve from tools to partners, particularly as foundation models demonstrate emergent capabilities and multi-agent systems exhibit autonomous goal-setting behaviours. As systems advance, agency redistributes in complex patterns that are better represented as positions along continua rather than discrete categories, though progression may include both gradual shifts and significant step changes. The HAIG framework operates across three levels: dimensions (Decision Authority Distribution, Process Autonomy, and Accountability Configuration), continua (gradual shifts along each dimension), and thresholds (critical points requiring governance adaptation). Unlike risk-based or principle-based approaches, HAIG adopts a trust-utility orientation, focusing on maintaining appropriate trust relationships that maximise utility while ensuring sufficient safeguards. Our analysis reveals how technical advances in self-supervision, reasoning authority, and distributed decision-making drive non-uniform trust evolution across both contextual variation and technological advancement. Case studies in healthcare and European regulation demonstrate how HAIG complements existing frameworks while offering a foundation for alternative approaches that anticipate governance challenges before they emerge.', 'abstract_zh': '基于HAIG框架的人机关系信任动态分析', 'title_zh': '人类-人工智能治理：信任-效用方法'}
{'arxiv_id': 'arXiv:2505.01468', 'title': 'One Search Fits All: Pareto-Optimal Eco-Friendly Model Selection', 'authors': 'Filippo Betello, Antonio Purificato, Vittoria Vineis, Gabriele Tolomei, Fabrizio Silvestri', 'link': 'https://arxiv.org/abs/2505.01468', 'abstract': 'The environmental impact of Artificial Intelligence (AI) is emerging as a significant global concern, particularly regarding model training. In this paper, we introduce GREEN (Guided Recommendations of Energy-Efficient Networks), a novel, inference-time approach for recommending Pareto-optimal AI model configurations that optimize validation performance and energy consumption across diverse AI domains and tasks. Our approach directly addresses the limitations of current eco-efficient neural architecture search methods, which are often restricted to specific architectures or tasks. Central to this work is EcoTaskSet, a dataset comprising training dynamics from over 1767 experiments across computer vision, natural language processing, and recommendation systems using both widely used and cutting-edge architectures. Leveraging this dataset and a prediction model, our approach demonstrates effectiveness in selecting the best model configuration based on user preferences. Experimental results show that our method successfully identifies energy-efficient configurations while ensuring competitive performance.', 'abstract_zh': '人工智能的环境影响正成为全球性的重要关切，特别是在模型训练方面。本文介绍了GREEN（引导推荐高效网络配置），这是一种新型的推理时方法，用于推荐在不同AI领域和任务中优化验证性能和能效的帕累托最优AI模型配置。本文的方法直接解决了现有生态友好型神经架构搜索方法的局限性，这些方法通常局限于特定架构或任务。本工作中核心的是EcoTaskSet数据集，该数据集包含来自超过1767项实验的训练动态，实验覆盖了计算机视觉、自然语言处理和推荐系统，并使用了广泛使用的和前沿的架构。借助该数据集和预测模型，我们的方法展示了根据用户偏好选择最佳模型配置的有效性。实验结果表明，我们的方法能够成功识别高效能配置，同时保持竞争力。', 'title_zh': '一种搜索方案适用于所有情况：帕累托最优环保模型选择'}
{'arxiv_id': 'arXiv:2505.02824', 'title': 'Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models', 'authors': 'Kuofeng Gao, Yufei Zhu, Yiming Li, Jiawang Bai, Yong Yang, Zhifeng Li, Shu-Tao Xia', 'link': 'https://arxiv.org/abs/2505.02824', 'abstract': 'Text-to-image (T2I) diffusion models have rapidly advanced, enabling high-quality image generation conditioned on textual prompts. However, the growing trend of fine-tuning pre-trained models for personalization raises serious concerns about unauthorized dataset usage. To combat this, dataset ownership verification (DOV) has emerged as a solution, embedding watermarks into the fine-tuning datasets using backdoor techniques. These watermarks remain inactive under benign samples but produce owner-specified outputs when triggered. Despite the promise of DOV for T2I diffusion models, its robustness against copyright evasion attacks (CEA) remains unexplored. In this paper, we explore how attackers can bypass these mechanisms through CEA, allowing models to circumvent watermarks even when trained on watermarked datasets. We propose the first copyright evasion attack (i.e., CEAT2I) specifically designed to undermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three stages: watermarked sample detection, trigger identification, and efficient watermark mitigation. A key insight driving our approach is that T2I models exhibit faster convergence on watermarked samples during the fine-tuning, evident through intermediate feature deviation. Leveraging this, CEAT2I can reliably detect the watermarked samples. Then, we iteratively ablate tokens from the prompts of detected watermarked samples and monitor shifts in intermediate features to pinpoint the exact trigger tokens. Finally, we adopt a closed-form concept erasure method to remove the injected watermark. Extensive experiments show that our CEAT2I effectively evades DOV mechanisms while preserving model performance.', 'abstract_zh': '版权规避攻击：针对T2I扩散模型的CEAT2I', 'title_zh': '面向个性化文本生成图像扩散模型的数据集版权绕过攻击'}
{'arxiv_id': 'arXiv:2505.02780', 'title': 'Beyond the Monitor: Mixed Reality Visualization and AI for Enhanced Digital Pathology Workflow', 'authors': 'Jai Prakash Veerla, Partha Sai Guttikonda, Helen H. Shang, Mohammad Sadegh Nasr, Cesar Torres, Jacob M. Luber', 'link': 'https://arxiv.org/abs/2505.02780', 'abstract': "Pathologists rely on gigapixel whole-slide images (WSIs) to diagnose diseases like cancer, yet current digital pathology tools hinder diagnosis. The immense scale of WSIs, often exceeding 100,000 X 100,000 pixels, clashes with the limited views traditional monitors offer. This mismatch forces constant panning and zooming, increasing pathologist cognitive load, causing diagnostic fatigue, and slowing pathologists' adoption of digital methods. PathVis, our mixed-reality visualization platform for Apple Vision Pro, addresses these challenges. It transforms the pathologist's interaction with data, replacing cumbersome mouse-and-monitor navigation with intuitive exploration using natural hand gestures, eye gaze, and voice commands in an immersive workspace. PathVis integrates AI to enhance diagnosis. An AI-driven search function instantly retrieves and displays the top five similar patient cases side-by-side, improving diagnostic precision and efficiency through rapid comparison. Additionally, a multimodal conversational AI assistant offers real-time image interpretation support and aids collaboration among pathologists across multiple Apple devices. By merging the directness of traditional pathology with advanced mixed-reality visualization and AI, PathVis improves diagnostic workflows, reduces cognitive strain, and makes pathology practice more effective and engaging. The PathVis source code and a demo video are publicly available at: this https URL", 'abstract_zh': '病理学家依赖 gigapixel 全切片图像（WSIs）来诊断癌症等疾病，但当前的数字病理学工具妨碍了诊断过程。WSIs 的巨大规模，通常超过 100,000 × 100,000 像素，与传统显示器所提供的有限视野冲突。这种不匹配迫使病理学家不断进行平移和缩放，增加了病理学家的认知负担，导致诊断疲劳，并减缓了病理学家对数字方法的采用。我们的苹果 Vision Pro 混合现实可视化平台 PathVis 解决了这些挑战。它改变了病理学家与数据的互动方式，将繁琐的鼠标和显示器导航替换为通过自然的手势、眼动和语音命令在沉浸式工作空间中进行直观探索。PathVis 结合了 AI，以提高诊断效果。其 AI 驱动的搜索功能可以即时检索并并排显示最相似的五个病例，通过快速比较提高诊断精确度和效率。此外，多模态对话式 AI 辅助提供了实时图像解释支持，并帮助病理学家在多个苹果设备上进行协作。通过将传统病理学的直接性与先进的混合现实可视化和 AI 相结合，PathVis 改进了诊断流程，减少了认知负担，并使病理学实践更加高效和引人入胜。PathVis 的源代码和演示视频可以在以下链接公开获取：this https URL。', 'title_zh': '超越监视：混合现实可视化与AI在增强数字病理学工作流程中的应用'}
{'arxiv_id': 'arXiv:2505.02712', 'title': 'Graph Neural Network-Based Reinforcement Learning for Controlling Biological Networks: The GATTACA Framework', 'authors': 'Andrzej Mizera, Jakub Zarzycki', 'link': 'https://arxiv.org/abs/2505.02712', 'abstract': 'Cellular reprogramming, the artificial transformation of one cell type into another, has been attracting increasing research attention due to its therapeutic potential for complex diseases. However, discovering reprogramming strategies through classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we explore the use of deep reinforcement learning (DRL) to control Boolean network models of complex biological systems, such as gene regulatory networks and signalling pathway networks. We formulate a novel control problem for Boolean network models under the asynchronous update mode in the context of cellular reprogramming. To facilitate scalability, we consider our previously introduced concept of a pseudo-attractor and we improve our procedure for effective identification of pseudo-attractor states. Finally, we devise a computational framework to solve the control problem. To leverage the structure of biological systems, we incorporate graph neural networks with graph convolutions into the artificial neural network approximator for the action-value function learned by the DRL agent. Experiments on a number of large real-world biological networks from literature demonstrate the scalability and effectiveness of our approach.', 'abstract_zh': '细胞重编程的深度强化学习控制：基于异步更新模式的布尔网络模型探索', 'title_zh': '基于图神经网络的强化学习在调控生物网络中的应用：GATTACA框架'}
{'arxiv_id': 'arXiv:2505.02655', 'title': 'SCFormer: Structured Channel-wise Transformer with Cumulative Historical State for Multivariate Time Series Forecasting', 'authors': 'Shiwei Guo, Ziang Chen, Yupeng Ma, Yunfei Han, Yi Wang', 'link': 'https://arxiv.org/abs/2505.02655', 'abstract': 'The Transformer model has shown strong performance in multivariate time series forecasting by leveraging channel-wise self-attention. However, this approach lacks temporal constraints when computing temporal features and does not utilize cumulative historical series this http URL address these limitations, we propose the Structured Channel-wise Transformer with Cumulative Historical state (SCFormer). SCFormer introduces temporal constraints to all linear transformations, including the query, key, and value matrices, as well as the fully connected layers within the Transformer. Additionally, SCFormer employs High-order Polynomial Projection Operators (HiPPO) to deal with cumulative historical time series, allowing the model to incorporate information beyond the look-back window during prediction. Extensive experiments on multiple real-world datasets demonstrate that SCFormer significantly outperforms mainstream baselines, highlighting its effectiveness in enhancing time series forecasting. The code is publicly available at this https URL', 'abstract_zh': '结构化通道卷积历史状态变换模型 (SCFormer)：解决累积历史时间序列的多变量时间序列预测', 'title_zh': 'SCFormer：带累积历史状态的结构化通道变换器多变量时间序列预测'}
{'arxiv_id': 'arXiv:2505.02649', 'title': 'Eye Movements as Indicators of Deception: A Machine Learning Approach', 'authors': 'Valentin Foucher, Santiago de Leon-Martinez, Robert Moro', 'link': 'https://arxiv.org/abs/2505.02649', 'abstract': 'Gaze may enhance the robustness of lie detectors but remains under-studied. This study evaluated the efficacy of AI models (using fixations, saccades, blinks, and pupil size) for detecting deception in Concealed Information Tests across two datasets. The first, collected with Eyelink 1000, contains gaze data from a computerized experiment where 87 participants revealed, concealed, or faked the value of a previously selected card. The second, collected with Pupil Neon, involved 36 participants performing a similar task but facing an experimenter. XGBoost achieved accuracies up to 74% in a binary classification task (Revealing vs. Concealing) and 49% in a more challenging three-classification task (Revealing vs. Concealing vs. Faking). Feature analysis identified saccade number, duration, amplitude, and maximum pupil size as the most important for deception prediction. These results demonstrate the feasibility of using gaze and AI to enhance lie detectors and encourage future research that may improve on this.', 'abstract_zh': '注视可能增强欺骗检测器的稳健性但尚未充分研究。本研究评估了AI模型（使用注视固定、眼跳、眨眼和瞳孔大小）在两个数据集中进行隐藏信息测试时检测欺骗的有效性。第一个数据集使用Eyelink 1000收集，包含来自一项计算机实验的眼球运动数据，87名参与者揭示、隐藏或伪装了一张先前选定的牌的价值。第二个数据集使用Pupil Neon收集，涉及36名参与者执行类似任务但面对实验者。XGBoost在二分类任务（揭示 vs. 隐藏）中达到了高达74%的准确率，在更具挑战性的三分类任务（揭示 vs. 隐藏 vs. 伪装）中达到了49%的准确率。特征分析表明，眼跳的数量、持续时间、幅度和最大瞳孔大小是欺骗预测最重要的特征。这些结果证明了使用眼球运动和AI增强欺骗检测器的可行性，并鼓励未来研究进一步改进这一领域。', 'title_zh': '眼动作为欺骗指标的机器学习方法'}
{'arxiv_id': 'arXiv:2505.02640', 'title': 'Adaptive Budgeted Multi-Armed Bandits for IoT with Dynamic Resource Constraints', 'authors': 'Shubham Vaishnav, Praveen Kumar Donta, Sindri Magnússon', 'link': 'https://arxiv.org/abs/2505.02640', 'abstract': "Internet of Things (IoT) systems increasingly operate in environments where devices must respond in real time while managing fluctuating resource constraints, including energy and bandwidth. Yet, current approaches often fall short in addressing scenarios where operational constraints evolve over time. To address these limitations, we propose a novel Budgeted Multi-Armed Bandit framework tailored for IoT applications with dynamic operational limits. Our model introduces a decaying violation budget, which permits limited constraint violations early in the learning process and gradually enforces stricter compliance over time. We present the Budgeted Upper Confidence Bound (UCB) algorithm, which adaptively balances performance optimization and compliance with time-varying constraints. We provide theoretical guarantees showing that Budgeted UCB achieves sublinear regret and logarithmic constraint violations over the learning horizon. Extensive simulations in a wireless communication setting show that our approach achieves faster adaptation and better constraint satisfaction than standard online learning methods. These results highlight the framework's potential for building adaptive, resource-aware IoT systems.", 'abstract_zh': '物联网（IoT）系统在资源约束（包括能源和带宽）波动的环境中越来越需要实时响应，同时管理动态的操作限制。当前的方法往往难以应对随时间演化的操作约束场景。为此，我们提出了一种新的预算化多臂 bandit 框架，旨在应对具有动态操作限制的物联网应用。该模型引入了递减的违背预算，允许在学习早期有限度地违背约束，并随着时间推移逐渐加强合规性。我们提出了预算化的上确信边界（UCB）算法，该算法能够适应性地平衡性能优化和随时间变化的约束下的合规性。我们提供了理论保证，证明预算化 UCB 在学习期内实现了次线性遗憾和对数级的约束违背。在无线通信环境下的广泛仿真表明，与标准的在线学习方法相比，我们的方法能够更快地适应并更好地满足约束。这些结果强调了该框架在构建适应性强、资源感知的物联网系统方面的潜力。', 'title_zh': '带有动态资源约束的自适应预算化多臂bandits算法在物联网中的应用'}
{'arxiv_id': 'arXiv:2505.02627', 'title': 'A Theoretical Analysis of Compositional Generalization in Neural Networks: A Necessary and Sufficient Condition', 'authors': 'Yuanpeng Li', 'link': 'https://arxiv.org/abs/2505.02627', 'abstract': 'Compositional generalization is a crucial property in artificial intelligence, enabling models to handle novel combinations of known components. While most deep learning models lack this capability, certain models succeed in specific tasks, suggesting the existence of governing conditions. This paper derives a necessary and sufficient condition for compositional generalization in neural networks. Conceptually, it requires that (i) the computational graph matches the true compositional structure, and (ii) components encode just enough information in training. The condition is supported by mathematical proofs. This criterion combines aspects of architecture design, regularization, and training data properties. A carefully designed minimal example illustrates an intuitive understanding of the condition. We also discuss the potential of the condition for assessing compositional generalization before training. This work is a fundamental theoretical study of compositional generalization in neural networks.', 'abstract_zh': '组成泛化是人工 Intelligence 中的一个关键属性，使模型能够处理已知组件的新颖组合。虽然大多数深度学习模型缺乏这种能力，但某些模型在特定任务中取得了成功，暗示了存在控制条件。本文推导出神经网络中组成泛化的必要且充分条件。概念上，该条件要求(i)计算图匹配真实的组成结构，(ii)组件在训练时编码足够的信息。该条件通过数学证明得到了支持。该标准结合了架构设计、正则化和训练数据特性方面的因素。精心设计的最小化示例说明了该条件的直观理解。我们还讨论了该条件在训练前评估组成泛化的潜力。本文是对神经网络中组成泛化的一项基本理论研究。', 'title_zh': '神经网络 compositional 通用性的理论分析：必要充分条件'}
{'arxiv_id': 'arXiv:2505.02573', 'title': 'Rethinking Federated Graph Learning: A Data Condensation Perspective', 'authors': 'Hao Zhang, Xunkai Li, Yinlin Zhu, Lianglin Hu', 'link': 'https://arxiv.org/abs/2505.02573', 'abstract': 'Federated graph learning is a widely recognized technique that promotes collaborative training of graph neural networks (GNNs) by multi-client this http URL, existing approaches heavily rely on the communication of model parameters or gradients for federated optimization and fail to adequately address the data heterogeneity introduced by intricate and diverse graph distributions. Although some methods attempt to share additional messages among the server and clients to improve federated convergence during communication, they introduce significant privacy risks and increase communication overhead. To address these issues, we introduce the concept of a condensed graph as a novel optimization carrier to address FGL data heterogeneity and propose a new FGL paradigm called FedGM. Specifically, we utilize a generalized condensation graph consensus to aggregate comprehensive knowledge from distributed graphs, while minimizing communication costs and privacy risks through a single transmission of the condensed data. Extensive experiments on six public datasets consistently demonstrate the superiority of FedGM over state-of-the-art baselines, highlighting its potential for a novel FGL paradigm.', 'abstract_zh': '联邦图学习是一种广泛认可的技术，通过多客户端促进图神经网络（GNNs）的协作训练。现有方法严重依赖于模型参数或梯度的通信来进行联邦优化，并未能充分解决由复杂的多样图分布引入的数据异质性问题。虽然一些方法尝试在通信过程中在服务器和客户端之间共享额外的消息以提高联邦收敛性，但这些方法带来了显著的隐私风险并增加了通信开销。为解决这些问题，我们引入了压缩图作为新的优化载体以应对FGL数据异质性，并提出了一种新的FGL范式FedGM。具体而言，我们利用广义压缩图共识从分布式图中聚合全面的知识，同时通过一次传输压缩数据来最小化通信成本和隐私风险。在六个公开数据集上的广泛实验一致表明，FedGM在与最新基线的比较中具有优越性，突显了其作为新型FGL范式的潜力。', 'title_zh': '重新思考联邦图学习：一种数据凝练视角'}
{'arxiv_id': 'arXiv:2505.02566', 'title': 'Robustness questions the interpretability of graph neural networks: what to do?', 'authors': 'Kirill Lukyanov, Georgii Sazonov, Serafim Boyarsky, Ilya Makarov', 'link': 'https://arxiv.org/abs/2505.02566', 'abstract': 'Graph Neural Networks (GNNs) have become a cornerstone in graph-based data analysis, with applications in diverse domains such as bioinformatics, social networks, and recommendation systems. However, the interplay between model interpretability and robustness remains poorly understood, especially under adversarial scenarios like poisoning and evasion attacks. This paper presents a comprehensive benchmark to systematically analyze the impact of various factors on the interpretability of GNNs, including the influence of robustness-enhancing defense mechanisms.\nWe evaluate six GNN architectures based on GCN, SAGE, GIN, and GAT across five datasets from two distinct domains, employing four interpretability metrics: Fidelity, Stability, Consistency, and Sparsity. Our study examines how defenses against poisoning and evasion attacks, applied before and during model training, affect interpretability and highlights critical trade-offs between robustness and interpretability. The framework will be published as open source.\nThe results reveal significant variations in interpretability depending on the chosen defense methods and model architecture characteristics. By establishing a standardized benchmark, this work provides a foundation for developing GNNs that are both robust to adversarial threats and interpretable, facilitating trust in their deployment in sensitive applications.', 'abstract_zh': '图神经网络（GNNs）已成为基于图的数据分析的基石，应用于生物信息技术、社交网络和推荐系统等领域。然而，在对抗性场景如污染攻击和规避攻击下，模型可解释性和鲁棒性之间的相互作用仍然不清楚。本文提出了一种全面的基准，系统地分析了各种因素对GNNs可解释性的影响，包括增强鲁棒性的防御机制的作用。', 'title_zh': '鲁棒性对图神经网络的可解释性提出质疑：我们应该怎么办？'}
{'arxiv_id': 'arXiv:2505.02550', 'title': 'Bielik v3 Small: Technical Report', 'authors': 'Krzysztof Ociepa, Łukasz Flis, Remigiusz Kinas, Krzysztof Wróbel, Adrian Gwoździej', 'link': 'https://arxiv.org/abs/2505.02550', 'abstract': 'We introduce Bielik v3, a series of parameter-efficient generative text models (1.5B and 4.5B) optimized for Polish language processing. These models demonstrate that smaller, well-optimized architectures can achieve performance comparable to much larger counterparts while requiring substantially fewer computational resources. Our approach incorporates several key innovations: a custom Polish tokenizer (APT4) that significantly improves token efficiency, Weighted Instruction Cross-Entropy Loss to balance learning across instruction types, and Adaptive Learning Rate that dynamically adjusts based on training progress. Trained on a meticulously curated corpus of 292 billion tokens spanning 303 million documents, these models excel across multiple benchmarks, including the Open PL LLM Leaderboard, Complex Polish Text Understanding Benchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter model achieves results competitive with models 2-3 times its size, while the 1.5B model delivers strong performance despite its extremely compact profile. These advances establish new benchmarks for parameter-efficient language modeling in less-represented languages, making high-quality Polish language AI more accessible for resource-constrained applications.', 'abstract_zh': '我们介绍了Bielik v3，这是一种针对波兰语处理优化的参数高效生成文本模型（包含1.5B和4.5B参数）。这些模型展示了较小且优化良好的架构可以在占用远少于更大模型的计算资源的情况下，达到性能相当的结果。我们的方法包含几个关键创新：一种自定义的波兰语分词器（APT4），它显著提高了分词效率；加权指令交叉熵损失，用于平衡不同指令类型的学习；以及自适应学习率，可以根据训练进程动态调整。这些模型在包含2920亿个令牌、跨越3亿份文档的精心筛选语料库上进行训练，并在包括Open PL LLM Leaderboard、复杂波兰文本理解基准、Polish EQ-Bench和波兰医学领域能力排行榜在内的多个基准测试中表现出色。4.5B参数的模型在结果上与规模为其2-3倍的模型竞争，而1.5B参数的模型即使在极其紧凑的配置下也表现出色。这些进展为较少被代表的语言建立了新的参数高效语言建模标杆，使高质量的波兰语言AI更容易在资源受限的应用中获得。', 'title_zh': 'Bielik v3 Small: 技术报告'}
{'arxiv_id': 'arXiv:2505.02540', 'title': 'Lazy But Effective: Collaborative Personalized Federated Learning with Heterogeneous Data', 'authors': 'Ljubomir Rokvic, Panayiotis Danassis, Boi Faltings', 'link': 'https://arxiv.org/abs/2505.02540', 'abstract': "In Federated Learning, heterogeneity in client data distributions often means that a single global model does not have the best performance for individual clients. Consider for example training a next-word prediction model for keyboards: user-specific language patterns due to demographics (dialect, age, etc.), language proficiency, and writing style result in a highly non-IID dataset across clients. Other examples are medical images taken with different machines, or driving data from different vehicle types. To address this, we propose a simple yet effective personalized federated learning framework (pFedLIA) that utilizes a computationally efficient influence approximation, called `Lazy Influence', to cluster clients in a distributed manner before model aggregation. Within each cluster, data owners collaborate to jointly train a model that captures the specific data patterns of the clients. Our method has been shown to successfully recover the global model's performance drop due to the non-IID-ness in various synthetic and real-world settings, specifically a next-word prediction task on the Nordic languages as well as several benchmark tasks. It matches the performance of a hypothetical Oracle clustering, and significantly improves on existing baselines, e.g., an improvement of 17% on CIFAR100.", 'abstract_zh': '在联邦学习中，客户端数据分布的异质性通常意味着全局模型对个别客户端的最佳性能可能不佳。为了解决这一问题，我们提出了一种简单而有效的个性化联邦学习框架（pFedLIA），该框架利用了一种计算高效的近似影响算法“Lazy Influence”，在模型聚合前按分布方式对客户端进行聚类。在每个集群中，数据所有者合作共同训练一个模型，以捕捉客户端的特定数据模式。我们的方法在多种合成和真实世界设置中有效恢复了由于数据非独立非一致（non-IID）性导致的全局模型性能下降，特别是在北欧语言的下一词预测任务以及多个基准任务中。该方法匹配了假设的Oracle聚类性能，并在现有baseline上取得了显著改进，例如在CIFAR100上的性能提高了17%。', 'title_zh': '懒加载但有效：异质数据下的协作个性化联邦学习'}
{'arxiv_id': 'arXiv:2505.02537', 'title': 'Advancing Constrained Monotonic Neural Networks: Achieving Universal Approximation Beyond Bounded Activations', 'authors': 'Davide Sartor, Alberto Sinigaglia, Gian Antonio Susto', 'link': 'https://arxiv.org/abs/2505.02537', 'abstract': 'Conventional techniques for imposing monotonicity in MLPs by construction involve the use of non-negative weight constraints and bounded activation functions, which pose well-known optimization challenges. In this work, we generalize previous theoretical results, showing that MLPs with non-negative weight constraint and activations that saturate on alternating sides are universal approximators for monotonic functions. Additionally, we show an equivalence between the saturation side in the activations and the sign of the weight constraint. This connection allows us to prove that MLPs with convex monotone activations and non-positive constrained weights also qualify as universal approximators, in contrast to their non-negative constrained counterparts. Our results provide theoretical grounding to the empirical effectiveness observed in previous works while leading to possible architectural simplification. Moreover, to further alleviate the optimization difficulties, we propose an alternative formulation that allows the network to adjust its activations according to the sign of the weights. This eliminates the requirement for weight reparameterization, easing initialization and improving training stability. Experimental evaluation reinforces the validity of the theoretical results, showing that our novel approach compares favourably to traditional monotonic architectures.', 'abstract_zh': '通过构建方法在MLPs中施加单调性的传统技术涉及非负权重约束和有界激活函数，这带来了众所周知的优化挑战。在本文中，我们推广了先前的理论结果，表明具有非负权重约束和在交替侧饱和的激活函数的MLPs是单调函数的普遍逼近器。此外，我们证明了激活函数的饱和侧与其权重约束的符号之间存在等价关系。这种连接使我们能够证明具有凸单调激活函数和非正权重约束的MLPs也符合普遍逼近器的标准，这与其非负权重约束的对应物形成对比。我们的结果为先前工作中观察到的经验有效性提供了理论基础，同时可能导致架构简化。此外，为了进一步缓解优化困难，我们提出了一种替代公式，允许网络根据权重的符号调整其激活。这消除了权重重参数化的需求，简化了初始化并提高了训练稳定性。实验评估证实了理论结果的有效性，表明我们提出的新方法在与传统单调架构的比较中表现出色。', 'title_zh': '增强约束单调神经网络：超越有界激活函数的普遍逼近能力'}
{'arxiv_id': 'arXiv:2505.02485', 'title': 'Integrating Column Generation and Large Neighborhood Search for Bus Driver Scheduling with Complex Break Constraints', 'authors': 'Lucas Kletzander, Tommaso Mannelli Mazzoli, Nysret Musliu, Pascal Van Hentenryck', 'link': 'https://arxiv.org/abs/2505.02485', 'abstract': 'The Bus Driver Scheduling Problem (BDSP) is a combinatorial optimization problem with the goal to design shifts to cover prearranged bus tours. The objective takes into account the operational cost as well as the satisfaction of drivers. This problem is heavily constrained due to strict legal rules and collective agreements. The objective of this article is to provide state-of-the-art exact and hybrid solution methods that can provide high-quality solutions for instances of different sizes. This work presents a comprehensive study of both an exact method, Branch and Price (B&P), as well as a Large Neighborhood Search (LNS) framework which uses B&P or Column Generation (CG) for the repair phase to solve the BDSP. It further proposes and evaluates a novel deeper integration of B&P and LNS, storing the generated columns from the LNS subproblems and reusing them for other subproblems, or to find better global solutions. The article presents a detailed analysis of several components of the solution methods and their impact, including general improvements for the B&P subproblem, which is a high-dimensional Resource Constrained Shortest Path Problem (RCSPP), and the components of the LNS. The evaluation shows that our approach provides new state-of-the-art results for instances of all sizes, including exact solutions for small instances, and low gaps to a known lower bound for mid-sized instances. Conclusions: We observe that B&P provides the best results for small instances, while the tight integration of LNS and CG can provide high-quality solutions for larger instances, further improving over LNS which just uses CG as a black box. The proposed methods are general and can also be applied to other rule sets and related optimization problems', 'abstract_zh': '公交司机调度问题（BDSP）是一个组合优化问题，目标是设计班次以覆盖预先安排的公交旅游。目标不仅考虑运营成本，还考虑司机的满意度。由于严格的法律法规和集体协议限制，该问题受到了严重约束。本文的目标是提供当前最先进的精确和混合解决方案方法，以提供不同规模实例的高质量解决方案。本文对精确方法分支定价（B&P）以及利用B&P或列生成（CG）进行修复阶段的大邻域搜索（LNS）框架进行了全面研究。进一步提出并评估了B&P和LNS更深整合的新方法，存储LNS子问题生成的列并重新用于其他子问题，或寻找更好的全局解决方案。本文详细分析了解决方案方法及其各个组件的影响，包括对B&P子问题的一般改进，这是高维资源受约束最短路径问题（RCSPP）的优化问题，以及LNS的各个组件。评估表明，我们的方法为所有规模的实例提供了新的最先进成果，包括为小实例提供了精确解，并为中等规模实例提供了低偏差的已知下界。结论：我们观察到，对于小实例，B&P提供了最佳结果，而LNS与CG的紧密集成可以为更大实例提供高质量的解决方案，并进一步改进了仅使用CG黑箱的LNS。提出的方法具有通用性，也可应用于其他规则集和其他相关优化问题。', 'title_zh': '基于复杂休息约束的公交车司机调度的列生成与大邻域搜索集成方法'}
{'arxiv_id': 'arXiv:2505.02435', 'title': 'A New Approach to Backtracking Counterfactual Explanations: A Causal Framework for Efficient Model Interpretability', 'authors': 'Pouria Fatemi, Ehsan Sharifian, Mohammad Hossein Yassaee', 'link': 'https://arxiv.org/abs/2505.02435', 'abstract': 'Counterfactual explanations enhance interpretability by identifying alternative inputs that produce different outputs, offering localized insights into model decisions. However, traditional methods often neglect causal relationships, leading to unrealistic examples. While newer approaches integrate causality, they are computationally expensive. To address these challenges, we propose an efficient method based on backtracking counterfactuals that incorporates causal reasoning to generate actionable explanations. We first examine the limitations of existing methods and then introduce our novel approach and its features. We also explore the relationship between our method and previous techniques, demonstrating that it generalizes them in specific scenarios. Finally, experiments show that our method provides deeper insights into model outputs.', 'abstract_zh': '基于回溯反事实的高效因果推理解释方法增强模型可解释性', 'title_zh': '一种新的回溯反事实解释方法：一种高效模型解释的因果框架'}
{'arxiv_id': 'arXiv:2505.02433', 'title': 'FairPO: Robust Preference Optimization for Fair Multi-Label Learning', 'authors': 'Soumen Kumar Mondal, Akshit Varmora, Prateek Chanda, Ganesh Ramakrishnan', 'link': 'https://arxiv.org/abs/2505.02433', 'abstract': 'We propose FairPO, a novel framework designed to promote fairness in multi-label classification by directly optimizing preference signals with a group robustness perspective. In our framework, the set of labels is partitioned into privileged and non-privileged groups, and a preference-based loss inspired by Direct Preference Optimization (DPO) is employed to more effectively differentiate true positive labels from confusing negatives within the privileged group, while preserving baseline classification performance for non-privileged labels. By framing the learning problem as a robust optimization over groups, our approach dynamically adjusts the training emphasis toward groups with poorer performance, thereby mitigating bias and ensuring a fairer treatment across diverse label categories. In addition, we outline plans to extend this approach by investigating alternative loss formulations such as Simple Preference Optimisation (SimPO) and Contrastive Preference Optimization (CPO) to exploit reference-free reward formulations and contrastive training signals. Furthermore, we plan to extend FairPO with multilabel generation capabilities, enabling the model to dynamically generate diverse and coherent label sets for ambiguous inputs.', 'abstract_zh': '我们提出FairPO，这是一种新型框架，旨在通过直接优化偏好信号并从群体稳健性视角促进多标签分类中的公平性。在我们的框架中，标签集被划分为特权组和非特权组，我们采用借鉴直接偏好优化（DPO）理念的基于偏好的损失函数，以更有效地在特权组内区分真实正标签和混淆负标签，同时保持对非特权标签的基本分类性能。通过将学习问题建模为对群体的鲁棒优化，我们的方法动态调整训练重点，朝着表现较差的群体调整，从而减轻偏差并确保不同类型标签的更公平处理。此外，我们计划通过研究替代损失形式，如简单偏好优化（SimPO）和对比偏好优化（CPO），来探索无参考奖励形式和对比训练信号的利用方式。进一步地，我们计划扩展FairPO以具备多标签生成能力，使模型能够为含糊输入动态生成多样且连贯的标签集。', 'title_zh': 'FairPO： robust preference optimization for fair multi-label learning'}
{'arxiv_id': 'arXiv:2505.02426', 'title': 'Towards One-shot Federated Learning: Advances, Challenges, and Future Directions', 'authors': 'Flora Amato, Lingyu Qiu, Mohammad Tanveer, Salvatore Cuomo, Fabio Giampaolo, Francesco Piccialli', 'link': 'https://arxiv.org/abs/2505.02426', 'abstract': 'One-shot FL enables collaborative training in a single round, eliminating the need for iterative communication, making it particularly suitable for use in resource-constrained and privacy-sensitive applications. This survey offers a thorough examination of One-shot FL, highlighting its distinct operational framework compared to traditional federated approaches. One-shot FL supports resource-limited devices by enabling single-round model aggregation while maintaining data locality. The survey systematically categorizes existing methodologies, emphasizing advancements in client model initialization, aggregation techniques, and strategies for managing heterogeneous data distributions. Furthermore, we analyze the limitations of current approaches, particularly in terms of scalability and generalization in non-IID settings. By analyzing cutting-edge techniques and outlining open challenges, this survey aspires to provide a comprehensive reference for researchers and practitioners aiming to design and implement One-shot FL systems, advancing the development and adoption of One-shot FL solutions in a real-world, resource-constrained scenario.', 'abstract_zh': 'One-shot联邦学习使单轮协作训练成为可能，消除了迭代通信的需要，特别适合资源受限和隐私敏感的应用。本文综述了One-shot联邦学习，详细探讨了其与传统联邦学习方法不同的独特操作框架。One-shot联邦学习通过支持单轮模型聚合同时保持数据局部性，为资源有限的设备提供支持。综述系统地分类了现有方法，强调了客户端模型初始化、聚合技术以及异质数据分布管理策略的进步。此外，我们分析了当前方法的局限性，特别是在非IID设置下的可扩展性和泛化能力。通过分析前沿技术和提出开放挑战，本文旨在为希望设计和实现One-shot联邦学习系统的研究人员和实践者提供全面参考，促进One-shot联邦学习解决方案在实际资源受限场景中的发展和应用。', 'title_zh': '面向单次联邦学习：进展、挑战与未来方向'}
{'arxiv_id': 'arXiv:2505.02417', 'title': 'T2S: High-resolution Time Series Generation with Text-to-Series Diffusion Models', 'authors': 'Yunfeng Ge, Jiawei Li, Yiji Zhao, Haomin Wen, Zhao Li, Meikang Qiu, Hongyan Li, Ming Jin, Shirui Pan', 'link': 'https://arxiv.org/abs/2505.02417', 'abstract': 'Text-to-Time Series generation holds significant potential to address challenges such as data sparsity, imbalance, and limited availability of multimodal time series datasets across domains. While diffusion models have achieved remarkable success in Text-to-X (e.g., vision and audio data) generation, their use in time series generation remains in its nascent stages. Existing approaches face two critical limitations: (1) the lack of systematic exploration of general-proposed time series captions, which are often domain-specific and struggle with generalization; and (2) the inability to generate time series of arbitrary lengths, limiting their applicability to real-world scenarios. In this work, we first categorize time series captions into three levels: point-level, fragment-level, and instance-level. Additionally, we introduce a new fragment-level dataset containing over 600,000 high-resolution time series-text pairs. Second, we propose Text-to-Series (T2S), a diffusion-based framework that bridges the gap between natural language and time series in a domain-agnostic manner. T2S employs a length-adaptive variational autoencoder to encode time series of varying lengths into consistent latent embeddings. On top of that, T2S effectively aligns textual representations with latent embeddings by utilizing Flow Matching and employing Diffusion Transformer as the denoiser. We train T2S in an interleaved paradigm across multiple lengths, allowing it to generate sequences of any desired length. Extensive evaluations demonstrate that T2S achieves state-of-the-art performance across 13 datasets spanning 12 domains.', 'abstract_zh': 'Text-to-Time序列生成在解决多模态时间序列数据稀疏性、不平衡性和可用性有限等挑战方面具有重要意义。尽管扩散模型在Text-to-X（如视觉和音频数据）生成方面取得了显著成效，但在时间序列生成中的应用仍处于起步阶段。现有方法面临两个关键限制：（1）缺乏对通用时间序列描述的系统性探索，这些描述往往是领域特定的且难以泛化；（2）无法生成任意长度的时间序列，限制了其在真实世界场景中的应用。在这项工作中，我们首先将时间序列描述分为三个层级：点级、片段级和实例级。此外，我们引入了一个包含超过60万个高分辨率时间序列-文本对的新片段级数据集。其次，我们提出了Text-to-Series（T2S），一个通用的扩散模型框架，通过利用长度自适应变分自编码器将不同长度的时间序列编码为一致的潜在嵌入。T2S还通过利用Flow Matching并采用Diffusion Transformer作为去噪器，有效对齐文本表示与潜在嵌入。我们采用了跨多个长度交替训练的方法来训练T2S，使其能够生成任意长度的序列。广泛评估表明，T2S在12个领域跨越13个数据集上实现了最先进的性能。', 'title_zh': 'T2S: 基于文本到时间序列扩散模型的高分辨率时间序列生成'}
{'arxiv_id': 'arXiv:2505.02390', 'title': 'Quantitative Analysis of Performance Drop in DeepSeek Model Quantization', 'authors': 'Enbo Zhao, Yi Shen, Shuming Shi, Jieyun Huang, Zhihao Chen, Ning Wang, Siqi Xiao, Jian Zhang, Kai Wang, Shiguo Lian', 'link': 'https://arxiv.org/abs/2505.02390', 'abstract': "Recently, there is a high demand for deploying DeepSeek-R1 and V3 locally, possibly because the official service often suffers from being busy and some organizations have data privacy concerns. While single-machine deployment offers infrastructure simplicity, the models' 671B FP8 parameter configuration exceeds the practical memory limits of a standard 8-GPU machine. Quantization is a widely used technique that helps reduce model memory consumption. However, it is unclear what the performance of DeepSeek-R1 and V3 will be after being quantized. This technical report presents the first quantitative evaluation of multi-bitwidth quantization across the complete DeepSeek model spectrum. Key findings reveal that 4-bit quantization maintains little performance degradation versus FP8 while enabling single-machine deployment on standard NVIDIA GPU devices. We further propose DQ3_K_M, a dynamic 3-bit quantization method that significantly outperforms traditional Q3_K_M variant on various benchmarks, which is also comparable with 4-bit quantization (Q4_K_M) approach in most tasks. Moreover, DQ3_K_M supports single-machine deployment configurations for both NVIDIA H100/A100 and Huawei 910B. Our implementation of DQ3\\_K\\_M is released at this https URL, containing optimized 3-bit quantized variants of both DeepSeek-R1 and DeepSeek-V3.", 'abstract_zh': '近期，由于官方服务经常繁忙以及一些组织的数据隐私担忧，对DeepSeek-R1和V3的本地部署需求很高。虽然单机部署简化了基础设施，但其671B FP8参数配置超出了标准8-GPU机器的内存限制。量化是一种常用的技术，有助于减少模型的内存占用。然而，量化后DeepSeek-R1和V3的性能表现尚不清楚。本技术报告首次对DeepSeek模型系列进行全面的多位宽量化定量评估。关键发现表明，4-bit量化与FP8相比性能退化很小，同时能够在标准NVIDIA GPU设备上实现单机部署。此外，我们提出了DQ3_K_M动态3-bit量化方法，在多种基准测试中显著优于传统的Q3_K_M变种，并且在大多数任务中与4-bit量化（Q4_K_M）方法相当。此外，DQ3_K_M方法支持NVIDIA H100/A100和华为910B的单机部署配置。我们实现的DQ3_K_M已发布在以下链接，包含优化的DeepSeek-R1和DeepSeek-V3的3-bit量化版本。', 'title_zh': '深度求索模型量化性能下降的定量分析'}
{'arxiv_id': 'arXiv:2505.02369', 'title': 'Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural Networks', 'authors': 'Juyoung Yun', 'link': 'https://arxiv.org/abs/2505.02369', 'abstract': 'Generalizing well in deep neural networks remains a core challenge, particularly due to their tendency to converge to sharp minima that degrade robustness. Sharpness-Aware Minimization (SAM) mitigates this by seeking flatter minima but perturbs parameters using the full gradient, which can include statistically insignificant directions. We propose ZSharp, a simple yet effective extension to SAM that applies layer-wise Z-score normalization followed by percentile-based filtering to retain only statistically significant gradient components. This selective perturbation aligns updates with curvature-sensitive directions, enhancing generalization without requiring architectural changes. ZSharp introduces only one additional hyperparameter, the percentile threshold, and remains fully compatible with existing SAM variants. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet, VGG, and Vision Transformers show that ZSharp consistently outperforms SAM and its variants in test accuracy, particularly on deeper and transformer-based models. These results demonstrate that ZSharp is a principled and lightweight improvement for sharpness-aware optimization.', 'abstract_zh': 'ZSharp：一种简单有效的Sharpness-Aware Minimization扩展方法，用于增强泛化性能', 'title_zh': '具有Z分数梯度滤波的锐度感知最小化用于神经网络'}
{'arxiv_id': 'arXiv:2505.02360', 'title': 'Catastrophic Overfitting, Entropy Gap and Participation Ratio: A Noiseless $l^p$ Norm Solution for Fast Adversarial Training', 'authors': 'Fares B. Mehouachi, Saif Eddin Jabari', 'link': 'https://arxiv.org/abs/2505.02360', 'abstract': 'Adversarial training is a cornerstone of robust deep learning, but fast methods like the Fast Gradient Sign Method (FGSM) often suffer from Catastrophic Overfitting (CO), where models become robust to single-step attacks but fail against multi-step variants. While existing solutions rely on noise injection, regularization, or gradient clipping, we propose a novel solution that purely controls the $l^p$ training norm to mitigate CO.\nOur study is motivated by the empirical observation that CO is more prevalent under the $l^{\\infty}$ norm than the $l^2$ norm. Leveraging this insight, we develop a framework for generalized $l^p$ attack as a fixed point problem and craft $l^p$-FGSM attacks to understand the transition mechanics from $l^2$ to $l^{\\infty}$. This leads to our core insight: CO emerges when highly concentrated gradients where information localizes in few dimensions interact with aggressive norm constraints. By quantifying gradient concentration through Participation Ratio and entropy measures, we develop an adaptive $l^p$-FGSM that automatically tunes the training norm based on gradient information. Extensive experiments demonstrate that this approach achieves strong robustness without requiring additional regularization or noise injection, providing a novel and theoretically-principled pathway to mitigate the CO problem.', 'abstract_zh': 'adversarial training是鲁棒深度学习的基石，但像快速梯度符号方法（FGSM）这样的快速方法往往遭受灾难性过拟合（CO）的问题，其中模型对单步攻击变得鲁棒，但对多步变体却失效。尽管现有解决方案依赖于噪声注入、正则化或梯度裁剪，我们提出了一种全新的方法，仅通过控制$l^p$训练范数来缓解CO问题。我们的研究动机来自于经验观察，即CO在$l^{\\infty}$范数下比在$l^2$范数下更为普遍。利用这一洞察，我们开发了一种广义$l^p$攻击框架，作为不动点问题，并构建了$l^p$-FGSM攻击以理解从$l^2$到$l^{\\infty}$的过渡机制。这导致我们的核心洞察：当高度集中的梯度与激进的范数约束相互作用时，CO产生。通过使用参与比和熵度量梯度集中度，我们开发了一种自适应的$l^p$-FGSM，该方法根据梯度信息自动调整训练范数。广泛的实验表明，这种方法能够在不需要额外正则化或噪声注入的情况下实现强大的鲁棒性，提供了一种新颖且理论指导的路径来缓解CO问题。', 'title_zh': '灾难性过拟合、熵差距和参与比：无噪声的$l^p$范数解决方案以实现快速对抗训练'}
{'arxiv_id': 'arXiv:2505.02352', 'title': 'Social Biases in Knowledge Representations of Wikidata separates Global North from Global South', 'authors': 'Paramita Das, Sai Keerthana Karnam, Aditya Soni, Animesh Mukherjee', 'link': 'https://arxiv.org/abs/2505.02352', 'abstract': 'Knowledge Graphs have become increasingly popular due to their wide usage in various downstream applications, including information retrieval, chatbot development, language model construction, and many others. Link prediction (LP) is a crucial downstream task for knowledge graphs, as it helps to address the problem of the incompleteness of the knowledge graphs. However, previous research has shown that knowledge graphs, often created in a (semi) automatic manner, are not free from social biases. These biases can have harmful effects on downstream applications, especially by leading to unfair behavior toward minority groups. To understand this issue in detail, we develop a framework -- AuditLP -- deploying fairness metrics to identify biased outcomes in LP, specifically how occupations are classified as either male or female-dominated based on gender as a sensitive attribute. We have experimented with the sensitive attribute of age and observed that occupations are categorized as young-biased, old-biased, and age-neutral. We conduct our experiments on a large number of knowledge triples that belong to 21 different geographies extracted from the open-sourced knowledge graph, Wikidata. Our study shows that the variance in the biased outcomes across geographies neatly mirrors the socio-economic and cultural division of the world, resulting in a transparent partition of the Global North from the Global South.', 'abstract_zh': '知识图谱由于在信息检索、聊天机器人开发、语言模型构建等多种下游应用中的广泛应用而日益流行。链接预测(LP)是知识图谱的一个关键下游任务，有助于解决知识图谱不完整的问题。然而，以往研究显示，知识图谱常常以半自动方式构建，不可避免地存在社会偏见，这些偏见可能对下游应用产生负面影响，尤其是导致对少数群体的不公平对待。为深入理解这一问题，我们开发了一个框架——AuditLP——利用公平性指标识别LP中的偏见结果，特别是基于性别这一敏感属性如何分类职业为男性主导或女性主导。我们还实验了敏感属性年龄，发现职业被分类为年龄偏向年轻、年龄偏向年长和年龄中性。我们在来自开源知识图谱Wikidata的21个不同地理区域的大规模知识三元组上进行了实验。我们的研究发现，偏见结果在不同地理区域之间的差异恰当地反映了世界的社会经济和文化分野，从而清晰地区划分割了全球北方与全球南方。', 'title_zh': 'Wikidata中关于全球北与全球南的社会偏见分离'}
{'arxiv_id': 'arXiv:2505.02347', 'title': 'Temporal Robustness in Discrete Time Linear Dynamical Systems', 'authors': 'Nilava Metya, Arunesh Sinha', 'link': 'https://arxiv.org/abs/2505.02347', 'abstract': 'Discrete time linear dynamical systems, including Markov chains, have found many applications. However, in some problems, there is uncertainty about the time horizon for which the system runs. This creates uncertainty about the cost (or reward) incurred based on the state distribution when the system stops. Given past data samples of how long a system ran, we propose to theoretically analyze a distributional robust cost estimation task in a Wasserstein ambiguity set, instead of learning a probability distribution from a few samples. Towards this, we show an equivalence between a discrete time Markov Chain on a probability simplex and a global asymptotic stable (GAS) discrete time linear dynamical system, allowing us to base our study on a GAS system only. Then, we provide various polynomial time algorithms and hardness results for different cases in our theoretical study, including a fundamental result about Wasserstein distance based polytope.', 'abstract_zh': '离散时间线性动力系统，包括马尔科夫链，已经在许多领域找到了应用。然而，在某些问题中，对于系统运行的时间 horizons 存在不确定性。这造成了基于系统停止时的状态分布所产生成本（或奖励）的不确定性。给定系统运行时间的历史数据样本，我们提出在一个 Wasserstein 不确定性集上理论分析分布鲁棒成本估计任务，而不是从少量样本中学习概率分布。为此，我们展示了概率单纯形上的离散时间马尔科夫链与全局渐近稳定（GAS）的离散时间线性动力系统之间等价，使得我们可以仅基于GAS系统进行研究。然后，我们为我们在理论研究中不同的情况进行提供了多项式时间算法和复杂性结果，包括基于Wasserstein距离的多面体的一个基本结果。', 'title_zh': '离散时间线性动态系统中的时间鲁棒性'}
{'arxiv_id': 'arXiv:2505.02314', 'title': 'NeuroSim V1.5: Improved Software Backbone for Benchmarking Compute-in-Memory Accelerators with Device and Circuit-level Non-idealities', 'authors': 'James Read, Ming-Yen Lee, Wei-Hsing Huang, Yuan-Chun Luo, Anni Lu, Shimeng Yu', 'link': 'https://arxiv.org/abs/2505.02314', 'abstract': "The exponential growth of artificial intelligence (AI) applications has exposed the inefficiency of conventional von Neumann architectures, where frequent data transfers between compute units and memory create significant energy and latency bottlenecks. Analog Computing-in-Memory (ACIM) addresses this challenge by performing multiply-accumulate (MAC) operations directly in the memory arrays, substantially reducing data movement. However, designing robust ACIM accelerators requires accurate modeling of device- and circuit-level non-idealities. In this work, we present NeuroSim V1.5, introducing several key advances: (1) seamless integration with TensorRT's post-training quantization flow enabling support for more neural networks including transformers, (2) a flexible noise injection methodology built on pre-characterized statistical models, making it straightforward to incorporate data from SPICE simulations or silicon measurements, (3) expanded device support including emerging non-volatile capacitive memories, and (4) up to 6.5x faster runtime than NeuroSim V1.4 through optimized behavioral simulation. The combination of these capabilities uniquely enables systematic design space exploration across both accuracy and hardware efficiency metrics. Through multiple case studies, we demonstrate optimization of critical design parameters while maintaining network accuracy. By bridging high-fidelity noise modeling with efficient simulation, NeuroSim V1.5 advances the design and validation of next-generation ACIM accelerators. All NeuroSim versions are available open-source at this https URL.", 'abstract_zh': '人工智能（AI）应用的指数级增长暴露出传统冯·诺伊曼架构的低效率，其中计算单元与内存之间的频繁数据传输造成了显著的能量和延迟瓶颈。计算即存储中的类比计算（ACIM）通过直接在存储阵列中执行乘累加（MAC）操作来解决这一挑战，大幅减少了数据移动。然而，设计稳健的ACIM加速器需要对器件和电路层面的非理想性进行准确建模。在本工作中，我们介绍了NeuroSim V1.5，并引入了数项关键进展：（1）无缝集成与TensorRT的后训练量化流程，支持更多类型的神经网络包括变压器，（2）基于先期定义的统计模型的灵活噪声注入方法，使其容易整合来自SPICE仿真或硅片测量的数据，（3）扩展了设备支持范围，包括新兴的非易失性电容性存储器，（4）通过优化行为仿真，NeuroSim V1.5的运行时间比NeuroSim V1.4快6.5倍。这些功能的结合使设计者能够在准确性和硬件效率指标上进行系统的设计空间探索。通过多项案例研究，我们展示了在保持网络准确性的前提下优化关键设计参数。凭借高保真噪声建模与高效仿真的结合，NeuroSim V1.5推动了下一代ACIM加速器的设计与验证。所有NeuroSim版本均可在以下网址获取。', 'title_zh': 'NeuroSim V1.5: 改进的软件架构，用于在考虑设备和电路非理想性的计算在内存加速器评估中基准测试'}
{'arxiv_id': 'arXiv:2505.02313', 'title': 'What Is AI Safety? What Do We Want It to Be?', 'authors': 'Jacqueline Harding, Cameron Domenico Kirk-Giannini', 'link': 'https://arxiv.org/abs/2505.02313', 'abstract': 'The field of AI safety seeks to prevent or reduce the harms caused by AI systems. A simple and appealing account of what is distinctive of AI safety as a field holds that this feature is constitutive: a research project falls within the purview of AI safety just in case it aims to prevent or reduce the harms caused by AI systems. Call this appealingly simple account The Safety Conception of AI safety. Despite its simplicity and appeal, we argue that The Safety Conception is in tension with at least two trends in the ways AI safety researchers and organizations think and talk about AI safety: first, a tendency to characterize the goal of AI safety research in terms of catastrophic risks from future systems; second, the increasingly popular idea that AI safety can be thought of as a branch of safety engineering. Adopting the methodology of conceptual engineering, we argue that these trends are unfortunate: when we consider what concept of AI safety it would be best to have, there are compelling reasons to think that The Safety Conception is the answer. Descriptively, The Safety Conception allows us to see how work on topics that have historically been treated as central to the field of AI safety is continuous with work on topics that have historically been treated as more marginal, like bias, misinformation, and privacy. Normatively, taking The Safety Conception seriously means approaching all efforts to prevent or mitigate harms from AI systems based on their merits rather than drawing arbitrary distinctions between them.', 'abstract_zh': 'AI安全领域旨在预防或减少AI系统造成的危害。尽管简单且具有吸引力，《安全概念》作为AI安全领域的界定与至少两种AI安全研究人员和组织思考和谈论AI安全的趋势存在紧张关系：首先，倾向于将AI安全研究的目标描述为未来系统带来的灾难性风险；其次，将AI安全视为安全工程的一个分支这一日益流行的观点。采用概念工程的方法，我们认为这些趋势是有遗憾的：当我们考虑最适合的AI安全概念时，有充分理由认为《安全概念》是最佳选择。描述上，《安全概念》使我们能够看到历史上被视为AI安全核心议题的工作与边缘议题如偏见、错误信息和隐私保护工作之间的连续性。规范上，认真对待《安全概念》意味着基于每个努力的 merits 来预防或减轻AI系统造成的危害，而不是人为地划清界限。', 'title_zh': '什么是人工智能安全？我们希望它是什么？'}
{'arxiv_id': 'arXiv:2505.02299', 'title': 'Adaptive Scoring and Thresholding with Human Feedback for Robust Out-of-Distribution Detection', 'authors': 'Daisuke Yamada, Harit Vishwakarma, Ramya Korlakai Vinayak', 'link': 'https://arxiv.org/abs/2505.02299', 'abstract': 'Machine Learning (ML) models are trained on in-distribution (ID) data but often encounter out-of-distribution (OOD) inputs during deployment -- posing serious risks in safety-critical domains. Recent works have focused on designing scoring functions to quantify OOD uncertainty, with score thresholds typically set based solely on ID data to achieve a target true positive rate (TPR), since OOD data is limited before deployment. However, these TPR-based thresholds leave false positive rates (FPR) uncontrolled, often resulting in high FPRs where OOD points are misclassified as ID. Moreover, fixed scoring functions and thresholds lack the adaptivity needed to handle newly observed, evolving OOD inputs, leading to sub-optimal performance. To address these challenges, we propose a human-in-the-loop framework that \\emph{safely updates both scoring functions and thresholds on the fly} based on real-world OOD inputs. Our method maximizes TPR while strictly controlling FPR at all times, even as the system adapts over time. We provide theoretical guarantees for FPR control under stationary conditions and present extensive empirical evaluations on OpenOOD benchmarks to demonstrate that our approach outperforms existing methods by achieving higher TPRs while maintaining FPR control.', 'abstract_zh': '基于人类在环的框架实现在线安全更新的异常输入检测函数与阈值', 'title_zh': '基于人类反馈的自适应评分与阈值调整以实现稳健的分布外检测'}
{'arxiv_id': 'arXiv:2505.02288', 'title': 'Universal Approximation Theorem of Deep Q-Networks', 'authors': 'Qian Qi', 'link': 'https://arxiv.org/abs/2505.02288', 'abstract': 'We establish a continuous-time framework for analyzing Deep Q-Networks (DQNs) via stochastic control and Forward-Backward Stochastic Differential Equations (FBSDEs). Considering a continuous-time Markov Decision Process (MDP) driven by a square-integrable martingale, we analyze DQN approximation properties. We show that DQNs can approximate the optimal Q-function on compact sets with arbitrary accuracy and high probability, leveraging residual network approximation theorems and large deviation bounds for the state-action process. We then analyze the convergence of a general Q-learning algorithm for training DQNs in this setting, adapting stochastic approximation theorems. Our analysis emphasizes the interplay between DQN layer count, time discretization, and the role of viscosity solutions (primarily for the value function $V^*$) in addressing potential non-smoothness of the optimal Q-function. This work bridges deep reinforcement learning and stochastic control, offering insights into DQNs in continuous-time settings, relevant for applications with physical systems or high-frequency data.', 'abstract_zh': '我们通过随机控制和前向后向随机微分方程（FBSDEs）建立了连续时间框架来分析深度Q网络（DQNs）。考虑由可平方积分鞅驱动的连续时间马尔可夫决策过程（MDP），我们分析了DQN的逼近性质。我们证明了DQNs可以在紧集上以任意精度和高概率逼近最优Q函数，利用残差网络逼近定理和状态-动作过程的大偏差界。然后，我们分析了在这种设置下用于训练DQNs的一般Q学习算法的收敛性，适应了随机逼近定理。我们的分析强调了DQN层数、时间离散化以及值函数$V^*$的粘性解在解决最优Q函数潜在非光滑性中的相互作用。这项工作将深度强化学习与随机控制相结合，为连续时间设置下的DQNs提供了见解，适用于物理系统或高频数据的应用。', 'title_zh': '深度Q网络的通用逼近定理'}
{'arxiv_id': 'arXiv:2505.02281', 'title': 'Minimisation of Quasar-Convex Functions Using Random Zeroth-Order Oracles', 'authors': 'Amir Ali Farzin, Yuen-Man Pun, Iman Shames', 'link': 'https://arxiv.org/abs/2505.02281', 'abstract': "This study explores the performance of a random Gaussian smoothing zeroth-order (ZO) scheme for minimising quasar-convex (QC) and strongly quasar-convex (SQC) functions in both unconstrained and constrained settings. For the unconstrained problem, we establish the ZO algorithm's convergence to a global minimum along with its complexity when applied to both QC and SQC functions. For the constrained problem, we introduce the new notion of proximal-quasar-convexity and prove analogous results to the unconstrained case. Specifically, we show the complexity bounds and the convergence of the algorithm to a neighbourhood of a global minimum whose size can be controlled under a variance reduction scheme. Theoretical findings are illustrated through investigating the performance of the algorithm applied to a range of problems in machine learning and optimisation. Specifically, we observe scenarios where the ZO method outperforms gradient descent. We provide a possible explanation for this phenomenon.", 'abstract_zh': '本研究探讨了随机高斯平滑零阶(ZO)方案在不等式约束和等式约束条件下分别对准亚射线状凸（QC）函数和强准亚射线状凸（SQC）函数最小化性能的表现。在不等式约束问题中，我们证明了ZO算法在QC和SQC函数上的全局最小值收敛性及其复杂性。在等式约束问题中，我们引入了邻近准亚射线状凸的新概念，并证明了与不等式约束情况下类似的收敛结果。具体而言，我们展示了在方差减少方案下，算法收敛到全局最小值邻域的复杂性界，并且该邻域大小可以控制。通过在机器学习和优化领域的多种问题上应用该算法进行实验，验证了上述理论发现，并观察到ZO方法在某些情况下优于梯度下降的方法，我们还提供了这一现象可能的解释。', 'title_zh': '利用随机零阶或acles最小化类星体凸函数'}
{'arxiv_id': 'arXiv:2505.02266', 'title': 'Parameter-Efficient Transformer Embeddings', 'authors': 'Henry Ndubuaku, Mouad Talhi', 'link': 'https://arxiv.org/abs/2505.02266', 'abstract': 'Embedding layers in transformer-based NLP models typically account for the largest share of model parameters, scaling with vocabulary size but not yielding performance gains proportional to scale. We propose an alternative approach in which token embedding vectors are first generated deterministically, directly from the token IDs using a Fourier expansion of their normalized values, followed by a lightweight multilayer perceptron (MLP) that captures higher-order interactions. We train standard transformers and our architecture on natural language inference tasks (SNLI and MNLI), and evaluate zero-shot performance on sentence textual similarity (STS-B). Our results demonstrate that the proposed method achieves competitive performance using significantly fewer parameters, trains faster, and operates effectively without the need for dropout. This proof-of-concept study highlights the potential for scalable, memory-efficient language models and motivates further large-scale experimentation based on our findings.', 'abstract_zh': '基于变压器的自然语言处理模型中的嵌入层通常占据模型参数的最大份额，这些嵌入层与词汇量大小成比例增长，但不成比例地提升性能。我们提出了一种替代方法，其中词嵌入向量首先通过其归一化值的傅里叶展开确定性生成，然后通过一个轻量级的多层感知器（MLP）捕捉更高阶的交互。我们在自然语言推理任务（SNLI和MNLI）上训练标准变压器和我们的架构，并在句子文本相似性（STS-B）上评估零样本性能。实验结果表明，所提出的方法使用显著较少的参数实现了具有竞争力的性能，训练速度更快，并且无需dropout即可有效运行。该概念验证研究突显了可扩展且内存高效的语言模型的潜力，并基于我们的发现激励进一步的大规模实验。', 'title_zh': '参数高效Transformer嵌入'}
{'arxiv_id': 'arXiv:2505.02247', 'title': 'RISE: Radius of Influence based Subgraph Extraction for 3D Molecular Graph Explanation', 'authors': 'Jingxiang Qu, Wenhan Gao, Jiaxing Zhang, Xufeng Liu, Hua Wei, Haibin Ling, Yi Liu', 'link': 'https://arxiv.org/abs/2505.02247', 'abstract': "3D Geometric Graph Neural Networks (GNNs) have emerged as transformative tools for modeling molecular data. Despite their predictive power, these models often suffer from limited interpretability, raising concerns for scientific applications that require reliable and transparent insights. While existing methods have primarily focused on explaining molecular substructures in 2D GNNs, the transition to 3D GNNs introduces unique challenges, such as handling the implicit dense edge structures created by a cut-off radius. To tackle this, we introduce a novel explanation method specifically designed for 3D GNNs, which localizes the explanation to the immediate neighborhood of each node within the 3D space. Each node is assigned an radius of influence, defining the localized region within which message passing captures spatial and structural interactions crucial for the model's predictions. This method leverages the spatial and geometric characteristics inherent in 3D graphs. By constraining the subgraph to a localized radius of influence, the approach not only enhances interpretability but also aligns with the physical and structural dependencies typical of 3D graph applications, such as molecular learning.", 'abstract_zh': '三维几何图神经网络(GNNs)已成为建模分子数据的变革性工具。尽管这些模型在预测能力方面表现出色，但在需要可靠和透明洞察的科学应用中，它们的可解释性往往有限，引发关注。虽然现有的方法主要集中在解释2D GNN中的分子亚结构，但向3D GNN的过渡带来了独特挑战，如截断半径创建的隐含密集边结构。为应对这一挑战，我们提出了一个针对3D GNN的新解释方法，该方法将解释局部化为每个节点在三维空间中的邻域。每个节点被赋予一个影响半径，定义了消息传递捕获对于模型预测至关重要的空间和结构交互的局部区域。该方法充分利用了3D图中固有的空间和几何特性。通过限定子图在局部影响半径内，该方法不仅增强了可解释性，还与分子学习等3D图应用中的物理和结构依赖性相一致。', 'title_zh': 'RISE：基于影响半径的3D分子子图提取及其解释'}
{'arxiv_id': 'arXiv:2505.02235', 'title': 'SEval-Ex: A Statement-Level Framework for Explainable Summarization Evaluation', 'authors': 'Tanguy Herserant, Vincent Guigue', 'link': 'https://arxiv.org/abs/2505.02235', 'abstract': 'Evaluating text summarization quality remains a critical challenge in Natural Language Processing. Current approaches face a trade-off between performance and interpretability. We present SEval-Ex, a framework that bridges this gap by decomposing summarization evaluation into atomic statements, enabling both high performance and explainability. SEval-Ex employs a two-stage pipeline: first extracting atomic statements from text source and summary using LLM, then a matching between generated statements. Unlike existing approaches that provide only summary-level scores, our method generates detailed evidence for its decisions through statement-level alignments. Experiments on the SummEval benchmark demonstrate that SEval-Ex achieves state-of-the-art performance with 0.580 correlation on consistency with human consistency judgments, surpassing GPT-4 based evaluators (0.521) while maintaining interpretability. Finally, our framework shows robustness against hallucination.', 'abstract_zh': '评估文本摘要质量仍然是自然语言处理中的一个关键挑战。当前的方法在性能和可解释性之间存在权衡。我们提出了SEval-Ex框架，通过将摘要评估分解为原子陈述，从而同时实现高性能和可解释性。SEval-Ex采用两阶段管道：首先使用LLM从文本来源和摘要中提取原子陈述，然后进行生成的陈述匹配。与现有仅提供摘要级别评分的方法不同，我们的方法通过陈述级别的对齐生成详细的决策依据。在SummEval基准测试上的实验显示，SEval-Ex达到了最先进的性能，相关性达到0.580，超越了基于GPT-4的评估器（0.521），同时保持了可解释性。此外，我们的框架显示了对幻觉的鲁棒性。', 'title_zh': 'SEval-Ex：一种声明级别可解释性摘要评估框架'}
{'arxiv_id': 'arXiv:2505.02230', 'title': 'The GenAI Generation: Student Views of Awareness, Preparedness, and Concern', 'authors': 'Micaela Siraj, Jon Duke', 'link': 'https://arxiv.org/abs/2505.02230', 'abstract': "Generative AI (GenAI) is revolutionizing education and workforce development, profoundly shaping how students learn, engage, and prepare for their future. Outpacing the development of uniform policies and structures, GenAI has heralded a unique era and given rise to the GenAI Generation: a cohort of students whose education has been increasingly shaped by the opportunities and challenges GenAI presents during its widespread adoption within society. This study examines our students' perceptions of GenAI through a concise survey with optional open-ended questions, focusing on their awareness, preparedness, and concerns. Evaluation of more than 250 responses with more than 40% providing detailed qualitative feedback reveals a core dual sentiment: while most students express enthusiasm for GenAI, an even greater proportion voice a spectrum of concerns about ethics, job displacement, and the adequacy of educational structures given the highly transformative technology. These findings offer critical insights into how students view the potential and pitfalls of GenAI for future career impacts, with accompanying recommendations to guide educational institutions in navigating a future driven by GenAI.", 'abstract_zh': '生成式人工智能（GenAI）正在革新教育和劳动力发展，深刻影响着学生的学习、参与和未来准备。GenAI的快速发展超前于统一政策和结构的制定，标志着一个独特时代到来，造就了GenAI一代：一个受广泛社会应用中的GenAI机遇与挑战影响日益密切的学子群体。本研究通过包含可选开放性问题的简要调查，关注学生对GenAI的认知、准备情况和担忧。对超过250份问卷的评价显示，核心观点呈现双面性：尽管大多数学生对GenAI表示热情，但更多比例的学生表达了对伦理、就业替代以及教育结构适应高度变革性技术能力的广泛担忧。这些发现为理解学生眼中的GenAI对未来职业影响的潜力与风险提供了关键见解，并提出建议以指导教育机构应对由GenAI驱动的未来。', 'title_zh': 'GenAI时代：学生对意识、准备与担忧的看法'}
{'arxiv_id': 'arXiv:2505.02206', 'title': 'DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities of Coding Units', 'authors': 'Lei Mao, Yuanhe Tian, Yan Song', 'link': 'https://arxiv.org/abs/2505.02206', 'abstract': "Genome modeling conventionally treats gene sequence as a language, reflecting its structured motifs and long-range dependencies analogous to linguistic units and organization principles such as words and syntax. Recent studies utilize advanced neural networks, ranging from convolutional and recurrent models to Transformer-based models, to capture contextual information of gene sequence, with the primary goal of obtaining effective gene sequence representations and thus enhance the models' understanding of various running gene samples. However, these approaches often directly apply language modeling techniques to gene sequences and do not fully consider the intrinsic information organization in them, where they do not consider how units at different granularities contribute to representation. In this paper, we propose DNAZEN, an enhanced genomic representation framework designed to learn from various granularities in gene sequences, including small polymers and G-grams that are combinations of several contiguous polymers. Specifically, we extract the G-grams from large-scale genomic corpora through an unsupervised approach to construct the G-gram vocabulary, which is used to provide G-grams in the learning process of DNA sequences through dynamically matching from running gene samples. A Transformer-based G-gram encoder is also proposed and the matched G-grams are fed into it to compute their representations and integrated into the encoder for basic unit (E4BU), which is responsible for encoding small units and maintaining the learning and inference process. To further enhance the learning process, we propose whole G-gram masking to train DNAZEN, where the model largely favors the selection of each entire G-gram to mask rather than an ordinary masking mechanism performed on basic units. Experiments on benchmark datasets demonstrate the effectiveness of DNAZEN on various downstream tasks.", 'abstract_zh': '基因组建模通常将基因序列视为一种语言，反映其结构 motif 和长程依赖关系，类似于语言单位和组织原则如单词和句法。近期的研究利用从卷积和递归模型到基于变换器的模型等多种先进的神经网络技术，捕捉基因序列的上下文信息，目标是获得有效的基因序列表示，从而增强模型对各类运行基因样本的理解。然而，这些方法常常直接将语言建模技术应用于基因序列，而未充分考虑其中固有的信息组织方式，忽略了不同粒度单元对表示的贡献。在本文中，我们提出了一种增强的基因组表示框架 DNAZEN，旨在从基因序列的多种粒度中学习，包括小多聚体和由几个连续多聚体组合而成的 G-grams。具体而言，我们通过无监督方法从大规模基因组语料库中提取 G-grams 构建 G-grams 词汇表，该词汇表用于基因序列的学习过程中，通过动态匹配来自运行基因样本的 G-grams 并输入基于变换器的 G-grams 编码器计算它们的表示并集成到负责编码小单元和保持学习与推理过程的基本单元编码器中。为进一步增强学习过程，我们提出了整体 G-grams 掩码来训练 DNAZEN，使模型更倾向于选择整个 G-grams 来掩码，而不是在基本单元上执行的普通掩码机制。在基准数据集上的实验展示了 DNAZEN 在各类下游任务中的有效性。', 'title_zh': 'DNAZEN: 基于编码单元混合粒度的基因序列表示增强方法'}
{'arxiv_id': 'arXiv:2505.02198', 'title': 'Student Perspectives on the Benefits and Risks of AI in Education', 'authors': 'Griffin Pitts, Viktoria Marcus, Sanaz Motamedi', 'link': 'https://arxiv.org/abs/2505.02198', 'abstract': "The use of chatbots equipped with artificial intelligence (AI) in educational settings has increased in recent years, showing potential to support teaching and learning. However, the adoption of these technologies has raised concerns about their impact on academic integrity, students' ability to problem-solve independently, and potential underlying biases. To better understand students' perspectives and experiences with these tools, a survey was conducted at a large public university in the United States. Through thematic analysis, 262 undergraduate students' responses regarding their perceived benefits and risks of AI chatbots in education were identified and categorized into themes.\nThe results discuss several benefits identified by the students, with feedback and study support, instruction capabilities, and access to information being the most cited. Their primary concerns included risks to academic integrity, accuracy of information, loss of critical thinking skills, the potential development of overreliance, and ethical considerations such as data privacy, system bias, environmental impact, and preservation of human elements in education.\nWhile student perceptions align with previously discussed benefits and risks of AI in education, they show heightened concerns about distinguishing between human and AI generated work - particularly in cases where authentic work is flagged as AI-generated. To address students' concerns, institutions can establish clear policies regarding AI use and develop curriculum around AI literacy. With these in place, practitioners can effectively develop and implement educational systems that leverage AI's potential in areas such as immediate feedback and personalized learning support. This approach can enhance the quality of students' educational experiences while preserving the integrity of the learning process with AI.", 'abstract_zh': '人工智能赋能的聊天机器人在教育环境中的应用近年来有所增加，显示出支持教学和学习的潜力。然而，这些技术的采用引发了人们对学术诚信、学生独立解决问题能力和潜在偏见影响的担忧。为了更好地了解学生对这些工具的看法和体验，一项调查在美国一所大型公立大学进行。通过对262名本科生关于人工智能聊天机器人在教育中感知到的优势和风险的回应进行主题分析，识别并分类出多个主题。\n\n结果讨论了学生识别出的多项优势，其中反馈和支持学习、指导能力以及获取信息被提及最多。主要担忧包括对学术诚信的风险、信息准确性、批判性思维技能的丧失、可能的发展过度依赖以及伦理考量，如数据隐私、系统偏见、环境影响和教育中人类元素的保存。\n\n学生对人工智能在教育中的感知与之前讨论的优势和风险相一致，但特别关注区分人类和人工智能生成的工作，尤其是在被标记为人工智能生成的原创工作时。为了缓解学生担忧，机构可以制定明确的人工智能使用政策，并开发人工智能素养课程。有了这些措施，实践者可以有效开发和实施利用人工智能潜力的教育系统，例如即时反馈和个性化学习支持，这有助于提升学生教育体验的质量，同时保持教育过程的完整性。', 'title_zh': 'AI在教育中的益处与风险：学生视角'}
{'arxiv_id': 'arXiv:2505.02170', 'title': 'Data-Driven Team Selection in Fantasy Premier League Using Integer Programming and Predictive Modeling Approach', 'authors': 'Danial Ramezani', 'link': 'https://arxiv.org/abs/2505.02170', 'abstract': "Fantasy football is a billion-dollar industry with millions of participants. Constrained by a fixed budget, decision-makers draft a squad whose players are expected to perform well in the upcoming weeks to maximize total points. This paper proposes novel deterministic and robust integer programming models that select the optimal starting eleven and the captain. A new hybrid scoring metric is constructed using an interpretable artificial intelligence framework and underlying match performance data. Several objective functions and estimation techniques are introduced for the programming model. To the best of my knowledge, this is the first study to approach fantasy football through this lens. The models' performance is evaluated using data from the 2023/24 Premier League season. Results indicate that the proposed hybrid method achieved the highest score while maintaining consistent performance. Utilizing the Monte Carlo simulation, the strategic choice of averaging techniques for estimating cost vectors, and the proposed hybrid approach are shown to be effective during the out-of-sample period. This paper also provides a thorough analysis of the optimal formations and players selected by the models, offering valuable insights into effective fantasy football strategies.", 'abstract_zh': 'Fantasy足球是一项价值十亿美元的产业，拥有数百万参与者。受固定预算的限制，决策者需选拔一支球员队伍，预期这些球员在即将来临的几周内表现良好，以最大化总积分。本文提出了新颖的确定性和鲁棒整数规划模型，用于选择最佳首发十一人和队长。我们使用一种可解释的人工智能框架和潜在的比赛表现数据构建了一种新的混合评分指标。本文引入了几种目标函数和估算技术用于规划模型。据我所知，这是首次从这种角度研究幻想足球的论文。模型的性能通过2023/24赛季英格兰超级联赛的数据进行评估，结果显示提出的混合方法不仅得分最高，而且保持了一致的性能。通过蒙特卡洛模拟，验证了成本向量平均技术的的战略选择及其提出的混合方法在样本外期间的有效性。本文还对模型选择的最优阵型和球员进行了详细分析，提供了有效的幻想足球策略的宝贵见解。', 'title_zh': '使用整数规划和预测建模方法的数据驱动队列表现优化研究'}
{'arxiv_id': 'arXiv:2505.02158', 'title': 'Pickup & Delivery with Time Windows and Transfers: combining decomposition with metaheuristics', 'authors': 'Ioannis Avgerinos, Ioannis Mourtos, Nikolaos Tsompanidis, Georgios Zois', 'link': 'https://arxiv.org/abs/2505.02158', 'abstract': 'This paper examines the generalisation of the Pickup and Delivery Problem that allows mid-route load exchanges among vehicles and obeys strict time-windows at all locations. We propose a novel Logic-Based Benders Decomposition (LBBD) that improves optimality gaps for all benchmarks in the literature and scales up to handle larger ones. To tackle even larger instances, we introduce a refined Large Neighborhood Search (LNS) algorithm that improves the adaptability of LNS beyond case-specific configurations appearing in related literature.\nTo bridge the gap in benchmark availability, we develop an instance generator that allows for extensive experimentation. For moderate datasets (25 and 50 requests), we evaluate the performance of both LBBD and LNS, the former being able to close the gap and the latter capable of providing near-optimal solutions. For larger instances (75 and 100 requests), we recreate indicative state-of-the-art metaheuristics to highlight the improvements introduced by our LNS refinements, while establishing its scalability.', 'abstract_zh': '本文探讨了允许车辆在中途进行货物交换并严格遵守所有地点时间窗的拾取与配送问题的泛化。我们提出了一种新颖的逻辑约束基础Benders分解（LBBD）方法，该方法提高了文献中所有基准的最佳解差距，并能够处理更大的实例。为了应对更大规模的实例，我们引入了一种精炼的大邻域搜索（LNS）算法，提高了LNS的适应性，超越了现有文献中特定配置的局限性。\n\n为弥补基准实例可用性的差距，我们开发了一种实例生成器，以支持广泛的实验。对于中等规模的数据集（25和50个请求），我们评估了LBBD和LNS的性能，前者能够缩小差距，后者能够提供接近最优的解。对于更大的实例（75和100个请求），我们再现了现有的最先进的元启发式算法，突显了我们LNS精炼带来的改进，并验证了其可扩展性。\n\n精炼的大邻域搜索算法', 'title_zh': '带时间窗和转运的 pickups and deliveries: 结合分解法与元启发式算法'}
{'arxiv_id': 'arXiv:2505.02154', 'title': 'Interpreting Multilingual and Document-Length Sensitive Relevance Computations in Neural Retrieval Models through Axiomatic Causal Interventions', 'authors': 'Oliver Savolainen, Dur e Najaf Amjad, Roxana Petcu', 'link': 'https://arxiv.org/abs/2505.02154', 'abstract': 'This reproducibility study analyzes and extends the paper "Axiomatic Causal Interventions for Reverse Engineering Relevance Computation in Neural Retrieval Models," which investigates how neural retrieval models encode task-relevant properties such as term frequency. We reproduce key experiments from the original paper, confirming that information on query terms is captured in the model encoding. We extend this work by applying activation patching to Spanish and Chinese datasets and by exploring whether document-length information is encoded in the model as well. Our results confirm that the designed activation patching method can isolate the behavior to specific components and tokens in neural retrieval models. Moreover, our findings indicate that the location of term frequency generalizes across languages and that in later layers, the information for sequence-level tasks is represented in the CLS token. The results highlight the need for further research into interpretability in information retrieval and reproducibility in machine learning research. Our code is available at this https URL.', 'abstract_zh': '本再现性研究分析并扩展了《公理化因果干预在反向工程神经检索模型相关性计算中的应用》一文，探讨了神经检索模型如何编码任务相关属性（如词频）。我们再现了原始论文中的关键实验，确认查询词的信息被模型捕获。在此基础上，我们将激活补丁方法应用于西班牙语和汉语数据集，并探索文档长度信息是否也在模型中得到编码。我们的实验结果确认了设计的激活补丁方法能够隔离神经检索模型中特定组件和标记的行为。此外，我们的发现还表明词频的位置在不同语言中具有泛化性，在较深层中，序列级任务的信息在CLS标记中得到表示。这些结果突显了信息检索中的可解释性研究以及机器学习研究中再现性研究的必要性。我们的代码可在以下链接获取：this https URL。', 'title_zh': '通过公理化因果干预解析多语言和文档长度敏感的相关性计算在神经检索模型中的意义'}
{'arxiv_id': 'arXiv:2505.02139', 'title': 'Representation Learning of Limit Order Book: A Comprehensive Study and Benchmarking', 'authors': 'Muyao Zhong, Yushi Lin, Peng Yang', 'link': 'https://arxiv.org/abs/2505.02139', 'abstract': 'The Limit Order Book (LOB), the mostly fundamental data of the financial market, provides a fine-grained view of market dynamics while poses significant challenges in dealing with the esteemed deep models due to its strong autocorrelation, cross-feature constrains, and feature scale disparity. Existing approaches often tightly couple representation learning with specific downstream tasks in an end-to-end manner, failed to analyze the learned representations individually and explicitly, limiting their reusability and generalization. This paper conducts the first systematic comparative study of LOB representation learning, aiming to identify the effective way of extracting transferable, compact features that capture essential LOB properties. We introduce LOBench, a standardized benchmark with real China A-share market data, offering curated datasets, unified preprocessing, consistent evaluation metrics, and strong baselines. Extensive experiments validate the sufficiency and necessity of LOB representations for various downstream tasks and highlight their advantages over both the traditional task-specific end-to-end models and the advanced representation learning models for general time series. Our work establishes a reproducible framework and provides clear guidelines for future research. Datasets and code will be publicly available at this https URL.', 'abstract_zh': 'LOBench：金融市场的限价订单簿表示学习标准化基准', 'title_zh': '限价订单簿的表示学习：一项全面研究与基准测试'}
{'arxiv_id': 'arXiv:2505.02129', 'title': 'Subspace Aggregation Query and Index Generation for Multidimensional Resource Space Mode', 'authors': 'Xiaoping Sun, Hai Zhuge', 'link': 'https://arxiv.org/abs/2505.02129', 'abstract': 'Organizing resources in a multidimensional classification space is an approach to efficiently managing and querying large-scale resources. This paper defines an aggregation query on subspace defined by a range on the partial order on coordinate tree at each dimension, where each point contains resources aggregated along the paths of partial order relations on the points so that aggregated resources at each point within the subspace can be measured, ranked and selected. To efficiently locate non-empty points in a large subspace, an approach to generating graph index is proposed to build inclusion links with partial order relations on coordinates of dimensions to enable a subspace query to reach non-empty points by following indexing links and aggregate resources along indexing paths back to their super points. Generating such an index is costly as the number of children of an index node can be very large so that the total number of indexing nodes is unbounded. The proposed approach adopts the following strategies to reduce the cost: (1) adding intersection links between two indexing nodes, which can better reduce query processing costs while controlling the number of nodes of the graph index; (2) intersection links are added between two nodes according to the probabilistic distribution calculated for estimating the costs of adding intersection between two nodes; (3) coordinates at one dimension having more resources are split by coordinates at another dimension to balance the number of resources hold by indexing nodes; and, (4) short-cut links are added between sibling coordinates of coordinate trees to make an efficient query on linear order coordinates. Analysis and experiments verified the effectiveness of the generated index in supporting subspace aggregation query. This work makes significant contributions to the development of data model based on multi-dimensional classification.', 'abstract_zh': '多维分类空间中资源的组织是一种高效管理与查询大规模资源的方法。本文在每个维度上由坐标树的部分有序关系定义的子空间内，定义了一种聚合查询，使得每个点包含沿点的部分有序关系路径聚合的资源，从而可以在子空间内的各点处度量、排序和选择聚合资源。为了高效地定位大型子空间内的非空点，提出了一种生成图索引的方法，利用维度坐标上的部分有序关系构建包含关系链接，使子空间查询可以通过跟随索引链接找到非空点，并沿索引路径回溯聚合资源到其超点。生成这样的索引代价较高，因为索引节点的子孙数量可能非常大，导致索引节点总数不受限。本文提出以下策略以降低成本：(1) 在两个索引节点之间添加交集链接，以更好地减少查询处理成本同时控制图索引的节点数量；(2) 根据用于估算两个节点之间添加交集成本的概率分布来在两个节点之间添加交集链接；(3) 通过在另一个维度的坐标上拆分一个维度上具有更多资源的坐标，以平衡索引节点持有的资源数量；(4) 在坐标树的兄弟坐标之间添加捷径链接，使在线性顺序坐标上的高效查询成为可能。分析和实验验证了生成的索引在支持子空间聚合查询方面的有效性。本工作对基于多维分类的数据模型的发展做出了重要贡献。', 'title_zh': '多维资源空间模式中的子空间聚合查询与索引生成'}
{'arxiv_id': 'arXiv:2505.02120', 'title': 'Tricolore: Multi-Behavior User Profiling for Enhanced Candidate Generation in Recommender Systems', 'authors': 'Xiao Zhou, Zhongxiang Zhao, Hanze Guo', 'link': 'https://arxiv.org/abs/2505.02120', 'abstract': "Online platforms aggregate extensive user feedback across diverse behaviors, providing a rich source for enhancing user engagement. Traditional recommender systems, however, typically optimize for a single target behavior and represent user preferences with a single vector, limiting their ability to handle multiple important behaviors or optimization objectives. This conventional approach also struggles to capture the full spectrum of user interests, resulting in a narrow item pool during candidate generation. To address these limitations, we present Tricolore, a versatile multi-vector learning framework that uncovers connections between different behavior types for more robust candidate generation. Tricolore's adaptive multi-task structure is also customizable to specific platform needs. To manage the variability in sparsity across behavior types, we incorporate a behavior-wise multi-view fusion module that dynamically enhances learning. Moreover, a popularity-balanced strategy ensures the recommendation list balances accuracy with item popularity, fostering diversity and improving overall performance. Extensive experiments on public datasets demonstrate Tricolore's effectiveness across various recommendation scenarios, from short video platforms to e-commerce. By leveraging a shared base embedding strategy, Tricolore also significantly improves the performance for cold-start users. The source code is publicly available at: this https URL.", 'abstract_zh': '在线平台聚合了多样行为的大量用户反馈，为提升用户参与度提供了丰富资源。然而，传统的推荐系统通常仅针对单一行为目标进行优化，并用单一向量表示用户偏好，这限制了它们处理多种重要行为或优化目标的能力。这种传统方法也无法充分捕捉用户广泛的兴趣，导致候选生成时项目池较低。为了克服这些局限性，我们提出Tricolore，一种多功能的多向量学习框架，旨在发现不同行为类型之间的联系，以实现更稳健的候选生成。Tricolore的自适应多任务结构还可以根据具体平台需求进行定制。为了管理不同行为类型在稀疏性上的变化，我们引入了行为间多视图融合模块，以动态增强学习效果。此外，流行度平衡策略确保推荐列表在准确性与项目流行度之间取得平衡，从而促进多样性并提升整体性能。在公开数据集上的广泛实验显示，Tricolore在从短视频平台到电子商务等多种推荐场景下均有效。通过采用共享基础嵌入策略，Tricolore还能显著改善冷启动用户的推荐性能。源代码已公开：this https URL。', 'title_zh': 'Tricolore: 多行为用户画像生成以增强推荐系统中的候选物品生成'}
{'arxiv_id': 'arXiv:2505.02078', 'title': 'LecEval: An Automated Metric for Multimodal Knowledge Acquisition in Multimedia Learning', 'authors': 'Joy Lim Jia Yin, Daniel Zhang-Li, Jifan Yu, Haoxuan Li, Shangqing Tu, Yuanchun Wang, Zhiyuan Liu, Huiqin Liu, Lei Hou, Juanzi Li, Bin Xu', 'link': 'https://arxiv.org/abs/2505.02078', 'abstract': "Evaluating the quality of slide-based multimedia instruction is challenging. Existing methods like manual assessment, reference-based metrics, and large language model evaluators face limitations in scalability, context capture, or bias. In this paper, we introduce LecEval, an automated metric grounded in Mayer's Cognitive Theory of Multimedia Learning, to evaluate multimodal knowledge acquisition in slide-based learning. LecEval assesses effectiveness using four rubrics: Content Relevance (CR), Expressive Clarity (EC), Logical Structure (LS), and Audience Engagement (AE). We curate a large-scale dataset of over 2,000 slides from more than 50 online course videos, annotated with fine-grained human ratings across these rubrics. A model trained on this dataset demonstrates superior accuracy and adaptability compared to existing metrics, bridging the gap between automated and human assessments. We release our dataset and toolkits at this https URL.", 'abstract_zh': '基于幻灯片的多媒体教学质量评估具有挑战性。现有方法如人工评估、参考基指标和大规模语言模型评估器在可扩展性、上下文捕捉或偏见方面存在局限性。本文介绍了一种基于梅耶的认知理论的多媒体学习的自动化评估工具LecEval，用于评估基于幻灯片的学习中的多模态知识获取。LecEval使用四个维度进行效果评估：内容相关性（CR）、表达清晰度（EC）、逻辑结构（LS）和受众参与度（AE）。我们构建了一个包含超过2,000张幻灯片的大规模数据集，并且这些幻灯片被细粒度的人类评价者在这些维度上进行了注解。基于该数据集训练的模型在准确性和适应性方面优于现有指标，填补了自动化评估与人工评估之间的差距。我们在此网址发布我们的数据集和工具包：[链接]。', 'title_zh': 'LecEval：多媒体学习中多模态知识获取的自动化评价指标'}
{'arxiv_id': 'arXiv:2505.02077', 'title': 'Open Challenges in Multi-Agent Security: Towards Secure Systems of Interacting AI Agents', 'authors': 'Christian Schroeder de Witt', 'link': 'https://arxiv.org/abs/2505.02077', 'abstract': "Decentralized AI agents will soon interact across internet platforms, creating security challenges beyond traditional cybersecurity and AI safety frameworks. Free-form protocols are essential for AI's task generalization but enable new threats like secret collusion and coordinated swarm attacks. Network effects can rapidly spread privacy breaches, disinformation, jailbreaks, and data poisoning, while multi-agent dispersion and stealth optimization help adversaries evade oversightcreating novel persistent threats at a systemic level. Despite their critical importance, these security challenges remain understudied, with research fragmented across disparate fields including AI security, multi-agent learning, complex systems, cybersecurity, game theory, distributed systems, and technical AI governance. We introduce \\textbf{multi-agent security}, a new field dedicated to securing networks of decentralized AI agents against threats that emerge or amplify through their interactionswhether direct or indirect via shared environmentswith each other, humans, and institutions, and characterize fundamental security-performance trade-offs. Our preliminary work (1) taxonomizes the threat landscape arising from interacting AI agents, (2) surveys security-performance tradeoffs in decentralized AI systems, and (3) proposes a unified research agenda addressing open challenges in designing secure agent systems and interaction environments. By identifying these gaps, we aim to guide research in this critical area to unlock the socioeconomic potential of large-scale agent deployment on the internet, foster public trust, and mitigate national security risks in critical infrastructure and defense contexts.", 'abstract_zh': '去中心化AI代理即将通过互联网平台进行交互，超越了传统网络安全和AI安全框架，带来了新的安全挑战。自由形式的协议对于AI任务泛化至关重要，但也会引发新的威胁，如秘密勾结和协调蜂群攻击。网络效应可以迅速传播隐私泄露、虚假信息、 Jailbreak 和数据污染，而多代理分散和隐身优化有助于对手逃避监管，在系统层面创造新型持久性威胁。尽管这些安全挑战至关重要，但它们仍研究不足，研究分散在包括AI安全、多代理学习、复杂系统、网络安全、博弈论、分布式系统和技术AI治理等多个领域。我们引入了“多代理安全”，这是一个新的领域，专注于保护由交互的去中心化AI代理组成的网络免受通过它们的交互（直接或间接通过共享环境）而出现或放大的威胁，Characterize基本的安全-性能权衡。我们的初步工作包括：（1）对交互AI代理产生的威胁 landscape 进行分类；（2）概述去中心化AI系统中的安全-性能权衡；（3）提出一个统一的研究议程，以解决设计安全代理系统和交互环境中的开放挑战。通过识别这些空白，我们旨在引导对该关键领域的研究，以解锁大规模代理部署在互联网上的经济社会潜力，促进公众信任，并在关键基础设施和国防领域减轻国家安全风险。', 'title_zh': '多智能体安全中的开放挑战：朝向交互AI代理的安全系统研究'}
{'arxiv_id': 'arXiv:2505.02073', 'title': 'Lightweight Defense Against Adversarial Attacks in Time Series Classification', 'authors': 'Yi Han', 'link': 'https://arxiv.org/abs/2505.02073', 'abstract': 'As time series classification (TSC) gains prominence, ensuring robust TSC models against adversarial attacks is crucial. While adversarial defense is well-studied in Computer Vision (CV), the TSC field has primarily relied on adversarial training (AT), which is computationally expensive. In this paper, five data augmentation-based defense methods tailored for time series are developed, with the most computationally intensive method among them increasing the computational resources by only 14.07% compared to the original TSC model. Moreover, the deployment process for these methods is straightforward. By leveraging these advantages of our methods, we create two combined methods. One of these methods is an ensemble of all the proposed techniques, which not only provides better defense performance than PGD-based AT but also enhances the generalization ability of TSC models. Moreover, the computational resources required for our ensemble are less than one-third of those required for PGD-based AT. These methods advance robust TSC in data mining. Furthermore, as foundation models are increasingly explored for time series feature learning, our work provides insights into integrating data augmentation-based adversarial defense with large-scale pre-trained models in future research.', 'abstract_zh': '随着时间序列分类（TSC）的重要性不断提升，确保TSC模型对抗 adversarial 攻击的鲁棒性变得至关重要。尽管 adversarial 防御在计算机视觉（CV）领域已有深入研究，TSC领域主要依赖于对抗训练（AT），这在计算上相对昂贵。在本文中，我们开发了五种针对时间序列的数据增强防御方法，其中计算最 intensive 的方法相较于原始 TSC 模型仅增加了 14.07% 的计算资源。此外，这些方法的部署过程简单明了。利用我们方法的优势，我们创建了两种结合方法。其中一种方法是所有提出技术的集成，不仅能提供比基于 PGD 的 AT 更好的防御性能，还能提高 TSC 模型的泛化能力。此外，我们方法的计算资源需求仅为基于 PGD 的 AT 的三分之一。这些方法推动了数据挖掘中 TSC 的鲁棒性发展。此外，随着基础模型在时间序列特征学习中的探索逐渐增多，我们的工作为未来研究中将数据增强基于的对抗防御与大规模预训练模型整合提供了见解。', 'title_zh': '时间序列分类中轻量级对抗攻击防御方法'}
{'arxiv_id': 'arXiv:2505.02048', 'title': 'Regression s all you need for medical image translation', 'authors': 'Sebastian Rassmann, David Kügler, Christian Ewert, Martin Reuter', 'link': 'https://arxiv.org/abs/2505.02048', 'abstract': 'The acquisition of information-rich images within a limited time budget is crucial in medical imaging. Medical image translation (MIT) can help enhance and supplement existing datasets by generating synthetic images from acquired data. While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have achieved remarkable success in natural image generation, their benefits - creativity and image realism - do not necessarily transfer to medical applications where highly accurate anatomical information is required. In fact, the imitation of acquisition noise or content hallucination hinder clinical utility. Here, we introduce YODA (You Only Denoise once - or Average), a novel 2.5D diffusion-based framework for volumetric MIT. YODA unites diffusion and regression paradigms to produce realistic or noise-free outputs. Furthermore, we propose Expectation-Approximation (ExpA) DM sampling, which draws inspiration from MRI signal averaging. ExpA-sampling suppresses generated noise and, thus, eliminates noise from biasing the evaluation of image quality. Through extensive experiments on four diverse multi-modal datasets - comprising multi-contrast brain MRI and pelvic MRI-CT - we show that diffusion and regression sampling yield similar results in practice. As such, the computational overhead of diffusion sampling does not provide systematic benefits in medical information translation. Building on these insights, we demonstrate that YODA outperforms several state-of-the-art GAN and DM methods. Notably, YODA-generated images are shown to be interchangeable with, or even superior to, physical acquisitions for several downstream tasks. Our findings challenge the presumed advantages of DMs in MIT and pave the way for the practical application of MIT in medical imaging.', 'abstract_zh': '信息丰富的图像在有限的时间预算内获取对于医学成像至关重要。医学图像转换（MIT）可以通过从获取的数据生成合成图像来增强和补充现有数据集。尽管生成对抗网络（GANs）和扩散模型（DMs）在自然图像生成中取得了显著成功，但它们在医学应用中的优势——创造力和图像真实性——不一定会转移到需要高度准确解剖信息的领域中。实际上，仿照采集噪声或内容错觉会妨碍临床应用。为此，我们介绍了YODA（你只需去噪一次或平均），一种新颖的基于扩散的2.5D体积医学图像转换框架。YODA将扩散和回归范式结合起来，生成逼真的或无噪声的输出。此外，我们提出了期望近似（ExpA）DM采样，该方法从MRI信号平均中汲取灵感。ExpA-采样抑制生成的噪声，从而消除噪声对图像质量评估的偏差。通过在四个不同的多模态数据集上的广泛实验——包括多对比度脑MRI和骨盆MRI-CT——我们展示了扩散和回归采样在实践中效果相似。因此，扩散采样的计算开销在医学信息转换中并没有系统的优势。基于这些见解，我们证明YODA优于几种最先进的GAN和DM方法。值得注意的是，YODA生成的图像在多个后续任务中可以与或超越物理获取。我们的研究结果挑战了扩散模型在医学图像变换中的假设优势，并为医学成像中实际应用医学图像变换铺平了道路。', 'title_zh': '回归即可实现医学图像翻译'}
{'arxiv_id': 'arXiv:2505.02027', 'title': 'GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph In-Context Learning', 'authors': 'Rui Lv, Zaixi Zhang, Kai Zhang, Qi Liu, Weibo Gao, Jiawei Liu, Jiaxia Yan, Linan Yue, Fangzhou Yao', 'link': 'https://arxiv.org/abs/2505.02027', 'abstract': 'Graph In-Context Learning, with the ability to adapt pre-trained graph models to novel and diverse downstream graphs without updating any parameters, has gained much attention in the community. The key to graph in-context learning is to perform downstream graphs conditioned on chosen prompt examples. Existing methods randomly select subgraphs or edges as prompts, leading to noisy graph prompts and inferior model performance. Additionally, due to the gap between pre-training and testing graphs, when the number of classes in the testing graphs is much greater than that in the training, the in-context learning ability will also significantly deteriorate. To tackle the aforementioned challenges, we develop a multi-stage adaptive prompt optimization method GraphPrompter, which optimizes the entire process of generating, selecting, and using graph prompts for better in-context learning capabilities. Firstly, Prompt Generator introduces a reconstruction layer to highlight the most informative edges and reduce irrelevant noise for graph prompt construction. Furthermore, in the selection stage, Prompt Selector employs the $k$-nearest neighbors algorithm and pre-trained selection layers to dynamically choose appropriate samples and minimize the influence of irrelevant prompts. Finally, we leverage a Prompt Augmenter with a cache replacement strategy to enhance the generalization capability of the pre-trained model on new datasets. Extensive experiments show that GraphPrompter effectively enhances the in-context learning ability of graph models. On average across all the settings, our approach surpasses the state-of-the-art baselines by over 8%. Our code is released at this https URL.', 'abstract_zh': '图内上下文学习：一种多阶段自适应提示优化方法以提高图模型的内上下文学习能力', 'title_zh': '图预prompter：多阶段自适应prompt优化图内视学习'}
{'arxiv_id': 'arXiv:2505.02020', 'title': 'Wide & Deep Learning for Node Classification', 'authors': 'Yancheng Chen, Wenguo Yang, Zhipeng Jiang', 'link': 'https://arxiv.org/abs/2505.02020', 'abstract': 'Wide & Deep, a simple yet effective learning architecture for recommendation systems developed by Google, has had a significant impact in both academia and industry due to its combination of the memorization ability of generalized linear models and the generalization ability of deep models. Graph convolutional networks (GCNs) remain dominant in node classification tasks; however, recent studies have highlighted issues such as heterophily and expressiveness, which focus on graph structure while seemingly neglecting the potential role of node features. In this paper, we propose a flexible framework GCNIII, which leverages the Wide & Deep architecture and incorporates three techniques: Intersect memory, Initial residual and Identity mapping. We provide comprehensive empirical evidence showing that GCNIII can more effectively balance the trade-off between over-fitting and over-generalization on various semi- and full- supervised tasks. Additionally, we explore the use of large language models (LLMs) for node feature engineering to enhance the performance of GCNIII in cross-domain node classification tasks. Our implementation is available at this https URL.', 'abstract_zh': 'Wide & Deep: 一种由Google开发的简单而有效的推荐系统学习架构，在学术界和工业界因结合了通用线性模型的记忆能力和深度模型的泛化能力而产生了重大影响。图卷积网络（GCNs）在节点分类任务中依然占据主导地位；然而，近期的研究指出了如异质性和表达能力等问题，这些问题集中于图结构的同时似乎忽视了节点特征的潜在作用。本文提出了一种灵活的框架GCNIII，结合了Wide & Deep架构并融入了三种技术：交集记忆、初始残差和恒等映射。我们提供了全面的实验证据，表明GCNIII在各种半监督和全监督任务中能够更有效地平衡过拟合和过度泛化的权衡。此外，我们探讨了使用大型语言模型（LLMs）进行节点特征工程，以增强GCNIII在跨域节点分类任务中的性能。相关实现可访问此链接。', 'title_zh': '节点分类中的宽深学习'}
{'arxiv_id': 'arXiv:2505.02011', 'title': 'CASA: CNN Autoencoder-based Score Attention for Efficient Multivariate Long-term Time-series Forecasting', 'authors': 'Minhyuk Lee, HyeKyung Yoon, MyungJoo Kang', 'link': 'https://arxiv.org/abs/2505.02011', 'abstract': 'Multivariate long-term time series forecasting is critical for applications such as weather prediction, and traffic analysis. In addition, the implementation of Transformer variants has improved prediction accuracy. Following these variants, different input data process approaches also enhanced the field, such as tokenization techniques including point-wise, channel-wise, and patch-wise tokenization. However, previous studies still have limitations in time complexity, computational resources, and cross-dimensional interactions. To address these limitations, we introduce a novel CNN Autoencoder-based Score Attention mechanism (CASA), which can be introduced in diverse Transformers model-agnosticically by reducing memory and leading to improvement in model performance. Experiments on eight real-world datasets validate that CASA decreases computational resources by up to 77.7%, accelerates inference by 44.0%, and achieves state-of-the-art performance, ranking first in 87.5% of evaluated metrics.', 'abstract_zh': '多变量长期时间序列 forecasting 对于天气预测和交通分析等应用至关重要。此外，Transformer 变体的实现已提高预测准确性。在此基础上，不同的输入数据处理方法，如点wise、通道wise和块wise标记化技术也提升了该领域。然而，先前的研究仍然在时间复杂性、计算资源和跨维度交互方面存在局限性。为解决这些局限性，我们提出了一种新型的基于CNN自编码器的评分注意机制（CASA），该机制可以在多种Transformer模型中通用，通过减少内存使用来提高模型性能。在八个实际数据集上的实验验证了CASA可以将计算资源减少高达77.7%，加速推理44.0%，并实现最佳性能，在评估指标中排名第一达87.5%。', 'title_zh': 'CASA: 基于CNN自编码器的分数注意力高效多变量长期时间序列预测'}
{'arxiv_id': 'arXiv:2505.01948', 'title': 'Multi-Scale Graph Learning for Anti-Sparse Downscaling', 'authors': 'Yingda Fan, Runlong Yu, Janet R. Barclay, Alison P. Appling, Yiming Sun, Yiqun Xie, Xiaowei Jia', 'link': 'https://arxiv.org/abs/2505.01948', 'abstract': 'Water temperature can vary substantially even across short distances within the same sub-watershed. Accurate prediction of stream water temperature at fine spatial resolutions (i.e., fine scales, $\\leq$ 1 km) enables precise interventions to maintain water quality and protect aquatic habitats. Although spatiotemporal models have made substantial progress in spatially coarse time series modeling, challenges persist in predicting at fine spatial scales due to the lack of data at that this http URL address the problem of insufficient fine-scale data, we propose a Multi-Scale Graph Learning (MSGL) method. This method employs a multi-task learning framework where coarse-scale graph learning, bolstered by larger datasets, simultaneously enhances fine-scale graph learning. Although existing multi-scale or multi-resolution methods integrate data from different spatial scales, they often overlook the spatial correspondences across graph structures at various scales. To address this, our MSGL introduces an additional learning task, cross-scale interpolation learning, which leverages the hydrological connectedness of stream locations across coarse- and fine-scale graphs to establish cross-scale connections, thereby enhancing overall model performance. Furthermore, we have broken free from the mindset that multi-scale learning is limited to synchronous training by proposing an Asynchronous Multi-Scale Graph Learning method (ASYNC-MSGL). Extensive experiments demonstrate the state-of-the-art performance of our method for anti-sparse downscaling of daily stream temperatures in the Delaware River Basin, USA, highlighting its potential utility for water resources monitoring and management.', 'abstract_zh': '水温在同一个次流域内短短的距离内可能会有显著差异。在亚公里尺度（≤1 km）的精细空间分辨率上准确预测溪流水温能够实现精准干预，以维持水质并保护 aquatic 生境。尽管时空模型在粗空间时间序列建模方面取得了显著进展，但在细空间尺度上预测仍然面临挑战，主要是由于缺乏细尺度数据。为了解决细尺度数据不足的问题，我们提出了一种多尺度图学习（MSGL）方法。该方法采用多任务学习框架，通过大型数据集增强粗尺度图学习，同时提升细尺度图学习。虽然现有的多尺度或多重分辨率方法整合了不同空间尺度的数据，但往往忽略了同一尺度不同图结构之间的空间对应关系。为此，我们的 MSGL 方法引入了一个额外的学习任务——跨尺度插值学习，通过利用粗尺度和细尺度图结构中溪流位置的水文连通性来建立跨尺度连接，从而提高整体模型性能。此外，我们提议了一种异步多尺度图学习方法（ASYNC-MSGL），打破了多尺度学习必须同步训练的思维定式。广泛的实验证明，我们的方法在 USA 德尔aware 河 basin 每日溪流水温的反稀疏下放尺度建模中表现出最先进的性能，突显了其在水资 海洋资源监测和管理方面的潜在应用价值。', 'title_zh': '多尺度图学习用于抗稀疏降采样'}
{'arxiv_id': 'arXiv:2505.01944', 'title': 'Explainability by design: an experimental analysis of the legal coding process', 'authors': 'Matteo Cristani, Guido Governatori, Francesco Olivieri, Monica Palmirani, Gabriele Buriola', 'link': 'https://arxiv.org/abs/2505.01944', 'abstract': 'Behind a set of rules in Deontic Defeasible Logic, there is a mapping process of normative background fragments. This process goes from text to rules and implicitly encompasses an explanation of the coded fragments.\nIn this paper we deliver a methodology for \\textit{legal coding} that starts with a fragment and goes onto a set of Deontic Defeasible Logic rules, involving a set of \\textit{scenarios} to test the correctness of the coded fragments. The methodology is illustrated by the coding process of an example text. We then show the results of a series of experiments conducted with humans encoding a variety of normative backgrounds and corresponding cases in which we have measured the efforts made in the coding process, as related to some measurable features. To process these examples, a recently developed technology, Houdini, that allows reasoning in Deontic Defeasible Logic, has been employed.\nFinally we provide a technique to forecast time required in coding, that depends on factors such as knowledge of the legal domain, knowledge of the coding processes, length of the text, and a measure of \\textit{depth} that refers to the length of the paths of legal references.', 'abstract_zh': '基于规范背景片段映射过程的规范命令缺省逻辑背后规则集的法律编码方法及其应用', 'title_zh': '设计中的可解释性：法律编码过程的实验分析'}
{'arxiv_id': 'arXiv:2505.01912', 'title': 'BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models', 'authors': 'Evan R. Antoniuk, Shehtab Zaman, Tal Ben-Nun, Peggy Li, James Diffenderfer, Busra Demirci, Obadiah Smolenski, Tim Hsu, Anna M. Hiszpanski, Kenneth Chiu, Bhavya Kailkhura, Brian Van Essen', 'link': 'https://arxiv.org/abs/2505.01912', 'abstract': 'Advances in deep learning and generative modeling have driven interest in data-driven molecule discovery pipelines, whereby machine learning (ML) models are used to filter and design novel molecules without requiring prohibitively expensive first-principles simulations. Although the discovery of novel molecules that extend the boundaries of known chemistry requires accurate out-of-distribution (OOD) predictions, ML models often struggle to generalize OOD. Furthermore, there are currently no systematic benchmarks for molecular OOD prediction tasks. We present BOOM, $\\boldsymbol{b}$enchmarks for $\\boldsymbol{o}$ut-$\\boldsymbol{o}$f-distribution $\\boldsymbol{m}$olecular property predictions -- a benchmark study of property-based out-of-distribution models for common molecular property prediction models. We evaluate more than 140 combinations of models and property prediction tasks to benchmark deep learning models on their OOD performance. Overall, we do not find any existing models that achieve strong OOD generalization across all tasks: even the top performing model exhibited an average OOD error 3x larger than in-distribution. We find that deep learning models with high inductive bias can perform well on OOD tasks with simple, specific properties. Although chemical foundation models with transfer and in-context learning offer a promising solution for limited training data scenarios, we find that current foundation models do not show strong OOD extrapolation capabilities. We perform extensive ablation experiments to highlight how OOD performance is impacted by data generation, pre-training, hyperparameter optimization, model architecture, and molecular representation. We propose that developing ML models with strong OOD generalization is a new frontier challenge in chemical ML model development. This open-source benchmark will be made available on Github.', 'abstract_zh': 'Advances in 深度学习和生成建模推动了数据驱动分子发现流程的兴趣，其中机器学习模型用于过滤和设计新型分子，无需进行昂贵的第一性原理模拟。虽然发现了扩展已知化学边界的新型分子需要准确的非分布外预测，但机器学习模型往往难以泛化到非分布外。此外，当前尚无系统的分子非分布外预测基准。我们提出BOOM——分子性质非分布外预测基准——一项基于属性的非分布外模型基准研究，用于评估常见分子性质预测模型。我们评估了超过140种模型和属性预测任务组合，以评估深度学习模型的非分布外性能。总体而言，我们没有发现任何现有模型在所有任务中都能实现强大的非分布外泛化：即使性能最佳的模型也表现出平均非分布外错误是分布内错误的3倍。我们发现，具有高归纳偏置的深度学习模型在简单的特定属性非分布外任务中表现良好。尽管化学基础模型结合迁移学习和上下文学习为有限训练数据场景提供了有希望的解决方案，但我们发现当前的基础模型并不显示强大的非分布外外推能力。我们进行了广泛的消融实验，以阐明数据生成、预训练、超参数优化、模型架构和分子表示对非分布外性能的影响。我们提出，开发具有强大非分布外泛化的机器学习模型是化学机器学习模型开发中的一个新前沿挑战。该开源基准将在Github上提供。', 'title_zh': 'BOOM: 评估机器学习模型在分布外分子性质预测中的基准测试'}
{'arxiv_id': 'arXiv:2505.01892', 'title': 'OODTE: A Differential Testing Engine for the ONNX Optimizer', 'authors': 'Nikolaos Louloudakis, Ajitha Rajan', 'link': 'https://arxiv.org/abs/2505.01892', 'abstract': 'With $700$ stars on GitHub and part of the official ONNX repository, the ONNX Optimizer consists of the standard method to apply graph-based optimizations on ONNX models. However, its ability to preserve model accuracy across optimizations, has not been rigorously explored. We propose OODTE, a utility to automatically and thoroughly assess the correctness of the ONNX Optimizer. OODTE follows a simple, yet effective differential testing and evaluation approach that can be easily adopted to other compiler optimizers. In particular, OODTE utilizes a number of ONNX models, then optimizes them and executes both the original and the optimized variants across a user-defined set of inputs, while automatically logging any issues with the optimization process. Finally, for successfully optimized models, OODTE compares the results, and, if any accuracy deviations are observed, it iteratively repeats the process for each pass of the ONNX Optimizer, to localize the root cause of the differences observed. Using OODTE, we sourced well-known $130$ models from the official ONNX Model Hub, used for a wide variety of tasks (classification, object detection, semantic segmentation, text summarization, question and answering, sentiment analysis) from the official ONNX model hub. We detected 15 issues, 14 of which were previously unknown, associated with optimizer crashes and accuracy deviations. We also observed $9.2$% of all model instances presenting issues leading into the crash of the optimizer, or the generation of an invalid model while using the primary optimizer strategies. In addition, $30$% of the classification models presented accuracy differences across the original and the optimized model variants, while $16.6$% of semantic segmentation and object detection models are also affected, at least to a limited extent.', 'abstract_zh': '具有700颗GitHub之星并部分隶属于官方ONNX仓库的ONNX优化器包含了一种基于图的ONNX模型优化的标准方法。然而，其在优化过程中保持模型准确性的能力尚未得到严格探索。我们提出了一种名为OODTE的工具，用于自动且全面地评估ONNX优化器的正确性。OODTE遵循一种简单而有效的差异测试和评估方法，易于应用于其他编译器优化器。具体而言，OODTE利用多个ONNX模型，对其进行优化并在用户定义的输入集中执行原始和优化的变体，同时自动记录优化过程中的任何问题。最后，对于成功优化的模型，OODTE比较结果，如果观察到任何准确性偏差，则会迭代重复过程，针对ONNX优化器的每一优化轮次本地化观察到差异的根本原因。使用OODTE，我们从官方ONNX模型库中提取了130个广为人知的模型，用于各种任务（分类、对象检测、语义分割、文本摘要、问答、情感分析）。我们检测到15个问题，其中14个是之前未知的问题，与优化器崩溃和准确性偏差有关。我们还观察到9.2%的模型实例在使用主要优化器策略时产生了问题，导致优化器崩溃或生成无效模型。此外，30%的分类模型在原始模型和优化模型版本之间显示了准确性差异，而33.3%的语义分割和对象检测模型也受到影响，至少在一定程度上。', 'title_zh': 'OODTE: ONNX优化器的差异测试引擎'}
{'arxiv_id': 'arXiv:2505.01884', 'title': 'Adversarial Robustness of Deep Learning Models for Inland Water Body Segmentation from SAR Images', 'authors': 'Siddharth Kothari, Srinivasan Murali, Sankalp Kothari, Ujjwal Verma, Jaya Sreevalsan-Nair', 'link': 'https://arxiv.org/abs/2505.01884', 'abstract': 'Inland water body segmentation from Synthetic Aperture Radar (SAR) images is an important task needed for several applications, such as flood mapping. While SAR sensors capture data in all-weather conditions as high-resolution images, differentiating water and water-like surfaces from SAR images is not straightforward. Inland water bodies, such as large river basins, have complex geometry, which adds to the challenge of segmentation. U-Net is a widely used deep learning model for land-water segmentation of SAR images. In practice, manual annotation is often used to generate the corresponding water masks as ground truth. Manual annotation of the images is prone to label noise owing to data poisoning attacks, especially due to complex geometry. In this work, we simulate manual errors in the form of adversarial attacks on the U-Net model and study the robustness of the model to human errors in annotation. Our results indicate that U-Net can tolerate a certain level of corruption before its performance drops significantly. This finding highlights the crucial role that the quality of manual annotations plays in determining the effectiveness of the segmentation model. The code and the new dataset, along with adversarial examples for robust training, are publicly available. (Github link - this https URL)', 'abstract_zh': '基于合成孔径雷达（SAR）图像的内陆水体分割的研究：人工标注噪声下的鲁棒性分析', 'title_zh': 'SAR图像内陆水体分割深度学习模型的对抗robust性'}
{'arxiv_id': 'arXiv:2505.01877', 'title': "Humans can learn to detect AI-generated texts, or at least learn when they can't", 'authors': 'Jiří Milička, Anna Marklová, Ondřej Drobil, Eva Pospíšilová', 'link': 'https://arxiv.org/abs/2505.01877', 'abstract': "This study investigates whether individuals can learn to accurately discriminate between human-written and AI-produced texts when provided with immediate feedback, and if they can use this feedback to recalibrate their self-perceived competence. We also explore the specific criteria individuals rely upon when making these decisions, focusing on textual style and perceived readability.\nWe used GPT-4o to generate several hundred texts across various genres and text types comparable to Koditex, a multi-register corpus of human-written texts. We then presented randomized text pairs to 255 Czech native speakers who identified which text was human-written and which was AI-generated. Participants were randomly assigned to two conditions: one receiving immediate feedback after each trial, the other receiving no feedback until experiment completion. We recorded accuracy in identification, confidence levels, response times, and judgments about text readability along with demographic data and participants' engagement with AI technologies prior to the experiment.\nParticipants receiving immediate feedback showed significant improvement in accuracy and confidence calibration. Participants initially held incorrect assumptions about AI-generated text features, including expectations about stylistic rigidity and readability. Notably, without feedback, participants made the most errors precisely when feeling most confident -- an issue largely resolved among the feedback group.\nThe ability to differentiate between human and AI-generated texts can be effectively learned through targeted training with explicit feedback, which helps correct misconceptions about AI stylistic features and readability, as well as potential other variables that were not explored, while facilitating more accurate self-assessment. This finding might be particularly important in educational contexts.", 'abstract_zh': '本研究探讨是否个体在接受即时反馈的情况下能够学会准确地辨别出人类撰写的文本和AI生成的文本，并且他们能否利用这种反馈来校准自己的能力感知。我们还探索了个体在做出这些决策时所依赖的具体标准，重点关注文本风格和可读性感知。', 'title_zh': '人类可以学会检测AI生成的文字，或者至少学会何时无法检测。'}
{'arxiv_id': 'arXiv:2505.01855', 'title': 'Intra-Layer Recurrence in Transformers for Language Modeling', 'authors': 'Anthony Nguyen, Wenjun Lin', 'link': 'https://arxiv.org/abs/2505.01855', 'abstract': 'Transformer models have established new benchmarks in natural language processing; however, their increasing depth results in substantial growth in parameter counts. While existing recurrent transformer methods address this issue by reprocessing layers multiple times, they often apply recurrence indiscriminately across entire blocks of layers. In this work, we investigate Intra-Layer Recurrence (ILR), a more targeted approach that applies recurrence selectively to individual layers within a single forward pass. Our experiments show that allocating more iterations to earlier layers yields optimal results. These findings suggest that ILR offers a promising direction for optimizing recurrent structures in transformer architectures.', 'abstract_zh': '基于Intra-Layer Recurrence的Transformer模型优化研究', 'title_zh': 'Transformer语言模型中的层内循环'}
{'arxiv_id': 'arXiv:2505.01822', 'title': 'Analytic Energy-Guided Policy Optimization for Offline Reinforcement Learning', 'authors': 'Jifeng Hu, Sili Huang, Zhejian Yang, Shengchao Hu, Li Shen, Hechang Chen, Lichao Sun, Yi Chang, Dacheng Tao', 'link': 'https://arxiv.org/abs/2505.01822', 'abstract': 'Conditional decision generation with diffusion models has shown powerful competitiveness in reinforcement learning (RL). Recent studies reveal the relation between energy-function-guidance diffusion models and constrained RL problems. The main challenge lies in estimating the intermediate energy, which is intractable due to the log-expectation formulation during the generation process. To address this issue, we propose the Analytic Energy-guided Policy Optimization (AEPO). Specifically, we first provide a theoretical analysis and the closed-form solution of the intermediate guidance when the diffusion model obeys the conditional Gaussian transformation. Then, we analyze the posterior Gaussian distribution in the log-expectation formulation and obtain the target estimation of the log-expectation under mild assumptions. Finally, we train an intermediate energy neural network to approach the target estimation of log-expectation formulation. We apply our method in 30+ offline RL tasks to demonstrate the effectiveness of our method. Extensive experiments illustrate that our method surpasses numerous representative baselines in D4RL offline reinforcement learning benchmarks.', 'abstract_zh': '基于扩散模型的条件决策生成在强化学习中的应用展示了强大的竞争力。最近的研究揭示了能量函数导向的扩散模型与受限强化学习问题之间的关系。主要挑战在于在生成过程中因对数期望形式难以估计中间能量。为解决这一问题，我们提出了一种分析型能量导向策略优化方法（AEPO）。具体而言，我们首先提供了在扩散模型遵循条件高斯变换时中间指导的理论分析及其解析解。然后，我们分析了对数期望形式下的后验高斯分布，并在温和假设下获得了对数期望的目标估计。最后，我们训练了一个中间能量神经网络以逼近对数期望形式的目标估计。我们在30多个离线强化学习任务中应用了该方法，以展示我们方法的有效性。广泛的实验表明，在D4RL离线强化学习基准测试中，我们的方法超越了众多代表性基线方法。', 'title_zh': '基于能量引导的分析性政策优化的离线强化学习'}
{'arxiv_id': 'arXiv:2505.01821', 'title': 'Edge-Cloud Collaborative Computing on Distributed Intelligence and Model Optimization: A Survey', 'authors': 'Jing Liu, Yao Du, Kun Yang, Yan Wang, Xiping Hu, Zehua Wang, Yang Liu, Peng Sun, Azzedine Boukerche, Victor C.M. Leung', 'link': 'https://arxiv.org/abs/2505.01821', 'abstract': 'Edge-cloud collaborative computing (ECCC) has emerged as a pivotal paradigm for addressing the computational demands of modern intelligent applications, integrating cloud resources with edge devices to enable efficient, low-latency processing. Recent advancements in AI, particularly deep learning and large language models (LLMs), have dramatically enhanced the capabilities of these distributed systems, yet introduce significant challenges in model deployment and resource management. In this survey, we comprehensive examine the intersection of distributed intelligence and model optimization within edge-cloud environments, providing a structured tutorial on fundamental architectures, enabling technologies, and emerging applications. Additionally, we systematically analyze model optimization approaches, including compression, adaptation, and neural architecture search, alongside AI-driven resource management strategies that balance performance, energy efficiency, and latency requirements. We further explore critical aspects of privacy protection and security enhancement within ECCC systems and examines practical deployments through diverse applications, spanning autonomous driving, healthcare, and industrial automation. Performance analysis and benchmarking techniques are also thoroughly explored to establish evaluation standards for these complex systems. Furthermore, the review identifies critical research directions including LLMs deployment, 6G integration, neuromorphic computing, and quantum computing, offering a roadmap for addressing persistent challenges in heterogeneity management, real-time processing, and scalability. By bridging theoretical advancements and practical deployments, this survey offers researchers and practitioners a holistic perspective on leveraging AI to optimize distributed computing environments, fostering innovation in next-generation intelligent systems.', 'abstract_zh': '边缘-云协作计算（ECCC）已成为应对现代智能应用计算需求的关键范式，通过整合云资源与边缘设备以实现高效、低延迟的处理。最近在人工智能领域的进展，尤其是深度学习和大型语言模型（LLMs），极大地提高了这些分布式系统的功能，但同时也带来了模型部署和资源管理的重大挑战。在本文综述中，我们全面探讨了分布式智能和模型优化在边缘-云环境中的交叉点，提供了一种结构化的导论，涵盖了基本架构、使能技术以及新兴应用。此外，我们系统地分析了模型优化方法，包括压缩、适应和神经架构搜索，以及由AI驱动的资源管理策略，这些策略旨在平衡性能、能源效率和延迟要求。我们进一步探讨了ECCC系统中的关键隐私保护和安全增强方面，并通过跨自主驾驶、医疗保健和工业自动化等多样化应用的实践部署进行了分析。我们还详细探讨了性能分析和基准测试技术，以建立这些复杂系统的评估标准。此外，本文综述确定了关键的研究方向，包括大型语言模型部署、6G集成、类脑计算和量子计算，为异构管理、实时处理和可扩展性等持续挑战提供了路线图。通过将理论进步与实践部署相结合，本文综述为研究人员和实践者提供了一种全面的视角，以利用AI优化分布式计算环境，促进下一代智能系统的创新。', 'title_zh': '分布式智能和模型优化下的边缘-云协作计算：一种综述'}
{'arxiv_id': 'arXiv:2505.01800', 'title': 'Distinguishing AI-Generated and Human-Written Text Through Psycholinguistic Analysis', 'authors': 'Chidimma Opara', 'link': 'https://arxiv.org/abs/2505.01800', 'abstract': 'The increasing sophistication of AI-generated texts highlights the urgent need for accurate and transparent detection tools, especially in educational settings, where verifying authorship is essential. Existing literature has demonstrated that the application of stylometric features with machine learning classifiers can yield excellent results. Building on this foundation, this study proposes a comprehensive framework that integrates stylometric analysis with psycholinguistic theories, offering a clear and interpretable approach to distinguishing between AI-generated and human-written texts. This research specifically maps 31 distinct stylometric features to cognitive processes such as lexical retrieval, discourse planning, cognitive load management, and metacognitive self-monitoring. In doing so, it highlights the unique psycholinguistic patterns found in human writing. Through the intersection of computational linguistics and cognitive science, this framework contributes to the development of reliable tools aimed at preserving academic integrity in the era of generative AI.', 'abstract_zh': 'AI生成文本日益 sophisticated：急需准确透明的检测工具，尤其是在教育领域，验证作者身份至关重要。基于现有文献，本研究提出了一种综合框架，将风格分析与心理语言学理论相结合，提供一种清晰可解释的方法来区分AI生成和人类撰写的文本。本研究将31种不同的风格特征与认知过程如词汇检索、话语规划、认知负荷管理及元认知自我监控相匹配，突显了人类书写中的独特心理语言学模式。通过计算语言学与认知科学的交叉，该框架为在生成AI时代保护学术诚信的发展提供了可靠工具。', 'title_zh': '通过心里语言学分析区分AI生成文本与人类撰写的文本'}
{'arxiv_id': 'arXiv:2505.01781', 'title': 'Enhancing Black-Litterman Portfolio via Hybrid Forecasting Model Combining Multivariate Decomposition and Noise Reduction', 'authors': 'Ziye Yang, Ke Lu', 'link': 'https://arxiv.org/abs/2505.01781', 'abstract': "The sensitivity to input parameters and lack of flexibility limits the traditional Mean-Variance model. In contrast, the Black-Litterman model has attracted widespread attention by integrating market equilibrium returns with investors' subjective views. This paper proposes a novel hybrid deep learning model combining Singular Spectrum analysis (SSA), Multivariate Aligned Empirical Mode Decomposition (MA-EMD), and Temporal Convolutional Networks (TCNs), aiming to improve the prediction accuracy of asset prices and thus enhance the ability of the Black-Litterman model to generate subjective views. Experimental results show that noise reduction pre-processing can improve the model's accuracy, and the prediction performance of the proposed model is significantly better than that of three multivariate decomposition benchmark models. We construct an investment portfolio by using 20 representative stocks from the NASDAQ 100 index. By combining the hybrid forecasting model with the Black-Litterman model, the generated investment portfolio exhibits better returns and risk control capabilities than the Mean-Variance, Equal-Weighted, and Market-Weighted models in the short holding period.", 'abstract_zh': '传统的均值-方差模型由于输入参数的敏感性和缺乏灵活性而受到限制。相比之下，通过将市场均衡回报与投资者的主观观点整合，布莱克-利特曼模型受到了广泛关注。本文提出了一种结合单曲线谱分析(SSA)、多变量对齐经验模式分解(MA-EMD)和时空卷积网络(TCNs)的新型混合深度学习模型，旨在提高资产价格预测精度，从而增强布莱克-利特曼模型生成主观观点的能力。实验结果表明，噪声减少预处理可以提高模型的准确性，所提出的模型的预测性能显著优于三种多变量分解基准模型。通过使用纳斯达克100指数的20只代表股票构建投资组合，在短期内，结合混合预测模型与布莱克-利特曼模型生成的投资组合展现了优于均值-方差、等权重和市场权重模型的回报和风险控制能力。', 'title_zh': '基于多变量分解和噪声减少的混合预测模型增强黑利特曼投资组合'}
{'arxiv_id': 'arXiv:2505.01780', 'title': 'Rate-Limited Closed-Loop Distributed ISAC Systems: An Autoencoder Approach', 'authors': 'Guangjin Pan, Zhixing Li, Ayça Özçelikkale, Christian Häger, Musa Furkan Keskin, Henk Wymeersch', 'link': 'https://arxiv.org/abs/2505.01780', 'abstract': 'In closed-loop distributed multi-sensor integrated sensing and communication (ISAC) systems, performance often hinges on transmitting high-dimensional sensor observations over rate-limited networks. In this paper, we first present a general framework for rate-limited closed-loop distributed ISAC systems, and then propose an autoencoder-based observation compression method to overcome the constraints imposed by limited transmission capacity. Building on this framework, we conduct a case study using a closed-loop linear quadratic regulator (LQR) system to analyze how the interplay among observation, compression, and state dimensions affects reconstruction accuracy, state estimation error, and control performance. In multi-sensor scenarios, our results further show that optimal resource allocation initially prioritizes low-noise sensors until the compression becomes lossless, after which resources are reallocated to high-noise sensors.', 'abstract_zh': '在闭环分布式多传感器集成传感与通信(ISAC)系统中，性能往往取决于在带宽受限的网络中传输高维传感器观测值。在本文中，我们首先提出了一种适用于带宽受限的闭环分布式ISAC系统的通用框架，然后提出了一种基于自动编码器的观测压缩方法以克服带宽限制带来的限制。在此框架基础上，我们使用闭环线性二次调节器(LQR)系统进行案例研究，以分析观测、压缩和状态维度之间的相互作用如何影响重构精度、状态估计误差和控制性能。在多传感器场景中，我们的结果进一步表明，在压缩变得无损之前，资源优化分配优先考虑低噪声传感器，之后资源重新分配给高噪声传感器。', 'title_zh': '带速率限制的闭环分布式ISAC系统：一种自动编码器方法'}
{'arxiv_id': 'arXiv:2505.01736', 'title': 'PeSANet: Physics-encoded Spectral Attention Network for Simulating PDE-Governed Complex Systems', 'authors': 'Han Wan, Rui Zhang, Qi Wang, Yang Liu, Hao Sun', 'link': 'https://arxiv.org/abs/2505.01736', 'abstract': 'Accurately modeling and forecasting complex systems governed by partial differential equations (PDEs) is crucial in various scientific and engineering domains. However, traditional numerical methods struggle in real-world scenarios due to incomplete or unknown physical laws. Meanwhile, machine learning approaches often fail to generalize effectively when faced with scarce observational data and the challenge of capturing local and global features. To this end, we propose the Physics-encoded Spectral Attention Network (PeSANet), which integrates local and global information to forecast complex systems with limited data and incomplete physical priors. The model consists of two key components: a physics-encoded block that uses hard constraints to approximate local differential operators from limited data, and a spectral-enhanced block that captures long-range global dependencies in the frequency domain. Specifically, we introduce a novel spectral attention mechanism to model inter-spectrum relationships and learn long-range spatial features. Experimental results demonstrate that PeSANet outperforms existing methods across all metrics, particularly in long-term forecasting accuracy, providing a promising solution for simulating complex systems with limited data and incomplete physics.', 'abstract_zh': '准确建模和预测受偏微分方程(PDEs)支配的复杂系统在各个科学和工程领域至关重要。然而，传统数值方法在实际场景中由于物理定律不完整或未知而难以应对。同时，机器学习方法在面临稀缺观测数据和捕捉局部与全局特征的挑战时表现不佳。为此，我们提出了物理编码谱注意网络(PeSANet)，该模型结合局部和全局信息，以有限数据和不完全物理先验预测复杂系统。该模型包含两个关键组件：一个物理编码块，使用硬约束从有限数据中近似局部微分算子；以及一个谱增强块，在频域中捕获长程全局依赖性。具体而言，我们引入了一种新颖的谱注意力机制来建模谱间关系并学习长程空间特征。实验结果表明，PeSANet在所有指标上均优于现有方法，特别是在长-term预测准确性方面表现尤为突出，为以有限数据和不完全物理模拟复杂系统提供了有前景的解决方案。', 'title_zh': '物理编码光谱注意力网络：用于模拟由偏微分方程支配的复杂系统'}
{'arxiv_id': 'arXiv:2505.01730', 'title': 'PASCAL: Precise and Efficient ANN- SNN Conversion using Spike Accumulation and Adaptive Layerwise Activation', 'authors': 'Pranav Ramesh, Gopalakrishnan Srinivasan', 'link': 'https://arxiv.org/abs/2505.01730', 'abstract': 'Spiking Neural Networks (SNNs) have been put forward as an energy-efficient alternative to Artificial Neural Networks (ANNs) since they perform sparse Accumulate operations instead of the power-hungry Multiply-and-Accumulate operations. ANN-SNN conversion is a widely used method to realize deep SNNs with accuracy comparable to that of ANNs.~\\citeauthor{bu2023optimal} recently proposed the Quantization-Clip-Floor-Shift (QCFS) activation as an alternative to ReLU to minimize the accuracy loss during ANN-SNN conversion. Nevertheless, SNN inferencing requires a large number of timesteps to match the accuracy of the source ANN for real-world datasets. In this work, we propose PASCAL, which performs ANN-SNN conversion in such a way that the resulting SNN is mathematically equivalent to an ANN with QCFS-activation, thereby yielding similar accuracy as the source ANN with minimal inference timesteps. In addition, we propose a systematic method to configure the quantization step of QCFS activation in a layerwise manner, which effectively determines the optimal number of timesteps per layer for the converted SNN. Our results show that the ResNet-34 SNN obtained using PASCAL achieves an accuracy of $\\approx$74\\% on ImageNet with a 64$\\times$ reduction in the number of inference timesteps compared to existing approaches.', 'abstract_zh': '基于Quantization-Clip-Floor-Shift激活的PASCAL：实现高效等价转换以减少推理时间步长', 'title_zh': 'PASCAL: 基于突触积累和适应层间激活的精确高效ANN-SNN转换方法'}
{'arxiv_id': 'arXiv:2505.01699', 'title': 'Component-Based Fairness in Face Attribute Classification with Bayesian Network-informed Meta Learning', 'authors': 'Yifan Liu, Ruichen Yao, Yaokun Liu, Ruohan Zong, Zelin Li, Yang Zhang, Dong Wang', 'link': 'https://arxiv.org/abs/2505.01699', 'abstract': 'The widespread integration of face recognition technologies into various applications (e.g., access control and personalized advertising) necessitates a critical emphasis on fairness. While previous efforts have focused on demographic fairness, the fairness of individual biological face components remains unexplored. In this paper, we focus on face component fairness, a fairness notion defined by biological face features. To our best knowledge, our work is the first work to mitigate bias of face attribute prediction at the biological feature level. In this work, we identify two key challenges in optimizing face component fairness: attribute label scarcity and attribute inter-dependencies, both of which limit the effectiveness of bias mitigation from previous approaches. To address these issues, we propose \\textbf{B}ayesian \\textbf{N}etwork-informed \\textbf{M}eta \\textbf{R}eweighting (BNMR), which incorporates a Bayesian Network calibrator to guide an adaptive meta-learning-based sample reweighting process. During the training process of our approach, the Bayesian Network calibrator dynamically tracks model bias and encodes prior probabilities for face component attributes to overcome the above challenges. To demonstrate the efficacy of our approach, we conduct extensive experiments on a large-scale real-world human face dataset. Our results show that BNMR is able to consistently outperform recent face bias mitigation baselines. Moreover, our results suggest a positive impact of face component fairness on the commonly considered demographic fairness (e.g., \\textit{gender}). Our findings pave the way for new research avenues on face component fairness, suggesting that face component fairness could serve as a potential surrogate objective for demographic fairness. The code for our work is publicly available~\\footnote{this https URL}.', 'abstract_zh': '面部识别技术在各种应用（如访问控制和个性化广告）中的广泛应用 necessitates 对公平性的关键重视。尽管先前的工作侧重于人口统计学公平性，但面部生物特征组成部分的公平性尚未得到探索。在本文中，我们专注于面部组成部分公平性，这是由生物面部特征定义的一种公平性概念。据我们所知，我们的工作是首次在生物特征水平上减轻面部属性预测偏差的工作。在本文中，我们识别出优化面部组成部分公平性的两个关键挑战：属性标签稀缺性和属性间依赖性，这两个问题限制了先前方法减少偏差的有效性。为了解决这些问题，我们提出了BNMR（Bayesian Network-informed Meta Reweighting），它结合了贝叶斯网络校准器来指导自适应元学习为基础的样本重加权过程。在我们方法的训练过程中，贝叶斯网络校准器动态跟踪模型偏差并编码面部组成部分属性的先验概率，以克服上述挑战。为了展示我们方法的有效性，我们在大规模真实世界的人脸数据集上进行了广泛的实验。我们的结果显示，BNMR能够一致地优于最近的面部偏差减轻基线。此外，我们的结果表明，面部组成部分公平性对通常考虑的人口统计学公平性（如性别）具有积极影响。我们的发现为面部组成部分公平性的新研究路径铺平了道路，表明面部组成部分公平性可能成为人口统计学公平性的潜在替代目标。我们的代码已公开。', 'title_zh': '基于组件的公平性在贝叶斯网络指导下元学习的面部属性分类中'}
{'arxiv_id': 'arXiv:2505.01696', 'title': 'Interpretable graph-based models on multimodal biomedical data integration: A technical review and benchmarking', 'authors': 'Alireza Sadeghi, Farshid Hajati, Ahmadreza Argha, Nigel H Lovell, Min Yang, Hamid Alinejad-Rokny', 'link': 'https://arxiv.org/abs/2505.01696', 'abstract': 'Integrating heterogeneous biomedical data including imaging, omics, and clinical records supports accurate diagnosis and personalised care. Graph-based models fuse such non-Euclidean data by capturing spatial and relational structure, yet clinical uptake requires regulator-ready interpretability. We present the first technical survey of interpretable graph based models for multimodal biomedical data, covering 26 studies published between Jan 2019 and Sep 2024. Most target disease classification, notably cancer and rely on static graphs from simple similarity measures, while graph-native explainers are rare; post-hoc methods adapted from non-graph domains such as gradient saliency, and SHAP predominate. We group existing approaches into four interpretability families, outline trends such as graph-in-graph hierarchies, knowledge-graph edges, and dynamic topology learning, and perform a practical benchmark. Using an Alzheimer disease cohort, we compare Sensitivity Analysis, Gradient Saliency, SHAP and Graph Masking. SHAP and Sensitivity Analysis recover the broadest set of known AD pathways and Gene-Ontology terms, whereas Gradient Saliency and Graph Masking surface complementary metabolic and transport signatures. Permutation tests show all four beat random gene sets, but with distinct trade-offs: SHAP and Graph Masking offer deeper biology at higher compute cost, while Gradient Saliency and Sensitivity Analysis are quicker though coarser. We also provide a step-by-step flowchart covering graph construction, explainer choice and resource budgeting to help researchers balance transparency and performance. This review synthesises the state of interpretable graph learning for multimodal medicine, benchmarks leading techniques, and charts future directions, from advanced XAI tools to under-studied diseases, serving as a concise reference for method developers and translational scientists.', 'abstract_zh': '集成异质生物医学数据（包括成像、组学和临床记录）以支持准确诊断和个性化护理。基于图的模型通过捕捉空间和关系结构来融合此类非欧几里得数据，但临床应用需要具备监管机构可接受的可解释性。我们提供了首个关于 multimodal 生物医学数据的可解释图基于模型的技术综述，覆盖了2019年1月到2024年9月间发表的26项研究。大多数研究针对疾病分类，尤其是癌症，并依赖于简单相似性度量的静态图；而原生图解释器稀少，后验方法如梯度显著性和SHAP占据主导。我们将现有方法分为四类解释性家族，概述了图中图层次结构、知识图边和动态拓扑学习等趋势，并进行了实际基准测试。使用阿尔茨海默病队列，我们比较了敏感性分析、梯度显著性、SHAP和图掩码。SHAP和敏感性分析恢复了最多的已知AD途径和GO术语，而梯度显著性和图掩码揭示了互补的代谢和转运特征。互换检验表明，所有方法都优于随机基因集，但各有权衡：SHAP和图掩码提供了更深入但计算成本更高的生物学解释，而梯度显著性和敏感性分析则更快但较粗糙。我们还提供了一个从图构建、解释器选择到资源配置预算的逐步流程图，以帮助研究者平衡透明度和性能。本综述综合了多模态医学中可解释图学习的状态，基准了领先技术，并为未研究疾病领域的未来方向指明了路径，为方法开发者和转化科学家提供了一份精练的参考。', 'title_zh': '基于图的可解释模型在多模态生物医学数据整合中的应用：技术综述与基准测试'}
{'arxiv_id': 'arXiv:2505.01694', 'title': 'Topology-Aware CLIP Few-Shot Learning', 'authors': 'Dazhi Huang', 'link': 'https://arxiv.org/abs/2505.01694', 'abstract': "Efficiently adapting large Vision-Language Models (VLMs) like CLIP for few-shot learning poses challenges in balancing pre-trained knowledge retention and task-specific adaptation. Existing methods often overlook valuable structural information within the VLM's latent space. We introduce a topology-aware tuning approach integrating Representation Topology Divergence (RTD) into the Task Residual (TR) framework. By explicitly aligning the topological structures of visual and text representations using a combined RTD and Cross-Entropy loss, while freezing base VLM encoders, our method enhances few-shot performance. We optimize only lightweight Task Residual parameters, effectively leveraging topological information. Across 6 diverse benchmark datasets, our approach demonstrates significant gains, achieving an average accuracy improvement of 1-2\\% over relevant baseline methods in few-shot settings. This work presents an effective strategy to boost VLM few-shot capabilities by incorporating topological alignment.", 'abstract_zh': '有效适应大型视觉-语言模型（VLMs）如CLIP进行少样本学习面临保留预训练知识和任务特定适应之间的平衡挑战。现有方法往往忽视了VLM潜在空间内的宝贵结构信息。我们介绍了结合代表拓扑分歧（RTD）的拓扑感知微调方法，将其整合到任务残差（TR）框架中。通过使用结合RTD和交叉熵损失显式对齐视觉和文本表示的拓扑结构，在冻结基VLM编码器的情况下，该方法提升了少样本性能。我们仅优化轻量级任务残差参数，有效地利用了拓扑信息。在6个不同的基准数据集中，我们的方法实现了显著的改进，在少样本设置中相对于相关基线方法平均准确率提升1-2%。本工作提出了一种有效策略，通过引入拓扑对齐来增强VLM的少样本能力。', 'title_zh': '拓扑感知CLIP少-shot学习'}
{'arxiv_id': 'arXiv:2505.01664', 'title': 'Soft-Masked Semi-Dual Optimal Transport for Partial Domain Adaptation', 'authors': 'Yi-Ming Zhai, Chuan-Xian Ren, Hong Yan', 'link': 'https://arxiv.org/abs/2505.01664', 'abstract': 'Visual domain adaptation aims to learn discriminative and domain-invariant representation for an unlabeled target domain by leveraging knowledge from a labeled source domain. Partial domain adaptation (PDA) is a general and practical scenario in which the target label space is a subset of the source one. The challenges of PDA exist due to not only domain shift but also the non-identical label spaces of domains. In this paper, a Soft-masked Semi-dual Optimal Transport (SSOT) method is proposed to deal with the PDA problem. Specifically, the class weights of domains are estimated, and then a reweighed source domain is constructed, which is favorable in conducting class-conditional distribution matching with the target domain. A soft-masked transport distance matrix is constructed by category predictions, which will enhance the class-oriented representation ability of optimal transport in the shared feature space. To deal with large-scale optimal transport problems, the semi-dual formulation of the entropy-regularized Kantorovich problem is employed since it can be optimized by gradient-based algorithms. Further, a neural network is exploited to approximate the Kantorovich potential due to its strong fitting ability. This network parametrization also allows the generalization of the dual variable outside the supports of the input distribution. The SSOT model is built upon neural networks, which can be optimized alternately in an end-to-end manner. Extensive experiments are conducted on four benchmark datasets to demonstrate the effectiveness of SSOT.', 'abstract_zh': '视觉领域适应旨在通过利用标记源领域知识来学习区分性且领域不变的表示，以应用于未标记的目标领域。部分领域适应（PDA）是一种一般且实用的场景，其中目标标签空间是源标签空间的子集。PDA的挑战不仅来源于领域偏移，还来源于领域之间非相同的标签空间。在本文中，提出了一种软门控半对偶最优传输（SSOT）方法来应对PDA问题。具体而言，估算了领域内的类别权重，然后构建了一个加权后的源领域，该领域有利于在目标领域上进行条件类别分布匹配。通过类别预测构建了一个软门控的传输距离矩阵，这将增强最优传输在共享特征空间中的类别导向表示能力。为解决大规模最优传输问题，采用了熵正则化的坎托罗维奇问题的半对偶形式，因为该形式可以通过梯度基算法进行优化。进一步地，由于其强大的拟合能力，使用神经网络来近似坎托罗维奇势。这种网络参数化还可以使得对偶变量在输入分布的支持之外进行一般化。SSOT模型基于神经网络，可以以端到端的方式交替优化。在四个基准数据集上进行了大量实验，以证明SSOT的有效性。', 'title_zh': '部分领域适应的软遮罩半对偶最优传输'}
{'arxiv_id': 'arXiv:2505.01652', 'title': 'Causally Fair Node Classification on Non-IID Graph Data', 'authors': 'Yucong Dai, Lu Zhang, Yaowei Hu, Susan Gauch, Yongkai Wu', 'link': 'https://arxiv.org/abs/2505.01652', 'abstract': 'Fair machine learning seeks to identify and mitigate biases in predictions against unfavorable populations characterized by demographic attributes, such as race and gender. Recently, a few works have extended fairness to graph data, such as social networks, but most of them neglect the causal relationships among data instances. This paper addresses the prevalent challenge in fairness-aware ML algorithms, which typically assume Independent and Identically Distributed (IID) data. We tackle the overlooked domain of non-IID, graph-based settings where data instances are interconnected, influencing the outcomes of fairness interventions. We base our research on the Network Structural Causal Model (NSCM) framework and posit two main assumptions: Decomposability and Graph Independence, which enable the computation of interventional distributions in non-IID settings using the $do$-calculus. Based on that, we develop the Message Passing Variational Autoencoder for Causal Inference (MPVA) to compute interventional distributions and facilitate causally fair node classification through estimated interventional distributions. Empirical evaluations on semi-synthetic and real-world datasets demonstrate that MPVA outperforms conventional methods by effectively approximating interventional distributions and mitigating bias. The implications of our findings underscore the potential of causality-based fairness in complex ML applications, setting the stage for further research into relaxing the initial assumptions to enhance model fairness.', 'abstract_zh': '公平的机器学习寻求识别并减轻基于人口统计属性（如种族和性别）的不利群体在预测中受到的偏见。最近，一些研究将公平性扩展到了图数据，如社交网络，但大多数研究忽视了数据实例之间的因果关系。本文解决了一些公平感知机器学习算法中的普遍挑战，通常假设这些算法的数据是独立同分布（IID）。我们针对数据实例相连、相互影响的非IID图基设置，探讨了这一未被充分考虑的研究领域。我们基于网络结构因果模型（NSCM）框架，提出两项主要假设：可分解性和图独立性，这些假设使我们能够使用$do$-因果演算在非IID设置中计算介入分布。在此基础上，我们开发了用于因果推断的消息传递变分自编码器（MPVA），用于计算介入分布，并通过估计的介入分布促进因果公平节点分类。在半合成数据和真实世界数据集上的实验评估表明，MPVA在有效逼近介入分布和减轻偏见方面优于传统方法。我们的研究结果表明，基于因果性的公平性在复杂的机器学习应用中有潜力，并为进一步研究如何放松初始假设以提升模型公平性奠定了基础。', 'title_zh': '因果公平节点分类在非同态图数据上'}
{'arxiv_id': 'arXiv:2505.01647', 'title': 'Scalable Speed-ups for the SMS-EMOA from a Simple Aging Strategy', 'authors': 'Mingfeng Li, Weijie Zheng, Benjamin Doerr', 'link': 'https://arxiv.org/abs/2505.01647', 'abstract': 'Different from single-objective evolutionary algorithms, where non-elitism is an established concept, multi-objective evolutionary algorithms almost always select the next population in a greedy fashion. In the only notable exception, Bian, Zhou, Li, and Qian (IJCAI 2023) proposed a stochastic selection mechanism for the SMS-EMOA and proved that it can speed up computing the Pareto front of the bi-objective jump benchmark with problem size $n$ and gap parameter $k$ by a factor of $\\max\\{1,2^{k/4}/n\\}$. While this constitutes the first proven speed-up from non-elitist selection, suggesting a very interesting research direction, it has to be noted that a true speed-up only occurs for $k \\ge 4\\log_2(n)$, where the runtime is super-polynomial, and that the advantage reduces for larger numbers of objectives as shown in a later work. In this work, we propose a different non-elitist selection mechanism based on aging, which exempts individuals younger than a certain age from a possible removal. This remedies the two shortcomings of stochastic selection: We prove a speed-up by a factor of $\\max\\{1,\\Theta(k)^{k-1}\\}$, regardless of the number of objectives. In particular, a positive speed-up can already be observed for constant $k$, the only setting for which polynomial runtimes can be witnessed. Overall, this result supports the use of non-elitist selection schemes, but suggests that aging-based mechanisms can be considerably more powerful than stochastic selection mechanisms.', 'abstract_zh': '不同单目标进化算法中的非精英策略，多目标进化算法几乎总是以贪婪的方式选择下一代种群。唯一例外的是Bian, Zhou, Li, and Qian (IJCAI 2023)提出的SMS-EMOA的一种随机选择机制，证明了它可以将具有规模$n$和间隙参数$k$的双目标跳跃基准 Pareto 前沿的计算速度提高因子$\\max\\{1,2^{k/4}/n\\}$。虽然这是首次通过非精英选择证明的速度提升，但这一发现提供了一条非常有趣的研究方向，需要注意的是，仅当$k \\ge 4\\log_2(n)$时才会出现真正的速度提升，在这种情况下运算是超多项式的，且随着目标数量的增加，优势会减弱。在本文中，我们提出了一种基于年龄的非精英选择机制，该机制将年龄小于一定时间的个体豁免于可能的删除。这弥补了随机选择机制的两个缺点：我们证明了一个因子$\\max\\{1,\\Theta(k)^{k-1}\\}$的加速，与目标数量无关。特别是，对于仅当存在多项式运行时间时的常数$k$，可以观察到正向加速。总体而言，这一结果支持使用非精英选择方案，但表明基于年龄的机制可能比随机选择机制更为强大。', 'title_zh': 'SMS-EMOA的可扩展加速策略研究'}
{'arxiv_id': 'arXiv:2505.01635', 'title': 'Dendritic Computing with Multi-Gate Ferroelectric Field-Effect Transistors', 'authors': 'A N M Nafiul Islam, Xuezhong Niu, Jiahui Duan, Shubham Kumar, Kai Ni, Abhronil Sengupta', 'link': 'https://arxiv.org/abs/2505.01635', 'abstract': 'Although inspired by neuronal systems in the brain, artificial neural networks generally employ point-neurons, which offer far less computational complexity than their biological counterparts. Neurons have dendritic arbors that connect to different sets of synapses and offer local non-linear accumulation - playing a pivotal role in processing and learning. Inspired by this, we propose a novel neuron design based on a multi-gate ferroelectric field-effect transistor that mimics dendrites. It leverages ferroelectric nonlinearity for local computations within dendritic branches, while utilizing the transistor action to generate the final neuronal output. The branched architecture paves the way for utilizing smaller crossbar arrays in hardware integration, leading to greater efficiency. Using an experimentally calibrated device-circuit-algorithm co-simulation framework, we demonstrate that networks incorporating our dendritic neurons achieve superior performance in comparison to much larger networks without dendrites ($\\sim$17$\\times$ fewer trainable weight parameters). These findings suggest that dendritic hardware can significantly improve computational efficiency, and learning capacity of neuromorphic systems optimized for edge applications.', 'abstract_zh': '尽管受到大脑神经系统的启发，人工神经网络通常使用点神经元，其计算复杂度远低于生物神经元。神经元具有树突结构，连接到不同的突触集合，并提供局部非线性累积，这对于处理和学习至关重要。受到这一点的启发，我们提出了一种基于多门铁电场效应晶体管的新型神经元设计，模仿树突的功能。该设计利用铁电非线性在树突分支内执行局部计算，同时利用晶体管的作用生成最终的神经元输出。分枝结构为硬件集成中使用更小的交叉阵列铺平了道路，从而提高了效率。通过使用实验校准的器件-电路-算法协同仿真框架，我们证明包含我们所提出的树突神经元的网络，在与没有树突的更大网络相比的情况下，具有更好的性能（大约17倍更少的可训练权重参数）。这些发现表明，树突硬件可以显著提高针对边缘应用优化的类脑系统中的计算效率和学习能力。', 'title_zh': '多门铁电场效应晶体管的树突式计算'}
{'arxiv_id': 'arXiv:2505.01632', 'title': 'Transfer Learning-Based Deep Residual Learning for Speech Recognition in Clean and Noisy Environments', 'authors': 'Noussaiba Djeffal, Djamel Addou, Hamza Kheddar, Sid Ahmed Selouani', 'link': 'https://arxiv.org/abs/2505.01632', 'abstract': 'Addressing the detrimental impact of non-stationary environmental noise on automatic speech recognition (ASR) has been a persistent and significant research focus. Despite advancements, this challenge continues to be a major concern. Recently, data-driven supervised approaches, such as deep neural networks, have emerged as promising alternatives to traditional unsupervised methods. With extensive training, these approaches have the potential to overcome the challenges posed by diverse real-life acoustic environments. In this light, this paper introduces a novel neural framework that incorporates a robust frontend into ASR systems in both clean and noisy environments. Utilizing the Aurora-2 speech database, the authors evaluate the effectiveness of an acoustic feature set for Mel-frequency, employing the approach of transfer learning based on Residual neural network (ResNet). The experimental results demonstrate a significant improvement in recognition accuracy compared to convolutional neural networks (CNN) and long short-term memory (LSTM) networks. They achieved accuracies of 98.94% in clean and 91.21% in noisy mode.', 'abstract_zh': '应对非平稳环境噪声对自动语音识别（ASR）的负面影响的研究一直是持续且重要的研究焦点。尽管取得了进步，这一挑战仍然是一个重要问题。近年来，基于数据驱动的监督方法，如深度神经网络，已经 emerged 作为传统无监督方法的有前途的替代方案。通过大量的训练，这些方法有可能克服各种实际声学环境带来的挑战。在这种背景下，本文引入了一种新颖的神经框架，将稳健的前端集成到清洁和噪声环境下的 ASR 系统中。作者利用 Aurora-2 语音数据库，基于残差神经网络（ResNet）进行迁移学习的方法，评估了 Mel 频率谱特性的有效性。实验结果表明，与卷积神经网络（CNN）和长短期记忆网络（LSTM）相比，该方法在清洁环境下达到了 98.94% 的识别准确率，在噪声环境下达到了 91.21% 的识别准确率。', 'title_zh': '基于转移学习的深度残差学习在清洁和噪声环境下的语音识别'}
{'arxiv_id': 'arXiv:2505.01584', 'title': 'Understanding and Exploiting Plasticity for Non-stationary Network Resource Adaptation', 'authors': 'Zhiqiang He, Zhi Liu', 'link': 'https://arxiv.org/abs/2505.01584', 'abstract': 'Adapting to non-stationary network conditions presents significant challenges for resource adaptation. However, current solutions primarily rely on stationary assumptions. While data-driven reinforcement learning approaches offer promising solutions for handling network dynamics, our systematic investigation reveals a critical limitation: neural networks suffer from plasticity loss, significantly impeding their ability to adapt to evolving network conditions. Through theoretical analysis of neural propagation mechanisms, we demonstrate that existing dormant neuron metrics inadequately characterize neural plasticity loss. To address this limitation, we have developed the Silent Neuron theory, which provides a more comprehensive framework for understanding plasticity degradation. Based on these theoretical insights, we propose the Reset Silent Neuron (ReSiN), which preserves neural plasticity through strategic neuron resets guided by both forward and backward propagation states. In our implementation of an adaptive video streaming system, ReSiN has shown significant improvements over existing solutions, achieving up to 168% higher bitrate and 108% better quality of experience (QoE) while maintaining comparable smoothness. Furthermore, ReSiN consistently outperforms in stationary environments, demonstrating its robust adaptability across different network conditions.', 'abstract_zh': '适应非稳态网络条件对资源适应性提出了重大挑战。然而，当前的解决方案主要基于稳态假设。尽管基于数据驱动的强化学习方法为处理网络动态提供了有希望的解决方案，但我们系统的调查揭示了一个关键限制：神经网络遭受灵活性损失，显著阻碍了它们适应 evolving 网络条件的能力。通过分析神经传播机制的理论，我们表明现有的静默神经评价指标不足以表征神经灵活性损失。为了解决这一限制，我们提出了静默神经理论，提供了一个更全面的框架来理解灵活性退化。基于这些理论洞见，我们提出了重置静默神经（ReSiN），通过前向和后向传播状态引导的战略性神经重置来保持神经灵活性。我们在一个自适应视频流媒体系统中的实现表明，与现有解决方案相比，ReSiN 显示出显著的改进，比特率提高高达 168%，用户体验质量提高 108%，同时保持类似的流畅性。此外，ReSiN 在稳态环境中表现出色，展示了其在不同网络条件下的一贯稳健适应性。', 'title_zh': '理解并利用塑性实现非稳态网络资源适应'}
{'arxiv_id': 'arXiv:2505.01557', 'title': 'Contextures: Representations from Contexts', 'authors': 'Runtian Zhai, Kai Yang, Che-Ping Tsai, Burak Varici, Zico Kolter, Pradeep Ravikumar', 'link': 'https://arxiv.org/abs/2505.01557', 'abstract': 'Despite the empirical success of foundation models, we do not have a systematic characterization of the representations that these models learn. In this paper, we establish the contexture theory. It shows that a large class of representation learning methods can be characterized as learning from the association between the input and a context variable. Specifically, we show that many popular methods aim to approximate the top-d singular functions of the expectation operator induced by the context, in which case we say that the representation learns the contexture. We demonstrate the generality of the contexture theory by proving that representation learning within various learning paradigms -- supervised, self-supervised, and manifold learning -- can all be studied from such a perspective. We also prove that the representations that learn the contexture are optimal on those tasks that are compatible with the context. One important implication of the contexture theory is that once the model is large enough to approximate the top singular functions, further scaling up the model size yields diminishing returns. Therefore, scaling is not all we need, and further improvement requires better contexts. To this end, we study how to evaluate the usefulness of a context without knowing the downstream tasks. We propose a metric and show by experiments that it correlates well with the actual performance of the encoder on many real datasets.', 'abstract_zh': '尽管基础模型在实证上取得了成功，但我们尚未系统地表征这些模型所学习的表示。在本文中，我们建立了上下文ure理论。该理论表明，一类广泛的表示学习方法可以被表征为从输入与上下文变量之间的关联中学习。具体而言，我们证明了许多流行方法旨在近似由上下文诱导的期望算子的 top-d 奇异函数，在这种情况下，我们认为表示学习了上下文ure。我们通过证明，不同学习范式（监督学习、无监督学习和流形学习）内的表示学习都可以从这种视角进行研究，来证明上下文ure理论的普适性。我们还证明，学习了上下文ure的表示在与上下文相容的任务中是优化的。上下文ure理论的一个重要含义是，当模型足够大可以逼近顶级奇异函数时，进一步增加模型规模将不再具有显著效果。因此，不仅仅是规模扩展，进一步改进还需要更好的上下文。为此，我们研究如何在不了解下游任务的情况下评估上下文的有效性。我们提出了一个指标，并通过实验表明它与许多真实数据集上编码器的实际性能有很好的相关性。', 'title_zh': '上下文中的表现：来自上下文的表示'}
{'arxiv_id': 'arXiv:2505.01531', 'title': 'An Adaptive Framework for Autoregressive Forecasting in CFD Using Hybrid Modal Decomposition and Deep Learning', 'authors': 'Rodrigo Abadía-Heredia, Manuel Lopez-Martin, Soledad Le Clainche', 'link': 'https://arxiv.org/abs/2505.01531', 'abstract': "This work presents, to the best of the authors' knowledge, the first generalizable and fully data-driven adaptive framework designed to stabilize deep learning (DL) autoregressive forecasting models over long time horizons, with the goal of reducing the computational cost required in computational fluid dynamics (CFD) this http URL proposed methodology alternates between two phases: (i) predicting the evolution of the flow field over a selected time interval using a trained DL model, and (ii) updating the model with newly generated CFD data when stability degrades, thus maintaining accurate long-term forecasting. This adaptive retraining strategy ensures robustness while avoiding the accumulation of predictive errors typical in autoregressive models. The framework is validated across three increasingly complex flow regimes, from laminar to turbulent, demonstrating from 30 \\% to 95 \\% reduction in computational cost without compromising physical consistency or accuracy. Its entirely data-driven nature makes it easily adaptable to a wide range of time-dependent simulation problems. The code implementing this methodology is available as open-source and it will be integrated into the upcoming release of the ModelFLOWs-app.", 'abstract_zh': '本工作据作者所知，首次提出了一个可用于在长时间范围内稳定深度学习自回归预测模型的一般化和完全数据驱动的自适应框架，旨在降低计算流体动力学（CFD）领域的计算成本。所提出的方法交替进行两个阶段：（i）使用训练好的深度学习模型预测选定时间间隔内流场的演化；（ii）当模型稳定性下降时，通过更新模型以新生成的CFD数据维持长期准确预测。该自适应重新训练策略确保了鲁棒性，同时避免了自回归模型中常见的预测误差累积问题。该框架在从层流到湍流的三种日益复杂的流场中得到了验证，展示了在不牺牲物理一致性和准确性的前提下，计算成本最多可降低95%。其完全数据驱动的特性使其能够轻松适应各种时间依赖性 simulation 问题。该方法的实现代码已开源，并将在即将发布的 ModelFLOWs-app 中集成。', 'title_zh': 'CFD中基于混合模式分解和深度学习的自适应自回归预测框架'}
{'arxiv_id': 'arXiv:2505.01530', 'title': 'Automated Parsing of Engineering Drawings for Structured Information Extraction Using a Fine-tuned Document Understanding Transformer', 'authors': 'Muhammad Tayyab Khan, Zane Yong, Lequn Chen, Jun Ming Tan, Wenhe Feng, Seung Ki Moon', 'link': 'https://arxiv.org/abs/2505.01530', 'abstract': 'Accurate extraction of key information from 2D engineering drawings is crucial for high-precision manufacturing. Manual extraction is time-consuming and error-prone, while traditional Optical Character Recognition (OCR) techniques often struggle with complex layouts and overlapping symbols, resulting in unstructured outputs. To address these challenges, this paper proposes a novel hybrid deep learning framework for structured information extraction by integrating an oriented bounding box (OBB) detection model with a transformer-based document parsing model (Donut). An in-house annotated dataset is used to train YOLOv11 for detecting nine key categories: Geometric Dimensioning and Tolerancing (GD&T), General Tolerances, Measures, Materials, Notes, Radii, Surface Roughness, Threads, and Title Blocks. Detected OBBs are cropped into images and labeled to fine-tune Donut for structured JSON output. Fine-tuning strategies include a single model trained across all categories and category-specific models. Results show that the single model consistently outperforms category-specific ones across all evaluation metrics, achieving higher precision (94.77% for GD&T), recall (100% for most), and F1 score (97.3%), while reducing hallucination (5.23%). The proposed framework improves accuracy, reduces manual effort, and supports scalable deployment in precision-driven industries.', 'abstract_zh': '一种集成定向包围盒检测模型和变压器文档解析模型的新型混合深度学习框架用于从二维工程图中精确提取关键信息', 'title_zh': '基于微调文档理解变换器的工程图纸自动化解析与结构化信息提取'}
{'arxiv_id': 'arXiv:2505.01524', 'title': 'The DCR Delusion: Measuring the Privacy Risk of Synthetic Data', 'authors': 'Zexi Yao, Nataša Krčo, Georgi Ganev, Yves-Alexandre de Montjoye', 'link': 'https://arxiv.org/abs/2505.01524', 'abstract': 'Synthetic data has become an increasingly popular way to share data without revealing sensitive information. Though Membership Inference Attacks (MIAs) are widely considered the gold standard for empirically assessing the privacy of a synthetic dataset, practitioners and researchers often rely on simpler proxy metrics such as Distance to Closest Record (DCR). These metrics estimate privacy by measuring the similarity between the training data and generated synthetic data. This similarity is also compared against that between the training data and a disjoint holdout set of real records to construct a binary privacy test. If the synthetic data is not more similar to the training data than the holdout set is, it passes the test and is considered private. In this work we show that, while computationally inexpensive, DCR and other distance-based metrics fail to identify privacy leakage. Across multiple datasets and both classical models such as Baynet and CTGAN and more recent diffusion models, we show that datasets deemed private by proxy metrics are highly vulnerable to MIAs. We similarly find both the binary privacy test and the continuous measure based on these metrics to be uninformative of actual membership inference risk. We further show that these failures are consistent across different metric hyperparameter settings and record selection methods. Finally, we argue DCR and other distance-based metrics to be flawed by design and show a example of a simple leakage they miss in practice. With this work, we hope to motivate practitioners to move away from proxy metrics to MIAs as the rigorous, comprehensive standard of evaluating privacy of synthetic data, in particular to make claims of datasets being legally anonymous.', 'abstract_zh': '合成数据已成为一种日益流行的方式，用于在不泄露敏感信息的情况下共享数据。尽管成员推断攻击（MIAs）被广泛认为是评估合成数据集隐私性的黄金标准，但实践者和研究人员经常依赖更简单的代理指标，如最近邻记录距离（DCR）。这些指标通过测量训练数据和生成的合成数据之间的相似性来估计隐私性。这种相似性还会与训练数据和一个分离的保留的真实记录集之间的相似性进行比较，以构建二元隐私测试。如果合成数据与训练数据的相似度不超过保留集与训练数据的相似度，那么它就通过测试，并被认为是私密的。在本研究中，我们展示了尽管计算成本低，DCR和其他基于距离的指标无法识别隐私泄露。我们在多个数据集以及从经典模型（如Baynet和CTGAN）到更近期的扩散模型中均展示了由代理指标认定为私密的数据集对MIAs高度易感。我们还发现，基于这些指标的二元隐私测试和连续度量指标对于实际的成员推断风险并无说明性。进一步研究表明，这些失败在不同的度量超参数设置和记录选择方法下是一致的。最后，我们论证设计上的缺陷使得DCR和其他基于距离的指标在实践中忽略了重要的隐私泄露。通过本项研究，我们希望促使实践者转向MIAs作为评估合成数据隐私性的严格且全面的标准，特别是对于宣称数据集具有法律匿名性的说法。', 'title_zh': 'DCR谬论：合成数据的隐私风险衡量'}
{'arxiv_id': 'arXiv:2505.01514', 'title': 'Securing the Future of IVR: AI-Driven Innovation with Agile Security, Data Regulation, and Ethical AI Integration', 'authors': 'Khushbu Mehboob Shaikh, Georgios Giannakopoulos', 'link': 'https://arxiv.org/abs/2505.01514', 'abstract': 'The rapid digitalization of communication systems has elevated Interactive Voice Response (IVR) technologies to become critical interfaces for customer engagement. With Artificial Intelligence (AI) now driving these platforms, ensuring secure, compliant, and ethically designed development practices is more imperative than ever. AI-powered IVRs leverage Natural Language Processing (NLP) and Machine Learning (ML) to personalize interactions, automate service delivery, and optimize user experiences. However, these innovations expose systems to heightened risks, including data privacy breaches, AI decision opacity, and model security vulnerabilities. This paper analyzes the evolution of IVRs from static code-based designs to adaptive AI-driven systems, presenting a cybersecurity-centric perspective. We propose a practical governance framework that embeds agile security principles, compliance with global data legislation, and user-centric ethics. Emphasizing privacy-by-design, adaptive risk modeling, and transparency, the paper argues that ethical AI integration is not a feature but a strategic imperative. Through this multidimensional lens, we highlight how modern IVRs can transition from communication tools to intelligent, secure, and accountable digital frontlines-resilient against emerging threats and aligned with societal expectations.', 'abstract_zh': '快速数字化通信系统促使交互式语音响应（IVR）技术成为关键的客户交互界面。随着人工智能（AI）驱动这些平台的发展，确保安全、合规且伦理的设计开发实践比以往任何时候都更为重要。基于人工智能的IVR利用自然语言处理（NLP）和机器学习（ML）来个性化交互、自动化服务交付并优化用户体验。然而，这些创新使系统面临更高的风险，包括数据隐私泄露、AI决策透明度低以及模型安全漏洞。本文分析了IVR从静态代码设计进化到适应性AI驱动系统的历程，从网络安全的角度进行阐述。我们提出了一种实用的治理框架，整合了敏捷安全原则、符合全球数据立法要求以及用户中心的伦理观。强调设计中嵌入隐私、动态风险建模和透明度，本文认为伦理的人工智能集成不仅是功能，而是战略必要性。通过这一多维度视角，我们强调现代IVR如何从通信工具演进为智能、安全且负责任的数字前线，能够抵御新兴威胁并与社会期望保持一致。', 'title_zh': '基于敏捷安全、数据合规与伦理AI集成的智能语音识别未来 security of the future for ivr: ai驱动的创新'}
{'arxiv_id': 'arXiv:2505.01476', 'title': 'CostFilter-AD: Enhancing Anomaly Detection through Matching Cost Filtering', 'authors': 'Zhe Zhang, Mingxiu Cai, Hanxiao Wang, Gaochang Wu, Tianyou Chai, Xiatian Zhu', 'link': 'https://arxiv.org/abs/2505.01476', 'abstract': 'Unsupervised anomaly detection (UAD) seeks to localize the anomaly mask of an input image with respect to normal samples. Either by reconstructing normal counterparts (reconstruction-based) or by learning an image feature embedding space (embedding-based), existing approaches fundamentally rely on image-level or feature-level matching to derive anomaly scores. Often, such a matching process is inaccurate yet overlooked, leading to sub-optimal detection. To address this issue, we introduce the concept of cost filtering, borrowed from classical matching tasks, such as depth and flow estimation, into the UAD problem. We call this approach {\\em CostFilter-AD}. Specifically, we first construct a matching cost volume between the input and normal samples, comprising two spatial dimensions and one matching dimension that encodes potential matches. To refine this, we propose a cost volume filtering network, guided by the input observation as an attention query across multiple feature layers, which effectively suppresses matching noise while preserving edge structures and capturing subtle anomalies. Designed as a generic post-processing plug-in, CostFilter-AD can be integrated with either reconstruction-based or embedding-based methods. Extensive experiments on MVTec-AD and VisA benchmarks validate the generic benefits of CostFilter-AD for both single- and multi-class UAD tasks. Code and models will be released at this https URL.', 'abstract_zh': '无监督异常检测中的成本过滤方法（CostFilter-AD）', 'title_zh': 'CostFilter-AD: 通过匹配成本过滤增强异常检测'}
{'arxiv_id': 'arXiv:2505.01475', 'title': 'BiGSCoder: State Space Model for Code Understanding', 'authors': 'Shweta Verma, Abhinav Anand, Mira Mezini', 'link': 'https://arxiv.org/abs/2505.01475', 'abstract': "We present BiGSCoder, a novel encoder-only bidirectional state-space model (SSM) featuring a gated architecture, pre-trained for code understanding on a code dataset using masked language modeling. Our work aims to systematically evaluate SSMs' capabilities in coding tasks compared to traditional transformer architectures; BiGSCoder is built for this purpose. Through comprehensive experiments across diverse pre-training configurations and code understanding benchmarks, we demonstrate that BiGSCoder outperforms transformer-based models, despite utilizing simpler pre-training strategies and much less training data. Our results indicate that BiGSCoder can serve as a more sample-efficient alternative to conventional transformer models. Furthermore, our study shows that SSMs perform better without positional embeddings and can effectively extrapolate to longer sequences during fine-tuning.", 'abstract_zh': 'BiGSCoder:一个基于门控架构的新型编码器-only双向状态空间模型及其在代码理解任务上的表现分析', 'title_zh': 'BiGSCoder: 状态空间模型在代码理解中的应用'}
{'arxiv_id': 'arXiv:2505.01474', 'title': 'Watermark Overwriting Attack on StegaStamp algorithm', 'authors': 'I.F.Serzhenko, L.A.Khaertdinova, M.A.Pautov, A.V.Antsiferova', 'link': 'https://arxiv.org/abs/2505.01474', 'abstract': 'This paper presents an attack method on the StegaStamp watermarking algorithm that completely removes watermarks from an image with minimal quality loss, developed as part of the NeurIPS "Erasing the invisible" competition.', 'abstract_zh': '本文介绍了在NeurIPS“消除无形之物”竞赛中开发的一种攻击方法，该方法能够从图像中完全移除StegaStamp水印，同时最大限度地减少质量损失。', 'title_zh': 'StegaStamp算法中的水印覆盖攻击'}
{'arxiv_id': 'arXiv:2505.01445', 'title': 'Explainable AI for Correct Root Cause Analysis of Product Quality in Injection Moulding', 'authors': 'Muhammad Muaz, Sameed Sajid, Tobias Schulze, Chang Liu, Nils Klasen, Benny Drescher', 'link': 'https://arxiv.org/abs/2505.01445', 'abstract': "If a product deviates from its desired properties in the injection moulding process, its root cause analysis can be aided by models that relate the input machine settings with the output quality characteristics. The machine learning models tested in the quality prediction are mostly black boxes; therefore, no direct explanation of their prognosis is given, which restricts their applicability in the quality control. The previously attempted explainability methods are either restricted to tree-based algorithms only or do not emphasize on the fact that some explainability methods can lead to wrong root cause identification of a product's deviation from its desired properties. This study first shows that the interactions among the multiple input machine settings do exist in real experimental data collected as per a central composite design. Then, the model-agnostic explainable AI methods are compared for the first time to show that different explainability methods indeed lead to different feature impact analysis in injection moulding. Moreover, it is shown that the better feature attribution translates to the correct cause identification and actionable insights for the injection moulding process. Being model agnostic, explanations on both random forest and multilayer perceptron are performed for the cause analysis, as both models have the mean absolute percentage error of less than 0.05% on the experimental dataset.", 'abstract_zh': '如果产品在注射模具过程中偏离其预期属性，通过关联输入机器设置与输出质量特征的模型进行根本原因分析可以帮助识别其根源。在质量预测中测试的机器学习模型大多是黑盒模型，因此无法直接解释其预测结果，这限制了它们在质量控制中的应用。之前尝试的可解释性方法要么仅适用于基于树的算法，要么没有强调某些可解释性方法可能导致产品偏离预期属性的错误根本原因识别。本研究首先展示了在根据中心复合设计收集的实际实验数据中，多个输入机器设置之间的相互作用确实存在。然后，首次将模型无偏差的解释性AI方法进行对比，以展示不同的解释性方法确实会导致注射模具过程中不同特征影响的分析结果。此外，研究还显示，更好的特征归因能够实现正确的根本原因识别和对注射模具过程的可操作性见解。由于两种模型（随机森林和多层感知机）在实验数据集上的平均绝对百分比误差均低于0.05%，因此对这两种模型进行了原因分析的解释。', 'title_zh': '可解释的人工智能在注塑产品质量故障根因分析中的应用'}
{'arxiv_id': 'arXiv:2505.01438', 'title': 'Global Stress Generation and Spatiotemporal Super-Resolution Physics-Informed Operator under Dynamic Loading for Two-Phase Random Materials', 'authors': 'Tengfei Xing, Xiaodan Ren, Jie Li', 'link': 'https://arxiv.org/abs/2505.01438', 'abstract': 'Material stress analysis is a critical aspect of material design and performance optimization. Under dynamic loading, the global stress evolution in materials exhibits complex spatiotemporal characteristics, especially in two-phase random materials (TRMs). Such kind of material failure is often associated with stress concentration, and the phase boundaries are key locations where stress concentration occurs. In practical engineering applications, the spatiotemporal resolution of acquired microstructural data and its dynamic stress evolution is often limited. This poses challenges for deep learning methods in generating high-resolution spatiotemporal stress fields, particularly for accurately capturing stress concentration regions. In this study, we propose a framework for global stress generation and spatiotemporal super-resolution in TRMs under dynamic loading. First, we introduce a diffusion model-based approach, named as Spatiotemporal Stress Diffusion (STS-diffusion), for generating global spatiotemporal stress data. This framework incorporates Space-Time U-Net (STU-net), and we systematically investigate the impact of different attention positions on model accuracy. Next, we develop a physics-informed network for spatiotemporal super-resolution, termed as Spatiotemporal Super-Resolution Physics-Informed Operator (ST-SRPINN). The proposed ST-SRPINN is an unsupervised learning method. The influence of data-driven and physics-informed loss function weights on model accuracy is explored in detail. Benefiting from physics-based constraints, ST-SRPINN requires only low-resolution stress field data during training and can upscale the spatiotemporal resolution of stress fields to arbitrary magnifications.', 'abstract_zh': '动态载荷下两相随机材料的全局应力生成与时空超分辨率分析', 'title_zh': '全球尺度应力生成及动态加载下两相随机材料的时空超分辨率物理约束操作符'}
{'arxiv_id': 'arXiv:2505.01437', 'title': 'Enhancing IoT-Botnet Detection using Variational Auto-encoder and Cost-Sensitive Learning: A Deep Learning Approach for Imbalanced Datasets', 'authors': 'Hassan Wasswa, Timothy Lynar, Hussein Abbass', 'link': 'https://arxiv.org/abs/2505.01437', 'abstract': 'The Internet of Things (IoT) technology has rapidly gained popularity with applications widespread across a variety of industries. However, IoT devices have been recently serving as a porous layer for many malicious attacks to both personal and enterprise information systems with the most famous attacks being botnet-related attacks. The work in this study leveraged Variational Auto-encoder (VAE) and cost-sensitive learning to develop lightweight, yet effective, models for IoT-botnet detection. The aim is to enhance the detection of minority class attack traffic instances which are often missed by machine learning models. The proposed approach is evaluated on a multi-class problem setting for the detection of traffic categories on highly imbalanced datasets. The performance of two deep learning models including the standard feed forward deep neural network (DNN), and Bidirectional-LSTM (BLSTM) was evaluated and both recorded commendable results in terms of accuracy, precision, recall and F1-score for all traffic classes.', 'abstract_zh': '物联网（IoT）技术由于在多个行业中广泛应用而迅速受到关注。然而，IoT设备最近成为了许多针对个人和企业信息系统恶意攻击的薄弱环节，其中最著名的攻击是僵尸网络相关的攻击。本研究利用变分自编码器（VAE）和成本敏感学习开发了轻量级而有效的模型来检测IoT-僵尸网络。目标是增强对通常被机器学习模型所忽略的少数类攻击流量实例的检测能力。该提出的方法在高度不平衡数据集的多类问题设置下对流量类别进行了评估，并评估了两种深度学习模型，包括标准前向深度神经网络（DNN）和双向长短期记忆（BLSTM），结果显示这两种模型在所有流量类别的准确性、精确率、召回率和F1分数方面均取得了令人满意的结果。', 'title_zh': '使用变分自编码器和成本敏感学习增强物联网僵尸网络检测：不平衡数据集的深度学习方法'}
{'arxiv_id': 'arXiv:2504.18793', 'title': 'Building Scalable AI-Powered Applications with Cloud Databases: Architectures, Best Practices and Performance Considerations', 'authors': 'Santosh Bhupathi', 'link': 'https://arxiv.org/abs/2504.18793', 'abstract': 'The rapid adoption of AI-powered applications demands high-performance, scalable, and efficient cloud database solutions, as traditional architectures often struggle with AI-driven workloads requiring real-time data access, vector search, and low-latency queries. This paper explores how cloud-native databases enable AI-driven applications by leveraging purpose-built technologies such as vector databases (pgvector), graph databases (AWS Neptune), NoSQL stores (Amazon DocumentDB, DynamoDB), and relational cloud databases (Aurora MySQL and PostgreSQL). It presents architectural patterns for integrating AI workloads with cloud databases, including Retrieval-Augmented Generation (RAG) [1] with LLMs, real-time data pipelines, AI-driven query optimization, and embeddings-based search. Performance benchmarks, scalability considerations, and cost-efficient strategies are evaluated to guide the design of AI-enabled applications. Real-world case studies from industries such as healthcare, finance, and customer experience illustrate how enterprises utilize cloud databases to enhance AI capabilities while ensuring security, governance, and compliance with enterprise and regulatory standards. By providing a comprehensive analysis of AI and cloud database integration, this paper serves as a practical guide for researchers, architects, and enterprises to build next-generation AI applications that optimize performance, scalability, and cost efficiency in cloud environments.', 'abstract_zh': 'AI驱动应用的快速采用需求高性能、可扩展和高效的云数据库解决方案，传统架构往往难以应对由实时数据访问、向量搜索和低延迟查询等AI驱动工作负载带来的挑战。本文探讨了云原生数据库如何通过利用矢量数据库（pgvector）、图数据库（AWS Neptune）、NoSQL存储（Amazon DocumentDB、DynamoDB）以及关系型云数据库（Aurora MySQL和PostgreSQL）等专用技术，支持AI驱动的应用程序。本文介绍了将AI工作负载与云数据库集成的架构模式，包括与LLMs结合的检索增强生成（RAG）、实时数据管道、基于AI的查询优化以及嵌入式搜索。性能基准测试、可扩展性考虑和成本优化策略被评估以指导AI驱动应用的设计。本文通过医疗、金融和客户体验等行业的真实案例研究，展示了企业如何利用云数据库增强AI能力，同时确保安全、治理和符合企业及监管标准。本文通过对AI和云数据库集成的全面分析，为研究人员、架构师和企业提供了一个实用指南，以在云环境中构建优化性能、可扩展性和成本效率的下一代AI应用程序。', 'title_zh': '基于云数据库构建可扩展的AI驱动应用：架构、最佳实践及性能考虑'}
{'arxiv_id': 'arXiv:2503.02910', 'title': 'LangGas: Introducing Language in Selective Zero-Shot Background Subtraction for Semi-Transparent Gas Leak Detection with a New Dataset', 'authors': 'Wenqi Guo, Yiyang Du, Shan Du', 'link': 'https://arxiv.org/abs/2503.02910', 'abstract': 'Gas leakage poses a significant hazard that requires prevention. Traditionally, human inspection has been used for detection, a slow and labour-intensive process. Recent research has applied machine learning techniques to this problem, yet there remains a shortage of high-quality, publicly available datasets. This paper introduces a synthetic dataset, SimGas, featuring diverse backgrounds, interfering foreground objects, diverse leak locations, and precise segmentation ground truth. We propose a zero-shot method that combines background subtraction, zero-shot object detection, filtering, and segmentation to leverage this dataset. Experimental results indicate that our approach significantly outperforms baseline methods based solely on background subtraction and zero-shot object detection with segmentation, reaching an IoU of 69%. We also present an analysis of various prompt configurations and threshold settings to provide deeper insights into the performance of our method. Finally, we qualitatively (because of the lack of ground truth) tested our performance on GasVid and reached decent results on the real-world dataset. The dataset, code, and full qualitative results are available at this https URL.', 'abstract_zh': '气体泄漏检测是亟待预防的重要隐患。传统的人工检测方法耗时且劳动密集。近期研究已应用机器学习技术解决该问题，但高质量的公开数据集仍有短缺。本文介绍了合成数据集SimGas，该数据集包含多样背景、干扰前景对象、多样泄漏位置以及精确的分割标注。我们提出了一种零样本方法，结合背景减除、零样本物体检测、过滤和分割，以利用该数据集。实验结果显示，与仅基于背景减除和零样本物体检测分割的基线方法相比，我们的方法显著提高了性能，达到IoU 69%。我们还分析了各种提示配置和阈值设置，以提供对我们方法性能的更深入见解。最后，由于缺乏地面 truth，我们在GasVid上进行了定性测试，并在真实数据集上获得了不错的结果。数据集、代码和完整定性结果可在以下链接获取：this https URL。', 'title_zh': 'LangGas: 在半透明气体泄漏检测的有条件零样本背景减除中引入语言信息与新数据集'}
