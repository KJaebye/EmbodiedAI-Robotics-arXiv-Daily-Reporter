{'arxiv_id': 'arXiv:2505.02833', 'title': 'TWIST: Teleoperated Whole-Body Imitation System', 'authors': 'Yanjie Ze, Zixuan Chen, João Pedro Araújo, Zi-ang Cao, Xue Bin Peng, Jiajun Wu, C. Karen Liu', 'link': 'https://arxiv.org/abs/2505.02833', 'abstract': 'Teleoperating humanoid robots in a whole-body manner marks a fundamental step toward developing general-purpose robotic intelligence, with human motion providing an ideal interface for controlling all degrees of freedom. Yet, most current humanoid teleoperation systems fall short of enabling coordinated whole-body behavior, typically limiting themselves to isolated locomotion or manipulation tasks. We present the Teleoperated Whole-Body Imitation System (TWIST), a system for humanoid teleoperation through whole-body motion imitation. We first generate reference motion clips by retargeting human motion capture data to the humanoid robot. We then develop a robust, adaptive, and responsive whole-body controller using a combination of reinforcement learning and behavior cloning (RL+BC). Through systematic analysis, we demonstrate how incorporating privileged future motion frames and real-world motion capture (MoCap) data improves tracking accuracy. TWIST enables real-world humanoid robots to achieve unprecedented, versatile, and coordinated whole-body motor skills--spanning whole-body manipulation, legged manipulation, locomotion, and expressive movement--using a single unified neural network controller. Our project website: this https URL', 'abstract_zh': '全身动作模仿的远程操作人形机器人系统(TWIST)：通过全身动作模仿进行人形机器人远程操作标志着开发通用机器人智能的基础步骤，人类动作提供了控制所有自由度的理想接口。然而，当前大多数人形远程操作系统尚不能实现协调的全身行为，通常仅限于孤立的移动或操作任务。我们提出了全身动作模仿远程操作系统(TWIST)，这是一种通过全身动作模仿进行人形机器人远程操作的系统。我们首先通过将人类动作捕捉数据重新定向到人形机器人来生成参考动作片段。然后，我们开发了一个健壮、自适应和响应迅速的全身控制器，结合使用强化学习和行为克隆(RL+BC)。通过系统分析，我们展示了如何引入未来的动作帧和真实世界动作捕捉(MoCap)数据以提高跟踪精度。TWIST使现实世界的人形机器人能够实现前所未有的多功能和协调的全身运动技能，涵盖全身操作、腿式操作、移动和表情动作，仅使用一个统一的神经网络控制器。项目网站: this https URL。', 'title_zh': 'TWIST: 远程操控全身模仿系统'}
{'arxiv_id': 'arXiv:2505.02744', 'title': 'Re-purposing a modular origami manipulator into an adaptive physical computer for machine learning and robotic perception', 'authors': 'Jun Wang, Suyi Li', 'link': 'https://arxiv.org/abs/2505.02744', 'abstract': 'Physical computing has emerged as a powerful tool for performing intelligent tasks directly in the mechanical domain of functional materials and robots, reducing our reliance on the more traditional COMS computers. However, no systematic study explains how mechanical design can influence physical computing performance. This study sheds insights into this question by repurposing an origami-inspired modular robotic manipulator into an adaptive physical reservoir and systematically evaluating its computing capacity with different physical configurations, input setups, and computing tasks. By challenging this adaptive reservoir computer to complete the classical NARMA benchmark tasks, this study shows that its time series emulation performance directly correlates to the Peak Similarity Index (PSI), which quantifies the frequency spectrum correlation between the target output and reservoir dynamics. The adaptive reservoir also demonstrates perception capabilities, accurately extracting its payload weight and orientation information from the intrinsic dynamics. Importantly, such information extraction capability can be measured by the spatial correlation between nodal dynamics within the reservoir body. Finally, by integrating shape memory alloy (SMA) actuation, this study demonstrates how to exploit such computing power embodied in the physical body for practical, robotic operations. This study provides a strategic framework for harvesting computing power from soft robots and functional materials, demonstrating how design parameters and input selection can be configured based on computing task requirements. Extending this framework to bio-inspired adaptive materials, prosthetics, and self-adaptive soft robotic systems could enable next-generation embodied intelligence, where the physical structure can compute and interact with their digital counterparts.', 'abstract_zh': '物理计算在功能材料和机器人机械领域直接执行智能任务中 emerge 作为一种强大工具，减少了我们对传统 COMS 计算机的依赖。然而，尚未有系统性的研究解释机械设计如何影响物理计算性能。本研究通过将受 Origami 启发的模块化机器人 manipulator 重新用于自适应物理蓄能池，并系统地评估其在不同物理配置、输入设置和计算任务下的计算能力，从而揭示了这一问题。通过要求这种自适应蓄能池完成经典的 NARMA 标准测试任务，本研究显示其时间序列模拟性能直接与峰值相似度指数 (PSI) 相关，该指数量化了目标输出和蓄能池动力学之间的频谱关联。自适应蓄能池还展示了感知能力，能够准确地从内在动力学中提取其承载物的重量和姿态信息。重要的是，这种信息提取能力可以通过蓄能池体内节点动力学的空间相关性来测量。最后，通过整合形状记忆合金 (SMA) 执行机构，本研究展示了如何利用物理身体中嵌入的这种计算能力进行实际的机器人操作。本研究为从软机器人和功能材料中获取计算能力提供了战略框架，展示了如何根据计算任务要求配置设计参数和输入选择。将该框架扩展到生物启发的自适应材料、假肢和自适应软机器人系统，可以实现下一代嵌入式智能，其中物理结构能够计算并与数字对应物进行交互。', 'title_zh': '将模块化 Origami 操作臂重新用于自适应物理计算机以实现机器学习与机器人感知'}
{'arxiv_id': 'arXiv:2505.02664', 'title': 'Grasp the Graph (GtG) 2.0: Ensemble of GNNs for High-Precision Grasp Pose Detection in Clutter', 'authors': 'Ali Rashidi Moghadam, Sayedmohammadreza Rastegari, Mehdi Tale Masouleh, Ahmad Kalhor', 'link': 'https://arxiv.org/abs/2505.02664', 'abstract': 'Grasp pose detection in cluttered, real-world environments remains a significant challenge due to noisy and incomplete sensory data combined with complex object geometries. This paper introduces Grasp the Graph 2.0 (GtG 2.0) method, a lightweight yet highly effective hypothesis-and-test robotics grasping framework which leverages an ensemble of Graph Neural Networks for efficient geometric reasoning from point cloud data. Building on the success of GtG 1.0, which demonstrated the potential of Graph Neural Networks for grasp detection but was limited by assumptions of complete, noise-free point clouds and 4-Dof grasping, GtG 2.0 employs a conventional Grasp Pose Generator to efficiently produce 7-Dof grasp candidates. Candidates are assessed with an ensemble Graph Neural Network model which includes points within the gripper jaws (inside points) and surrounding contextual points (outside points). This improved representation boosts grasp detection performance over previous methods using the same generator. GtG 2.0 shows up to a 35% improvement in Average Precision on the GraspNet-1Billion benchmark compared to hypothesis-and-test and Graph Neural Network-based methods, ranking it among the top three frameworks. Experiments with a 3-Dof Delta Parallel robot and Kinect-v1 camera show a success rate of 91% and a clutter completion rate of 100%, demonstrating its flexibility and reliability.', 'abstract_zh': 'Grasp Pose Detection in Cluttered, Real-World Environments via Grasp the Graph 2.0 (GtG 2.0) Method', 'title_zh': 'Grasp the Graph (GtG) 2.0: 基于GNNs的高精度杂乱环境中抓取姿态检测集成方法'}
{'arxiv_id': 'arXiv:2505.02598', 'title': 'LiDAR-Inertial SLAM-Based Navigation and Safety-Oriented AI-Driven Control System for Skid-Steer Robots', 'authors': 'Mehdi Heydari Shahna, Eemil Haaparanta, Pauli Mustalahti, Jouni Mattila', 'link': 'https://arxiv.org/abs/2505.02598', 'abstract': 'Integrating artificial intelligence (AI) and stochastic technologies into the mobile robot navigation and control (MRNC) framework while adhering to rigorous safety standards presents significant challenges. To address these challenges, this paper proposes a comprehensively integrated MRNC framework for skid-steer wheeled mobile robots (SSWMRs), in which all components are actively engaged in real-time execution. The framework comprises: 1) a LiDAR-inertial simultaneous localization and mapping (SLAM) algorithm for estimating the current pose of the robot within the built map; 2) an effective path-following control system for generating desired linear and angular velocity commands based on the current pose and the desired pose; 3) inverse kinematics for transferring linear and angular velocity commands into left and right side velocity commands; and 4) a robust AI-driven (RAID) control system incorporating a radial basis function network (RBFN) with a new adaptive algorithm to enforce in-wheel actuation systems to track each side motion commands. To further meet safety requirements, the proposed RAID control within the MRNC framework of the SSWMR constrains AI-generated tracking performance within predefined overshoot and steady-state error limits, while ensuring robustness and system stability by compensating for modeling errors, unknown RBF weights, and external forces. Experimental results verify the proposed MRNC framework performance for a 4,836 kg SSWMR operating on soft terrain.', 'abstract_zh': '将人工智能和随机技术集成到履带式移动机器人导航与控制框架中，同时遵守严格的安全标准，面临着显著的挑战。本文提出了一种全面集成的履带式移动机器人（SSWMR）导航与控制框架，所有组件都在实时执行中积极参与，该框架包括：1）LiDAR-惯性同时定位与建图（SLAM）算法，用于估计机器人在构建地图中的当前位置；2）有效的路径跟踪控制系统，根据当前位置和期望位置生成期望的线性和角速度指令；3）逆运动学，将线性和角速度指令转换为左右侧速度指令；4）一种鲁棒的基于人工智能驱动（RAID）控制系统，结合径向基函数网络（RBFN）和一种新的自适应算法，以确保轮内执行系统能够跟踪每侧运动指令。为了进一步满足安全要求，提出的RAID控制限制了SSWMR MRNC框架中由人工智能生成的跟踪性能在预定义的超调和稳态误差限制内，同时通过补偿建模误差、未知RBF权重和外部力，确保系统的鲁棒性和稳定性。实验结果验证了该提出的MRNC框架在软地形上操作的4,836 kg SSWMR的性能。', 'title_zh': '基于LiDAR-惯性SLAM的履带式机器人导航及安全导向AI驱动控制系统'}
{'arxiv_id': 'arXiv:2505.02574', 'title': 'Learning and Online Replication of Grasp Forces from Electromyography Signals for Prosthetic Finger Control', 'authors': 'Robin Arbaud, Elisa Motta, Marco Domenico Avaro, Stefano Picinich, Marta Lorenzini, Arash Ajoudani', 'link': 'https://arxiv.org/abs/2505.02574', 'abstract': 'Partial hand amputations significantly affect the physical and psychosocial well-being of individuals, yet intuitive control of externally powered prostheses remains an open challenge. To address this gap, we developed a force-controlled prosthetic finger activated by electromyography (EMG) signals. The prototype, constructed around a wrist brace, functions as a supernumerary finger placed near the index, allowing for early-stage evaluation on unimpaired subjects. A neural network-based model was then implemented to estimate fingertip forces from EMG inputs, allowing for online adjustment of the prosthetic finger grip strength. The force estimation model was validated through experiments with ten participants, demonstrating its effectiveness in predicting forces. Additionally, online trials with four users wearing the prosthesis exhibited precise control over the device. Our findings highlight the potential of using EMG-based force estimation to enhance the functionality of prosthetic fingers.', 'abstract_zh': '部分手部截肢显著影响个体的生理和心理福祉，但外部动力假肢的直观控制仍是一项挑战。为解决这一问题，我们开发了一种由 electromyography (EMG) 信号控制的力控制假指，并将其原型构建于手腕固定器上，作为位于食指附近的人工额外手指，以在未受损个体上进行初步评估。随后，实现了一种基于神经网络的模型来从 EMG 输入中估计指尖力，从而实现假指握力的在线调整。通过十名参与者的实验验证了力估计模型的有效性，展示了其在预测力方面的性能。此外，四位穿戴该假肢的用户在线试验中展示了对设备的精确控制。我们的研究结果强调了使用基于 EMG 的力估计来增强假指功能的潜力。', 'title_zh': '基于 electromyography 信号的学习与在线复制握力控制arrings for 仿生指控制'}
{'arxiv_id': 'arXiv:2505.02569', 'title': 'HapticVLM: VLM-Driven Texture Recognition Aimed at Intelligent Haptic Interaction', 'authors': 'Muhammad Haris Khan, Miguel Altamirano Cabrera, Dmitrii Iarchuk, Yara Mahmoud, Daria Trinitatova, Issatay Tokmurziyev, Dzmitry Tsetserukou', 'link': 'https://arxiv.org/abs/2505.02569', 'abstract': "This paper introduces HapticVLM, a novel multimodal system that integrates vision-language reasoning with deep convolutional networks to enable real-time haptic feedback. HapticVLM leverages a ConvNeXt-based material recognition module to generate robust visual embeddings for accurate identification of object materials, while a state-of-the-art Vision-Language Model (Qwen2-VL-2B-Instruct) infers ambient temperature from environmental cues. The system synthesizes tactile sensations by delivering vibrotactile feedback through speakers and thermal cues via a Peltier module, thereby bridging the gap between visual perception and tactile experience. Experimental evaluations demonstrate an average recognition accuracy of 84.67% across five distinct auditory-tactile patterns and a temperature estimation accuracy of 86.7% based on a tolerance-based evaluation method with an 8°C margin of error across 15 scenarios. Although promising, the current study is limited by the use of a small set of prominent patterns and a modest participant pool. Future work will focus on expanding the range of tactile patterns and increasing user studies to further refine and validate the system's performance. Overall, HapticVLM presents a significant step toward context-aware, multimodal haptic interaction with potential applications in virtual reality, and assistive technologies.", 'abstract_zh': 'HapticVLM：一种将视觉-语言推理与深度卷积网络结合的新型多模态系统', 'title_zh': 'HapticVLM: 面向智能触觉交互的VLM驱动的纹理识别'}
{'arxiv_id': 'arXiv:2505.02543', 'title': 'Data-Driven Energy Modeling of Industrial IoT Systems: A Benchmarking Approach', 'authors': 'Dimitris Kallis, Moysis Symeonides, Marios D. Dikaiakos', 'link': 'https://arxiv.org/abs/2505.02543', 'abstract': 'The widespread adoption of IoT has driven the development of cyber-physical systems (CPS) in industrial environments, leveraging Industrial IoTs (IIoTs) to automate manufacturing processes and enhance productivity. The transition to autonomous systems introduces significant operational costs, particularly in terms of energy consumption. Accurate modeling and prediction of IIoT energy requirements are critical, but traditional physics- and engineering-based approaches often fall short in addressing these challenges comprehensively. In this paper, we propose a novel methodology for benchmarking and analyzing IIoT devices and applications to uncover insights into their power demands, energy consumption, and performance. To demonstrate this methodology, we develop a comprehensive framework and apply it to study an industrial CPS comprising an educational robotic arm, a conveyor belt, a smart camera, and a compute node. By creating micro-benchmarks and an end-to-end application within this framework, we create an extensive performance and power consumption dataset, which we use to train and analyze ML models for predicting energy usage from features of the application and the CPS system. The proposed methodology and framework provide valuable insights into the energy dynamics of industrial CPS, offering practical implications for researchers and practitioners aiming to enhance the efficiency and sustainability of IIoT-driven automation.', 'abstract_zh': '物联网的广泛应用推动了工业环境中的赛博物理系统（CPS）的发展，利用工业物联网（IIoTs）自动化工厂流程并提高生产效率。向自主系统过渡带来了显著的运营成本，尤其是在能源消耗方面。准确建模和预测IIoT的能源需求至关重要，但传统基于物理和工程的方法往往未能全面解决这些挑战。在本文中，我们提出了一种新的方法论，用于基准测试和分析IIoT设备和应用，以揭示其功率需求、能源消耗和性能的见解。为了验证此方法论，我们开发了一个综合框架，并将其应用于一个工业CPS的研究，该系统包括一个教育型机器人臂、传送带、智能相机和计算节点。通过在这个框架中创建微基准和端到端应用，我们生成了一个广泛的性能和功耗数据集，用于训练和分析机器学习模型，以从应用程序和CPS系统的特征预测能源消耗。所提出的框架和方法论为工业CPS中的能源动态提供了有价值的认识，对旨在提高IIoT驱动自动化效率和可持续性的研究者和实践者具有实际意义。', 'title_zh': '基于数据驱动的工业物联网系统能量建模：一个基准分析方法'}
{'arxiv_id': 'arXiv:2505.02483', 'title': 'Automated Hybrid Reward Scheduling via Large Language Models for Robotic Skill Learning', 'authors': 'Changxin Huang, Junyang Liang, Yanbin Chang, Jingzhao Xu, Jianqiang Li', 'link': 'https://arxiv.org/abs/2505.02483', 'abstract': "Enabling a high-degree-of-freedom robot to learn specific skills is a challenging task due to the complexity of robotic dynamics. Reinforcement learning (RL) has emerged as a promising solution; however, addressing such problems requires the design of multiple reward functions to account for various constraints in robotic motion. Existing approaches typically sum all reward components indiscriminately to optimize the RL value function and policy. We argue that this uniform inclusion of all reward components in policy optimization is inefficient and limits the robot's learning performance. To address this, we propose an Automated Hybrid Reward Scheduling (AHRS) framework based on Large Language Models (LLMs). This paradigm dynamically adjusts the learning intensity of each reward component throughout the policy optimization process, enabling robots to acquire skills in a gradual and structured manner. Specifically, we design a multi-branch value network, where each branch corresponds to a distinct reward component. During policy optimization, each branch is assigned a weight that reflects its importance, and these weights are automatically computed based on rules designed by LLMs. The LLM generates a rule set in advance, derived from the task description, and during training, it selects a weight calculation rule from the library based on language prompts that evaluate the performance of each branch. Experimental results demonstrate that the AHRS method achieves an average 6.48% performance improvement across multiple high-degree-of-freedom robotic tasks.", 'abstract_zh': '基于大规模语言模型的自动混合奖励调度框架使高自由度机器人学习特定技能', 'title_zh': '基于大型语言模型的自动化混合奖励调度在机器人技能学习中的应用'}
{'arxiv_id': 'arXiv:2505.02476', 'title': 'Point Cloud Recombination: Systematic Real Data Augmentation Using Robotic Targets for LiDAR Perception Validation', 'authors': 'Hubert Padusinski, Christian Steinhauser, Christian Scherl, Julian Gaal, Jacob Langner', 'link': 'https://arxiv.org/abs/2505.02476', 'abstract': 'The validation of LiDAR-based perception of intelligent mobile systems operating in open-world applications remains a challenge due to the variability of real environmental conditions. Virtual simulations allow the generation of arbitrary scenes under controlled conditions but lack physical sensor characteristics, such as intensity responses or material-dependent effects. In contrast, real-world data offers true sensor realism but provides less control over influencing factors, hindering sufficient validation. Existing approaches address this problem with augmentation of real-world point cloud data by transferring objects between scenes. However, these methods do not consider validation and remain limited in controllability because they rely on empirical data. We solve these limitations by proposing Point Cloud Recombination, which systematically augments captured point cloud scenes by integrating point clouds acquired from physical target objects measured in controlled laboratory environments. Thus enabling the creation of vast amounts and varieties of repeatable, physically accurate test scenes with respect to phenomena-aware occlusions with registered 3D meshes. Using the Ouster OS1-128 Rev7 sensor, we demonstrate the augmentation of real-world urban and rural scenes with humanoid targets featuring varied clothing and poses, for repeatable positioning. We show that the recombined scenes closely match real sensor outputs, enabling targeted testing, scalable failure analysis, and improved system safety. By providing controlled yet sensor-realistic data, our method enables trustworthy conclusions about the limitations of specific sensors in compound with their algorithms, e.g., object detection.', 'abstract_zh': '基于LiDAR的智能移动系统在开放世界应用中的感知验证因实际环境条件的多变性而面临挑战。虚拟模拟可以在受控条件下生成任意场景，但缺乏物理传感器特性，如强度响应或材料依赖效果。相比之下，真实世界数据提供了真实的传感器现实性，但在控制影响因素方面受到限制，妨碍了充分的验证。现有方法通过在场景之间转移对象来增强现实点云数据，但这些方法不考虑验证，且在可控性方面仍有限制，因为它们依赖于经验数据。我们通过提出点云重新组合解决了这些限制，该方法系统地通过将受控实验室环境中物理目标对象测量获取的点云整合到捕获的点云场景中来增强捕获的点云场景。因此，能够创建大量和多样化的可重复、物理准确的测试场景，考虑到现象感知的遮挡，并与注册的3D网格对齐。使用Ouster OS1-128 Rev7传感器，我们演示了通过引入人形目标（具有不同的服装和姿势）来增强真实世界的城市和农村场景，以实现可重复定位。结果显示，重新组合的场景与实际传感器输出高度一致，从而实现有针对性的测试、可扩展的故障分析和改进的系统安全性。通过提供受控但传感器现实的数据，我们的方法能够就特定传感器与其算法组合的限制性得出可信结论，例如物体检测。', 'title_zh': '点云重组：用于LiDAR感知验证的机器人目标驱动的系统化现实数据增强'}
{'arxiv_id': 'arXiv:2505.02460', 'title': 'ZeloS -- A Research Platform for Early-Stage Validation of Research Findings Related to Automated Driving', 'authors': 'Christopher Bohn, Florian Siebenrock, Janne Bosch, Tobias Hetzner, Samuel Mauch, Philipp Reis, Timo Staudt, Manuel Hess, Ben-Micha Piscol, Sören Hohmann', 'link': 'https://arxiv.org/abs/2505.02460', 'abstract': "This paper presents ZeloS, a research platform designed and built for practical validation of automated driving methods in an early stage of research. We overview ZeloS' hardware setup and automation architecture and focus on motion planning and control. ZeloS weighs 69 kg, measures a length of 117 cm, and is equipped with all-wheel steering, all-wheel drive, and various onboard sensors for localization. The hardware setup and the automation architecture of ZeloS are designed and built with a focus on modularity and the goal of being simple yet effective. The modular design allows the modification of individual automation modules without the need for extensive onboarding into the automation architecture. As such, this design supports ZeloS in being a versatile research platform for validating various automated driving methods. The motion planning component and control of ZeloS feature optimization-based methods that allow for explicitly considering constraints. We demonstrate the hardware and automation setup by presenting experimental data.", 'abstract_zh': '本文介绍了ZeloS，一个用于早期研究阶段自动驾驶方法实践验证的研究平台。我们概述了ZeloS的硬件配置和自动化架构，并重点介绍了运动规划与控制。', 'title_zh': 'ZeloS -- 一种用于自动化驾驶相关研究发现早期验证的研究平台'}
{'arxiv_id': 'arXiv:2505.02414', 'title': 'Quadrupedal Spine Control Strategies: Exploring Correlations Between System Dynamic Responses and Human Perspectives', 'authors': 'Nicholas Hafner, Chaoran Liu, Carlos Ishi, Hiroshi Ishiguro', 'link': 'https://arxiv.org/abs/2505.02414', 'abstract': 'Unlike their biological cousins, the majority of existing quadrupedal robots are constructed with rigid chassis. This results in motion that is either beetle-like or distinctly robotic, lacking the natural fluidity characteristic of mammalian movements. Existing literature on quadrupedal robots with spinal configurations primarily focuses on energy efficiency and does not consider the effects in human-robot interaction scenarios. Our contributions include an initial investigation into various trajectory generation strategies for a quadrupedal robot with a four degree of freedom spine, and an analysis on the effect that such methods have on human perception of gait naturalness compared to a fixed spine baseline. The strategies were evaluated using videos of walking, trotting and turning simulations. Among the four different strategies developed, the optimised time varying and the foot-tracking strategies were perceived to be more natural than the baseline in a randomised trial with 50 participants. Although none of the strategies demonstrated any energy efficiency improvements over the no-spine baseline, some showed greater footfall consistency at higher speeds. Given the greater likeability drawn from the more natural locomotion patterns, this type of robot displays potential for applications in social robot scenarios such as elderly care, where energy efficiency is not a primary concern.', 'abstract_zh': '不同于其生物原型，现有大多数四足机器人采用刚性机身构造，导致其运动方式要么像甲虫，要么显得十足机械，缺乏哺乳动物运动的自然流畅性。关于具有脊柱配置的四足机器人的现有文献主要集中在能量效率上，并未考虑其在人机交互场景中的影响。本研究的贡献在于初步探讨了四足机器人四种自由度脊柱下不同轨迹生成策略，并分析了这些方法对步态自然性感知的影响，相较于固定脊柱基准而言。通过行走、慢跑和转弯的模拟视频评估了这些策略。在随机试验证实中，优化的时间变化策略和足部跟踪策略被50名参与者认为比基准更具自然性。虽然这些策略并未在无脊柱基准情况下显示出任何能量效率的提升，但其中一些在更高速度下显示了更好的足部着地一致性。鉴于更自然运动模式带来的更高受欢迎度，这种类型的机器人在老年人护理等社交机器人应用场景中具有潜力，而不以能量效率为主要考量因素。', 'title_zh': '四足脊椎控制策略：探索系统动力学响应与人类视角之间的关联'}
{'arxiv_id': 'arXiv:2505.02405', 'title': 'Estimating Commonsense Scene Composition on Belief Scene Graphs', 'authors': 'Mario A.V. Saucedo, Vignesh Kottayam Viswanathan, Christoforos Kanellakis, George Nikolakopoulos', 'link': 'https://arxiv.org/abs/2505.02405', 'abstract': 'This work establishes the concept of commonsense scene composition, with a focus on extending Belief Scene Graphs by estimating the spatial distribution of unseen objects. Specifically, the commonsense scene composition capability refers to the understanding of the spatial relationships among related objects in the scene, which in this article is modeled as a joint probability distribution for all possible locations of the semantic object class. The proposed framework includes two variants of a Correlation Information (CECI) model for learning probability distributions: (i) a baseline approach based on a Graph Convolutional Network, and (ii) a neuro-symbolic extension that integrates a spatial ontology based on Large Language Models (LLMs). Furthermore, this article provides a detailed description of the dataset generation process for such tasks. Finally, the framework has been validated through multiple runs on simulated data, as well as in a real-world indoor environment, demonstrating its ability to spatially interpret scenes across different room types.', 'abstract_zh': '此研究建立了常识场景组成的概念，并focus于通过估计未见物体的空间分布来扩展信念场景图。具体而言，常识场景组成能力指的是对场景中相关物体的空间关系的理解，本文将其建模为所有可能的语义对象类位置的联合概率分布。提出的框架包括用于学习概率分布的Correlation Information (CECI)模型的两种变体：（i）基于图卷积网络的基线方法，以及（ii）结合大语言模型（LLMs）的空间本体的神经符号扩展。此外，本文详细描述了此类任务的数据集生成过程。最后，该框架通过在模拟数据以及真实室内环境中的多次运行得到验证，展示了其在不同房间类型中对场景进行空间解释的能力。', 'title_zh': '在信念场景图上估计常识场景组成'}
{'arxiv_id': 'arXiv:2505.02395', 'title': 'A Real-Time Control Barrier Function-Based Safety Filter for Motion Planning with Arbitrary Road Boundary Constraints', 'authors': 'Jianye Xu, Chang Che, Bassam Alrifaee', 'link': 'https://arxiv.org/abs/2505.02395', 'abstract': 'We present a real-time safety filter for motion planning, such as learning-based methods, using Control Barrier Functions (CBFs), which provides formal guarantees for collision avoidance with road boundaries. A key feature of our approach is its ability to directly incorporate road geometries of arbitrary shape without resorting to conservative overapproximations. We formulate the safety filter as a constrained optimization problem in the form of a Quadratic Program (QP). It achieves safety by making minimal, necessary adjustments to the control actions issued by the nominal motion planner. We validate our safety filter through extensive numerical experiments across a variety of traffic scenarios featuring complex roads. The results confirm its reliable safety and high computational efficiency (execution frequency up to 40 Hz). Code & Video Demo: this http URL', 'abstract_zh': '基于Control Barrier Functions的实时安全过滤器：一种无需保守近似的动态规划方法及其在复杂道路场景下的验证', 'title_zh': '基于实时控制障碍函数的安全滤波器在任意道路边界约束下的运动规划'}
{'arxiv_id': 'arXiv:2505.02323', 'title': 'Riemannian Direct Trajectory Optimization of Rigid Bodies on Matrix Lie Groups', 'authors': 'Sangli Teng, Tzu-Yuan Lin, William A Clark, Ram Vasudevan, Maani Ghaffari', 'link': 'https://arxiv.org/abs/2505.02323', 'abstract': 'Designing dynamically feasible trajectories for rigid bodies is a fundamental problem in robotics. Although direct trajectory optimization is widely applied to solve this problem, inappropriate parameterizations of rigid body dynamics often result in slow convergence and violations of the intrinsic topological structure of the rotation group. This paper introduces a Riemannian optimization framework for direct trajectory optimization of rigid bodies. We first use the Lie Group Variational Integrator to formulate the discrete rigid body dynamics on matrix Lie groups. We then derive the closed-form first- and second-order Riemannian derivatives of the dynamics. Finally, this work applies a line-search Riemannian Interior Point Method (RIPM) to perform trajectory optimization with general nonlinear constraints. As the optimization is performed on matrix Lie groups, it is correct-by-construction to respect the topological structure of the rotation group and be free of singularities. The paper demonstrates that both the derivative evaluations and Newton steps required to solve the RIPM exhibit linear complexity with respect to the planning horizon and system degrees of freedom. Simulation results illustrate that the proposed method is faster than conventional methods by an order of magnitude in challenging robotics tasks.', 'abstract_zh': '基于黎曼优化的刚体直接轨迹优化设计', 'title_zh': '刚体在矩阵李群上的黎曼直接轨迹优化'}
{'arxiv_id': 'arXiv:2505.02294', 'title': 'RNBF: Real-Time RGB-D Based Neural Barrier Functions for Safe Robotic Navigation', 'authors': 'Satyajeet Das, Yifan Xue, Haoming Li, Nadia Figueroa', 'link': 'https://arxiv.org/abs/2505.02294', 'abstract': 'Autonomous safe navigation in unstructured and novel environments poses significant challenges, especially when environment information can only be provided through low-cost vision sensors. Although safe reactive approaches have been proposed to ensure robot safety in complex environments, many base their theory off the assumption that the robot has prior knowledge on obstacle locations and geometries. In this paper, we present a real-time, vision-based framework that constructs continuous, first-order differentiable Signed Distance Fields (SDFs) of unknown environments without any pre-training. Our proposed method ensures full compatibility with established SDF-based reactive controllers. To achieve robust performance under practical sensing conditions, our approach explicitly accounts for noise in affordable RGB-D cameras, refining the neural SDF representation online for smoother geometry and stable gradient estimates. We validate the proposed method in simulation and real-world experiments using a Fetch robot.', 'abstract_zh': '自主导航于未结构化和新颖环境中的安全性面临显著挑战，尤其是在只能通过低成本视觉传感器提供环境信息的情况下。尽管安全反应式方法已被提出以确保机器人在复杂环境中的安全性，但许多方法的前提假设是机器人对障碍物的位置和几何结构有先验知识。在本文中，我们提出了一种实时的视觉基于框架，无需任何预训练即可构建未知环境的连续可微签量距离场（SDF）。所提出的方法确保了与现有的基于SDF的反应式控制器的完全兼容性。为了在实际传感条件下实现稳健的性能，我们的方法明确考虑了经济实惠的RGB-D摄像头中的噪声，通过在线细化神经网络表示的签量距离场，以获得更平滑的几何结构和稳定的梯度估计。我们在使用Fetch机器人进行的仿真和实际实验中验证了所提出的方法。', 'title_zh': 'RNBF: 基于实时RGB-D神经屏障函数的机器人安全导航'}
{'arxiv_id': 'arXiv:2505.02293', 'title': 'Resolving Conflicting Constraints in Multi-Agent Reinforcement Learning with Layered Safety', 'authors': 'Jason J. Choi, Jasmine Jerry Aloor, Jingqi Li, Maria G. Mendoza, Hamsa Balakrishnan, Claire J. Tomlin', 'link': 'https://arxiv.org/abs/2505.02293', 'abstract': 'Preventing collisions in multi-robot navigation is crucial for deployment. This requirement hinders the use of learning-based approaches, such as multi-agent reinforcement learning (MARL), on their own due to their lack of safety guarantees. Traditional control methods, such as reachability and control barrier functions, can provide rigorous safety guarantees when interactions are limited only to a small number of robots. However, conflicts between the constraints faced by different agents pose a challenge to safe multi-agent coordination.\nTo overcome this challenge, we propose a method that integrates multiple layers of safety by combining MARL with safety filters. First, MARL is used to learn strategies that minimize multiple agent interactions, where multiple indicates more than two. Particularly, we focus on interactions likely to result in conflicting constraints within the engagement distance. Next, for agents that enter the engagement distance, we prioritize pairs requiring the most urgent corrective actions. Finally, a dedicated safety filter provides tactical corrective actions to resolve these conflicts. Crucially, the design decisions for all layers of this framework are grounded in reachability analysis and a control barrier-value function-based filtering mechanism.\nWe validate our Layered Safe MARL framework in 1) hardware experiments using Crazyflie drones and 2) high-density advanced aerial mobility (AAM) operation scenarios, where agents navigate to designated waypoints while avoiding collisions. The results show that our method significantly reduces conflict while maintaining safety without sacrificing much efficiency (i.e., shorter travel time and distance) compared to baselines that do not incorporate layered safety. The project website is available at \\href{this https URL}{[this https URL]}', 'abstract_zh': '多层次安全多智能体强化学习框架在多机器人导航中的应用', 'title_zh': '多智能体强化学习中层次化安全约束的冲突解决'}
{'arxiv_id': 'arXiv:2505.02291', 'title': 'Dexterous Contact-Rich Manipulation via the Contact Trust Region', 'authors': 'H.J. Terry Suh, Tao Pang, Tong Zhao, Russ Tedrake', 'link': 'https://arxiv.org/abs/2505.02291', 'abstract': 'What is a good local description of contact dynamics for contact-rich manipulation, and where can we trust this local description? While many approaches often rely on the Taylor approximation of dynamics with an ellipsoidal trust region, we argue that such approaches are fundamentally inconsistent with the unilateral nature of contact. As a remedy, we present the Contact Trust Region (CTR), which captures the unilateral nature of contact while remaining efficient for computation. With CTR, we first develop a Model-Predictive Control (MPC) algorithm capable of synthesizing local contact-rich plans. Then, we extend this capability to plan globally by stitching together local MPC plans, enabling efficient and dexterous contact-rich manipulation. To verify the performance of our method, we perform comprehensive evaluations, both in high-fidelity simulation and on hardware, on two contact-rich systems: a planar IiwaBimanual system and a 3D AllegroHand system. On both systems, our method offers a significantly lower-compute alternative to existing RL-based approaches to contact-rich manipulation. In particular, our Allegro in-hand manipulation policy, in the form of a roadmap, takes fewer than 10 minutes to build offline on a standard laptop using just its CPU, with online inference taking just a few seconds. Experiment data, video and code are available at this http URL.', 'abstract_zh': '接触丰富的操作中良好的局部描述是什么，我们在哪里可以信任这种局部描述？尽管许多方法往往依靠具有椭球信任区域的动力学泰勒近似，我们argue认为这些方法本质上无法与单向接触的性质相一致。作为解决方案，我们提出了接触信任区域（CTR），它捕捉了接触的单向性质同时保持计算效率。使用CTR，我们首先开发了一个模型预测控制（MPC）算法，能够合成局部接触丰富的计划。然后，通过拼接局部MPC计划，将这一能力扩展到全局规划，从而实现高效的灵巧接触丰富操作。为了验证我们方法的性能，我们在高保真仿真和硬件上对两个接触丰富的系统——平面双臂iiwa系统和3D AllegroHand系统——进行了全面评估。在两个系统上，我们的方法提供了比现有基于强化学习的接触丰富操作方法更低计算量的替代方案。特别是，我们的Allegro手内操作策略以路网的形式，在标准笔记本电脑上仅使用CPU构建所需的时间少于10分钟，而在线推理只需几秒钟。更多实验数据、视频和代码可访问此链接。', 'title_zh': 'Dexterous接触丰富的 manipulation 通过接触信任区域'}
{'arxiv_id': 'arXiv:2505.02272', 'title': 'Robust Localization, Mapping, and Navigation for Quadruped Robots', 'authors': 'Dyuman Aditya, Junning Huang, Nico Bohlinger, Piotr Kicki, Krzysztof Walas, Jan Peters, Matteo Luperto, Davide Tateo', 'link': 'https://arxiv.org/abs/2505.02272', 'abstract': 'Quadruped robots are currently a widespread platform for robotics research, thanks to powerful Reinforcement Learning controllers and the availability of cheap and robust commercial platforms. However, to broaden the adoption of the technology in the real world, we require robust navigation stacks relying only on low-cost sensors such as depth cameras. This paper presents a first step towards a robust localization, mapping, and navigation system for low-cost quadruped robots. In pursuit of this objective we combine contact-aided kinematic, visual-inertial odometry, and depth-stabilized vision, enhancing stability and accuracy of the system. Our results in simulation and two different real-world quadruped platforms show that our system can generate an accurate 2D map of the environment, robustly localize itself, and navigate autonomously. Furthermore, we present in-depth ablation studies of the important components of the system and their impact on localization accuracy. Videos, code, and additional experiments can be found on the project website: this https URL', 'abstract_zh': '低成本四足机器人 robust 定位、建图及导航系统的第一步：结合接触辅助运动学、视觉-惯性里程计和深度稳定视觉增强稳定性与准确性', 'title_zh': '四足机器人 robust 定位、建图与导航'}
{'arxiv_id': 'arXiv:2505.02232', 'title': 'Prompt-responsive Object Retrieval with Memory-augmented Student-Teacher Learning', 'authors': 'Malte Mosbach, Sven Behnke', 'link': 'https://arxiv.org/abs/2505.02232', 'abstract': 'Building models responsive to input prompts represents a transformative shift in machine learning. This paradigm holds significant potential for robotics problems, such as targeted manipulation amidst clutter. In this work, we present a novel approach to combine promptable foundation models with reinforcement learning (RL), enabling robots to perform dexterous manipulation tasks in a prompt-responsive manner. Existing methods struggle to link high-level commands with fine-grained dexterous control. We address this gap with a memory-augmented student-teacher learning framework. We use the Segment-Anything 2 (SAM 2) model as a perception backbone to infer an object of interest from user prompts. While detections are imperfect, their temporal sequence provides rich information for implicit state estimation by memory-augmented models. Our approach successfully learns prompt-responsive policies, demonstrated in picking objects from cluttered scenes. Videos and code are available at this https URL', 'abstract_zh': '构建响应输入提示的模型代表了机器学习的一个变革性转变。这种范式在机器人学问题，如在杂乱环境中进行目标操作方面具有重要意义。在本文中，我们提出了一种将可提示基础模型与强化学习相结合的新方法，使机器人能够以响应提示的方式执行灵巧操作任务。现有方法难以将高层命令与精细的灵巧控制链接起来。我们通过一种记忆增强的学生-教师学习框架来解决这一问题。我们使用Segment-Anything 2 (SAM 2) 模型作为感知主干，从用户提示中推断出感兴趣的对象。尽管检测结果不完美，但其时间序列提供了丰富的信息，供记忆增强模型进行隐式状态估计。我们的方法成功学习了响应提示的策略，并在杂乱场景中拾取物体的任务中得到了验证。更多视频和代码详见此链接。', 'title_zh': '带有记忆增强学生-教师学习的提示响应对象检索'}
{'arxiv_id': 'arXiv:2505.02166', 'title': 'CrayonRobo: Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation', 'authors': 'Xiaoqi Li, Lingyun Xu, Mingxu Zhang, Jiaming Liu, Yan Shen, Iaroslav Ponomarenko, Jiahui Xu, Liang Heng, Siyuan Huang, Shanghang Zhang, Hao Dong', 'link': 'https://arxiv.org/abs/2505.02166', 'abstract': 'In robotic, task goals can be conveyed through various modalities, such as language, goal images, and goal videos. However, natural language can be ambiguous, while images or videos may offer overly detailed specifications. To tackle these challenges, we introduce CrayonRobo that leverages comprehensive multi-modal prompts that explicitly convey both low-level actions and high-level planning in a simple manner. Specifically, for each key-frame in the task sequence, our method allows for manual or automatic generation of simple and expressive 2D visual prompts overlaid on RGB images. These prompts represent the required task goals, such as the end-effector pose and the desired movement direction after contact. We develop a training strategy that enables the model to interpret these visual-language prompts and predict the corresponding contact poses and movement directions in SE(3) space. Furthermore, by sequentially executing all key-frame steps, the model can complete long-horizon tasks. This approach not only helps the model explicitly understand the task objectives but also enhances its robustness on unseen tasks by providing easily interpretable prompts. We evaluate our method in both simulated and real-world environments, demonstrating its robust manipulation capabilities.', 'abstract_zh': '机器人领域，任务目标可以通过语言、目标图像和目标视频等多种模态传达。然而，自然语言可能存在歧义，而图像或视频可能提供过于详细的具体信息。为应对这些挑战，我们引入了CrayonRobo，该系统利用全面的多模态提示，以简单明确的方式传达低级动作和高级规划。具体而言，对于任务序列中的每个关键帧，我们的方法允许手动或自动生成简洁且富有表现力的2D视觉提示，并叠加在RGB图像上。这些提示表示所需的任务目标，例如末端执行器的姿态和接触后的期望运动方向。我们开发了一种训练策略，使模型能够解释这些视觉语言提示，并在SE(3)空间中预测相应的接触姿态和运动方向。此外，通过顺序执行所有关键帧步骤，模型可以完成长时序任务。该方法不仅有助于模型明确理解任务目标，还能通过提供易于解释的提示增强其在未见过的任务上的鲁棒性。我们分别在模拟和实际环境中评估了该方法，展示了其稳健的操作能力。', 'title_zh': 'CrayonRobo: 以物体为中心的提示驱动视觉语言行动模型用于机器人 manipulation'}
{'arxiv_id': 'arXiv:2505.02152', 'title': 'Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions', 'authors': 'Cunxin Fan, Xiaosong Jia, Yihang Sun, Yixiao Wang, Jianglan Wei, Ziyang Gong, Xiangyu Zhao, Masayoshi Tomizuka, Xue Yang, Junchi Yan, Mingyu Ding', 'link': 'https://arxiv.org/abs/2505.02152', 'abstract': "Vision-Language-Action (VLA) models have shown great promise for generalist robotic manipulation in the physical world. However, existing models are restricted to robot observations and text-only instructions, lacking the flexibility of interleaved multimodal instructions enabled by recent advances in foundation models in the digital world. In this paper, we present Interleave-VLA, the first framework capable of comprehending interleaved image-text instructions and directly generating continuous action sequences in the physical world. It offers a flexible, model-agnostic paradigm that extends state-of-the-art VLA models with minimal modifications and strong zero-shot generalization. A key challenge in realizing Interleave-VLA is the absence of large-scale interleaved embodied datasets. To bridge this gap, we develop an automatic pipeline that converts text-only instructions from real-world datasets in Open X-Embodiment into interleaved image-text instructions, resulting in the first large-scale real-world interleaved embodied dataset with 210k episodes. Through comprehensive evaluation on simulation benchmarks and real-robot experiments, we demonstrate that Interleave-VLA offers significant benefits: 1) it improves out-of-domain generalization to unseen objects by 2-3x compared to state-of-the-art baselines, 2) supports flexible task interfaces, and 3) handles diverse user-provided image instructions in a zero-shot manner, such as hand-drawn sketches. We further analyze the factors behind Interleave-VLA's strong zero-shot performance, showing that the interleaved paradigm effectively leverages heterogeneous datasets and diverse instruction images, including those from the Internet, which demonstrates strong potential for scaling up. Our model and dataset will be open-sourced.", 'abstract_zh': '基于视觉-语言-行动的交互式框架：理解和生成物理世界的交错指令与连续动作序列', 'title_zh': '交错-VLA：增强机器人操作的交错图像-文本指令'}
{'arxiv_id': 'arXiv:2505.02123', 'title': 'DriveAgent: Multi-Agent Structured Reasoning with LLM and Multimodal Sensor Fusion for Autonomous Driving', 'authors': 'Xinmeng Hou, Wuqi Wang, Long Yang, Hao Lin, Jinglun Feng, Haigen Min, Xiangmo Zhao', 'link': 'https://arxiv.org/abs/2505.02123', 'abstract': 'We introduce DriveAgent, a novel multi-agent autonomous driving framework that leverages large language model (LLM) reasoning combined with multimodal sensor fusion to enhance situational understanding and decision-making. DriveAgent uniquely integrates diverse sensor modalities-including camera, LiDAR, GPS, and IMU-with LLM-driven analytical processes structured across specialized agents. The framework operates through a modular agent-based pipeline comprising four principal modules: (i) a descriptive analysis agent identifying critical sensor data events based on filtered timestamps, (ii) dedicated vehicle-level analysis conducted by LiDAR and vision agents that collaboratively assess vehicle conditions and movements, (iii) environmental reasoning and causal analysis agents explaining contextual changes and their underlying mechanisms, and (iv) an urgency-aware decision-generation agent prioritizing insights and proposing timely maneuvers. This modular design empowers the LLM to effectively coordinate specialized perception and reasoning agents, delivering cohesive, interpretable insights into complex autonomous driving scenarios. Extensive experiments on challenging autonomous driving datasets demonstrate that DriveAgent is achieving superior performance on multiple metrics against baseline methods. These results validate the efficacy of the proposed LLM-driven multi-agent sensor fusion framework, underscoring its potential to substantially enhance the robustness and reliability of autonomous driving systems.', 'abstract_zh': 'DriveAgent：一种结合大型语言模型推理和多模态传感器融合的新型多代理自主驾驶框架', 'title_zh': 'DriveAgent：基于LLM和多模态传感器融合的多Agent结构化推理自主驾驶'}
{'arxiv_id': 'arXiv:2505.02081', 'title': 'Simulation Based Control Architecture Using Webots and Simulink', 'authors': 'Harun Kurt, Ahmet Cayir, Kadir Erkan', 'link': 'https://arxiv.org/abs/2505.02081', 'abstract': 'This paper presents a simulation based control architecture that integrates Webots and Simulink for the development and testing of robotic systems. Using Webots for 3D physics based simulation and Simulink for control system design, real time testing and controller validation are achieved efficiently. The proposed approach aims to reduce hardware in the loop dependency in early development stages, offering a cost effective and modular control framework for academic, industrial, and robotics applications.', 'abstract_zh': '基于Webots和Simulink的仿真驱动控制架构及其在机器人系统开发与测试中的应用', 'title_zh': '基于Webots和Simulink的仿真驱动控制架构'}
{'arxiv_id': 'arXiv:2505.02049', 'title': 'Enhancing Lidar Point Cloud Sampling via Colorization and Super-Resolution of Lidar Imagery', 'authors': 'Sier Ha, Honghao Du, Xianjia Yu, Tomi Westerlund', 'link': 'https://arxiv.org/abs/2505.02049', 'abstract': 'Recent advancements in lidar technology have led to improved point cloud resolution as well as the generation of 360 degrees, low-resolution images by encoding depth, reflectivity, or near-infrared light within each pixel. These images enable the application of deep learning (DL) approaches, originally developed for RGB images from cameras to lidar-only systems, eliminating other efforts, such as lidar-camera calibration. Compared with conventional RGB images, lidar imagery demonstrates greater robustness in adverse environmental conditions, such as low light and foggy weather. Moreover, the imaging capability addresses the challenges in environments where the geometric information in point clouds may be degraded, such as long corridors, and dense point clouds may be misleading, potentially leading to drift errors.\nTherefore, this paper proposes a novel framework that leverages DL-based colorization and super-resolution techniques on lidar imagery to extract reliable samples from lidar point clouds for odometry estimation. The enhanced lidar images, enriched with additional information, facilitate improved keypoint detection, which is subsequently employed for more effective point cloud downsampling. The proposed method enhances point cloud registration accuracy and mitigates mismatches arising from insufficient geometric information or misleading extra points. Experimental results indicate that our approach surpasses previous methods, achieving lower translation and rotation errors while using fewer points.', 'abstract_zh': '最近lidar技术的进步提高了点云分辨率，并可通过在每个像素中编码深度、反射率或近红外光生成360度低分辨率图像。这些图像使得能够将原本针对相机RGB图像开发的深度学习方法应用于纯lidar系统，从而避免了其他努力，如lidar-相机校准。与传统的RGB图像相比，lidar图像在低光和雾天等恶劣环境条件下表现出更大的鲁棒性。此外，成像能力解决了点云几何信息在长走廊等环境可能降级以及密集点云可能导致错误引导的问题，从而可能引起漂移误差。\n因此，本文提出了一种新的框架，利用基于DL的颜色化和超分辨率技术处理lidar图像，从lidar点云中提取可靠的样本用于里程计估计。增强的lidar图像富含额外信息，促进了更有效的关键点检测，进而用于点云下采样。所提出的方法增强了点云配准精度，并减轻了由于几何信息不足或误导性额外点而导致的匹配错误。实验结果表明，我们的方法超过先前的方法，在使用较少点的情况下实现了更低的平移和旋转误差。', 'title_zh': '通过激光雷达图像着色和超分辨率增强激光雷达点云采样'}
{'arxiv_id': 'arXiv:2505.01998', 'title': 'A Synergistic Framework of Nonlinear Acoustic Computing and Reinforcement Learning for Real-World Human-Robot Interaction', 'authors': 'Xiaoliang Chen, Xin Yu, Le Chang, Yunhe Huang, Jiashuai He, Shibo Zhang, Jin Li, Likai Lin, Ziyu Zeng, Xianling Tu, Shuyu Zhang', 'link': 'https://arxiv.org/abs/2505.01998', 'abstract': 'This paper introduces a novel framework integrating nonlinear acoustic computing and reinforcement learning to enhance advanced human-robot interaction under complex noise and reverberation. Leveraging physically informed wave equations (e.g., Westervelt, KZK), the approach captures higher-order phenomena such as harmonic generation and shock formation. By embedding these models in a reinforcement learning-driven control loop, the system adaptively optimizes key parameters (e.g., absorption, beamforming) to mitigate multipath interference and non-stationary noise. Experimental evaluations-covering far-field localization, weak signal detection, and multilingual speech recognition-demonstrate that this hybrid strategy surpasses traditional linear methods and purely data-driven baselines, achieving superior noise suppression, minimal latency, and robust accuracy in demanding real-world scenarios. The proposed system demonstrates broad application prospects in AI hardware, robot, machine audition, artificial audition, and brain-machine interfaces.', 'abstract_zh': '本文引入了一种将非线性声学计算与强化学习结合起来的新框架，以增强在复杂噪声和混响条件下的高级人机交互。通过利用物理信息波方程（如韦斯特维尔特方程、KZK方程），该方法捕捉更高阶的现象，如谐波生成和冲击形成。通过将这些模型嵌入到基于强化学习的控制循环中，系统自适应地优化关键参数（如吸收、波束形成）以减轻多路径干扰和非平稳噪声。实验评估涵盖远场定位、弱信号检测和多语种语音识别，表明这种混合策略超越了传统的线性方法和纯数据驱动的基础模型，在苛刻的现实世界场景中实现卓越的噪声抑制、最小的延迟和稳健的准确性。所提出的系统在人工智能硬件、机器人、机器听觉、人工听觉和脑机接口等领域具有广泛的应用前景。', 'title_zh': '非线性声学计算与强化学习协同框架在真实世界人机交互中的应用'}
{'arxiv_id': 'arXiv:2505.01974', 'title': 'KineDex: Learning Tactile-Informed Visuomotor Policies via Kinesthetic Teaching for Dexterous Manipulation', 'authors': 'Di Zhang, Chengbo Yuan, Chuan Wen, Hai Zhang, Junqiao Zhao, Yang Gao', 'link': 'https://arxiv.org/abs/2505.01974', 'abstract': "Collecting demonstrations enriched with fine-grained tactile information is critical for dexterous manipulation, particularly in contact-rich tasks that require precise force control and physical interaction. While prior works primarily focus on teleoperation or video-based retargeting, they often suffer from kinematic mismatches and the absence of real-time tactile feedback, hindering the acquisition of high-fidelity tactile data. To mitigate this issue, we propose KineDex, a hand-over-hand kinesthetic teaching paradigm in which the operator's motion is directly transferred to the dexterous hand, enabling the collection of physically grounded demonstrations enriched with accurate tactile feedback. To resolve occlusions from human hand, we apply inpainting technique to preprocess the visual observations. Based on these demonstrations, we then train a visuomotor policy using tactile-augmented inputs and implement force control during deployment for precise contact-rich manipulation. We evaluate KineDex on a suite of challenging contact-rich manipulation tasks, including particularly difficult scenarios such as squeezing toothpaste onto a toothbrush, which require precise multi-finger coordination and stable force regulation. Across these tasks, KineDex achieves an average success rate of 74.4%, representing a 57.7% improvement over the variant without force control. Comparative experiments with teleoperation and user studies further validate the advantages of KineDex in data collection efficiency and operability. Specifically, KineDex collects data over twice as fast as teleoperation across two tasks of varying difficulty, while maintaining a near-100% success rate, compared to under 50% for teleoperation.", 'abstract_zh': '富含细粒度触觉信息的演示收集对于灵巧操作至关重要，特别是在需要精确力控制和物理交互的接触丰富任务中。尽管前期工作主要关注于遥操作或基于视频的重目标，但它们经常遭受运动学不匹配和实时触觉反馈缺失的问题，阻碍了高质量触觉数据的采集。为解决这一问题，我们提出了KineDex，一种手把手的触觉教学范式，操作者的运动直接转移到灵巧手中，从而能够收集富含准确触觉反馈的物理接地演示。为解决人类手部遮挡问题，我们应用 inpainting 技术预处理视觉观察。基于这些演示，我们使用触觉增强的输入训练了一种视触觉策略，并在部署过程中实施了力控制以实现精确的接触丰富操作。我们在一系列接触丰富操作任务上评估了KineDex，包括需要精确多指协调和稳定力调节的特别困难场景，如将牙膏挤到牙刷上。在这些任务中，KineDex 的平均成功率达到了 74.4%，与不使用力控制的变体相比，提高了 57.7%。与遥操作的比较实验和用户研究表明，KineDex 在数据采集效率和可操作性方面具有优势。特别是，KineDex 在两个不同难度级别的任务中数据采集速度比遥操作快两倍，且成功率达到近 100%，而遥操作的成功率不到 50%。', 'title_zh': 'KineDex: 通过动能教学学习触觉指导的视知觉运动策略以实现灵巧 manipulation'}
{'arxiv_id': 'arXiv:2505.01966', 'title': 'A Goal-Oriented Reinforcement Learning-Based Path Planning Algorithm for Modular Self-Reconfigurable Satellites', 'authors': 'Bofei Liu, Dong Ye, Zunhao Yao, Zhaowei Sun', 'link': 'https://arxiv.org/abs/2505.01966', 'abstract': 'Modular self-reconfigurable satellites refer to satellite clusters composed of individual modular units capable of altering their configurations. The configuration changes enable the execution of diverse tasks and mission objectives. Existing path planning algorithms for reconfiguration often suffer from high computational complexity, poor generalization capability, and limited support for diverse target configurations. To address these challenges, this paper proposes a goal-oriented reinforcement learning-based path planning algorithm. This algorithm is the first to address the challenge that previous reinforcement learning methods failed to overcome, namely handling multiple target configurations. Moreover, techniques such as Hindsight Experience Replay and Invalid Action Masking are incorporated to overcome the significant obstacles posed by sparse rewards and invalid actions. Based on these designs, our model achieves a 95% and 73% success rate in reaching arbitrary target configurations in a modular satellite cluster composed of four and six units, respectively.', 'abstract_zh': '模块化自重构卫星的路径规划算法：目标导向的强化学习方法及其应用', 'title_zh': '面向目标的基于强化学习的模块化自重构卫星路径规划算法'}
{'arxiv_id': 'arXiv:2505.01956', 'title': 'SafeNav: Safe Path Navigation using Landmark Based Localization in a GPS-denied Environment', 'authors': 'Ganesh Sapkota, Sanjay Madria', 'link': 'https://arxiv.org/abs/2505.01956', 'abstract': 'In battlefield environments, adversaries frequently disrupt GPS signals, requiring alternative localization and navigation methods. Traditional vision-based approaches like Simultaneous Localization and Mapping (SLAM) and Visual Odometry (VO) involve complex sensor fusion and high computational demand, whereas range-free methods like DV-HOP face accuracy and stability challenges in sparse, dynamic networks. This paper proposes LanBLoc-BMM, a navigation approach using landmark-based localization (LanBLoc) combined with a battlefield-specific motion model (BMM) and Extended Kalman Filter (EKF). Its performance is benchmarked against three state-of-the-art visual localization algorithms integrated with BMM and Bayesian filters, evaluated on synthetic and real-imitated trajectory datasets using metrics including Average Displacement Error (ADE), Final Displacement Error (FDE), and a newly introduced Average Weighted Risk Score (AWRS). LanBLoc-BMM (with EKF) demonstrates superior performance in ADE, FDE, and AWRS on real-imitated datasets. Additionally, two safe navigation methods, SafeNav-CHull and SafeNav-Centroid, are introduced by integrating LanBLoc-BMM(EKF) with a novel Risk-Aware RRT* (RAw-RRT*) algorithm for obstacle avoidance and risk exposure minimization. Simulation results in battlefield scenarios indicate SafeNav-Centroid excels in accuracy, risk exposure, and trajectory efficiency, while SafeNav-CHull provides superior computational speed.', 'abstract_zh': '基于地标定位和战场特定运动模型的扩展卡尔曼滤波导航方法', 'title_zh': 'SafeNav: 在GPS受限环境下的基于 landmark 的路径导航及其安全定位'}
{'arxiv_id': 'arXiv:2505.01931', 'title': 'Semantic Intelligence: Integrating GPT-4 with A Planning in Low-Cost Robotics', 'authors': 'Jesse Barkley, Abraham George, Amir Barati Farimani', 'link': 'https://arxiv.org/abs/2505.01931', 'abstract': "Classical robot navigation often relies on hardcoded state machines and purely geometric path planners, limiting a robot's ability to interpret high-level semantic instructions. In this paper, we first assess GPT-4's ability to act as a path planner compared to the A* algorithm, then present a hybrid planning framework that integrates GPT-4's semantic reasoning with A* on a low-cost robot platform operating on ROS2 Humble. Our approach eliminates explicit finite state machine (FSM) coding by using prompt-based GPT-4 reasoning to handle task logic while maintaining the accurate paths computed by A*. The GPT-4 module provides semantic understanding of instructions and environmental cues (e.g., recognizing toxic obstacles or crowded areas to avoid, or understanding low-battery situations requiring alternate route selection), and dynamically adjusts the robot's occupancy grid via obstacle buffering to enforce semantic constraints. We demonstrate multi-step reasoning for sequential tasks, such as first navigating to a resource goal and then reaching a final destination safely. Experiments on a Petoi Bittle robot with an overhead camera and Raspberry Pi Zero 2W compare classical A* against GPT-4-assisted planning. Results show that while A* is faster and more accurate for basic route generation and obstacle avoidance, the GPT-4-integrated system achieves high success rates (96-100%) on semantic tasks that are infeasible for pure geometric planners. This work highlights how affordable robots can exhibit intelligent, context-aware behaviors by leveraging large language model reasoning with minimal hardware and no fine-tuning.", 'abstract_zh': '经典机器人导航往往依赖硬编码的状态机和纯粹的几何路径规划器，限制了机器人对高级语义指令的解释能力。本文首先评估GPT-4与A*算法相比作为路径规划器的能力，然后提出了一种将GPT-4的语义推理与A*算法结合的混合规划框架，该框架在运行ROS2 Humble的低成本机器人平台上实现。我们的方法通过使用基于提示的GPT-4推理来处理任务逻辑，同时保留A*计算的精确路径，从而消除了显式有限状态机（FSM）的编码。GPT-4模块提供了对指令和环境线索的语义理解（例如，识别有毒障碍物或拥挤区域以避免，或理解低电量情况需要选择替代路线），并通过障碍物缓冲动态调整机器人的占用网格，以遵守语义约束。我们展示了多步推理，如首先导航到资源目标，然后安全地到达最终目的地。使用带有上方摄像头的Petoi Bittle机器人和Raspberry Pi Zero 2W进行实验，比较了经典的A*算法与GPT-4辅助规划。结果表明，虽然A*在基本路线生成和障碍物避免方面更快更准确，但结合GPT-4的系统在纯几何规划器无法实现的语义任务中实现了高成功率（96-100%）。这项工作突显了如何通过利用大型语言模型推理实现低成本机器人展现出智能、情境感知的行为，无需精细调优和额外的硬件。', 'title_zh': '语义智能：将GPT-4与规划集成应用于低成本机器人'}
{'arxiv_id': 'arXiv:2505.01893', 'title': 'DriveNetBench: An Affordable and Configurable Single-Camera Benchmarking System for Autonomous Driving Networks', 'authors': 'Ali Al-Bustami, Humberto Ruiz-Ochoa, Jaerock Kwon', 'link': 'https://arxiv.org/abs/2505.01893', 'abstract': 'Validating autonomous driving neural networks often demands expensive equipment and complex setups, limiting accessibility for researchers and educators. We introduce DriveNetBench, an affordable and configurable benchmarking system designed to evaluate autonomous driving networks using a single-camera setup. Leveraging low-cost, off-the-shelf hardware, and a flexible software stack, DriveNetBench enables easy integration of various driving models, such as object detection and lane following, while ensuring standardized evaluation in real-world scenarios. Our system replicates common driving conditions and provides consistent, repeatable metrics for comparing network performance. Through preliminary experiments with representative vision models, we illustrate how DriveNetBench effectively measures inference speed and accuracy within a controlled test environment. The key contributions of this work include its affordability, its replicability through open-source software, and its seamless integration into existing workflows, making autonomous vehicle research more accessible.', 'abstract_zh': 'DriveNetBench：一种基于单摄像头配置的经济高效且可配置的自动驾驶神经网络评估系统', 'title_zh': 'DriveNetBench: 一种可配置的单摄像头自动驾驶网络基准测试系统'}
{'arxiv_id': 'arXiv:2505.01862', 'title': 'ReLI: A Language-Agnostic Approach to Human-Robot Interaction', 'authors': 'Linus Nwankwo, Bjoern Ellensohn, Ozan Özdenizci, Elmar Rueckert', 'link': 'https://arxiv.org/abs/2505.01862', 'abstract': "Adapting autonomous agents to industrial, domestic, and other daily tasks is currently gaining momentum. However, in the global or cross-lingual application contexts, ensuring effective interaction with the environment and executing unrestricted human task-specified instructions in diverse languages remains an unsolved problem. To address this challenge, we propose ReLI, a language-agnostic framework designed to enable autonomous agents to converse naturally, semantically reason about the environment, and to perform downstream tasks, regardless of the task instruction's linguistic origin. First, we ground large-scale pre-trained foundation models and transform them into language-to-action models that can directly provide common-sense reasoning and high-level robot control through natural, free-flow human-robot conversational interactions. Further, we perform cross-lingual grounding of the models to ensure that ReLI generalises across the global languages. To demonstrate the ReLI's robustness, we conducted extensive simulated and real-world experiments on various short- and long-horizon tasks, including zero-shot and few-shot spatial navigation, scene information retrieval, and query-oriented tasks. We benchmarked the performance on 140 languages involving over 70K multi-turn conversations. On average, ReLI achieved over 90%$\\pm$0.2 accuracy in cross-lingual instruction parsing and task execution success rates. These results demonstrate the ReLI's potential to enhance natural human-robot interaction in the real world while championing linguistic diversity. Demonstrations and resources will be publicly available at this https URL.", 'abstract_zh': '面向工业、家用及其它日常任务的自主代理适应性正在逐步提升。然而，在全球或跨语言应用场景中，确保自主代理与环境有效互动并执行多语言的不限制人类任务指令仍是一个未解决的问题。为应对这一挑战，我们提出ReLI，这是一种语言无关的框架，旨在使自主代理能够进行自然对话、语义推理以及执行下游任务，而不受任务指令语言来源的影响。首先，我们基于大规模预训练基础模型并将其转化为语言到行动的模型，这些模型可以直接通过自然、流畅的人机对话互动进行常识推理和高级机器人控制。进一步地，我们对模型进行跨语言grounding，确保ReLI在多种全球语言间通用。为了证明ReLI的鲁棒性，我们在各种短期和长期任务上进行了广泛的模拟和现实世界实验，包括零样本和少样本的空间导航、场景信息检索以及查询导向任务。我们在超过70,000轮多轮对话中对140种语言进行了基准测试。结果显示，ReLI在跨语言指令解析和任务执行成功率方面的平均准确率超过90%±0.2。这些结果展示了ReLI在增强现实世界中的人机自然交互方面的潜力，同时也支持语言多样性。具体内容和资源将于以下链接公开：this https URL。', 'title_zh': 'ReLI: 一种语言无关的人机交互方法'}
{'arxiv_id': 'arXiv:2505.01752', 'title': 'NMPCB: A Lightweight and Safety-Critical Motion Control Framework', 'authors': 'Longze Zheng, Qinghe Liu', 'link': 'https://arxiv.org/abs/2505.01752', 'abstract': 'In multi-obstacle environments, real-time performance and safety in robot motion control have long been challenging issues, as conventional methods often struggle to balance the two. In this paper, we propose a novel motion control framework composed of a Neural network-based path planner and a Model Predictive Control (MPC) controller based on control Barrier function (NMPCB) . The planner predicts the next target point through a lightweight neural network and generates a reference trajectory for the controller. In the design of the controller, we introduce the dual problem of control barrier function (CBF) as the obstacle avoidance constraint, enabling it to ensure robot motion safety while significantly reducing computation time. The controller directly outputs control commands to the robot by tracking the reference trajectory. This framework achieves a balance between real-time performance and safety. We validate the feasibility of the framework through numerical simulations and real-world experiments.', 'abstract_zh': '在多障碍物环境中，机器人的实时性能和运动控制安全性长期是具有挑战性的问题，因为传统方法往往难以在这两者之间找到平衡。本文提出了一种新颖的运动控制框架，该框架由基于神经网络的路径规划器和基于控制屏障函数（CBF）的模型预测控制（NMPCB）控制器组成。规划器通过轻量级神经网络预测下一个目标点，并生成供控制器使用的参考轨迹。在控制器的设计中，我们引入控制屏障函数的对偶问题作为避障约束，使其能够在显著减少计算时间的同时保证机器人的运动安全。控制器直接通过跟踪参考轨迹输出控制命令。该框架实现了实时性能和安全性之间的平衡。我们通过数值仿真和实际实验验证了该框架的可行性。', 'title_zh': 'NMPCB: 一种轻量级和安全关键的运动控制框架'}
{'arxiv_id': 'arXiv:2505.01718', 'title': 'Mitigating Compensatory Movements in Prosthesis Users via Adaptive Collaborative Robotics', 'authors': 'Marta Lagomarsino, Robin Arbaud, Francesco Tassi, Arash Ajoudani', 'link': 'https://arxiv.org/abs/2505.01718', 'abstract': "Prosthesis users can regain partial limb functionality, however, full natural limb mobility is rarely restored, often resulting in compensatory movements that lead to discomfort, inefficiency, and long-term physical strain. To address this issue, we propose a novel human-robot collaboration framework to mitigate compensatory mechanisms in upper-limb prosthesis users by exploiting their residual motion capabilities while respecting task requirements. Our approach introduces a personalised mobility model that quantifies joint-specific functional limitations and the cost of compensatory movements. This model is integrated into a constrained optimisation framework that computes optimal user postures for task performance, balancing functionality and comfort. The solution guides a collaborative robot to reconfigure the task environment, promoting effective interaction. We validated the framework using a new body-powered prosthetic device for single-finger amputation, which enhances grasping capabilities through synergistic closure with the hand but imposes wrist constraints. Initial experiments with healthy subjects wearing the prosthesis as a supernumerary finger demonstrated that a robotic assistant embedding the user-specific mobility model outperformed human partners in handover tasks, improving both the efficiency of the prosthesis user's grasp and reducing compensatory movements in functioning joints. These results highlight the potential of collaborative robots as effective workplace and caregiving assistants, promoting inclusion and better integration of prosthetic devices into daily tasks.", 'abstract_zh': '假肢用户可以恢复部分肢体功能，但通常难以恢复完全自然的肢体灵活性，往往会导致代偿动作，引起不适、效率低下和长期的身体负担。为解决这一问题，我们提出了一种新型的人机协作框架，通过利用用户残存的运动能力来减轻上肢假肢单用户中的代偿机制，同时尊重任务要求。该方法引入了一个个性化的运动模型，量化了关节特异性的功能限制和代偿动作的成本。该模型被整合到一个受约束的优化框架中，该框架计算任务执行时的最佳用户姿势，平衡功能和舒适性。该解决方案引导协作机器人重新配置任务环境，促进有效互动。我们使用一种新的体驱动假肢装置对单指缺失进行了验证，该装置通过与手的协同闭合来增强抓握能力，但对手腕施加了限制。初步实验表明，配备了用户特定运动模型的机器人助手在手递手任务中优于人类伙伴，既提高了假肢使用者的抓握效率，又减少了功能关节的代偿动作。这些结果突显了协作机器人作为有效的工作场所和护理助手的潜力，促进了假肢设备在日常任务中的包容性和更好整合。', 'title_zh': '通过自适应协作机器人减轻假肢用户补偿运动的影响'}
{'arxiv_id': 'arXiv:2505.01709', 'title': 'RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation', 'authors': 'Kaidong Zhang, Rongtao Xu, Pengzhen Ren, Junfan Lin, Hefeng Wu, Liang Lin, Xiaodan Liang', 'link': 'https://arxiv.org/abs/2505.01709', 'abstract': "Operating robots in open-ended scenarios with diverse tasks is a crucial research and application direction in robotics. While recent progress in natural language processing and large multimodal models has enhanced robots' ability to understand complex instructions, robot manipulation still faces the procedural skill dilemma and the declarative skill dilemma in open environments. Existing methods often compromise cognitive and executive capabilities. To address these challenges, in this paper, we propose RoBridge, a hierarchical intelligent architecture for general robotic manipulation. It consists of a high-level cognitive planner (HCP) based on a large-scale pre-trained vision-language model (VLM), an invariant operable representation (IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA). RoBridge maintains the declarative skill of VLM and unleashes the procedural skill of reinforcement learning, effectively bridging the gap between cognition and execution. RoBridge demonstrates significant performance improvements over existing baselines, achieving a 75% success rate on new tasks and an 83% average success rate in sim-to-real generalization using only five real-world data samples per task. This work represents a significant step towards integrating cognitive reasoning with physical execution in robotic systems, offering a new paradigm for general robotic manipulation.", 'abstract_zh': '在开放环境中执行多样任务的机器人具有重要的研究和应用价值：RoBridge——一种通用机器人 manipulation的层次智能架构', 'title_zh': 'RoBridge: 一种连接认知与执行的分层架构用于通用机器人操作'}
{'arxiv_id': 'arXiv:2505.01654', 'title': 'T-REX: Vision-Based System for Autonomous Leaf Detection and Grasp Estimation', 'authors': 'Srecharan Selvam, Abhisesh Silwal, George Kantor', 'link': 'https://arxiv.org/abs/2505.01654', 'abstract': 'T-Rex (The Robot for Extracting Leaf Samples) is a gantry-based robotic system developed for autonomous leaf localization, selection, and grasping in greenhouse environments. The system integrates a 6-degree-of-freedom manipulator with a stereo vision pipeline to identify and interact with target leaves. YOLOv8 is used for real-time leaf segmentation, and RAFT-Stereo provides dense depth maps, allowing the reconstruction of 3D leaf masks. These observations are processed through a leaf grasping algorithm that selects the optimal leaf based on clutter, visibility, and distance, and determines a grasp point by analyzing local surface flatness, top-down approachability, and margin from edges. The selected grasp point guides a trajectory executed by ROS-based motion controllers, driving a custom microneedle-equipped end-effector to clamp the leaf and simulate tissue sampling. Experiments conducted with artificial plants under varied poses demonstrate that the T-Rex system can consistently detect, plan, and perform physical interactions with plant-like targets, achieving a grasp success rate of 66.6\\%. This paper presents the system architecture, implementation, and testing of T-Rex as a step toward plant sampling automation in Controlled Environment Agriculture (CEA).', 'abstract_zh': 'T-Rex（用于提取叶片样本的机器人）是为温室环境中自主叶片定位、选择和抓取而开发的基于龙门架的机器人系统。该系统集成了六自由度 manipulator 与立体视觉处理管道，用于识别和与目标叶片交互。YOLOv8 用于实时叶片分割，RAFT-Stereo 提供密集深度图，允许重建叶片的 3D 掩模。这些观察结果通过叶片抓取算法处理，该算法根据杂物、可见性和距离选择最优叶片，并通过分析局部表面平坦度、顶部可接近性和边缘距离来确定握持点。选定的握持点引导由 ROS 基础运动控制器执行的轨迹，驱动配备微针的自定义末端执行器夹紧叶片并模拟组织取样。在不同姿态的人工植物上进行的实验表明，T-Rex 系统可以一致地检测、规划并与植物样靶进行物理交互，实现夹持成功率 66.6%。本文介绍了 T-Rex 的系统架构、实现和测试，旨在推进受控环境农业（CEA）中的植物采样自动化。', 'title_zh': '基于视觉的自主叶片检测与抓取估计系统'}
{'arxiv_id': 'arXiv:2505.01630', 'title': 'Deformable Cargo Transport in Microgravity with Astrobee', 'authors': 'Daniel Morton, Rika Antonova, Brian Coltin, Marco Pavone, Jeannette Bohg', 'link': 'https://arxiv.org/abs/2505.01630', 'abstract': "We present pyastrobee: a simulation environment and control stack for Astrobee in Python, with an emphasis on cargo manipulation and transport tasks. We also demonstrate preliminary success from a sampling-based MPC controller, using reduced-order models of NASA's cargo transfer bag (CTB) to control a high-order deformable finite element model. Our code is open-source, fully documented, and available at this https URL", 'abstract_zh': 'Python中的pyastrobee：Astrobee的仿真环境和控制栈，侧重于货物操作与运输任务。基于采样 MPC 控制器的初步成功演示，使用NASA货物转移袋（CTB）的降阶模型控制高阶可变形有限元模型。代码开源、完全文档化，并可从以下链接获取。', 'title_zh': '微重力环境下Astrobee的可变形载荷运输'}
{'arxiv_id': 'arXiv:2505.01624', 'title': 'Triangle-Decomposable Graphs for Isoperimetric Robots', 'authors': 'Nathan Usevitch, Isaac Weaver, James Usevitch', 'link': 'https://arxiv.org/abs/2505.01624', 'abstract': 'Isoperimetric robots are large scale, untethered inflatable robots that can undergo large shape changes, but have only been demonstrated in one 3D shape -- an octahedron. These robots consist of independent triangles that can change shape while maintaining their perimeter by moving the relative position of their joints. We introduce an optimization routine that determines if an arbitrary graph can be partitioned into unique triangles, and thus be constructed as an isoperimetric robotic system. We enumerate all minimally rigid graphs that can be constructed with unique triangles up to 9 nodes (7 triangles), and characterize the workspace of one node of each these robots. We also present a method for constructing larger graphs that can be partitioned by assembling subgraphs that are already partitioned into triangles. This enables a wide variety of isoperimetric robot configurations.', 'abstract_zh': '可周长性的机器人是由独立三角形组成的大型无缆充气机器人，能够发生显著形状变化，但仅在八面体这一种三维形状中得到证明。本文介绍了一种优化方法，用于判断任意图形是否可以分解为唯一的三角形，从而被构建为可周长性机器人系统。我们列出了最多包含9个节点（7个三角形）的全部最小刚性图形，并对其每个节点的工作空间进行了characterization。我们还提出了一种方法，通过组装已经分解为三角形的子图形来构建更大的图形。这使得可周长性机器人的配置更为多样化。', 'title_zh': '三角分解图中的等周机器人'}
{'arxiv_id': 'arXiv:2505.01617', 'title': 'High Speed Robotic Table Tennis Swinging Using Lightweight Hardware with Model Predictive Control', 'authors': 'David Nguyen, Kendrick D. Cancio, Sangbae Kim', 'link': 'https://arxiv.org/abs/2505.01617', 'abstract': 'We present a robotic table tennis platform that achieves a variety of hit styles and ball-spins with high precision, power, and consistency. This is enabled by a custom lightweight, high-torque, low rotor inertia, five degree-of-freedom arm capable of high acceleration. To generate swing trajectories, we formulate an optimal control problem (OCP) that constrains the state of the paddle at the time of the strike. The terminal position is given by a predicted ball trajectory, and the terminal orientation and velocity of the paddle are chosen to match various possible styles of hits: loops (topspin), drives (flat), and chops (backspin). Finally, we construct a fixed-horizon model predictive controller (MPC) around this OCP to allow the hardware to quickly react to changes in the predicted ball trajectory. We validate on hardware that the system is capable of hitting balls with an average exit velocity of 11 m/s at an 88% success rate across the three swing types.', 'abstract_zh': '一种实现高精度、高功率和高一致性的乒乓球机器人平台及其控制方法', 'title_zh': '使用轻量化硬件和模型预测控制的高速机器人乒乓球摆动'}
{'arxiv_id': 'arXiv:2505.01589', 'title': 'Phasing Through the Flames: Rapid Motion Planning with the AGHF PDE for Arbitrary Objective Functions and Constraints', 'authors': 'Challen Enninful Adu, César E. Ramos Chuquiure, Yutong Zhou, Pearl Lin, Ruikai Yang, Bohao Zhang, Shubham Singh, Ram Vasudevan', 'link': 'https://arxiv.org/abs/2505.01589', 'abstract': 'The generation of optimal trajectories for high-dimensional robotic systems under constraints remains computationally challenging due to the need to simultaneously satisfy dynamic feasibility, input limits, and task-specific objectives while searching over high-dimensional spaces. Recent approaches using the Affine Geometric Heat Flow (AGHF) Partial Differential Equation (PDE) have demonstrated promising results, generating dynamically feasible trajectories for complex systems like the Digit V3 humanoid within seconds. These methods efficiently solve trajectory optimization problems over a two-dimensional domain by evolving an initial trajectory to minimize control effort. However, these AGHF approaches are limited to a single type of optimal control problem (i.e., minimizing the integral of squared control norms) and typically require initial guesses that satisfy constraints to ensure satisfactory convergence. These limitations restrict the potential utility of the AGHF PDE especially when trying to synthesize trajectories for robotic systems. This paper generalizes the AGHF formulation to accommodate arbitrary cost functions, significantly expanding the classes of trajectories that can be generated. This work also introduces a Phase1 - Phase 2 Algorithm that enables the use of constraint-violating initial guesses while guaranteeing satisfactory convergence. The effectiveness of the proposed method is demonstrated through comparative evaluations against state-of-the-art techniques across various dynamical systems and challenging trajectory generation problems. Project Page: this https URL', 'abstract_zh': '高维约束条件下最优轨迹生成对于高性能机器人系统而言仍具有计算挑战性，现有的方法如使用Affine Geometric Heat Flow (AGHF)偏微分方程（PDE）虽能在秒内生成复杂的Digit V3人形机器人动态可行轨迹，但仅适用于特定类型的最优控制问题，并且通常需要满足约束条件的初始猜测以确保收敛性。为克服这些限制，本文将AGHF公式化扩展至任意成本函数，显著扩展了可生成的轨迹类型。此外，本文还引入了Phase1-Phase2算法，可使用违反约束的初始猜测并保证收敛性。通过与最新技术在多种动力学系统和挑战性轨迹生成问题上的对比评估，证明了所提出方法的有效性。项目页面：this https URL', 'title_zh': '火焰中的相位跃迁：任意目标函数和约束条件下的快速运动规划'}
{'arxiv_id': 'arXiv:2505.01547', 'title': 'ASAP-MO:Advanced Situational Awareness and Perception for Mission-critical Operations', 'authors': 'Veronica Vannini, William Dubois, Olivier Gamache, Jean-Michel Fortin, Nicolas Samson, Effie Daum, François Pomerleau, Edith Brotherton', 'link': 'https://arxiv.org/abs/2505.01547', 'abstract': 'Deploying robotic missions can be challenging due to the complexity of controlling robots with multiple degrees of freedom, fusing diverse sensory inputs, and managing communication delays and interferences. In nuclear inspection, robots can be crucial in assessing environments where human presence is limited, requiring precise teleoperation and coordination. Teleoperation requires extensive training, as operators must process multiple outputs while ensuring safe interaction with critical assets. These challenges are amplified when operating a fleet of heterogeneous robots across multiple environments, as each robot may have distinct control interfaces, sensory systems, and operational constraints. Efficient coordination in such settings remains an open problem. This paper presents a field report on how we integrated robot fleet capabilities - including mapping, localization, and telecommunication - toward a joint mission. We simulated a nuclear inspection scenario for exposed areas, using lights to represent a radiation source. We deployed two Unmanned Ground Vehicles (UGVs) tasked with mapping indoor and outdoor environments while remotely controlled from a single base station. Despite having distinct operational goals, the robots produced a unified map output, demonstrating the feasibility of coordinated multi-robot missions. Our results highlight key operational challenges and provide insights into improving adaptability and situational awareness in remote robotic deployments.', 'abstract_zh': '部署机器人任务由于多自由度控制的复杂性、多种传感器输入的融合以及通信延迟和干扰的管理而具有挑战性。在核检查中，机器人在人类存在受限的环境中可以发挥关键作用，要求精准的遥控和协调。遥控需要广泛训练，因为操作员必须处理多个输出并确保与关键资产的安全互动。当操作多种环境中的异构机器人舰队时，这些挑战会加剧，因为每台机器人都可能具有不同的控制界面、感测系统和运行约束。在这种环境中有效地协调仍然是一个开放问题。本文报告了我们如何集成机器人舰队的能力——包括测绘、定位和通信——以为联合任务做准备。我们模拟了一个暴露区域的核检查场景，使用灯光代表辐射源。我们部署了两辆无人地面车辆（UGVs），任务是在单个基站远程控制下测绘室内和室外环境。尽管具备不同的操作目标，机器人仍生成了统一的地图输出，展示了协调多机器人任务的可行性。我们的结果突显了关键的操作挑战，并提供了关于改善远程机器人部署中的适应性和情况意识的见解。', 'title_zh': 'ASAP-MO：高级态势感知与关键任务操作中的感知'}
{'arxiv_id': 'arXiv:2505.01515', 'title': 'Comparison of Waymo Rider-Only Crash Rates by Crash Type to Human Benchmarks at 56.7 Million Miles', 'authors': 'Kristofer D. Kusano, John M. Scanlon, Yin-Hsiu Chen, Timothy L. McMurry, Tilia Gode, Trent Victor', 'link': 'https://arxiv.org/abs/2505.01515', 'abstract': "SAE Level 4 Automated Driving Systems (ADSs) are deployed on public roads, including Waymo's Rider-Only (RO) ride-hailing service (without a driver behind the steering wheel). The objective of this study was to perform a retrospective safety assessment of Waymo's RO crash rate compared to human benchmarks, including disaggregated by crash type.\nEleven crash type groups were identified from commonly relied upon crash typologies that are derived from human crash databases. Human benchmarks were aligned to the same vehicle types, road types, and locations as where the Waymo Driver operated. Waymo crashes were extracted from the NHTSA Standing General Order (SGO). RO mileage was provided by the company via a public website. Any-injury-reported, Airbag Deployment, and Suspected Serious Injury+ crash outcomes were examined because they represented previously established, safety-relevant benchmarks where statistical testing could be performed at the current mileage.\nData was examined over 56.7 million RO miles through the end of January 2025, resulting in a statistically significant lower crashed vehicle rate for all crashes compared to the benchmarks in Any-Injury-Reported and Airbag Deployment, and Suspected Serious Injury+ crashes. Of the crash types, V2V Intersection crash events represented the largest total crash reduction, with a 96% reduction in Any-injury-reported (87%-99% CI) and a 91% reduction in Airbag Deployment (76%-98% CI) events. Cyclist, Motorcycle, Pedestrian, Secondary Crash, and Single Vehicle crashes were also statistically reduced for the Any-Injury-Reported outcome. There was no statistically significant disbenefit found in any of the 11 crash type groups.\nThis study represents the first retrospective safety assessment of an RO ADS that made statistical conclusions about more serious crash outcomes and analyzed crash rates on a crash type basis.", 'abstract_zh': 'SAE Level 4 自动驾驶系统 (ADS) 在公共道路上的应用，包括 Waymo 仅乘客（RO）网约车服务（无驾驶者）。本研究旨在对比 Waymo RO 碰撞率与人类基准，包括按碰撞类型细分。', 'title_zh': 'Waymo仅乘客车祸类型比率与人类基准在5670万公里的比较'}
{'arxiv_id': 'arXiv:2505.01486', 'title': 'Aerial Path Online Planning for Urban Scene Updation', 'authors': 'Mingfeng Tang, Ziyuan Xie, Ke Xie, Hui Huang, Jianwei Hu, Ningna Wang, Xiaohu Guo', 'link': 'https://arxiv.org/abs/2505.01486', 'abstract': 'We present the first scene-update aerial path planning algorithm specifically designed for detecting and updating change areas in urban environments. While existing methods for large-scale 3D urban scene reconstruction focus on achieving high accuracy and completeness, they are inefficient for scenarios requiring periodic updates, as they often re-explore and reconstruct entire scenes, wasting significant time and resources on unchanged areas. To address this limitation, our method leverages prior reconstructions and change probability statistics to guide UAVs in detecting and focusing on areas likely to have changed. Our approach introduces a novel changeability heuristic to evaluate the likelihood of changes, driving the planning of two flight paths: a prior path informed by static priors and a dynamic real-time path that adapts to newly detected changes. The framework integrates surface sampling and candidate view generation strategies, ensuring efficient coverage of change areas with minimal redundancy. Extensive experiments on real-world urban datasets demonstrate that our method significantly reduces flight time and computational overhead, while maintaining high-quality updates comparable to full-scene re-exploration and reconstruction. These contributions pave the way for efficient, scalable, and adaptive UAV-based scene updates in complex urban environments.', 'abstract_zh': '我们提出了首个专门用于检测和更新城市环境中变化区域的场景更新空中路径规划算法。现有的大规模3D城市场景重建方法侧重于提高准确性和完整性，但在需要定期更新的场景中效率低下，因为它们往往会重新探索和重建整个场景，浪费大量时间和资源在未发生变化的区域上。为解决这一限制，我们的方法利用先验重建和变化概率统计来引导无人机检测和关注可能发生变化的区域。我们的方法引入了一种新颖的变化性启发式方法，以评估变化的可能性，并据此规划两种飞行路径：一种基于静态先验信息的先验路径，一种动态实时路径，能够根据新检测到的变化进行适应调整。该框架集成了表面采样和候选视图生成策略，以确保高效覆盖变化区域，同时减少冗余。在现实世界的城市数据集上的广泛实验表明，我们的方法显著减少了飞行时间和计算开销，同时保持了与完全场景重新探索和重建相当的质量更新。这些贡献为复杂城市环境中高效的、可扩展的和自适应的无人机基于场景更新铺平了道路。', 'title_zh': '城市场景更新的空中路径在线规划'}
{'arxiv_id': 'arXiv:2505.01458', 'title': 'A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI', 'authors': 'Lik Hang Kenny Wong, Xueyang Kang, Kaixin Bai, Jianwei Zhang', 'link': 'https://arxiv.org/abs/2505.01458', 'abstract': 'Navigation and manipulation are core capabilities in Embodied AI, yet training agents with these capabilities in the real world faces high costs and time complexity. Therefore, sim-to-real transfer has emerged as a key approach, yet the sim-to-real gap persists. This survey examines how physics simulators address this gap by analyzing their properties overlooked in previous surveys. We also analyze their features for navigation and manipulation tasks, along with hardware requirements. Additionally, we offer a resource with benchmark datasets, metrics, simulation platforms, and cutting-edge methods-such as world models and geometric equivariance-to help researchers select suitable tools while accounting for hardware constraints.', 'abstract_zh': '物理模拟器如何弥合导航与操作任务中模拟到现实的差距：综述与资源指南', 'title_zh': 'embodied AI时代基于物理模拟的机器人导航与操作综述'}
{'arxiv_id': 'arXiv:2505.01446', 'title': 'Waymo Driverless Car Data Analysis and Driving Modeling using CNN and LSTM', 'authors': 'Aashish Kumar Misraa, Naman Jain, Saurav Singh Dhakad', 'link': 'https://arxiv.org/abs/2505.01446', 'abstract': "Self driving cars has been the biggest innovation in the automotive industry, but to achieve human level accuracy or near human level accuracy is the biggest challenge that research scientists are facing today. Unlike humans autonomous vehicles do not work on instincts rather they make a decision based on the training data that has been fed to them using machine learning models using which they can make decisions in different conditions they face in the real world. With the advancements in machine learning especially deep learning the self driving car research skyrocketed. In this project we have presented multiple ways to predict acceleration of the autonomous vehicle using Waymo's open dataset. Our main approach was to using CNN to mimic human action and LSTM to treat this as a time series problem.", 'abstract_zh': '自动驾驶汽车一直是汽车行业的最大创新，但要实现或接近人类级别的准确性是研究人员今天面临的最大挑战。与人类不同，自动驾驶车辆不是基于本能作出决策，而是根据机器学习模型提供的训练数据，在面对现实世界中的各种情况时作出决策。随着机器学习尤其是深度学习的进步，自动驾驶汽车的研究取得了迅速发展。在本项目中，我们使用Waymo的开放数据集展示了多种预测自动驾驶车辆加速度的方法。我们的主要方法是使用CNN来模拟人类行为，并使用LSTM将此问题视为时间序列问题。', 'title_zh': '使用CNN和LSTM进行Waymo无人驾驶汽车数据分析与驾驶建模'}
{'arxiv_id': 'arXiv:2505.02766', 'title': 'Giving Simulated Cells a Voice: Evolving Prompt-to-Intervention Models for Cellular Control', 'authors': 'Nam H. Le, Patrick Erikson, Yanbo Zhang, Michael Levin, Josh Bongard', 'link': 'https://arxiv.org/abs/2505.02766', 'abstract': 'Guiding biological systems toward desired states, such as morphogenetic outcomes, remains a fundamental challenge with far-reaching implications for medicine and synthetic biology. While large language models (LLMs) have enabled natural language as an interface for interpretable control in AI systems, their use as mediators for steering biological or cellular dynamics remains largely unexplored.\nIn this work, we present a functional pipeline that translates natural language prompts into spatial vector fields capable of directing simulated cellular collectives. Our approach combines a large language model with an evolvable neural controller (Prompt-to-Intervention, or P2I), optimized via evolutionary strategies to generate behaviors such as clustering or scattering in a simulated 2D environment.\nWe demonstrate that even with constrained vocabulary and simplified cell models, evolved P2I networks can successfully align cellular dynamics with user-defined goals expressed in plain language. This work offers a complete loop from language input to simulated bioelectric-like intervention to behavioral output, providing a foundation for future systems capable of natural language-driven cellular control.', 'abstract_zh': '引导生物系统朝向所需的稳态，例如形态发生结果，仍然是一个基本挑战，对医学和合成生物学具有深远的意义。尽管大型语言模型（LLMs）使自然语言成为AI系统可解释控制的界面，但它们作为引导生物或细胞动力学的中介的应用仍鲜有探索。\n\n在本工作中，我们提出了一套功能性的管道，能够将自然语言提示转化为用于指导模拟细胞群体的空间向量场。我们的方法结合了大型语言模型和可进化神经控制器（Prompt-to-Intervention，或P2I），通过进化策略优化生成聚集或分散等行为，以在模拟的2D环境中进行。\n\n我们证明，即使使用受限的词汇表和简化的细胞模型，进化出的P2I网络也能成功地将细胞动力学与用户以自然语言表达的目标对齐。本工作提供了一个从语言输入到模拟生物电式干预再到行为输出的完整循环，为未来能够实现自然语言驱动的细胞控制系统奠定了基础。', 'title_zh': '赋予模拟细胞声音：演化提示到干预的模型以控制细胞行为'}
{'arxiv_id': 'arXiv:2505.02501', 'title': 'Corr2Distrib: Making Ambiguous Correspondences an Ally to Predict Reliable 6D Pose Distributions', 'authors': 'Asma Brazi, Boris Meden, Fabrice Mayran de Chamisso, Steve Bourgeois, Vincent Lepetit', 'link': 'https://arxiv.org/abs/2505.02501', 'abstract': "We introduce Corr2Distrib, the first correspondence-based method which estimates a 6D camera pose distribution from an RGB image, explaining the observations. Indeed, symmetries and occlusions introduce visual ambiguities, leading to multiple valid poses. While a few recent methods tackle this problem, they do not rely on local correspondences which, according to the BOP Challenge, are currently the most effective way to estimate a single 6DoF pose solution. Using correspondences to estimate a pose distribution is not straightforward, since ambiguous correspondences induced by visual ambiguities drastically decrease the performance of PnP. With Corr2Distrib, we turn these ambiguities into an advantage to recover all valid poses. Corr2Distrib first learns a symmetry-aware representation for each 3D point on the object's surface, characterized by a descriptor and a local frame. This representation enables the generation of 3DoF rotation hypotheses from single 2D-3D correspondences. Next, we refine these hypotheses into a 6DoF pose distribution using PnP and pose scoring. Our experimental evaluations on complex non-synthetic scenes show that Corr2Distrib outperforms state-of-the-art solutions for both pose distribution estimation and single pose estimation from an RGB image, demonstrating the potential of correspondences-based approaches.", 'abstract_zh': '基于对应关系的2D到分布方法：从RGB图像估计6D相机姿态分布', 'title_zh': 'Corr2Distrib: 将含糊对应关系转化为预测可靠6D姿态分布的助力'}
{'arxiv_id': 'arXiv:2505.02388', 'title': 'MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans', 'authors': 'Huangyue Yu, Baoxiong Jia, Yixin Chen, Yandan Yang, Puhao Li, Rongpeng Su, Jiaxin Li, Qing Li, Wei Liang, Song-Chun Zhu, Tengyu Liu, Siyuan Huang', 'link': 'https://arxiv.org/abs/2505.02388', 'abstract': "Embodied AI (EAI) research requires high-quality, diverse 3D scenes to effectively support skill acquisition, sim-to-real transfer, and generalization. Achieving these quality standards, however, necessitates the precise replication of real-world object diversity. Existing datasets demonstrate that this process heavily relies on artist-driven designs, which demand substantial human effort and present significant scalability challenges. To scalably produce realistic and interactive 3D scenes, we first present MetaScenes, a large-scale, simulatable 3D scene dataset constructed from real-world scans, which includes 15366 objects spanning 831 fine-grained categories. Then, we introduce Scan2Sim, a robust multi-modal alignment model, which enables the automated, high-quality replacement of assets, thereby eliminating the reliance on artist-driven designs for scaling 3D scenes. We further propose two benchmarks to evaluate MetaScenes: a detailed scene synthesis task focused on small item layouts for robotic manipulation and a domain transfer task in vision-and-language navigation (VLN) to validate cross-domain transfer. Results confirm MetaScene's potential to enhance EAI by supporting more generalizable agent learning and sim-to-real applications, introducing new possibilities for EAI research. Project website: this https URL.", 'abstract_zh': 'Body-嵌入AI（EAI）研究需要高质量的多元化3D场景，以有效地支持技能获取、模拟到现实的转移和泛化。然而，实现这些质量标准需要精确复制现实世界的对象多样性。现有数据集表明，这一过程高度依赖于艺术家驱动的设计，这需要大量的人工努力并导致可扩展性挑战。为了可扩展地生成逼真且可交互的3D场景，我们首先介绍了MetaScenes，这是一个从真实世界扫描构建的大规模、可模拟的3D场景数据集，包含涵盖831个细分类别的15366个对象。随后，我们介绍了Scan2Sim，这是一种稳健的多模态对齐模型，能够实现自动化、高质量的资产替换，从而消除依赖艺术家驱动的设计来扩展3D场景。此外，我们提出了两个基准来评估MetaScenes：一个专门针对机器人操作的小项布局详细场景合成任务，以及用于视觉-语言导航（VLN）的领域迁移任务，以验证跨域迁移。结果证实了MetaScenes增强EAI的潜力，支持更通用的代理学习和模拟到现实的应用，为EAI研究引入了新的可能性。项目网站：此链接。', 'title_zh': 'MetaScenes: 向着现实世界3D扫描的自动化副本创建'}
{'arxiv_id': 'arXiv:2505.02274', 'title': 'On the Need for a Statistical Foundation in Scenario-Based Testing of Autonomous Vehicles', 'authors': 'Xingyu Zhao, Robab Aghazadeh-Chakherlou, Chih-Hong Cheng, Peter Popov, Lorenzo Strigini', 'link': 'https://arxiv.org/abs/2505.02274', 'abstract': 'Scenario-based testing has emerged as a common method for autonomous vehicles (AVs) safety, offering a more efficient alternative to mile-based testing by focusing on high-risk scenarios. However, fundamental questions persist regarding its stopping rules, residual risk estimation, debug effectiveness, and the impact of simulation fidelity on safety claims. This paper argues that a rigorous statistical foundation is essential to address these challenges and enable rigorous safety assurance. By drawing parallels between AV testing and traditional software testing methodologies, we identify shared research gaps and reusable solutions. We propose proof-of-concept models to quantify the probability of failure per scenario (pfs) and evaluate testing effectiveness under varying conditions. Our analysis reveals that neither scenario-based nor mile-based testing universally outperforms the other. Furthermore, we introduce Risk Estimation Fidelity (REF), a novel metric to certify the alignment of synthetic and real-world testing outcomes, ensuring simulation-based safety claims are statistically defensible.', 'abstract_zh': '基于场景的测试已成为自动驾驶车辆安全评估的常见方法，通过关注高风险场景，提供了一种更高效的替代基于里程的测试的方法。然而，关于其停止规则、剩余风险估算、调试效果以及仿真保真度对安全声明的影响等基础问题依然存疑。本文认为，严格的统计基础是解决这些挑战并实现严格的安全生产保障所必需的。通过将自动驾驶车辆测试与传统的软件测试方法进行比较，我们识别了共同的研究空白并提出了可重用的解决方案。我们提议的概念验证模型用于量化每个场景的故障概率（pfs），并在不同条件下评估测试的有效性。我们的分析表明，基于场景的测试和基于里程的测试在普遍性能上不存在优势。此外，我们提出了仿真保真度风险估计（REF）这一新型度量标准，以确保合成和实际测试结果的对齐，并确保基于仿真的安全声明具有统计上的可辩护性。', 'title_zh': '基于场景的自动驾驶车辆测试中统计基础的必要性'}
{'arxiv_id': 'arXiv:2505.02050', 'title': 'Enhancing Safety Standards in Automated Systems Using Dynamic Bayesian Networks', 'authors': 'Kranthi Kumar Talluri, Anders L. Madsen, Galia Weidl', 'link': 'https://arxiv.org/abs/2505.02050', 'abstract': "Cut-in maneuvers in high-speed traffic pose critical challenges that can lead to abrupt braking and collisions, necessitating safe and efficient lane change strategies. We propose a Dynamic Bayesian Network (DBN) framework to integrate lateral evidence with safety assessment models, thereby predicting lane changes and ensuring safe cut-in maneuvers effectively. Our proposed framework comprises three key probabilistic hypotheses (lateral evidence, lateral safety, and longitudinal safety) that facilitate the decision-making process through dynamic data processing and assessments of vehicle positions, lateral velocities, relative distance, and Time-to-Collision (TTC) computations. The DBN model's performance compared with other conventional approaches demonstrates superior performance in crash reduction, especially in critical high-speed scenarios, while maintaining a competitive performance in low-speed scenarios. This paves the way for robust, scalable, and efficient safety validation in automated driving systems.", 'abstract_zh': '高速交通中的切变 maneuver 操作提出了关键挑战，可能导致紧急制动和碰撞，需要安全有效的变道策略。我们提出了一种动态贝叶斯网络（DBN）框架，以整合横向证据与安全性评估模型，从而预测变道并有效确保安全的切变操作。提出的框架包括三个关键的概率假设（横向证据、横向安全性和纵向安全性），通过动态数据处理和对车辆位置、横向速度、相对距离以及碰撞时间（TTC）的评估，促进决策过程。与其它传统方法相比，DBN模型在降低事故方面表现出更优越的性能，尤其是在关键的高速场景中，同时在低速场景中保持竞争力。这为自动驾驶系统的稳健、可扩展和高效的安全验证铺平了道路。', 'title_zh': '使用动态贝叶斯网络提高自动化系统的安全标准'}
{'arxiv_id': 'arXiv:2505.01947', 'title': 'Runtime Anomaly Detection for Drones: An Integrated Rule-Mining and Unsupervised-Learning Approach', 'authors': 'Ivan Tan, Wei Minn, Christopher M. Poskitt, Lwin Khin Shar, Lingxiao Jiang', 'link': 'https://arxiv.org/abs/2505.01947', 'abstract': 'UAVs, commonly referred to as drones, have witnessed a remarkable surge in popularity due to their versatile applications. These cyber-physical systems depend on multiple sensor inputs, such as cameras, GPS receivers, accelerometers, and gyroscopes, with faults potentially leading to physical instability and serious safety concerns. To mitigate such risks, anomaly detection has emerged as a crucial safeguarding mechanism, capable of identifying the physical manifestations of emerging issues and allowing operators to take preemptive action at runtime. Recent anomaly detection methods based on LSTM neural networks have shown promising results, but three challenges persist: the need for models that can generalise across the diverse mission profiles of drones; the need for interpretability, enabling operators to understand the nature of detected problems; and the need for capturing domain knowledge that is difficult to infer solely from log data. Motivated by these challenges, this paper introduces RADD, an integrated approach to anomaly detection in drones that combines rule mining and unsupervised learning. In particular, we leverage rules (or invariants) to capture expected relationships between sensors and actuators during missions, and utilise unsupervised learning techniques to cover more subtle relationships that the rules may have missed. We implement this approach using the ArduPilot drone software in the Gazebo simulator, utilising 44 rules derived across the main phases of drone missions, in conjunction with an ensemble of five unsupervised learning models. We find that our integrated approach successfully detects 93.84% of anomalies over six types of faults with a low false positive rate (2.33%), and can be deployed effectively at runtime. Furthermore, RADD outperforms a state-of-the-art LSTM-based method in detecting the different types of faults evaluated in our study.', 'abstract_zh': 'UAVs（无人机）由于其多样的应用而见证了显著的 popularity 增长。这些网络物理系统依赖于多种传感器输入，如摄像头、GPS 接收器、加速度计和陀螺仪，故障可能导致物理不稳定性和严重的安全问题。为了减轻这些风险，异常检测已成为一种关键的安全机制，能够识别新兴问题的物理表现，并允许操作者在运行时采取预防措施。基于 LSTM 神经网络的最近异常检测方法展示了有前途的结果，但存在三个挑战：能够普遍适用于无人机多样化任务配置的模型需求；增加可解释性，使操作者能够理解检测到的问题的性质；以及捕捉仅从日志数据中难以推断的领域知识的需求。基于这些挑战，本文介绍了一种集成的无人机异常检测方法 RADD，该方法结合了规则挖掘和无监督学习。特别是，我们利用规则（或不变量）来捕获任务期间传感器和执行器之间预期的关系，并利用无监督学习技术来覆盖规则可能遗漏的更微妙的关系。我们使用 ArduPilot 无人机软件在 Gazebo 模拟器中实现该方法，利用跨越无人机任务主要阶段的 44 条规则，结合五种无监督学习模型的集成。我们发现，我们的集成方法在六种类型故障中成功检测了 93.84% 的异常，且假阳性率较低（2.33%），并且可以在运行时有效部署。此外，RADD 在检测我们在研究中评估的不同类型故障方面优于最先进的基于 LSTM 的方法。', 'title_zh': '无人机运行异常检测：一种集成规则挖掘和无监督学习的方法'}
{'arxiv_id': 'arXiv:2505.01945', 'title': 'Act Natural! Extending Naturalistic Projection to Multimodal Behavior Scenarios', 'authors': 'Hamzah I. Khan, David Fridovich-Keil', 'link': 'https://arxiv.org/abs/2505.01945', 'abstract': 'Autonomous agents operating in public spaces must consider how their behaviors might affect the humans around them, even when not directly interacting with them. To this end, it is often beneficial to be predictable and appear naturalistic. Existing methods for this purpose use human actor intent modeling or imitation learning techniques, but these approaches rarely capture all possible motivations for human behavior and/or require significant amounts of data. Our work extends a technique for modeling unimodal naturalistic behaviors with an explicit convex set representation, to account for multimodal behavior by using multiple convex sets. This more flexible representation provides a higher degree of fidelity in data-driven modeling of naturalistic behavior that arises in real-world scenarios in which human behavior is, in some sense, discrete, e.g. whether or not to yield at a roundabout. Equipped with this new set representation, we develop an optimization-based filter to project arbitrary trajectories into the set so that they appear naturalistic to humans in the scene, while also satisfying vehicle dynamics, actuator limits, etc. We demonstrate our methods on real-world human driving data from the inD (intersection) and rounD (roundabout) datasets.', 'abstract_zh': '自主操作于公共空间的代理必须考虑其行为可能对周围的行人造成的影响，即使不直接与他们互动。为此，表现出可预测性和自然性通常是有益的。现有方法通过人类行为意图建模或仿真人学习技术来实现这一目标，但这些方法很少能够捕捉到所有可能的人类行为动机，或者需要大量数据。我们的工作扩展了一种用于建模单模态自然行为的技术，通过使用多个凸集来考虑多模态行为，从而提供更灵活的表示形式，在真实场景中以更高的保真度数据驱动建模自然行为，例如在环岛是否礼让等具有一种意义上离散的人类行为。借助这种新的集合表示法，我们开发了一种基于优化的滤波器，将任意轨迹投影到集合中，使其在场景中显得自然，同时满足车辆动力学、执行器限制等要求。我们通过inD（交叉口）和rounD（环岛）数据集中的真实人类驾驶数据，展示了我们的方法。', 'title_zh': 'Act Natural！将自然投影扩展到多模态行为场景'}
{'arxiv_id': 'arXiv:2505.01881', 'title': 'PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications', 'authors': 'Trisanth Srinivasan, Santosh Patapati', 'link': 'https://arxiv.org/abs/2505.01881', 'abstract': 'Robust navigation in diverse environments and domains requires both accurate state estimation and transparent decision making. We present PhysNav-DG, a novel framework that integrates classical sensor fusion with the semantic power of vision-language models. Our dual-branch architecture predicts navigation actions from multi-sensor inputs while simultaneously generating detailed chain-of-thought explanations. A modified Adaptive Kalman Filter dynamically adjusts its noise parameters based on environmental context. It leverages several streams of raw sensor data along with semantic insights from models such as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the MD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation, autonomous driving, and social navigation tasks with ground-truth actions and human-validated explanations. Extensive experiments and ablations show that PhysNav-DG improves navigation success rates by over 20% and achieves high efficiency, with explanations that are both highly grounded and clear. This work connects high-level semantic reasoning and geometric planning for safer and more trustworthy autonomous systems.', 'abstract_zh': 'robust导航在不同环境和领域中要求既准确的状态估计又透明的决策制定。我们提出PhysNav-DG，这是一个将经典传感器融合与视觉语言模型的语义能力相结合的新框架。我们的双分支架构从多传感器输入中预测导航动作，同时生成详细的推理解释。改良的自适应卡尔曼滤波器根据环境上下文动态调整其噪声参数。它利用多路原始传感器数据，并结合如LLaMA 3.2 11B和BLIP-2等模型的语义洞察。为了评估我们的方法，我们引入了MD-NEX基准，这是一个新颖的多领域数据集，统一了室内导航、自主驾驶和社会导航任务，并包含真实动作和人类验证的解释。广泛的经验和消融实验表明，PhysNav-DG 将导航成功率提高了超过20%，并且效率高，解释既具体又清晰。这项工作将高层次的语义推理与几何规划相结合，以实现更安全、更可信赖的自主系统。', 'title_zh': 'PhysNav-DG：一种用于导航应用的新型鲁棒的VLM-传感器融合自适应框架'}
{'arxiv_id': 'arXiv:2505.01766', 'title': 'Multimodal Graph Representation Learning for Robust Surgical Workflow Recognition with Adversarial Feature Disentanglement', 'authors': 'Long Bai, Boyi Ma, Ruohan Wang, Guankun Wang, Beilei Cui, Zhongliang Jiang, Mobarakol Islam, Zhe Min, Jiewen Lai, Nassir Navab, Hongliang Ren', 'link': 'https://arxiv.org/abs/2505.01766', 'abstract': 'Surgical workflow recognition is vital for automating tasks, supporting decision-making, and training novice surgeons, ultimately improving patient safety and standardizing procedures. However, data corruption can lead to performance degradation due to issues like occlusion from bleeding or smoke in surgical scenes and problems with data storage and transmission. In this case, we explore a robust graph-based multimodal approach to integrating vision and kinematic data to enhance accuracy and reliability. Vision data captures dynamic surgical scenes, while kinematic data provides precise movement information, overcoming limitations of visual recognition under adverse conditions. We propose a multimodal Graph Representation network with Adversarial feature Disentanglement (GRAD) for robust surgical workflow recognition in challenging scenarios with domain shifts or corrupted data. Specifically, we introduce a Multimodal Disentanglement Graph Network that captures fine-grained visual information while explicitly modeling the complex relationships between vision and kinematic embeddings through graph-based message modeling. To align feature spaces across modalities, we propose a Vision-Kinematic Adversarial framework that leverages adversarial training to reduce modality gaps and improve feature consistency. Furthermore, we design a Contextual Calibrated Decoder, incorporating temporal and contextual priors to enhance robustness against domain shifts and corrupted data. Extensive comparative and ablation experiments demonstrate the effectiveness of our model and proposed modules. Moreover, our robustness experiments show that our method effectively handles data corruption during storage and transmission, exhibiting excellent stability and robustness. Our approach aims to advance automated surgical workflow recognition, addressing the complexities and dynamism inherent in surgical procedures.', 'abstract_zh': '手术流程识别对于自动化任务、支持决策制定和培训新手外科医生至关重要，最终提高患者安全性和标准化手术程序。然而，数据损坏可能会由于出血或手术场景中的烟雾导致的遮挡等问题以及数据存储和传输问题而导致性能下降。在这种情况下，我们探索了一种鲁棒的基于图的多模态方法，结合视觉和运动数据以提高准确性和可靠性。视觉数据捕捉动态的手术场景，而运动数据提供精确的运动信息，克服了不利条件下视觉识别的局限性。我们提出了一种具有对抗特征解耦的多模态图表示网络（GRAD），以在迁移或数据损坏的挑战性场景中实现鲁棒的手术流程识别。具体而言，我们引入了一种多模态分解图网络，以捕获细粒度的视觉信息，并通过基于图的消息建模显式地建模视觉和运动嵌入之间的复杂关系。为了跨模态对齐特征空间，我们提出了一种视觉-运动对抗框架，利用对抗训练减少模态差异并提高特征一致性。此外，我们设计了一种上下文校准解码器，结合时间上下文先验以增强对迁移和数据损坏的鲁棒性。广泛的对比实验和消融实验展示了我们模型及其模块的有效性。此外，我们的鲁棒性实验表明，我们的方法能够有效处理存储和传输过程中的数据损坏，表现出出色的稳定性和鲁棒性。我们的方法旨在推动自动手术流程识别的发展，以应对手术过程中的复杂性和动态性。', 'title_zh': '多模态图表示学习在对抗特征解耦下的手术工作流程robust识别'}
{'arxiv_id': 'arXiv:2505.01453', 'title': 'Safe and Efficient CAV Lane Changing using Decentralised Safety Shields', 'authors': 'Bharathkumar Hegde, Melanie Bouroche', 'link': 'https://arxiv.org/abs/2505.01453', 'abstract': 'Lane changing is a complex decision-making problem for Connected and Autonomous Vehicles (CAVs) as it requires balancing traffic efficiency with safety. Although traffic efficiency can be improved by using vehicular communication for training lane change controllers using Multi-Agent Reinforcement Learning (MARL), ensuring safety is difficult. To address this issue, we propose a decentralised Hybrid Safety Shield (HSS) that combines optimisation and a rule-based approach to guarantee safety. Our method applies control barrier functions to constrain longitudinal and lateral control inputs of a CAV to ensure safe manoeuvres. Additionally, we present an architecture to integrate HSS with MARL, called MARL-HSS, to improve traffic efficiency while ensuring safety. We evaluate MARL-HSS using a gym-like environment that simulates an on-ramp merging scenario with two levels of traffic densities, such as light and moderate densities. The results show that HSS provides a safety guarantee by strictly enforcing a dynamic safety constraint defined on a time headway, even in moderate traffic density that offers challenging lane change scenarios. Moreover, the proposed method learns stable policies compared to the baseline, a state-of-the-art MARL lane change controller without a safety shield. Further policy evaluation shows that our method achieves a balance between safety and traffic efficiency with zero crashes and comparable average speeds in light and moderate traffic densities.', 'abstract_zh': 'Connected和自主车辆（CAVs）的变道是一个复杂的决策问题，需要在交通效率与安全之间取得平衡。为了应对这一挑战，我们提出了一种去中心化的混合安全保护（HSS）方法，该方法结合了优化和基于规则的方法以确保安全。我们的方法使用控制屏障函数来约束CAV的纵向和横向控制输入，以确保安全操作。此外，我们提出了一种将HSS与多智能体强化学习（MARL）集成的架构，称为MARL-HSS，以在确保安全的同时提高交通效率。我们使用一个类似于健身房环境的仿真环境评估了MARL-HSS，在该环境中模拟了匝道汇入场景，并设置了两种交通密度水平，如轻度和中度密度。结果表明，HSS通过严格实现基于时间间隔的动力学安全约束，即使在提供挑战性变道场景的中度交通密度下，也能提供安全保障。此外，与没有安全保护的最新MARL变道控制器基准相比，我们的方法学习到更稳定的策略。进一步的策略评估显示，我们的方法在轻度和中度交通密度下实现了安全性和交通效率之间的平衡，且无撞车事件，平均速度接近基准。', 'title_zh': '基于去中心化安全遮罩的safe和高效CAV变道'}
{'arxiv_id': 'arXiv:2505.01440', 'title': 'Interactive Double Deep Q-network: Integrating Human Interventions and Evaluative Predictions in Reinforcement Learning of Autonomous Driving', 'authors': 'Alkis Sygkounas, Ioannis Athanasiadis, Andreas Persson, Michael Felsberg, Amy Loutfi', 'link': 'https://arxiv.org/abs/2505.01440', 'abstract': "Integrating human expertise with machine learning is crucial for applications demanding high accuracy and safety, such as autonomous driving. This study introduces Interactive Double Deep Q-network (iDDQN), a Human-in-the-Loop (HITL) approach that enhances Reinforcement Learning (RL) by merging human insights directly into the RL training process, improving model performance. Our proposed iDDQN method modifies the Q-value update equation to integrate human and agent actions, establishing a collaborative approach for policy development. Additionally, we present an offline evaluative framework that simulates the agent's trajectory as if no human intervention had occurred, to assess the effectiveness of human interventions. Empirical results in simulated autonomous driving scenarios demonstrate that iDDQN outperforms established approaches, including Behavioral Cloning (BC), HG-DAgger, Deep Q-Learning from Demonstrations (DQfD), and vanilla DRL in leveraging human expertise for improving performance and adaptability.", 'abstract_zh': '将人类专家知识与机器学习整合对于需求高精度和安全性的应用（如自动驾驶）至关重要。本研究介绍了交互式双深Q网络（iDDQN），这是一种基于人类在环（HITL）的方法，通过直接将人类洞察力融入强化学习（RL）训练过程，提高模型性能。我们提出的方法iDDQN修改了Q值更新方程，以整合人类和代理人的行动，建立起一种合作性的策略开发方法。此外，我们还提出了一种离线评估框架，模拟代理人的轨迹，仿佛没有人类干预的情况，以评估人类干预的有效性。在模拟的自主驾驶场景中的实证结果表明，iDDQN在利用人类专业知识提高性能和适应性方面优于现有方法，包括行为克隆（BC）、HG-DAgger、深度Q学习从演示（DQfD）以及vanilla DRL。', 'title_zh': '交互式双深度Q网络：在自动驾驶强化学习中的人类干预与评价预测集成'}
