# HMC: Learning Heterogeneous Meta-Control for Contact-Rich Loco-Manipulation 

**Title (ZH)**: HMC：学习接触丰富的异质元控制 

**Authors**: Lai Wei, Xuanbin Peng, Ri-Zhao Qiu, Tianshu Huang, Xuxin Cheng, Xiaolong Wang  

**Link**: [PDF](https://arxiv.org/pdf/2511.14756)  

**Abstract**: Learning from real-world robot demonstrations holds promise for interacting with complex real-world environments. However, the complexity and variability of interaction dynamics often cause purely positional controllers to struggle with contacts or varying payloads. To address this, we propose a Heterogeneous Meta-Control (HMC) framework for Loco-Manipulation that adaptively stitches multiple control modalities: position, impedance, and hybrid force-position. We first introduce an interface, HMC-Controller, for blending actions from different control profiles continuously in the torque space. HMC-Controller facilitates both teleoperation and policy deployment. Then, to learn a robust force-aware policy, we propose HMC-Policy to unify different controllers into a heterogeneous architecture. We adopt a mixture-of-experts style routing to learn from large-scale position-only data and fine-grained force-aware demonstrations. Experiments on a real humanoid robot show over 50% relative improvement vs. baselines on challenging tasks such as compliant table wiping and drawer opening, demonstrating the efficacy of HMC. 

**Abstract (ZH)**: 实时机器人演示学习在交互复杂现实环境中的前景：一种异构元控制（HMC）框架用于移动操作 

---
# HMC: Learning Heterogeneous Meta-Control for Contact-Rich Loco-Manipulation 

**Title (ZH)**: HMC：学习异质元控制以应对丰富接触的 locomo-manipulation 

**Authors**: Lai Wei, Xuanbin Peng, Ri-Zhao Qiu, Tianshu Huang, Xuxin Cheng, Xiaolong Wang  

**Link**: [PDF](https://arxiv.org/pdf/2511.14756)  

**Abstract**: Learning from real-world robot demonstrations holds promise for interacting with complex real-world environments. However, the complexity and variability of interaction dynamics often cause purely positional controllers to struggle with contacts or varying payloads. To address this, we propose a Heterogeneous Meta-Control (HMC) framework for Loco-Manipulation that adaptively stitches multiple control modalities: position, impedance, and hybrid force-position. We first introduce an interface, HMC-Controller, for blending actions from different control profiles continuously in the torque space. HMC-Controller facilitates both teleoperation and policy deployment. Then, to learn a robust force-aware policy, we propose HMC-Policy to unify different controllers into a heterogeneous architecture. We adopt a mixture-of-experts style routing to learn from large-scale position-only data and fine-grained force-aware demonstrations. Experiments on a real humanoid robot show over 50% relative improvement vs. baselines on challenging tasks such as compliant table wiping and drawer opening, demonstrating the efficacy of HMC. 

**Abstract (ZH)**: 基于真实世界机器人演示的学习为复杂现实环境交互提供了可能性。然而，交互动力学的复杂性和变异性往往导致仅位置控制在处理接触或变化负载时遇到困难。为解决这一问题，我们提出了一种异构元控制（HMC）框架，用于适应性地拼接多种控制模态：位置控制、阻抗控制以及混合力-位置控制。首先，我们引入了一个接口HMC-Controller，用于在扭矩空间中连续融合不同控制配置文件的动作。HMC-Controller 既支持远程操作又促进策略部署。然后，为了学习鲁棒的力感知策略，我们提出HMC-Policy来将不同控制器统一到一个异构架构中。我们采用专家混合风格的路由来从大规模仅位置数据和细粒度的力感知演示中进行学习。实验证实在诸如顺应性餐桌擦洗和抽屉开启等挑战性任务中，HMC相对于基线方法获得了超过50%的相对改进，证明了HMC的有效性。 

---
# HMC: Learning Heterogeneous Meta-Control for Contact-Rich Loco-Manipulation 

**Title (ZH)**: HMC: 学习异构元控制以应对丰富的接触式移动物体操作 

**Authors**: Lai Wei, Xuanbin Peng, Ri-Zhao Qiu, Tianshu Huang, Xuxin Cheng, Xiaolong Wang  

**Link**: [PDF](https://arxiv.org/pdf/2511.14756)  

**Abstract**: Learning from real-world robot demonstrations holds promise for interacting with complex real-world environments. However, the complexity and variability of interaction dynamics often cause purely positional controllers to struggle with contacts or varying payloads. To address this, we propose a Heterogeneous Meta-Control (HMC) framework for Loco-Manipulation that adaptively stitches multiple control modalities: position, impedance, and hybrid force-position. We first introduce an interface, HMC-Controller, for blending actions from different control profiles continuously in the torque space. HMC-Controller facilitates both teleoperation and policy deployment. Then, to learn a robust force-aware policy, we propose HMC-Policy to unify different controllers into a heterogeneous architecture. We adopt a mixture-of-experts style routing to learn from large-scale position-only data and fine-grained force-aware demonstrations. Experiments on a real humanoid robot show over 50% relative improvement vs. baselines on challenging tasks such as compliant table wiping and drawer opening, demonstrating the efficacy of HMC. 

**Abstract (ZH)**: 基于现实世界机器人演示的学习有潜力用于与复杂现实环境交互。然而，交互动力学的复杂性和变异性往往导致仅基于位置的控制器在处理接触或变化载荷时遇到困难。为了解决这一问题，我们提出了一种异构元控制（HMC）框架用于移动操作，该框架能够自适应地结合多种控制模式：位置、阻抗和混合力-位置控制。我们首先引入了一种接口，HMC-Controller，在力矩空间中连续混合不同控制配置文件的动作。HMC-Controller 既支持远程操作又便于策略部署。然后，为了学习一种稳健的力感知策略，我们提出了HMC-Policy来统一不同的控制器，形成一种异构架构。我们采用了专家混合风格的路由方式，从大规模仅位置数据和细粒度力感知演示中进行学习。在实际人形机器人上的实验结果显示，在诸如顺应桌面擦洗和抽屉打开等挑战性任务中，与基线相比，相对性能提升了超过50%，证明了HMC的有效性。 

---
# HMC: Learning Heterogeneous Meta-Control for Contact-Rich Loco-Manipulation 

**Title (ZH)**: HMC：学习异构元控制以应对丰富的接触式移动操作 

**Authors**: Lai Wei, Xuanbin Peng, Ri-Zhao Qiu, Tianshu Huang, Xuxin Cheng, Xiaolong Wang  

**Link**: [PDF](https://arxiv.org/pdf/2511.14756)  

**Abstract**: Learning from real-world robot demonstrations holds promise for interacting with complex real-world environments. However, the complexity and variability of interaction dynamics often cause purely positional controllers to struggle with contacts or varying payloads. To address this, we propose a Heterogeneous Meta-Control (HMC) framework for Loco-Manipulation that adaptively stitches multiple control modalities: position, impedance, and hybrid force-position. We first introduce an interface, HMC-Controller, for blending actions from different control profiles continuously in the torque space. HMC-Controller facilitates both teleoperation and policy deployment. Then, to learn a robust force-aware policy, we propose HMC-Policy to unify different controllers into a heterogeneous architecture. We adopt a mixture-of-experts style routing to learn from large-scale position-only data and fine-grained force-aware demonstrations. Experiments on a real humanoid robot show over 50% relative improvement vs. baselines on challenging tasks such as compliant table wiping and drawer opening, demonstrating the efficacy of HMC. 

**Abstract (ZH)**: 基于真实机器人演示的学习为处理复杂真实环境中的交互提供了 promise。然而，交互动力学的复杂性和变异性往往导致仅位置控制在处理接触或变化载荷时遇到困难。为了解决这一问题，我们提出了一种异构元控制（HMC）框架，用于适应性地组合多种控制模态：位置、阻抗和混合力-位置控制。我们首先引入了一个接口 HMC-Controller，用于在扭矩空间中连续混合不同控制配置文件下的动作。HMC-Controller 既支持远程操作，也支持策略部署。然后，为了学习一种稳健的力感知策略，我们提出了 HMC-Policy 以将不同控制器统一到一个异构架构中。我们采用专家混合样式路由来从大规模仅位置数据和细腻的力感知演示中学习。实验在真实的人形机器人上表明，在诸如顺应性桌子擦洗和抽屉打开等具有挑战性的任务上，HMC 相较于基线有超过 50% 的相对改进，这展示了 HMC 的有效性。 

---
# Robust Verification of Controllers under State Uncertainty via Hamilton-Jacobi Reachability Analysis 

**Title (ZH)**: 基于哈密尔顿-雅可比可达性分析的控制器在状态不确定性下的鲁棒验证 

**Authors**: Albert Lin, Alessandro Pinto, Somil Bansal  

**Link**: [PDF](https://arxiv.org/pdf/2511.14755)  

**Abstract**: As perception-based controllers for autonomous systems become increasingly popular in the real world, it is important that we can formally verify their safety and performance despite perceptual uncertainty. Unfortunately, the verification of such systems remains challenging, largely due to the complexity of the controllers, which are often nonlinear, nonconvex, learning-based, and/or black-box. Prior works propose verification algorithms that are based on approximate reachability methods, but they often restrict the class of controllers and systems that can be handled or result in overly conservative analyses. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for general nonlinear systems that can compute optimal reachable sets under worst-case system uncertainties; however, its application to perception-based systems is currently underexplored. In this work, we propose RoVer-CoRe, a framework for the Robust Verification of Controllers via HJ Reachability. To the best of our knowledge, RoVer-CoRe is the first HJ reachability-based framework for the verification of perception-based systems under perceptual uncertainty. Our key insight is to concatenate the system controller, observation function, and the state estimation modules to obtain an equivalent closed-loop system that is readily compatible with existing reachability frameworks. Within RoVer-CoRe, we propose novel methods for formal safety verification and robust controller design. We demonstrate the efficacy of the framework in case studies involving aircraft taxiing and NN-based rover navigation. Code is available at the link in the footnote. 

**Abstract (ZH)**: 基于感知的自主系统鲁棒验证框架：通过哈密尔顿-雅可比可达性分析 

---
# Robust Verification of Controllers under State Uncertainty via Hamilton-Jacobi Reachability Analysis 

**Title (ZH)**: 基于哈密尔顿-雅可比可达性分析的控制器在状态不确定性下的稳健验证 

**Authors**: Albert Lin, Alessandro Pinto, Somil Bansal  

**Link**: [PDF](https://arxiv.org/pdf/2511.14755)  

**Abstract**: As perception-based controllers for autonomous systems become increasingly popular in the real world, it is important that we can formally verify their safety and performance despite perceptual uncertainty. Unfortunately, the verification of such systems remains challenging, largely due to the complexity of the controllers, which are often nonlinear, nonconvex, learning-based, and/or black-box. Prior works propose verification algorithms that are based on approximate reachability methods, but they often restrict the class of controllers and systems that can be handled or result in overly conservative analyses. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for general nonlinear systems that can compute optimal reachable sets under worst-case system uncertainties; however, its application to perception-based systems is currently underexplored. In this work, we propose RoVer-CoRe, a framework for the Robust Verification of Controllers via HJ Reachability. To the best of our knowledge, RoVer-CoRe is the first HJ reachability-based framework for the verification of perception-based systems under perceptual uncertainty. Our key insight is to concatenate the system controller, observation function, and the state estimation modules to obtain an equivalent closed-loop system that is readily compatible with existing reachability frameworks. Within RoVer-CoRe, we propose novel methods for formal safety verification and robust controller design. We demonstrate the efficacy of the framework in case studies involving aircraft taxiing and NN-based rover navigation. Code is available at the link in the footnote. 

**Abstract (ZH)**: 基于感知的自主系统控制器的鲁棒验证框架：Hamilton-Jacobi可达性分析方法 

---
# Robust Verification of Controllers under State Uncertainty via Hamilton-Jacobi Reachability Analysis 

**Title (ZH)**: 基于哈密尔顿-雅可比可达性分析的控制器在状态不确定性下的鲁棒验证 

**Authors**: Albert Lin, Alessandro Pinto, Somil Bansal  

**Link**: [PDF](https://arxiv.org/pdf/2511.14755)  

**Abstract**: As perception-based controllers for autonomous systems become increasingly popular in the real world, it is important that we can formally verify their safety and performance despite perceptual uncertainty. Unfortunately, the verification of such systems remains challenging, largely due to the complexity of the controllers, which are often nonlinear, nonconvex, learning-based, and/or black-box. Prior works propose verification algorithms that are based on approximate reachability methods, but they often restrict the class of controllers and systems that can be handled or result in overly conservative analyses. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for general nonlinear systems that can compute optimal reachable sets under worst-case system uncertainties; however, its application to perception-based systems is currently underexplored. In this work, we propose RoVer-CoRe, a framework for the Robust Verification of Controllers via HJ Reachability. To the best of our knowledge, RoVer-CoRe is the first HJ reachability-based framework for the verification of perception-based systems under perceptual uncertainty. Our key insight is to concatenate the system controller, observation function, and the state estimation modules to obtain an equivalent closed-loop system that is readily compatible with existing reachability frameworks. Within RoVer-CoRe, we propose novel methods for formal safety verification and robust controller design. We demonstrate the efficacy of the framework in case studies involving aircraft taxiing and NN-based rover navigation. Code is available at the link in the footnote. 

**Abstract (ZH)**: 基于感知的自主系统控制器的鲁棒验证：通过哈密尔顿-雅可比可达性分析 

---
# NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards 

**Title (ZH)**: NORA-1.5：一种使用世界模型和基于动作的偏好奖励训练的视觉-语言-行动模型 

**Authors**: Chia-Yu Hung, Navonil Majumder, Haoyuan Deng, Liu Renhang, Yankang Ang, Amir Zadeh, Chuan Li, Dorien Herremans, Ziwei Wang, Soujanya Poria  

**Link**: [PDF](https://arxiv.org/pdf/2511.14659)  

**Abstract**: Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment. 

**Abstract (ZH)**: 基于视觉-语言-行动的NORA-1.5模型：通过奖励导向的后训练提升鲁棒性与任务成功率 

---
# Robust Verification of Controllers under State Uncertainty via Hamilton-Jacobi Reachability Analysis 

**Title (ZH)**: 基于Hamilton-Jacobi可达性分析的控制器在状态不确定性下的健壮验证 

**Authors**: Albert Lin, Alessandro Pinto, Somil Bansal  

**Link**: [PDF](https://arxiv.org/pdf/2511.14755)  

**Abstract**: As perception-based controllers for autonomous systems become increasingly popular in the real world, it is important that we can formally verify their safety and performance despite perceptual uncertainty. Unfortunately, the verification of such systems remains challenging, largely due to the complexity of the controllers, which are often nonlinear, nonconvex, learning-based, and/or black-box. Prior works propose verification algorithms that are based on approximate reachability methods, but they often restrict the class of controllers and systems that can be handled or result in overly conservative analyses. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for general nonlinear systems that can compute optimal reachable sets under worst-case system uncertainties; however, its application to perception-based systems is currently underexplored. In this work, we propose RoVer-CoRe, a framework for the Robust Verification of Controllers via HJ Reachability. To the best of our knowledge, RoVer-CoRe is the first HJ reachability-based framework for the verification of perception-based systems under perceptual uncertainty. Our key insight is to concatenate the system controller, observation function, and the state estimation modules to obtain an equivalent closed-loop system that is readily compatible with existing reachability frameworks. Within RoVer-CoRe, we propose novel methods for formal safety verification and robust controller design. We demonstrate the efficacy of the framework in case studies involving aircraft taxiing and NN-based rover navigation. Code is available at the link in the footnote. 

**Abstract (ZH)**: 基于感知的自主系统鲁棒验证框架：通过哈密尔顿-雅各比可达性分析 

---
# NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards 

**Title (ZH)**: NORA-1.5: 一种使用世界模型和基于行动的偏好奖励训练的视觉-语言-行动模型 

**Authors**: Chia-Yu Hung, Navonil Majumder, Haoyuan Deng, Liu Renhang, Yankang Ang, Amir Zadeh, Chuan Li, Dorien Herremans, Ziwei Wang, Soujanya Poria  

**Link**: [PDF](https://arxiv.org/pdf/2511.14659)  

**Abstract**: Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment. 

**Abstract (ZH)**: 基于视觉-语言-行动的NORA-1.5模型：通过奖励引导的后训练增强实现可靠的体素代理 

---
# NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards 

**Title (ZH)**: NORA-1.5：一种使用世界模型和动作偏好奖励训练的视觉-语言-行动模型 

**Authors**: Chia-Yu Hung, Navonil Majumder, Haoyuan Deng, Liu Renhang, Yankang Ang, Amir Zadeh, Chuan Li, Dorien Herremans, Ziwei Wang, Soujanya Poria  

**Link**: [PDF](https://arxiv.org/pdf/2511.14659)  

**Abstract**: Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment. 

**Abstract (ZH)**: Vision-Language-Action (VLA) 模型在多种具身任务中已经展示了令人鼓舞的性能，但在可靠性和泛化能力方面仍有不足，尤其是在跨不同具身体系或现实环境部署时。本文介绍了 NORA-1.5，这是一种基于预训练 NORA 主干构建的 VLA 模型，并通过添加基于流动匹配的动作专家进行增强。这种架构上的改进本身就带来了显著的性能提升，使 NORA-1.5 能够在模拟和现实世界基准测试中均超越 NORA 和多项最新 VLA 模型。为了进一步提高鲁棒性和任务成功率，我们开发了一套用于后训练 VLA 策略的奖励模型。我们的奖励模型结合了 (i) 动作条件的世界模型 (WM)，评估生成的动作是否能朝着目标发展，以及 (ii) 从错误中区分良动作和差动作的经验法则。利用这些奖励信号，我们构建了偏好数据集，并通过直接偏好优化 (DPO) 调整 NORA-1.5，使其针对特定的具身体系。广泛的评估表明，通过简单的有效奖励模型驱动的后训练奖励能够持续提升模拟和真实机器人设置中的性能，显著提高了 VLA 模型的可靠性。我们的研究结果强调了 NORA-1.5 和奖励引导的后训练作为一种可行路径，以开发更可靠的具身代理，适用于现实世界的部署。 

---
# Gallant: Voxel Grid-based Humanoid Locomotion and Local-navigation across 3D Constrained Terrains 

**Title (ZH)**: Gallant: 基于体素网格的人形移动与3D约束地形中的局部导航 

**Authors**: Qingwei Ben, Botian Xu, Kailin Li, Feiyu Jia, Wentao Zhang, Jingping Wang, Jingbo Wang, Dahua Lin, Jiangmiao Pang  

**Link**: [PDF](https://arxiv.org/pdf/2511.14625)  

**Abstract**: Robust humanoid locomotion requires accurate and globally consistent perception of the surrounding 3D environment. However, existing perception modules, mainly based on depth images or elevation maps, offer only partial and locally flattened views of the environment, failing to capture the full 3D structure. This paper presents Gallant, a voxel-grid-based framework for humanoid locomotion and local navigation in 3D constrained terrains. It leverages voxelized LiDAR data as a lightweight and structured perceptual representation, and employs a z-grouped 2D CNN to map this representation to the control policy, enabling fully end-to-end optimization. A high-fidelity LiDAR simulation that dynamically generates realistic observations is developed to support scalable, LiDAR-based training and ensure sim-to-real consistency. Experimental results show that Gallant's broader perceptual coverage facilitates the use of a single policy that goes beyond the limitations of previous methods confined to ground-level obstacles, extending to lateral clutter, overhead constraints, multi-level structures, and narrow passages. Gallant also firstly achieves near 100% success rates in challenging scenarios such as stair climbing and stepping onto elevated platforms through improved end-to-end optimization. 

**Abstract (ZH)**: Humanoid行走在3D受限地形中的稳健运动需要准确且全局一致的周围三维环境感知。现有基于深度图像或高程图的感知模块仅提供环境的部分且局部扁平化视图，无法捕捉完整的三维结构。本文提出了一种基于体素网格的Gallant框架，用于 humanoid 在3D受限地形中的行进与局部导航。该框架利用体素化LiDAR数据作为轻量级且结构化的感知表示，并采用按z分组的2D CNN将该表示映射到控制策略，从而实现端到端优化的完全优化。开发了一种高保真LiDAR仿真，可以动态生成真实观察结果，以支持可扩展的基于LiDAR的训练并确保仿真到现实的一致性。实验结果表明，Gallant更广泛的感知覆盖范围使得使用单一策略成为可能，该策略超越了以往仅限于地面上障碍物的限制，扩展到侧向杂乱、上方约束、多层结构和狭窄通道。Gallant还首次通过改进的端到端优化，在楼梯攀爬和踏上升高平台等具有挑战性的场景中实现了接近100%的成功率。 

---
# NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards 

**Title (ZH)**: NORA-1.5：一种使用世界模型和基于行动的偏好奖励训练的视觉-语言-行动模型 

**Authors**: Chia-Yu Hung, Navonil Majumder, Haoyuan Deng, Liu Renhang, Yankang Ang, Amir Zadeh, Chuan Li, Dorien Herremans, Ziwei Wang, Soujanya Poria  

**Link**: [PDF](https://arxiv.org/pdf/2511.14659)  

**Abstract**: Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment. 

**Abstract (ZH)**: Vision-语言-动作(VLA)模型在多种具身任务中展示了有希望的性能，但仍存在可靠性和泛化能力不足的问题，特别是在跨不同具身形式或现实环境部署时。在本工作中，我们引入了NORA-1.5，这是一种基于预训练NORA骨干构建的VLA模型，并在其上添加了一个基于流匹配的动作专家。这种架构增强本身带来了显著的性能提升，使得NORA-1.5在模拟和真实环境基准测试中均优于NORA和多个最先进的VLA模型。为了进一步提高鲁棒性和任务成功率，我们开发了一套用于后训练VLA策略的奖励模型。我们的奖励组合了(i)一种动作条件下的世界模型(WM)，评估生成的动作是否朝向目标，并结合(ii)一种偏离真实值的经验法则，以区分好的动作和差的动作。利用这些奖励信号，我们构建了偏好数据集，并通过直接偏好优化(DPO)将NORA-1.5适应特定的具身形式。广泛的评估表明，奖励驱动的后训练在仿真和真实机器人环境中都能一致地提升性能，通过简单的有效奖励模型显著提高了VLA模型的可靠性。我们的研究结果强调了NORA-1.5和奖励导向的后训练作为构建更加可靠且适用于实际部署的具身代理的有效途径。 

---
# Gallant: Voxel Grid-based Humanoid Locomotion and Local-navigation across 3D Constrained Terrains 

**Title (ZH)**: Gallant: 基于体素网格的类人机器人在3D受限地形上的运动与局部导航 

**Authors**: Qingwei Ben, Botian Xu, Kailin Li, Feiyu Jia, Wentao Zhang, Jingping Wang, Jingbo Wang, Dahua Lin, Jiangmiao Pang  

**Link**: [PDF](https://arxiv.org/pdf/2511.14625)  

**Abstract**: Robust humanoid locomotion requires accurate and globally consistent perception of the surrounding 3D environment. However, existing perception modules, mainly based on depth images or elevation maps, offer only partial and locally flattened views of the environment, failing to capture the full 3D structure. This paper presents Gallant, a voxel-grid-based framework for humanoid locomotion and local navigation in 3D constrained terrains. It leverages voxelized LiDAR data as a lightweight and structured perceptual representation, and employs a z-grouped 2D CNN to map this representation to the control policy, enabling fully end-to-end optimization. A high-fidelity LiDAR simulation that dynamically generates realistic observations is developed to support scalable, LiDAR-based training and ensure sim-to-real consistency. Experimental results show that Gallant's broader perceptual coverage facilitates the use of a single policy that goes beyond the limitations of previous methods confined to ground-level obstacles, extending to lateral clutter, overhead constraints, multi-level structures, and narrow passages. Gallant also firstly achieves near 100% success rates in challenging scenarios such as stair climbing and stepping onto elevated platforms through improved end-to-end optimization. 

**Abstract (ZH)**: 鲁棒的人形步行需要准确且全局一致的周围3D环境感知。然而，现有的感知模块主要基于深度图像或高程图，仅提供环境的局部扁平化视图，无法捕捉完整的3D结构。本文提出Gallant，一种基于体素网格的框架，用于3D受限地形上的人形步行和局部导航。该框架利用体素化LiDAR数据作为轻量级且结构化的感知表示，并采用z分组的2D CNN将其映射到控制策略，从而实现端到端优化的完全优化。开发了一种高保真LiDAR仿真，动态生成逼真观察，支持可扩展的、基于LiDAR的训练，并确保模拟到现实的一致性。实验结果表明，Gallant更广阔的感知覆盖范围使得使用单一策略成为可能，该策略超越了先前仅限于地面障碍的方法限制，扩展到侧向杂乱、上方限制、多层结构和狭窄通道等。此外，Gallant首次通过改进的端到端优化，在诸如上楼梯和踏上高台等具有挑战性的场景中实现了接近100%的成功率。 

---
# Gallant: Voxel Grid-based Humanoid Locomotion and Local-navigation across 3D Constrained Terrains 

**Title (ZH)**: Gallant: 基于体素网格的类人三维受限地形行进与局部导航 

**Authors**: Qingwei Ben, Botian Xu, Kailin Li, Feiyu Jia, Wentao Zhang, Jingping Wang, Jingbo Wang, Dahua Lin, Jiangmiao Pang  

**Link**: [PDF](https://arxiv.org/pdf/2511.14625)  

**Abstract**: Robust humanoid locomotion requires accurate and globally consistent perception of the surrounding 3D environment. However, existing perception modules, mainly based on depth images or elevation maps, offer only partial and locally flattened views of the environment, failing to capture the full 3D structure. This paper presents Gallant, a voxel-grid-based framework for humanoid locomotion and local navigation in 3D constrained terrains. It leverages voxelized LiDAR data as a lightweight and structured perceptual representation, and employs a z-grouped 2D CNN to map this representation to the control policy, enabling fully end-to-end optimization. A high-fidelity LiDAR simulation that dynamically generates realistic observations is developed to support scalable, LiDAR-based training and ensure sim-to-real consistency. Experimental results show that Gallant's broader perceptual coverage facilitates the use of a single policy that goes beyond the limitations of previous methods confined to ground-level obstacles, extending to lateral clutter, overhead constraints, multi-level structures, and narrow passages. Gallant also firstly achieves near 100% success rates in challenging scenarios such as stair climbing and stepping onto elevated platforms through improved end-to-end optimization. 

**Abstract (ZH)**: Robust类人行走需要准确且全局一致的周围3D环境感知。然而，现有的感知模块主要基于深度图像或高程图，只能提供环境的局部扁平视图，无法捕获完整的3D结构。本文提出了Gallant，一种基于体素网格的类人行走和3D受限地形局部导航框架。该框架利用体素化LiDAR数据作为轻量级且结构化的感知表示，并采用按z分组的2D CNN将此表示映射到控制策略，从而实现完全端到端优化。开发了高保真LiDAR仿真，动态生成真实观察，支持可扩展的LiDAR基训练，并确保模拟到现实的一致性。实验结果表明，Gallant更广泛的感知覆盖范围使得使用单一策略成为可能，超越了局限于地面障碍的先前方法的限制，可以处理侧向杂乱、上方约束、多层结构和狭窄通道。Gallant首次通过改进的端到端优化，在stairs climbing和进入升高平台等具有挑战性的场景中实现了接近100%的成功率。 

---
# Gallant: Voxel Grid-based Humanoid Locomotion and Local-navigation across 3D Constrained Terrains 

**Title (ZH)**: Gallant: 基于体素网格的人形移动和3D受限地形中的局部导航 

**Authors**: Qingwei Ben, Botian Xu, Kailin Li, Feiyu Jia, Wentao Zhang, Jingping Wang, Jingbo Wang, Dahua Lin, Jiangmiao Pang  

**Link**: [PDF](https://arxiv.org/pdf/2511.14625)  

**Abstract**: Robust humanoid locomotion requires accurate and globally consistent perception of the surrounding 3D environment. However, existing perception modules, mainly based on depth images or elevation maps, offer only partial and locally flattened views of the environment, failing to capture the full 3D structure. This paper presents Gallant, a voxel-grid-based framework for humanoid locomotion and local navigation in 3D constrained terrains. It leverages voxelized LiDAR data as a lightweight and structured perceptual representation, and employs a z-grouped 2D CNN to map this representation to the control policy, enabling fully end-to-end optimization. A high-fidelity LiDAR simulation that dynamically generates realistic observations is developed to support scalable, LiDAR-based training and ensure sim-to-real consistency. Experimental results show that Gallant's broader perceptual coverage facilitates the use of a single policy that goes beyond the limitations of previous methods confined to ground-level obstacles, extending to lateral clutter, overhead constraints, multi-level structures, and narrow passages. Gallant also firstly achieves near 100% success rates in challenging scenarios such as stair climbing and stepping onto elevated platforms through improved end-to-end optimization. 

**Abstract (ZH)**: 基于体素网格的鲁棒类人三维环境感知框架：用于三维受限制地形的类人行动与局部导航 

---
# Is Your VLM for Autonomous Driving Safety-Ready? A Comprehensive Benchmark for Evaluating External and In-Cabin Risks 

**Title (ZH)**: 你的VLM用于自动驾驶安全可靠吗？一个全面的风险评估基准（评估外部和车内风险） 

**Authors**: Xianhui Meng, Yuchen Zhang, Zhijian Huang, Zheng Lu, Ziling Ji, Yaoyao Yin, Hongyuan Zhang, Guangfeng Jiang, Yandan Lin, Long Chen, Hangjun Ye, Li Zhang, Jun Liu, Xiaoshuai Hao  

**Link**: [PDF](https://arxiv.org/pdf/2511.14592)  

**Abstract**: Vision-Language Models (VLMs) show great promise for autonomous driving, but their suitability for safety-critical scenarios is largely unexplored, raising safety concerns. This issue arises from the lack of comprehensive benchmarks that assess both external environmental risks and in-cabin driving behavior safety simultaneously. To bridge this critical gap, we introduce DSBench, the first comprehensive Driving Safety Benchmark designed to assess a VLM's awareness of various safety risks in a unified manner. DSBench encompasses two major categories: external environmental risks and in-cabin driving behavior safety, divided into 10 key categories and a total of 28 sub-categories. This comprehensive evaluation covers a wide range of scenarios, ensuring a thorough assessment of VLMs' performance in safety-critical contexts. Extensive evaluations across various mainstream open-source and closed-source VLMs reveal significant performance degradation under complex safety-critical situations, highlighting urgent safety concerns. To address this, we constructed a large dataset of 98K instances focused on in-cabin and external safety scenarios, showing that fine-tuning on this dataset significantly enhances the safety performance of existing VLMs and paves the way for advancing autonomous driving technology. The benchmark toolkit, code, and model checkpoints will be publicly accessible. 

**Abstract (ZH)**: 视觉-语言模型 (VLMs) 在自动驾驶领域展现出巨大潜力，但其在安全关键场景下的适用性尚未得到充分探索，引发了安全方面的担忧。为解决这一问题，我们引入了DSBench，这是首个全面的驾驶安全性基准，旨在以统一的方式评估VLM对各种安全风险的意识。DSBench 包含两大类：外部环境风险和车内驾驶行为安全性，涵盖10个关键类别和28个子类别。该基准测试涵盖了多种场景，确保对VLM在安全关键环境下的性能进行全面评估。对各种主流开源和闭源VLM的广泛评估显示，在复杂的安全关键情况下性能显著下降，突显了迫切的安全问题。为此，我们构建了一个包含98,000个实例的大规模数据集，专注于车内和外部安全场景，结果显示，在该数据集上进行微调显著提高了现有VLM的安全性能，并为推动自动驾驶技术的发展铺平了道路。基准测试工具包、代码和模型检查点将公开可用。 

---
# Is Your VLM for Autonomous Driving Safety-Ready? A Comprehensive Benchmark for Evaluating External and In-Cabin Risks 

**Title (ZH)**: 你的大规模视觉模型用于自动驾驶安全吗？一个全面的风险评估基准（外部和车内风险） 

**Authors**: Xianhui Meng, Yuchen Zhang, Zhijian Huang, Zheng Lu, Ziling Ji, Yaoyao Yin, Hongyuan Zhang, Guangfeng Jiang, Yandan Lin, Long Chen, Hangjun Ye, Li Zhang, Jun Liu, Xiaoshuai Hao  

**Link**: [PDF](https://arxiv.org/pdf/2511.14592)  

**Abstract**: Vision-Language Models (VLMs) show great promise for autonomous driving, but their suitability for safety-critical scenarios is largely unexplored, raising safety concerns. This issue arises from the lack of comprehensive benchmarks that assess both external environmental risks and in-cabin driving behavior safety simultaneously. To bridge this critical gap, we introduce DSBench, the first comprehensive Driving Safety Benchmark designed to assess a VLM's awareness of various safety risks in a unified manner. DSBench encompasses two major categories: external environmental risks and in-cabin driving behavior safety, divided into 10 key categories and a total of 28 sub-categories. This comprehensive evaluation covers a wide range of scenarios, ensuring a thorough assessment of VLMs' performance in safety-critical contexts. Extensive evaluations across various mainstream open-source and closed-source VLMs reveal significant performance degradation under complex safety-critical situations, highlighting urgent safety concerns. To address this, we constructed a large dataset of 98K instances focused on in-cabin and external safety scenarios, showing that fine-tuning on this dataset significantly enhances the safety performance of existing VLMs and paves the way for advancing autonomous driving technology. The benchmark toolkit, code, and model checkpoints will be publicly accessible. 

**Abstract (ZH)**: Vision-Language模型（VLMs）在自动驾驶领域展现出巨大的潜力，但在安全关键场景中的适用性尚未得到充分探索，引发了安全方面的担忧。为解决这一问题，我们引入了DSBench，这是第一个综合驾驶安全基准，旨在以统一的方式评估VLM对各种安全风险的意识。DSBench 包含两大类：外部环境风险和车内驾驶行为安全，分为10个关键类别和28个子类别。该全面评估涵盖了广泛的场景，确保对VLM在安全关键环境下的表现进行全面评估。对多种主流开源和闭源VLM的广泛评估表明，在复杂的安全关键情况下存在显著的性能下降，突显了迫切的安全问题。为此，我们构建了一个包含98K实例的大规模数据集，专注于车内和外部安全场景，显示通过在此数据集上进行微调可以显著提高现有VLM的安全性能，并为推进自动驾驶技术铺平道路。该基准工具包、代码和模型检查点将公开发布。 

---
# Is Your VLM for Autonomous Driving Safety-Ready? A Comprehensive Benchmark for Evaluating External and In-Cabin Risks 

**Title (ZH)**: 你的VLM用于自动驾驶安全可靠吗？一个全面的评估外部和车内风险基准 

**Authors**: Xianhui Meng, Yuchen Zhang, Zhijian Huang, Zheng Lu, Ziling Ji, Yaoyao Yin, Hongyuan Zhang, Guangfeng Jiang, Yandan Lin, Long Chen, Hangjun Ye, Li Zhang, Jun Liu, Xiaoshuai Hao  

**Link**: [PDF](https://arxiv.org/pdf/2511.14592)  

**Abstract**: Vision-Language Models (VLMs) show great promise for autonomous driving, but their suitability for safety-critical scenarios is largely unexplored, raising safety concerns. This issue arises from the lack of comprehensive benchmarks that assess both external environmental risks and in-cabin driving behavior safety simultaneously. To bridge this critical gap, we introduce DSBench, the first comprehensive Driving Safety Benchmark designed to assess a VLM's awareness of various safety risks in a unified manner. DSBench encompasses two major categories: external environmental risks and in-cabin driving behavior safety, divided into 10 key categories and a total of 28 sub-categories. This comprehensive evaluation covers a wide range of scenarios, ensuring a thorough assessment of VLMs' performance in safety-critical contexts. Extensive evaluations across various mainstream open-source and closed-source VLMs reveal significant performance degradation under complex safety-critical situations, highlighting urgent safety concerns. To address this, we constructed a large dataset of 98K instances focused on in-cabin and external safety scenarios, showing that fine-tuning on this dataset significantly enhances the safety performance of existing VLMs and paves the way for advancing autonomous driving technology. The benchmark toolkit, code, and model checkpoints will be publicly accessible. 

**Abstract (ZH)**: 基于视觉-语言模型的驾驶安全基准 (DSBench): 评估自主驾驶中的各种安全风险 

---
# Is Your VLM for Autonomous Driving Safety-Ready? A Comprehensive Benchmark for Evaluating External and In-Cabin Risks 

**Title (ZH)**: 你的VLM用于自动驾驶安全可靠吗？一种全面的评估外部和车内风险的标准。 

**Authors**: Xianhui Meng, Yuchen Zhang, Zhijian Huang, Zheng Lu, Ziling Ji, Yaoyao Yin, Hongyuan Zhang, Guangfeng Jiang, Yandan Lin, Long Chen, Hangjun Ye, Li Zhang, Jun Liu, Xiaoshuai Hao  

**Link**: [PDF](https://arxiv.org/pdf/2511.14592)  

**Abstract**: Vision-Language Models (VLMs) show great promise for autonomous driving, but their suitability for safety-critical scenarios is largely unexplored, raising safety concerns. This issue arises from the lack of comprehensive benchmarks that assess both external environmental risks and in-cabin driving behavior safety simultaneously. To bridge this critical gap, we introduce DSBench, the first comprehensive Driving Safety Benchmark designed to assess a VLM's awareness of various safety risks in a unified manner. DSBench encompasses two major categories: external environmental risks and in-cabin driving behavior safety, divided into 10 key categories and a total of 28 sub-categories. This comprehensive evaluation covers a wide range of scenarios, ensuring a thorough assessment of VLMs' performance in safety-critical contexts. Extensive evaluations across various mainstream open-source and closed-source VLMs reveal significant performance degradation under complex safety-critical situations, highlighting urgent safety concerns. To address this, we constructed a large dataset of 98K instances focused on in-cabin and external safety scenarios, showing that fine-tuning on this dataset significantly enhances the safety performance of existing VLMs and paves the way for advancing autonomous driving technology. The benchmark toolkit, code, and model checkpoints will be publicly accessible. 

**Abstract (ZH)**: 基于视觉-语言模型的驾驶安全基准 (DSBench):全面评估自主驾驶中的安全风险 

---
# Masked IRL: LLM-Guided Reward Disambiguation from Demonstrations and Language 

**Title (ZH)**: Masked IRL: LLM-Guided Reward Disambiguation from Demonstrations and Language 

**Authors**: Minyoung Hwang, Alexandra Forsey-Smerek, Nathaniel Dennler, Andreea Bobu  

**Link**: [PDF](https://arxiv.org/pdf/2511.14565)  

**Abstract**: Robots can adapt to user preferences by learning reward functions from demonstrations, but with limited data, reward models often overfit to spurious correlations and fail to generalize. This happens because demonstrations show robots how to do a task but not what matters for that task, causing the model to focus on irrelevant state details. Natural language can more directly specify what the robot should focus on, and, in principle, disambiguate between many reward functions consistent with the demonstrations. However, existing language-conditioned reward learning methods typically treat instructions as simple conditioning signals, without fully exploiting their potential to resolve ambiguity. Moreover, real instructions are often ambiguous themselves, so naive conditioning is unreliable. Our key insight is that these two input types carry complementary information: demonstrations show how to act, while language specifies what is important. We propose Masked Inverse Reinforcement Learning (Masked IRL), a framework that uses large language models (LLMs) to combine the strengths of both input types. Masked IRL infers state-relevance masks from language instructions and enforces invariance to irrelevant state components. When instructions are ambiguous, it uses LLM reasoning to clarify them in the context of the demonstrations. In simulation and on a real robot, Masked IRL outperforms prior language-conditioned IRL methods by up to 15% while using up to 4.7 times less data, demonstrating improved sample-efficiency, generalization, and robustness to ambiguous language. Project page: this https URL and Code: this https URL 

**Abstract (ZH)**: 机器人可以通过从示范中学习奖励函数来适应用户偏好，但在数据有限的情况下，奖励模型往往会过度拟合到虚假的相关性并失败于泛化。这发生的原因是示范展现了机器人完成任务的方法，但没有说明任务的关键因素，导致模型聚焦于无关的状态细节。自然语言可以更直接地指明机器人应关注的内容，并在原则上区分多种与示范一致的奖励函数。然而，现有的基于语言条件的奖励学习方法通常将指令视为简单的条件信号，而未能充分利用其解决歧义的潜力。此外，真实的指令本身往往也是模棱两可的，因此简单的条件化是不可靠的。我们的关键见解是这两种输入类型携带互补的信息：示范展示了如何行动，而语言指明了什么是重要的。我们提出了一种掩码逆向强化学习（Masked IRL）框架，该框架利用大型语言模型（LLMs）结合这两种输入类型的优点。掩码逆向强化学习从语言指令中推断状态相关掩码，并确保对无关状态组件的不变性。当指令存在歧义时，它使用LLM推理在示范的上下文中澄清指令。在模拟和实际机器人中，掩码逆向强化学习在使用最多4.7倍少的数据情况下，相比之前的基于语言条件的逆向强化学习方法性能高出最多15%，展示了更好的样本效率、泛化能力和对模棱两可语言的鲁棒性。项目页面: this https URL 代码: this https URL 

---
# Masked IRL: LLM-Guided Reward Disambiguation from Demonstrations and Language 

**Title (ZH)**: 掩码IRL：由示范和语言指导的奖励消歧译codegen 

**Authors**: Minyoung Hwang, Alexandra Forsey-Smerek, Nathaniel Dennler, Andreea Bobu  

**Link**: [PDF](https://arxiv.org/pdf/2511.14565)  

**Abstract**: Robots can adapt to user preferences by learning reward functions from demonstrations, but with limited data, reward models often overfit to spurious correlations and fail to generalize. This happens because demonstrations show robots how to do a task but not what matters for that task, causing the model to focus on irrelevant state details. Natural language can more directly specify what the robot should focus on, and, in principle, disambiguate between many reward functions consistent with the demonstrations. However, existing language-conditioned reward learning methods typically treat instructions as simple conditioning signals, without fully exploiting their potential to resolve ambiguity. Moreover, real instructions are often ambiguous themselves, so naive conditioning is unreliable. Our key insight is that these two input types carry complementary information: demonstrations show how to act, while language specifies what is important. We propose Masked Inverse Reinforcement Learning (Masked IRL), a framework that uses large language models (LLMs) to combine the strengths of both input types. Masked IRL infers state-relevance masks from language instructions and enforces invariance to irrelevant state components. When instructions are ambiguous, it uses LLM reasoning to clarify them in the context of the demonstrations. In simulation and on a real robot, Masked IRL outperforms prior language-conditioned IRL methods by up to 15% while using up to 4.7 times less data, demonstrating improved sample-efficiency, generalization, and robustness to ambiguous language. Project page: this https URL and Code: this https URL 

**Abstract (ZH)**: 机器人可以通过从演示学习奖励函数来适应用户偏好，但在数据有限的情况下，奖励模型往往会过拟合到虚假的相关性并无法泛化。这是因为演示展示了机器人如何完成任务，但并没有说明该任务的关键要素，导致模型专注于无关的状态细节。自然语言可以直接指定机器人应该关注什么，并且原则上可以澄清与演示一致的多种奖励函数之间的歧义。然而，现有的基于语言的奖励学习方法通常将指令视为简单的条件信号，未能充分利用其解决歧义的潜力。此外，实际的指令本身往往也是模糊的，因此简单的条件处理不可靠。我们的关键洞察是这两种输入类型携带互补的信息：演示展示了如何行动，而语言指定了什么是重要的。我们提出了屏蔽逆强化学习（Masked IRL）框架，该框架利用大型语言模型（LLMs）结合这两种输入类型的优势。屏蔽逆强化学习从语言指令中推断状态相关掩码，并确保对无关状态组件的不变性。当指令模糊时，它使用LLM推理在演示的背景下澄清它们。在模拟和实际机器人中，屏蔽逆强化学习在使用最多4.7倍少的数据情况下，相比之前的基于语言的逆强化学习方法性能高出15%，展示了更好的样本效率、泛化能力和对模糊语言的鲁棒性。项目页面: this https URL 代码: this https URL 

---
# Masked IRL: LLM-Guided Reward Disambiguation from Demonstrations and Language 

**Title (ZH)**: masked IRL: LLM 引导的奖励消歧从示范和语言引导来实现 

**Authors**: Minyoung Hwang, Alexandra Forsey-Smerek, Nathaniel Dennler, Andreea Bobu  

**Link**: [PDF](https://arxiv.org/pdf/2511.14565)  

**Abstract**: Robots can adapt to user preferences by learning reward functions from demonstrations, but with limited data, reward models often overfit to spurious correlations and fail to generalize. This happens because demonstrations show robots how to do a task but not what matters for that task, causing the model to focus on irrelevant state details. Natural language can more directly specify what the robot should focus on, and, in principle, disambiguate between many reward functions consistent with the demonstrations. However, existing language-conditioned reward learning methods typically treat instructions as simple conditioning signals, without fully exploiting their potential to resolve ambiguity. Moreover, real instructions are often ambiguous themselves, so naive conditioning is unreliable. Our key insight is that these two input types carry complementary information: demonstrations show how to act, while language specifies what is important. We propose Masked Inverse Reinforcement Learning (Masked IRL), a framework that uses large language models (LLMs) to combine the strengths of both input types. Masked IRL infers state-relevance masks from language instructions and enforces invariance to irrelevant state components. When instructions are ambiguous, it uses LLM reasoning to clarify them in the context of the demonstrations. In simulation and on a real robot, Masked IRL outperforms prior language-conditioned IRL methods by up to 15% while using up to 4.7 times less data, demonstrating improved sample-efficiency, generalization, and robustness to ambiguous language. Project page: this https URL and Code: this https URL 

**Abstract (ZH)**: 机器人可以通过学习演示中的奖励函数来适应用户偏好，但在数据有限的情况下，奖励模型往往会过度拟合到错误的相关性并丧失泛化能力。这发生是因为演示展示了机器人如何完成任务，但没有说明任务中重要的因素，导致模型关注无关的状态细节。自然语言可以直接指定机器人应关注的内容，并在原则上区分与演示一致的多种奖励函数。然而，现有的基于语言的奖励学习方法通常将指令视为简单的条件信号，未能充分利用其解决歧义的潜力。此外，实际指令本身往往模糊不清，所以简单的条件处理是不可靠的。我们的核心见解是这两种输入类型携带互补的信息：演示展示了如何行动，而语言指明了什么才是重要的。我们提出了掩码逆强化学习（Masked Inverse Reinforcement Learning, Masked IRL）框架，该框架利用大型语言模型（LLMs）结合这两种输入类型的优势。掩码逆强化学习从语言指令中推断状态相关性掩码，并确保对无关状态组件的不变性。当指令模糊时，它使用LLM推理在演示的背景下澄清这些指令。在仿真和实际机器人上，掩码逆强化学习比之前的基于语言的逆强化学习方法在数据使用量最多减少4.7倍的情况下性能最高提高15%，展示了更好的样本效率、泛化能力和对模糊语言的鲁棒性。 

---
# Masked IRL: LLM-Guided Reward Disambiguation from Demonstrations and Language 

**Title (ZH)**: 掩码IRL：由示范和语言引导的奖励歧义消解 

**Authors**: Minyoung Hwang, Alexandra Forsey-Smerek, Nathaniel Dennler, Andreea Bobu  

**Link**: [PDF](https://arxiv.org/pdf/2511.14565)  

**Abstract**: Robots can adapt to user preferences by learning reward functions from demonstrations, but with limited data, reward models often overfit to spurious correlations and fail to generalize. This happens because demonstrations show robots how to do a task but not what matters for that task, causing the model to focus on irrelevant state details. Natural language can more directly specify what the robot should focus on, and, in principle, disambiguate between many reward functions consistent with the demonstrations. However, existing language-conditioned reward learning methods typically treat instructions as simple conditioning signals, without fully exploiting their potential to resolve ambiguity. Moreover, real instructions are often ambiguous themselves, so naive conditioning is unreliable. Our key insight is that these two input types carry complementary information: demonstrations show how to act, while language specifies what is important. We propose Masked Inverse Reinforcement Learning (Masked IRL), a framework that uses large language models (LLMs) to combine the strengths of both input types. Masked IRL infers state-relevance masks from language instructions and enforces invariance to irrelevant state components. When instructions are ambiguous, it uses LLM reasoning to clarify them in the context of the demonstrations. In simulation and on a real robot, Masked IRL outperforms prior language-conditioned IRL methods by up to 15% while using up to 4.7 times less data, demonstrating improved sample-efficiency, generalization, and robustness to ambiguous language. Project page: this https URL and Code: this https URL 

**Abstract (ZH)**: 基于掩码逆强化学习的奖励函数学习：利用语言指令提升机器人适应用户偏好能力 

---
# Aerial Assistance System for Automated Firefighting during Turntable Ladder Operations 

**Title (ZH)**: 自动灭火的回转梯架操作空中辅助系统 

**Authors**: Jan Quenzel, Valerij Sekin, Daniel Schleich, Alexander Miller, Merlin Stampa, Norbert Pahlke, Christof Röhrig, Sven Behnke  

**Link**: [PDF](https://arxiv.org/pdf/2511.14504)  

**Abstract**: Fires in industrial facilities pose special challenges to firefighters, e.g., due to the sheer size and scale of the buildings. The resulting visual obstructions impair firefighting accuracy, further compounded by inaccurate assessments of the fire's location. Such imprecision simultaneously increases the overall damage and prolongs the fire-brigades operation unnecessarily.
We propose an automated assistance system for firefighting using a motorized fire monitor on a turntable ladder with aerial support from an unmanned aerial vehicle (UAV). The UAV flies autonomously within an obstacle-free flight funnel derived from geodata, detecting and localizing heat sources. An operator supervises the operation on a handheld controller and selects a fire target in reach. After the selection, the UAV automatically plans and traverses between two triangulation poses for continued fire localization. Simultaneously, our system steers the fire monitor to ensure the water jet reaches the detected heat source. In preliminary tests, our assistance system successfully localized multiple heat sources and directed a water jet towards the fires. 

**Abstract (ZH)**: 工业设施火灾对消防员构成特殊挑战，例如，由于建筑的规模庞大。由此产生的视觉障碍削弱了灭火的准确性，进一步因对火源位置的不准确评估而加剧。这种不精确性不仅增加了总体损失，还无谓地延长了消防队的作业时间。
我们提出了一种使用可移动灭火器的机械化消防辅助系统，该系统依托回转梯架，并由无人直升机提供空中支持。无人直升机在由地理数据定义的无障碍飞行通道内自主飞行，检测并定位热源。操作员通过手持控制器监督操作，并选择可触及的火源。选定后，无人直升机自动规划并穿越两个三角定位姿态，以继续进行火源定位。同时，我们的系统操控灭火器确保水柱射向检测到的热源。初步测试表明，我们的辅助系统成功定位了多个热源，并将水柱导向火灾。 

---
# Aerial Assistance System for Automated Firefighting during Turntable Ladder Operations 

**Title (ZH)**: 自动驾驶消防系统在回转梯架操作中的空中辅助系统 

**Authors**: Jan Quenzel, Valerij Sekin, Daniel Schleich, Alexander Miller, Merlin Stampa, Norbert Pahlke, Christof Röhrig, Sven Behnke  

**Link**: [PDF](https://arxiv.org/pdf/2511.14504)  

**Abstract**: Fires in industrial facilities pose special challenges to firefighters, e.g., due to the sheer size and scale of the buildings. The resulting visual obstructions impair firefighting accuracy, further compounded by inaccurate assessments of the fire's location. Such imprecision simultaneously increases the overall damage and prolongs the fire-brigades operation unnecessarily.
We propose an automated assistance system for firefighting using a motorized fire monitor on a turntable ladder with aerial support from an unmanned aerial vehicle (UAV). The UAV flies autonomously within an obstacle-free flight funnel derived from geodata, detecting and localizing heat sources. An operator supervises the operation on a handheld controller and selects a fire target in reach. After the selection, the UAV automatically plans and traverses between two triangulation poses for continued fire localization. Simultaneously, our system steers the fire monitor to ensure the water jet reaches the detected heat source. In preliminary tests, our assistance system successfully localized multiple heat sources and directed a water jet towards the fires. 

**Abstract (ZH)**: 工业设施火灾对消防员构成了特殊挑战，例如，由于建筑物的规模庞大。由此产生的视觉障碍妨碍了灭火的准确性，进一步加剧了对火灾位置的不准确评估。这种不精确同时增加了总体损失，并无谓地延长了消防队的作业时间。
我们提出了一种使用旋转消防梯上的自动化消防炮，并由无人机提供空中支持的消防辅助系统。无人机在基于地理数据的无障碍飞行通道内自主飞行，检测并定位热源。操作员在手持控制器上监督操作并选择可及的火源。选定后，无人机自动规划并穿越两个三角定位姿态以继续进行火灾定位。同时，我们的系统操控消防炮以确保水柱达到检测到的热源。初步测试显示，我们的辅助系统成功地定位了多个热源，并将水柱导向了火灾。 

---
# Aerial Assistance System for Automated Firefighting during Turntable Ladder Operations 

**Title (ZH)**: 自动灭火的旋转云梯作业空中辅助系统 

**Authors**: Jan Quenzel, Valerij Sekin, Daniel Schleich, Alexander Miller, Merlin Stampa, Norbert Pahlke, Christof Röhrig, Sven Behnke  

**Link**: [PDF](https://arxiv.org/pdf/2511.14504)  

**Abstract**: Fires in industrial facilities pose special challenges to firefighters, e.g., due to the sheer size and scale of the buildings. The resulting visual obstructions impair firefighting accuracy, further compounded by inaccurate assessments of the fire's location. Such imprecision simultaneously increases the overall damage and prolongs the fire-brigades operation unnecessarily.
We propose an automated assistance system for firefighting using a motorized fire monitor on a turntable ladder with aerial support from an unmanned aerial vehicle (UAV). The UAV flies autonomously within an obstacle-free flight funnel derived from geodata, detecting and localizing heat sources. An operator supervises the operation on a handheld controller and selects a fire target in reach. After the selection, the UAV automatically plans and traverses between two triangulation poses for continued fire localization. Simultaneously, our system steers the fire monitor to ensure the water jet reaches the detected heat source. In preliminary tests, our assistance system successfully localized multiple heat sources and directed a water jet towards the fires. 

**Abstract (ZH)**: 工业设施火灾对消防员构成特殊挑战，例如，由于建筑的巨大规模。由此产生的视觉障碍影响了灭火的准确性，进一步受到对火灾位置不准确评估的困扰。这种不精确同时增加了整体损失，并无谓地延长了消防队的操作时间。
我们提出了一种使用云梯上的机动灭火器，并由无人驾驶航空车辆（UAV）提供空中支持的自动化援助系统。UAV在由地理数据确定的无障碍飞行锥体中自主飞行，探测并定位热源。操作员在手持控制器上监督操作，并选择可触及的火源。选定后，UAV自动规划并穿越两个三角测量位置，以继续进行火灾定位。同时，我们的系统控制灭火器确保水柱达到探测到的热源。初步试验中，我们的援助系统成功地定位了多个热源，并将水柱引导至火灾处。 

---
# Aerial Assistance System for Automated Firefighting during Turntable Ladder Operations 

**Title (ZH)**: 旋转云梯操作期间自动灭火的 aerial 辅助系统 

**Authors**: Jan Quenzel, Valerij Sekin, Daniel Schleich, Alexander Miller, Merlin Stampa, Norbert Pahlke, Christof Röhrig, Sven Behnke  

**Link**: [PDF](https://arxiv.org/pdf/2511.14504)  

**Abstract**: Fires in industrial facilities pose special challenges to firefighters, e.g., due to the sheer size and scale of the buildings. The resulting visual obstructions impair firefighting accuracy, further compounded by inaccurate assessments of the fire's location. Such imprecision simultaneously increases the overall damage and prolongs the fire-brigades operation unnecessarily.
We propose an automated assistance system for firefighting using a motorized fire monitor on a turntable ladder with aerial support from an unmanned aerial vehicle (UAV). The UAV flies autonomously within an obstacle-free flight funnel derived from geodata, detecting and localizing heat sources. An operator supervises the operation on a handheld controller and selects a fire target in reach. After the selection, the UAV automatically plans and traverses between two triangulation poses for continued fire localization. Simultaneously, our system steers the fire monitor to ensure the water jet reaches the detected heat source. In preliminary tests, our assistance system successfully localized multiple heat sources and directed a water jet towards the fires. 

**Abstract (ZH)**: 工业设施中的火灾对消防员构成了特殊挑战，例如，由于建筑规模庞大。由此产生的视觉障碍影响了灭火的准确性，进一步由于对火灾位置的不准确评估而加剧。这种不精确同时增加了整体损失，并无谓地延长了消防队的行动时间。
我们提出了一种利用回转梯架上的机动消防炮，并由无人驾驶航空车辆（UAV）提供空中支持的自动化辅助系统，用于灭火。UAV在地面数据衍生的无障碍飞行漏斗内自主飞行，检测并定位热源。操作员在手持控制器上监督操作，并选择可触及的火源。选定后，UAV自动规划并穿越两个三角定位姿态以继续火源定位。同时，我们的系统操控消防炮以确保水柱达到检测到的热源。初步测试中，我们的辅助系统成功地定位了多个热源，并将水柱导向火灾。 

---
# Advancing Minimally Invasive Precision Surgery in Open Cavities with Robotic Flexible Endoscopy 

**Title (ZH)**: 在开放腔体中推进微创精准手术：基于机器人柔性内窥镜的技术应用 

**Authors**: Michelle Mattille, Alexandre Mesot, Miriam Weisskopf, Nicole Ochsenbein-Kölble, Ueli Moehrlen, Bradley J. Nelson, Quentin Boehler  

**Link**: [PDF](https://arxiv.org/pdf/2511.14458)  

**Abstract**: Flexible robots hold great promise for enhancing minimally invasive surgery (MIS) by providing superior dexterity, precise control, and safe tissue interaction. Yet, translating these advantages into endoscopic interventions within open cavities remains challenging. The lack of anatomical constraints and the inherent flexibility of such devices complicate their control, while the limited field of view of endoscopes restricts situational awareness. We present a robotic platform designed to overcome these challenges and demonstrate its potential in fetoscopic laser coagulation, a complex MIS procedure typically performed only by highly experienced surgeons. Our system combines a magnetically actuated flexible endoscope with teleoperated and semi-autonomous navigation capabilities for performing targeted laser ablations. To enhance surgical awareness, the platform reconstructs real-time mosaics of the endoscopic scene, providing an extended and continuous visual context. The ability of this system to address the key limitations of MIS in open spaces is validated in vivo in an ovine model. 

**Abstract (ZH)**: 柔性机器人在增强开放空腔内微创手术（MIS）中的潜力通过提供卓越的灵巧性、精确控制和安全的组织交互展现出来。然而，将这些优势转化为内镜干预依然充满挑战。缺乏解剖约束和此类设备的固有柔韧性使其控制复杂化，而内镜的有限视野限制了术中情况的认知。我们提出了一种机器人平台，旨在克服这些挑战，并展示了其在胎儿镜激光凝固中的潜力，这是一种通常仅由经验丰富的外科医生执行的复杂MIS程序。该系统结合了磁驱动的柔性内镜以及进行目标激光消融的遥操作和半自主导航能力。为了增强手术意识，该平台实时重建内镜场景的马赛克图，提供扩展和连续的视觉上下文。该系统在开放空间中解决MIS关键限制的能力在羊羔模型中进行了在 vivo 验证。 

---
# Advancing Minimally Invasive Precision Surgery in Open Cavities with Robotic Flexible Endoscopy 

**Title (ZH)**: 在开放腔体内推进微创精准手术的机器人柔性内镜技术 

**Authors**: Michelle Mattille, Alexandre Mesot, Miriam Weisskopf, Nicole Ochsenbein-Kölble, Ueli Moehrlen, Bradley J. Nelson, Quentin Boehler  

**Link**: [PDF](https://arxiv.org/pdf/2511.14458)  

**Abstract**: Flexible robots hold great promise for enhancing minimally invasive surgery (MIS) by providing superior dexterity, precise control, and safe tissue interaction. Yet, translating these advantages into endoscopic interventions within open cavities remains challenging. The lack of anatomical constraints and the inherent flexibility of such devices complicate their control, while the limited field of view of endoscopes restricts situational awareness. We present a robotic platform designed to overcome these challenges and demonstrate its potential in fetoscopic laser coagulation, a complex MIS procedure typically performed only by highly experienced surgeons. Our system combines a magnetically actuated flexible endoscope with teleoperated and semi-autonomous navigation capabilities for performing targeted laser ablations. To enhance surgical awareness, the platform reconstructs real-time mosaics of the endoscopic scene, providing an extended and continuous visual context. The ability of this system to address the key limitations of MIS in open spaces is validated in vivo in an ovine model. 

**Abstract (ZH)**: 柔性机器人在增强开放腔内微创手术中的广泛应用前景受到很大程度的期待，通过提供卓越的灵巧性、精确的控制以及安全的组织交互。然而，将这些优势转化为内窥镜干预仍然具有挑战性。由于缺乏解剖约束以及此类装置的固有柔韧性使其控制复杂化，而内窥镜的有限视野则限制了态势感知。我们提出了一种机器人平台，旨在克服这些挑战，并展示了其在胎儿镜激光光凝术中的潜力，这是一种通常仅由经验丰富的外科医生执行的复杂微创手术过程。该系统结合了磁驱动的柔性内窥镜以及遥控和半自主导航功能，以执行定向激光消融。为增强手术意识，该平台实时重建内窥镜场景的拼接图像，提供扩展的连续视觉环境。该系统的这项能力通过在羊羔模型中的体内验证，证明了其在开放空间中解决微创手术关键限制的潜力。 

---
# Advancing Minimally Invasive Precision Surgery in Open Cavities with Robotic Flexible Endoscopy 

**Title (ZH)**: 在开放腔体中推进微创精准手术的机器人柔性内窥镜技术 

**Authors**: Michelle Mattille, Alexandre Mesot, Miriam Weisskopf, Nicole Ochsenbein-Kölble, Ueli Moehrlen, Bradley J. Nelson, Quentin Boehler  

**Link**: [PDF](https://arxiv.org/pdf/2511.14458)  

**Abstract**: Flexible robots hold great promise for enhancing minimally invasive surgery (MIS) by providing superior dexterity, precise control, and safe tissue interaction. Yet, translating these advantages into endoscopic interventions within open cavities remains challenging. The lack of anatomical constraints and the inherent flexibility of such devices complicate their control, while the limited field of view of endoscopes restricts situational awareness. We present a robotic platform designed to overcome these challenges and demonstrate its potential in fetoscopic laser coagulation, a complex MIS procedure typically performed only by highly experienced surgeons. Our system combines a magnetically actuated flexible endoscope with teleoperated and semi-autonomous navigation capabilities for performing targeted laser ablations. To enhance surgical awareness, the platform reconstructs real-time mosaics of the endoscopic scene, providing an extended and continuous visual context. The ability of this system to address the key limitations of MIS in open spaces is validated in vivo in an ovine model. 

**Abstract (ZH)**: 柔顺机器人在增强开放空腔内的微创手术（MIS）中的灵活操作、精确控制和安全组织交互方面具有巨大潜力。然而，将这些优势转化为内镜干预仍然颇具挑战性。缺乏解剖约束和此类设备的内在柔韧性使其控制复杂化，而内镜视野的限制则限制了手术情境意识。我们提出了一种旨在克服这些挑战的机器人平台，并展示了其在胎儿内镜激光凝固术中的潜力，这是一种通常仅由经验丰富的外科医生执行的复杂MIS程序。该系统结合了磁驱动的柔顺内窥镜和远程操控及半自主导航能力，以执行精确的激光消融。为增强手术意识，该平台实时重建内镜场景的拼接图像，提供扩展和连续的视觉上下文。该系统的功能在活体羊模型中验证了其解决开放空间内MIS关键限制的能力。 

---
# Advancing Minimally Invasive Precision Surgery in Open Cavities with Robotic Flexible Endoscopy 

**Title (ZH)**: 在开放腔隙中推进微创精准手术的柔性内窥镜机器人技术 

**Authors**: Michelle Mattille, Alexandre Mesot, Miriam Weisskopf, Nicole Ochsenbein-Kölble, Ueli Moehrlen, Bradley J. Nelson, Quentin Boehler  

**Link**: [PDF](https://arxiv.org/pdf/2511.14458)  

**Abstract**: Flexible robots hold great promise for enhancing minimally invasive surgery (MIS) by providing superior dexterity, precise control, and safe tissue interaction. Yet, translating these advantages into endoscopic interventions within open cavities remains challenging. The lack of anatomical constraints and the inherent flexibility of such devices complicate their control, while the limited field of view of endoscopes restricts situational awareness. We present a robotic platform designed to overcome these challenges and demonstrate its potential in fetoscopic laser coagulation, a complex MIS procedure typically performed only by highly experienced surgeons. Our system combines a magnetically actuated flexible endoscope with teleoperated and semi-autonomous navigation capabilities for performing targeted laser ablations. To enhance surgical awareness, the platform reconstructs real-time mosaics of the endoscopic scene, providing an extended and continuous visual context. The ability of this system to address the key limitations of MIS in open spaces is validated in vivo in an ovine model. 

**Abstract (ZH)**: 柔性机器人在增强开放空腔内微创手术（MIS）中的潜力通过提供卓越的灵巧性、精确控制和安全的组织交互显示出巨大前景。然而，将这些优势转化为内窥镜干预仍然具有挑战性。缺乏解剖约束和此类设备的固有灵活性使其控制复杂化，而内窥镜的有限视野限制了情景意识。我们提出了一种机器人平台，旨在克服这些挑战，并展示了其在胎儿镜激光凝固术中的潜在应用，这是一种通常仅由经验丰富的外科医生执行的复杂MIS程序。该系统结合了磁控柔性内窥镜，并具备遥操作和半自主导航能力，用于执行目标激光消融。为增强外科手术意识，该平台实时重建内窥镜场景的马赛克图像，提供扩展且连续的视觉上下文。此系统在开放空间内解决MIS关键限制的能力，在羊模型中进行了体内验证。 

---
# Achieving Safe Control Online through Integration of Harmonic Control Lyapunov-Barrier Functions with Unsafe Object-Centric Action Policies 

**Title (ZH)**: 通过将谐波控制李雅普诺夫-屏障函数与危险对象中心的动作策略集成实现在线安全控制 

**Authors**: Marlow Fawn, Matthias Scheutz  

**Link**: [PDF](https://arxiv.org/pdf/2511.14434)  

**Abstract**: We propose a method for combining Harmonic Control Lyapunov-Barrier Functions (HCLBFs) derived from Signal Temporal Logic (STL) specifications with any given robot policy to turn an unsafe policy into a safe one with formal guarantees.  The two components are combined via HCLBF-derived safety certificates, thus producing commands that preserve both safety and task-driven behavior.  We demonstrate with a simple proof-of-concept implementation for an object-centric force-based policy trained through reinforcement learning for a movement task of a stationary robot arm that is able to avoid colliding with obstacles on a table top after combining the policy with the safety constraints.  The proposed method can be generalized to more complex specifications and dynamic task settings. 

**Abstract (ZH)**: 我们提出了一种将源自信号时序逻辑(STL)规范的谐波控制李雅普诺夫-障碍函数(HCLBFs)与任何给定的机器人策略相结合的方法，以将不安全的策略转换为带有形式保证的安全策略。通过HCLBF衍生的安全证书将两者结合，从而生成既能保持安全性又能保持任务驱动行为的命令。我们通过一个简单概念性的实现，展示了对于通过强化学习训练的一种基于力的对象中心策略，在结合安全性约束后，能够使静止机器人手臂在移动任务中避免与桌面障碍物相撞。所提出的方法可以推广到更复杂的规范和动态任务设置。 

---
# Achieving Safe Control Online through Integration of Harmonic Control Lyapunov-Barrier Functions with Unsafe Object-Centric Action Policies 

**Title (ZH)**: 通过将谐波控制Lyapunov-屏障函数与不安全对象中心的动作策略集成实现在线安全控制 

**Authors**: Marlow Fawn, Matthias Scheutz  

**Link**: [PDF](https://arxiv.org/pdf/2511.14434)  

**Abstract**: We propose a method for combining Harmonic Control Lyapunov-Barrier Functions (HCLBFs) derived from Signal Temporal Logic (STL) specifications with any given robot policy to turn an unsafe policy into a safe one with formal guarantees.  The two components are combined via HCLBF-derived safety certificates, thus producing commands that preserve both safety and task-driven behavior.  We demonstrate with a simple proof-of-concept implementation for an object-centric force-based policy trained through reinforcement learning for a movement task of a stationary robot arm that is able to avoid colliding with obstacles on a table top after combining the policy with the safety constraints.  The proposed method can be generalized to more complex specifications and dynamic task settings. 

**Abstract (ZH)**: 我们提出了一种将由信号时序逻辑(STL)规范衍生的谐波控制拉普拉斯-障碍函数(HCLBFs)与任何给定的机器人策略结合以将不安全策略转换为具有形式保证的安全策略的方法。通过使用HCLBF衍生的安全证书将这两个组件结合起来，从而产生同时保持安全性和任务驱动行为的命令。我们通过一个简单的概念验证实现，展示了一种基于强化学习训练的面向物体的力量策略，在结合安全约束后，能够使静止机器人臂在执行桌面上的运动任务时避免与障碍物相撞。该方法可以推广到更复杂的规范和动态任务设置中。 

---
# Achieving Safe Control Online through Integration of Harmonic Control Lyapunov-Barrier Functions with Unsafe Object-Centric Action Policies 

**Title (ZH)**: 通过将谐波控制李雅普诺夫-障碍函数与危险对象中心的动作策略集成实现在线安全控制 

**Authors**: Marlow Fawn, Matthias Scheutz  

**Link**: [PDF](https://arxiv.org/pdf/2511.14434)  

**Abstract**: We propose a method for combining Harmonic Control Lyapunov-Barrier Functions (HCLBFs) derived from Signal Temporal Logic (STL) specifications with any given robot policy to turn an unsafe policy into a safe one with formal guarantees.  The two components are combined via HCLBF-derived safety certificates, thus producing commands that preserve both safety and task-driven behavior.  We demonstrate with a simple proof-of-concept implementation for an object-centric force-based policy trained through reinforcement learning for a movement task of a stationary robot arm that is able to avoid colliding with obstacles on a table top after combining the policy with the safety constraints.  The proposed method can be generalized to more complex specifications and dynamic task settings. 

**Abstract (ZH)**: 我们提出了一种将信号时逻辑（STL）规范衍生的谐波控制临界-屏障函数（HCLBFs）与任何给定的机器人策略结合的方法，以正式保证的方式将不安全的策略转换为安全策略。通过HCLBF衍生的安全证书将两个组件结合起来，从而生成既保持安全性又驱动任务行为的指令。通过一个简单的概念验证实现，我们展示了该方法在通过强化学习训练的对象中心力基策略中的应用，该策略能够在结合安全约束后避免碰撞桌面上的障碍物。所提出的方法可以泛化应用于更复杂的规范和动态任务设置。 

---
# Achieving Safe Control Online through Integration of Harmonic Control Lyapunov-Barrier Functions with Unsafe Object-Centric Action Policies 

**Title (ZH)**: 通过结合谐波控制李雅普诺夫-障碍函数与不安全对象中心动作政策实现在线安全控制 

**Authors**: Marlow Fawn, Matthias Scheutz  

**Link**: [PDF](https://arxiv.org/pdf/2511.14434)  

**Abstract**: We propose a method for combining Harmonic Control Lyapunov-Barrier Functions (HCLBFs) derived from Signal Temporal Logic (STL) specifications with any given robot policy to turn an unsafe policy into a safe one with formal guarantees.  The two components are combined via HCLBF-derived safety certificates, thus producing commands that preserve both safety and task-driven behavior.  We demonstrate with a simple proof-of-concept implementation for an object-centric force-based policy trained through reinforcement learning for a movement task of a stationary robot arm that is able to avoid colliding with obstacles on a table top after combining the policy with the safety constraints.  The proposed method can be generalized to more complex specifications and dynamic task settings. 

**Abstract (ZH)**: 我们提出了一种将信号时逻辑(STL)规范导出的谐波控制拉普拉斯障碍函数(HCLBF)与任何给定的机器人策略结合的方法，以将一个不安全的策略转换为一个在形式上有保证的安全策略。通过HCLBF导出的安全证书将两者结合，从而产生同时保持安全性和任务驱动行为的命令。我们通过一个简单的概念性实现展示了结合安全约束后，一个用于避免桌上障碍物的固定机器人手臂基于力的对象中心策略训练后的安全策略验证。该方法可以推广到更复杂的要求和动态任务设置。 

---
# Mutation Testing for Industrial Robotic Systems 

**Title (ZH)**: 工业机器人系统的突变测试 

**Authors**: Marcela Gonçalves dos Santos, Sylvain Hallé, Fábio Petrillo  

**Link**: [PDF](https://arxiv.org/pdf/2511.14432)  

**Abstract**: Industrial robotic systems (IRS) are increasingly deployed in diverse environments, where failures can result in severe accidents and costly downtime. Ensuring the reliability of the software controlling these systems is therefore critical. Mutation testing, a technique widely used in software engineering, evaluates the effectiveness of test suites by introducing small faults, or mutants, into the code. However, traditional mutation operators are poorly suited to robotic programs, which involve message-based commands and interactions with the physical world. This paper explores the adaptation of mutation testing to IRS by defining domain-specific mutation operators that capture the semantics of robot actions and sensor readings. We propose a methodology for generating meaningful mutants at the level of high-level read and write operations, including movement, gripper actions, and sensor noise injection. An empirical study on a pick-and-place scenario demonstrates that our approach produces more informative mutants and reduces the number of invalid or equivalent cases compared to conventional operators. Results highlight the potential of mutation testing to enhance test suite quality and contribute to safer, more reliable industrial robotic systems. 

**Abstract (ZH)**: 工业机器人系统（IRS）的mutation测试adaptation研究：面向高阶读写操作的领域特定突变操作及其应用 

---
# Mutation Testing for Industrial Robotic Systems 

**Title (ZH)**: 工业机器人系统的突变测试 

**Authors**: Marcela Gonçalves dos Santos, Sylvain Hallé, Fábio Petrillo  

**Link**: [PDF](https://arxiv.org/pdf/2511.14432)  

**Abstract**: Industrial robotic systems (IRS) are increasingly deployed in diverse environments, where failures can result in severe accidents and costly downtime. Ensuring the reliability of the software controlling these systems is therefore critical. Mutation testing, a technique widely used in software engineering, evaluates the effectiveness of test suites by introducing small faults, or mutants, into the code. However, traditional mutation operators are poorly suited to robotic programs, which involve message-based commands and interactions with the physical world. This paper explores the adaptation of mutation testing to IRS by defining domain-specific mutation operators that capture the semantics of robot actions and sensor readings. We propose a methodology for generating meaningful mutants at the level of high-level read and write operations, including movement, gripper actions, and sensor noise injection. An empirical study on a pick-and-place scenario demonstrates that our approach produces more informative mutants and reduces the number of invalid or equivalent cases compared to conventional operators. Results highlight the potential of mutation testing to enhance test suite quality and contribute to safer, more reliable industrial robotic systems. 

**Abstract (ZH)**: 工业机器人系统（IRS）的变异测试 Adaptation of Mutation Testing for Industrial Robotic Systems 

---
# Mutation Testing for Industrial Robotic Systems 

**Title (ZH)**: 工业机器人系统中的突变测试 

**Authors**: Marcela Gonçalves dos Santos, Sylvain Hallé, Fábio Petrillo  

**Link**: [PDF](https://arxiv.org/pdf/2511.14432)  

**Abstract**: Industrial robotic systems (IRS) are increasingly deployed in diverse environments, where failures can result in severe accidents and costly downtime. Ensuring the reliability of the software controlling these systems is therefore critical. Mutation testing, a technique widely used in software engineering, evaluates the effectiveness of test suites by introducing small faults, or mutants, into the code. However, traditional mutation operators are poorly suited to robotic programs, which involve message-based commands and interactions with the physical world. This paper explores the adaptation of mutation testing to IRS by defining domain-specific mutation operators that capture the semantics of robot actions and sensor readings. We propose a methodology for generating meaningful mutants at the level of high-level read and write operations, including movement, gripper actions, and sensor noise injection. An empirical study on a pick-and-place scenario demonstrates that our approach produces more informative mutants and reduces the number of invalid or equivalent cases compared to conventional operators. Results highlight the potential of mutation testing to enhance test suite quality and contribute to safer, more reliable industrial robotic systems. 

**Abstract (ZH)**: 工业机器人系统（IRS）的变异测试 Adaptation of Mutation Testing for Industrial Robotic Systems 

---
# Mutation Testing for Industrial Robotic Systems 

**Title (ZH)**: 工业机器人系统的突变测试 

**Authors**: Marcela Gonçalves dos Santos, Sylvain Hallé, Fábio Petrillo  

**Link**: [PDF](https://arxiv.org/pdf/2511.14432)  

**Abstract**: Industrial robotic systems (IRS) are increasingly deployed in diverse environments, where failures can result in severe accidents and costly downtime. Ensuring the reliability of the software controlling these systems is therefore critical. Mutation testing, a technique widely used in software engineering, evaluates the effectiveness of test suites by introducing small faults, or mutants, into the code. However, traditional mutation operators are poorly suited to robotic programs, which involve message-based commands and interactions with the physical world. This paper explores the adaptation of mutation testing to IRS by defining domain-specific mutation operators that capture the semantics of robot actions and sensor readings. We propose a methodology for generating meaningful mutants at the level of high-level read and write operations, including movement, gripper actions, and sensor noise injection. An empirical study on a pick-and-place scenario demonstrates that our approach produces more informative mutants and reduces the number of invalid or equivalent cases compared to conventional operators. Results highlight the potential of mutation testing to enhance test suite quality and contribute to safer, more reliable industrial robotic systems. 

**Abstract (ZH)**: 工业机器人系统（IRS）的变异测试适应研究：面向高级读写操作的领域特定变异操作及其应用 

---
# Self-Supervised Multisensory Pretraining for Contact-Rich Robot Reinforcement Learning 

**Title (ZH)**: 基于联系丰富的机器人强化学习的自监督多感觉预训练 

**Authors**: Rickmer Krohn, Vignesh Prasad, Gabriele Tiboni, Georgia Chalvatzaki  

**Link**: [PDF](https://arxiv.org/pdf/2511.14427)  

**Abstract**: Effective contact-rich manipulation requires robots to synergistically leverage vision, force, and proprioception. However, Reinforcement Learning agents struggle to learn in such multisensory settings, especially amidst sensory noise and dynamic changes. We propose MultiSensory Dynamic Pretraining (MSDP), a novel framework for learning expressive multisensory representations tailored for task-oriented policy learning. MSDP is based on masked autoencoding and trains a transformer-based encoder by reconstructing multisensory observations from only a subset of sensor embeddings, leading to cross-modal prediction and sensor fusion. For downstream policy learning, we introduce a novel asymmetric architecture, where a cross-attention mechanism allows the critic to extract dynamic, task-specific features from the frozen embeddings, while the actor receives a stable pooled representation to guide its actions. Our method demonstrates accelerated learning and robust performance under diverse perturbations, including sensor noise, and changes in object dynamics. Evaluations in multiple challenging, contact-rich robot manipulation tasks in simulation and the real world showcase the effectiveness of MSDP. Our approach exhibits strong robustness to perturbations and achieves high success rates on the real robot with as few as 6,000 online interactions, offering a simple yet powerful solution for complex multisensory robotic control. 

**Abstract (ZH)**: 多感官动态预训练在丰富接触的机器人操作中的有效学习 

---
# Self-Supervised Multisensory Pretraining for Contact-Rich Robot Reinforcement Learning 

**Title (ZH)**: 自监督多感官预训练在接触丰富的机器人强化学习中的应用 

**Authors**: Rickmer Krohn, Vignesh Prasad, Gabriele Tiboni, Georgia Chalvatzaki  

**Link**: [PDF](https://arxiv.org/pdf/2511.14427)  

**Abstract**: Effective contact-rich manipulation requires robots to synergistically leverage vision, force, and proprioception. However, Reinforcement Learning agents struggle to learn in such multisensory settings, especially amidst sensory noise and dynamic changes. We propose MultiSensory Dynamic Pretraining (MSDP), a novel framework for learning expressive multisensory representations tailored for task-oriented policy learning. MSDP is based on masked autoencoding and trains a transformer-based encoder by reconstructing multisensory observations from only a subset of sensor embeddings, leading to cross-modal prediction and sensor fusion. For downstream policy learning, we introduce a novel asymmetric architecture, where a cross-attention mechanism allows the critic to extract dynamic, task-specific features from the frozen embeddings, while the actor receives a stable pooled representation to guide its actions. Our method demonstrates accelerated learning and robust performance under diverse perturbations, including sensor noise, and changes in object dynamics. Evaluations in multiple challenging, contact-rich robot manipulation tasks in simulation and the real world showcase the effectiveness of MSDP. Our approach exhibits strong robustness to perturbations and achieves high success rates on the real robot with as few as 6,000 online interactions, offering a simple yet powerful solution for complex multisensory robotic control. 

**Abstract (ZH)**: 多感知动态预训练在富接触机器人操作中的有效学习和鲁棒表现 

---
# Self-Supervised Multisensory Pretraining for Contact-Rich Robot Reinforcement Learning 

**Title (ZH)**: 自监督多感知预训练在接触丰富的机器人强化学习中的应用 

**Authors**: Rickmer Krohn, Vignesh Prasad, Gabriele Tiboni, Georgia Chalvatzaki  

**Link**: [PDF](https://arxiv.org/pdf/2511.14427)  

**Abstract**: Effective contact-rich manipulation requires robots to synergistically leverage vision, force, and proprioception. However, Reinforcement Learning agents struggle to learn in such multisensory settings, especially amidst sensory noise and dynamic changes. We propose MultiSensory Dynamic Pretraining (MSDP), a novel framework for learning expressive multisensory representations tailored for task-oriented policy learning. MSDP is based on masked autoencoding and trains a transformer-based encoder by reconstructing multisensory observations from only a subset of sensor embeddings, leading to cross-modal prediction and sensor fusion. For downstream policy learning, we introduce a novel asymmetric architecture, where a cross-attention mechanism allows the critic to extract dynamic, task-specific features from the frozen embeddings, while the actor receives a stable pooled representation to guide its actions. Our method demonstrates accelerated learning and robust performance under diverse perturbations, including sensor noise, and changes in object dynamics. Evaluations in multiple challenging, contact-rich robot manipulation tasks in simulation and the real world showcase the effectiveness of MSDP. Our approach exhibits strong robustness to perturbations and achieves high success rates on the real robot with as few as 6,000 online interactions, offering a simple yet powerful solution for complex multisensory robotic control. 

**Abstract (ZH)**: 多感官动态预训练在复杂多接触机器人操作中的有效学习与稳健性能 

---
# Self-Supervised Multisensory Pretraining for Contact-Rich Robot Reinforcement Learning 

**Title (ZH)**: 接触丰富的机器人强化学习自我监督多感知预训练 

**Authors**: Rickmer Krohn, Vignesh Prasad, Gabriele Tiboni, Georgia Chalvatzaki  

**Link**: [PDF](https://arxiv.org/pdf/2511.14427)  

**Abstract**: Effective contact-rich manipulation requires robots to synergistically leverage vision, force, and proprioception. However, Reinforcement Learning agents struggle to learn in such multisensory settings, especially amidst sensory noise and dynamic changes. We propose MultiSensory Dynamic Pretraining (MSDP), a novel framework for learning expressive multisensory representations tailored for task-oriented policy learning. MSDP is based on masked autoencoding and trains a transformer-based encoder by reconstructing multisensory observations from only a subset of sensor embeddings, leading to cross-modal prediction and sensor fusion. For downstream policy learning, we introduce a novel asymmetric architecture, where a cross-attention mechanism allows the critic to extract dynamic, task-specific features from the frozen embeddings, while the actor receives a stable pooled representation to guide its actions. Our method demonstrates accelerated learning and robust performance under diverse perturbations, including sensor noise, and changes in object dynamics. Evaluations in multiple challenging, contact-rich robot manipulation tasks in simulation and the real world showcase the effectiveness of MSDP. Our approach exhibits strong robustness to perturbations and achieves high success rates on the real robot with as few as 6,000 online interactions, offering a simple yet powerful solution for complex multisensory robotic control. 

**Abstract (ZH)**: 多感官动态预训练：面向任务的多感官表示学习的新框架 

---
# Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning 

**Title (ZH)**: 连续的视觉-语言-动作协同学习及其语义物理对齐行为 cloning 

**Authors**: Xiuxiu Qi, Yu Yang, Jiannong Cao, Luyao Bai, Chongshan Fan, Chengtai Cao, Hongpeng Wang  

**Link**: [PDF](https://arxiv.org/pdf/2511.14396)  

**Abstract**: Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states. 

**Abstract (ZH)**: 基于语言条件的操控助力通过行为克隆优化人机交互：连续视觉-语言-动作联合学习与语义-物理对齐（CCoL） 

---
# Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning 

**Title (ZH)**: 持续的视知觉-语言-行动协同学习及其语义-物理对齐行为克隆 

**Authors**: Xiuxiu Qi, Yu Yang, Jiannong Cao, Luyao Bai, Chongshan Fan, Chengtai Cao, Hongpeng Wang  

**Link**: [PDF](https://arxiv.org/pdf/2511.14396)  

**Abstract**: Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states. 

**Abstract (ZH)**: 基于语言条件的操控有助于通过行为克隆促进人机交互：Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL) 

---
# Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning 

**Title (ZH)**: 持续的视觉-语言-动作协同学习：基于语义-物理对齐的行为克隆 

**Authors**: Xiuxiu Qi, Yu Yang, Jiannong Cao, Luyao Bai, Chongshan Fan, Chengtai Cao, Hongpeng Wang  

**Link**: [PDF](https://arxiv.org/pdf/2511.14396)  

**Abstract**: Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states. 

**Abstract (ZH)**: 基于语言条件的操控促进行为克隆的人机交互：连续视觉-语言-动作联合学习与语义-物理对齐（CCoL） 

---
# Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning 

**Title (ZH)**: 连续的视觉-语言-行动协同学习：基于语义-物理对齐的行为克隆 

**Authors**: Xiuxiu Qi, Yu Yang, Jiannong Cao, Luyao Bai, Chongshan Fan, Chengtai Cao, Hongpeng Wang  

**Link**: [PDF](https://arxiv.org/pdf/2511.14396)  

**Abstract**: Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states. 

**Abstract (ZH)**: 基于语言条件的操控促进通过行为克隆的机器人操控，同时实现语义-物理对齐（Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment） 

---
# Perception-aware Exploration for Consumer-grade UAVs 

**Title (ZH)**: 感知导向的有效探索方法在消费级无人机中 

**Authors**: Svetlana Seliunina, Daniel Schleich, Sven Behnke  

**Link**: [PDF](https://arxiv.org/pdf/2511.14393)  

**Abstract**: In our work, we extend the current state-of-the-art approach for autonomous multi-UAV exploration to consumer-level UAVs, such as the DJI Mini 3 Pro. We propose a pipeline that selects viewpoint pairs from which the depth can be estimated and plans the trajectory that satisfies motion constraints necessary for odometry estimation. For the multi-UAV exploration, we propose a semi-distributed communication scheme that distributes the workload in a balanced manner. We evaluate our model performance in simulation for different numbers of UAVs and prove its ability to safely explore the environment and reconstruct the map even with the hardware limitations of consumer-grade UAVs. 

**Abstract (ZH)**: 我们在工作中将当前最先进的自主多无人机探索方法扩展到消费级无人机，如大疆Mini 3 Pro。我们提出了一种管道，从其中可以选择能够估计深度的视角对，并计划满足用于里程计估计的运动约束的轨迹。对于多无人机探索，我们提出了一种半分布式通信方案，以平衡分配工作负载。我们在不同的无人机数量下进行仿真评估，证明了其在面临消费级无人机硬件限制的情况下，仍能安全地探索环境并重建地图的能力。 

---
# Perception-aware Exploration for Consumer-grade UAVs 

**Title (ZH)**: 面向消费级无人机的感知驱动探索 

**Authors**: Svetlana Seliunina, Daniel Schleich, Sven Behnke  

**Link**: [PDF](https://arxiv.org/pdf/2511.14393)  

**Abstract**: In our work, we extend the current state-of-the-art approach for autonomous multi-UAV exploration to consumer-level UAVs, such as the DJI Mini 3 Pro. We propose a pipeline that selects viewpoint pairs from which the depth can be estimated and plans the trajectory that satisfies motion constraints necessary for odometry estimation. For the multi-UAV exploration, we propose a semi-distributed communication scheme that distributes the workload in a balanced manner. We evaluate our model performance in simulation for different numbers of UAVs and prove its ability to safely explore the environment and reconstruct the map even with the hardware limitations of consumer-grade UAVs. 

**Abstract (ZH)**: 我们在工作中将当前最先进的自主多UAV探索方法扩展到消费级UAV，如大疆Mini 3 Pro。我们提出了一种管道方法，从这些视角中选择可用于估算深度的视角对，并规划满足用于姿跟踪估计必要运动约束的轨迹。对于多UAV探索，我们提出了一种半分布式通信方案，以均衡分配工作负载。我们在不同的UAV数量下进行仿真评估，证明该模型即使在消费级UAV的硬件限制下，也能安全地探索环境并重建地图。 

---
# Perception-aware Exploration for Consumer-grade UAVs 

**Title (ZH)**: 感知导向的消费级无人机探索 

**Authors**: Svetlana Seliunina, Daniel Schleich, Sven Behnke  

**Link**: [PDF](https://arxiv.org/pdf/2511.14393)  

**Abstract**: In our work, we extend the current state-of-the-art approach for autonomous multi-UAV exploration to consumer-level UAVs, such as the DJI Mini 3 Pro. We propose a pipeline that selects viewpoint pairs from which the depth can be estimated and plans the trajectory that satisfies motion constraints necessary for odometry estimation. For the multi-UAV exploration, we propose a semi-distributed communication scheme that distributes the workload in a balanced manner. We evaluate our model performance in simulation for different numbers of UAVs and prove its ability to safely explore the environment and reconstruct the map even with the hardware limitations of consumer-grade UAVs. 

**Abstract (ZH)**: 我们的工作将当前最先进的自主多无人机探索方法扩展到消费级无人机，如大疆Mini 3 Pro。我们提出了一种管道方法，从这些视角中选择深度可以估计的视角对，并计划满足用于运动估计必要约束的轨迹。对于多无人机探索，我们提出了一种半分布式通信方案，以均衡分配工作负载。我们在不同数量的无人机仿真中评估了模型性能，并证明了即使在消费级无人机硬件限制的情况下，该模型也能够安全地探索环境并重建地图。 

---
# Perception-aware Exploration for Consumer-grade UAVs 

**Title (ZH)**: 感知驱动的消费级无人机探索 

**Authors**: Svetlana Seliunina, Daniel Schleich, Sven Behnke  

**Link**: [PDF](https://arxiv.org/pdf/2511.14393)  

**Abstract**: In our work, we extend the current state-of-the-art approach for autonomous multi-UAV exploration to consumer-level UAVs, such as the DJI Mini 3 Pro. We propose a pipeline that selects viewpoint pairs from which the depth can be estimated and plans the trajectory that satisfies motion constraints necessary for odometry estimation. For the multi-UAV exploration, we propose a semi-distributed communication scheme that distributes the workload in a balanced manner. We evaluate our model performance in simulation for different numbers of UAVs and prove its ability to safely explore the environment and reconstruct the map even with the hardware limitations of consumer-grade UAVs. 

**Abstract (ZH)**: 我们将当前最先进的自主多无人机探索方法扩展到消费级无人机，如大疆Mini 3 Pro。我们提出了一种管道方法，从其中选择视角对深度进行估计，并规划满足用于里程计估计的运动约束的轨迹。对于多无人机探索，我们提出了一种半分布式通信方案，以均衡分配工作量。我们在不同数量的无人机下在仿真中评估了我们模型的性能，并证明了即使在消费级无人机硬件限制下，该方法也能安全地探索环境并重建地图。 

---
# Going Places: Place Recognition in Artificial and Natural Systems 

**Title (ZH)**: 前往目的地：人工与自然系统中的位置识别 

**Authors**: Michael Milford, Tobias Fischer  

**Link**: [PDF](https://arxiv.org/pdf/2511.14341)  

**Abstract**: Place recognition, the ability to identify previously visited locations, is critical for both biological navigation and autonomous systems. This review synthesizes findings from robotic systems, animal studies, and human research to explore how different systems encode and recall place. We examine the computational and representational strategies employed across artificial systems, animals, and humans, highlighting convergent solutions such as topological mapping, cue integration, and memory management. Animal systems reveal evolved mechanisms for multimodal navigation and environmental adaptation, while human studies provide unique insights into semantic place concepts, cultural influences, and introspective capabilities. Artificial systems showcase scalable architectures and data-driven models. We propose a unifying set of concepts by which to consider and develop place recognition mechanisms and identify key challenges such as generalization, robustness, and environmental variability. This review aims to foster innovations in artificial localization by connecting future developments in artificial place recognition systems to insights from both animal navigation research and human spatial cognition studies. 

**Abstract (ZH)**: 基于机器人系统、动物研究和人类研究的综述：探索不同系统中地点识别的编码与回忆机制 

---
# Going Places: Place Recognition in Artificial and Natural Systems 

**Title (ZH)**: 前往目的地：人工与自然系统中的位置识别 

**Authors**: Michael Milford, Tobias Fischer  

**Link**: [PDF](https://arxiv.org/pdf/2511.14341)  

**Abstract**: Place recognition, the ability to identify previously visited locations, is critical for both biological navigation and autonomous systems. This review synthesizes findings from robotic systems, animal studies, and human research to explore how different systems encode and recall place. We examine the computational and representational strategies employed across artificial systems, animals, and humans, highlighting convergent solutions such as topological mapping, cue integration, and memory management. Animal systems reveal evolved mechanisms for multimodal navigation and environmental adaptation, while human studies provide unique insights into semantic place concepts, cultural influences, and introspective capabilities. Artificial systems showcase scalable architectures and data-driven models. We propose a unifying set of concepts by which to consider and develop place recognition mechanisms and identify key challenges such as generalization, robustness, and environmental variability. This review aims to foster innovations in artificial localization by connecting future developments in artificial place recognition systems to insights from both animal navigation research and human spatial cognition studies. 

**Abstract (ZH)**: 场所识别，即识别之前访问过的地点的能力，对于生物导航和自主系统都至关重要。本文综述了来自机器人系统、动物研究和人类研究的发现，探讨不同系统如何编码和回忆场所。本文考察了人工系统、动物和人类在计算和表示策略上的差异，突出显示了拓扑映射、线索整合和记忆管理等一致的解决方案。动物系统揭示了多模态导航和环境适应的进化机制，而人类研究提供了关于语义场所概念、文化影响和自我反省能力的独特见解。人工系统展示了可扩展的架构和数据驱动的模型。我们提出了一套统一的概念框架来考虑和发展场所识别机制，并识别出泛化、鲁棒性以及环境变化等关键挑战。本文旨在通过将未来的人工场所识别系统发展与动物导航研究和人类空间认知研究的见解相结合，促进人工定位技术的创新。 

---
# Going Places: Place Recognition in Artificial and Natural Systems 

**Title (ZH)**: 前往目的地：人工与自然系统中的地点识别 

**Authors**: Michael Milford, Tobias Fischer  

**Link**: [PDF](https://arxiv.org/pdf/2511.14341)  

**Abstract**: Place recognition, the ability to identify previously visited locations, is critical for both biological navigation and autonomous systems. This review synthesizes findings from robotic systems, animal studies, and human research to explore how different systems encode and recall place. We examine the computational and representational strategies employed across artificial systems, animals, and humans, highlighting convergent solutions such as topological mapping, cue integration, and memory management. Animal systems reveal evolved mechanisms for multimodal navigation and environmental adaptation, while human studies provide unique insights into semantic place concepts, cultural influences, and introspective capabilities. Artificial systems showcase scalable architectures and data-driven models. We propose a unifying set of concepts by which to consider and develop place recognition mechanisms and identify key challenges such as generalization, robustness, and environmental variability. This review aims to foster innovations in artificial localization by connecting future developments in artificial place recognition systems to insights from both animal navigation research and human spatial cognition studies. 

**Abstract (ZH)**: 生物导航和自主系统中至关重要的位置识别能力：从机器人系统、动物研究和人类研究中探索不同系统如何编码和回忆位置 

---
# Going Places: Place Recognition in Artificial and Natural Systems 

**Title (ZH)**: 前往目的地：人工和自然系统中的位置识别 

**Authors**: Michael Milford, Tobias Fischer  

**Link**: [PDF](https://arxiv.org/pdf/2511.14341)  

**Abstract**: Place recognition, the ability to identify previously visited locations, is critical for both biological navigation and autonomous systems. This review synthesizes findings from robotic systems, animal studies, and human research to explore how different systems encode and recall place. We examine the computational and representational strategies employed across artificial systems, animals, and humans, highlighting convergent solutions such as topological mapping, cue integration, and memory management. Animal systems reveal evolved mechanisms for multimodal navigation and environmental adaptation, while human studies provide unique insights into semantic place concepts, cultural influences, and introspective capabilities. Artificial systems showcase scalable architectures and data-driven models. We propose a unifying set of concepts by which to consider and develop place recognition mechanisms and identify key challenges such as generalization, robustness, and environmental variability. This review aims to foster innovations in artificial localization by connecting future developments in artificial place recognition systems to insights from both animal navigation research and human spatial cognition studies. 

**Abstract (ZH)**: 位置识别，即识别之前访问过的地点的能力，对于生物导航和自主系统都至关重要。本文综述了来自机器人系统、动物研究和人类研究的发现，探讨了不同系统如何编码和回忆地点。我们分析了人工系统、动物和人类在计算和表示策略上的差异，强调了拓扑映射、线索整合和记忆管理等一致的解决方案。动物系统揭示了多模态导航和环境适应的进化机制，而人类研究提供了关于语义地点概念、文化影响和内省能力的独特见解。人工系统展示了可扩展的架构和数据驱动的模型。我们提出了一组统一的概念来考虑和发展位置识别机制，并指出了泛化能力、鲁棒性和环境异质性等关键挑战。本文旨在通过将未来的人工位置识别系统的发展与动物导航研究和人类空间认知研究的见解联系起来，促进人工定位技术的创新。 

---
# Simultaneous Localization and 3D-Semi Dense Mapping for Micro Drones Using Monocular Camera and Inertial Sensors 

**Title (ZH)**: 使用单目摄像头和惯性传感器进行微无人机的同時定位与三维半稠密映射 

**Authors**: Jeryes Danial, Yosi Ben Asher, Itzik Klein  

**Link**: [PDF](https://arxiv.org/pdf/2511.14335)  

**Abstract**: Monocular simultaneous localization and mapping (SLAM) algorithms estimate drone poses and build a 3D map using a single camera. Current algorithms include sparse methods that lack detailed geometry, while learning-driven approaches produce dense maps but are computationally intensive. Monocular SLAM also faces scale ambiguities, which affect its accuracy. To address these challenges, we propose an edge-aware lightweight monocular SLAM system combining sparse keypoint-based pose estimation with dense edge reconstruction. Our method employs deep learning-based depth prediction and edge detection, followed by optimization to refine keypoints and edges for geometric consistency, without relying on global loop closure or heavy neural computations. We fuse inertial data with vision by using an extended Kalman filter to resolve scale ambiguity and improve accuracy. The system operates in real time on low-power platforms, as demonstrated on a DJI Tello drone with a monocular camera and inertial sensors. In addition, we demonstrate robust autonomous navigation and obstacle avoidance in indoor corridors and on the TUM RGBD dataset. Our approach offers an effective, practical solution to real-time mapping and navigation in resource-constrained environments. 

**Abstract (ZH)**: 基于边缘感知的轻量级单目SLAM系统：结合稀疏关键点姿态估计与密集边缘重建 

---
# Simultaneous Localization and 3D-Semi Dense Mapping for Micro Drones Using Monocular Camera and Inertial Sensors 

**Title (ZH)**: 单目相机和惯性传感器结合的微小型无人机同时定位与半稠密三维建图 

**Authors**: Jeryes Danial, Yosi Ben Asher, Itzik Klein  

**Link**: [PDF](https://arxiv.org/pdf/2511.14335)  

**Abstract**: Monocular simultaneous localization and mapping (SLAM) algorithms estimate drone poses and build a 3D map using a single camera. Current algorithms include sparse methods that lack detailed geometry, while learning-driven approaches produce dense maps but are computationally intensive. Monocular SLAM also faces scale ambiguities, which affect its accuracy. To address these challenges, we propose an edge-aware lightweight monocular SLAM system combining sparse keypoint-based pose estimation with dense edge reconstruction. Our method employs deep learning-based depth prediction and edge detection, followed by optimization to refine keypoints and edges for geometric consistency, without relying on global loop closure or heavy neural computations. We fuse inertial data with vision by using an extended Kalman filter to resolve scale ambiguity and improve accuracy. The system operates in real time on low-power platforms, as demonstrated on a DJI Tello drone with a monocular camera and inertial sensors. In addition, we demonstrate robust autonomous navigation and obstacle avoidance in indoor corridors and on the TUM RGBD dataset. Our approach offers an effective, practical solution to real-time mapping and navigation in resource-constrained environments. 

**Abstract (ZH)**: 单目同时定位与建图（SLAM）算法使用单个相机估计无人机姿态并构建3D地图。当前的算法包括缺乏详细几何结构的稀疏方法，而基于学习的方法则产生密集的地图但计算强度大。单目SLAM还面临比例缩放的不确定性，这影响其准确性。为了解决这些挑战，我们提出了一种结合基于稀疏关键点的姿态估计和密集边缘重建的边缘感知轻量级单目SLAM系统。该方法采用基于深度学习的深度预测和边缘检测，然后通过优化来细化关键点和边缘以确保几何一致性，无需依赖全局环回闭合或重大的神经计算。通过使用扩展卡尔曼滤波器融合惯性数据与视觉数据来解决比例缩放不确定性并提高准确性。该系统能够在低功耗平台上实时运行，如在配备单目相机和惯性传感器的DJI Tello无人机上演示所示。此外，我们在室内走廊和TUM RGBD数据集上展示了 robust自主导航和避障功能。我们的方法提供了在资源受限环境中实现实时映射和导航的有效、实用的解决方案。 

---
# Simultaneous Localization and 3D-Semi Dense Mapping for Micro Drones Using Monocular Camera and Inertial Sensors 

**Title (ZH)**: 使用单目相机和惯性传感器进行微无人机的同时定位与三维半密集映射 

**Authors**: Jeryes Danial, Yosi Ben Asher, Itzik Klein  

**Link**: [PDF](https://arxiv.org/pdf/2511.14335)  

**Abstract**: Monocular simultaneous localization and mapping (SLAM) algorithms estimate drone poses and build a 3D map using a single camera. Current algorithms include sparse methods that lack detailed geometry, while learning-driven approaches produce dense maps but are computationally intensive. Monocular SLAM also faces scale ambiguities, which affect its accuracy. To address these challenges, we propose an edge-aware lightweight monocular SLAM system combining sparse keypoint-based pose estimation with dense edge reconstruction. Our method employs deep learning-based depth prediction and edge detection, followed by optimization to refine keypoints and edges for geometric consistency, without relying on global loop closure or heavy neural computations. We fuse inertial data with vision by using an extended Kalman filter to resolve scale ambiguity and improve accuracy. The system operates in real time on low-power platforms, as demonstrated on a DJI Tello drone with a monocular camera and inertial sensors. In addition, we demonstrate robust autonomous navigation and obstacle avoidance in indoor corridors and on the TUM RGBD dataset. Our approach offers an effective, practical solution to real-time mapping and navigation in resource-constrained environments. 

**Abstract (ZH)**: 单目同时定位与建图（SLAM）算法使用单个摄像头估算无人机姿态并构建3D地图。当前算法包括缺乏详细几何结构的稀疏方法，而基于学习的方法会产生密集地图但计算强度大。单目SLAM还面临尺度歧义问题，影响其精度。为应对这些挑战，我们提出了一种边缘感知轻量级单目SLAM系统，结合稀疏关键点基于的方法进行姿态估计和密集边缘重建。我们的方法采用基于深度学习的深度预测和边缘检测，随后通过优化来细化关键点和边缘以实现几何一致性，无需依赖全局环回闭合或重型神经计算。通过使用扩展卡尔曼滤波器融合惯性数据与视觉数据来解决尺度歧义并提高精度。该系统可以在低功耗平台上实时运行，如在DJI Tello无人机上进行演示，该无人机配备单目摄像头和惯性传感器。此外，我们在室内走廊和TUM RGBD数据集上展示了鲁棒的自主导航和障碍物回避能力。我们的方法为资源受限环境下的实时建图和导航提供了一种有效的实际解决方案。 

---
# MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning 

**Title (ZH)**: MA-SLAM: 基于地图意识的深度强化学习在大规模未知环境中的主动SLAM 

**Authors**: Yizhen Yin, Yuhua Qi, Dapeng Feng, Hongbo Chen, Hongjun Ma, Jin Wu, Yi Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2511.14330)  

**Abstract**: Active Simultaneous Localization and Mapping (Active SLAM) involves the strategic planning and precise control of a robotic system's movement in order to construct a highly accurate and comprehensive representation of its surrounding environment, which has garnered significant attention within the research community. While the current methods demonstrate efficacy in small and controlled settings, they face challenges when applied to large-scale and diverse environments, marked by extended periods of exploration and suboptimal paths of discovery. In this paper, we propose MA-SLAM, a Map-Aware Active SLAM system based on Deep Reinforcement Learning (DRL), designed to address the challenge of efficient exploration in large-scale environments. In pursuit of this objective, we put forward a novel structured map representation. By discretizing the spatial data and integrating the boundary points and the historical trajectory, the structured map succinctly and effectively encapsulates the visited regions, thereby serving as input for the deep reinforcement learning based decision module. Instead of sequentially predicting the next action step within the decision module, we have implemented an advanced global planner to optimize the exploration path by leveraging long-range target points. We conducted experiments in three simulation environments and deployed in a real unmanned ground vehicle (UGV), the results demonstrate that our approach significantly reduces both the duration and distance of exploration compared with state-of-the-art methods. 

**Abstract (ZH)**: 基于深度强化学习的图aware主动SLAM（MA-SLAM）：大规模环境高效探索方法 

---
# Simultaneous Localization and 3D-Semi Dense Mapping for Micro Drones Using Monocular Camera and Inertial Sensors 

**Title (ZH)**: 使用单目摄像头和惯性传感器进行微无人机的同时定位与三维半稠密建图 

**Authors**: Jeryes Danial, Yosi Ben Asher, Itzik Klein  

**Link**: [PDF](https://arxiv.org/pdf/2511.14335)  

**Abstract**: Monocular simultaneous localization and mapping (SLAM) algorithms estimate drone poses and build a 3D map using a single camera. Current algorithms include sparse methods that lack detailed geometry, while learning-driven approaches produce dense maps but are computationally intensive. Monocular SLAM also faces scale ambiguities, which affect its accuracy. To address these challenges, we propose an edge-aware lightweight monocular SLAM system combining sparse keypoint-based pose estimation with dense edge reconstruction. Our method employs deep learning-based depth prediction and edge detection, followed by optimization to refine keypoints and edges for geometric consistency, without relying on global loop closure or heavy neural computations. We fuse inertial data with vision by using an extended Kalman filter to resolve scale ambiguity and improve accuracy. The system operates in real time on low-power platforms, as demonstrated on a DJI Tello drone with a monocular camera and inertial sensors. In addition, we demonstrate robust autonomous navigation and obstacle avoidance in indoor corridors and on the TUM RGBD dataset. Our approach offers an effective, practical solution to real-time mapping and navigation in resource-constrained environments. 

**Abstract (ZH)**: 单目同时定位与建图（SLAM）算法使用单个摄像头估计无人机姿势并构建3D地图。现有算法包括缺乏详细几何信息的稀疏方法，而基于学习的方法则生成密集地图但计算量大。单目SLAM还面临尺度模糊问题，影响其精度。为应对这些挑战，我们提出了一种结合基于稀疏关键点的姿势估计和密集边缘重建的边缘意识轻量级单目SLAM系统。该方法利用基于深度学习的深度预测和边缘检测，并通过优化来细化关键点和边缘以确保几何一致性，不依赖全局回环闭合或复杂的神经计算。我们利用扩展卡尔曼滤波融合惯性数据与视觉信息以解决尺度模糊问题并提高精度。该系统能够在低功率平台上实时运行，如在使用单目摄像头和惯性传感器的DJI Tello无人机上验证。此外，我们在室内走廊和TUM RGBD数据集上展示了稳健的自主导航和避障能力。我们的方法为资源受限环境下的实时建图与导航提供了有效解决方案。 

---
# MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning 

**Title (ZH)**: MA-SLAM: 基于地图感知的深度强化学习在大规模未知环境中的主动SLAM 

**Authors**: Yizhen Yin, Yuhua Qi, Dapeng Feng, Hongbo Chen, Hongjun Ma, Jin Wu, Yi Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2511.14330)  

**Abstract**: Active Simultaneous Localization and Mapping (Active SLAM) involves the strategic planning and precise control of a robotic system's movement in order to construct a highly accurate and comprehensive representation of its surrounding environment, which has garnered significant attention within the research community. While the current methods demonstrate efficacy in small and controlled settings, they face challenges when applied to large-scale and diverse environments, marked by extended periods of exploration and suboptimal paths of discovery. In this paper, we propose MA-SLAM, a Map-Aware Active SLAM system based on Deep Reinforcement Learning (DRL), designed to address the challenge of efficient exploration in large-scale environments. In pursuit of this objective, we put forward a novel structured map representation. By discretizing the spatial data and integrating the boundary points and the historical trajectory, the structured map succinctly and effectively encapsulates the visited regions, thereby serving as input for the deep reinforcement learning based decision module. Instead of sequentially predicting the next action step within the decision module, we have implemented an advanced global planner to optimize the exploration path by leveraging long-range target points. We conducted experiments in three simulation environments and deployed in a real unmanned ground vehicle (UGV), the results demonstrate that our approach significantly reduces both the duration and distance of exploration compared with state-of-the-art methods. 

**Abstract (ZH)**: 基于深度强化学习的感知图aware主动SLAM (MA-SLAM)：大规模环境高效探索 

---
# MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning 

**Title (ZH)**: MA-SLAM：基于地图aware的深度强化学习在大规模未知环境中的主动SLAM 

**Authors**: Yizhen Yin, Yuhua Qi, Dapeng Feng, Hongbo Chen, Hongjun Ma, Jin Wu, Yi Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2511.14330)  

**Abstract**: Active Simultaneous Localization and Mapping (Active SLAM) involves the strategic planning and precise control of a robotic system's movement in order to construct a highly accurate and comprehensive representation of its surrounding environment, which has garnered significant attention within the research community. While the current methods demonstrate efficacy in small and controlled settings, they face challenges when applied to large-scale and diverse environments, marked by extended periods of exploration and suboptimal paths of discovery. In this paper, we propose MA-SLAM, a Map-Aware Active SLAM system based on Deep Reinforcement Learning (DRL), designed to address the challenge of efficient exploration in large-scale environments. In pursuit of this objective, we put forward a novel structured map representation. By discretizing the spatial data and integrating the boundary points and the historical trajectory, the structured map succinctly and effectively encapsulates the visited regions, thereby serving as input for the deep reinforcement learning based decision module. Instead of sequentially predicting the next action step within the decision module, we have implemented an advanced global planner to optimize the exploration path by leveraging long-range target points. We conducted experiments in three simulation environments and deployed in a real unmanned ground vehicle (UGV), the results demonstrate that our approach significantly reduces both the duration and distance of exploration compared with state-of-the-art methods. 

**Abstract (ZH)**: 基于深度 reinforcement学习的Map-Aware Active SLAM (MA-SLAM)：大规模环境高效探索方法 

---
# Dual-Variable Force Characterisation method for Human-Robot Interaction in Wearable Robotics 

**Title (ZH)**: 可穿戴机器人中人类-机器人交互的双变量力特性分析方法 

**Authors**: Felipe Ballen-Moreno, Pasquale Ferrentino, Milan Amighi, Bram Vanderborght, Tom Verstraten  

**Link**: [PDF](https://arxiv.org/pdf/2511.14327)  

**Abstract**: Understanding the physical interaction with wearable robots is essential to ensure safety and comfort. However, this interaction is complex in two key aspects: (1) the motion involved, and (2) the non-linear behaviour of soft tissues. Multiple approaches have been undertaken to better understand this interaction and to improve the quantitative metrics of physical interfaces or cuffs. As these two topics are closely interrelated, finite modelling and soft tissue characterisation offer valuable insights into pressure distribution and shear stress induced by the cuff. Nevertheless, current characterisation methods typically rely on a single fitting variable along one degree of freedom, which limits their applicability, given that interactions with wearable robots often involve multiple degrees of freedom. To address this limitation, this work introduces a dual-variable characterisation method, involving normal and tangential forces, aimed at identifying reliable material parameters and evaluating the impact of single-variable fitting on force and torque responses. This method demonstrates the importance of incorporating two variables into the characterisation process by analysing the normalized mean square error (NMSE) across different scenarios and material models, providing a foundation for simulation at the closest possible level, with a focus on the cuff and the human limb involved in the physical interaction between the user and the wearable robot. 

**Abstract (ZH)**: 理解穿戴式机器人的人体物理交互对于确保安全与舒适至关重要。然而，这种交互在两个关键方面具有复杂性：(1) 动态运动，(2) 软组织的非线性行为。多种方法已被用于更深入地理解这种交互，并改进物理接口或袖套的定量评价指标。鉴于这两者密切相关，有限元建模和软组织特性分析为研究袖套引起的压应力和剪切应力提供了宝贵的见解。然而，当前的特性化方法通常依赖单一拟合变量在一个自由度上，这限制了它们的应用性，尤其是当穿戴式机器人的人体交互涉及多个自由度时。为此，本工作引入了一种双变量特性化方法，涉及法向力和切向力，旨在确定可靠的材料参数，并评估单一变量拟合对力和扭矩响应的影响。该方法通过分析不同场景和材料模型下的归一化均方误差（NMSE），证明了在特性化过程中纳入两个变量的重要性，并为仿真研究提供了基础，重点关注穿戴式机器人与人体之间的物理交互中的袖带和人体肢体。 

---
# MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning 

**Title (ZH)**: MA-SLAM: 基于地图aware的深度强化学习在大规模未知环境中的主动SLAM 

**Authors**: Yizhen Yin, Yuhua Qi, Dapeng Feng, Hongbo Chen, Hongjun Ma, Jin Wu, Yi Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2511.14330)  

**Abstract**: Active Simultaneous Localization and Mapping (Active SLAM) involves the strategic planning and precise control of a robotic system's movement in order to construct a highly accurate and comprehensive representation of its surrounding environment, which has garnered significant attention within the research community. While the current methods demonstrate efficacy in small and controlled settings, they face challenges when applied to large-scale and diverse environments, marked by extended periods of exploration and suboptimal paths of discovery. In this paper, we propose MA-SLAM, a Map-Aware Active SLAM system based on Deep Reinforcement Learning (DRL), designed to address the challenge of efficient exploration in large-scale environments. In pursuit of this objective, we put forward a novel structured map representation. By discretizing the spatial data and integrating the boundary points and the historical trajectory, the structured map succinctly and effectively encapsulates the visited regions, thereby serving as input for the deep reinforcement learning based decision module. Instead of sequentially predicting the next action step within the decision module, we have implemented an advanced global planner to optimize the exploration path by leveraging long-range target points. We conducted experiments in three simulation environments and deployed in a real unmanned ground vehicle (UGV), the results demonstrate that our approach significantly reduces both the duration and distance of exploration compared with state-of-the-art methods. 

**Abstract (ZH)**: 基于深度强化学习的意识地图规划SLAM (MA-SLAM) 

---
# Dual-Variable Force Characterisation method for Human-Robot Interaction in Wearable Robotics 

**Title (ZH)**: 可穿戴机器人中人机交互的双变量力特征化方法 

**Authors**: Felipe Ballen-Moreno, Pasquale Ferrentino, Milan Amighi, Bram Vanderborght, Tom Verstraten  

**Link**: [PDF](https://arxiv.org/pdf/2511.14327)  

**Abstract**: Understanding the physical interaction with wearable robots is essential to ensure safety and comfort. However, this interaction is complex in two key aspects: (1) the motion involved, and (2) the non-linear behaviour of soft tissues. Multiple approaches have been undertaken to better understand this interaction and to improve the quantitative metrics of physical interfaces or cuffs. As these two topics are closely interrelated, finite modelling and soft tissue characterisation offer valuable insights into pressure distribution and shear stress induced by the cuff. Nevertheless, current characterisation methods typically rely on a single fitting variable along one degree of freedom, which limits their applicability, given that interactions with wearable robots often involve multiple degrees of freedom. To address this limitation, this work introduces a dual-variable characterisation method, involving normal and tangential forces, aimed at identifying reliable material parameters and evaluating the impact of single-variable fitting on force and torque responses. This method demonstrates the importance of incorporating two variables into the characterisation process by analysing the normalized mean square error (NMSE) across different scenarios and material models, providing a foundation for simulation at the closest possible level, with a focus on the cuff and the human limb involved in the physical interaction between the user and the wearable robot. 

**Abstract (ZH)**: 理解穿戴式机器人的人体物理交互对于确保安全和舒适至关重要。然而，这种交互在两个关键方面具有复杂性：（1）运动参与，（2）软组织的非线性行为。为更好地理解这种交互并改进物理界面或手袖的量化指标，已经采取了多种方法。由于这些两个主题密切相关，有限元建模和软组织特征化为了解手袖引起的压分布和剪切应力提供了有价值的见解。然而，当前的特征化方法通常依赖于一个单一的适应变量，并且局限于一个自由度，这限制了它们的应用，因为穿戴式机器人与人体的交互往往涉及多个自由度。为解决这一局限，本研究引入了一种双变量特征化方法，涉及正交力和切向力，旨在识别可靠的材料参数并评估单变量适应对力和扭矩响应的影响。通过分析不同场景和材料模型下的归一化均方误差（NMSE），该方法展示了在特征化过程中考虑两个变量的重要性，并为模拟提供了一个最低程度的基础，重点关注与用户和穿戴式机器人物理交互相关的手袖和人体肢体。 

---
# Dual-Variable Force Characterisation method for Human-Robot Interaction in Wearable Robotics 

**Title (ZH)**: 可穿戴机器人中人机交互的双变量力特性表征方法 

**Authors**: Felipe Ballen-Moreno, Pasquale Ferrentino, Milan Amighi, Bram Vanderborght, Tom Verstraten  

**Link**: [PDF](https://arxiv.org/pdf/2511.14327)  

**Abstract**: Understanding the physical interaction with wearable robots is essential to ensure safety and comfort. However, this interaction is complex in two key aspects: (1) the motion involved, and (2) the non-linear behaviour of soft tissues. Multiple approaches have been undertaken to better understand this interaction and to improve the quantitative metrics of physical interfaces or cuffs. As these two topics are closely interrelated, finite modelling and soft tissue characterisation offer valuable insights into pressure distribution and shear stress induced by the cuff. Nevertheless, current characterisation methods typically rely on a single fitting variable along one degree of freedom, which limits their applicability, given that interactions with wearable robots often involve multiple degrees of freedom. To address this limitation, this work introduces a dual-variable characterisation method, involving normal and tangential forces, aimed at identifying reliable material parameters and evaluating the impact of single-variable fitting on force and torque responses. This method demonstrates the importance of incorporating two variables into the characterisation process by analysing the normalized mean square error (NMSE) across different scenarios and material models, providing a foundation for simulation at the closest possible level, with a focus on the cuff and the human limb involved in the physical interaction between the user and the wearable robot. 

**Abstract (ZH)**: 理解可穿戴机器人的人体物理交互对于确保安全性和舒适性至关重要。然而，这种交互在两个关键方面具有复杂性：（1）运动的涉及，（2）软组织的非线性行为。为更好地理解这种交互并改进物理界面或束缚带的定量指标，已经采取了多种方法。鉴于这些两个话题是紧密相关的，有限元建模和软组织表征为了解束缚带引起的压分布和剪切应力提供了宝贵见解。然而，当前的表征方法通常依赖于单一拟合变量在一个自由度上，这限制了它们的应用性，因为与可穿戴机器人的人体交互往往涉及多个自由度。为解决这一局限性，本工作提出了一种双变量表征方法，涉及正交和切向力，旨在识别可靠的材料参数并评估单一变量拟合对力和扭矩响应的影响。该方法通过分析不同场景和材料模型下的归一化均方根误差（NMSE），证明了将两个变量纳入表征过程的重要性，并为最接近水平的仿真奠定了基础，重点是涉及用户与可穿戴机器人物理交互的束缚带和人体肢体。 

---
# Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion 

**Title (ZH)**: 无需微调的VLA部署 towards 插件式 inference 时期 VLA 策略导向通过封装进化扩散。 

**Authors**: Zhuo Li, Junjia Liu, Zhipeng Dong, Tao Teng, Quentin Rouxel, Darwin Caldwell, Fei Chen  

**Link**: [PDF](https://arxiv.org/pdf/2511.14178)  

**Abstract**: Vision-Language-Action (VLA) models have demonstrated significant potential in real-world robotic manipulation. However, pre-trained VLA policies still suffer from substantial performance degradation during downstream deployment. Although fine-tuning can mitigate this issue, its reliance on costly demonstration collection and intensive computation makes it impractical in real-world settings. In this work, we introduce VLA-Pilot, a plug-and-play inference-time policy steering method for zero-shot deployment of pre-trained VLA without any additional fine-tuning or data collection. We evaluate VLA-Pilot on six real-world downstream manipulation tasks across two distinct robotic embodiments, encompassing both in-distribution and out-of-distribution scenarios. Experimental results demonstrate that VLA-Pilot substantially boosts the success rates of off-the-shelf pre-trained VLA policies, enabling robust zero-shot generalization to diverse tasks and embodiments. Experimental videos and code are available at: this https URL. 

**Abstract (ZH)**: Vision-Language-Action (VLA) 模型在现实中的机器人操作中表现出显著的潜力。然而，预训练的 VLA 策略在下游部署时仍遭受显著性能退化。尽管微调可以缓解这一问题，但其对昂贵的演示收集和密集计算的依赖使其在现实世界环境中不切实际。在本文中，我们介绍了 VLA-Pilot，一种方便插拔的推理时策略引导方法，可在无需任何额外微调或数据收集的情况下实现预训练 VLA 的零样本部署。我们在两个不同机器人载体上六项现实世界的下游操作任务上评估了 VLA-Pilot，涵盖分布内和分布外场景。实验结果表明，VLA-Pilot 显著提升了现成的预训练 VLA 策略的成功率，使其能够在多种任务和载体上实现稳健的零样本泛化。实验视频和代码可在以下链接获取：这个https链接。 

---
# Dual-Variable Force Characterisation method for Human-Robot Interaction in Wearable Robotics 

**Title (ZH)**: 穿戴机器人中人类-机器人交互的双变量力特性化方法 

**Authors**: Felipe Ballen-Moreno, Pasquale Ferrentino, Milan Amighi, Bram Vanderborght, Tom Verstraten  

**Link**: [PDF](https://arxiv.org/pdf/2511.14327)  

**Abstract**: Understanding the physical interaction with wearable robots is essential to ensure safety and comfort. However, this interaction is complex in two key aspects: (1) the motion involved, and (2) the non-linear behaviour of soft tissues. Multiple approaches have been undertaken to better understand this interaction and to improve the quantitative metrics of physical interfaces or cuffs. As these two topics are closely interrelated, finite modelling and soft tissue characterisation offer valuable insights into pressure distribution and shear stress induced by the cuff. Nevertheless, current characterisation methods typically rely on a single fitting variable along one degree of freedom, which limits their applicability, given that interactions with wearable robots often involve multiple degrees of freedom. To address this limitation, this work introduces a dual-variable characterisation method, involving normal and tangential forces, aimed at identifying reliable material parameters and evaluating the impact of single-variable fitting on force and torque responses. This method demonstrates the importance of incorporating two variables into the characterisation process by analysing the normalized mean square error (NMSE) across different scenarios and material models, providing a foundation for simulation at the closest possible level, with a focus on the cuff and the human limb involved in the physical interaction between the user and the wearable robot. 

**Abstract (ZH)**: 理解可穿戴机器人的人体物理交互对于确保安全与舒适至关重要。然而，这种交互在两个关键方面具有复杂性：（1）运动方式，（2）软组织的非线性行为。为更好地理解这种交互并改进物理界面或袖套的定量指标，已采取了多种方法。鉴于这两个主题紧密相关，有限元建模和软组织特性分析提供了有关袖套引起的压力分布和剪切应力的重要见解。然而，当前的特性化方法通常依赖于一个单一的拟合变量在一个自由度上，这限制了它们的应用范围，因为与可穿戴机器人的人体交互通常涉及多个自由度。为解决这一限制，本研究引入了一种双变量特性化方法，涉及法向力和切向力，旨在确定可靠的材料参数并评估单一变量拟合对力和扭矩响应的影响。通过分析不同场景和材料模型下的归一化均方误差（NMSE），该方法强调了在特性化过程中纳入两个变量的重要性，从而为最接近实际的模拟奠定了基础，重点关注参与人体与可穿戴机器人物理交互的袖套和人体肢体。 

---
# Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion 

**Title (ZH)**: 无需微调部署VLA：基于体化进化扩散的插拔式推理时VLA策略引导 

**Authors**: Zhuo Li, Junjia Liu, Zhipeng Dong, Tao Teng, Quentin Rouxel, Darwin Caldwell, Fei Chen  

**Link**: [PDF](https://arxiv.org/pdf/2511.14178)  

**Abstract**: Vision-Language-Action (VLA) models have demonstrated significant potential in real-world robotic manipulation. However, pre-trained VLA policies still suffer from substantial performance degradation during downstream deployment. Although fine-tuning can mitigate this issue, its reliance on costly demonstration collection and intensive computation makes it impractical in real-world settings. In this work, we introduce VLA-Pilot, a plug-and-play inference-time policy steering method for zero-shot deployment of pre-trained VLA without any additional fine-tuning or data collection. We evaluate VLA-Pilot on six real-world downstream manipulation tasks across two distinct robotic embodiments, encompassing both in-distribution and out-of-distribution scenarios. Experimental results demonstrate that VLA-Pilot substantially boosts the success rates of off-the-shelf pre-trained VLA policies, enabling robust zero-shot generalization to diverse tasks and embodiments. Experimental videos and code are available at: this https URL. 

**Abstract (ZH)**: Vision-Language-Action (VLA) 模型在真实世界机器人操作中展现了巨大潜力。然而，预训练的 VLA 策略在下游部署时仍会遭受显著性能退化。尽管微调可以缓解这一问题，但其依赖于昂贵的示范采集和密集计算使其在实际应用中不可行。在本工作中，我们提出了 VLA-Pilot，一种即插即用的推理时策略导向方法，可在无需额外微调或数据采集的情况下实现预训练 VLA 的零样本部署。我们在两种不同的机器人载体上评估了 VLA-Pilot 的六个实际下游操作任务，涵盖了分布内和分布外场景。实验结果表明，VLA-Pilot 显著提升了现成的预训练 VLA 策略的成功率，使其能够在多样化任务和载体上实现稳健的零样本泛化。实验视频和代码可在以下链接获取：this https URL。 

---
# Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion 

**Title (ZH)**: 无需微调部署VLA：基于封装进化扩散的推理时VLA策略自适应 

**Authors**: Zhuo Li, Junjia Liu, Zhipeng Dong, Tao Teng, Quentin Rouxel, Darwin Caldwell, Fei Chen  

**Link**: [PDF](https://arxiv.org/pdf/2511.14178)  

**Abstract**: Vision-Language-Action (VLA) models have demonstrated significant potential in real-world robotic manipulation. However, pre-trained VLA policies still suffer from substantial performance degradation during downstream deployment. Although fine-tuning can mitigate this issue, its reliance on costly demonstration collection and intensive computation makes it impractical in real-world settings. In this work, we introduce VLA-Pilot, a plug-and-play inference-time policy steering method for zero-shot deployment of pre-trained VLA without any additional fine-tuning or data collection. We evaluate VLA-Pilot on six real-world downstream manipulation tasks across two distinct robotic embodiments, encompassing both in-distribution and out-of-distribution scenarios. Experimental results demonstrate that VLA-Pilot substantially boosts the success rates of off-the-shelf pre-trained VLA policies, enabling robust zero-shot generalization to diverse tasks and embodiments. Experimental videos and code are available at: this https URL. 

**Abstract (ZH)**: Vision-Language-Action (VLA) 模型在实际机器人操作中展示了显著的潜力。然而，预训练的 VLA 策略在下游部署时仍遭受显著性能下降。尽管微调可以在一定程度上缓解这一问题，但其依赖昂贵的演示收集和密集的计算使其在实际场景中不切实际。在本文中，我们引入了 VLA-Pilot，这是一种即插即用的推理时策略引导方法，可以在无需任何额外微调或数据收集的情况下实现出-of-shot 部署的预训练 VLA。我们将在两种不同的机器人实体上六种实际的下游操作任务上评估 VLA-Pilot，涵盖分布内和分布外场景。实验结果表明，VLA-Pilot 显著提升了现成的预训练 VLA 策略的成功率，使其能够在多种任务和实体上实现鲁棒的 out-of-shot 通用性。实验视频和代码可在以下链接获取：this https URL。 

---
# RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action 

**Title (ZH)**: RoboTidy : 一种用于体态导航和操作的家庭整理3D高斯点标注基准 

**Authors**: Xiaoquan Sun, Ruijian Zhang, Kang Pang, Bingchen Miao, Yuxiang Tan, Zhen Yang, Ming Li, Jiayu Chen  

**Link**: [PDF](https://arxiv.org/pdf/2511.14161)  

**Abstract**: Household tidying is an important application area, yet current benchmarks neither model user preferences nor support mobility, and they generalize poorly, making it hard to comprehensively assess integrated language-to-action capabilities. To address this, we propose RoboTidy, a unified benchmark for language-guided household tidying that supports Vision-Language-Action (VLA) and Vision-Language-Navigation (VLN) training and evaluation. RoboTidy provides 500 photorealistic 3D Gaussian Splatting (3DGS) household scenes (covering 500 objects and containers) with collisions, formulates tidying as an "Action (Object, Container)" list, and supplies 6.4k high-quality manipulation demonstration trajectories and 1.5k naviagtion trajectories to support both few-shot and large-scale training. We also deploy RoboTidy in the real world for object tidying, establishing an end-to-end benchmark for household tidying. RoboTidy offers a scalable platform and bridges a key gap in embodied AI by enabling holistic and realistic evaluation of language-guided robots. 

**Abstract (ZH)**: 基于语言指导的家庭整理统一基准RoboTidy：面向视觉-语言-行动与视觉-语言-导航的综合评估 

---
# Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion 

**Title (ZH)**: 无需微调部署VLA：基于封装演化扩散的推理时VLA策略插拔式控制 

**Authors**: Zhuo Li, Junjia Liu, Zhipeng Dong, Tao Teng, Quentin Rouxel, Darwin Caldwell, Fei Chen  

**Link**: [PDF](https://arxiv.org/pdf/2511.14178)  

**Abstract**: Vision-Language-Action (VLA) models have demonstrated significant potential in real-world robotic manipulation. However, pre-trained VLA policies still suffer from substantial performance degradation during downstream deployment. Although fine-tuning can mitigate this issue, its reliance on costly demonstration collection and intensive computation makes it impractical in real-world settings. In this work, we introduce VLA-Pilot, a plug-and-play inference-time policy steering method for zero-shot deployment of pre-trained VLA without any additional fine-tuning or data collection. We evaluate VLA-Pilot on six real-world downstream manipulation tasks across two distinct robotic embodiments, encompassing both in-distribution and out-of-distribution scenarios. Experimental results demonstrate that VLA-Pilot substantially boosts the success rates of off-the-shelf pre-trained VLA policies, enabling robust zero-shot generalization to diverse tasks and embodiments. Experimental videos and code are available at: this https URL. 

**Abstract (ZH)**: Vision-Language-Action (VLA)模型在实际机器人操作中展示了巨大的潜力。然而，预训练的VLA策略在下游部署时仍然遭受显著的性能退化。尽管微调可以缓解这一问题，但其对昂贵的数据演示收集和高强度计算的依赖使其在实际应用场景中不可行。在本文中，我们提出了VLA-Pilot，这是一种即插即用的推理时策略引导方法，可在无需任何额外微调或数据收集的情况下实现预训练VLA的零样本部署。我们在两个不同机器人实体的六个实际下游操作任务上评估了VLA-Pilot，涵盖分布内和分布外场景。实验结果表明，VLA-Pilot显著提升了现成的预训练VLA策略的成功率，使其能够实现对多种任务和实体的稳健零样本泛化。相关实验视频和代码可在以下链接获取：this https URL。 

---
# RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action 

**Title (ZH)**: RoboTidy : 一种用于体态导航和动作的3D高斯点状家庭整理基准测试 

**Authors**: Xiaoquan Sun, Ruijian Zhang, Kang Pang, Bingchen Miao, Yuxiang Tan, Zhen Yang, Ming Li, Jiayu Chen  

**Link**: [PDF](https://arxiv.org/pdf/2511.14161)  

**Abstract**: Household tidying is an important application area, yet current benchmarks neither model user preferences nor support mobility, and they generalize poorly, making it hard to comprehensively assess integrated language-to-action capabilities. To address this, we propose RoboTidy, a unified benchmark for language-guided household tidying that supports Vision-Language-Action (VLA) and Vision-Language-Navigation (VLN) training and evaluation. RoboTidy provides 500 photorealistic 3D Gaussian Splatting (3DGS) household scenes (covering 500 objects and containers) with collisions, formulates tidying as an "Action (Object, Container)" list, and supplies 6.4k high-quality manipulation demonstration trajectories and 1.5k naviagtion trajectories to support both few-shot and large-scale training. We also deploy RoboTidy in the real world for object tidying, establishing an end-to-end benchmark for household tidying. RoboTidy offers a scalable platform and bridges a key gap in embodied AI by enabling holistic and realistic evaluation of language-guided robots. 

**Abstract (ZH)**: RoboTidy：面向语言引导家庭整理的统一基准 

---
# RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action 

**Title (ZH)**: RoboTidy : 一种用于体态导航和操作的家庭整理3D高斯斑点基准测试 

**Authors**: Xiaoquan Sun, Ruijian Zhang, Kang Pang, Bingchen Miao, Yuxiang Tan, Zhen Yang, Ming Li, Jiayu Chen  

**Link**: [PDF](https://arxiv.org/pdf/2511.14161)  

**Abstract**: Household tidying is an important application area, yet current benchmarks neither model user preferences nor support mobility, and they generalize poorly, making it hard to comprehensively assess integrated language-to-action capabilities. To address this, we propose RoboTidy, a unified benchmark for language-guided household tidying that supports Vision-Language-Action (VLA) and Vision-Language-Navigation (VLN) training and evaluation. RoboTidy provides 500 photorealistic 3D Gaussian Splatting (3DGS) household scenes (covering 500 objects and containers) with collisions, formulates tidying as an "Action (Object, Container)" list, and supplies 6.4k high-quality manipulation demonstration trajectories and 1.5k naviagtion trajectories to support both few-shot and large-scale training. We also deploy RoboTidy in the real world for object tidying, establishing an end-to-end benchmark for household tidying. RoboTidy offers a scalable platform and bridges a key gap in embodied AI by enabling holistic and realistic evaluation of language-guided robots. 

**Abstract (ZH)**: RoboTidy：一种支持视觉-语言-动作和视觉-语言-导航的语言引导家庭整理统一基准 

---
# AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models 

**Title (ZH)**: AsyncVLA: 异步流匹配for 视觉-语言-动作模型 

**Authors**: Yuhua Jiang, Shuang Cheng, Yan Ding, Feifei Gao, Biqing Qi  

**Link**: [PDF](https://arxiv.org/pdf/2511.14148)  

**Abstract**: Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at this https URL. 

**Abstract (ZH)**: 具有异步流匹配的视觉-语言-动作模型（AsyncVLA）：一种新的框架及其应用 

---
# RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action 

**Title (ZH)**: RoboTidy : 一种用于物理导航和操作的三维高斯点家庭整理基准 

**Authors**: Xiaoquan Sun, Ruijian Zhang, Kang Pang, Bingchen Miao, Yuxiang Tan, Zhen Yang, Ming Li, Jiayu Chen  

**Link**: [PDF](https://arxiv.org/pdf/2511.14161)  

**Abstract**: Household tidying is an important application area, yet current benchmarks neither model user preferences nor support mobility, and they generalize poorly, making it hard to comprehensively assess integrated language-to-action capabilities. To address this, we propose RoboTidy, a unified benchmark for language-guided household tidying that supports Vision-Language-Action (VLA) and Vision-Language-Navigation (VLN) training and evaluation. RoboTidy provides 500 photorealistic 3D Gaussian Splatting (3DGS) household scenes (covering 500 objects and containers) with collisions, formulates tidying as an "Action (Object, Container)" list, and supplies 6.4k high-quality manipulation demonstration trajectories and 1.5k naviagtion trajectories to support both few-shot and large-scale training. We also deploy RoboTidy in the real world for object tidying, establishing an end-to-end benchmark for household tidying. RoboTidy offers a scalable platform and bridges a key gap in embodied AI by enabling holistic and realistic evaluation of language-guided robots. 

**Abstract (ZH)**: RoboTidy：一个支持视觉-语言-动作和视觉-语言-导航的居家整理统一基准 

---
# AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models 

**Title (ZH)**: AsyncVLA：视觉-语言-动作模型中的异步流匹配 

**Authors**: Yuhua Jiang, Shuang Cheng, Yan Ding, Feifei Gao, Biqing Qi  

**Link**: [PDF](https://arxiv.org/pdf/2511.14148)  

**Abstract**: Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at this https URL. 

**Abstract (ZH)**: 异步视图-语言-动作（AsyncVLA）模型：一种新型框架及其在机器人领域的应用 

---
# AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models 

**Title (ZH)**: AsyncVLA: 异步流匹配在视觉-语言-行动模型中的应用 

**Authors**: Yuhua Jiang, Shuang Cheng, Yan Ding, Feifei Gao, Biqing Qi  

**Link**: [PDF](https://arxiv.org/pdf/2511.14148)  

**Abstract**: Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at this https URL. 

**Abstract (ZH)**: 异步视觉-语言-动作模型：基于异步流匹配的时间灵活框架及其应用 

---
# AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models 

**Title (ZH)**: AsyncVLA: 异步流匹配的 vision-language-action 模型 

**Authors**: Yuhua Jiang, Shuang Cheng, Yan Ding, Feifei Gao, Biqing Qi  

**Link**: [PDF](https://arxiv.org/pdf/2511.14148)  

**Abstract**: Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at this https URL. 

**Abstract (ZH)**: 异步视觉-语言-行动（AsyncVLA）模型：时空灵活的动作生成框架 

---
# FlexiCup: Wireless Multimodal Suction Cup with Dual-Zone Vision-Tactile Sensing 

**Title (ZH)**: FlexiCup：具有双区视觉-触觉感知的无线多模态吸杯 

**Authors**: Junhao Gong, Shoujie Li, Kit-Wa Sou, Changqing Guo, Hourong Huang, Tong Wu, Yifan Xie, Chenxin Liang, Chuqiao Lyu, Xiaojun Liang, Wenbo Ding  

**Link**: [PDF](https://arxiv.org/pdf/2511.14139)  

**Abstract**: Conventional suction cups lack sensing capabilities for contact-aware manipulation in unstructured environments. This paper presents FlexiCup, a fully wireless multimodal suction cup that integrates dual-zone vision-tactile sensing. The central zone dynamically switches between vision and tactile modalities via illumination control for contact detection, while the peripheral zone provides continuous spatial awareness for approach planning. FlexiCup supports both vacuum and Bernoulli suction modes through modular mechanical configurations, achieving complete wireless autonomy with onboard computation and power. We validate hardware versatility through dual control paradigms. Modular perception-driven grasping across structured surfaces with varying obstacle densities demonstrates comparable performance between vacuum (90.0% mean success) and Bernoulli (86.7% mean success) modes. Diffusion-based end-to-end learning achieves 73.3% success on inclined transport and 66.7% on orange extraction tasks. Ablation studies confirm that multi-head attention coordinating dual-zone observations provides 13% improvements for contact-aware manipulation. Hardware designs and firmware are available at this https URL. 

**Abstract (ZH)**: 传统的吸盘缺乏在非结构化环境中接触感知能力的操控功能。本文提出了FlexiCup，这是一种集成双区视觉触觉传感功能的完全无线多模态吸盘。中央区通过照明控制动态切换视觉和触觉模态以进行接触检测，而外围区提供连续的空间感知以进行接近规划。FlexiCup通过模块化机械配置支持真空和伯努利两种吸盘模式，实现完全无线自主并配备 onboard 计算和电源。通过双控制范式验证硬件的灵活性。在不同障碍密度的结构化表面上实现感知驱动的模块化抓取显示真空模式（平均成功率90.0%）和伯努利模式（平均成功率86.7%）之间的可比性能。基于扩散的端到端学习在倾斜输送任务中成功率为73.3%，在橙子提取任务中为66.7%。消融研究证实，协调双区观察的多头注意力机制在接触感知操控中提供了13%的性能提升。硬件设计和固件可在以下链接获取：this https URL。 

---
# FlexiCup: Wireless Multimodal Suction Cup with Dual-Zone Vision-Tactile Sensing 

**Title (ZH)**: FlexiCup：具有双区视觉触觉传感的无线多功能吸杯 

**Authors**: Junhao Gong, Shoujie Li, Kit-Wa Sou, Changqing Guo, Hourong Huang, Tong Wu, Yifan Xie, Chenxin Liang, Chuqiao Lyu, Xiaojun Liang, Wenbo Ding  

**Link**: [PDF](https://arxiv.org/pdf/2511.14139)  

**Abstract**: Conventional suction cups lack sensing capabilities for contact-aware manipulation in unstructured environments. This paper presents FlexiCup, a fully wireless multimodal suction cup that integrates dual-zone vision-tactile sensing. The central zone dynamically switches between vision and tactile modalities via illumination control for contact detection, while the peripheral zone provides continuous spatial awareness for approach planning. FlexiCup supports both vacuum and Bernoulli suction modes through modular mechanical configurations, achieving complete wireless autonomy with onboard computation and power. We validate hardware versatility through dual control paradigms. Modular perception-driven grasping across structured surfaces with varying obstacle densities demonstrates comparable performance between vacuum (90.0% mean success) and Bernoulli (86.7% mean success) modes. Diffusion-based end-to-end learning achieves 73.3% success on inclined transport and 66.7% on orange extraction tasks. Ablation studies confirm that multi-head attention coordinating dual-zone observations provides 13% improvements for contact-aware manipulation. Hardware designs and firmware are available at this https URL. 

**Abstract (ZH)**: 传统吸盘缺乏在无结构环境中进行接触感知操作的能力。本文提出了一种集成了双区视觉触觉感知的完全无线多模态吸盘FlexiCup。中央区域通过照明控制在视觉和触觉感知之间动态切换以进行接触检测，而外围区域提供连续的空间感知以进行接近规划。FlexiCup通过模块化机械配置支持真空和伯努利抽吸模式，实现内置计算和电源的完全无线自主操作。我们通过双控制范式验证了硬件的多功能性。在不同障碍密度的结构表面实现感知驱动的抓取展示了真空模式（平均成功率90.0%）和伯努利模式（平均成功率86.7%）的可比性能。基于扩散的端到端学习在倾斜运输任务中取得了73.3%的成功率，在橙子提取任务中取得了66.7%的成功率。消融研究表明，协调双区观测的多头注意力机制可以提高13%的接触感知操作性能。硬件设计和固件可在以下链接获取。 

---
# FlexiCup: Wireless Multimodal Suction Cup with Dual-Zone Vision-Tactile Sensing 

**Title (ZH)**: FlexiCup：具有双区视觉-触觉感知的无线多模态吸盘 

**Authors**: Junhao Gong, Shoujie Li, Kit-Wa Sou, Changqing Guo, Hourong Huang, Tong Wu, Yifan Xie, Chenxin Liang, Chuqiao Lyu, Xiaojun Liang, Wenbo Ding  

**Link**: [PDF](https://arxiv.org/pdf/2511.14139)  

**Abstract**: Conventional suction cups lack sensing capabilities for contact-aware manipulation in unstructured environments. This paper presents FlexiCup, a fully wireless multimodal suction cup that integrates dual-zone vision-tactile sensing. The central zone dynamically switches between vision and tactile modalities via illumination control for contact detection, while the peripheral zone provides continuous spatial awareness for approach planning. FlexiCup supports both vacuum and Bernoulli suction modes through modular mechanical configurations, achieving complete wireless autonomy with onboard computation and power. We validate hardware versatility through dual control paradigms. Modular perception-driven grasping across structured surfaces with varying obstacle densities demonstrates comparable performance between vacuum (90.0% mean success) and Bernoulli (86.7% mean success) modes. Diffusion-based end-to-end learning achieves 73.3% success on inclined transport and 66.7% on orange extraction tasks. Ablation studies confirm that multi-head attention coordinating dual-zone observations provides 13% improvements for contact-aware manipulation. Hardware designs and firmware are available at this https URL. 

**Abstract (ZH)**: 传统吸盘缺乏在未结构化环境中进行接触感知操控的能力。本文介绍了一种名为FlexiCup的完全无线多模态吸盘，集成了双区视力触觉传感。中央区域通过照明控制动态切换视觉和触觉模态以进行接触检测，外围区域提供连续的空间感知以实现接近规划。FlexiCup通过模块化机械配置支持真空和伯努利吸盘模式，实现完全的无线自主操作，具备内部计算和电源。通过双控制模式验证了硬件的多功能性。在结构化表面不同障碍密度下，感知驱动的模块化抓取展示了真空模式（平均成功率为90.0%）和伯努利模式（平均成功率为86.7%）之间的相当性能。基于扩散的端到端学习在倾斜运输任务中的成功率为73.3%，在提取橙子任务中的成功率为66.7%。消融研究证实，协调双区观测的多头注意力机制提高了13%的接触感知操控性能。硬件设计和固件可在以下链接获取：这个=https://example.com（请替换为实际链接）。 

---
# BIM-Discrepancy-Driven Active Sensing for Risk-Aware UAV-UGV Navigation 

**Title (ZH)**: 基于BIM差异的主动传感以实现风险感知的无人机-地面机器人导航 

**Authors**: Hesam Mojtahedi, Reza Akhavian  

**Link**: [PDF](https://arxiv.org/pdf/2511.14037)  

**Abstract**: This paper presents a BIM-discrepancy-driven active sensing framework for cooperative navigation between unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) in dynamic construction environments. Traditional navigation approaches rely on static Building Information Modeling (BIM) priors or limited onboard perception. In contrast, our framework continuously fuses real-time LiDAR data from aerial and ground robots with BIM priors to maintain an evolving 2D occupancy map. We quantify navigation safety through a unified corridor-risk metric integrating occupancy uncertainty, BIM-map discrepancy, and clearance. When risk exceeds safety thresholds, the UAV autonomously re-scans affected regions to reduce uncertainty and enable safe replanning. Validation in PX4-Gazebo simulation with Robotec GPU LiDAR demonstrates that risk-triggered re-scanning reduces mean corridor risk by 58% and map entropy by 43% compared to static BIM navigation, while maintaining clearance margins above 0.4 m. Compared to frontier-based exploration, our approach achieves similar uncertainty reduction in half the mission time. These results demonstrate that integrating BIM priors with risk-adaptive aerial sensing enables scalable, uncertainty-aware autonomy for construction robotics. 

**Abstract (ZH)**: 基于BIM差异的自主感知识别框架在动态建筑环境中实现无人机与地面机器人协作导航 

---
# FlexiCup: Wireless Multimodal Suction Cup with Dual-Zone Vision-Tactile Sensing 

**Title (ZH)**: FlexiCup：具有双区视觉触觉感知的无线多模式吸杯 

**Authors**: Junhao Gong, Shoujie Li, Kit-Wa Sou, Changqing Guo, Hourong Huang, Tong Wu, Yifan Xie, Chenxin Liang, Chuqiao Lyu, Xiaojun Liang, Wenbo Ding  

**Link**: [PDF](https://arxiv.org/pdf/2511.14139)  

**Abstract**: Conventional suction cups lack sensing capabilities for contact-aware manipulation in unstructured environments. This paper presents FlexiCup, a fully wireless multimodal suction cup that integrates dual-zone vision-tactile sensing. The central zone dynamically switches between vision and tactile modalities via illumination control for contact detection, while the peripheral zone provides continuous spatial awareness for approach planning. FlexiCup supports both vacuum and Bernoulli suction modes through modular mechanical configurations, achieving complete wireless autonomy with onboard computation and power. We validate hardware versatility through dual control paradigms. Modular perception-driven grasping across structured surfaces with varying obstacle densities demonstrates comparable performance between vacuum (90.0% mean success) and Bernoulli (86.7% mean success) modes. Diffusion-based end-to-end learning achieves 73.3% success on inclined transport and 66.7% on orange extraction tasks. Ablation studies confirm that multi-head attention coordinating dual-zone observations provides 13% improvements for contact-aware manipulation. Hardware designs and firmware are available at this https URL. 

**Abstract (ZH)**: 传统的吸盘缺乏在未结构化环境中进行接触感知操作的能力。本文介绍了一种名为FlexiCup的全无线多模式吸盘，集成了双区视觉触觉感知。中央区域通过照明控制动态切换视觉和触觉模态进行接触检测，而外围区域提供连续的空间感知以供接近规划。FlexiCup通过模块化机械配置支持真空和伯努利吸取模式，实现搭载计算和电源的完全无线自主操作。我们通过双控制范式验证了硬件的灵活性。在不同障碍密度的结构化表面上进行模块化感知驱动的抓取展示了真空模式（平均成功率90.0%）与伯努利模式（平均成功率86.7%）之间的相似性能。基于扩散的端到端学习在倾斜运输任务中实现了73.3%的成功率，在橙子提取任务中实现了66.7%的成功率。消融研究证实，协调双区观察的多头注意力机制可提高接触感知操作13%。硬件设计和固件可在以下链接获取：this https URL。 

---
# BIM-Discrepancy-Driven Active Sensing for Risk-Aware UAV-UGV Navigation 

**Title (ZH)**: 基于BIM差异的主动传感风险感知UAV-UGV导航 

**Authors**: Hesam Mojtahedi, Reza Akhavian  

**Link**: [PDF](https://arxiv.org/pdf/2511.14037)  

**Abstract**: This paper presents a BIM-discrepancy-driven active sensing framework for cooperative navigation between unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) in dynamic construction environments. Traditional navigation approaches rely on static Building Information Modeling (BIM) priors or limited onboard perception. In contrast, our framework continuously fuses real-time LiDAR data from aerial and ground robots with BIM priors to maintain an evolving 2D occupancy map. We quantify navigation safety through a unified corridor-risk metric integrating occupancy uncertainty, BIM-map discrepancy, and clearance. When risk exceeds safety thresholds, the UAV autonomously re-scans affected regions to reduce uncertainty and enable safe replanning. Validation in PX4-Gazebo simulation with Robotec GPU LiDAR demonstrates that risk-triggered re-scanning reduces mean corridor risk by 58% and map entropy by 43% compared to static BIM navigation, while maintaining clearance margins above 0.4 m. Compared to frontier-based exploration, our approach achieves similar uncertainty reduction in half the mission time. These results demonstrate that integrating BIM priors with risk-adaptive aerial sensing enables scalable, uncertainty-aware autonomy for construction robotics. 

**Abstract (ZH)**: 基于BIM差异的自主感应框架：无人机与无人地面车辆在动态建设环境中的协同导航 

---
# BIM-Discrepancy-Driven Active Sensing for Risk-Aware UAV-UGV Navigation 

**Title (ZH)**: 基于BIM差异的主动传感以实现风险意识的无人机-地面机器人导航 

**Authors**: Hesam Mojtahedi, Reza Akhavian  

**Link**: [PDF](https://arxiv.org/pdf/2511.14037)  

**Abstract**: This paper presents a BIM-discrepancy-driven active sensing framework for cooperative navigation between unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) in dynamic construction environments. Traditional navigation approaches rely on static Building Information Modeling (BIM) priors or limited onboard perception. In contrast, our framework continuously fuses real-time LiDAR data from aerial and ground robots with BIM priors to maintain an evolving 2D occupancy map. We quantify navigation safety through a unified corridor-risk metric integrating occupancy uncertainty, BIM-map discrepancy, and clearance. When risk exceeds safety thresholds, the UAV autonomously re-scans affected regions to reduce uncertainty and enable safe replanning. Validation in PX4-Gazebo simulation with Robotec GPU LiDAR demonstrates that risk-triggered re-scanning reduces mean corridor risk by 58% and map entropy by 43% compared to static BIM navigation, while maintaining clearance margins above 0.4 m. Compared to frontier-based exploration, our approach achieves similar uncertainty reduction in half the mission time. These results demonstrate that integrating BIM priors with risk-adaptive aerial sensing enables scalable, uncertainty-aware autonomy for construction robotics. 

**Abstract (ZH)**: 基于BIM差异驱动的主动传感框架：在动态施工环境中的无人机与地面机器人协同导航 

---
# FACA: Fair and Agile Multi-Robot Collision Avoidance in Constrained Environments with Dynamic Priorities 

**Title (ZH)**: FACA：在约束环境中具有动态优先级的公平灵活多机器人碰撞避免算法 

**Authors**: Jaskirat Singh, Rohan Chandra  

**Link**: [PDF](https://arxiv.org/pdf/2511.14024)  

**Abstract**: Multi-robot systems are increasingly being used for critical applications such as rescuing injured people, delivering food and medicines, and monitoring key areas. These applications usually involve navigating at high speeds through constrained spaces such as small gaps. Navigating such constrained spaces becomes particularly challenging when the space is crowded with multiple heterogeneous agents all of which have urgent priorities. What makes the problem even harder is that during an active response situation, roles and priorities can quickly change on a dime without informing the other agents. In order to complete missions in such environments, robots must not only be safe, but also agile, able to dodge and change course at a moment's notice. In this paper, we propose FACA, a fair and agile collision avoidance approach where robots coordinate their tasks by talking to each other via natural language (just as people do). In FACA, robots balance safety with agility via a novel artificial potential field algorithm that creates an automatic ``roundabout'' effect whenever a conflict arises. Our experiments show that FACA achieves a improvement in efficiency, completing missions more than 3.5X faster than baselines with a time reduction of over 70% while maintaining robust safety margins. 

**Abstract (ZH)**: 多机器人系统在救援伤员、配送食物和药品以及监控关键区域等关键应用中的敏捷公平避碰方法 

---
# BIM-Discrepancy-Driven Active Sensing for Risk-Aware UAV-UGV Navigation 

**Title (ZH)**: 基于BIM差异的主动感知风险意识UAV-UGV导航 

**Authors**: Hesam Mojtahedi, Reza Akhavian  

**Link**: [PDF](https://arxiv.org/pdf/2511.14037)  

**Abstract**: This paper presents a BIM-discrepancy-driven active sensing framework for cooperative navigation between unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) in dynamic construction environments. Traditional navigation approaches rely on static Building Information Modeling (BIM) priors or limited onboard perception. In contrast, our framework continuously fuses real-time LiDAR data from aerial and ground robots with BIM priors to maintain an evolving 2D occupancy map. We quantify navigation safety through a unified corridor-risk metric integrating occupancy uncertainty, BIM-map discrepancy, and clearance. When risk exceeds safety thresholds, the UAV autonomously re-scans affected regions to reduce uncertainty and enable safe replanning. Validation in PX4-Gazebo simulation with Robotec GPU LiDAR demonstrates that risk-triggered re-scanning reduces mean corridor risk by 58% and map entropy by 43% compared to static BIM navigation, while maintaining clearance margins above 0.4 m. Compared to frontier-based exploration, our approach achieves similar uncertainty reduction in half the mission time. These results demonstrate that integrating BIM priors with risk-adaptive aerial sensing enables scalable, uncertainty-aware autonomy for construction robotics. 

**Abstract (ZH)**: 基于BIM数据驱动的主动传感框架：在动态施工环境中的无人机与地面机器人协作导航 

---
# FACA: Fair and Agile Multi-Robot Collision Avoidance in Constrained Environments with Dynamic Priorities 

**Title (ZH)**: FACA：在动态优先级约束环境中的公平灵活多机器人碰撞避障 

**Authors**: Jaskirat Singh, Rohan Chandra  

**Link**: [PDF](https://arxiv.org/pdf/2511.14024)  

**Abstract**: Multi-robot systems are increasingly being used for critical applications such as rescuing injured people, delivering food and medicines, and monitoring key areas. These applications usually involve navigating at high speeds through constrained spaces such as small gaps. Navigating such constrained spaces becomes particularly challenging when the space is crowded with multiple heterogeneous agents all of which have urgent priorities. What makes the problem even harder is that during an active response situation, roles and priorities can quickly change on a dime without informing the other agents. In order to complete missions in such environments, robots must not only be safe, but also agile, able to dodge and change course at a moment's notice. In this paper, we propose FACA, a fair and agile collision avoidance approach where robots coordinate their tasks by talking to each other via natural language (just as people do). In FACA, robots balance safety with agility via a novel artificial potential field algorithm that creates an automatic ``roundabout'' effect whenever a conflict arises. Our experiments show that FACA achieves a improvement in efficiency, completing missions more than 3.5X faster than baselines with a time reduction of over 70% while maintaining robust safety margins. 

**Abstract (ZH)**: 多机器人系统在救援受伤人员、运送食品和药品以及监管关键区域等关键应用中的高效避障方法：基于自然语言通信的公平敏捷碰撞避免方法（FACA） 

---
# FACA: Fair and Agile Multi-Robot Collision Avoidance in Constrained Environments with Dynamic Priorities 

**Title (ZH)**: FACA：受限环境中文动态优先级下的公平灵活多机器人碰撞避障算法 

**Authors**: Jaskirat Singh, Rohan Chandra  

**Link**: [PDF](https://arxiv.org/pdf/2511.14024)  

**Abstract**: Multi-robot systems are increasingly being used for critical applications such as rescuing injured people, delivering food and medicines, and monitoring key areas. These applications usually involve navigating at high speeds through constrained spaces such as small gaps. Navigating such constrained spaces becomes particularly challenging when the space is crowded with multiple heterogeneous agents all of which have urgent priorities. What makes the problem even harder is that during an active response situation, roles and priorities can quickly change on a dime without informing the other agents. In order to complete missions in such environments, robots must not only be safe, but also agile, able to dodge and change course at a moment's notice. In this paper, we propose FACA, a fair and agile collision avoidance approach where robots coordinate their tasks by talking to each other via natural language (just as people do). In FACA, robots balance safety with agility via a novel artificial potential field algorithm that creates an automatic ``roundabout'' effect whenever a conflict arises. Our experiments show that FACA achieves a improvement in efficiency, completing missions more than 3.5X faster than baselines with a time reduction of over 70% while maintaining robust safety margins. 

**Abstract (ZH)**: 多机器人系统在救援受伤人员、配送食品和药物以及监控关键区域等关键应用中的自主导航 

---
# FACA: Fair and Agile Multi-Robot Collision Avoidance in Constrained Environments with Dynamic Priorities 

**Title (ZH)**: FACA：在受限环境中的动态优先级公平灵活多机器人碰撞避免 

**Authors**: Jaskirat Singh, Rohan Chandra  

**Link**: [PDF](https://arxiv.org/pdf/2511.14024)  

**Abstract**: Multi-robot systems are increasingly being used for critical applications such as rescuing injured people, delivering food and medicines, and monitoring key areas. These applications usually involve navigating at high speeds through constrained spaces such as small gaps. Navigating such constrained spaces becomes particularly challenging when the space is crowded with multiple heterogeneous agents all of which have urgent priorities. What makes the problem even harder is that during an active response situation, roles and priorities can quickly change on a dime without informing the other agents. In order to complete missions in such environments, robots must not only be safe, but also agile, able to dodge and change course at a moment's notice. In this paper, we propose FACA, a fair and agile collision avoidance approach where robots coordinate their tasks by talking to each other via natural language (just as people do). In FACA, robots balance safety with agility via a novel artificial potential field algorithm that creates an automatic ``roundabout'' effect whenever a conflict arises. Our experiments show that FACA achieves a improvement in efficiency, completing missions more than 3.5X faster than baselines with a time reduction of over 70% while maintaining robust safety margins. 

**Abstract (ZH)**: 多机器人系统在关键应用中的敏捷避碰方法：基于自然语言的公平敏捷避碰（FACA） 

---
# Searching in Space and Time: Unified Memory-Action Loops for Open-World Object Retrieval 

**Title (ZH)**: 在空间与时间中搜索：开放世界物体检索的统一记忆-动作循环 

**Authors**: Taijing Chen, Sateesh Kumar, Junhong Xu, George Pavlakos, J oydeep Biswas, Roberto Martín-Martín  

**Link**: [PDF](https://arxiv.org/pdf/2511.14004)  

**Abstract**: Service robots must retrieve objects in dynamic, open-world settings where requests may reference attributes ("the red mug"), spatial context ("the mug on the table"), or past states ("the mug that was here yesterday"). Existing approaches capture only parts of this problem: scene graphs capture spatial relations but ignore temporal grounding, temporal reasoning methods model dynamics but do not support embodied interaction, and dynamic scene graphs handle both but remain closed-world with fixed vocabularies. We present STAR (SpatioTemporal Active Retrieval), a framework that unifies memory queries and embodied actions within a single decision loop. STAR leverages non-parametric long-term memory and a working memory to support efficient recall, and uses a vision-language model to select either temporal or spatial actions at each step. We introduce STARBench, a benchmark of spatiotemporal object search tasks across simulated and real environments. Experiments in STARBench and on a Tiago robot show that STAR consistently outperforms scene-graph and memory-only baselines, demonstrating the benefits of treating search in time and search in space as a unified problem. 

**Abstract (ZH)**: 服务机器人必须在动态的开放世界环境中检索物体，这些环境中的请求可能参考属性（“那个红色的杯子”）、空间上下文（“桌子上的杯子”）或过去状态（“昨天在这里的杯子”）。现有方法只解决了这一问题的部分方面：场景图捕捉空间关系但忽略时间接地，时间推理方法建模动态但不支持实体交互，动态场景图同时处理上述问题但仍然保持封闭世界且具有固定词汇表。我们提出了STAR（SpatioTemporal Active Retrieval）框架，该框架在一个决策循环中统一了记忆查询和实体动作。STAR 利用非参数的长期记忆和工作记忆以支持高效的回忆，并使用视觉语言模型在每一步选择时间或空间动作。我们介绍了STARBench这一基准，用于跨模拟和实际环境下的时空物体搜索任务。在STARBench和Tiago机器人上的实验表明，STAR 一致地优于场景图和仅基于记忆的基线，展示了将时间中的搜索和空间中的搜索视为统一问题的好处。 

---
# Searching in Space and Time: Unified Memory-Action Loops for Open-World Object Retrieval 

**Title (ZH)**: 在空间与时间中搜索：开放世界物体检索的统一记忆-行动循环 

**Authors**: Taijing Chen, Sateesh Kumar, Junhong Xu, George Pavlakos, J oydeep Biswas, Roberto Martín-Martín  

**Link**: [PDF](https://arxiv.org/pdf/2511.14004)  

**Abstract**: Service robots must retrieve objects in dynamic, open-world settings where requests may reference attributes ("the red mug"), spatial context ("the mug on the table"), or past states ("the mug that was here yesterday"). Existing approaches capture only parts of this problem: scene graphs capture spatial relations but ignore temporal grounding, temporal reasoning methods model dynamics but do not support embodied interaction, and dynamic scene graphs handle both but remain closed-world with fixed vocabularies. We present STAR (SpatioTemporal Active Retrieval), a framework that unifies memory queries and embodied actions within a single decision loop. STAR leverages non-parametric long-term memory and a working memory to support efficient recall, and uses a vision-language model to select either temporal or spatial actions at each step. We introduce STARBench, a benchmark of spatiotemporal object search tasks across simulated and real environments. Experiments in STARBench and on a Tiago robot show that STAR consistently outperforms scene-graph and memory-only baselines, demonstrating the benefits of treating search in time and search in space as a unified problem. 

**Abstract (ZH)**: 服务机器人必须在动态的开放环境中检索对象，这些环境中的请求可能引用属性（“红色的茶杯”）、空间上下文（“桌子上的茶杯”）或过去状态（“昨天在这里的茶杯”）。现有的方法只解决了其中一部分问题：场景图捕获空间关系但忽略时间上下文，时间推理方法建模动力学但不支持实体交互，动态场景图同时处理两者但仍是封闭世界，具有固定词汇表。我们提出了STAR（时空主动检索）框架，该框架在一个决策循环中统一了记忆查询和实体动作。STAR利用非参数长期记忆和工作记忆来支持高效的回忆，并使用视觉语言模型在每一步选择时间或空间操作。我们引入了STARBench基准，该基准涵盖了模拟和真实环境中的时空对象搜索任务。STARBench和Tiago机器人上的实验表明，STAR一致地优于场景图和仅有记忆的基础方法，展示了将时间和空间中的搜索视为统一问题的好处。 

---
# Searching in Space and Time: Unified Memory-Action Loops for Open-World Object Retrieval 

**Title (ZH)**: 时空搜索：开放世界物体检索的统一记忆-行动循环 

**Authors**: Taijing Chen, Sateesh Kumar, Junhong Xu, George Pavlakos, J oydeep Biswas, Roberto Martín-Martín  

**Link**: [PDF](https://arxiv.org/pdf/2511.14004)  

**Abstract**: Service robots must retrieve objects in dynamic, open-world settings where requests may reference attributes ("the red mug"), spatial context ("the mug on the table"), or past states ("the mug that was here yesterday"). Existing approaches capture only parts of this problem: scene graphs capture spatial relations but ignore temporal grounding, temporal reasoning methods model dynamics but do not support embodied interaction, and dynamic scene graphs handle both but remain closed-world with fixed vocabularies. We present STAR (SpatioTemporal Active Retrieval), a framework that unifies memory queries and embodied actions within a single decision loop. STAR leverages non-parametric long-term memory and a working memory to support efficient recall, and uses a vision-language model to select either temporal or spatial actions at each step. We introduce STARBench, a benchmark of spatiotemporal object search tasks across simulated and real environments. Experiments in STARBench and on a Tiago robot show that STAR consistently outperforms scene-graph and memory-only baselines, demonstrating the benefits of treating search in time and search in space as a unified problem. 

**Abstract (ZH)**: 服务机器人必须在动态的开放环境中检索对象，这些环境中的请求可能会引用属性（“那个红色的咖啡杯”）、空间上下文（“桌子上的那个咖啡杯”）或过去的状态（“昨天在这里的那个咖啡杯”）。现有的方法只能解决这个问题的部分方面：场景图可以捕捉空间关系但忽略了时间关联，时间推理方法可以建模动态过程但不支持具身交互，而动态场景图可以同时处理这两个方面但仍然是封闭世界的，具有固定词汇表。我们提出了STAR（SpatioTemporal Active Retrieval）框架，将记忆查询和具身动作统一在一个决策循环中。STAR利用非参数化的长时记忆和工作记忆来支持高效回忆，并使用视觉语言模型在每一步选择时间或空间动作。我们引入了STARBench基准测试，涵盖模拟和现实环境中的时空对象搜索任务。在STARBench和Tiago机器人上的实验表明，STAR稳定地优于场景图和仅记忆基准，展示了将时间中的搜索和空间中的搜索视为统一问题的好处。 

---
# LIO-MARS: Non-uniform Continuous-time Trajectories for Real-time LiDAR-Inertial-Odometry 

**Title (ZH)**: LIO-MARS: 非均匀连续时间轨迹用于实时激光雷达-惯性里程计 

**Authors**: Jan Quenzel, Sven Behnke  

**Link**: [PDF](https://arxiv.org/pdf/2511.13985)  

**Abstract**: Autonomous robotic systems heavily rely on environment knowledge to safely navigate. For search & rescue, a flying robot requires robust real-time perception, enabled by complementary sensors. IMU data constrains acceleration and rotation, whereas LiDAR measures accurate distances around the robot. Building upon the LiDAR odometry MARS, our LiDAR-inertial odometry (LIO) jointly aligns multi-resolution surfel maps with a Gaussian mixture model (GMM) using a continuous-time B-spline trajectory. Our new scan window uses non-uniform temporal knot placement to ensure continuity over the whole trajectory without additional scan delay. Moreover, we accelerate essential covariance and GMM computations with Kronecker sums and products by a factor of 3.3. An unscented transform de-skews surfels, while a splitting into intra-scan segments facilitates motion compensation during spline optimization. Complementary soft constraints on relative poses and preintegrated IMU pseudo-measurements further improve robustness and accuracy. Extensive evaluation showcases the state-of-the-art quality of our LIO-MARS w.r.t. recent LIO systems on various handheld, ground and aerial vehicle-based datasets. 

**Abstract (ZH)**: 自主机器人系统强烈依赖环境知识以安全导航。在搜救任务中，飞行机器人需要 robust 实时感知，这得益于互补传感器。惯性测量单元 (IMU) 数据限制加速度和旋转，而 LiDAR 测量机器人周围准确的距离。建立在 LiDAR 奥迪وم特里 MARS 的基础上，我们的 LiDAR-惯性奥迪瘤特里（LIO）通过使用连续时间 B-样条轨迹联合对多分辨率 surfel 地图进行高斯混合模型（GMM）对齐。我们提出的新扫描窗口使用非均匀时间间隔的结点放置，以确保在整个轨迹上连续性而不增加额外的扫描延迟。此外，我们通过克罗内克和积运算将关键的协方差和 GMM 计算加速 3.3 倍。用无迹变换校正 surfel，将扫描段分割为内部段简化了在样条优化期间的运动补偿。相对姿态的互补软约束和预整合 IMU 伪度量进一步提高了鲁棒性和准确性。广泛的评估展示了与最近的 LIO 系统相比，我们的 LIO-MARS 在多种手持、地面和空中车辆数据集上的先进质量。 

---
# Searching in Space and Time: Unified Memory-Action Loops for Open-World Object Retrieval 

**Title (ZH)**: 在空间与时间中搜索：开放世界物体检索的统一记忆-动作循环 

**Authors**: Taijing Chen, Sateesh Kumar, Junhong Xu, George Pavlakos, J oydeep Biswas, Roberto Martín-Martín  

**Link**: [PDF](https://arxiv.org/pdf/2511.14004)  

**Abstract**: Service robots must retrieve objects in dynamic, open-world settings where requests may reference attributes ("the red mug"), spatial context ("the mug on the table"), or past states ("the mug that was here yesterday"). Existing approaches capture only parts of this problem: scene graphs capture spatial relations but ignore temporal grounding, temporal reasoning methods model dynamics but do not support embodied interaction, and dynamic scene graphs handle both but remain closed-world with fixed vocabularies. We present STAR (SpatioTemporal Active Retrieval), a framework that unifies memory queries and embodied actions within a single decision loop. STAR leverages non-parametric long-term memory and a working memory to support efficient recall, and uses a vision-language model to select either temporal or spatial actions at each step. We introduce STARBench, a benchmark of spatiotemporal object search tasks across simulated and real environments. Experiments in STARBench and on a Tiago robot show that STAR consistently outperforms scene-graph and memory-only baselines, demonstrating the benefits of treating search in time and search in space as a unified problem. 

**Abstract (ZH)**: 服务机器人必须在动态的开放世界环境中检索对象，这些请求可能引用属性（“那个红色的茶杯”）、空间上下文（“桌子上的茶杯”）或过去的状态（“昨天在这儿的茶杯”）。现有方法仅解决了该问题的一部分：场景图捕捉空间关系但忽略了时间grounding，时间推理方法建模动力学但不支持实物互动，而动态场景图同时处理这两方面，但仍然局限于固定的词汇表。我们提出了STAR（时空主动检索）框架，该框架在单一决策循环内统一了记忆查询和实物操作。STAR利用非参数长时记忆和工作记忆来支持高效的回忆，并使用视觉-语言模型在每一步选择时间或空间操作。我们引入了STARBench基准测试，涵盖模拟和真实环境中的时空对象搜索任务。STARBench和Tiago机器人上的实验表明，STAR在时空检索方面始终优于场景图和仅记忆的基线，展示了将时间中的搜索和空间中的搜索视为统一问题的好处。 

---
# LIO-MARS: Non-uniform Continuous-time Trajectories for Real-time LiDAR-Inertial-Odometry 

**Title (ZH)**: LIO-MARS：非均匀连续时间轨迹的LiDAR-惯性-里程计 

**Authors**: Jan Quenzel, Sven Behnke  

**Link**: [PDF](https://arxiv.org/pdf/2511.13985)  

**Abstract**: Autonomous robotic systems heavily rely on environment knowledge to safely navigate. For search & rescue, a flying robot requires robust real-time perception, enabled by complementary sensors. IMU data constrains acceleration and rotation, whereas LiDAR measures accurate distances around the robot. Building upon the LiDAR odometry MARS, our LiDAR-inertial odometry (LIO) jointly aligns multi-resolution surfel maps with a Gaussian mixture model (GMM) using a continuous-time B-spline trajectory. Our new scan window uses non-uniform temporal knot placement to ensure continuity over the whole trajectory without additional scan delay. Moreover, we accelerate essential covariance and GMM computations with Kronecker sums and products by a factor of 3.3. An unscented transform de-skews surfels, while a splitting into intra-scan segments facilitates motion compensation during spline optimization. Complementary soft constraints on relative poses and preintegrated IMU pseudo-measurements further improve robustness and accuracy. Extensive evaluation showcases the state-of-the-art quality of our LIO-MARS w.r.t. recent LIO systems on various handheld, ground and aerial vehicle-based datasets. 

**Abstract (ZH)**: 自主机器人系统高度依赖环境知识以实现安全导航。在搜索与救援任务中，飞行机器人需要通过互补传感器实现稳健的实时感知。IMU数据约束加速度和旋转，而LiDAR测量机器人周围准确的距离。基于LiDAR里程计MARS，我们的LiDAR-惯性里程计（LIO）使用连续时间B-样条轨迹联合对多分辨率_surfel_地图进行高斯混合模型（GMM）对齐。我们的新扫描窗口通过非均匀时间节点布局确保整个轨迹上的连续性而无需额外的扫描延迟。此外，我们通过Kronecker和乘积加速关键的协方差和GMM计算3.3倍。未受限制的变化器使surfel直立，而扫描内分割便于在样条优化期间进行运动补偿。相对pose的互补软约束和预积分IMU伪测量进一步提高了鲁棒性和准确性。广泛的评估展示了我们LIO-MARS相对于近期LIO系统的卓越质量，尤其是在各种手持、地面和空中车辆数据集上的表现。 

---
# LIO-MARS: Non-uniform Continuous-time Trajectories for Real-time LiDAR-Inertial-Odometry 

**Title (ZH)**: LIO-MARS：非均匀连续时间轨迹用于实时LiDAR-惯性-里程计 

**Authors**: Jan Quenzel, Sven Behnke  

**Link**: [PDF](https://arxiv.org/pdf/2511.13985)  

**Abstract**: Autonomous robotic systems heavily rely on environment knowledge to safely navigate. For search & rescue, a flying robot requires robust real-time perception, enabled by complementary sensors. IMU data constrains acceleration and rotation, whereas LiDAR measures accurate distances around the robot. Building upon the LiDAR odometry MARS, our LiDAR-inertial odometry (LIO) jointly aligns multi-resolution surfel maps with a Gaussian mixture model (GMM) using a continuous-time B-spline trajectory. Our new scan window uses non-uniform temporal knot placement to ensure continuity over the whole trajectory without additional scan delay. Moreover, we accelerate essential covariance and GMM computations with Kronecker sums and products by a factor of 3.3. An unscented transform de-skews surfels, while a splitting into intra-scan segments facilitates motion compensation during spline optimization. Complementary soft constraints on relative poses and preintegrated IMU pseudo-measurements further improve robustness and accuracy. Extensive evaluation showcases the state-of-the-art quality of our LIO-MARS w.r.t. recent LIO systems on various handheld, ground and aerial vehicle-based datasets. 

**Abstract (ZH)**: 自主机器人系统高度依赖环境知识以实现安全导航。对于搜索与救援任务，飞行机器人需要强大的实时感知能力，这得益于互补传感器的支持。惯性测量单元（IMU）数据约束加速度和旋转，而激光雷达（LiDAR）测量机器人周围准确的距离。基于LiDAR里程计MARS，我们的LiDAR-惯性里程计（LIO）联合使用连续时间B-样条轨迹与高斯混合模型（GMM），对多分辨率surfel地图进行对齐。我们新的扫描窗口采用非均匀时间节段分布，以确保整个轨迹的连续性，而不增加额外的扫描延迟。此外，我们通过Kronecker和积加速关键的协方差和GMM计算，加速比达到3.3倍。使用无迹变换纠偏surfel，将扫描段划分为内部段落有助于在样条优化期间进行运动补偿。相对位姿的互补软约束和IMU预积分伪测量进一步提高稳健性和准确性。广泛的评估展示了我们的LIO-MARS在各种手持、地面和航空车辆数据集上优于近期的LIO系统。 

---
# FICO: Finite-Horizon Closed-Loop Factorization for Unified Multi-Agent Path Finding 

**Title (ZH)**: FICO: 有限时限闭环因子分解统一多agent路径规划 

**Authors**: Jiarui Li, Alessandro Zanardi, Runyu Zhang, Gioele Zardini  

**Link**: [PDF](https://arxiv.org/pdf/2511.13961)  

**Abstract**: Multi-Agent Path Finding is a fundamental problem in robotics and AI, yet most existing formulations treat planning and execution separately and address variants of the problem in an ad hoc manner. This paper presents a system-level framework for MAPF that integrates planning and execution, generalizes across variants, and explicitly models uncertainties. At its core is the MAPF system, a formal model that casts MAPF as a control design problem encompassing classical and uncertainty-aware formulations. To solve it, we introduce Finite-Horizon Closed-Loop Factorization (FICO), a factorization-based algorithm inspired by receding-horizon control that exploits compositional structure for efficient closed-loop operation. FICO enables real-time responses -- commencing execution within milliseconds -- while scaling to thousands of agents and adapting seamlessly to execution-time uncertainties. Extensive case studies demonstrate that it reduces computation time by up to two orders of magnitude compared with open-loop baselines, while delivering significantly higher throughput under stochastic delays and agent arrivals. These results establish a principled foundation for analyzing and advancing MAPF through system-level modeling, factorization, and closed-loop design. 

**Abstract (ZH)**: 多智能体路径规划是机器人学和AI领域的基础问题，但现有的大多数形式分离了规划与执行，并以非标准化方式处理该问题的不同变体。本文提出了一种综合规划与执行、跨不同变体进行泛化并明确建模不确定性的系统级框架。其核心是多智能体路径规划系统，这是一种形式化模型，将多智能体路径规划问题视为包含经典和不确定性意识表述的控制设计问题。为了解决这一问题，我们引入了有限 horizon 闭环因子分解（FICO）算法，这是一种受到滞后控制启发的基于因子分解的算法，利用组合结构实现高效的闭环操作。FICO能够在毫秒内启动执行，同时支持数千个智能体，并且能够无缝适应执行时的不确定性。广泛的案例研究显示，与开环基线相比，它能够减少计算时间两个数量级，并且在随机延迟和智能体到达的情况下能实现显著更高的吞吐量。这些结果为通过系统级建模、因子分解和闭环设计分析和推进多智能体路径规划奠定了原则性基础。 

---
# LIO-MARS: Non-uniform Continuous-time Trajectories for Real-time LiDAR-Inertial-Odometry 

**Title (ZH)**: LIO-MARS：非均匀连续时间轨迹的LiDAR-惯性-.odometry-real-time 

**Authors**: Jan Quenzel, Sven Behnke  

**Link**: [PDF](https://arxiv.org/pdf/2511.13985)  

**Abstract**: Autonomous robotic systems heavily rely on environment knowledge to safely navigate. For search & rescue, a flying robot requires robust real-time perception, enabled by complementary sensors. IMU data constrains acceleration and rotation, whereas LiDAR measures accurate distances around the robot. Building upon the LiDAR odometry MARS, our LiDAR-inertial odometry (LIO) jointly aligns multi-resolution surfel maps with a Gaussian mixture model (GMM) using a continuous-time B-spline trajectory. Our new scan window uses non-uniform temporal knot placement to ensure continuity over the whole trajectory without additional scan delay. Moreover, we accelerate essential covariance and GMM computations with Kronecker sums and products by a factor of 3.3. An unscented transform de-skews surfels, while a splitting into intra-scan segments facilitates motion compensation during spline optimization. Complementary soft constraints on relative poses and preintegrated IMU pseudo-measurements further improve robustness and accuracy. Extensive evaluation showcases the state-of-the-art quality of our LIO-MARS w.r.t. recent LIO systems on various handheld, ground and aerial vehicle-based datasets. 

**Abstract (ZH)**: 自主机器人系统强烈依赖环境知识以实现安全导航。在搜索与救援任务中，飞行机器人需要 robust 的实时感知能力，这得益于互补传感器的应用。IMU 数据限制加速度和旋转，而 LiDAR 测量机器人周围精确的距离。基于 LiDAR 里程计 MARS，我们提出的 LiDAR-惯性里程计 (LIO) 使用高斯混合模型 (GMM) 和连续时间 B-样条轨迹联合对多分辨率 surfel 地图进行对齐。我们的新扫描窗口通过非均匀时间结点放置来确保轨迹全程连续性，而无需额外的扫描延迟。此外，我们通过克罗内cker和积加速关键的协方差和 GMM 计算，加速比达到 3.3。使用无迹变换 (unscented transform) 使 surfel 偏斜，而将扫描区间分割成子段有助于在样条优化过程中进行运动补偿。相对姿态的互补软约束和预整合 IMU 模态进一步提高鲁棒性和准确性。广泛的评估展示了我们的 LIO-MARS 在多种手持、地面和空中机器人数据集上的先进质量，与最近的 LIO 系统相比。 

---
# FICO: Finite-Horizon Closed-Loop Factorization for Unified Multi-Agent Path Finding 

**Title (ZH)**: FICO: 有限时限闭环因子分解统一多智能体路径规划 

**Authors**: Jiarui Li, Alessandro Zanardi, Runyu Zhang, Gioele Zardini  

**Link**: [PDF](https://arxiv.org/pdf/2511.13961)  

**Abstract**: Multi-Agent Path Finding is a fundamental problem in robotics and AI, yet most existing formulations treat planning and execution separately and address variants of the problem in an ad hoc manner. This paper presents a system-level framework for MAPF that integrates planning and execution, generalizes across variants, and explicitly models uncertainties. At its core is the MAPF system, a formal model that casts MAPF as a control design problem encompassing classical and uncertainty-aware formulations. To solve it, we introduce Finite-Horizon Closed-Loop Factorization (FICO), a factorization-based algorithm inspired by receding-horizon control that exploits compositional structure for efficient closed-loop operation. FICO enables real-time responses -- commencing execution within milliseconds -- while scaling to thousands of agents and adapting seamlessly to execution-time uncertainties. Extensive case studies demonstrate that it reduces computation time by up to two orders of magnitude compared with open-loop baselines, while delivering significantly higher throughput under stochastic delays and agent arrivals. These results establish a principled foundation for analyzing and advancing MAPF through system-level modeling, factorization, and closed-loop design. 

**Abstract (ZH)**: 多代理路径规划是机器人学和AI领域的基础问题，然而大多数现有模型将规划和执行分开处理，并以半成品的方式解决该问题的变体。本文提出了一种系统级框架，该框架将规划和执行整合在一起，跨越不同变体进行泛化，并明确建模不确定性。核心是MAPF系统，一个形式化模型，将MAPF问题表述为一个包含经典和不确定性aware形式化问题的控制设计问题。为了解决这个问题，我们引入了有限时间闭环因子分解（FICO），这是一种受到预测性控制启发的基于因子分解的算法，利用可组合结构实现高效的闭环操作。FICO能够实现实时响应——在毫秒内开始执行，同时可扩展到数千个代理，并在执行时无缝适应不确定性。大量案例研究显示，与开环基线相比，它可将计算时间减少两个数量级以上，在随机延迟和代理到达的情况下，可显著提高吞吐量。这些结果为通过系统级建模、因子分解和闭环设计分析和发展MAPF奠定了原则性的基础。 

---
# FICO: Finite-Horizon Closed-Loop Factorization for Unified Multi-Agent Path Finding 

**Title (ZH)**: FICO: 有限时间 horizons 闭环因子化方法统一多智能体路径寻觅 

**Authors**: Jiarui Li, Alessandro Zanardi, Runyu Zhang, Gioele Zardini  

**Link**: [PDF](https://arxiv.org/pdf/2511.13961)  

**Abstract**: Multi-Agent Path Finding is a fundamental problem in robotics and AI, yet most existing formulations treat planning and execution separately and address variants of the problem in an ad hoc manner. This paper presents a system-level framework for MAPF that integrates planning and execution, generalizes across variants, and explicitly models uncertainties. At its core is the MAPF system, a formal model that casts MAPF as a control design problem encompassing classical and uncertainty-aware formulations. To solve it, we introduce Finite-Horizon Closed-Loop Factorization (FICO), a factorization-based algorithm inspired by receding-horizon control that exploits compositional structure for efficient closed-loop operation. FICO enables real-time responses -- commencing execution within milliseconds -- while scaling to thousands of agents and adapting seamlessly to execution-time uncertainties. Extensive case studies demonstrate that it reduces computation time by up to two orders of magnitude compared with open-loop baselines, while delivering significantly higher throughput under stochastic delays and agent arrivals. These results establish a principled foundation for analyzing and advancing MAPF through system-level modeling, factorization, and closed-loop design. 

**Abstract (ZH)**: 多智能体路径finding是机器人学和AI的基本问题，但现有的大多数形式化方法将规划和执行分开，并以临时的方式解决问题的变种。本文提出了一种针对路径finding的系统级框架，该框架将规划与执行集成在一起，能够适用于各种变种，并明确地建模不确定性。其核心是MAPF系统，这是一种形式化的模型，将路径finding问题表述为一个包括经典方法和不确定性意识方法的控制设计问题。为了解决这一问题，我们引入了有限时域闭环因子分解（FICO）算法，该算法借鉴了预测控制的理念，并利用组合结构实现高效的闭环操作。FICO允许实时响应，在毫秒内启动执行，同时能够处理数千个智能体，并且能够无缝适应执行时的不确定性。广泛的案例研究表明，相比于开环基线，它可以将计算时间减少两个数量级，并且在随机延迟和智能体到达的条件下能够提供显著更高的吞吐量。这些结果为通过系统级建模、因子分解和闭环设计分析和推进路径finding奠定了理论基础。 

---
# $π^{*}_{0.6}$: a VLA That Learns From Experience 

**Title (ZH)**: $π^{*}_{0.6}$: 一种基于经验学习的VLA 

**Authors**: Ali Amin, Raichelle Aniceto, Ashwin Balakrishna, Kevin Black, Ken Conley, Grace Connors, James Darpinian, Karan Dhabalia, Jared DiCarlo, Danny Driess, Michael Equi, Adnan Esmail, Yunhao Fang, Chelsea Finn, Catherine Glossop, Thomas Godden, Ivan Goryachev, Lachy Groom, Hunter Hancock, Karol Hausman, Gashon Hussein, Brian Ichter, Szymon Jakubczak, Rowan Jen, Tim Jones, Ben Katz, Liyiming Ke, Chandra Kuchi, Marinda Lamb, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Yao Lu, Vishnu Mano, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Charvi Sharma, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, Will Stoeckle, Alex Swerdlow, James Tanner, Marcel Torne, Quan Vuong, Anna Walling, Haohuan Wang, Blake Williams, Sukwon Yoo, Lili Yu, Ury Zhilinsky, Zhiyuan Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2511.14759)  

**Abstract**: We study how vision-language-action (VLA) models can improve through real-world deployments via reinforcement learning (RL). We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning. Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution. RECAP starts by pre-training a generalist VLA with offline RL, which we call $\pi^{*}_{0.6}$, that can then be specialized to attain high performance on downstream tasks through on-robot data collection. We show that the $\pi^{*}_{0.6}$ model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine. On some of the hardest tasks, RECAP more than doubles task throughput and roughly halves the task failure rate. 

**Abstract (ZH)**: 我们通过强化学习研究如何提升视觉-语言-动作（VLA）模型在实际部署中的性能。我们提出了一种通用方法——利用优势条件策略的经验和修正（RECAP），该方法通过优势条件化为VLAs提供强化学习训练。我们的方法将异构数据整合到自我改进过程中，包括演示、策略调整期间收集的数据以及自主执行过程中专家远程操作的介入。RECAP首先使用离线RL预训练一个通用的VLA模型，称为$\pi^{*}_{0.6}$，该模型可以通过在机器人上收集数据进一步专门化以在下游任务中取得高性能。我们展示了使用完整RECAP方法训练的$\pi^{*}_{0.6}$模型可以在真实家庭中折叠衣物、可靠地组装箱盒，并使用专业咖啡机制作意式咖啡饮品。在某些最难的任务中，RECAP将任务 throughput 提高了两倍多，并将任务失败率降低了约一半。 

---
# FICO: Finite-Horizon Closed-Loop Factorization for Unified Multi-Agent Path Finding 

**Title (ZH)**: FICO: 有限时域闭环因子分解方法统一多代理路径规划 

**Authors**: Jiarui Li, Alessandro Zanardi, Runyu Zhang, Gioele Zardini  

**Link**: [PDF](https://arxiv.org/pdf/2511.13961)  

**Abstract**: Multi-Agent Path Finding is a fundamental problem in robotics and AI, yet most existing formulations treat planning and execution separately and address variants of the problem in an ad hoc manner. This paper presents a system-level framework for MAPF that integrates planning and execution, generalizes across variants, and explicitly models uncertainties. At its core is the MAPF system, a formal model that casts MAPF as a control design problem encompassing classical and uncertainty-aware formulations. To solve it, we introduce Finite-Horizon Closed-Loop Factorization (FICO), a factorization-based algorithm inspired by receding-horizon control that exploits compositional structure for efficient closed-loop operation. FICO enables real-time responses -- commencing execution within milliseconds -- while scaling to thousands of agents and adapting seamlessly to execution-time uncertainties. Extensive case studies demonstrate that it reduces computation time by up to two orders of magnitude compared with open-loop baselines, while delivering significantly higher throughput under stochastic delays and agent arrivals. These results establish a principled foundation for analyzing and advancing MAPF through system-level modeling, factorization, and closed-loop design. 

**Abstract (ZH)**: 多智能体路径规划是机器人学和AI中的一个基础问题，但大多数现有形式将计划和执行分开处理，并针对问题的不同变体采取非系统的方法。本文提出了一种多智能体路径规划的系统级框架，该框架将计划和执行整合，跨不同变体进行推广，并明确建模不确定性。其核心是多智能体路径规划系统，这是一种形式模型，将多智能体路径规划问题表述为一个控制设计问题，涵盖了经典的和不确定性意识的形式模型。为了解决这个问题，我们引入了有限时域闭环因子分解（FICO）算法，这是一种受回溯_horizon控制启发、利用组合结构实现高效闭环操作的因子分解算法。FICO能够实现实时响应——在毫秒内开始执行，同时能扩展到数千个智能体，并无缝适应执行时的不确定性。广泛的实际案例研究表明，与开环基准相比，它将计算时间减少了两个数量级，同时在随机延迟和智能体到达时显著提高了吞吐量。这些结果为通过系统级建模、因子分解和闭环设计分析和推进多智能体路径规划奠定了原理性的基础。 

---
# $π^{*}_{0.6}$: a VLA That Learns From Experience 

**Title (ZH)**: $π^{*}_{0.6}$: 一种基于经验学习的VLA 

**Authors**: Ali Amin, Raichelle Aniceto, Ashwin Balakrishna, Kevin Black, Ken Conley, Grace Connors, James Darpinian, Karan Dhabalia, Jared DiCarlo, Danny Driess, Michael Equi, Adnan Esmail, Yunhao Fang, Chelsea Finn, Catherine Glossop, Thomas Godden, Ivan Goryachev, Lachy Groom, Hunter Hancock, Karol Hausman, Gashon Hussein, Brian Ichter, Szymon Jakubczak, Rowan Jen, Tim Jones, Ben Katz, Liyiming Ke, Chandra Kuchi, Marinda Lamb, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Yao Lu, Vishnu Mano, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Charvi Sharma, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, Will Stoeckle, Alex Swerdlow, James Tanner, Marcel Torne, Quan Vuong, Anna Walling, Haohuan Wang, Blake Williams, Sukwon Yoo, Lili Yu, Ury Zhilinsky, Zhiyuan Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2511.14759)  

**Abstract**: We study how vision-language-action (VLA) models can improve through real-world deployments via reinforcement learning (RL). We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning. Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution. RECAP starts by pre-training a generalist VLA with offline RL, which we call $\pi^{*}_{0.6}$, that can then be specialized to attain high performance on downstream tasks through on-robot data collection. We show that the $\pi^{*}_{0.6}$ model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine. On some of the hardest tasks, RECAP more than doubles task throughput and roughly halves the task failure rate. 

**Abstract (ZH)**: 我们通过强化学习研究如何提高视觉-语言-动作（VLA）模型在实际部署中的性能。我们提出了一种通用方法——基于优势条件策略的经验与修正的强化学习（RECAP），该方法通过优势条件化促进了VLA的RL训练。该方法整合了异构数据到自我改进过程中，包括演示、在线策略收集的数据以及自主执行期间的专家遥操作干预。RECAP首先通过离线RL预训练一个通用VLA模型$\pi^{*}_{0.6}$，该模型可以通过机器人数据收集进一步特化以在下游任务中达到高性能。我们展示了使用完整RECAP方法训练的$\pi^{*}_{0.6}$模型能够在真实家庭中折叠衣物、可靠地组装盒子，并使用专业咖啡机制作意式咖啡饮品。在一些最难的任务上，RECAP将任务 throughput 提高了一倍多，并将任务失败率降低了约一半。 

---
# $π^{*}_{0.6}$: a VLA That Learns From Experience 

**Title (ZH)**: $π^{*}_{0.6}$：一种基于经验的学习VLA 

**Authors**: Ali Amin, Raichelle Aniceto, Ashwin Balakrishna, Kevin Black, Ken Conley, Grace Connors, James Darpinian, Karan Dhabalia, Jared DiCarlo, Danny Driess, Michael Equi, Adnan Esmail, Yunhao Fang, Chelsea Finn, Catherine Glossop, Thomas Godden, Ivan Goryachev, Lachy Groom, Hunter Hancock, Karol Hausman, Gashon Hussein, Brian Ichter, Szymon Jakubczak, Rowan Jen, Tim Jones, Ben Katz, Liyiming Ke, Chandra Kuchi, Marinda Lamb, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Yao Lu, Vishnu Mano, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Charvi Sharma, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, Will Stoeckle, Alex Swerdlow, James Tanner, Marcel Torne, Quan Vuong, Anna Walling, Haohuan Wang, Blake Williams, Sukwon Yoo, Lili Yu, Ury Zhilinsky, Zhiyuan Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2511.14759)  

**Abstract**: We study how vision-language-action (VLA) models can improve through real-world deployments via reinforcement learning (RL). We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning. Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution. RECAP starts by pre-training a generalist VLA with offline RL, which we call $\pi^{*}_{0.6}$, that can then be specialized to attain high performance on downstream tasks through on-robot data collection. We show that the $\pi^{*}_{0.6}$ model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine. On some of the hardest tasks, RECAP more than doubles task throughput and roughly halves the task failure rate. 

**Abstract (ZH)**: 我们通过强化学习研究视觉-语言-行动（VLA）模型在现实世界部署中的改进方法，并提出了一个通用方法RECAP，该方法通过优势条件策略提供VLAs的RL训练。RECAP方法将异构数据纳入自我改进过程，包括演示、在线策略收集的数据以及自主执行期间由专家远程操作提供的干预。RECAP首先使用离线RL预训练一个通用VLA模型$\pi^{*}_{0.6}$，该模型然后可以通过在机器人上的数据收集专门化以在下游任务上获得高性能。我们展示了使用完整RECAP方法训练的$\pi^{*}_{0.6}$模型能够在真实家庭中折叠衣物、可靠地组装箱子，并使用专业咖啡机制作 espresso 饮料。在某些最困难的任务上，RECAP将任务 throughput 提高了两倍以上，并将任务失败率降低了约一半。 

---
# Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers 

**Title (ZH)**: Co-Me: 基于信心的令牌合并方法用于视觉几何变换器 

**Authors**: Yutian Chen, Yuheng Qiu, Ruogu Li, Ali Agha, Shayegan Omidshafiei, Jay Patrikar, Sebastian Scherer  

**Link**: [PDF](https://arxiv.org/pdf/2511.14751)  

**Abstract**: We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model. Co-Me distilled a light-weight confidence predictor to rank tokens by uncertainty and selectively merge low-confidence ones, effectively reducing computation while maintaining spatial coverage. Compared to similarity-based merging or pruning, the confidence signal in Co-Me reliably indicates regions emphasized by the transformer, enabling substantial acceleration without degrading performance. Co-Me applies seamlessly to various multi-view and streaming visual geometric transformers, achieving speedups that scale with sequence length. When applied to VGGT and MapAnything, Co-Me achieves up to $11.3\times$ and $7.2\times$ speedup, making visual geometric transformers practical for real-time 3D perception and reconstruction. 

**Abstract (ZH)**: 我们提出了一种加速视觉几何变换器的机制——基于信心引导的标记合并（Co-Me），无需重新训练或微调基础模型。Co-Me 提取出一个轻量级的信心预测器，根据不确定性对标记进行排序，并选择性地合并低信心的标记，有效减少了计算量同时保持了空间覆盖。与基于相似性的合并或剪枝相比，Co-Me 中的信心信号可靠地指示了变换器强调的区域，从而在不牺牲性能的前提下实现了显著的加速。Co-Me 可无缝应用于各种多视图和流式视觉几何变换器，并随着序列长度的增加实现加速。当应用于 VGGT 和 MapAnything 时，Co-Me 分别实现了高达 $11.3\times$ 和 $7.2\times$ 的加速，使得视觉几何变换器适用于实时 3D 感知和重建。 

---
# $π^{*}_{0.6}$: a VLA That Learns From Experience 

**Title (ZH)**: $π^{*}_{0.6}$: 一种基于经验学习的VLA 

**Authors**: Ali Amin, Raichelle Aniceto, Ashwin Balakrishna, Kevin Black, Ken Conley, Grace Connors, James Darpinian, Karan Dhabalia, Jared DiCarlo, Danny Driess, Michael Equi, Adnan Esmail, Yunhao Fang, Chelsea Finn, Catherine Glossop, Thomas Godden, Ivan Goryachev, Lachy Groom, Hunter Hancock, Karol Hausman, Gashon Hussein, Brian Ichter, Szymon Jakubczak, Rowan Jen, Tim Jones, Ben Katz, Liyiming Ke, Chandra Kuchi, Marinda Lamb, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Yao Lu, Vishnu Mano, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Charvi Sharma, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, Will Stoeckle, Alex Swerdlow, James Tanner, Marcel Torne, Quan Vuong, Anna Walling, Haohuan Wang, Blake Williams, Sukwon Yoo, Lili Yu, Ury Zhilinsky, Zhiyuan Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2511.14759)  

**Abstract**: We study how vision-language-action (VLA) models can improve through real-world deployments via reinforcement learning (RL). We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning. Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution. RECAP starts by pre-training a generalist VLA with offline RL, which we call $\pi^{*}_{0.6}$, that can then be specialized to attain high performance on downstream tasks through on-robot data collection. We show that the $\pi^{*}_{0.6}$ model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine. On some of the hardest tasks, RECAP more than doubles task throughput and roughly halves the task failure rate. 

**Abstract (ZH)**: 我们研究了通过实际部署和强化学习（RL）如何提升视觉-语言-行动（VLA）模型。我们提出了一种通用方法——基于优势条件策略的经验和纠正强化学习（RECAP），该方法通过优势条件提供了VLAs的RL训练。该方法将异质数据整合进自我改进过程，包括演示、政策收集的数据以及自主执行期间的专家远程操作干预。RECAP首先通过离线RL预训练一个通用的VLA模型，记为$\pi^{*}_{0.6}$，然后通过机器人数据收集使其专门化以在下游任务中实现高性能。实验表明，使用完整RECAP方法训练的$\pi^{*}_{0.6}$模型可以在真实家庭中折叠衣物、可靠地组装盒子，并使用专业自动咖啡机制作意式咖啡。在某些最困难的任务上，RECAP将任务吞吐量提高了一倍多，将任务失败率降低了一半左右。 

---
# Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers 

**Title (ZH)**: 共引导词合并：基于置信度的token合并方法用于视觉几何变换器 

**Authors**: Yutian Chen, Yuheng Qiu, Ruogu Li, Ali Agha, Shayegan Omidshafiei, Jay Patrikar, Sebastian Scherer  

**Link**: [PDF](https://arxiv.org/pdf/2511.14751)  

**Abstract**: We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model. Co-Me distilled a light-weight confidence predictor to rank tokens by uncertainty and selectively merge low-confidence ones, effectively reducing computation while maintaining spatial coverage. Compared to similarity-based merging or pruning, the confidence signal in Co-Me reliably indicates regions emphasized by the transformer, enabling substantial acceleration without degrading performance. Co-Me applies seamlessly to various multi-view and streaming visual geometric transformers, achieving speedups that scale with sequence length. When applied to VGGT and MapAnything, Co-Me achieves up to $11.3\times$ and $7.2\times$ speedup, making visual geometric transformers practical for real-time 3D perception and reconstruction. 

**Abstract (ZH)**: 基于置信度指导的令牌合并（Co-Me）：视觉几何变换器的加速机制 

---
# Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers 

**Title (ZH)**: Co-Me: 信心引导的令牌合并用于视觉几何变换器 

**Authors**: Yutian Chen, Yuheng Qiu, Ruogu Li, Ali Agha, Shayegan Omidshafiei, Jay Patrikar, Sebastian Scherer  

**Link**: [PDF](https://arxiv.org/pdf/2511.14751)  

**Abstract**: We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model. Co-Me distilled a light-weight confidence predictor to rank tokens by uncertainty and selectively merge low-confidence ones, effectively reducing computation while maintaining spatial coverage. Compared to similarity-based merging or pruning, the confidence signal in Co-Me reliably indicates regions emphasized by the transformer, enabling substantial acceleration without degrading performance. Co-Me applies seamlessly to various multi-view and streaming visual geometric transformers, achieving speedups that scale with sequence length. When applied to VGGT and MapAnything, Co-Me achieves up to $11.3\times$ and $7.2\times$ speedup, making visual geometric transformers practical for real-time 3D perception and reconstruction. 

**Abstract (ZH)**: 基于置信引导的令牌合并（Co-Me）：视觉几何变换器的加速机制 

---
# Active Matter as a framework for living systems-inspired Robophysics 

**Title (ZH)**: 活性物质作为受生命系统启发的机器人物理学的框架 

**Authors**: Giulia Janzen, Gaia Maselli, Juan F. Jimenez, Lia Garcia-Perez, D A Matoz Fernandez, Chantal Valeriani  

**Link**: [PDF](https://arxiv.org/pdf/2511.14624)  

**Abstract**: Robophysics investigates the physical principles that govern living-like robots operating in complex, realworld environments. Despite remarkable technological advances, robots continue to face fundamental efficiency limitations. At the level of individual units, locomotion remains a challenge, while at the collective level, robot swarms struggle to achieve shared purpose, coordination, communication, and cost efficiency. This perspective article examines the key challenges faced by bio-inspired robotic collectives and highlights recent research efforts that incorporate principles from active-matter physics and biology into the modeling and design of robot swarms. 

**Abstract (ZH)**: 生物启发机器人集体中的关键挑战及研究进展：结合活性物质物理与生物学原理设计机器人集群 

---
# Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers 

**Title (ZH)**: Co-Me: 信心引导的令牌合并方法用于视觉几何变换器 

**Authors**: Yutian Chen, Yuheng Qiu, Ruogu Li, Ali Agha, Shayegan Omidshafiei, Jay Patrikar, Sebastian Scherer  

**Link**: [PDF](https://arxiv.org/pdf/2511.14751)  

**Abstract**: We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model. Co-Me distilled a light-weight confidence predictor to rank tokens by uncertainty and selectively merge low-confidence ones, effectively reducing computation while maintaining spatial coverage. Compared to similarity-based merging or pruning, the confidence signal in Co-Me reliably indicates regions emphasized by the transformer, enabling substantial acceleration without degrading performance. Co-Me applies seamlessly to various multi-view and streaming visual geometric transformers, achieving speedups that scale with sequence length. When applied to VGGT and MapAnything, Co-Me achieves up to $11.3\times$ and $7.2\times$ speedup, making visual geometric transformers practical for real-time 3D perception and reconstruction. 

**Abstract (ZH)**: 我们提出了一种加速视觉几何变换器的机制Confidence-Guided Token Merging (Co-Me)，无需重新训练或微调基础模型便能有效加速。Co-Me 提炼出一个轻量级的置信度预测器，根据不确定性对令牌进行排名，并选择性地合并低置信度的令牌，从而有效地减少计算量同时保持空间覆盖率。与基于相似性的合并或剪枝相比，Co-Me 中的置信度信号可靠地指示了变换器所强调的区域，能够在不降低性能的情况下实现显著加速。Co-Me 可无缝应用于各种多视图和流式视觉几何变换器，并实现了随序列长度而扩展的加速效果。当应用于VGGT和MapAnything时，Co-Me 分别实现了高达11.3倍和7.2倍的加速，使视觉几何变换器适用于实时三维感知和重构。 

---
# Active Matter as a framework for living systems-inspired Robophysics 

**Title (ZH)**: 活物质作为受生物系统启发的机器人物理学的框架 

**Authors**: Giulia Janzen, Gaia Maselli, Juan F. Jimenez, Lia Garcia-Perez, D A Matoz Fernandez, Chantal Valeriani  

**Link**: [PDF](https://arxiv.org/pdf/2511.14624)  

**Abstract**: Robophysics investigates the physical principles that govern living-like robots operating in complex, realworld environments. Despite remarkable technological advances, robots continue to face fundamental efficiency limitations. At the level of individual units, locomotion remains a challenge, while at the collective level, robot swarms struggle to achieve shared purpose, coordination, communication, and cost efficiency. This perspective article examines the key challenges faced by bio-inspired robotic collectives and highlights recent research efforts that incorporate principles from active-matter physics and biology into the modeling and design of robot swarms. 

**Abstract (ZH)**: 基于物理原理的机器人研究探讨了在复杂真实环境中共生机理性机器人所遵循的物理原理。尽管取得了显著的技术进步，机器人仍然面临基本的效率限制。在个体层面，运动仍然是一个挑战，而在群体层面，机器人 swarm 在实现共享目标、协调、通信和成本效率方面遇到困难。本文视角考察了共生机理性机器人所面临的关键挑战，并强调了将活性物质物理和生物学原理整合到机器人 swarm 模型和设计中的 recent 研究努力。 

---
# Active Matter as a framework for living systems-inspired Robophysics 

**Title (ZH)**: 活物质作为受生物系统启发的机器人物理学的框架 

**Authors**: Giulia Janzen, Gaia Maselli, Juan F. Jimenez, Lia Garcia-Perez, D A Matoz Fernandez, Chantal Valeriani  

**Link**: [PDF](https://arxiv.org/pdf/2511.14624)  

**Abstract**: Robophysics investigates the physical principles that govern living-like robots operating in complex, realworld environments. Despite remarkable technological advances, robots continue to face fundamental efficiency limitations. At the level of individual units, locomotion remains a challenge, while at the collective level, robot swarms struggle to achieve shared purpose, coordination, communication, and cost efficiency. This perspective article examines the key challenges faced by bio-inspired robotic collectives and highlights recent research efforts that incorporate principles from active-matter physics and biology into the modeling and design of robot swarms. 

**Abstract (ZH)**: 生物启发的机器人群体探究：基于活性物质物理与生物学原理的机器人群体建模与设计面临的挑战 

---
# Active Matter as a framework for living systems-inspired Robophysics 

**Title (ZH)**: 活性物质作为受生物系统启发的 robophysics 的框架 

**Authors**: Giulia Janzen, Gaia Maselli, Juan F. Jimenez, Lia Garcia-Perez, D A Matoz Fernandez, Chantal Valeriani  

**Link**: [PDF](https://arxiv.org/pdf/2511.14624)  

**Abstract**: Robophysics investigates the physical principles that govern living-like robots operating in complex, realworld environments. Despite remarkable technological advances, robots continue to face fundamental efficiency limitations. At the level of individual units, locomotion remains a challenge, while at the collective level, robot swarms struggle to achieve shared purpose, coordination, communication, and cost efficiency. This perspective article examines the key challenges faced by bio-inspired robotic collectives and highlights recent research efforts that incorporate principles from active-matter physics and biology into the modeling and design of robot swarms. 

**Abstract (ZH)**: 仿生机器人物理学探讨受生物启发的机器人集群在复杂现实环境中的物理原理及其面临的挑战，并强调了将活性物质物理学和生物学原理融入机器人集群建模与设计的近期研究进展。 

---
# Enhancing End-to-End Autonomous Driving with Risk Semantic Distillaion from VLM 

**Title (ZH)**: 基于VLM的风险语义提炼增强端到端自动驾驶 

**Authors**: Jack Qin, Zhitao Wang, Yinan Zheng, Keyu Chen, Yang Zhou, Yuanxin Zhong, Siyuan Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2511.14499)  

**Abstract**: The autonomous driving (AD) system has exhibited remarkable performance in complex driving scenarios. However, generalization is still a key limitation for the current system, which refers to the ability to handle unseen scenarios or unfamiliar sensor this http URL works have explored the use of Vision-Language Models (VLMs) to address few-shot or zero-shot tasks. While promising, these methods introduce a new challenge: the emergence of a hybrid AD system, where two distinct systems are used to plan a trajectory, leading to potential inconsistencies. Alternative research directions have explored Vision-Language-Action (VLA) frameworks that generate control actions from VLM directly. However, these end-to-end solutions demonstrate prohibitive computational demands. To overcome these challenges, we introduce Risk Semantic Distillation (RSD), a novel framework that leverages VLMs to enhance the training of End-to-End (E2E) AD backbones. By providing risk attention for key objects, RSD addresses the issue of generalization. Specifically, we introduce RiskHead, a plug-in module that distills causal risk estimates from Vision-Language Models into Bird's-Eye-View (BEV) features, yielding interpretable risk-attention this http URL approach allows BEV features to learn richer and more nuanced risk attention representations, which directly enhance the model's ability to handle spatial boundaries and risky this http URL focusing on risk attention, RSD aligns better with human-like driving behavior, which is essential to navigate in complex and dynamic environments. Our experiments on the Bench2Drive benchmark demonstrate the effectiveness of RSD in managing complex and unpredictable driving conditions. Due to the enhanced BEV representations enabled by RSD, we observed a significant improvement in both perception and planning capabilities. 

**Abstract (ZH)**: 自主驾驶（AD）系统在复杂驾驶场景中表现出色，然而泛化仍然是当前系统的关键限制，指的是处理未见过的场景或不熟悉传感器数据的能力。已有研究探索了使用Vision-Language Models (VLMs)来解决少样本或零样本任务。尽管具有前景，但这些方法引入了一个新挑战：混合AD系统的出现，其中两个独立系统用于规划轨迹，可能导致潜在的不一致性。替代研究方向探索了Vision-Language-Action (VLA)框架，直接从VLM生成控制动作。然而，这些端到端的解决方案表现出计算需求过高的问题。为克服这些挑战，我们提出了风险语义蒸馏（RSD）这一新颖框架，它利用VLMs来增强端到端（E2E）AD主干网络的训练。通过提供关键对象的风险关注，RSD解决了泛化问题。具体而言，我们引入了RiskHead模块，它从Vision-Language Models中提取因果风险估计并将其蒸馏至鸟瞰图（BEV）特征中，从而产生可解释的风险关注。该方法允许BEV特征学习更丰富和复杂的风险关注表示，直接增强模型处理空间边界和风险的能力。重点关注风险关注，RSD更好地与类人驾驶行为对齐，这对于在复杂和动态环境中导航至关重要。我们在Bench2Drive基准上的实验结果证明了RSD在管理复杂和不可预测的驾驶条件方面的有效性。由于RSD增强了BEV表示，我们观察到感知能力和规划能力都有显著提升。 

---
# Enhancing End-to-End Autonomous Driving with Risk Semantic Distillaion from VLM 

**Title (ZH)**: 基于VLM的风险语义精炼增强端到端自动驾驶 

**Authors**: Jack Qin, Zhitao Wang, Yinan Zheng, Keyu Chen, Yang Zhou, Yuanxin Zhong, Siyuan Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2511.14499)  

**Abstract**: The autonomous driving (AD) system has exhibited remarkable performance in complex driving scenarios. However, generalization is still a key limitation for the current system, which refers to the ability to handle unseen scenarios or unfamiliar sensor this http URL works have explored the use of Vision-Language Models (VLMs) to address few-shot or zero-shot tasks. While promising, these methods introduce a new challenge: the emergence of a hybrid AD system, where two distinct systems are used to plan a trajectory, leading to potential inconsistencies. Alternative research directions have explored Vision-Language-Action (VLA) frameworks that generate control actions from VLM directly. However, these end-to-end solutions demonstrate prohibitive computational demands. To overcome these challenges, we introduce Risk Semantic Distillation (RSD), a novel framework that leverages VLMs to enhance the training of End-to-End (E2E) AD backbones. By providing risk attention for key objects, RSD addresses the issue of generalization. Specifically, we introduce RiskHead, a plug-in module that distills causal risk estimates from Vision-Language Models into Bird's-Eye-View (BEV) features, yielding interpretable risk-attention this http URL approach allows BEV features to learn richer and more nuanced risk attention representations, which directly enhance the model's ability to handle spatial boundaries and risky this http URL focusing on risk attention, RSD aligns better with human-like driving behavior, which is essential to navigate in complex and dynamic environments. Our experiments on the Bench2Drive benchmark demonstrate the effectiveness of RSD in managing complex and unpredictable driving conditions. Due to the enhanced BEV representations enabled by RSD, we observed a significant improvement in both perception and planning capabilities. 

**Abstract (ZH)**: 基于视觉-语言模型的风险语义蒸馏在端到端自动驾驶中的应用 

---
# Enhancing End-to-End Autonomous Driving with Risk Semantic Distillaion from VLM 

**Title (ZH)**: 基于VLM的风险语义精炼增强端到端自动驾驶 

**Authors**: Jack Qin, Zhitao Wang, Yinan Zheng, Keyu Chen, Yang Zhou, Yuanxin Zhong, Siyuan Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2511.14499)  

**Abstract**: The autonomous driving (AD) system has exhibited remarkable performance in complex driving scenarios. However, generalization is still a key limitation for the current system, which refers to the ability to handle unseen scenarios or unfamiliar sensor this http URL works have explored the use of Vision-Language Models (VLMs) to address few-shot or zero-shot tasks. While promising, these methods introduce a new challenge: the emergence of a hybrid AD system, where two distinct systems are used to plan a trajectory, leading to potential inconsistencies. Alternative research directions have explored Vision-Language-Action (VLA) frameworks that generate control actions from VLM directly. However, these end-to-end solutions demonstrate prohibitive computational demands. To overcome these challenges, we introduce Risk Semantic Distillation (RSD), a novel framework that leverages VLMs to enhance the training of End-to-End (E2E) AD backbones. By providing risk attention for key objects, RSD addresses the issue of generalization. Specifically, we introduce RiskHead, a plug-in module that distills causal risk estimates from Vision-Language Models into Bird's-Eye-View (BEV) features, yielding interpretable risk-attention this http URL approach allows BEV features to learn richer and more nuanced risk attention representations, which directly enhance the model's ability to handle spatial boundaries and risky this http URL focusing on risk attention, RSD aligns better with human-like driving behavior, which is essential to navigate in complex and dynamic environments. Our experiments on the Bench2Drive benchmark demonstrate the effectiveness of RSD in managing complex and unpredictable driving conditions. Due to the enhanced BEV representations enabled by RSD, we observed a significant improvement in both perception and planning capabilities. 

**Abstract (ZH)**: 自主驾驶（AD）系统在复杂驾驶场景中展现了 remarkable 的性能，但泛化仍是当前系统的主要局限性。为了克服这一挑战，我们提出了风险语义蒸馏（RSD），一种利用视觉-语言模型（VLM）增强端到端（E2E）AD主干网络训练的新框架。通过提供关键对象的风险注意力，RSD解决了泛化的问题。具体而言，我们引入了RiskHead模块，将VLM的风险因果估计蒸馏进鸟瞰图（BEV）特征，生成可解释的风险注意力。该方法使BEV特征能够学习更丰富和更细腻的风险注意力表示，从而直接增强模型处理空间边界和风险的能力。重点关注风险注意力，RSD更符合人类驾驶行为，这对于在复杂和动态环境中导航至关重要。在Bench2Drive基准测试中，我们的实验验证了RSD在处理复杂和不可预测的驾驶条件方面的有效性。由于RSD增强了BEV表示，我们观察到在感知和规划能力上都取得了显著提高。 

---
# Enhancing End-to-End Autonomous Driving with Risk Semantic Distillaion from VLM 

**Title (ZH)**: 基于VLM的风险语义精炼增强端到端自主驾驶 

**Authors**: Jack Qin, Zhitao Wang, Yinan Zheng, Keyu Chen, Yang Zhou, Yuanxin Zhong, Siyuan Cheng  

**Link**: [PDF](https://arxiv.org/pdf/2511.14499)  

**Abstract**: The autonomous driving (AD) system has exhibited remarkable performance in complex driving scenarios. However, generalization is still a key limitation for the current system, which refers to the ability to handle unseen scenarios or unfamiliar sensor this http URL works have explored the use of Vision-Language Models (VLMs) to address few-shot or zero-shot tasks. While promising, these methods introduce a new challenge: the emergence of a hybrid AD system, where two distinct systems are used to plan a trajectory, leading to potential inconsistencies. Alternative research directions have explored Vision-Language-Action (VLA) frameworks that generate control actions from VLM directly. However, these end-to-end solutions demonstrate prohibitive computational demands. To overcome these challenges, we introduce Risk Semantic Distillation (RSD), a novel framework that leverages VLMs to enhance the training of End-to-End (E2E) AD backbones. By providing risk attention for key objects, RSD addresses the issue of generalization. Specifically, we introduce RiskHead, a plug-in module that distills causal risk estimates from Vision-Language Models into Bird's-Eye-View (BEV) features, yielding interpretable risk-attention this http URL approach allows BEV features to learn richer and more nuanced risk attention representations, which directly enhance the model's ability to handle spatial boundaries and risky this http URL focusing on risk attention, RSD aligns better with human-like driving behavior, which is essential to navigate in complex and dynamic environments. Our experiments on the Bench2Drive benchmark demonstrate the effectiveness of RSD in managing complex and unpredictable driving conditions. Due to the enhanced BEV representations enabled by RSD, we observed a significant improvement in both perception and planning capabilities. 

**Abstract (ZH)**: 基于Vision-Language模型的自主驾驶风险语义蒸馏框架 

---
# Towards A Catalogue of Requirement Patterns for Space Robotic Missions 

**Title (ZH)**: 面向太空机器人任务的 requisites 模式目录 

**Authors**: Mahdi Etumi, Hazel M. Taylor, Marie Farrell  

**Link**: [PDF](https://arxiv.org/pdf/2511.14438)  

**Abstract**: In the development of safety and mission-critical systems, including autonomous space robotic missions, complex behaviour is captured during the requirements elicitation phase. Requirements are typically expressed using natural language which is ambiguous and not amenable to formal verification methods that can provide robust guarantees of system behaviour. To support the definition of formal requirements, specification patterns provide reusable, logic-based templates. A suite of robotic specification patterns, along with their formalisation in NASA's Formal Requirements Elicitation Tool (FRET) already exists. These pre-existing requirement patterns are domain agnostic and, in this paper we explore their applicability for space missions. To achieve this we carried out a literature review of existing space missions and formalised their requirements using FRET, contributing a corpus of space mission requirements. We categorised these requirements using pre-existing specification patterns which demonstrated their applicability in space missions. However, not all of the requirements that we formalised corresponded to an existing pattern so we have contributed 5 new requirement specification patterns as well as several variants of the existing and new patterns. We also conducted an expert evaluation of the new patterns, highlighting their benefits and limitations. 

**Abstract (ZH)**: 在安全和任务关键系统，包括自主太空机器人任务的研发中，需求获取阶段捕获了复杂行为。需求通常用自然语言表达，这种表达方式存在歧义，不适合进行提供系统行为坚实保障的正式验证方法。为了支持正式需求的定义，规格模式提供了可重复使用、基于逻辑的模板。NASA的正式需求获取工具（FRET）中已经存在一套机器人规格模式及其形式化表示。这些预先存在的需求模式是领域无关的，本文探讨了它们在太空任务中的适用性。为了实现这一目标，我们对现有太空任务进行了文献回顾，并使用FRET形式化其需求，贡献了一套太空任务需求语料库。我们使用预先存在的规格模式对这些需求进行了分类，证明了它们在太空任务中的适用性。然而，并非所有形式化的需求都对应于现有的模式，因此我们贡献了5个新的需求规格模式以及现有和新模式的几个变体。我们还对新的模式进行了专家评估，指出了它们的优缺点。 

---
# Towards A Catalogue of Requirement Patterns for Space Robotic Missions 

**Title (ZH)**: 面向空间机器人任务的需求模式 catalogue 

**Authors**: Mahdi Etumi, Hazel M. Taylor, Marie Farrell  

**Link**: [PDF](https://arxiv.org/pdf/2511.14438)  

**Abstract**: In the development of safety and mission-critical systems, including autonomous space robotic missions, complex behaviour is captured during the requirements elicitation phase. Requirements are typically expressed using natural language which is ambiguous and not amenable to formal verification methods that can provide robust guarantees of system behaviour. To support the definition of formal requirements, specification patterns provide reusable, logic-based templates. A suite of robotic specification patterns, along with their formalisation in NASA's Formal Requirements Elicitation Tool (FRET) already exists. These pre-existing requirement patterns are domain agnostic and, in this paper we explore their applicability for space missions. To achieve this we carried out a literature review of existing space missions and formalised their requirements using FRET, contributing a corpus of space mission requirements. We categorised these requirements using pre-existing specification patterns which demonstrated their applicability in space missions. However, not all of the requirements that we formalised corresponded to an existing pattern so we have contributed 5 new requirement specification patterns as well as several variants of the existing and new patterns. We also conducted an expert evaluation of the new patterns, highlighting their benefits and limitations. 

**Abstract (ZH)**: 在安全和任务关键系统，包括自主太空机器人任务的发展中，需求获取阶段捕捉到了复杂的行为。需求通常用自然语言表达，具有模糊性，无法满足提供系统行为稳健保证的正式验证方法。为了支持正式需求的定义，规范模式提供了可重用、基于逻辑的模板。NASA的正式需求获取工具（FRET）中已存在一系列用于机器人的规范模式及其形式化。这些先有的需求模式是领域无关的，本文探讨了它们在太空任务中的适用性。为此，我们对现有太空任务进行了文献综述，并使用FRET形式化了它们的需求，贡献了一组太空使命需求。我们使用现成的规范模式对这些需求进行了分类，证明了它们在太空任务中的适用性。然而，并非所有我们形式化的需求都对应于现有模式，因此我们贡献了5种新的需求规范模式，以及现成和新模式的若干变体。此外，我们还对新的模式进行了专家评估，指出了它们的优点和局限性。 

---
# Towards A Catalogue of Requirement Patterns for Space Robotic Missions 

**Title (ZH)**: 面向空间机器人任务的结构化需求模式目录 

**Authors**: Mahdi Etumi, Hazel M. Taylor, Marie Farrell  

**Link**: [PDF](https://arxiv.org/pdf/2511.14438)  

**Abstract**: In the development of safety and mission-critical systems, including autonomous space robotic missions, complex behaviour is captured during the requirements elicitation phase. Requirements are typically expressed using natural language which is ambiguous and not amenable to formal verification methods that can provide robust guarantees of system behaviour. To support the definition of formal requirements, specification patterns provide reusable, logic-based templates. A suite of robotic specification patterns, along with their formalisation in NASA's Formal Requirements Elicitation Tool (FRET) already exists. These pre-existing requirement patterns are domain agnostic and, in this paper we explore their applicability for space missions. To achieve this we carried out a literature review of existing space missions and formalised their requirements using FRET, contributing a corpus of space mission requirements. We categorised these requirements using pre-existing specification patterns which demonstrated their applicability in space missions. However, not all of the requirements that we formalised corresponded to an existing pattern so we have contributed 5 new requirement specification patterns as well as several variants of the existing and new patterns. We also conducted an expert evaluation of the new patterns, highlighting their benefits and limitations. 

**Abstract (ZH)**: 在安全和任务关键系统，包括自主太空机器人任务的发展中，复杂行为在需求获取阶段被捕获。需求通常用自然语言表达，这具有模糊性，不适用于能提供系统行为稳健保证的形式化验证方法。为了支持形式化需求的定义，规格模式提供了可重用的、基于逻辑的模板。NASA的形式化需求获取工具（FRET）中已经存在一系列的机器人规格模式及其形式化表示。这些预先存在的需求模式是领域无关的，在本文中我们探讨了它们在太空任务中的适用性。为此，我们对现有太空任务进行了文献综述，并使用FRET形式化其需求，贡献了一组太空任务需求。我们使用预先存在的规格模式对这些需求进行了分类，以展示它们在太空任务中的适用性。然而，并非所有我们形式化的需求都对应于现有的模式，因此我们贡献了5种新的需求规格模式以及现有和新模式的几个变体。我们还对新的模式进行了专家评估，指出了其优缺点。 

---
# Towards A Catalogue of Requirement Patterns for Space Robotic Missions 

**Title (ZH)**: 面向空间机器人任务的需求模式catalogue研究 

**Authors**: Mahdi Etumi, Hazel M. Taylor, Marie Farrell  

**Link**: [PDF](https://arxiv.org/pdf/2511.14438)  

**Abstract**: In the development of safety and mission-critical systems, including autonomous space robotic missions, complex behaviour is captured during the requirements elicitation phase. Requirements are typically expressed using natural language which is ambiguous and not amenable to formal verification methods that can provide robust guarantees of system behaviour. To support the definition of formal requirements, specification patterns provide reusable, logic-based templates. A suite of robotic specification patterns, along with their formalisation in NASA's Formal Requirements Elicitation Tool (FRET) already exists. These pre-existing requirement patterns are domain agnostic and, in this paper we explore their applicability for space missions. To achieve this we carried out a literature review of existing space missions and formalised their requirements using FRET, contributing a corpus of space mission requirements. We categorised these requirements using pre-existing specification patterns which demonstrated their applicability in space missions. However, not all of the requirements that we formalised corresponded to an existing pattern so we have contributed 5 new requirement specification patterns as well as several variants of the existing and new patterns. We also conducted an expert evaluation of the new patterns, highlighting their benefits and limitations. 

**Abstract (ZH)**: 在安全和关键任务系统的开发中，包括自主太空机器人任务在内，复杂行为在需求获取阶段被捕捉。需求通常用自然语言表达，这种表达方式具有模糊性，不适合进行提供系统行为稳健保证的形式化验证方法。为了支持形式化需求的定义，规格模式提供了可重用的基于逻辑的模板。NASA的形式化需求获取工具（FRET）中已经存在一套机器人规格模式及其形式化表示。这些预先存在的需求模式是领域无关的，本文探讨了它们在太空任务中的适用性。为此，我们进行了现有太空任务的文献综述，并使用FRET对其需求进行了形式化，贡献了一套太空任务需求语料库。我们使用预先存在的规格模式对这些需求进行了分类，证明了它们在太空任务中的适用性。然而，并不是我们形式化的所有需求都对应现有的模式，因此我们贡献了5种新的需求规格模式及其现有和新模式的各种变体。我们还对新模式进行了专家评估，指出了它们的优势和局限性。 

---
# Safe-ROS: An Architecture for Autonomous Robots in Safety-Critical Domains 

**Title (ZH)**: Safe-ROS：安全关键领域自主机器人架构 

**Authors**: Diana C. Benjumea, Marie Farrell, Louise A. Dennis  

**Link**: [PDF](https://arxiv.org/pdf/2511.14433)  

**Abstract**: Deploying autonomous robots in safety-critical domains requires architectures that ensure operational effectiveness and safety compliance. In this paper, we contribute the Safe-ROS architecture for developing reliable and verifiable autonomous robots in such domains. It features two distinct subsystems: (1) an intelligent control system that is responsible for normal/routine operations, and (2) a Safety System consisting of Safety Instrumented Functions (SIFs) that provide formally verifiable independent oversight. We demonstrate Safe-ROS on an AgileX Scout Mini robot performing autonomous inspection in a nuclear environment. One safety requirement is selected and instantiated as a SIF. To support verification, we implement the SIF as a cognitive agent, programmed to stop the robot whenever it detects that it is too close to an obstacle. We verify that the agent meets the safety requirement and integrate it into the autonomous inspection. This integration is also verified, and the full deployment is validated in a Gazebo simulation, and lab testing. We evaluate this architecture in the context of the UK nuclear sector, where safety and regulation are crucial aspects of deployment. Success criteria include the development of a formal property from the safety requirement, implementation, and verification of the SIF, and the integration of the SIF into the operational robotic autonomous system. Our results demonstrate that the  Safe-ROS architecture can provide safety verifiable oversight while deploying autonomous robots in safety-critical domains, offering a robust framework that can be extended to additional requirements and various applications. 

**Abstract (ZH)**: 自主机器人在安全关键领域中的部署需要确保操作有效性与安全合规的架构。本文贡献了Safe-ROS架构，用于在这种领域中开发可靠的可验证自主机器人。该架构包含两个独立的子系统：（1）智能控制系统，负责常规操作；（2）安全系统，由安全仪表功能（SIFs）组成，提供形式可验证的独立监督。我们以核环境中灵活X侦察迷你机器人为例，演示Safe-ROS。选择了一个安全要求并实例化为一个SIF。为支持验证，我们实现该SIF为认知代理，编程使其在检测到机器人距离障碍物过近时停止。我们验证该代理满足了安全要求，并将其整合到自主检测中。此整合也进行了验证，并在Gazebo模拟和实验室测试中验证了完整部署。我们在英国核产业背景下评估了该架构，安全与法规是部署中的关键方面。成功标准包括从安全要求中开发出正式属性、实现和验证SIF，以及将SIF整合到运营自主机器人系统中。我们的结果表明，Safe-ROS架构可以在安全关键领域部署自主机器人时提供可验证的安全监督，提供一个稳健的框架，该框架可扩展至更多需求和各种应用。 

---
# Safe-ROS: An Architecture for Autonomous Robots in Safety-Critical Domains 

**Title (ZH)**: Safe-ROS：安全关键领域自主机器人架构 

**Authors**: Diana C. Benjumea, Marie Farrell, Louise A. Dennis  

**Link**: [PDF](https://arxiv.org/pdf/2511.14433)  

**Abstract**: Deploying autonomous robots in safety-critical domains requires architectures that ensure operational effectiveness and safety compliance. In this paper, we contribute the Safe-ROS architecture for developing reliable and verifiable autonomous robots in such domains. It features two distinct subsystems: (1) an intelligent control system that is responsible for normal/routine operations, and (2) a Safety System consisting of Safety Instrumented Functions (SIFs) that provide formally verifiable independent oversight. We demonstrate Safe-ROS on an AgileX Scout Mini robot performing autonomous inspection in a nuclear environment. One safety requirement is selected and instantiated as a SIF. To support verification, we implement the SIF as a cognitive agent, programmed to stop the robot whenever it detects that it is too close to an obstacle. We verify that the agent meets the safety requirement and integrate it into the autonomous inspection. This integration is also verified, and the full deployment is validated in a Gazebo simulation, and lab testing. We evaluate this architecture in the context of the UK nuclear sector, where safety and regulation are crucial aspects of deployment. Success criteria include the development of a formal property from the safety requirement, implementation, and verification of the SIF, and the integration of the SIF into the operational robotic autonomous system. Our results demonstrate that the  Safe-ROS architecture can provide safety verifiable oversight while deploying autonomous robots in safety-critical domains, offering a robust framework that can be extended to additional requirements and various applications. 

**Abstract (ZH)**: 在安全关键领域部署自主机器人需要确保运行有效性与安全合规的架构。本文贡献了Safe-ROS架构，用于在这些领域开发可靠和可验证的自主机器人。该架构包含两个独立的子系统：（1）智能控制系统，负责常规操作；（2）安全系统，包含安全仪表功能（SIFs），提供形式可验证的独立监督。我们在一个名为AgileX Scout Mini的机器人上，演示Safe-ROS进行在核环境中的自主巡检任务。选择了一个安全需求实例化为一个SIF。为支持验证，我们将SIF实现为一个认知代理，编程在其检测到机器人距离障碍物过近时停止机器人。我们验证该代理满足了安全需求，并将其整合到自主巡检中。该整合过程本身也得到了验证，并在Gazebo仿真和实验室测试中对整个部署进行了验证。我们在英国核产业的背景下评估了该架构，该领域中的安全和监管是部署的关键方面。成功标准包括从安全需求中开发出正式属性，实现和验证SIF，以及将SIF整合到操作性的自主机器人系统中。我们的结果表明，Safe-ROS架构可以在安全关键领域部署自主机器人时提供可验证的安全监督，提供了一个可以扩展到更多需求和应用场景的稳健框架。 

---
# Safe-ROS: An Architecture for Autonomous Robots in Safety-Critical Domains 

**Title (ZH)**: Safe-ROS：安全关键领域自主机器人的一种架构 

**Authors**: Diana C. Benjumea, Marie Farrell, Louise A. Dennis  

**Link**: [PDF](https://arxiv.org/pdf/2511.14433)  

**Abstract**: Deploying autonomous robots in safety-critical domains requires architectures that ensure operational effectiveness and safety compliance. In this paper, we contribute the Safe-ROS architecture for developing reliable and verifiable autonomous robots in such domains. It features two distinct subsystems: (1) an intelligent control system that is responsible for normal/routine operations, and (2) a Safety System consisting of Safety Instrumented Functions (SIFs) that provide formally verifiable independent oversight. We demonstrate Safe-ROS on an AgileX Scout Mini robot performing autonomous inspection in a nuclear environment. One safety requirement is selected and instantiated as a SIF. To support verification, we implement the SIF as a cognitive agent, programmed to stop the robot whenever it detects that it is too close to an obstacle. We verify that the agent meets the safety requirement and integrate it into the autonomous inspection. This integration is also verified, and the full deployment is validated in a Gazebo simulation, and lab testing. We evaluate this architecture in the context of the UK nuclear sector, where safety and regulation are crucial aspects of deployment. Success criteria include the development of a formal property from the safety requirement, implementation, and verification of the SIF, and the integration of the SIF into the operational robotic autonomous system. Our results demonstrate that the  Safe-ROS architecture can provide safety verifiable oversight while deploying autonomous robots in safety-critical domains, offering a robust framework that can be extended to additional requirements and various applications. 

**Abstract (ZH)**: 安全关键领域的自主机器人部署需要确保运营有效性和安全合规的架构。本文贡献了Safe-ROS架构，用于在这些领域开发可靠和可验证的自主机器人。该架构包含两个独立的子系统：（1）智能控制系统，负责常规操作；（2）安全系统，包括安全仪表函数（SIFs），提供形式可验证的独立监督。我们通过一个使用AgileX Scout Mini机器人在核环境中进行自主检查的应用案例，演示了Safe-ROS架构。选择了一个安全要求并实例化为SIF。为了支持验证，我们将SIF实现为一个认知代理，编程使其在检测到机器人与障碍物太近时停止。我们验证代理满足安全要求并将其集成到自主检查中，该集成也进行了验证，并在Gazebo仿真和实验室测试中验证了完整部署。我们以英国核行业为背景评估此架构，该行业对安全和监管有严格要求。成功标准包括从安全要求中推导出形式属性，实现和验证SIF，以及将SIF集成到运营自主系统中。我们的结果表明，Safe-ROS架构可以在安全关键领域部署自主机器人时提供可验证的安全监督，并提供一个可扩展到其他要求和各种应用的稳健框架。 

---
# Safe-ROS: An Architecture for Autonomous Robots in Safety-Critical Domains 

**Title (ZH)**: Safe-ROS：安全关键领域自主机器人的一种架构 

**Authors**: Diana C. Benjumea, Marie Farrell, Louise A. Dennis  

**Link**: [PDF](https://arxiv.org/pdf/2511.14433)  

**Abstract**: Deploying autonomous robots in safety-critical domains requires architectures that ensure operational effectiveness and safety compliance. In this paper, we contribute the Safe-ROS architecture for developing reliable and verifiable autonomous robots in such domains. It features two distinct subsystems: (1) an intelligent control system that is responsible for normal/routine operations, and (2) a Safety System consisting of Safety Instrumented Functions (SIFs) that provide formally verifiable independent oversight. We demonstrate Safe-ROS on an AgileX Scout Mini robot performing autonomous inspection in a nuclear environment. One safety requirement is selected and instantiated as a SIF. To support verification, we implement the SIF as a cognitive agent, programmed to stop the robot whenever it detects that it is too close to an obstacle. We verify that the agent meets the safety requirement and integrate it into the autonomous inspection. This integration is also verified, and the full deployment is validated in a Gazebo simulation, and lab testing. We evaluate this architecture in the context of the UK nuclear sector, where safety and regulation are crucial aspects of deployment. Success criteria include the development of a formal property from the safety requirement, implementation, and verification of the SIF, and the integration of the SIF into the operational robotic autonomous system. Our results demonstrate that the  Safe-ROS architecture can provide safety verifiable oversight while deploying autonomous robots in safety-critical domains, offering a robust framework that can be extended to additional requirements and various applications. 

**Abstract (ZH)**: 在安全关键领域部署自主机器人需要确保运行有效性与安全合规的架构。本文贡献了Safe-ROS架构，用于此类领域开发可靠可验证的自主机器人。该架构包含两个独立子系统：（1）智能控制系统，负责正常/常规操作；（2）安全系统，包括安全仪表功能（SIFs），提供形式可验证的独立监督。我们以AgileX Scout Mini机器人在核环境中进行自主检测为例，选取一个安全需求并实例化为一个SIF。为支持验证，我们将SIF实现为认知代理，编程在检测到机器人与障碍物距离过近时停止机器人。验证代理满足安全需求并在自主检测中集成。此集成亦进行了验证，并在Gazebo仿真和实验室测试中对完整部署进行了验证。我们在英国核产业背景下评估了该架构，安全与监管是部署中的关键因素。成功标准包括从安全需求开发形式属性、实现并验证SIF以及将SIF集成到运营自主机器人系统中。我们的结果证明，Safe-ROS架构可以在安全关键领域部署自主机器人时提供可验证的安全监督，提供一个可以扩展到更多要求和各种应用的稳健框架。 

---
# Identifying Time-varying Costs in Finite-horizon Linear Quadratic Gaussian Games 

**Title (ZH)**: 识别有限_horizon线性二次高斯博弈中的时变成本 

**Authors**: Kai Ren, Maryam Kamgarpour  

**Link**: [PDF](https://arxiv.org/pdf/2511.14358)  

**Abstract**: We address cost identification in a finite-horizon linear quadratic Gaussian game. We characterize the set of cost parameters that generate a given Nash equilibrium policy. We propose a backpropagation algorithm to identify the time-varying cost parameters. We derive a probabilistic error bound when the cost parameters are identified from finite trajectories. We test our method in numerical and driving simulations. Our algorithm identifies the cost parameters that can reproduce the Nash equilibrium policy and trajectory observations. 

**Abstract (ZH)**: 我们在有限时段线性二次高斯博弈中解决成本识别问题。我们刻画生成给定纳什均衡策略的成本参数集。我们提出一种反向传播算法来识别时变成本参数。我们推导出当从有限轨迹中识别成本参数时的概率误差界。我们在数值和驾驶模拟中测试了我们的方法。我们的算法能够识别出能够再现纳什均衡策略和轨迹观测的成本参数。 

---
# Identifying Time-varying Costs in Finite-horizon Linear Quadratic Gaussian Games 

**Title (ZH)**: 有限 horizon 线性二次高斯博弈中时变成本的识别 

**Authors**: Kai Ren, Maryam Kamgarpour  

**Link**: [PDF](https://arxiv.org/pdf/2511.14358)  

**Abstract**: We address cost identification in a finite-horizon linear quadratic Gaussian game. We characterize the set of cost parameters that generate a given Nash equilibrium policy. We propose a backpropagation algorithm to identify the time-varying cost parameters. We derive a probabilistic error bound when the cost parameters are identified from finite trajectories. We test our method in numerical and driving simulations. Our algorithm identifies the cost parameters that can reproduce the Nash equilibrium policy and trajectory observations. 

**Abstract (ZH)**: 我们在有限_horizon线性二次高斯博弈中解决成本识别问题。我们刻画生成给定纳什均衡策略的成本参数集。我们提出一种反向传播算法来识别时间变化的成本参数。我们推导出当从有限轨迹中识别成本参数时的概率误差界。我们在数值和驾驶模拟中测试了该方法。我们的算法能够识别出能够重现纳什均衡策略和轨迹观测的成本参数。 

---
# Identifying Time-varying Costs in Finite-horizon Linear Quadratic Gaussian Games 

**Title (ZH)**: 识别有限时段线性二次高斯博弈中的时变成本 

**Authors**: Kai Ren, Maryam Kamgarpour  

**Link**: [PDF](https://arxiv.org/pdf/2511.14358)  

**Abstract**: We address cost identification in a finite-horizon linear quadratic Gaussian game. We characterize the set of cost parameters that generate a given Nash equilibrium policy. We propose a backpropagation algorithm to identify the time-varying cost parameters. We derive a probabilistic error bound when the cost parameters are identified from finite trajectories. We test our method in numerical and driving simulations. Our algorithm identifies the cost parameters that can reproduce the Nash equilibrium policy and trajectory observations. 

**Abstract (ZH)**: 我们探讨在有限时段线性二次高斯博弈中的成本识别问题。我们刻画能够生成给定纳什均衡策略的成本参数集。我们提出一种反向传播算法来识别时变成本参数。我们推导出成本参数从有限轨迹中被识别时的概率误差界。我们在数值和驾驶模拟中测试了该方法。我们的算法能够识别出能够再现纳什均衡策略和轨迹观察的成本参数。 

---
# Identifying Time-varying Costs in Finite-horizon Linear Quadratic Gaussian Games 

**Title (ZH)**: 有限区间线性二次高斯博弈中时变成本的识别 

**Authors**: Kai Ren, Maryam Kamgarpour  

**Link**: [PDF](https://arxiv.org/pdf/2511.14358)  

**Abstract**: We address cost identification in a finite-horizon linear quadratic Gaussian game. We characterize the set of cost parameters that generate a given Nash equilibrium policy. We propose a backpropagation algorithm to identify the time-varying cost parameters. We derive a probabilistic error bound when the cost parameters are identified from finite trajectories. We test our method in numerical and driving simulations. Our algorithm identifies the cost parameters that can reproduce the Nash equilibrium policy and trajectory observations. 

**Abstract (ZH)**: 我们在有限_horizon 线性二次高斯博弈中解决成本识别问题。我们刻画生成给定纳什均衡策略的成本参数集。我们提出一种反向传播算法来识别时间变化的成本参数。我们推导出成本参数从有限轨迹中被识别时的概率误差上界。我们在数值和驾驶模拟中测试了我们的方法。我们的算法能够识别出能够重现纳什均衡策略和轨迹观察的成本参数。 

---
# Multi-Timescale Model Predictive Control for Slow-Fast Systems 

**Title (ZH)**: 多时间尺度模型预测控制在慢快速系统中的应用 

**Authors**: Lukas Schroth, Daniel Morton, Amon Lahr, Daniele Gammelli, Andrea Carron, Marco Pavone  

**Link**: [PDF](https://arxiv.org/pdf/2511.14311)  

**Abstract**: Model Predictive Control (MPC) has established itself as the primary methodology for constrained control, enabling autonomy across diverse applications. While model fidelity is crucial in MPC, solving the corresponding optimization problem in real time remains challenging when combining long horizons with high-fidelity models that capture both short-term dynamics and long-term behavior. Motivated by results on the Exponential Decay of Sensitivities (EDS), which imply that, under certain conditions, the influence of modeling inaccuracies decreases exponentially along the prediction horizon, this paper proposes a multi-timescale MPC scheme for fast-sampled control. Tailored to systems with both fast and slow dynamics, the proposed approach improves computational efficiency by i) switching to a reduced model that captures only the slow, dominant dynamics and ii) exponentially increasing integration step sizes to progressively reduce model detail along the horizon. We evaluate the method on three practically motivated robotic control problems in simulation and observe speed-ups of up to an order of magnitude. 

**Abstract (ZH)**: 基于指数衰减敏感性结果的多时间尺度模型预测控制方法 

---
# Multi-Timescale Model Predictive Control for Slow-Fast Systems 

**Title (ZH)**: 多时间尺度模型预测控制用于慢快系统 

**Authors**: Lukas Schroth, Daniel Morton, Amon Lahr, Daniele Gammelli, Andrea Carron, Marco Pavone  

**Link**: [PDF](https://arxiv.org/pdf/2511.14311)  

**Abstract**: Model Predictive Control (MPC) has established itself as the primary methodology for constrained control, enabling autonomy across diverse applications. While model fidelity is crucial in MPC, solving the corresponding optimization problem in real time remains challenging when combining long horizons with high-fidelity models that capture both short-term dynamics and long-term behavior. Motivated by results on the Exponential Decay of Sensitivities (EDS), which imply that, under certain conditions, the influence of modeling inaccuracies decreases exponentially along the prediction horizon, this paper proposes a multi-timescale MPC scheme for fast-sampled control. Tailored to systems with both fast and slow dynamics, the proposed approach improves computational efficiency by i) switching to a reduced model that captures only the slow, dominant dynamics and ii) exponentially increasing integration step sizes to progressively reduce model detail along the horizon. We evaluate the method on three practically motivated robotic control problems in simulation and observe speed-ups of up to an order of magnitude. 

**Abstract (ZH)**: 基于指数衰减敏感性的多时间尺度模型预测控制 

---
# Multi-Timescale Model Predictive Control for Slow-Fast Systems 

**Title (ZH)**: 多时间尺度模型预测控制方法及其应用于慢快系统 

**Authors**: Lukas Schroth, Daniel Morton, Amon Lahr, Daniele Gammelli, Andrea Carron, Marco Pavone  

**Link**: [PDF](https://arxiv.org/pdf/2511.14311)  

**Abstract**: Model Predictive Control (MPC) has established itself as the primary methodology for constrained control, enabling autonomy across diverse applications. While model fidelity is crucial in MPC, solving the corresponding optimization problem in real time remains challenging when combining long horizons with high-fidelity models that capture both short-term dynamics and long-term behavior. Motivated by results on the Exponential Decay of Sensitivities (EDS), which imply that, under certain conditions, the influence of modeling inaccuracies decreases exponentially along the prediction horizon, this paper proposes a multi-timescale MPC scheme for fast-sampled control. Tailored to systems with both fast and slow dynamics, the proposed approach improves computational efficiency by i) switching to a reduced model that captures only the slow, dominant dynamics and ii) exponentially increasing integration step sizes to progressively reduce model detail along the horizon. We evaluate the method on three practically motivated robotic control problems in simulation and observe speed-ups of up to an order of magnitude. 

**Abstract (ZH)**: 基于指数衰减敏感性的多时间尺度模型预测控制方法 

---
# Multi-Timescale Model Predictive Control for Slow-Fast Systems 

**Title (ZH)**: 多时间尺度模型预测控制方法及其在慢-快系统中的应用 

**Authors**: Lukas Schroth, Daniel Morton, Amon Lahr, Daniele Gammelli, Andrea Carron, Marco Pavone  

**Link**: [PDF](https://arxiv.org/pdf/2511.14311)  

**Abstract**: Model Predictive Control (MPC) has established itself as the primary methodology for constrained control, enabling autonomy across diverse applications. While model fidelity is crucial in MPC, solving the corresponding optimization problem in real time remains challenging when combining long horizons with high-fidelity models that capture both short-term dynamics and long-term behavior. Motivated by results on the Exponential Decay of Sensitivities (EDS), which imply that, under certain conditions, the influence of modeling inaccuracies decreases exponentially along the prediction horizon, this paper proposes a multi-timescale MPC scheme for fast-sampled control. Tailored to systems with both fast and slow dynamics, the proposed approach improves computational efficiency by i) switching to a reduced model that captures only the slow, dominant dynamics and ii) exponentially increasing integration step sizes to progressively reduce model detail along the horizon. We evaluate the method on three practically motivated robotic control problems in simulation and observe speed-ups of up to an order of magnitude. 

**Abstract (ZH)**: 基于指数衰减灵敏度的多时间尺度模型预测控制 

---
# Hessians in Birkhoff-Theoretic Trajectory Optimization 

**Title (ZH)**: Birkhoff理论轨迹优化中的海森矩阵 

**Authors**: I. M. Ross  

**Link**: [PDF](https://arxiv.org/pdf/2511.13963)  

**Abstract**: This paper derives various Hessians associated with Birkhoff-theoretic methods for trajectory optimization. According to a theorem proved in this paper, approximately 80% of the eigenvalues are contained in the narrow interval [-2, 4] for all Birkhoff-discretized optimal control problems. A preliminary analysis of computational complexity is also presented with further discussions on the grand challenge of solving a million point trajectory optimization problem. 

**Abstract (ZH)**: 本文推导了与Birkhoff理论方法在轨迹优化中相关的各种海森矩阵。根据本文证明的一个定理，几乎所有（约80%）的特征值都包含在狭小区间[-2, 4]内，涵盖所有Birkhoff离散化最优控制问题。还对计算复杂性进行了初步分析，并进一步讨论了解包含一百万点轨迹优化问题这一宏大挑战。 

---
# Hessians in Birkhoff-Theoretic Trajectory Optimization 

**Title (ZH)**: Birkhoff理论轨迹优化中的海森矩阵 

**Authors**: I. M. Ross  

**Link**: [PDF](https://arxiv.org/pdf/2511.13963)  

**Abstract**: This paper derives various Hessians associated with Birkhoff-theoretic methods for trajectory optimization. According to a theorem proved in this paper, approximately 80% of the eigenvalues are contained in the narrow interval [-2, 4] for all Birkhoff-discretized optimal control problems. A preliminary analysis of computational complexity is also presented with further discussions on the grand challenge of solving a million point trajectory optimization problem. 

**Abstract (ZH)**: 本文推导了与Birkhoff理论方法用于轨迹优化相关的各种海森矩阵。根据本文证明的定理，几乎所有（约为80%）的特征值都包含在[-2, 4]这个狭窄区间内，适用于所有Birkhoff离散化最优控制问题。还呈现了初步的计算复杂性分析，并进一步讨论了解包含百万点的轨迹优化问题这一宏伟挑战。 

---
# Hessians in Birkhoff-Theoretic Trajectory Optimization 

**Title (ZH)**: Birkhoff理论轨迹优化中的Hessian分析 

**Authors**: I. M. Ross  

**Link**: [PDF](https://arxiv.org/pdf/2511.13963)  

**Abstract**: This paper derives various Hessians associated with Birkhoff-theoretic methods for trajectory optimization. According to a theorem proved in this paper, approximately 80% of the eigenvalues are contained in the narrow interval [-2, 4] for all Birkhoff-discretized optimal control problems. A preliminary analysis of computational complexity is also presented with further discussions on the grand challenge of solving a million point trajectory optimization problem. 

**Abstract (ZH)**: 本文推导了与Birkhoff理论方法相关的各种海森矩阵，并证明了一个定理：所有Birkhoff离散化最优控制问题的特征值大约有80%位于狭窄区间[-2, 4]内。还呈现了初步的计算复杂度分析，并进一步讨论了解决百万点轨迹优化问题的重大挑战。 

---
# Hessians in Birkhoff-Theoretic Trajectory Optimization 

**Title (ZH)**: Birkhoff论域轨迹优化中的海森矩阵 

**Authors**: I. M. Ross  

**Link**: [PDF](https://arxiv.org/pdf/2511.13963)  

**Abstract**: This paper derives various Hessians associated with Birkhoff-theoretic methods for trajectory optimization. According to a theorem proved in this paper, approximately 80% of the eigenvalues are contained in the narrow interval [-2, 4] for all Birkhoff-discretized optimal control problems. A preliminary analysis of computational complexity is also presented with further discussions on the grand challenge of solving a million point trajectory optimization problem. 

**Abstract (ZH)**: 本文推导了与Birkhoff理论方法相关的各种Hessian矩阵。根据本文证明的一个定理，所有Birkhoff离散化的最优控制问题中约80%的特征值包含在狭窄的区间[-2, 4]内。还提供了初步的计算复杂度分析，并进一步讨论了一百万点轨迹优化问题的挑战。 

---
# A Trajectory-free Crash Detection Framework with Generative Approach and Segment Map Diffusion 

**Title (ZH)**: 基于生成方法和段地图扩散的无轨迹碰撞检测框架 

**Authors**: Weiying Shen, Hao Yu, Yu Dong, Pan Liu, Yu Han, Xin Wen  

**Link**: [PDF](https://arxiv.org/pdf/2511.13795)  

**Abstract**: Real-time crash detection is essential for developing proactive safety management strategy and enhancing overall traffic efficiency. To address the limitations associated with trajectory acquisition and vehicle tracking, road segment maps recording the individual-level traffic dynamic data were directly served in crash detection. A novel two-stage trajectory-free crash detection framework, was present to generate the rational future road segment map and identify crashes. The first-stage diffusion-based segment map generation model, Mapfusion, conducts a noisy-to-normal process that progressively adds noise to the road segment map until the map is corrupted to pure Gaussian noise. The denoising process is guided by sequential embedding components capturing the temporal dynamics of segment map sequences. Furthermore, the generation model is designed to incorporate background context through ControlNet to enhance generation control. Crash detection is achieved by comparing the monitored segment map with the generations from diffusion model in second stage. Trained on non-crash vehicle motion data, Mapfusion successfully generates realistic road segment evolution maps based on learned motion patterns and remains robust across different sampling intervals. Experiments on real-world crashes indicate the effectiveness of the proposed two-stage method in accurately detecting crashes. 

**Abstract (ZH)**: 基于路段地图的无轨迹实时碰撞检测框架 

---
# A Trajectory-free Crash Detection Framework with Generative Approach and Segment Map Diffusion 

**Title (ZH)**: 基于生成方法和段落地图扩散的无轨迹碰撞检测框架 

**Authors**: Weiying Shen, Hao Yu, Yu Dong, Pan Liu, Yu Han, Xin Wen  

**Link**: [PDF](https://arxiv.org/pdf/2511.13795)  

**Abstract**: Real-time crash detection is essential for developing proactive safety management strategy and enhancing overall traffic efficiency. To address the limitations associated with trajectory acquisition and vehicle tracking, road segment maps recording the individual-level traffic dynamic data were directly served in crash detection. A novel two-stage trajectory-free crash detection framework, was present to generate the rational future road segment map and identify crashes. The first-stage diffusion-based segment map generation model, Mapfusion, conducts a noisy-to-normal process that progressively adds noise to the road segment map until the map is corrupted to pure Gaussian noise. The denoising process is guided by sequential embedding components capturing the temporal dynamics of segment map sequences. Furthermore, the generation model is designed to incorporate background context through ControlNet to enhance generation control. Crash detection is achieved by comparing the monitored segment map with the generations from diffusion model in second stage. Trained on non-crash vehicle motion data, Mapfusion successfully generates realistic road segment evolution maps based on learned motion patterns and remains robust across different sampling intervals. Experiments on real-world crashes indicate the effectiveness of the proposed two-stage method in accurately detecting crashes. 

**Abstract (ZH)**: 实时碰撞检测对于开发主动安全管理策略和提升整体交通效率至关重要。为了解决轨迹获取和车辆跟踪的限制，直接使用记录个体级交通动态数据的道路区段地图用于碰撞检测。提出了一种新的两阶段无轨迹碰撞检测框架，以生成合理的未来道路区段地图并识别碰撞。第一阶段基于扩散的道路区段地图生成模型Mapfusion，通过逐步向道路区段地图添加噪声直至地图被完全污染为高斯噪声，实现去噪过程。去噪过程由捕捉区段地图序列中时间动态的顺序嵌入组件指导。此外，生成模型设计了通过ControlNet纳入背景上下文以增强生成控制。碰撞检测通过将监测的道路区段地图与第二阶段扩散模型的生成结果进行比较实现。Mapfusion基于非碰撞车辆运动数据训练，成功基于学习到的运动模式生成逼真道路区段进化地图，并在不同的采样间隔下保持稳健。实验表明，所提出的两阶段方法在准确检测碰撞方面具有有效性。 

---
# A Trajectory-free Crash Detection Framework with Generative Approach and Segment Map Diffusion 

**Title (ZH)**: 基于生成方法和段落地图扩散的无轨迹碰撞检测框架 

**Authors**: Weiying Shen, Hao Yu, Yu Dong, Pan Liu, Yu Han, Xin Wen  

**Link**: [PDF](https://arxiv.org/pdf/2511.13795)  

**Abstract**: Real-time crash detection is essential for developing proactive safety management strategy and enhancing overall traffic efficiency. To address the limitations associated with trajectory acquisition and vehicle tracking, road segment maps recording the individual-level traffic dynamic data were directly served in crash detection. A novel two-stage trajectory-free crash detection framework, was present to generate the rational future road segment map and identify crashes. The first-stage diffusion-based segment map generation model, Mapfusion, conducts a noisy-to-normal process that progressively adds noise to the road segment map until the map is corrupted to pure Gaussian noise. The denoising process is guided by sequential embedding components capturing the temporal dynamics of segment map sequences. Furthermore, the generation model is designed to incorporate background context through ControlNet to enhance generation control. Crash detection is achieved by comparing the monitored segment map with the generations from diffusion model in second stage. Trained on non-crash vehicle motion data, Mapfusion successfully generates realistic road segment evolution maps based on learned motion patterns and remains robust across different sampling intervals. Experiments on real-world crashes indicate the effectiveness of the proposed two-stage method in accurately detecting crashes. 

**Abstract (ZH)**: 实时碰撞检测对于开发主动安全管理策略和提高整体交通效率至关重要。为了解决轨迹获取和车辆跟踪的限制，利用记录个体级交通动态数据的道路段落地图直接用于碰撞检测。提出了一种新颖的无轨迹两阶段碰撞检测框架，用于生成合理的未来道路段落地图并识别碰撞。第一阶段基于扩散的段落地图生成模型Mapfusion，通过逐步向道路段落地图添加噪声，直到地图被破坏为纯高斯噪声，进行噪声到正常的转换过程。去噪过程由捕捉段落地图序列时序动态的顺序嵌入组件引导。此外，生成模型通过ControlNet融入背景上下文，以增强生成控制。通过将第二阶段扩散模型生成的地图与监测的地图进行比较实现碰撞检测。Mapfusion基于非碰撞车辆运动数据进行训练，成功生成基于学习到的运动模式的现实道路段落演变地图，并在不同的采样间隔下保持鲁棒性。实验证明所提出两阶段方法在准确检测碰撞方面的有效性。 

---
# A Trajectory-free Crash Detection Framework with Generative Approach and Segment Map Diffusion 

**Title (ZH)**: 基于生成方法和段落地图扩散的无轨迹碰撞检测框架 

**Authors**: Weiying Shen, Hao Yu, Yu Dong, Pan Liu, Yu Han, Xin Wen  

**Link**: [PDF](https://arxiv.org/pdf/2511.13795)  

**Abstract**: Real-time crash detection is essential for developing proactive safety management strategy and enhancing overall traffic efficiency. To address the limitations associated with trajectory acquisition and vehicle tracking, road segment maps recording the individual-level traffic dynamic data were directly served in crash detection. A novel two-stage trajectory-free crash detection framework, was present to generate the rational future road segment map and identify crashes. The first-stage diffusion-based segment map generation model, Mapfusion, conducts a noisy-to-normal process that progressively adds noise to the road segment map until the map is corrupted to pure Gaussian noise. The denoising process is guided by sequential embedding components capturing the temporal dynamics of segment map sequences. Furthermore, the generation model is designed to incorporate background context through ControlNet to enhance generation control. Crash detection is achieved by comparing the monitored segment map with the generations from diffusion model in second stage. Trained on non-crash vehicle motion data, Mapfusion successfully generates realistic road segment evolution maps based on learned motion patterns and remains robust across different sampling intervals. Experiments on real-world crashes indicate the effectiveness of the proposed two-stage method in accurately detecting crashes. 

**Abstract (ZH)**: 实时碰撞检测对于发展前瞻性安全管理策略和提升整体交通效率至关重要。为了解决轨迹获取和车辆跟踪的限制，采用了记录个体级交通动态数据的道路段落地图进行碰撞检测。提出了一种新颖的两阶段轨迹无导向碰撞检测框架，以生成合理的未来道路段落地图并识别碰撞。在第一阶段，基于扩散的段落地图生成模型Mapfusion通过逐步向道路段落地图添加噪声，直到地图被腐蚀为纯高斯噪声，实现从嘈杂到正常的转换过程。去噪过程由捕捉段落地图序列时空动态的顺序嵌入组件引导。此外，生成模型通过ControlNet整合背景上下文，以增强生成控制。碰撞检测通过将监测的段落地图与第二阶段扩散模型的生成结果进行比较来实现。Mapfusion在非碰撞车辆运动数据上训练，能够基于学习到的运动模式生成真实的道路段落演化地图，并且能够跨越不同的采样间隔保持鲁棒性。实验表明，所提出的两阶段方法在准确检测碰撞方面是有效的。 

---
# nuCarla: A nuScenes-Style Bird's-Eye View Perception Dataset for CARLA Simulation 

**Title (ZH)**: nuCarla：一种基于nuScenes样式鸟瞰图感知的数据集用于CARLA仿真 

**Authors**: Zhijie Qiao, Zhong Cao, Henry X. Liu  

**Link**: [PDF](https://arxiv.org/pdf/2511.13744)  

**Abstract**: End-to-end (E2E) autonomous driving heavily relies on closed-loop simulation, where perception, planning, and control are jointly trained and evaluated in interactive environments. Yet, most existing datasets are collected from the real world under non-interactive conditions, primarily supporting open-loop learning while offering limited value for closed-loop testing. Due to the lack of standardized, large-scale, and thoroughly verified datasets to facilitate learning of meaningful intermediate representations, such as bird's-eye-view (BEV) features, closed-loop E2E models remain far behind even simple rule-based baselines. To address this challenge, we introduce nuCarla, a large-scale, nuScenes-style BEV perception dataset built within the CARLA simulator. nuCarla features (1) full compatibility with the nuScenes format, enabling seamless transfer of real-world perception models; (2) a dataset scale comparable to nuScenes, but with more balanced class distributions; (3) direct usability for closed-loop simulation deployment; and (4) high-performance BEV backbones that achieve state-of-the-art detection results. By providing both data and models as open benchmarks, nuCarla substantially accelerates closed-loop E2E development, paving the way toward reliable and safety-aware research in autonomous driving. 

**Abstract (ZH)**: 端到端自主驾驶高度依赖闭环仿真，其中感知、规划和控制在互动环境中联合训练和评估。然而，大多数现有数据集是在非互动条件下从现实世界中收集的，主要支持开环学习，对闭环测试的价值有限。由于缺乏标准化、大规模且彻底验证的数据集来促进有意义中间表示（如鸟瞰图特征）的学习，闭环端到端模型仍然落后于简单的基于规则的基准模型。为解决这一挑战，我们引入了nuCarla，这是一种在CARLA模拟器中构建的具有nuScenes风格的鸟瞰图感知数据集。nuCarla的特点包括：（1）完全兼容nuScenes格式，便于将真实世界的感知模型无缝转移；（2）与nuScenes相当的数据集规模，但类别分布更为均衡；（3）直接支持闭环仿真部署；（4）高性能的鸟瞰图骨干网络，实现最先进的检测结果。通过提供数据和模型作为开放基准，nuCarla显著加速了闭环端到端模型的发展，为自主驾驶的可靠性和安全性研究铺平了道路。 

---
# nuCarla: A nuScenes-Style Bird's-Eye View Perception Dataset for CARLA Simulation 

**Title (ZH)**: nuCarla：CARLA仿真中的nuScenes风格鸟瞰视角感知数据集 

**Authors**: Zhijie Qiao, Zhong Cao, Henry X. Liu  

**Link**: [PDF](https://arxiv.org/pdf/2511.13744)  

**Abstract**: End-to-end (E2E) autonomous driving heavily relies on closed-loop simulation, where perception, planning, and control are jointly trained and evaluated in interactive environments. Yet, most existing datasets are collected from the real world under non-interactive conditions, primarily supporting open-loop learning while offering limited value for closed-loop testing. Due to the lack of standardized, large-scale, and thoroughly verified datasets to facilitate learning of meaningful intermediate representations, such as bird's-eye-view (BEV) features, closed-loop E2E models remain far behind even simple rule-based baselines. To address this challenge, we introduce nuCarla, a large-scale, nuScenes-style BEV perception dataset built within the CARLA simulator. nuCarla features (1) full compatibility with the nuScenes format, enabling seamless transfer of real-world perception models; (2) a dataset scale comparable to nuScenes, but with more balanced class distributions; (3) direct usability for closed-loop simulation deployment; and (4) high-performance BEV backbones that achieve state-of-the-art detection results. By providing both data and models as open benchmarks, nuCarla substantially accelerates closed-loop E2E development, paving the way toward reliable and safety-aware research in autonomous driving. 

**Abstract (ZH)**: 端到端自主驾驶高度依赖闭环仿真，其中感知、规划和控制在交互环境中联合训练和评估。然而，现有大多数数据集在非交互条件下从现实世界收集，主要支持开环学习，对闭环测试提供的价值有限。由于缺乏标准化、大规模且经过全面验证的数据集，以促进有意义的中间表示学习，如鸟瞰图（BEV）特征，闭环端到端模型仍然落后于简单的基于规则的基础模型。为解决这一挑战，我们引入了nuCarla，一个在CARLA仿真器中构建的、具有nuScenes风格的BEV感知数据集。nuCarla的特点包括：（1）完全兼容nuScenes格式，使现实世界的感知模型无缝转移；（2）数据集规模与nuScenes相当，但类别分布更加平衡；（3）直接适用于闭环仿真部署；（4）高性能BEV骨干网，实现最先进的检测结果。通过提供开放基准的数据和模型，nuCarla大幅加速了闭环端到端开发，为自主驾驶中的可靠和安全研究铺平了道路。 

---
# nuCarla: A nuScenes-Style Bird's-Eye View Perception Dataset for CARLA Simulation 

**Title (ZH)**: nuCarla: 一个基于nuScenes的鸟瞰视角感知数据集用于CARLA模拟环境 

**Authors**: Zhijie Qiao, Zhong Cao, Henry X. Liu  

**Link**: [PDF](https://arxiv.org/pdf/2511.13744)  

**Abstract**: End-to-end (E2E) autonomous driving heavily relies on closed-loop simulation, where perception, planning, and control are jointly trained and evaluated in interactive environments. Yet, most existing datasets are collected from the real world under non-interactive conditions, primarily supporting open-loop learning while offering limited value for closed-loop testing. Due to the lack of standardized, large-scale, and thoroughly verified datasets to facilitate learning of meaningful intermediate representations, such as bird's-eye-view (BEV) features, closed-loop E2E models remain far behind even simple rule-based baselines. To address this challenge, we introduce nuCarla, a large-scale, nuScenes-style BEV perception dataset built within the CARLA simulator. nuCarla features (1) full compatibility with the nuScenes format, enabling seamless transfer of real-world perception models; (2) a dataset scale comparable to nuScenes, but with more balanced class distributions; (3) direct usability for closed-loop simulation deployment; and (4) high-performance BEV backbones that achieve state-of-the-art detection results. By providing both data and models as open benchmarks, nuCarla substantially accelerates closed-loop E2E development, paving the way toward reliable and safety-aware research in autonomous driving. 

**Abstract (ZH)**: 端到端自主驾驶高度依赖闭环模拟，其中感知、规划和控制在交互环境中共同训练和评估。然而，现有的大多数数据集是在非交互条件下从现实世界收集的，主要支持开环学习，对于闭环测试的价值有限。由于缺乏标准化、大规模且彻底验证的数据集来促进有意义中间表示（如顶视图特征）的学习，闭环端到端模型仍然落后于简单的基于规则的基础模型。为了应对这一挑战，我们引入了nuCarla，这是一个在CARLA模拟器中构建的、具有nuScenes风格的顶视图感知数据集。nuCarla的特点包括：（1）完全兼容nuScenes格式，使现实世界的感知模型能够无缝迁移；（2）数据集规模与nuScenes相当，但类别分布更均衡；（3）直接适用于闭环模拟部署；和（4）高性能的顶视图骨干网络，实现了最先进的检测结果。通过提供数据和模型作为开放基准，nuCarla显著加速了闭环端到端模型的发展，为自主驾驶领域可靠性和安全性研究铺平了道路。 

---
# Who Moved My Distribution? Conformal Prediction for Interactive Multi-Agent Systems 

**Title (ZH)**: 谁移动了我的分布？交互式多agent系统的置信预测 

**Authors**: Allen Emmanuel Binny, Anushri Dixit  

**Link**: [PDF](https://arxiv.org/pdf/2511.11567)  

**Abstract**: Uncertainty-aware prediction is essential for safe motion planning, especially when using learned models to forecast the behavior of surrounding agents. Conformal prediction is a statistical tool often used to produce uncertainty-aware prediction regions for machine learning models. Most existing frameworks utilizing conformal prediction-based uncertainty predictions assume that the surrounding agents are non-interactive. This is because in closed-loop, as uncertainty-aware agents change their behavior to account for prediction uncertainty, the surrounding agents respond to this change, leading to a distribution shift which we call endogenous distribution shift. To address this challenge, we introduce an iterative conformal prediction framework that systematically adapts the uncertainty-aware ego-agent controller to the endogenous distribution shift. The proposed method provides probabilistic safety guarantees while adapting to the evolving behavior of reactive, non-ego agents. We establish a model for the endogenous distribution shift and provide the conditions for the iterative conformal prediction pipeline to converge under such a distribution shift. We validate our framework in simulation for 2- and 3- agent interaction scenarios, demonstrating collision avoidance without resulting in overly conservative behavior and an overall improvement in success rates of up to 9.6% compared to other conformal prediction-based baselines. 

**Abstract (ZH)**: 面向安全运动规划的不确定性感知预测至关重要：迭代自适应 conformal 预测框架及其在内生分布偏移中的应用 

---
# nuCarla: A nuScenes-Style Bird's-Eye View Perception Dataset for CARLA Simulation 

**Title (ZH)**: nuCarla: 一个基于nuScenes风格的Bird's-Eye-View感知数据集用于CARLA模拟 

**Authors**: Zhijie Qiao, Zhong Cao, Henry X. Liu  

**Link**: [PDF](https://arxiv.org/pdf/2511.13744)  

**Abstract**: End-to-end (E2E) autonomous driving heavily relies on closed-loop simulation, where perception, planning, and control are jointly trained and evaluated in interactive environments. Yet, most existing datasets are collected from the real world under non-interactive conditions, primarily supporting open-loop learning while offering limited value for closed-loop testing. Due to the lack of standardized, large-scale, and thoroughly verified datasets to facilitate learning of meaningful intermediate representations, such as bird's-eye-view (BEV) features, closed-loop E2E models remain far behind even simple rule-based baselines. To address this challenge, we introduce nuCarla, a large-scale, nuScenes-style BEV perception dataset built within the CARLA simulator. nuCarla features (1) full compatibility with the nuScenes format, enabling seamless transfer of real-world perception models; (2) a dataset scale comparable to nuScenes, but with more balanced class distributions; (3) direct usability for closed-loop simulation deployment; and (4) high-performance BEV backbones that achieve state-of-the-art detection results. By providing both data and models as open benchmarks, nuCarla substantially accelerates closed-loop E2E development, paving the way toward reliable and safety-aware research in autonomous driving. 

**Abstract (ZH)**: 端到端自动驾驶高度依赖闭环模拟，其中感知、规划和控制在互动环境中联合训练和评估。然而，现有大多数数据集是在非互动条件下从现实世界收集的，主要支持开环学习，对闭环测试提供的价值有限。由于缺乏标准化、大规模且彻底验证的数据集来促进有意义中间表示的学习，如鸟瞰视图（BEV）特征，闭环端到端模型仍落后于简单的基于规则的基线。为应对这一挑战，我们引入了nuCarla，这是一个在CARLA模拟器中构建的、仿照nuScenes格式的BEV感知数据集。（1）完全兼容nuScenes格式，使现实世界感知模型无缝转移；（2）数据集规模与nuScenes相当，但类分布更均衡；（3）直接适用于闭环模拟部署；（4）高性能BEV骨干网络，实现了最先进的检测结果。通过提供开放基准的数据和模型，nuCarla显著加速了闭环端到端开发，为自动驾驶可靠性和安全意识研究铺平了道路。 

---
# Who Moved My Distribution? Conformal Prediction for Interactive Multi-Agent Systems 

**Title (ZH)**: 谁移动了我的分布？交互式多智能体系统的同构预测 

**Authors**: Allen Emmanuel Binny, Anushri Dixit  

**Link**: [PDF](https://arxiv.org/pdf/2511.11567)  

**Abstract**: Uncertainty-aware prediction is essential for safe motion planning, especially when using learned models to forecast the behavior of surrounding agents. Conformal prediction is a statistical tool often used to produce uncertainty-aware prediction regions for machine learning models. Most existing frameworks utilizing conformal prediction-based uncertainty predictions assume that the surrounding agents are non-interactive. This is because in closed-loop, as uncertainty-aware agents change their behavior to account for prediction uncertainty, the surrounding agents respond to this change, leading to a distribution shift which we call endogenous distribution shift. To address this challenge, we introduce an iterative conformal prediction framework that systematically adapts the uncertainty-aware ego-agent controller to the endogenous distribution shift. The proposed method provides probabilistic safety guarantees while adapting to the evolving behavior of reactive, non-ego agents. We establish a model for the endogenous distribution shift and provide the conditions for the iterative conformal prediction pipeline to converge under such a distribution shift. We validate our framework in simulation for 2- and 3- agent interaction scenarios, demonstrating collision avoidance without resulting in overly conservative behavior and an overall improvement in success rates of up to 9.6% compared to other conformal prediction-based baselines. 

**Abstract (ZH)**: 不确定性意识预测对于安全运动规划至关重要，尤其是在使用学习模型预测周围代理行为时。基于统一预测的不确定性预测通常假设周围代理是非交互的。这是因为闭环中，随着不确定性意识代理改变其行为以应对预测不确定性，周围代理会对此变化做出响应，导致我们称之为内生分布转移的分布偏移。为了解决这一挑战，我们提出了一种迭代统一预测框架，该框架系统地适应内生分布转移。所提出的方法在适应反应性非交界代理不断变化行为的同时提供概率安全保证。我们建立了内生分布转移的模型，并提供了在 such a 分布转移下迭代统一预测管道收敛的条件。我们在包含2个和3个代理的交互场景中进行了仿真验证，证明了可以避免碰撞且不会导致过度保守的行为，并且与基于其他统一预测的方法相比，总体成功率提高了最多9.6%。 

---
# Who Moved My Distribution? Conformal Prediction for Interactive Multi-Agent Systems 

**Title (ZH)**: 谁动了我的分布？交互式多智能体系统的置信预测 

**Authors**: Allen Emmanuel Binny, Anushri Dixit  

**Link**: [PDF](https://arxiv.org/pdf/2511.11567)  

**Abstract**: Uncertainty-aware prediction is essential for safe motion planning, especially when using learned models to forecast the behavior of surrounding agents. Conformal prediction is a statistical tool often used to produce uncertainty-aware prediction regions for machine learning models. Most existing frameworks utilizing conformal prediction-based uncertainty predictions assume that the surrounding agents are non-interactive. This is because in closed-loop, as uncertainty-aware agents change their behavior to account for prediction uncertainty, the surrounding agents respond to this change, leading to a distribution shift which we call endogenous distribution shift. To address this challenge, we introduce an iterative conformal prediction framework that systematically adapts the uncertainty-aware ego-agent controller to the endogenous distribution shift. The proposed method provides probabilistic safety guarantees while adapting to the evolving behavior of reactive, non-ego agents. We establish a model for the endogenous distribution shift and provide the conditions for the iterative conformal prediction pipeline to converge under such a distribution shift. We validate our framework in simulation for 2- and 3- agent interaction scenarios, demonstrating collision avoidance without resulting in overly conservative behavior and an overall improvement in success rates of up to 9.6% compared to other conformal prediction-based baselines. 

**Abstract (ZH)**: 不确定性意识的预测对于安全运动规划至关重要，尤其是在使用学习模型预测周围代理行为时。共形预测是一种常用于为机器学习模型生成不确定性意识预测区域的统计工具。现有的大多数基于共形预测的不确定性预测框架假设周围代理是非交互的。这是因为，在闭环中，当不确定性意识代理改变行为以应对预测不确定性时，周围的代理会对此做出反应，导致我们称之为内生分布转换的分布变化。为解决这一挑战，我们引入了一种迭代共形预测框架，系统地使不确定性意识的自身代理控制器适应内生分布转换。所提出的方法提供了概率安全保证，并能够适应反应性非自身代理行为的演变。我们建立了内生分布转换的模型，并提供了在该分布转换下迭代共形预测流水线收敛的条件。我们在具有2个和3个代理交互场景的仿真实验中验证了我们的框架，结果表明能够避免碰撞且不会导致过于保守的行为，并且成功率为其他基于共形预测的基线高出9.6%。 

---
# Who Moved My Distribution? Conformal Prediction for Interactive Multi-Agent Systems 

**Title (ZH)**: 谁移走了我的分布？交互式多智能体系统的同核预测 

**Authors**: Allen Emmanuel Binny, Anushri Dixit  

**Link**: [PDF](https://arxiv.org/pdf/2511.11567)  

**Abstract**: Uncertainty-aware prediction is essential for safe motion planning, especially when using learned models to forecast the behavior of surrounding agents. Conformal prediction is a statistical tool often used to produce uncertainty-aware prediction regions for machine learning models. Most existing frameworks utilizing conformal prediction-based uncertainty predictions assume that the surrounding agents are non-interactive. This is because in closed-loop, as uncertainty-aware agents change their behavior to account for prediction uncertainty, the surrounding agents respond to this change, leading to a distribution shift which we call endogenous distribution shift. To address this challenge, we introduce an iterative conformal prediction framework that systematically adapts the uncertainty-aware ego-agent controller to the endogenous distribution shift. The proposed method provides probabilistic safety guarantees while adapting to the evolving behavior of reactive, non-ego agents. We establish a model for the endogenous distribution shift and provide the conditions for the iterative conformal prediction pipeline to converge under such a distribution shift. We validate our framework in simulation for 2- and 3- agent interaction scenarios, demonstrating collision avoidance without resulting in overly conservative behavior and an overall improvement in success rates of up to 9.6% compared to other conformal prediction-based baselines. 

**Abstract (ZH)**: 基于不确定性预测的安全运动规划至关重要，尤其是在使用学习模型预测周围代理行为时。内生分布迁移下的迭代自适应典型预测框架为反应性非ego代理的 evolving 行为提供了概率安全保证。我们建立了内生分布迁移模型，并提供了在该分布迁移下迭代典型预测管道收敛的条件。我们在模拟的2-和3-代理交互场景中验证了该框架，展示了有效的碰撞避免且未导致过于保守的行为，并在成功率上提高了最多9.6%，优于其他基于典型预测的基线方法。 

---
