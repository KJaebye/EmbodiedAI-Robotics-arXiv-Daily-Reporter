{'arxiv_id': 'arXiv:2508.14042', 'title': 'Train Once, Deploy Anywhere: Realize Data-Efficient Dynamic Object Manipulation', 'authors': 'Zhuoling Li, Xiaoyang Wu, Zhenhua Xu, Hengshuang Zhao', 'link': 'https://arxiv.org/abs/2508.14042', 'abstract': 'Realizing generalizable dynamic object manipulation is important for enhancing manufacturing efficiency, as it eliminates specialized engineering for various scenarios. To this end, imitation learning emerges as a promising paradigm, leveraging expert demonstrations to teach a policy manipulation skills. Although the generalization of an imitation learning policy can be improved by increasing demonstrations, demonstration collection is labor-intensive. To address this problem, this paper investigates whether strong generalization in dynamic object manipulation is achievable with only a few demonstrations. Specifically, we develop an entropy-based theoretical framework to quantify the optimization of imitation learning. Based on this framework, we propose a system named Generalizable Entropy-based Manipulation (GEM). Extensive experiments in simulated and real tasks demonstrate that GEM can generalize across diverse environment backgrounds, robot embodiments, motion dynamics, and object geometries. Notably, GEM has been deployed in a real canteen for tableware collection. Without any in-scene demonstration, it achieves a success rate of over 97% across more than 10,000 operations.', 'abstract_zh': '基于熵的通用动态对象操作系统（GEM）', 'title_zh': '一次训练，随处部署：实现高效的动态物体操控'}
{'arxiv_id': 'arXiv:2508.13998', 'title': 'Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation', 'authors': 'Yifu Yuan, Haiqin Cui, Yaoting Huang, Yibin Chen, Fei Ni, Zibin Dong, Pengyi Li, Yan Zheng, Jianye Hao', 'link': 'https://arxiv.org/abs/2508.13998', 'abstract': 'Generalization in embodied AI is hindered by the "seeing-to-doing gap," which stems from data scarcity and embodiment heterogeneity. To address this, we pioneer "pointing" as a unified, embodiment-agnostic intermediate representation, defining four core embodied pointing abilities that bridge high-level vision-language comprehension with low-level action primitives. We introduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed for embodied reasoning and pointing. We use a wide range of embodied and general visual reasoning datasets as sources to construct a large-scale dataset, Embodied-Points-200K, which supports key embodied pointing capabilities. We then train Embodied-R1 using a two-stage Reinforced Fine-tuning (RFT) curriculum with a specialized multi-task reward design. Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and pointing benchmarks. Critically, it demonstrates robust zero-shot generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5% across 8 real-world XArm tasks without any task-specific fine-tuning, representing a 62% improvement over strong baselines. Furthermore, the model exhibits high robustness against diverse visual disturbances. Our work shows that a pointing-centric representation, combined with an RFT training paradigm, offers an effective and generalizable pathway to closing the perception-action gap in robotics.', 'abstract_zh': 'Generalized Embodied AI is Impeded by the "Seeing-to-Doing Gap," Addressed by a Unified Pointing Representation and Reinforced Fine-tuning Curriculum', 'title_zh': '体态-R1：强化体态推理在通用机器人操作中的应用'}
{'arxiv_id': 'arXiv:2508.13976', 'title': 'Toward an Interaction-Centered Approach to Robot Trustworthiness', 'authors': 'Carlo Mazzola, Hassan Ali, Kristína Malinovská, Igor Farkaš', 'link': 'https://arxiv.org/abs/2508.13976', 'abstract': 'As robots get more integrated into human environments, fostering trustworthiness in embodied robotic agents becomes paramount for an effective and safe human-robot interaction (HRI). To achieve that, HRI applications must promote human trust that aligns with robot skills and avoid misplaced trust or overtrust, which can pose safety risks and ethical concerns. To achieve that, HRI applications must promote human trust that aligns with robot skills and avoid misplaced trust or overtrust, which can pose safety risks and ethical concerns. In this position paper, we outline an interaction-based framework for building trust through mutual understanding between humans and robots. We emphasize two main pillars: human awareness and transparency, referring to the robot ability to interpret human actions accurately and to clearly communicate its intentions and goals, respectively. By integrating these two pillars, robots can behave in a manner that aligns with human expectations and needs while providing their human partners with both comprehension and control over their actions. We also introduce four components that we think are important for bridging the gap between a human-perceived sense of trust and a robot true capabilities.', 'abstract_zh': '随着机器人在人类环境中的整合程度提高，培养实体机器人代理的信任度对于实现有效安全的人机交互（HRI）至关重要。为此，HRI应用必须促进与机器人技能相一致的人类信任，避免错误信任或过度信任，这些可能会带来安全风险和伦理问题。在此观点论文中，我们提出了一个基于交互的框架，旨在通过人机之间的相互理解来建立信任。我们强调了两个主要支柱：人类意识和透明度，前者指机器人能够准确解读人类行为，后者指机器人清晰地沟通其意图和目标。通过整合这两个支柱，机器人可以表现出符合人类期望和需求的行为，同时为人类伙伴提供对其行动的理解和控制。我们还介绍了我们认为对于弥合人类感知的信任感与机器人实际能力之间差距的四个关键组成部分。', 'title_zh': '面向交互中心的机器人可信赖性方法'}
{'arxiv_id': 'arXiv:2508.13901', 'title': 'Multimodal Data Storage and Retrieval for Embodied AI: A Survey', 'authors': 'Yihao Lu, Hao Tang', 'link': 'https://arxiv.org/abs/2508.13901', 'abstract': "Embodied AI (EAI) agents continuously interact with the physical world, generating vast, heterogeneous multimodal data streams that traditional management systems are ill-equipped to handle. In this survey, we first systematically evaluate five storage architectures (Graph Databases, Multi-Model Databases, Data Lakes, Vector Databases, and Time-Series Databases), focusing on their suitability for addressing EAI's core requirements, including physical grounding, low-latency access, and dynamic scalability. We then analyze five retrieval paradigms (Fusion Strategy-Based Retrieval, Representation Alignment-Based Retrieval, Graph-Structure-Based Retrieval, Generation Model-Based Retrieval, and Efficient Retrieval-Based Optimization), revealing a fundamental tension between achieving long-term semantic coherence and maintaining real-time responsiveness. Based on this comprehensive analysis, we identify key bottlenecks, spanning from the foundational Physical Grounding Gap to systemic challenges in cross-modal integration, dynamic adaptation, and open-world generalization. Finally, we outline a forward-looking research agenda encompassing physics-aware data models, adaptive storage-retrieval co-optimization, and standardized benchmarking, to guide future research toward principled data management solutions for EAI. Our survey is based on a comprehensive review of more than 180 related studies, providing a rigorous roadmap for designing the robust, high-performance data management frameworks essential for the next generation of autonomous embodied systems.", 'abstract_zh': '机器人体感智能（EAI）代理连续与物理世界互动，生成大量异构多模态数据流，传统管理系统难以处理。在本文综述中，我们首先系统性评估了五种存储架构（图数据库、多模型数据库、数据湖、向量数据库和时间序列数据库），重点关注它们满足EAI核心要求（包括物理接地、低延迟访问和动态扩展）的适用性。接着，我们分析了五种检索范式（融合策略基检索、表示对齐基检索、图结构基检索、生成模型基检索和高效检索基优化），揭示了长期语义一致性与实时响应性之间的基本矛盾。基于这一综合分析，我们识别了关键瓶颈，从基础的物理接地差距到跨模态集成、动态适应和开放世界泛化的系统性挑战。最后，我们概述了一个前瞻性的研究议程，涵盖物理感知的数据模型、自适应存储检索协同优化以及标准化基准测试，以指导未来研究朝着针对EAI的原理性数据管理解决方案的方向发展。我们的综述基于对超过180项相关研究的全面回顾，为设计下一代自主体感系统所需的高度 robust 和高性能数据管理框架提供了严谨的路线图。', 'title_zh': '具身人工智能的多模态数据存储与检索：一项综述'}
{'arxiv_id': 'arXiv:2508.13877', 'title': 'Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided Decision Transformer', 'authors': 'Rathnam Vidushika Rasanji, Jin Wei-Kocsis, Jiansong Zhang, Dongming Gan, Ragu Athinarayanan, Paul Asunda', 'link': 'https://arxiv.org/abs/2508.13877', 'abstract': 'Reinforcement learning (RL) has demonstrated great potential in robotic operations. However, its data-intensive nature and reliance on the Markov Decision Process (MDP) assumption limit its practical deployment in real-world scenarios involving complex dynamics and long-term temporal dependencies, such as multi-robot manipulation. Decision Transformers (DTs) have emerged as a promising offline alternative by leveraging causal transformers for sequence modeling in RL tasks. However, their applications to multi-robot manipulations still remain underexplored. To address this gap, we propose a novel framework, Symbolically-Guided Decision Transformer (SGDT), which integrates a neuro-symbolic mechanism with a causal transformer to enable deployable multi-robot collaboration. In the proposed SGDT framework, a neuro-symbolic planner generates a high-level task-oriented plan composed of symbolic subgoals. Guided by these subgoals, a goal-conditioned decision transformer (GCDT) performs low-level sequential decision-making for multi-robot manipulation. This hierarchical architecture enables structured, interpretable, and generalizable decision making in complex multi-robot collaboration tasks. We evaluate the performance of SGDT across a range of task scenarios, including zero-shot and few-shot scenarios. To our knowledge, this is the first work to explore DT-based technology for multi-robot manipulation.', 'abstract_zh': '符号引导决策转换器：面向多机器人操作的可部署框架', 'title_zh': '基于符号引导决策转换的可部署多机器人协作研究'}
{'arxiv_id': 'arXiv:2508.13795', 'title': 'Trajectory Tracking and Stabilization of Quadrotors Using Deep Koopman Model Predictive Control', 'authors': 'Haitham El-Hussieny', 'link': 'https://arxiv.org/abs/2508.13795', 'abstract': 'This paper presents a data-driven control framework for quadrotor systems that integrates a deep Koopman operator with model predictive control (DK-MPC). The deep Koopman operator is trained on sampled flight data to construct a high-dimensional latent representation in which the nonlinear quadrotor dynamics are approximated by linear models. This linearization enables the application of MPC to efficiently optimize control actions over a finite prediction horizon, ensuring accurate trajectory tracking and stabilization. The proposed DK-MPC approach is validated through a series of trajectory-following and point-stabilization numerical experiments, where it demonstrates superior tracking accuracy and significantly lower computation time compared to conventional nonlinear MPC. These results highlight the potential of Koopman-based learning methods to handle complex quadrotor dynamics while meeting the real-time requirements of embedded flight control. Future work will focus on extending the framework to more agile flight scenarios and improving robustness against external disturbances.', 'abstract_zh': '基于深度科帕曼算子与模型预测控制的四旋翼数据驱动控制框架', 'title_zh': '基于深度寇普曼模型预测控制的四旋翼轨迹跟踪与稳定'}
{'arxiv_id': 'arXiv:2508.13699', 'title': 'Assessing Pedestrian Behavior Around Autonomous Cleaning Robots in Public Spaces: Findings from a Field Observation', 'authors': 'Maren Raab, Linda Miller, Zhe Zeng, Pascal Jansen, Martin Baumann, Johannes Kraus', 'link': 'https://arxiv.org/abs/2508.13699', 'abstract': "As autonomous robots become more common in public spaces, spontaneous encounters with laypersons are more frequent. For this, robots need to be equipped with communication strategies that enhance momentary transparency and reduce the probability of critical situations. Adapting these robotic strategies requires consideration of robot movements, environmental conditions, and user characteristics and states. While numerous studies have investigated the impact of distraction on pedestrians' movement behavior, limited research has examined this behavior in the presence of autonomous robots. This research addresses the impact of robot type and robot movement pattern on distracted and undistracted pedestrians' movement behavior. In a field setting, unaware pedestrians were videotaped while moving past two working, autonomous cleaning robots. Out of N=498 observed pedestrians, approximately 8% were distracted by smartphones. Distracted and undistracted pedestrians did not exhibit significant differences in their movement behaviors around the robots. Instead, both the larger sweeping robot and the offset rectangular movement pattern significantly increased the number of lateral adaptations compared to the smaller cleaning robot and the circular movement pattern. The offset rectangular movement pattern also led to significantly more close lateral adaptations. Depending on the robot type, the movement patterns led to differences in the distances of lateral adaptations. The study provides initial insights into pedestrian movement behavior around an autonomous cleaning robot in public spaces, contributing to the growing field of HRI research.", 'abstract_zh': '随着自主机器人在公共空间中的普及，与普通人的偶然相遇更加频繁。因此，机器人需要配备能够增强短暂透明度并降低潜在危险情况概率的沟通策略。调整这些机器人策略需要考虑机器人的移动方式、环境条件以及用户特点和状态。尽管已有大量研究探讨了分心对行人移动行为的影响，但在机器人存在的情况下行人分心行为的研究却相对有限。本研究考察了不同类型的机器人和移动模式对受分心影响的行人与未受分心影响的行人移动行为的影响。在实地环境下，记录了行人经过两台正在工作的自主清洁机器人的移动情况。在观察的N=498名行人中，大约8%的人因使用智能手机而分心。无论是受分心影响的行人还是未受分心影响的行人，在机器人周围的移动行为均无显著差异。然而，与较小的清洁机器人和圆形移动模式相比，较大的清扫机器人和偏移矩形移动模式显著增加了侧向调整的频次。偏移矩形移动模式还导致了显著更多的近距离侧向调整。根据机器人类型，移动模式导致了侧向调整距离的不同。该研究提供了对行人围绕自主清洁机器人在公共空间中的移动行为的初步见解，为不断增加的人机交互研究领域做出了贡献。', 'title_zh': '公共空间中自主清洁机器人周边行人行为评估：实地观察研究发现'}
{'arxiv_id': 'arXiv:2508.13534', 'title': 'MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence', 'authors': 'Chao Tang, Anxing Xiao, Yuhong Deng, Tianrun Hu, Wenlong Dong, Hanbo Zhang, David Hsu, Hong Zhang', 'link': 'https://arxiv.org/abs/2508.13534', 'abstract': "Imitating tool manipulation from human videos offers an intuitive approach to teaching robots, while also providing a promising and scalable alternative to labor-intensive teleoperation data collection for visuomotor policy learning. While humans can mimic tool manipulation behavior by observing others perform a task just once and effortlessly transfer the skill to diverse tools for functionally equivalent tasks, current robots struggle to achieve this level of generalization. A key challenge lies in establishing function-level correspondences, considering the significant geometric variations among functionally similar tools, referred to as intra-function variations. To address this challenge, we propose MimicFunc, a framework that establishes functional correspondences with function frame, a function-centric local coordinate frame constructed with keypoint-based abstraction, for imitating tool manipulation skills. Experiments demonstrate that MimicFunc effectively enables the robot to generalize the skill from a single RGB-D human video to manipulating novel tools for functionally equivalent tasks. Furthermore, leveraging MimicFunc's one-shot generalization capability, the generated rollouts can be used to train visuomotor policies without requiring labor-intensive teleoperation data collection for novel objects. Our code and video are available at this https URL.", 'abstract_zh': '从人类视频中模仿工具操作为机器人教学提供了直观的方法，并且为视觉运动策略学习提供了富有潜力且可扩展的替代方案，以减少劳动密集型的遥操作数据收集。尽管人类可以通过观察他人一次完成任务并轻松地将技能转移到功能等效的不同工具上，当前的机器人却难以达到这一泛化水平。一个关键挑战在于建立功能层面的对应关系，考虑到功能相似工具之间存在的显著几何变化，即同功能内变化。为了解决这一挑战，我们提出了MimicFunc框架，该框架利用以关键点为基础的抽象构建的功能中心局部坐标框架（function frame）来建立功能对应关系，以便模仿工具操作技能。实验表明，MimicFunc能够有效地使机器人能够从单个RGB-D人类视频中泛化技能，以操作新型工具进行功能等效任务。此外，借助MimicFunc的一次性泛化能力，生成的轨迹可以用于训练视觉运动策略，而无需为新型对象进行劳动密集型的遥操作数据收集。我们的代码和视频可在以下链接获取。', 'title_zh': 'MimicFunc: 从单个人类视频中模仿工具操作的函数对应方法'}
{'arxiv_id': 'arXiv:2508.13531', 'title': 'A Three-Level Whole-Body Disturbance Rejection Control Framework for Dynamic Motions in Legged Robots', 'authors': 'Bolin Li, Gewei Zuo, Zhixiang Wang, Xiaotian Ke, Lijun Zhu, Han Ding', 'link': 'https://arxiv.org/abs/2508.13531', 'abstract': 'This paper presents a control framework designed to enhance the stability and robustness of legged robots in the presence of uncertainties, including model uncertainties, external disturbances, and faults. The framework enables the full-state feedback estimator to estimate and compensate for uncertainties in whole-body dynamics of the legged robots. First, we propose a novel moving horizon extended state observer (MH-ESO) to estimate uncertainties and mitigate noise in legged systems, which can be integrated into the framework for disturbance compensation. Second, we introduce a three-level whole-body disturbance rejection control framework (T-WB-DRC). Unlike the previous two-level approach, this three-level framework considers both the plan based on whole-body dynamics without uncertainties and the plan based on dynamics with uncertainties, significantly improving payload transportation, external disturbance rejection, and fault tolerance. Third, simulations of both humanoid and quadruped robots in the Gazebo simulator demonstrate the effectiveness and versatility of T-WB-DRC. Finally, extensive experimental trials on a quadruped robot validate the robustness and stability of the system when using T-WB-DRC under various disturbance conditions.', 'abstract_zh': '一种用于增强腿式机器人稳定性和鲁棒性的控制框架：考虑不确定性的全身动态 disturbance 抵抗控制三级架构', 'title_zh': '基于腿式机器人动态运动的三层次全身扰动 rejection 控制框架'}
{'arxiv_id': 'arXiv:2508.13488', 'title': 'ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments', 'authors': 'Jingwen Yu, Jiayi Yang, Anjun Hu, Jiankun Wang, Ping Tan, Hong Zhang', 'link': 'https://arxiv.org/abs/2508.13488', 'abstract': "Loop closure detection is important for simultaneous localization and mapping (SLAM), which associates current observations with historical keyframes, achieving drift correction and global relocalization. However, a falsely detected loop can be fatal, and this is especially difficult in repetitive environments where appearance-based features fail due to the high similarity. Therefore, verification of a loop closure is a critical step in avoiding false positive detections. Existing works in loop closure verification predominantly focus on learning invariant appearance features, neglecting the prior knowledge of the robot's spatial-temporal motion cue, i.e., trajectory. In this letter, we propose ROVER, a loop closure verification method that leverages the historical trajectory as a prior constraint to reject false loops in challenging repetitive environments. For each loop candidate, it is first used to estimate the robot trajectory with pose-graph optimization. This trajectory is then submitted to a scoring scheme that assesses its compliance with the trajectory without the loop, which we refer to as the trajectory prior, to determine if the loop candidate should be accepted. Benchmark comparisons and real-world experiments demonstrate the effectiveness of the proposed method. Furthermore, we integrate ROVER into state-of-the-art SLAM systems to verify its robustness and efficiency. Our source code and self-collected dataset are available at this https URL.", 'abstract_zh': '基于历史轨迹的循环闭合验证方法ROVER', 'title_zh': 'ROVER：在重复环境中具有轨迹先验的鲁棒循环闭合验证'}
{'arxiv_id': 'arXiv:2508.13446', 'title': 'CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models', 'authors': 'Catherine Glossop, William Chen, Arjun Bhorkar, Dhruv Shah, Sergey Levine', 'link': 'https://arxiv.org/abs/2508.13446', 'abstract': "Generalist robots should be able to understand and follow user instructions, but current vision-language-action (VLA) models struggle with following fine-grained commands despite providing a powerful architecture for mapping open-vocabulary natural language instructions to robot actions. One cause for this is a lack of semantic diversity and language grounding in existing robot datasets and, specifically, a lack of fine-grained task diversity for similar observations. To address this, we present a novel method to augment existing robot datasets by leveraging vision language models to create counterfactual labels. Our method improves the language-following capabilities of VLAs by increasing the diversity and granularity of language grounding for robot datasets by generating counterfactual language and actions. We evaluate the resulting model's ability to follow language instructions, ranging from simple object-centric commands to complex referential tasks, by conducting visual language navigation experiments in 3 different indoor and outdoor environments. Our experiments demonstrate that counterfactual relabeling, without any additional data collection, significantly improves instruction-following in VLA policies, making them competitive with state-of-the-art methods and increasing success rate by 27% on navigation tasks.", 'abstract_zh': '通用机器人应该能够理解并遵循用户指令，但现有的视觉-语言-动作（VLA）模型在遵循精细粒度的命令方面仍然存在困难，尽管VLA模型为将开放词汇自然语言指令映射到机器人动作提供了强大的架构。这种困难的一个原因是现有机器人数据集在语义多样性和语言接地方面的不足，特别是相似观察下的精细任务多样性不足。为了解决这一问题，我们提出了一种利用视觉语言模型来扩充现有机器人数据集的新方法，以生成反事实标签。该方法通过生成反事实语言和动作来提高机器人数据集的语义多样性和粒度，从而提升VLA的语言遵循能力。我们通过在3种不同室内外环境下进行视觉语言导航实验，评估所生成模型遵循语言指令的能力，该指令从简单的物体中心命令到复杂的参照任务。实验结果表明，无需额外数据收集，反事实重新标签显著提高了VLA策略的指令遵循能力，使其与先进方法竞争，在导航任务上的成功率提高了27%。', 'title_zh': 'CAST: 反事实标签 improves 视觉-语言-行动模型的指令跟随'}
{'arxiv_id': 'arXiv:2508.13444', 'title': 'Switch4EAI: Leveraging Console Game Platform for Benchmarking Robotic Athletics', 'authors': 'Tianyu Li, Jeonghwan Kim, Wontaek Kim, Donghoon Baek, Seungeun Rho, Sehoon Ha', 'link': 'https://arxiv.org/abs/2508.13444', 'abstract': "Recent advances in whole-body robot control have enabled humanoid and legged robots to execute increasingly agile and coordinated movements. However, standardized benchmarks for evaluating robotic athletic performance in real-world settings and in direct comparison to humans remain scarce. We present Switch4EAI(Switch-for-Embodied-AI), a low-cost and easily deployable pipeline that leverages motion-sensing console games to evaluate whole-body robot control policies. Using Just Dance on the Nintendo Switch as a representative example, our system captures, reconstructs, and retargets in-game choreography for robotic execution. We validate the system on a Unitree G1 humanoid with an open-source whole-body controller, establishing a quantitative baseline for the robot's performance against a human player. In the paper, we discuss these results, which demonstrate the feasibility of using commercial games platform as physically grounded benchmarks and motivate future work to for benchmarking embodied AI.", 'abstract_zh': "Recent Advances inWhole-Body Robot Control Have Enabled Humanoid and Legged Robots to Execute Increasingly Agile and Coordinated Movements. However, Standardized Benchmarks for Evaluating Robotic Athletic Performance in Real-World Settings and in Direct Comparison to Humans Remain Scarce. We Present Switch4EAI (Switch-for-Embodied-AI), a Low-Cost and Easily Deployable Pipeline That Leverages Motion-Sensing Console Games to Evaluate Whole-Body Robot Control Policies. Using Just Dance on the Nintendo Switch as a Representative Example, Our System Captures, Reconstructs, and Retargets In-Game Choreography for Robotic Execution. We Validate the System on a Unitree G1 Humanoid with an Open-Source Whole-Body Controller, Establishing a Quantitative Baseline for the Robot's Performance Against a Human Player. In the Paper, We Discuss These Results, Which Demonstrate the Feasibility of Using Commercial Games Platforms as Physically Grounded Benchmarks and Motivate Future Work for Benchmarking Embodied AI.", 'title_zh': 'Switch4EAI: 基于杠杆的ConsoleE平台平台用于机器人运动benchmarkbenchmark评估'}
{'arxiv_id': 'arXiv:2508.13407', 'title': 'Accelerating Signal-Temporal-Logic-Based Task and Motion Planning of Bipedal Navigation using Benders Decomposition', 'authors': 'Jiming Ren, Xuan Lin, Roman Mineyev, Karen M. Feigh, Samuel Coogan, Ye Zhao', 'link': 'https://arxiv.org/abs/2508.13407', 'abstract': 'Task and motion planning under Signal Temporal Logic constraints is known to be NP-hard. A common class of approaches formulates these hybrid problems, which involve discrete task scheduling and continuous motion planning, as mixed-integer programs (MIP). However, in applications for bipedal locomotion, introduction of non-convex constraints such as kinematic reachability and footstep rotation exacerbates the computational complexity of MIPs. In this work, we present a method based on Benders Decomposition to address scenarios where solving the entire monolithic optimization problem is prohibitively intractable. Benders Decomposition proposes an iterative cutting-plane technique that partitions the problem into a master problem to prototype a plan that meets the task specification, and a series of subproblems for kinematics and dynamics feasibility checks. Our experiments demonstrate that this method achieves faster planning compared to alternative algorithms for solving the resulting optimization program with nonlinear constraints.', 'abstract_zh': '基于信号时逻辑约束的任务与运动规划在已知是NP难问题的情况下，一种常见的方法将其形式化为混合整数规划问题。然而，在双足运动应用中，引入非凸约束如运动学可达性和足部旋转会进一步增加混合整数规划问题的计算复杂性。本文提出了一种基于Benders分解的方法，以应对求解整体优化问题不可行的场景。Benders分解提出了一种迭代切割平面技术，将问题分解为主问题和一系列子问题，主问题原型化满足任务规范的计划，子问题则进行运动学和动力学可行性检查。我们的实验表明，该方法在解决具有非线性约束的优化程序时比替代算法实现了更快的规划。', 'title_zh': '基于贝纳德斯分解的双足导航信号-时序逻辑任务与运动规划加速方法'}
{'arxiv_id': 'arXiv:2508.13303', 'title': 'Diff-MSM: Differentiable MusculoSkeletal Model for Simultaneous Identification of Human Muscle and Bone Parameters', 'authors': 'Yingfan Zhou, Philip Sanderink, Sigurd Jager Lemming, Cheng Fang', 'link': 'https://arxiv.org/abs/2508.13303', 'abstract': 'High-fidelity personalized human musculoskeletal models are crucial for simulating realistic behavior of physically coupled human-robot interactive systems and verifying their safety-critical applications in simulations before actual deployment, such as human-robot co-transportation and rehabilitation through robotic exoskeletons. Identifying subject-specific Hill-type muscle model parameters and bone dynamic parameters is essential for a personalized musculoskeletal model, but very challenging due to the difficulty of measuring the internal biomechanical variables in vivo directly, especially the joint torques. In this paper, we propose using Differentiable MusculoSkeletal Model (Diff-MSM) to simultaneously identify its muscle and bone parameters with an end-to-end automatic differentiation technique differentiating from the measurable muscle activation, through the joint torque, to the resulting observable motion without the need to measure the internal joint torques. Through extensive comparative simulations, the results manifested that our proposed method significantly outperformed the state-of-the-art baseline methods, especially in terms of accurate estimation of the muscle parameters (i.e., initial guess sampled from a normal distribution with the mean being the ground truth and the standard deviation being 10% of the ground truth could end up with an average of the percentage errors of the estimated values as low as 0.05%). In addition to human musculoskeletal modeling and simulation, the new parameter identification technique with the Diff-MSM has great potential to enable new applications in muscle health monitoring, rehabilitation, and sports science.', 'abstract_zh': '高保真个性化人体 musculoskeletal 模型对于模拟人-机器人交互系统的真实行为以及在实际部署前通过仿真验证其关键安全应用（如人-机器人共同运输和基于外骨骼的康复）至关重要。通过关节扭矩自最终可测量的肌肉激活反向自动微分确定个性化 musculoskeletal 模型的肌肉和骨骼参数是非常必要的，但由于难以直接测量体内的生物力学变量（尤其是关节扭矩），这一任务极具挑战性。本文提出使用可微 musculoskeletal 模型（Diff-MSM）结合端到端自动微分技术，通过关节扭矩到可观测运动的反向传播，无需测量内部关节扭矩即可同时识别其肌肉和骨骼参数。通过广泛比较仿真，结果表明，我们提出的方法在肌肉参数准确估计方面显著优于现有最先进的基准方法，尤其是在肌肉参数估计的准确性方面（初始猜测来自正态分布，均值为真实值，标准差为真实值的10%，最终估计值的平均百分比误差低至0.05%）。除了人体 musculoskeletal 模型和仿真之外，Diff-MSM 的新参数识别技术还具有在肌肉健康监测、康复和体育科学等领域启用新应用的巨大潜力。', 'title_zh': 'Diff-MSM: 可微肌骨模型同时识别人类肌肉和骨骼参数'}
{'arxiv_id': 'arXiv:2508.14040', 'title': 'ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents', 'authors': 'Hanyu Lai, Xiao Liu, Yanxiao Zhao, Han Xu, Hanchen Zhang, Bohao Jing, Yanyu Ren, Shuntian Yao, Yuxiao Dong, Jie Tang', 'link': 'https://arxiv.org/abs/2508.14040', 'abstract': 'We introduce ComputerRL, a framework for autonomous desktop intelligence that enables agents to operate complex digital workspaces skillfully. ComputerRL features the API-GUI paradigm, which unifies programmatic API calls and direct GUI interaction to address the inherent mismatch between machine agents and human-centric desktop environments. Scaling end-to-end RL training is crucial for improvement and generalization across diverse desktop tasks, yet remains challenging due to environmental inefficiency and instability in extended training. To support scalable and robust training, we develop a distributed RL infrastructure capable of orchestrating thousands of parallel virtual desktop environments to accelerate large-scale online RL. Furthermore, we propose Entropulse, a training strategy that alternates reinforcement learning with supervised fine-tuning, effectively mitigating entropy collapse during extended training runs. We employ ComputerRL on open models GLM-4-9B-0414 and Qwen2.5-14B, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B based on GLM-4-9B-0414 achieves a new state-of-the-art accuracy of 48.1%, demonstrating significant improvements for general agents in desktop automation. The algorithm and framework are adopted in building AutoGLM (Liu et al., 2024a)', 'abstract_zh': 'ComputerRL：面向自主桌面智能的框架', 'title_zh': 'ComputerRL: 扩展面向计算机使用代理的端到端在线强化学习'}
{'arxiv_id': 'arXiv:2508.13530', 'title': 'CrafterDojo: A Suite of Foundation Models for Building Open-Ended Embodied Agents in Crafter', 'authors': 'Junyeong Park, Hyeonseo Cho, Sungjin Ahn', 'link': 'https://arxiv.org/abs/2508.13530', 'abstract': 'Developing general-purpose embodied agents is a core challenge in AI. Minecraft provides rich complexity and internet-scale data, but its slow speed and engineering overhead make it unsuitable for rapid prototyping. Crafter offers a lightweight alternative that retains key challenges from Minecraft, yet its use has remained limited to narrow tasks due to the absence of foundation models that have driven progress in the Minecraft setting. In this paper, we present CrafterDojo, a suite of foundation models and tools that unlock the Crafter environment as a lightweight, prototyping-friendly, and Minecraft-like testbed for general-purpose embodied agent research. CrafterDojo addresses this by introducing CrafterVPT, CrafterCLIP, and CrafterSteve-1 for behavior priors, vision-language grounding, and instruction following, respectively. In addition, we provide toolkits for generating behavior and caption datasets (CrafterPlay and CrafterCaption), reference agent implementations, benchmark evaluations, and a complete open-source codebase.', 'abstract_zh': '开发通用体态智能体是人工智能的核心挑战。Minecraft提供了丰富的复杂性和互联网规模的数据，但由于其缓慢的速度和工程开销，它不适合快速原型设计。Crafter提供了一种轻量级的替代方案，保留了Minecraft的关键挑战，但由于缺乏推动Minecraft环境进步的基础模型，其使用一直局限于狭小的任务。本文介绍了CrafterDojo，一个基础模型和工具套件，解锁了Crafter环境作为轻量级、易于原型设计且类似Minecraft的测试床，用于通用体态智能体研究。CrafterDojo通过引入CrafterVPT、CrafterCLIP和CrafterSteve-1分别用于行为先验、视觉-语言对接和指令跟随。此外，我们还提供了用于生成行为和描述数据集（CrafterPlay和CrafterCaption）、参考智能体实现、基准评估和完整的开源代码库的工具包。', 'title_zh': 'CrafterDojo: 一套构建开放-ended 体态智能体的基础模型'}
{'arxiv_id': 'arXiv:2508.13421', 'title': 'Virtuous Machines: Towards Artificial General Science', 'authors': 'Gabrielle Wehr, Reuben Rideaux, Amaya J. Fox, David R. Lightfoot, Jason Tangen, Jason B. Mattingley, Shane E. Ehrhardt', 'link': 'https://arxiv.org/abs/2508.13421', 'abstract': "Artificial intelligence systems are transforming scientific discovery by accelerating specific research tasks, from protein structure prediction to materials design, yet remain confined to narrow domains requiring substantial human oversight. The exponential growth of scientific literature and increasing domain specialisation constrain researchers' capacity to synthesise knowledge across disciplines and develop unifying theories, motivating exploration of more general-purpose AI systems for science. Here we show that a domain-agnostic, agentic AI system can independently navigate the scientific workflow - from hypothesis generation through data collection to manuscript preparation. The system autonomously designed and executed three psychological studies on visual working memory, mental rotation, and imagery vividness, executed one new online data collection with 288 participants, developed analysis pipelines through 8-hour+ continuous coding sessions, and produced completed manuscripts. The results demonstrate the capability of AI scientific discovery pipelines to conduct non-trivial research with theoretical reasoning and methodological rigour comparable to experienced researchers, though with limitations in conceptual nuance and theoretical interpretation. This is a step toward embodied AI that can test hypotheses through real-world experiments, accelerating discovery by autonomously exploring regions of scientific space that human cognitive and resource constraints might otherwise leave unexplored. It raises important questions about the nature of scientific understanding and the attribution of scientific credit.", 'abstract_zh': '人工智能系统正在通过加速特定研究任务（从蛋白质结构预测到材料设计）来变革科学发现，但仍局限于需要大量人类监督的狭窄领域。不断增长的科学文献和日益专业的学科限制了研究人员跨学科综合知识和建立统一理论的能力，促使人们探索更能适应各种研究任务的通用人工智能系统。在这里，我们展示了具备学科普适性和自主性的人工智能系统可以独立导航科学研究流程——从假设生成到数据收集再到论文准备。该系统自主设计并执行了三个关于视觉工作记忆、心理旋转和想象生动性的心理研究，实施了一项新的在线数据收集（共288名参与者），开发了分析管道并通过连续超过8小时的编程会话，并产生了完整的论文。研究结果表明，人工智能科学发现管道能够在理论推理和方法论严谨性方面与经验丰富的研究人员相媲美，尽管在概念细微差别和理论解释方面存在局限。这向着能够通过实际实验验证假设的具身人工智能迈出了一步，有助于自主探索人类认知和资源限制可能未被开发的科学领域。这引发了关于科学理解的本质以及科学信用归属的重要问题。', 'title_zh': '君子机器：通往通用人工科学之路'}
{'arxiv_id': 'arXiv:2508.13371', 'title': 'LOOP: A Plug-and-Play Neuro-Symbolic Framework for Enhancing Planning in Autonomous Systems', 'authors': 'Ronit Virwani, Ruchika Suryawanshi', 'link': 'https://arxiv.org/abs/2508.13371', 'abstract': "Planning is one of the most critical tasks in autonomous systems, where even a small error can lead to major failures or million-dollar losses. Current state-of-the-art neural planning approaches struggle with complex domains, producing plans with missing preconditions, inconsistent goals, and hallucinations. While classical planners provide logical guarantees, they lack the flexibility and natural language understanding capabilities needed for modern autonomous systems. Existing neuro-symbolic approaches use one-shot translation from natural language to formal plans, missing the opportunity for neural and symbolic components to work and refine solutions together. To address this gap, we develop LOOP -- a novel neuro-symbolic planning framework that treats planning as an iterative conversation between neural and symbolic components rather than simple translation. LOOP integrates 13 coordinated neural features including graph neural networks for spatial relationships, multi-agent validation for consensus-based correctness, hierarchical decomposition for complex task management, and causal memory that learns from both successes and failures. Unlike existing approaches, LOOP generates PDDL specifications, refines them iteratively based on symbolic feedback, and builds a causal knowledge base from execution traces. LOOP was evaluated on six standard IPC benchmark domains, where it achieved 85.8% success rate compared to LLM+P (55.0%), LLM-as-Planner (19.2%), and Tree-of-Thoughts (3.3%). This work shows that the key to reliable planning is not in choosing between neural networks or symbolic reasoners but it lies in making them actually ``talk'' to each other during the entire process. LOOP provides a thorough blueprint for building autonomous systems that can finally be trusted with critical real-world applications.", 'abstract_zh': '一种迭代对话式神经符号规划框架LOOP：在关键现实应用中实现可信赖自主系统的蓝图', 'title_zh': 'LOOP：一种增强自主系统规划的即插即用神经符号框架'}
{'arxiv_id': 'arXiv:2508.13177', 'title': 'A Hardware-oriented Approach for Efficient Active Inference Computation and Deployment', 'authors': 'Nikola Pižurica, Nikola Milović, Igor Jovančević, Conor Heins, Miguel de Prado', 'link': 'https://arxiv.org/abs/2508.13177', 'abstract': "Active Inference (AIF) offers a robust framework for decision-making, yet its computational and memory demands pose challenges for deployment, especially in resource-constrained environments. This work presents a methodology that facilitates AIF's deployment by integrating pymdp's flexibility and efficiency with a unified, sparse, computational graph tailored for hardware-efficient execution. Our approach reduces latency by over 2x and memory by up to 35%, advancing the deployment of efficient AIF agents for real-time and embedded applications.", 'abstract_zh': 'Active Inference 的部署方法：通过结合 pymdp 的灵活性和效率及硬件友好的稀疏计算图以减少延迟和内存使用', 'title_zh': '面向硬件的高效主动推断计算与部署方法'}
{'arxiv_id': 'arXiv:2508.13167', 'title': 'Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL', 'authors': 'Weizhen Li, Jianbo Lin, Zhuosong Jiang, Jingyi Cao, Xinpeng Liu, Jiayu Zhang, Zhenqiang Huang, Qianben Chen, Weichen Sun, Qiexiang Wang, Hongxuan Lu, Tianrui Qin, Chenghao Zhu, Yi Yao, Shuying Fan, Xiaowan Li, Tiannan Wang, Pai Liu, King Zhu, He Zhu, Dingfeng Shi, Piaohong Wang, Yeyi Guan, Xiangru Tang, Minghao Liu, Yuchen Eleanor Jiang, Jian Yang, Jiaheng Liu, Ge Zhang, Wangchunshu Zhou', 'link': 'https://arxiv.org/abs/2508.13167', 'abstract': "Recent advances in large language models (LLMs) and multi-agent systems have demonstrated remarkable capabilities in complex problem-solving tasks such as deep research, vibe coding, and mathematical reasoning. However, most existing multi-agent systems are built upon manual prompt/workflow engineering with sophisticated agent frameworks, making them computationally inefficient, less capable, and can not benefit from data-centric learning. In this work, we introduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables native end-to-end complex problem-solving in the same way as a multi-agent system (i.e., multi-turn problem solving with multiple tools and multiple agents) within one model. In chain-of-agents problem-solving, the model dynamically activates different tool agents and role-playing agents to simulate multi-agent collaboration in an end-to-end fashion. To elicit end-to-end chain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent distillation framework to distill state-of-the-art multi-agent systems into chain-of-agents trajectories for agentic supervised fine-tuning. We then use agentic reinforcement learning on verifiable agentic tasks to further improve the models' capabilities on chain-of-agents problem solving. We call the resulting models Agent Foundation Models (AFMs). Our empirical studies demonstrate that AFM establishes new state-of-the-art performance across diverse benchmarks in both web agent and code agent settings. We make the entire research, including the model weights, code for training and evaluation, and the training data, fully open-sourced, which offers a solid starting point for future research on agent models and agentic RL.", 'abstract_zh': '近期大规模语言模型（LLMs）和多智能体系统的进展在深度研究、代码编写和数学推理等复杂问题解决任务中展现了显著的能力。然而，大多数现有的多智能体系统依赖于手工构建的提示/工作流工程及复杂的智能体框架，这使得它们在计算效率、功能以及从数据驱动学习中受益等方面存在不足。在本工作中，我们引入了多智能体链（Chain-of-Agents, CoA）这一新颖的LLM推理范式，以类似于多智能体系统的内方式（即多轮次、多工具和多智能体的问题解决）实现复杂的端到端问题解决。在多智能体链问题解决过程中，模型动态激活不同的工具智能体和角色扮演智能体，以端到端的方式模拟多智能体合作。为了在LLMs中引发端到端的多智能体链问题解决能力，我们提出了一种多智能体蒸馏框架，将前沿的多智能体系统蒸馏为多智能体链轨迹，用于具有智能体监督的微调。然后，我们使用智能体强化学习来进一步提高模型在多智能体链问题解决方面的能力，我们称之为代理基础模型（Agent Foundation Models, AFMs）。我们的实证研究表明，AFM在网页代理和代码代理设置下的多种基准测试中均达到了新的性能最佳。我们全面开源了整个研究，包括模型权重、训练和评估代码以及训练数据，为未来代理模型研究和智能体强化学习提供了坚实的基础。', 'title_zh': '多\nuser\n标题翻译如下：\n\nChain-of-Agents：通过多\niffany\n标题翻译 如下：\n\nChain-of-Agents： 通过多 Agent 多智能体蒸馏和能动强化学习端到端智能体基础模型'}
{'arxiv_id': 'arXiv:2508.13982', 'title': 'The Social Context of Human-Robot Interactions', 'authors': 'Sydney Thompson, Kate Candon, Marynel Vázquez', 'link': 'https://arxiv.org/abs/2508.13982', 'abstract': 'The Human-Robot Interaction (HRI) community often highlights the social context of an interaction as a key consideration when designing, implementing, and evaluating robot behavior. Unfortunately, researchers use the term "social context" in varied ways. This can lead to miscommunication, making it challenging to draw connections between related work on understanding and modeling the social contexts of human-robot interactions. To address this gap, we survey the HRI literature for existing definitions and uses of the term "social context". Then, we propose a conceptual model for describing the social context of a human-robot interaction. We apply this model to existing work, and we discuss a range of attributes of social contexts that can help researchers plan for interactions, develop behavior models for robots, and gain insights after interactions have taken place. We conclude with a discussion of open research questions in relation to understanding and modeling the social contexts of human-robot interactions.', 'abstract_zh': '人机交互（HRI）领域常强调互动的社会背景是设计、实现和评估机器人行为时的关键考虑因素。不幸的是，研究人员在使用“社会背景”这一术语时存在差异性。这可能导致交流不当，使得在理解与建模人机互动的社会背景方面难以建立相关工作的联系。为解决这一问题，我们调研了HRI领域的现有文献，探索“社会背景”这一术语的现有定义和使用方式。然后，我们提出了一种概念模型来描述人机互动的社会背景。我们将该模型应用于现有研究，并讨论一组有助于研究人员规划互动、为机器人开发行为模型以及互动完成后获得见解的社会背景属性。最后，我们讨论了关于理解与建模人机互动的社会背景方面的开放性研究问题。', 'title_zh': '人类与机器人互动的社会背景'}
{'arxiv_id': 'arXiv:2508.13319', 'title': 'A Surveillance Based Interactive Robot', 'authors': 'Kshitij Kavimandan, Pooja Mangal, Devanshi Mehta', 'link': 'https://arxiv.org/abs/2508.13319', 'abstract': 'We build a mobile surveillance robot that streams video in real time and responds to speech so a user can monitor and steer it from a phone or browser. The system uses two Raspberry Pi 4 units: a front unit on a differential drive base with camera, mic, and speaker, and a central unit that serves the live feed and runs perception. Video is sent with FFmpeg. Objects in the scene are detected using YOLOv3 to support navigation and event awareness. For voice interaction, we use Python libraries for speech recognition, multilingual translation, and text-to-speech, so the robot can take spoken commands and read back responses in the requested language. A Kinect RGB-D sensor provides visual input and obstacle cues. In indoor tests the robot detects common objects at interactive frame rates on CPU, recognises commands reliably, and translates them to actions without manual control. The design relies on off-the-shelf hardware and open software, making it easy to reproduce. We discuss limits and practical extensions, including sensor fusion with ultrasonic range data, GPU acceleration, and adding face and text recognition.', 'abstract_zh': '一种用于实时视频流和语音交互的移动监控机器人系统', 'title_zh': '基于监控的交互式机器人'}
