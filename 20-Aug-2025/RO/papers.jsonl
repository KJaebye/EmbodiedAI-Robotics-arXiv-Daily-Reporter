{'arxiv_id': 'arXiv:2508.14042', 'title': 'Train Once, Deploy Anywhere: Realize Data-Efficient Dynamic Object Manipulation', 'authors': 'Zhuoling Li, Xiaoyang Wu, Zhenhua Xu, Hengshuang Zhao', 'link': 'https://arxiv.org/abs/2508.14042', 'abstract': 'Realizing generalizable dynamic object manipulation is important for enhancing manufacturing efficiency, as it eliminates specialized engineering for various scenarios. To this end, imitation learning emerges as a promising paradigm, leveraging expert demonstrations to teach a policy manipulation skills. Although the generalization of an imitation learning policy can be improved by increasing demonstrations, demonstration collection is labor-intensive. To address this problem, this paper investigates whether strong generalization in dynamic object manipulation is achievable with only a few demonstrations. Specifically, we develop an entropy-based theoretical framework to quantify the optimization of imitation learning. Based on this framework, we propose a system named Generalizable Entropy-based Manipulation (GEM). Extensive experiments in simulated and real tasks demonstrate that GEM can generalize across diverse environment backgrounds, robot embodiments, motion dynamics, and object geometries. Notably, GEM has been deployed in a real canteen for tableware collection. Without any in-scene demonstration, it achieves a success rate of over 97% across more than 10,000 operations.', 'abstract_zh': '基于熵的通用动态对象操作系统（GEM）', 'title_zh': '一次训练，随处部署：实现高效的动态物体操控'}
{'arxiv_id': 'arXiv:2508.13998', 'title': 'Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation', 'authors': 'Yifu Yuan, Haiqin Cui, Yaoting Huang, Yibin Chen, Fei Ni, Zibin Dong, Pengyi Li, Yan Zheng, Jianye Hao', 'link': 'https://arxiv.org/abs/2508.13998', 'abstract': 'Generalization in embodied AI is hindered by the "seeing-to-doing gap," which stems from data scarcity and embodiment heterogeneity. To address this, we pioneer "pointing" as a unified, embodiment-agnostic intermediate representation, defining four core embodied pointing abilities that bridge high-level vision-language comprehension with low-level action primitives. We introduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed for embodied reasoning and pointing. We use a wide range of embodied and general visual reasoning datasets as sources to construct a large-scale dataset, Embodied-Points-200K, which supports key embodied pointing capabilities. We then train Embodied-R1 using a two-stage Reinforced Fine-tuning (RFT) curriculum with a specialized multi-task reward design. Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and pointing benchmarks. Critically, it demonstrates robust zero-shot generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5% across 8 real-world XArm tasks without any task-specific fine-tuning, representing a 62% improvement over strong baselines. Furthermore, the model exhibits high robustness against diverse visual disturbances. Our work shows that a pointing-centric representation, combined with an RFT training paradigm, offers an effective and generalizable pathway to closing the perception-action gap in robotics.', 'abstract_zh': 'Generalized Embodied AI Is Impeded by the "Seeing-to-Doing Gap": Pointing as a Unified, Embodiment-Agnostic Intermediate Representation', 'title_zh': 'Embodied-R1: 强化 bodied 原则的一般机器人操作推理'}
{'arxiv_id': 'arXiv:2508.13982', 'title': 'The Social Context of Human-Robot Interactions', 'authors': 'Sydney Thompson, Kate Candon, Marynel Vázquez', 'link': 'https://arxiv.org/abs/2508.13982', 'abstract': 'The Human-Robot Interaction (HRI) community often highlights the social context of an interaction as a key consideration when designing, implementing, and evaluating robot behavior. Unfortunately, researchers use the term "social context" in varied ways. This can lead to miscommunication, making it challenging to draw connections between related work on understanding and modeling the social contexts of human-robot interactions. To address this gap, we survey the HRI literature for existing definitions and uses of the term "social context". Then, we propose a conceptual model for describing the social context of a human-robot interaction. We apply this model to existing work, and we discuss a range of attributes of social contexts that can help researchers plan for interactions, develop behavior models for robots, and gain insights after interactions have taken place. We conclude with a discussion of open research questions in relation to understanding and modeling the social contexts of human-robot interactions.', 'abstract_zh': '人机交互（HRI）社区常强调在设计、实现和评估机器人行为时，社交背景是关键考虑因素。然而，研究人员对“社交背景”一词的使用方式各异，这可能导致沟通不畅，使得难以在理解与建模人机交互的社交背景方面建立相关研究之间的联系。为解决这一问题，我们回顾了HRI文献中对“社交背景”这一术语的现有定义和使用方式，并提出了一种描述人机交互社交背景的概念模型。我们将该模型应用于现有研究，并讨论了一系列有助于研究人员规划交互、为机器人开发行为模型以及在交互发生后获得见解的社交背景属性。最后，我们讨论了在理解与建模人机交互的社交背景方面的开放性研究问题。', 'title_zh': '人类与机器人互动的社会背景'}
{'arxiv_id': 'arXiv:2508.13976', 'title': 'Toward an Interaction-Centered Approach to Robot Trustworthiness', 'authors': 'Carlo Mazzola, Hassan Ali, Kristína Malinovská, Igor Farkaš', 'link': 'https://arxiv.org/abs/2508.13976', 'abstract': 'As robots get more integrated into human environments, fostering trustworthiness in embodied robotic agents becomes paramount for an effective and safe human-robot interaction (HRI). To achieve that, HRI applications must promote human trust that aligns with robot skills and avoid misplaced trust or overtrust, which can pose safety risks and ethical concerns. To achieve that, HRI applications must promote human trust that aligns with robot skills and avoid misplaced trust or overtrust, which can pose safety risks and ethical concerns. In this position paper, we outline an interaction-based framework for building trust through mutual understanding between humans and robots. We emphasize two main pillars: human awareness and transparency, referring to the robot ability to interpret human actions accurately and to clearly communicate its intentions and goals, respectively. By integrating these two pillars, robots can behave in a manner that aligns with human expectations and needs while providing their human partners with both comprehension and control over their actions. We also introduce four components that we think are important for bridging the gap between a human-perceived sense of trust and a robot true capabilities.', 'abstract_zh': '随着机器人在人类环境中的整合程度提高，培养实体机器人代理的信任度对于实现有效安全的人机交互（HRI）至关重要。为此，HRI应用必须促进与机器人技能相一致的人类信任，避免错误信任或过度信任，这些可能会带来安全风险和伦理问题。在此观点论文中，我们提出了一个基于交互的框架，旨在通过人机之间的相互理解来建立信任。我们强调了两个主要支柱：人类意识和透明度，前者指机器人能够准确解读人类行为，后者指机器人清晰地沟通其意图和目标。通过整合这两个支柱，机器人可以表现出符合人类期望和需求的行为，同时为人类伙伴提供对其行动的理解和控制。我们还介绍了我们认为对于弥合人类感知的信任感与机器人实际能力之间差距的四个关键组成部分。', 'title_zh': '面向交互中心的机器人可信赖性方法'}
{'arxiv_id': 'arXiv:2508.13964', 'title': 'Augmenting cobots for sheet-metal SMEs with 3D object recognition and localisation', 'authors': 'Martijn Cramer, Yanming Wu, David De Schepper, Eric Demeester', 'link': 'https://arxiv.org/abs/2508.13964', 'abstract': 'Due to high-mix-low-volume production, sheet-metal workshops today are challenged by small series and varying orders. As standard automation solutions tend to fall short, SMEs resort to repetitive manual labour impacting production costs and leading to tech-skilled workforces not being used to their full potential. The COOCK+ ROBUST project aims to transform cobots into mobile and reconfigurable production assistants by integrating existing technologies, including 3D object recognition and localisation. This article explores both the opportunities and challenges of enhancing cobotic systems with these technologies in an industrial setting, outlining the key steps involved in the process. Additionally, insights from a past project, carried out by the ACRO research unit in collaboration with an industrial partner, serves as a concrete implementation example throughout.', 'abstract_zh': '由于高混低量生产，今天的片材车间面临着小批量和多变订单的挑战。由于标准自动化解决方案往往不尽如人意，中小企业转而依赖重复性手工劳动，这影响了生产成本，并导致技术熟练的劳动力未能充分发挥潜力。COOCK+ ROBUST项目旨在通过集成现有的技术，如3D物体识别和定位，将协作机器人转化为移动和可重构的生产助手。本文探讨了在工业环境中增强协作机器人系统的机会与挑战，并概述了该过程中的关键步骤。此外，ACRO研究单元与一家工业合作伙伴合作开展的过去项目提供了具体实施例。', 'title_zh': '基于3D物体识别与定位增强协作机器人在薄板中小型企业中的应用'}
{'arxiv_id': 'arXiv:2508.13901', 'title': 'Multimodal Data Storage and Retrieval for Embodied AI: A Survey', 'authors': 'Yihao Lu, Hao Tang', 'link': 'https://arxiv.org/abs/2508.13901', 'abstract': "Embodied AI (EAI) agents continuously interact with the physical world, generating vast, heterogeneous multimodal data streams that traditional management systems are ill-equipped to handle. In this survey, we first systematically evaluate five storage architectures (Graph Databases, Multi-Model Databases, Data Lakes, Vector Databases, and Time-Series Databases), focusing on their suitability for addressing EAI's core requirements, including physical grounding, low-latency access, and dynamic scalability. We then analyze five retrieval paradigms (Fusion Strategy-Based Retrieval, Representation Alignment-Based Retrieval, Graph-Structure-Based Retrieval, Generation Model-Based Retrieval, and Efficient Retrieval-Based Optimization), revealing a fundamental tension between achieving long-term semantic coherence and maintaining real-time responsiveness. Based on this comprehensive analysis, we identify key bottlenecks, spanning from the foundational Physical Grounding Gap to systemic challenges in cross-modal integration, dynamic adaptation, and open-world generalization. Finally, we outline a forward-looking research agenda encompassing physics-aware data models, adaptive storage-retrieval co-optimization, and standardized benchmarking, to guide future research toward principled data management solutions for EAI. Our survey is based on a comprehensive review of more than 180 related studies, providing a rigorous roadmap for designing the robust, high-performance data management frameworks essential for the next generation of autonomous embodied systems.", 'abstract_zh': '机器人体感智能（EAI）代理连续与物理世界互动，生成大量异构多模态数据流，传统管理系统难以处理。在本文综述中，我们首先系统性评估了五种存储架构（图数据库、多模型数据库、数据湖、向量数据库和时间序列数据库），重点关注它们满足EAI核心要求（包括物理接地、低延迟访问和动态扩展）的适用性。接着，我们分析了五种检索范式（融合策略基检索、表示对齐基检索、图结构基检索、生成模型基检索和高效检索基优化），揭示了长期语义一致性与实时响应性之间的基本矛盾。基于这一综合分析，我们识别了关键瓶颈，从基础的物理接地差距到跨模态集成、动态适应和开放世界泛化的系统性挑战。最后，我们概述了一个前瞻性的研究议程，涵盖物理感知的数据模型、自适应存储检索协同优化以及标准化基准测试，以指导未来研究朝着针对EAI的原理性数据管理解决方案的方向发展。我们的综述基于对超过180项相关研究的全面回顾，为设计下一代自主体感系统所需的高度 robust 和高性能数据管理框架提供了严谨的路线图。', 'title_zh': '具身人工智能的多模态数据存储与检索：一项综述'}
{'arxiv_id': 'arXiv:2508.13881', 'title': 'Driving Style Recognition Like an Expert Using Semantic Privileged Information from Large Language Models', 'authors': 'Zhaokun Chen, Chaopeng Zhang, Xiaohan Li, Wenshuo Wang, Gentiane Venture, Junqiang Xi', 'link': 'https://arxiv.org/abs/2508.13881', 'abstract': 'Existing driving style recognition systems largely depend on low-level sensor-derived features for training, neglecting the rich semantic reasoning capability inherent to human experts. This discrepancy results in a fundamental misalignment between algorithmic classifications and expert judgments. To bridge this gap, we propose a novel framework that integrates Semantic Privileged Information (SPI) derived from large language models (LLMs) to align recognition outcomes with human-interpretable reasoning. First, we introduce DriBehavGPT, an interactive LLM-based module that generates natural-language descriptions of driving behaviors. These descriptions are then encoded into machine learning-compatible representations via text embedding and dimensionality reduction. Finally, we incorporate them as privileged information into Support Vector Machine Plus (SVM+) for training, enabling the model to approximate human-like interpretation patterns. Experiments across diverse real-world driving scenarios demonstrate that our SPI-enhanced framework outperforms conventional methods, achieving F1-score improvements of 7.6% (car-following) and 7.9% (lane-changing). Importantly, SPI is exclusively used during training, while inference relies solely on sensor data, ensuring computational efficiency without sacrificing performance. These results highlight the pivotal role of semantic behavioral representations in improving recognition accuracy while advancing interpretable, human-centric driving systems.', 'abstract_zh': '基于语义特权信息的驾驶风格识别框架', 'title_zh': '使用大型语言模型的语义特权信息像专家一样识别驾驶风格'}
{'arxiv_id': 'arXiv:2508.13877', 'title': 'Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided Decision Transformer', 'authors': 'Rathnam Vidushika Rasanji, Jin Wei-Kocsis, Jiansong Zhang, Dongming Gan, Ragu Athinarayanan, Paul Asunda', 'link': 'https://arxiv.org/abs/2508.13877', 'abstract': 'Reinforcement learning (RL) has demonstrated great potential in robotic operations. However, its data-intensive nature and reliance on the Markov Decision Process (MDP) assumption limit its practical deployment in real-world scenarios involving complex dynamics and long-term temporal dependencies, such as multi-robot manipulation. Decision Transformers (DTs) have emerged as a promising offline alternative by leveraging causal transformers for sequence modeling in RL tasks. However, their applications to multi-robot manipulations still remain underexplored. To address this gap, we propose a novel framework, Symbolically-Guided Decision Transformer (SGDT), which integrates a neuro-symbolic mechanism with a causal transformer to enable deployable multi-robot collaboration. In the proposed SGDT framework, a neuro-symbolic planner generates a high-level task-oriented plan composed of symbolic subgoals. Guided by these subgoals, a goal-conditioned decision transformer (GCDT) performs low-level sequential decision-making for multi-robot manipulation. This hierarchical architecture enables structured, interpretable, and generalizable decision making in complex multi-robot collaboration tasks. We evaluate the performance of SGDT across a range of task scenarios, including zero-shot and few-shot scenarios. To our knowledge, this is the first work to explore DT-based technology for multi-robot manipulation.', 'abstract_zh': '基于符号引导的决策变换器：面向多机器人操作的可部署框架', 'title_zh': '面向部署的符号引导决策变换器驱动的多机器人协作'}
{'arxiv_id': 'arXiv:2508.13795', 'title': 'Trajectory Tracking and Stabilization of Quadrotors Using Deep Koopman Model Predictive Control', 'authors': 'Haitham El-Hussieny', 'link': 'https://arxiv.org/abs/2508.13795', 'abstract': 'This paper presents a data-driven control framework for quadrotor systems that integrates a deep Koopman operator with model predictive control (DK-MPC). The deep Koopman operator is trained on sampled flight data to construct a high-dimensional latent representation in which the nonlinear quadrotor dynamics are approximated by linear models. This linearization enables the application of MPC to efficiently optimize control actions over a finite prediction horizon, ensuring accurate trajectory tracking and stabilization. The proposed DK-MPC approach is validated through a series of trajectory-following and point-stabilization numerical experiments, where it demonstrates superior tracking accuracy and significantly lower computation time compared to conventional nonlinear MPC. These results highlight the potential of Koopman-based learning methods to handle complex quadrotor dynamics while meeting the real-time requirements of embedded flight control. Future work will focus on extending the framework to more agile flight scenarios and improving robustness against external disturbances.', 'abstract_zh': '基于深度科帕曼算子与模型预测控制的四旋翼数据驱动控制框架', 'title_zh': '基于深度寇普曼模型预测控制的四旋翼轨迹跟踪与稳定'}
{'arxiv_id': 'arXiv:2508.13785', 'title': 'Blast Hole Seeking and Dipping -- The Navigation and Perception Framework in a Mine Site Inspection Robot', 'authors': 'Liyang Liu, Ehsan Mihankhah, Nathan Wallace, Javier Martinez, Andrew J. Hill', 'link': 'https://arxiv.org/abs/2508.13785', 'abstract': 'In open-pit mining, holes are drilled into the surface of the excavation site and detonated with explosives to facilitate digging. These blast holes need to be inspected internally for investigation of downhole material types and properties. Knowing these properties can lead to significant savings in material handling costs in downstream processes. Manual hole inspection is slow and expensive, with major limitations in revealing the geometric and geological properties of the holes and their contents. This has been the motivation for the development of our autonomous mine-site inspection robot - "DIPPeR". In this paper, the automation aspect of the project is explained. We present a robust blast hole seeking and detection framework that enables target-based navigation and accurate down-hole sensor positioning. The pipeline first processes point-cloud data collected by the on-board LiDAR sensors, extracting the cone-shaped volume of drill-waste above the ground. By projecting the 3D cone points into a virtual depth image, segmentation is achieved in the 2D domain, yielding a circular hole at the image centre and a collared cone face. We then identify the hole centre using a robust detection module while suppressing non-maximum candidates, ensuring precise sensor placement for down-hole inspection and avoiding collisions with the cavity wall. To enable autonomous hole-seeking, the pipeline automatically adjusts its projection parameters during robot navigation to account for variations in point sparsity and hole opening size, ensuring a consistent hole appearance in 2D images. This allows continuous tracking of the target hole as the robot approaches the goal point. We demonstrate the effectiveness of our navigation and perception system in both high-fidelity simulation environments and on-site field tests. A demonstration video is available at "this https URL.', 'abstract_zh': '露天矿钻孔检测与自动化勘探机器人“DIPPeR”的研究：基于目标的导航与精确孔内传感器定位框架', 'title_zh': '爆破孔搜索与倾斜测量——矿场检查机器人中的导航与感知框架'}
{'arxiv_id': 'arXiv:2508.13699', 'title': 'Assessing Pedestrian Behavior Around Autonomous Cleaning Robots in Public Spaces: Findings from a Field Observation', 'authors': 'Maren Raab, Linda Miller, Zhe Zeng, Pascal Jansen, Martin Baumann, Johannes Kraus', 'link': 'https://arxiv.org/abs/2508.13699', 'abstract': "As autonomous robots become more common in public spaces, spontaneous encounters with laypersons are more frequent. For this, robots need to be equipped with communication strategies that enhance momentary transparency and reduce the probability of critical situations. Adapting these robotic strategies requires consideration of robot movements, environmental conditions, and user characteristics and states. While numerous studies have investigated the impact of distraction on pedestrians' movement behavior, limited research has examined this behavior in the presence of autonomous robots. This research addresses the impact of robot type and robot movement pattern on distracted and undistracted pedestrians' movement behavior. In a field setting, unaware pedestrians were videotaped while moving past two working, autonomous cleaning robots. Out of N=498 observed pedestrians, approximately 8% were distracted by smartphones. Distracted and undistracted pedestrians did not exhibit significant differences in their movement behaviors around the robots. Instead, both the larger sweeping robot and the offset rectangular movement pattern significantly increased the number of lateral adaptations compared to the smaller cleaning robot and the circular movement pattern. The offset rectangular movement pattern also led to significantly more close lateral adaptations. Depending on the robot type, the movement patterns led to differences in the distances of lateral adaptations. The study provides initial insights into pedestrian movement behavior around an autonomous cleaning robot in public spaces, contributing to the growing field of HRI research.", 'abstract_zh': '随着自主机器人在公共空间中的普及，与普通人的偶然相遇更加频繁。因此，机器人需要配备能够增强短暂透明度并降低潜在危险情况概率的沟通策略。调整这些机器人策略需要考虑机器人的移动方式、环境条件以及用户特点和状态。尽管已有大量研究探讨了分心对行人移动行为的影响，但在机器人存在的情况下行人分心行为的研究却相对有限。本研究考察了不同类型的机器人和移动模式对受分心影响的行人与未受分心影响的行人移动行为的影响。在实地环境下，记录了行人经过两台正在工作的自主清洁机器人的移动情况。在观察的N=498名行人中，大约8%的人因使用智能手机而分心。无论是受分心影响的行人还是未受分心影响的行人，在机器人周围的移动行为均无显著差异。然而，与较小的清洁机器人和圆形移动模式相比，较大的清扫机器人和偏移矩形移动模式显著增加了侧向调整的频次。偏移矩形移动模式还导致了显著更多的近距离侧向调整。根据机器人类型，移动模式导致了侧向调整距离的不同。该研究提供了对行人围绕自主清洁机器人在公共空间中的移动行为的初步见解，为不断增加的人机交互研究领域做出了贡献。', 'title_zh': '公共空间中自主清洁机器人周边行人行为评估：实地观察研究发现'}
{'arxiv_id': 'arXiv:2508.13534', 'title': 'MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence', 'authors': 'Chao Tang, Anxing Xiao, Yuhong Deng, Tianrun Hu, Wenlong Dong, Hanbo Zhang, David Hsu, Hong Zhang', 'link': 'https://arxiv.org/abs/2508.13534', 'abstract': "Imitating tool manipulation from human videos offers an intuitive approach to teaching robots, while also providing a promising and scalable alternative to labor-intensive teleoperation data collection for visuomotor policy learning. While humans can mimic tool manipulation behavior by observing others perform a task just once and effortlessly transfer the skill to diverse tools for functionally equivalent tasks, current robots struggle to achieve this level of generalization. A key challenge lies in establishing function-level correspondences, considering the significant geometric variations among functionally similar tools, referred to as intra-function variations. To address this challenge, we propose MimicFunc, a framework that establishes functional correspondences with function frame, a function-centric local coordinate frame constructed with keypoint-based abstraction, for imitating tool manipulation skills. Experiments demonstrate that MimicFunc effectively enables the robot to generalize the skill from a single RGB-D human video to manipulating novel tools for functionally equivalent tasks. Furthermore, leveraging MimicFunc's one-shot generalization capability, the generated rollouts can be used to train visuomotor policies without requiring labor-intensive teleoperation data collection for novel objects. Our code and video are available at this https URL.", 'abstract_zh': '从人类视频中模仿工具操作为机器人教学提供了直观的方法，同时提供了劳动密集型电信操作数据收集在视觉运动策略学习方面的有 promise 和可扩展的替代方案。当前机器人难以实现人类一次观察任务并轻松将技能转移到多种功能等效工具的水平。关键挑战在于建立功能级别的对应关系，考虑到功能类似工具之间显著的几何变化，即功能内的变异。为了应对这一挑战，我们提出了一种名为MimicFunc的框架，该框架通过基于关键点的抽象构建以功能为中心的局部坐标系（功能框架）来建立功能对应关系，以模仿工具操作技能。实验表明，MimicFunc有效地使机器人能够从单个RGB-D人类视频中将技能泛化到新工具的功能等效任务中。此外，利用MimicFunc的一次性泛化能力，生成的滚动轨迹可用于训练视觉运动策略，而无需收集新的对象的劳动密集型电信操作数据。我们的代码和视频见此网址。', 'title_zh': 'MimicFunc: 从单个人类视频中模仿工具操作的函数对应方法'}
{'arxiv_id': 'arXiv:2508.13531', 'title': 'A Three-Level Whole-Body Disturbance Rejection Control Framework for Dynamic Motions in Legged Robots', 'authors': 'Bolin Li, Gewei Zuo, Zhixiang Wang, Xiaotian Ke, Lijun Zhu, Han Ding', 'link': 'https://arxiv.org/abs/2508.13531', 'abstract': 'This paper presents a control framework designed to enhance the stability and robustness of legged robots in the presence of uncertainties, including model uncertainties, external disturbances, and faults. The framework enables the full-state feedback estimator to estimate and compensate for uncertainties in whole-body dynamics of the legged robots. First, we propose a novel moving horizon extended state observer (MH-ESO) to estimate uncertainties and mitigate noise in legged systems, which can be integrated into the framework for disturbance compensation. Second, we introduce a three-level whole-body disturbance rejection control framework (T-WB-DRC). Unlike the previous two-level approach, this three-level framework considers both the plan based on whole-body dynamics without uncertainties and the plan based on dynamics with uncertainties, significantly improving payload transportation, external disturbance rejection, and fault tolerance. Third, simulations of both humanoid and quadruped robots in the Gazebo simulator demonstrate the effectiveness and versatility of T-WB-DRC. Finally, extensive experimental trials on a quadruped robot validate the robustness and stability of the system when using T-WB-DRC under various disturbance conditions.', 'abstract_zh': '一种用于增强腿式机器人稳定性和鲁棒性的控制框架：考虑不确定性的全身动态 disturbance 抵抗控制三级架构', 'title_zh': '基于腿式机器人动态运动的三层次全身扰动 rejection 控制框架'}
{'arxiv_id': 'arXiv:2508.13513', 'title': 'Unified Hierarchical MPC in Task Executing for Modular Manipulators across Diverse Morphologies', 'authors': 'Maolin Lei, Edoardo Romiti, Arturo Laurenzi, Cheng Zhou, Wanli Xing, Liang Lu, Nikos G. Tsagarakis', 'link': 'https://arxiv.org/abs/2508.13513', 'abstract': 'This work proposes a unified Hierarchical Model Predictive Control (H-MPC) for modular manipulators across various morphologies, as the controller can adapt to different configurations to execute the given task without extensive parameter tuning in the controller. The H-MPC divides the control process into two levels: a high-level MPC and a low-level MPC. The high-level MPC predicts future states and provides trajectory information, while the low-level MPC refines control actions by updating the predictive model based on this high-level information. This hierarchical structure allows for the integration of kinematic constraints and ensures smooth joint-space trajectories, even near singular configurations. Moreover, the low-level MPC incorporates secondary linearization by leveraging predictive information from the high-level MPC, effectively capturing the second-order Taylor expansion information of the kinematic model while still maintaining a linearized model formulation. This approach not only preserves the simplicity of a linear control model but also enhances the accuracy of the kinematic representation, thereby improving overall control precision and reliability. To validate the effectiveness of the control policy, we conduct extensive evaluations across different manipulator morphologies and demonstrate the execution of pick-and-place tasks in real-world scenarios.', 'abstract_zh': '一种统一层次模型预测控制（H-MPC）方法：针对不同形态模块化 manipulator 的通用控制策略', 'title_zh': '模块化 manipulator 跨越 diverse 形态执行任务时的统一分级 MPC 控制'}
{'arxiv_id': 'arXiv:2508.13488', 'title': 'ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments', 'authors': 'Jingwen Yu, Jiayi Yang, Anjun Hu, Jiankun Wang, Ping Tan, Hong Zhang', 'link': 'https://arxiv.org/abs/2508.13488', 'abstract': "Loop closure detection is important for simultaneous localization and mapping (SLAM), which associates current observations with historical keyframes, achieving drift correction and global relocalization. However, a falsely detected loop can be fatal, and this is especially difficult in repetitive environments where appearance-based features fail due to the high similarity. Therefore, verification of a loop closure is a critical step in avoiding false positive detections. Existing works in loop closure verification predominantly focus on learning invariant appearance features, neglecting the prior knowledge of the robot's spatial-temporal motion cue, i.e., trajectory. In this letter, we propose ROVER, a loop closure verification method that leverages the historical trajectory as a prior constraint to reject false loops in challenging repetitive environments. For each loop candidate, it is first used to estimate the robot trajectory with pose-graph optimization. This trajectory is then submitted to a scoring scheme that assesses its compliance with the trajectory without the loop, which we refer to as the trajectory prior, to determine if the loop candidate should be accepted. Benchmark comparisons and real-world experiments demonstrate the effectiveness of the proposed method. Furthermore, we integrate ROVER into state-of-the-art SLAM systems to verify its robustness and efficiency. Our source code and self-collected dataset are available at this https URL.", 'abstract_zh': '基于历史轨迹的循环闭合验证方法ROVER', 'title_zh': 'ROVER：在重复环境中具有轨迹先验的鲁棒循环闭合验证'}
{'arxiv_id': 'arXiv:2508.13459', 'title': 'Multi-Robot Navigation in Social Mini-Games: Definitions, Taxonomy, and Algorithms', 'authors': 'Rohan Chandra, Shubham Singh, Abhishek Jha, Dannon Andrade, Hriday Sainathuni, Katia Sycara', 'link': 'https://arxiv.org/abs/2508.13459', 'abstract': "The ``Last Mile Challenge'' has long been considered an important, yet unsolved, challenge for autonomous vehicles, public service robots, and delivery robots. A central issue in this challenge is the ability of robots to navigate constrained and cluttered environments (e.g., doorways, hallways, corridor intersections), often while competing for space with other robots and humans. We refer to these environments as ``Social Mini-Games'' (SMGs). SMGs are tightly coupled, high-agency interactions that arise within general multi-robot navigation (MRN) scenarios. They are identified through certain distinct characteristics and require specialized metrics to evaluate them. Traditional navigation approaches designed for MRN do not perform well in SMGs, which has led to focused research on dedicated SMG solvers (navigation methods specialized to navigate in SMGs), which has flourished in recent years. However, publications on SMG navigation research make different assumptions (on centralized versus decentralized, observability, communication, cooperation, etc.), and have different objective functions (safety versus liveness). These assumptions and objectives are sometimes implicitly assumed or described informally. This makes it difficult to establish appropriate baselines for comparison in research papers, as well as making it difficult for practitioners to find the papers relevant to their concrete application. Such ad-hoc representation of the field also presents a barrier to new researchers wanting to start research in this area. SMG navigation research requires its own taxonomy, definitions, and evaluation protocols to guide effective research moving forward. This survey is the first to catalog SMG solvers using a well-defined and unified taxonomy and to classify existing methods accordingly.", 'abstract_zh': 'SMG导航研究的分类、定义与评估protocol：一种严格分类和统一 taxonomy 的综述', 'title_zh': '多人机器人在社会迷你游戏中导航：定义、分类和算法'}
{'arxiv_id': 'arXiv:2508.13457', 'title': 'Modeling and Control of AWOISV: A Filtered Tube-Based MPC Approach for Simultaneous Tracking of Lateral Position and Heading Angle', 'authors': 'Xu Yang, Jun Ni, Hengyang Feng, Feiyu Wang, Tiezhen Wang', 'link': 'https://arxiv.org/abs/2508.13457', 'abstract': 'An all-wheel omni-directional independent steering vehicle (AWOISV) is a specialized all-wheel independent steering vehicle with each wheel capable of steering up to 90°, enabling unique maneuvers like yaw and diagonal movement. This paper introduces a theoretical steering radius angle and sideslip angle (\\( \\theta_R \\)-\\(\\beta_R \\)) representation, based on the position of the instantaneous center of rotation relative to the wheel rotation center, defining the motion modes and switching criteria for AWOISVs. A generalized \\( v\\)-\\(\\beta\\)-\\(r \\) dynamic model is developed with forward velocity \\(v\\), sideslip angle \\(\\beta\\), and yaw rate \\(r\\) as states, and \\(\\theta_R\\) and \\(\\beta_R\\) as control inputs. This model decouples longitudinal and lateral motions into forward and rotational motions, allowing seamless transitions across all motion modes under specific conditions. A filtered tube-based linear time-varying MPC (FT-LTVMPC) strategy is proposed, achieving simultaneous tracking of lateral position and arbitrary heading angles, with robustness to model inaccuracies and parameter uncertainties. Co-simulation and hardware-in-loop (HIL) experiments confirm that FT-LTVMPC enables high-precision control of both position and heading while ensuring excellent real-time performance.', 'abstract_zh': '全轮全方位独立转向车辆（AWOISV）的理论转向半径角和侧滑角（\\(\\theta_R\\)-\\(\\beta_R\\））表示及其通用\\(v\\)-\\(\\beta\\)-\\(r\\)动力学模型与过滤管基线时变模型预测控制（FT-LTVMPC）策略', 'title_zh': '基于滤波管件的模型预测控制方法：AWOISV的横向位置和 方向角同时跟踪建模与控制'}
{'arxiv_id': 'arXiv:2508.13446', 'title': 'CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models', 'authors': 'Catherine Glossop, William Chen, Arjun Bhorkar, Dhruv Shah, Sergey Levine', 'link': 'https://arxiv.org/abs/2508.13446', 'abstract': "Generalist robots should be able to understand and follow user instructions, but current vision-language-action (VLA) models struggle with following fine-grained commands despite providing a powerful architecture for mapping open-vocabulary natural language instructions to robot actions. One cause for this is a lack of semantic diversity and language grounding in existing robot datasets and, specifically, a lack of fine-grained task diversity for similar observations. To address this, we present a novel method to augment existing robot datasets by leveraging vision language models to create counterfactual labels. Our method improves the language-following capabilities of VLAs by increasing the diversity and granularity of language grounding for robot datasets by generating counterfactual language and actions. We evaluate the resulting model's ability to follow language instructions, ranging from simple object-centric commands to complex referential tasks, by conducting visual language navigation experiments in 3 different indoor and outdoor environments. Our experiments demonstrate that counterfactual relabeling, without any additional data collection, significantly improves instruction-following in VLA policies, making them competitive with state-of-the-art methods and increasing success rate by 27% on navigation tasks.", 'abstract_zh': '通用机器人应该能够理解并遵循用户指令，但现有的视觉-语言-动作（VLA）模型在遵循精细粒度的命令方面仍然存在困难，尽管VLA模型为将开放词汇自然语言指令映射到机器人动作提供了强大的架构。这种困难的一个原因是现有机器人数据集在语义多样性和语言接地方面的不足，特别是相似观察下的精细任务多样性不足。为了解决这一问题，我们提出了一种利用视觉语言模型来扩充现有机器人数据集的新方法，以生成反事实标签。该方法通过生成反事实语言和动作来提高机器人数据集的语义多样性和粒度，从而提升VLA的语言遵循能力。我们通过在3种不同室内外环境下进行视觉语言导航实验，评估所生成模型遵循语言指令的能力，该指令从简单的物体中心命令到复杂的参照任务。实验结果表明，无需额外数据收集，反事实重新标签显著提高了VLA策略的指令遵循能力，使其与先进方法竞争，在导航任务上的成功率提高了27%。', 'title_zh': 'CAST: 反事实标签 improves 视觉-语言-行动模型的指令跟随'}
{'arxiv_id': 'arXiv:2508.13444', 'title': 'Switch4EAI: Leveraging Console Game Platform for Benchmarking Robotic Athletics', 'authors': 'Tianyu Li, Jeonghwan Kim, Wontaek Kim, Donghoon Baek, Seungeun Rho, Sehoon Ha', 'link': 'https://arxiv.org/abs/2508.13444', 'abstract': "Recent advances in whole-body robot control have enabled humanoid and legged robots to execute increasingly agile and coordinated movements. However, standardized benchmarks for evaluating robotic athletic performance in real-world settings and in direct comparison to humans remain scarce. We present Switch4EAI(Switch-for-Embodied-AI), a low-cost and easily deployable pipeline that leverages motion-sensing console games to evaluate whole-body robot control policies. Using Just Dance on the Nintendo Switch as a representative example, our system captures, reconstructs, and retargets in-game choreography for robotic execution. We validate the system on a Unitree G1 humanoid with an open-source whole-body controller, establishing a quantitative baseline for the robot's performance against a human player. In the paper, we discuss these results, which demonstrate the feasibility of using commercial games platform as physically grounded benchmarks and motivate future work to for benchmarking embodied AI.", 'abstract_zh': "Recent Advances inWhole-Body Robot Control Have Enabled Humanoid and Legged Robots to Execute Increasingly Agile and Coordinated Movements. However, Standardized Benchmarks for Evaluating Robotic Athletic Performance in Real-World Settings and in Direct Comparison to Humans Remain Scarce. We Present Switch4EAI (Switch-for-Embodied-AI), a Low-Cost and Easily Deployable Pipeline That Leverages Motion-Sensing Console Games to Evaluate Whole-Body Robot Control Policies. Using Just Dance on the Nintendo Switch as a Representative Example, Our System Captures, Reconstructs, and Retargets In-Game Choreography for Robotic Execution. We Validate the System on a Unitree G1 Humanoid with an Open-Source Whole-Body Controller, Establishing a Quantitative Baseline for the Robot's Performance Against a Human Player. In the Paper, We Discuss These Results, Which Demonstrate the Feasibility of Using Commercial Games Platforms as Physically Grounded Benchmarks and Motivate Future Work for Benchmarking Embodied AI.", 'title_zh': 'Switch4EAI: 基于杠杆的ConsoleE平台平台用于机器人运动benchmarkbenchmark评估'}
{'arxiv_id': 'arXiv:2508.13407', 'title': 'Accelerating Signal-Temporal-Logic-Based Task and Motion Planning of Bipedal Navigation using Benders Decomposition', 'authors': 'Jiming Ren, Xuan Lin, Roman Mineyev, Karen M. Feigh, Samuel Coogan, Ye Zhao', 'link': 'https://arxiv.org/abs/2508.13407', 'abstract': 'Task and motion planning under Signal Temporal Logic constraints is known to be NP-hard. A common class of approaches formulates these hybrid problems, which involve discrete task scheduling and continuous motion planning, as mixed-integer programs (MIP). However, in applications for bipedal locomotion, introduction of non-convex constraints such as kinematic reachability and footstep rotation exacerbates the computational complexity of MIPs. In this work, we present a method based on Benders Decomposition to address scenarios where solving the entire monolithic optimization problem is prohibitively intractable. Benders Decomposition proposes an iterative cutting-plane technique that partitions the problem into a master problem to prototype a plan that meets the task specification, and a series of subproblems for kinematics and dynamics feasibility checks. Our experiments demonstrate that this method achieves faster planning compared to alternative algorithms for solving the resulting optimization program with nonlinear constraints.', 'abstract_zh': '基于信号时逻辑约束的任务与运动规划在已知是NP难问题的情况下，一种常见的方法将其形式化为混合整数规划问题。然而，在双足运动应用中，引入非凸约束如运动学可达性和足部旋转会进一步增加混合整数规划问题的计算复杂性。本文提出了一种基于Benders分解的方法，以应对求解整体优化问题不可行的场景。Benders分解提出了一种迭代切割平面技术，将问题分解为主问题和一系列子问题，主问题原型化满足任务规范的计划，子问题则进行运动学和动力学可行性检查。我们的实验表明，该方法在解决具有非线性约束的优化程序时比替代算法实现了更快的规划。', 'title_zh': '基于贝纳德斯分解的双足导航信号-时序逻辑任务与运动规划加速方法'}
{'arxiv_id': 'arXiv:2508.13392', 'title': 'Incremental Generalized Hybrid A*', 'authors': 'Sidharth Talia, Oren Salzman, Siddhartha Srinivasa', 'link': 'https://arxiv.org/abs/2508.13392', 'abstract': 'We address the problem of efficiently organizing search over very large trees, which arises in many applications ranging from autonomous driving to aerial vehicles. Here, we are motivated by off-road autonomy, where real-time planning is essential. Classical approaches use graphs of motion primitives and exploit dominance to mitigate the curse of dimensionality and prune expansions efficiently. However, for complex dynamics, repeatedly solving two-point boundary-value problems makes graph construction too slow for fast kinodynamic planning. Hybrid A* (HA*) addressed this challenge by searching over a tree of motion primitives and introducing approximate pruning using a grid-based dominance check. However, choosing the grid resolution is difficult: too coarse risks failure, while too fine leads to excessive expansions and slow planning. We propose Incremental Generalized Hybrid A* (IGHA*), an anytime tree-search framework that dynamically organizes vertex expansions without rigid pruning. IGHA* provably matches or outperforms HA*. For both on-road kinematic and off-road kinodynamic planning queries for a car-like robot, variants of IGHA* use 6x fewer expansions to the best solution compared to an optimized version of HA*. In simulated off-road experiments in a high fidelity simulator, IGHA* outperforms HA*M when both are used in the loop with a model predictive controller. We demonstrate real-time performance both in simulation and on a small-scale off-road vehicle, enabling fast, robust planning under complex dynamics. Code: this https URL', 'abstract_zh': '高效组织大规模树结构搜索的问题及其应用：增量通用混合A*在离路面自主导航中的表现', 'title_zh': '增量广义混合A*算法'}
{'arxiv_id': 'arXiv:2508.13319', 'title': 'A Surveillance Based Interactive Robot', 'authors': 'Kshitij Kavimandan, Pooja Mangal, Devanshi Mehta', 'link': 'https://arxiv.org/abs/2508.13319', 'abstract': 'We build a mobile surveillance robot that streams video in real time and responds to speech so a user can monitor and steer it from a phone or browser. The system uses two Raspberry Pi 4 units: a front unit on a differential drive base with camera, mic, and speaker, and a central unit that serves the live feed and runs perception. Video is sent with FFmpeg. Objects in the scene are detected using YOLOv3 to support navigation and event awareness. For voice interaction, we use Python libraries for speech recognition, multilingual translation, and text-to-speech, so the robot can take spoken commands and read back responses in the requested language. A Kinect RGB-D sensor provides visual input and obstacle cues. In indoor tests the robot detects common objects at interactive frame rates on CPU, recognises commands reliably, and translates them to actions without manual control. The design relies on off-the-shelf hardware and open software, making it easy to reproduce. We discuss limits and practical extensions, including sensor fusion with ultrasonic range data, GPU acceleration, and adding face and text recognition.', 'abstract_zh': '我们构建了一种移动 surveillance 机器人，能够实时传输视频并响应语音指令，以便用户通过手机或浏览器监控和操控。该系统使用了两台Raspberry Pi 4单板：一台装有差速驱动底盘、摄像头、麦克风和扬声器的前端单板，以及一台中央单板，用于提供实时视频流并运行感知模块。视频通过FFmpeg传输。使用YOLOv3检测场景中的物体以支持导航和事件感知。对于语音交互，我们使用了Python库进行语音识别、多语言翻译和文本转语音，使机器人能够接受语音指令并在请求的语言中朗读回应。Kinect RGB-D传感器提供了视觉输入和障碍物提示。在室内测试中，机器人能够在CPU上以交互帧率检测常见物体、可靠地识别命令并将它们转换为动作，无需手动控制。该设计依赖于现成的硬件和开源软件，使其易于复制。我们讨论了其局限性和实用扩展，包括与超声波距离数据融合、使用GPU加速以及增加人脸识别和文字识别功能。', 'title_zh': '基于监视的交互式机器人'}
{'arxiv_id': 'arXiv:2508.13303', 'title': 'Diff-MSM: Differentiable MusculoSkeletal Model for Simultaneous Identification of Human Muscle and Bone Parameters', 'authors': 'Yingfan Zhou, Philip Sanderink, Sigurd Jager Lemming, Cheng Fang', 'link': 'https://arxiv.org/abs/2508.13303', 'abstract': 'High-fidelity personalized human musculoskeletal models are crucial for simulating realistic behavior of physically coupled human-robot interactive systems and verifying their safety-critical applications in simulations before actual deployment, such as human-robot co-transportation and rehabilitation through robotic exoskeletons. Identifying subject-specific Hill-type muscle model parameters and bone dynamic parameters is essential for a personalized musculoskeletal model, but very challenging due to the difficulty of measuring the internal biomechanical variables in vivo directly, especially the joint torques. In this paper, we propose using Differentiable MusculoSkeletal Model (Diff-MSM) to simultaneously identify its muscle and bone parameters with an end-to-end automatic differentiation technique differentiating from the measurable muscle activation, through the joint torque, to the resulting observable motion without the need to measure the internal joint torques. Through extensive comparative simulations, the results manifested that our proposed method significantly outperformed the state-of-the-art baseline methods, especially in terms of accurate estimation of the muscle parameters (i.e., initial guess sampled from a normal distribution with the mean being the ground truth and the standard deviation being 10% of the ground truth could end up with an average of the percentage errors of the estimated values as low as 0.05%). In addition to human musculoskeletal modeling and simulation, the new parameter identification technique with the Diff-MSM has great potential to enable new applications in muscle health monitoring, rehabilitation, and sports science.', 'abstract_zh': '高保真个性化人体 musculoskeletal 模型对于模拟人-机器人交互系统的真实行为以及在实际部署前通过仿真验证其关键安全应用（如人-机器人共同运输和基于外骨骼的康复）至关重要。通过关节扭矩自最终可测量的肌肉激活反向自动微分确定个性化 musculoskeletal 模型的肌肉和骨骼参数是非常必要的，但由于难以直接测量体内的生物力学变量（尤其是关节扭矩），这一任务极具挑战性。本文提出使用可微 musculoskeletal 模型（Diff-MSM）结合端到端自动微分技术，通过关节扭矩到可观测运动的反向传播，无需测量内部关节扭矩即可同时识别其肌肉和骨骼参数。通过广泛比较仿真，结果表明，我们提出的方法在肌肉参数准确估计方面显著优于现有最先进的基准方法，尤其是在肌肉参数估计的准确性方面（初始猜测来自正态分布，均值为真实值，标准差为真实值的10%，最终估计值的平均百分比误差低至0.05%）。除了人体 musculoskeletal 模型和仿真之外，Diff-MSM 的新参数识别技术还具有在肌肉健康监测、康复和体育科学等领域启用新应用的巨大潜力。', 'title_zh': 'Diff-MSM: 可微肌骨模型同时识别人类肌肉和骨骼参数'}
{'arxiv_id': 'arXiv:2508.14006', 'title': 'ResPlan: A Large-Scale Vector-Graph Dataset of 17,000 Residential Floor Plans', 'authors': 'Mohamed Abouagour, Eleftherios Garyfallidis', 'link': 'https://arxiv.org/abs/2508.14006', 'abstract': 'We introduce ResPlan, a large-scale dataset of 17,000 detailed, structurally rich, and realistic residential floor plans, created to advance spatial AI research. Each plan includes precise annotations of architectural elements (walls, doors, windows, balconies) and functional spaces (such as kitchens, bedrooms, and bathrooms). ResPlan addresses key limitations of existing datasets such as RPLAN (Wu et al., 2019) and MSD (van Engelenburg et al., 2024) by offering enhanced visual fidelity and greater structural diversity, reflecting realistic and non-idealized residential layouts. Designed as a versatile, general-purpose resource, ResPlan supports a wide range of applications including robotics, reinforcement learning, generative AI, virtual and augmented reality, simulations, and game development. Plans are provided in both geometric and graph-based formats, enabling direct integration into simulation engines and fast 3D conversion. A key contribution is an open-source pipeline for geometry cleaning, alignment, and annotation refinement. Additionally, ResPlan includes structured representations of room connectivity, supporting graph-based spatial reasoning tasks. Finally, we present comparative analyses with existing benchmarks and outline several open benchmark tasks enabled by ResPlan. Ultimately, ResPlan offers a significant advance in scale, realism, and usability, providing a robust foundation for developing and benchmarking next-generation spatial intelligence systems.', 'abstract_zh': 'ResPlan：一个包含17,000个详细、结构丰富且逼真的住宅平面图的大规模数据集', 'title_zh': 'ResPlan: 一个包含17,000个住宅平面图的大型向量-图数据集'}
{'arxiv_id': 'arXiv:2508.13802', 'title': 'A Screw Approach to the Approximation of the Local Geometry of the Configuration Space and of the set of Configurations of Certain Rank of Lower Pair Linkages', 'authors': 'Andreas Mueller', 'link': 'https://arxiv.org/abs/2508.13802', 'abstract': "A motion of a mechanism is a curve in its configuration space (c-space). Singularities of the c-space are kinematic singularities of the mechanism. Any mobility analysis of a particular mechanism amounts to investigating the c-space geometry at a given configuration. A higher-order analysis is necessary to determine the finite mobility. To this end, past research lead to approaches using higher-order time derivatives of loop closure constraints assuming (implicitly) that all possible motions are smooth. This continuity assumption limits the generality of these methods. In this paper an approach to the higher-order local mobility analysis of lower pair multi-loop linkages is presented. This is based on a higher-order Taylor series expansion of the geometric constraint mapping, for which a recursive algebraic expression in terms of joint screws is presented. An exhaustive local analysis includes analysis of the set of constraint singularities (configurations where the constraint Jacobian has certain corank). A local approximation of the set of configurations with certain rank is presented, along with an explicit expression for the differentials of Jacobian minors in terms of instantaneous joint screws. The c-space and the set of points of certain corank are therewith locally approximated by an algebraic variety determined algebraically from the mechanism's screw system. Results are shown for a simple planar 4-bar linkage, which exhibits a bifurcation singularity, and for a planar three-loop linkage exhibiting a cusp in c-space. The latter cannot be treated by the higher-order local analysis methods proposed in the literature.", 'abstract_zh': '一种机制的运动是其配置空间（c-空间）中的曲线。c-空间的奇异点是机制的运动奇异点。对某一特定机制的自由度分析等同于研究给定配置下的c-空间几何。要确定有限自由度，需要进行更高阶分析。过去的研究通过假设（隐含地）所有可能运动都平滑来进行更高阶时间导数的环闭约束分析。这一连续性假设限制了这些方法的普遍性。本文提出了低副多环杆机构更高阶局部自由度分析的方法，基于几何约束映射的更高阶泰勒级数展开，给出了基于关节楔形的递归代数表达式。完整的局部分析包括约束奇异点集（约束雅可比矩阵具有特定秩亏度的配置）的分析。给出了具有特定秩的配置集的局部近似表示，以及雅可比子式微分的显式表达式，与瞬时关节楔形相关。c-空间和特定秩亏度点的集合通过机制楔形系统的代数方法确定的代数簇进行局部近似。结果展示了具有分支奇异点的简单平面4杆机构以及具有c-空间尖点的平面三环杆机构。后者无法被文献中提出的局部高阶分析方法处理。', 'title_zh': '螺杆方法在连续刚体体系局部几何结构及其特定秩的配置集逼近中的应用'}
{'arxiv_id': 'arXiv:2508.13775', 'title': 'MR6D: Benchmarking 6D Pose Estimation for Mobile Robots', 'authors': 'Anas Gouda, Shrutarv Awasthi, Christian Blesing, Lokeshwaran Manohar, Frank Hoffmann, Alice Kirchheim', 'link': 'https://arxiv.org/abs/2508.13775', 'abstract': 'Existing 6D pose estimation datasets primarily focus on small household objects typically handled by robot arm manipulators, limiting their relevance to mobile robotics. Mobile platforms often operate without manipulators, interact with larger objects, and face challenges such as long-range perception, heavy self-occlusion, and diverse camera perspectives. While recent models generalize well to unseen objects, evaluations remain confined to household-like settings that overlook these factors. We introduce MR6D, a dataset designed for 6D pose estimation for mobile robots in industrial environments. It includes 92 real-world scenes featuring 16 unique objects across static and dynamic interactions. MR6D captures the challenges specific to mobile platforms, including distant viewpoints, varied object configurations, larger object sizes, and complex occlusion/self-occlusion patterns. Initial experiments reveal that current 6D pipelines underperform in these settings, with 2D segmentation being another hurdle. MR6D establishes a foundation for developing and evaluating pose estimation methods tailored to the demands of mobile robotics. The dataset is available at this https URL.', 'abstract_zh': '现有的6D姿态估计数据集主要关注由机器人臂操作的 Household 小型物体，限制了其在移动机器人领域的相关性。移动平台通常不配备操作器，与大型物体互动，并面临远程感知、严重的自遮挡以及多变的相机视角等挑战。尽管最近的模型在未见过的对象上表现良好，但评估仍然局限于类似的家居环境设置，而忽略了这些因素。我们介绍了 MR6D 数据集，旨在为工业环境中移动机器人提供6D姿态估计。该数据集包含92个真实场景，涵盖了16种不同物体在静态和动态互动中的表现。MR6D 捕捉了移动平台特有的挑战，包括远程视角、多样的物体配置、较大的物体尺寸以及复杂的遮挡/自遮挡模式。初步实验表明，当前的6D管道在这些场景中表现不佳，2D分割也是另一个障碍。MR6D 为开发和评估符合移动机器人需求的姿态估计方法奠定了基础。数据集可在以下链接获取：this https URL。', 'title_zh': 'MR6D: 手持机器人6D位姿估计基准测试'}
{'arxiv_id': 'arXiv:2508.13656', 'title': 'AutoMPC: A Code Generator for MPC-based Automated Driving', 'authors': 'Georg Schildbach, Jasper Pflughaupt', 'link': 'https://arxiv.org/abs/2508.13656', 'abstract': 'Model Predictive Control (MPC) is a powerful technique to control nonlinear, multi-input multi-output systems subject to input and state constraints. It is now a standard tool for trajectory tracking control of automated vehicles. As such it has been used in many research and development projects. However, MPC faces several challenges to be integrated into industrial production vehicles. The most important ones are its high computational demands and the complexity of implementation. The software packages AutoMPC aims to address both of these challenges. It builds on a robustified version of an active set algorithm for Nonlinear MPC. The algorithm is embedded into a framework for vehicle trajectory tracking, which makes it easy to used, yet highly customizable. Automatic code generation transforms the selections into a standalone, computationally efficient C-code file with static memory allocation. As such it can be readily deployed on a wide range of embedded platforms, e.g., based on Matlab/Simulink or Robot Operating System (ROS). Compared to a previous version of the code, the vehicle model and the numerical integration method can be manually specified, besides basic algorithm parameters. All of this information and all specifications are directly baked into the generated C-code. The algorithm is suitable driving scenarios at low or high speeds, even drifting, and supports direction changes. Multiple simulation scenarios show the versatility and effectiveness of the AutoMPC code, with the guarantee of a feasible solution, a high degree of robustness, and computational efficiency.', 'abstract_zh': '模型预测控制（MPC）是用于控制受输入和状态约束的非线性多输入多输出系统的强大技术，现已成为自动驾驶车辆轨迹跟踪控制的标准工具。尽管如此，MPC 集成到工业生产车辆中仍面临诸多挑战，其中最重要的包括其高计算需求和复杂的实现方式。软件包AutoMPC旨在解决这两个问题。它基于鲁棒化的非线性MPC活性集算法版本，并将其嵌入到车辆轨迹跟踪框架中，使其易于使用且高度可定制。自动代码生成将选择转换为独立的、计算效率高的C代码文件，具有静态内存分配。因此，它可以便捷地部署到各种嵌入式平台，例如基于Matlab/Simulink或Robot Operating System (ROS)。与之前版本的代码相比，不仅可以手动指定车辆模型和数值积分方法，还可以指定基本算法参数。所有这些信息和所有规范都直接嵌入到生成的C代码中。该算法适用于低速或高速驾驶场景，甚至漂移，并支持方向变化。多个仿真场景展示了AutoMPC代码的多样性和有效性，并保证了可行解、高鲁棒性和计算效率。', 'title_zh': 'AutoMPC: 一种基于MPC的自动行驶代码生成器'}
{'arxiv_id': 'arXiv:2508.13564', 'title': 'The 9th AI City Challenge', 'authors': 'Zheng Tang, Shuo Wang, David C. Anastasiu, Ming-Ching Chang, Anuj Sharma, Quan Kong, Norimasa Kobori, Munkhjargal Gochoo, Ganzorig Batnasan, Munkh-Erdene Otgonbold, Fady Alnajjar, Jun-Wei Hsieh, Tomasz Kornuta, Xiaolong Li, Yilin Zhao, Han Zhang, Subhashree Radhakrishnan, Arihant Jain, Ratnesh Kumar, Vidya N. Murali, Yuxing Wang, Sameer Satish Pusegaonkar, Yizhou Wang, Sujit Biswas, Xunlei Wu, Zhedong Zheng, Pranamesh Chakraborty, Rama Chellappa', 'link': 'https://arxiv.org/abs/2508.13564', 'abstract': 'The ninth AI City Challenge continues to advance real-world applications of computer vision and AI in transportation, industrial automation, and public safety. The 2025 edition featured four tracks and saw a 17% increase in participation, with 245 teams from 15 countries registered on the evaluation server. Public release of challenge datasets led to over 30,000 downloads to date. Track 1 focused on multi-class 3D multi-camera tracking, involving people, humanoids, autonomous mobile robots, and forklifts, using detailed calibration and 3D bounding box annotations. Track 2 tackled video question answering in traffic safety, with multi-camera incident understanding enriched by 3D gaze labels. Track 3 addressed fine-grained spatial reasoning in dynamic warehouse environments, requiring AI systems to interpret RGB-D inputs and answer spatial questions that combine perception, geometry, and language. Both Track 1 and Track 3 datasets were generated in NVIDIA Omniverse. Track 4 emphasized efficient road object detection from fisheye cameras, supporting lightweight, real-time deployment on edge devices. The evaluation framework enforced submission limits and used a partially held-out test set to ensure fair benchmarking. Final rankings were revealed after the competition concluded, fostering reproducibility and mitigating overfitting. Several teams achieved top-tier results, setting new benchmarks in multiple tasks.', 'abstract_zh': '第九届AI城市挑战赛继续推动计算机视觉和人工智能在交通、工业自动化和公共安全领域的实际应用。2025年版设四个赛道，参与者人数增加了17%，共有来自15个国家的245支队伍在评估服务器上注册。挑战赛数据集的公开发布了超过30,000次下载。赛道1关注多类3D多相机跟踪，涉及人员、类人机器人、自主移动机器人和叉车，使用详细的校准和3D边界框注释。赛道2处理交通安全性中的视频问答，通过3D注视标签增强多相机事故理解。赛道3解决动态仓储环境中的精细空间推理，要求AI系统解释RGB-D输入并回答结合感知、几何和语言的空间问题。赛道1和赛道3的数据集均在NVIDIA Omniverse中生成。赛道4强调从鱼眼相机高效检测道路物体，支持在边缘设备上进行轻量级、实时部署。评估框架实施提交限制，并使用部分保留的测试集以确保公平基准测试。最终排名在比赛结束后揭晓，促进可重复性和减轻过拟合。多支队伍取得了顶尖成绩，多项任务设置了新的基准。', 'title_zh': '第九届AI城市挑战赛'}
{'arxiv_id': 'arXiv:2508.13339', 'title': 'Observed Control -- Linearly Scalable Nonlinear Model Predictive Control with Adaptive Horizons', 'authors': 'Eugene T. Hamzezadeh, Andrew J. Petruska', 'link': 'https://arxiv.org/abs/2508.13339', 'abstract': 'This work highlights the duality between state estimation methods and model predictive control. A predictive controller, observed control, is presented that uses this duality to efficiently compute control actions with linear time-horizon length scalability. The proposed algorithms provide exceptional computational efficiency, adaptive time horizon lengths, and early optimization termination criteria. The use of Kalman smoothers as the backend optimization framework provides for a straightforward implementation supported by strong theoretical guarantees. Additionally, a formulation is presented that separates linear model predictive control into purely reactive and anticipatory components, enabling any-time any-horizon observed control while ensuring controller stability for short time horizons. Finally, numerical case studies confirm that nonlinear filter extensions, i.e., the extended Kalman filter and unscented Kalman filter, effectively extend observed control to nonlinear systems and objectives.', 'abstract_zh': '这项工作强调了状态估计方法与模型预测控制之间的二重性。提出了一种预测控制器，称为观察控制，该控制器利用这种二重性高效地计算控制动作，并具有线性时间范围长度可扩展性。所提出的算法提供了出色的计算效率、自适应的时间范围长度以及早期优化终止标准。用卡尔曼平滑器作为后端优化框架，提供了一种直接实现方式，并具有强大的理论保证。此外，提出了一种将线性模型预测控制分解为纯粹的反应性和预见性组件的公式，从而实现任何时间和任何时间范围的观察控制，同时确保控制器在短时间范围内的稳定性。最后，数值案例研究证实，非线性滤波器扩展，即扩展卡尔曼滤波器和无迹卡尔曼滤波器，有效地将观察控制扩展到非线性系统和目标。', 'title_zh': '观测控制——具有自适应 horizons 的线性可扩展非线性模型预测控制'}
{'arxiv_id': 'arXiv:2409.11041', 'title': 'Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming', 'authors': 'Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen', 'link': 'https://arxiv.org/abs/2409.11041', 'abstract': "While there has been a lot of research recently on robots in household environments, at the present time, most robots in existence can be found on shop floors, and most interactions between humans and robots happen there. ``Collaborative robots'' (cobots) designed to work alongside humans on assembly lines traditionally require expert programming, limiting ability to make changes, or manual guidance, limiting expressivity of the resulting programs. To address these limitations, we explore using Large Language Models (LLMs), and in particular, their abilities of doing in-context learning, for conversational code generation. As a first step, we define RATS, the ``Repetitive Assembly Task'', a 2D building task designed to lay the foundation for simulating industry assembly scenarios. In this task, a `programmer' instructs a cobot, using natural language, on how a certain assembly is to be built; that is, the programmer induces a program, through natural language. We create a dataset that pairs target structures with various example instructions (human-authored, template-based, and model-generated) and example code. With this, we systematically evaluate the capabilities of state-of-the-art LLMs for synthesising this kind of code, given in-context examples. Evaluating in a simulated environment, we find that LLMs are capable of generating accurate `first order code' (instruction sequences), but have problems producing `higher-order code' (abstractions such as functions, or use of loops).", 'abstract_zh': '虽然近年来关于家庭环境中的机器人研究取得了很大进展，但目前大多数机器人仍然位于工厂车间，人类与机器人之间的大多数互动也发生在这些地方。“协作机器人”（cobots）设计用于在装配线上与人类协同工作，传统上需要专家编程，限制了对其作出更改的能力，或者需要手动指导，限制了生成程序的表达性。为了解决这些问题，我们探索使用大语言模型（LLMs），尤其是它们的上下文学习能力来进行对话式代码生成。作为第一步，我们定义了RATS，即“重复装配任务”，这是一个旨在模拟工业装配场景的2D构建任务。在这个任务中，一个“程序员”使用自然语言指示cobot如何构建特定的装配件；也就是说，程序员通过自然语言诱导出一个程序。我们创建了一个数据集，该数据集将目标结构与各种示例指令（由人类撰写、基于模板以及模型生成）和示例代码配对。通过这种方式，我们系统地评估了最先进的LLMs生成此类代码的能力，给定上下文示例。在模拟环境中评估后，我们发现LLMs能够生成准确的“一阶代码”（指令序列），但在生成“高阶代码”（如函数抽象或循环使用）方面存在问题。', 'title_zh': '面向协作机器人的零代码编程：基于大型代码模型的对话编程代码合成实验'}
