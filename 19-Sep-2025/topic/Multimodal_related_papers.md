# CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human 

**Title (ZH)**: CollabVLA: 自我反思的视觉-语言-行动模型与人类共同梦想 

**Authors**: Nan Sun, Yongchang Li, Chenxu Wang, Huiying Li, Huaping Liu  

**Link**: [PDF](https://arxiv.org/pdf/2509.14889)  

**Abstract**: In this work, we present CollabVLA, a self-reflective vision-language-action framework that transforms a standard visuomotor policy into a collaborative assistant. CollabVLA tackles key limitations of prior VLAs, including domain overfitting, non-interpretable reasoning, and the high latency of auxiliary generative models, by integrating VLM-based reflective reasoning with diffusion-based action generation under a mixture-of-experts design. Through a two-stage training recipe of action grounding and reflection tuning, it supports explicit self-reflection and proactively solicits human guidance when confronted with uncertainty or repeated failure. It cuts normalized Time by ~2x and Dream counts by ~4x vs. generative agents, achieving higher success rates, improved interpretability, and balanced low latency compared with existing methods. This work takes a pioneering step toward shifting VLAs from opaque controllers to genuinely assistive agents capable of reasoning, acting, and collaborating with humans. 

**Abstract (ZH)**: CollabVLA: 一种自省的视觉-语言-行动框架 

---
# Generalizable Geometric Image Caption Synthesis 

**Title (ZH)**: 可泛化的几何图像说明生成 

**Authors**: Yue Xin, Wenyuan Wang, Rui Pan, Ruida Wang, Howard Meng, Renjie Pi, Shizhe Diao, Tong Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2509.15217)  

**Abstract**: Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of $2.8\%\text{-}4.8\%$ in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with $2.4\%\text{-}3.9\%$ improvements in Art, Design, Tech, and Engineering tasks in MMMU. 

**Abstract (ZH)**: 多模态大语言模型在解决复杂几何问题方面具有多种实际应用，但仍然面临挑战。主要挑战来自于高质量图像-文本对数据集的缺乏，以及基于模板的数据合成管道难以泛化到预定义模板之外的问题。本文通过将可验证奖励强化学习（RLVR）引入数据生成管道，填补了这一空白。通过采用RLVR精炼从50个基本几何关系合成的几何图像的描述，并使用基于数学问题求解任务的奖励信号，我们的管道成功捕获了解决几何问题的关键特征，从而实现了更好的任务泛化和显著改进。此外，即使在分布外场景中，生成的数据集也能增强多模态大语言模型的一般推理能力，在MathVista和MathVerse的非几何输入图像的统计、算术、代数和数值任务中提高了2.8%至4.8%的准确率，在MMMU的艺术、设计、科技和工程任务中提高了2.4%至3.9%。 

---
# ProtoMedX: Towards Explainable Multi-Modal Prototype Learning for Bone Health Classification 

**Title (ZH)**: ProtoMedX: 向量化的多模态原型学习及其在骨健康分类中的可解释性研究 

**Authors**: Alvaro Lopez Pellicer, Andre Mariucci, Plamen Angelov, Marwan Bukhari, Jemma G. Kerns  

**Link**: [PDF](https://arxiv.org/pdf/2509.14830)  

**Abstract**: Bone health studies are crucial in medical practice for the early detection and treatment of Osteopenia and Osteoporosis. Clinicians usually make a diagnosis based on densitometry (DEXA scans) and patient history. The applications of AI in this field are ongoing research. Most successful methods rely on deep learning models that use vision alone (DEXA/X-ray imagery) and focus on prediction accuracy, while explainability is often disregarded and left to post hoc assessments of input contributions. We propose ProtoMedX, a multi-modal model that uses both DEXA scans of the lumbar spine and patient records. ProtoMedX's prototype-based architecture is explainable by design, which is crucial for medical applications, especially in the context of the upcoming EU AI Act, as it allows explicit analysis of model decisions, including incorrect ones. ProtoMedX demonstrates state-of-the-art performance in bone health classification while also providing explanations that can be visually understood by clinicians. Using a dataset of 4,160 real NHS patients, the proposed ProtoMedX achieves 87.58% accuracy in vision-only tasks and 89.8% in its multi-modal variant, both surpassing existing published methods. 

**Abstract (ZH)**: 骨健康研究对于早期检测和治疗骨质疏松和骨质减少在临床实践中至关重要。基于densitometry（DEXA扫描）和患者病史，临床医生通常进行诊断。该领域的AI应用正处于研究之中。大多数成功的方法依赖于基于图像的深度学习模型（如DEXA/X光影像），专注于预测准确性，而可解释性通常被忽略并留给事后的输入贡献评估。我们提出ProtoMedX，一个利用腰椎DEXA扫描和患者记录的多模态模型。ProtoMedX基于原型的设计使其具有可解释性，这对于医疗应用尤其重要，特别是在即将到来的欧盟AI法案的背景下，它允许对模型决策进行明确分析，包括错误的决策。ProtoMedX在骨健康分类上取得了目前最先进的性能，同时提供了可以由临床医生视觉理解的解释。使用包含4,160名真实NHS患者的 dataset，所提出的ProtoMedX在仅基于图像的任务中的准确性为87.58%，在多模态变体中的准确性为89.8%，均超过了现有发表的方法。 

---
# TableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding 

**Title (ZH)**: TableDART: 动态自适应多模态路由表理解 

**Authors**: Xiaobo Xing, Wei Yuan, Tong Chen, Quoc Viet Hung Nguyen, Xiangliang Zhang, Hongzhi Yin  

**Link**: [PDF](https://arxiv.org/pdf/2509.14671)  

**Abstract**: Modeling semantic and structural information from tabular data remains a core challenge for effective table understanding. Existing Table-as-Text approaches flatten tables for large language models (LLMs), but lose crucial structural cues, while Table-as-Image methods preserve structure yet struggle with fine-grained semantics. Recent Table-as-Multimodality strategies attempt to combine textual and visual views, but they (1) statically process both modalities for every query-table pair within a large multimodal LLMs (MLLMs), inevitably introducing redundancy and even conflicts, and (2) depend on costly fine-tuning of MLLMs. In light of this, we propose TableDART, a training-efficient framework that integrates multimodal views by reusing pretrained single-modality models. TableDART introduces a lightweight 2.59M-parameter MLP gating network that dynamically selects the optimal path (either Text-only, Image-only, or Fusion) for each table-query pair, effectively reducing redundancy and conflicts from both modalities. In addition, we propose a novel agent to mediate cross-modal knowledge integration by analyzing outputs from text- and image-based models, either selecting the best result or synthesizing a new answer through reasoning. This design avoids the prohibitive costs of full MLLM fine-tuning. Extensive experiments on seven benchmarks show that TableDART establishes new state-of-the-art performance among open-source models, surpassing the strongest baseline by an average of 4.02%. The code is available at: this https URL 

**Abstract (ZH)**: 从表格数据中建模语义和结构信息仍然是有效表理解的核心挑战。现有的Table-as-Text方法将表格扁平化供大规模语言模型使用，但会失去关键的结构线索，而Table-as-Image方法保留了结构但难以处理细粒度的语义。近期的Table-as-Multimodality策略试图结合文本文本和视觉视图，但它们（1）在大型多模态语言模型中静态处理每对查询-表格，不可避免地引入冗余甚至冲突，（2）依赖于大型多模态语言模型的昂贵微调。为了应对这一挑战，我们提出了一种训练高效框架TableDART，该框架通过重用预训练的单模态模型来整合多模态视图。TableDART引入了一种轻量级的2.59M参数MLP门控网络，动态选择每对表格-查询的最佳路径（仅文本、仅图像或融合），有效减少了来自两种模态的冗余和冲突。此外，我们提出了一种新型代理来调解跨模态知识整合，通过分析基于文本文本和图像模型的输出，选择最佳结果或通过推理综合新答案。这种设计避免了全面微调大型多模态语言模型的高昂成本。在七个基准上的广泛实验表明，TableDART在开源模型中建立了新的最佳性能，平均超越最强基线4.02%。代码可在以下链接获取：this https URL 

---
# Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech 

**Title (ZH)**: 面向人类似真的多模态对话代理的生成性说法研究 

**Authors**: Taesoo Kim, Yongsik Jo, Hyunmin Song, Taehwan Kim  

**Link**: [PDF](https://arxiv.org/pdf/2509.14627)  

**Abstract**: Human conversation involves language, speech, and visual cues, with each medium providing complementary information. For instance, speech conveys a vibe or tone not fully captured by text alone. While multimodal LLMs focus on generating text responses from diverse inputs, less attention has been paid to generating natural and engaging speech. We propose a human-like agent that generates speech responses based on conversation mood and responsive style information. To achieve this, we build a novel MultiSensory Conversation dataset focused on speech to enable agents to generate natural speech. We then propose a multimodal LLM-based model for generating text responses and voice descriptions, which are used to generate speech covering paralinguistic information. Experimental results demonstrate the effectiveness of utilizing both visual and audio modalities in conversation to generate engaging speech. The source code is available in this https URL 

**Abstract (ZH)**: 人类对话涉及语言、语音和视觉提示，每种媒介都提供了互补的信息。例如，语音传达了文字无法完全捕捉的情绪或语调。尽管多模态LLM专注于从各种输入中生成文本响应，但生成自然流畅的语音仍然较少受到关注。我们提出了一种基于对话情绪和响应风格信息生成语音响应的人类-like代理。为了实现这一目标，我们构建了一个聚焦于语音的新型多感知对话数据集，以使代理能够生成自然流畅的语音。然后，我们提出了一种基于多模态LLM的模型，用于生成文本响应和语音描述，这些描述用于生成涵盖副语言信息的语音。实验结果表明，在对话中同时利用视觉和音频模态生成引人入胜的语音的有效性。源代码可在以下链接获取。 

---
# AToken: A Unified Tokenizer for Vision 

**Title (ZH)**: 一个统一的视觉分词器：AToken 

**Authors**: Jiasen Lu, Liangchen Song, Mingze Xu, Byeongjoo Ahn, Yanjun Wang, Chen Chen, Afshin Dehghan, Yinfei Yang  

**Link**: [PDF](https://arxiv.org/pdf/2509.14476)  

**Abstract**: We present AToken, the first unified visual tokenizer that achieves both high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets. Unlike existing tokenizers that specialize in either reconstruction or understanding for single modalities, AToken encodes these diverse visual inputs into a shared 4D latent space, unifying both tasks and modalities in a single framework. Specifically, we introduce a pure transformer architecture with 4D rotary position embeddings to process visual inputs of arbitrary resolutions and temporal durations. To ensure stable training, we introduce an adversarial-free training objective that combines perceptual and Gram matrix losses, achieving state-of-the-art reconstruction quality. By employing a progressive training curriculum, AToken gradually expands from single images, videos, and 3D, and supports both continuous and discrete latent tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01 rFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9% classification accuracy for 3D. In downstream applications, AToken enables both visual generation tasks (e.g., image generation with continuous and discrete tokens, text-to-video generation, image-to-3D synthesis) and understanding tasks (e.g., multimodal LLMs), achieving competitive performance across all benchmarks. These results shed light on the next-generation multimodal AI systems built upon unified visual tokenization. 

**Abstract (ZH)**: AToken：统一视觉分词器，实现跨图像、视频和3D资产的高保真重构和语义理解 

---
