{'arxiv_id': 'arXiv:2509.14641', 'title': 'Efficient 3D Perception on Embedded Systems via Interpolation-Free Tri-Plane Lifting and Volume Fusion', 'authors': 'Sibaek Lee, Jiung Yeon, Hyeonwoo Yu', 'link': 'https://arxiv.org/abs/2509.14641', 'abstract': 'Dense 3D convolutions provide high accuracy for perception but are too computationally expensive for real-time robotic systems. Existing tri-plane methods rely on 2D image features with interpolation, point-wise queries, and implicit MLPs, which makes them computationally heavy and unsuitable for embedded 3D inference. As an alternative, we propose a novel interpolation-free tri-plane lifting and volumetric fusion framework, that directly projects 3D voxels into plane features and reconstructs a feature volume through broadcast and summation. This shifts nonlinearity to 2D convolutions, reducing complexity while remaining fully parallelizable. To capture global context, we add a low-resolution volumetric branch fused with the lifted features through a lightweight integration layer, yielding a design that is both efficient and end-to-end GPU-accelerated. To validate the effectiveness of the proposed method, we conduct experiments on classification, completion, segmentation, and detection, and we map the trade-off between efficiency and accuracy across tasks. Results show that classification and completion retain or improve accuracy, while segmentation and detection trade modest drops in accuracy for significant computational savings. On-device benchmarks on an NVIDIA Jetson Orin nano confirm robust real-time throughput, demonstrating the suitability of the approach for embedded robotic perception.', 'abstract_zh': '密集三维卷积在感知方面提供了高精度，但对实时机器人系统来说计算成本过高。现有基于三平面的方法依赖于插值、点查询和隐式MLP的二维图像特征，这使得它们计算量大不适用于嵌入式三维推理。为此，我们提出了一种新的无插值三平面提升和体素融合框架，直接将三维体素投影到平面特征，并通过广播和求和重构特征体素。这将非线性转移到二维卷积中，减少了复杂性同时保持完全并行化。为了捕获全局上下文，我们通过一个轻量级整合层将低分辨率体素分支与提升特征融合，从而获得一种既高效又端到端GPU加速的设计。为了验证所提出方法的有效性，我们在分类、完成、分割和检测任务上进行了实验，并映射了效率与准确性的权衡。结果显示，分类和完成保留或提高了准确性，而分割和检测则以适度降低准确性为代价获得显著的计算节省。基于NVIDIA Jetson Orin nano的设备基准测试证实了稳健的实时吞吐量，展示了该方法适用于嵌入式机器人感知的适用性。', 'title_zh': '无需插值的三平面提升与体素融合在嵌入式系统中的高效3D感知'}
{'arxiv_id': 'arXiv:2509.14636', 'title': 'BEV-ODOM2: Enhanced BEV-based Monocular Visual Odometry with PV-BEV Fusion and Dense Flow Supervision for Ground Robots', 'authors': 'Yufei Wei, Wangtao Lu, Sha Lu, Chenxiao Hu, Fuzhang Han, Rong Xiong, Yue Wang', 'link': 'https://arxiv.org/abs/2509.14636', 'abstract': "Bird's-Eye-View (BEV) representation offers a metric-scaled planar workspace, facilitating the simplification of 6-DoF ego-motion to a more robust 3-DoF model for monocular visual odometry (MVO) in intelligent transportation systems. However, existing BEV methods suffer from sparse supervision signals and information loss during perspective-to-BEV projection. We present BEV-ODOM2, an enhanced framework addressing both limitations without additional annotations. Our approach introduces: (1) dense BEV optical flow supervision constructed from 3-DoF pose ground truth for pixel-level guidance; (2) PV-BEV fusion that computes correlation volumes before projection to preserve 6-DoF motion cues while maintaining scale consistency. The framework employs three supervision levels derived solely from pose data: dense BEV flow, 5-DoF for the PV branch, and final 3-DoF output. Enhanced rotation sampling further balances diverse motion patterns in training. Extensive evaluation on KITTI, NCLT, Oxford, and our newly collected ZJH-VO multi-scale dataset demonstrates state-of-the-art performance, achieving 40 improvement in RTE compared to previous BEV methods. The ZJH-VO dataset, covering diverse ground vehicle scenarios from underground parking to outdoor plazas, is publicly available to facilitate future research.", 'abstract_zh': "BEV-ODOM2：一种增强的 bird's-eye-view Odometry 框架", 'title_zh': 'BEV-ODOM2：基于BEV的单目视觉里程计增强方法，结合PV-BEV融合和密集流监督，应用于地面机器人'}
{'arxiv_id': 'arXiv:2509.14510', 'title': 'Object Recognition and Force Estimation with the GelSight Baby Fin Ray', 'authors': 'Sandra Q. Liu, Yuxiang Ma, Edward H. Adelson', 'link': 'https://arxiv.org/abs/2509.14510', 'abstract': 'Recent advances in soft robotic hands and tactile sensing have enabled both to perform an increasing number of complex tasks with the aid of machine learning. In particular, we presented the GelSight Baby Fin Ray in our previous work, which integrates a camera with a soft, compliant Fin Ray structure. Camera-based tactile sensing gives the GelSight Baby Fin Ray the ability to capture rich contact information like forces, object geometries, and textures. Moreover, our previous work showed that the GelSight Baby Fin Ray can dig through clutter, and classify in-shell nuts. To further examine the potential of the GelSight Baby Fin Ray, we leverage learning to distinguish nut-in-shell textures and to perform force and position estimation. We implement ablation studies with popular neural network structures, including ResNet50, GoogLeNet, and 3- and 5-layer convolutional neural network (CNN) structures. We conclude that machine learning is a promising technique to extract useful information from high-resolution tactile images and empower soft robotics to better understand and interact with the environments.', 'abstract_zh': '近期软体手和触觉感知的进展使得两者在机器学习的辅助下能够执行越来越多的复杂任务。我们之前的工作介绍了GelSight Baby Fin Ray，该技术结合了摄像头与柔软顺应性的Fin Ray结构。基于摄像头的触觉感知使GelSight Baby Fin Ray能够捕捉丰富的接触信息，如力、物体几何形状和纹理。此外，我们之前的工作表明GelSight Baby Fin Ray能够穿透杂乱环境，并对带壳坚果进行分类。为进一步检验GelSight Baby Fin Ray的潜力，我们利用学习来区分带壳坚果的纹理，并进行力和位置估计。我们使用流行的神经网络结构，包括ResNet50、GoogLeNet以及3层和5层卷积神经网络（CNN）结构，实施了消融研究。我们得出结论，机器学习是提取高分辨率触觉图像中有用信息的一种有前途的技术，并促使软体机器人更好地理解和互动环境。', 'title_zh': '基于GelSight Baby Fin Ray的物体识别与力估计'}
{'arxiv_id': 'arXiv:2509.14460', 'title': 'Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring', 'authors': 'Abhiroop Ajith, Constantinos Chamzas', 'link': 'https://arxiv.org/abs/2509.14460', 'abstract': 'Learning abstractions directly from data is a core challenge in robotics. Humans naturally operate at an abstract level, reasoning over high-level subgoals while delegating execution to low-level motor skills -- an ability that enables efficient problem solving in complex environments. In robotics, abstractions and hierarchical reasoning have long been central to planning, yet they are typically hand-engineered, demanding significant human effort and limiting scalability. Automating the discovery of useful abstractions directly from visual data would make planning frameworks more scalable and more applicable to real-world robotic domains. In this work, we focus on rearrangement tasks where the state is represented with raw images, and propose a method to induce discrete, graph-structured abstractions by combining structural constraints with an attention-guided visual distance. Our approach leverages the inherent bipartite structure of rearrangement problems, integrating structural constraints and visual embeddings into a unified framework. This enables the autonomous discovery of abstractions from vision alone, which can subsequently support high-level planning. We evaluate our method on two rearrangement tasks in simulation and show that it consistently identifies meaningful abstractions that facilitate effective planning and outperform existing approaches.', 'abstract_zh': '直接从数据中学习抽象是机器人领域的一个核心挑战。在机器人领域，抽象和层次推理长期以来一直是规划的核心内容，但它们通常需要手工设计，消耗大量的手工努力并限制了可扩展性。直接从视觉数据中自动发现有用的抽象将使规划框架更具可扩展性，并更适用于真实的机器人领域。在本文中，我们关注状态用原始图像表示的重新排列任务，提出了一种方法，通过结合结构约束和注意力引导的视觉距离来诱导离散的、图结构化的抽象。我们的方法利用重新排列问题固有的二分结构，将结构约束和视觉嵌入整合到一个统一的框架中。这使得仅从视觉数据中自主发现抽象成为可能，并可支持高层次的规划。我们在模拟中对两种重新排列任务进行了评估，并表明这种方法能够一致地识别出有意义的抽象，促进有效的规划，并优于现有方法。', 'title_zh': '使用视觉引导图着色学习离散抽象以完成视觉重组任务'}
{'arxiv_id': 'arXiv:2509.14303', 'title': 'FlowDrive: Energy Flow Field for End-to-End Autonomous Driving', 'authors': 'Hao Jiang, Zhipeng Zhang, Yu Gao, Zhigang Sun, Yiru Wang, Yuwen Heng, Shuo Wang, Jinhao Chai, Zhuo Chen, Hao Zhao, Hao Sun, Xi Zhang, Anqing Jiang, Chuan Hu', 'link': 'https://arxiv.org/abs/2509.14303', 'abstract': 'Recent advances in end-to-end autonomous driving leverage multi-view images to construct BEV representations for motion planning. In motion planning, autonomous vehicles need considering both hard constraints imposed by geometrically occupied obstacles (e.g., vehicles, pedestrians) and soft, rule-based semantics with no explicit geometry (e.g., lane boundaries, traffic priors). However, existing end-to-end frameworks typically rely on BEV features learned in an implicit manner, lacking explicit modeling of risk and guidance priors for safe and interpretable planning. To address this, we propose FlowDrive, a novel framework that introduces physically interpretable energy-based flow fields-including risk potential and lane attraction fields-to encode semantic priors and safety cues into the BEV space. These flow-aware features enable adaptive refinement of anchor trajectories and serve as interpretable guidance for trajectory generation. Moreover, FlowDrive decouples motion intent prediction from trajectory denoising via a conditional diffusion planner with feature-level gating, alleviating task interference and enhancing multimodal diversity. Experiments on the NAVSIM v2 benchmark demonstrate that FlowDrive achieves state-of-the-art performance with an EPDMS of 86.3, surpassing prior baselines in both safety and planning quality. The project is available at this https URL.', 'abstract_zh': '近期端到端自主驾驶的进展通过多视图图像构建BEV表示以进行运动规划', 'title_zh': 'FlowDrive：端到端自主驾驶的能量流场'}
{'arxiv_id': 'arXiv:2509.15219', 'title': 'Out-of-Sight Trajectories: Tracking, Fusion, and Prediction', 'authors': 'Haichao Zhang, Yi Xu, Yun Fu', 'link': 'https://arxiv.org/abs/2509.15219', 'abstract': 'Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at this http URL', 'abstract_zh': '视觉不可见轨迹预测：一种利用噪声音频数据预测视觉不可见对象的噪声自由视觉轨迹的新任务', 'title_zh': '隐藏轨迹：跟踪、融合与预测'}
{'arxiv_id': 'arXiv:2509.14966', 'title': 'RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching', 'authors': 'Xingwu Zhang, Guanxuan Li, Zhuocheng Zhang, Zijun Long', 'link': 'https://arxiv.org/abs/2509.14966', 'abstract': 'The rapidly growing number of product categories in large-scale e-commerce makes accurate object identification for automated packing in warehouses substantially more difficult. As the catalog grows, intra-class variability and a long tail of rare or visually similar items increase, and when combined with diverse packaging, cluttered containers, frequent occlusion, and large viewpoint changes-these factors amplify discrepancies between query and reference images, causing sharp performance drops for methods that rely solely on 2D appearance features. Thus, we propose RoboEye, a two-stage identification framework that dynamically augments 2D semantic features with domain-adapted 3D reasoning and lightweight adapters to bridge training deployment gaps. In the first stage, we train a large vision model to extract 2D features for generating candidate rankings. A lightweight 3D-feature-awareness module then estimates 3D feature quality and predicts whether 3D re-ranking is necessary, preventing performance degradation and avoiding unnecessary computation. When invoked, the second stage uses our robot 3D retrieval transformer, comprising a 3D feature extractor that produces geometry-aware dense features and a keypoint-based matcher that computes keypoint-correspondence confidences between query and reference images instead of conventional cosine-similarity scoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior state of the art (RoboLLM). Moreover, RoboEye operates using only RGB images, avoiding reliance on explicit 3D inputs and reducing deployment costs. The code used in this paper is publicly available at: this https URL.', 'abstract_zh': '大规模电子商务中迅速增长的产品类别使得仓库自动包装中的准确物体识别变得更加困难。随着商品目录的增长，类内变异性以及稀有或外观相似项目的长尾效应增加，再加上多样化的包装、拥挤的容器、频繁的遮挡和大幅的视角变化，这些因素加剧了查询图像和参考图像之间的差异，导致依赖于二维外观特征的方法性能急剧下降。因此，我们提出了RoboEye，这是一种两阶段识别框架，能够动态增强二维语义特征，结合领域适应的三维推理和轻量级适配器以弥合训练与部署之间的差距。在第一阶段，我们训练一个大型视觉模型以提取二维特征用于生成候选-ranking。随后的轻量级三维特征意识模块估计三维特征质量并预测是否需要进行三维重新排名，从而防止性能下降并避免不必要的计算。在被触发时，第二阶段使用我们的机器人三维检索变换器，该变换器包括一个三维特征提取器用于生成几何感知密集特征，以及一个基于关键点的匹配器用于在查询图像和参考图像之间计算关键点对应置信度，而不是使用传统的余弦相似度评分。实验表明，RoboEye 在召回率（@1）上相比于之前的最先进的技术（RoboLLM）提高了 7.1%。此外，RoboEye 仅使用 RGB 图像运行，避免了对外部显式三维输入的依赖，从而降低了部署成本。本文使用的代码可以在：this https URL 获取。', 'title_zh': 'RoboEye: 通过选择性3D几何关键点匹配增强2D机器人物体识别'}
{'arxiv_id': 'arXiv:2509.15167', 'title': 'Semi-Supervised 3D Medical Segmentation from 2D Natural Images Pretrained Model', 'authors': 'Pak-Hei Yeung, Jayroop Ramesh, Pengfei Lyu, Ana Namburete, Jagath Rajapakse', 'link': 'https://arxiv.org/abs/2509.15167', 'abstract': "This paper explores the transfer of knowledge from general vision models pretrained on 2D natural images to improve 3D medical image segmentation. We focus on the semi-supervised setting, where only a few labeled 3D medical images are available, along with a large set of unlabeled images. To tackle this, we propose a model-agnostic framework that progressively distills knowledge from a 2D pretrained model to a 3D segmentation model trained from scratch. Our approach, M&N, involves iterative co-training of the two models using pseudo-masks generated by each other, along with our proposed learning rate guided sampling that adaptively adjusts the proportion of labeled and unlabeled data in each training batch to align with the models' prediction accuracy and stability, minimizing the adverse effect caused by inaccurate pseudo-masks. Extensive experiments on multiple publicly available datasets demonstrate that M&N achieves state-of-the-art performance, outperforming thirteen existing semi-supervised segmentation approaches under all different settings. Importantly, ablation studies show that M&N remains model-agnostic, allowing seamless integration with different architectures. This ensures its adaptability as more advanced models emerge. The code is available at this https URL.", 'abstract_zh': '本文探讨了从预训练于2D自然图像的一般视觉模型向3D医学图像分割模型的知识迁移，以改善3D医学图像分割。我们关注半监督设置，在这种情况下，只有少量标记的3D医学图像，以及大量的未标记图像。为此，我们提出了一种模型无关的框架，该框架逐步从2D预训练模型向零初始化训练的3D分割模型传递知识。我们的方法M&N涉及通过彼此生成的伪标签对两个模型进行迭代联合训练，并采用我们提出的由学习率引导的采样策略，该策略在每个训练批次中自适应调整标记和未标记数据的比例，以与模型的预测准确性和稳定性相匹配，从而最小化由不准确伪标签引起的不良影响。在多个公开可用的数据集上的广泛实验表明，M&N实现了最先进的性能，在所有不同的设置下均优于十三种现有半监督分割方法。重要的是，消融研究显示M&N保持了模型无关性，允许无缝集成到不同的架构中。这确保了其在更先进的模型出现时的适应性。相关代码可在以下链接获取。', 'title_zh': '半监督的3D医学分割：从2D自然图像预训练模型'}
{'arxiv_id': 'arXiv:2509.15156', 'title': 'Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models', 'authors': 'Haobo Yang, Minghao Guo, Dequan Yang, Wenyu Wang', 'link': 'https://arxiv.org/abs/2509.15156', 'abstract': 'Contemporary deep learning models have achieved impressive performance in image classification by primarily leveraging statistical regularities within large datasets, but they rarely incorporate structured insights drawn directly from perceptual psychology. To explore the potential of perceptually motivated inductive biases, we propose integrating classic geometric visual illusions well-studied phenomena from human perception into standard image-classification training pipelines. Specifically, we introduce a synthetic, parametric geometric-illusion dataset and evaluate three multi-source learning strategies that combine illusion recognition tasks with ImageNet classification objectives. Our experiments reveal two key conceptual insights: (i) incorporating geometric illusions as auxiliary supervision systematically improves generalization, especially in visually challenging cases involving intricate contours and fine textures; and (ii) perceptually driven inductive biases, even when derived from synthetic stimuli traditionally considered unrelated to natural image recognition, can enhance the structural sensitivity of both CNN and transformer-based architectures. These results demonstrate a novel integration of perceptual science and machine learning and suggest new directions for embedding perceptual priors into vision model design.', 'abstract_zh': '基于感知动机的归纳偏置：几何错觉在图像分类中的集成与评估', 'title_zh': '利用几何视错觉作为视觉模型的知觉归纳偏置'}
{'arxiv_id': 'arXiv:2509.15130', 'title': 'WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance', 'authors': 'Chenxi Song, Yanming Yang, Tong Zhao, Ruibo Li, Chi Zhang', 'link': 'https://arxiv.org/abs/2509.15130', 'abstract': "Recent video diffusion models demonstrate strong potential in spatial intelligence tasks due to their rich latent world priors. However, this potential is hindered by their limited controllability and geometric inconsistency, creating a gap between their strong priors and their practical use in 3D/4D tasks. As a result, current approaches often rely on retraining or fine-tuning, which risks degrading pretrained knowledge and incurs high computational costs. To address this, we propose WorldForge, a training-free, inference-time framework composed of three tightly coupled modules. Intra-Step Recursive Refinement introduces a recursive refinement mechanism during inference, which repeatedly optimizes network predictions within each denoising step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Extensive experiments across diverse benchmarks validate our method's superiority in realism, trajectory consistency, and visual fidelity. This work introduces a novel plug-and-play paradigm for controllable video synthesis, offering a new perspective on leveraging generative priors for spatial intelligence.", 'abstract_zh': 'Recent视频扩散模型在空间智能任务中展示出了强大的潜力，这得益于它们丰富的潜在先验世界知识。然而，这一潜力受到了控制能力有限和几何不一致性的限制，导致它们在3D/4D任务中的实用价值大打折扣。当前的方法通常依赖于重新训练或微调，这可能会损害预训练知识，并带来高昂的计算成本。为了解决这一问题，我们提出WorldForge，一个无需训练、在推理时使用的框架，由三个紧密耦合的模块组成。轮内递归精化引入了一种在推理过程中重复优化网络预测的递归精化机制，以实现精确的轨迹注入。流门控潜在融合利用光流相似性在潜在空间中解耦运动与外观，并选择性地将轨迹引导注入与运动相关的通道。双路径自我校正引导将引导和非引导去噪路径进行比较，以适应性地纠正由噪声或对齐错误的结构信号引起的轨迹漂移。这些组件在无需训练的情况下注入细微的、与轨迹对齐的引导，既实现了精确的运动控制，又生成了照片级真实的视觉内容。广泛的信任度在多个基准上验证了我们方法在现实性、轨迹一致性以及视觉保真度上的优越性。这项工作引入了一种新的即插即用范式，用于可控视频合成，提供了一种利用生成先验进行空间智能的新视角。', 'title_zh': 'WorldForge: 通过无训练指导解锁视频扩散模型中的 emergent 3D/4D 生成'}
{'arxiv_id': 'arXiv:2509.15058', 'title': 'Communication Efficient Split Learning of ViTs with Attention-based Double Compression', 'authors': 'Federico Alvetreti, Jary Pomponi, Paolo Di Lorenzo, Simone Scardapane', 'link': 'https://arxiv.org/abs/2509.15058', 'abstract': "This paper proposes a novel communication-efficient Split Learning (SL) framework, named Attention-based Double Compression (ADC), which reduces the communication overhead required for transmitting intermediate Vision Transformers activations during the SL training process. ADC incorporates two parallel compression strategies. The first one merges samples' activations that are similar, based on the average attention score calculated in the last client layer; this strategy is class-agnostic, meaning that it can also merge samples having different classes, without losing generalization ability nor decreasing final results. The second strategy follows the first and discards the least meaningful tokens, further reducing the communication cost. Combining these strategies not only allows for sending less during the forward pass, but also the gradients are naturally compressed, allowing the whole model to be trained without additional tuning or approximations of the gradients. Simulation results demonstrate that Attention-based Double Compression outperforms state-of-the-art SL frameworks by significantly reducing communication overheads while maintaining high accuracy.", 'abstract_zh': '基于注意力的双压缩剪枝学习框架（ADC）：一种通信高效的学习方法', 'title_zh': '基于注意力双压缩的ViTs通信高效分割学习'}
{'arxiv_id': 'arXiv:2509.15011', 'title': 'Sea-ing Through Scattered Rays: Revisiting the Image Formation Model for Realistic Underwater Image Generation', 'authors': 'Vasiliki Ismiroglou, Malte Pedersen, Stefan H. Bengtson, Andreas Aakerberg, Thomas B. Moeslund', 'link': 'https://arxiv.org/abs/2509.15011', 'abstract': "In recent years, the underwater image formation model has found extensive use in the generation of synthetic underwater data. Although many approaches focus on scenes primarily affected by discoloration, they often overlook the model's ability to capture the complex, distance-dependent visibility loss present in highly turbid environments. In this work, we propose an improved synthetic data generation pipeline that includes the commonly omitted forward scattering term, while also considering a nonuniform medium. Additionally, we collected the BUCKET dataset under controlled turbidity conditions to acquire real turbid footage with the corresponding reference images. Our results demonstrate qualitative improvements over the reference model, particularly under increasing turbidity, with a selection rate of 82. 5\\% by survey participants. Data and code can be accessed on the project page: this http URL.", 'abstract_zh': '近年来，水下图像形成模型在生成合成水下数据方面得到了广泛应用。尽管许多方法主要关注受颜色失真影响较大的场景，但它们往往忽视了该模型在捕捉高浑浊环境中距离相关的可见度损失方面的能力。在本文中，我们提出了一种改进的合成数据生成管道，包括通常被忽视的前向散射项，并考虑了非均匀介质。此外，我们在受控浑浊度条件下收集了BUCKET数据集，以获取相应的参考图像和实际浑浊视频。我们的结果在浑浊度增加时在定性上优于参考模型，并且受试者选择率为82.5%。数据和代码可在项目页面访问：this http URL。', 'title_zh': '透过散射光看海底：重新审视 realistic 水下图像生成的成像模型'}
{'arxiv_id': 'arXiv:2509.14912', 'title': 'Back to Ear: Perceptually Driven High Fidelity Music Reconstruction', 'authors': 'Kangdi Wang, Zhiyue Wu, Dinghao Zhou, Rui Lin, Junyu Dai, Tao Jiang', 'link': 'https://arxiv.org/abs/2509.14912', 'abstract': 'Variational Autoencoders (VAEs) are essential for large-scale audio tasks like diffusion-based generation. However, existing open-source models often neglect auditory perceptual aspects during training, leading to weaknesses in phase accuracy and stereophonic spatial representation. To address these challenges, we propose {\\epsilon}ar-VAE, an open-source music signal reconstruction model that rethinks and optimizes the VAE training paradigm. Our contributions are threefold: (i) A K-weighting perceptual filter applied prior to loss calculation to align the objective with auditory perception. (ii) Two novel phase losses: a Correlation Loss for stereo coherence, and a Phase Loss using its derivatives--Instantaneous Frequency and Group Delay--for precision. (iii) A new spectral supervision paradigm where magnitude is supervised by all four Mid/Side/Left/Right components, while phase is supervised only by the LR components. Experiments show {\\epsilon}ar-VAE at 44.1kHz substantially outperforms leading open-source models across diverse metrics, showing particular strength in reconstructing high-frequency harmonics and the spatial characteristics.', 'abstract_zh': 'εar-VAE：一种用于音乐信号重构的感知优化变分自编码器', 'title_zh': '回归耳朵：基于感知的高保真音乐重建'}
{'arxiv_id': 'arXiv:2509.14846', 'title': '[Re] Improving Interpretation Faithfulness for Vision Transformers', 'authors': 'Izabela Kurek, Wojciech Trejter, Stipe Frkovic, Andro Erdelez', 'link': 'https://arxiv.org/abs/2509.14846', 'abstract': "This work aims to reproduce the results of Faithful Vision Transformers (FViTs) proposed by arXiv:2311.17983 alongside interpretability methods for Vision Transformers from arXiv:2012.09838 and Xu (2022) et al. We investigate claims made by arXiv:2311.17983, namely that the usage of Diffusion Denoised Smoothing (DDS) improves interpretability robustness to (1) attacks in a segmentation task and (2) perturbation and attacks in a classification task. We also extend the original study by investigating the authors' claims that adding DDS to any interpretability method can improve its robustness under attack. This is tested on baseline methods and the recently proposed Attribution Rollout method. In addition, we measure the computational costs and environmental impact of obtaining an FViT through DDS. Our results broadly agree with the original study's findings, although minor discrepancies were found and discussed.", 'abstract_zh': '本研究旨在重现arXiv:2311.17983提出的忠实视觉变换器（FViTs）的结果，并结合arXiv:2012.09838和Xu (2022)等的研究中提出的解释性方法。我们调查了arXiv:2311.17983中的主张，即使用去噪平滑（DDS）可以提高解释性在（1）分割任务中的攻击鲁棒性和（2）分类任务中的扰动和攻击鲁棒性。我们还扩展了原始研究，调查作者提出的将DDS添加到任何解释性方法中可以提升其在攻击下的鲁棒性的主张。这一研究在基准方法和最近提出的归因展开方法上进行了测试。此外，我们还测量了通过DDS获得FViT的计算成本和环境影响。我们的结果总体上与原始研究的发现一致，尽管存在一些细微的差异并进行了讨论。', 'title_zh': '[重] 提升视觉变换器的解释忠实度'}
{'arxiv_id': 'arXiv:2509.14841', 'title': 'Not All Degradations Are Equal: A Targeted Feature Denoising Framework for Generalizable Image Super-Resolution', 'authors': 'Hongjun Wang, Jiyuan Chen, Zhengwei Yin, Xuan Song, Yinqiang Zheng', 'link': 'https://arxiv.org/abs/2509.14841', 'abstract': "Generalizable Image Super-Resolution aims to enhance model generalization capabilities under unknown degradations. To achieve this goal, the models are expected to focus only on image content-related features instead of overfitting degradations. Recently, numerous approaches such as Dropout and Feature Alignment have been proposed to suppress models' natural tendency to overfit degradations and yield promising results. Nevertheless, these works have assumed that models overfit to all degradation types (e.g., blur, noise, JPEG), while through careful investigations in this paper, we discover that models predominantly overfit to noise, largely attributable to its distinct degradation pattern compared to other degradation types. In this paper, we propose a targeted feature denoising framework, comprising noise detection and denoising modules. Our approach presents a general solution that can be seamlessly integrated with existing super-resolution models without requiring architectural modifications. Our framework demonstrates superior performance compared to previous regularization-based methods across five traditional benchmarks and datasets, encompassing both synthetic and real-world scenarios.", 'abstract_zh': '可泛化的图像超分辨率旨在提升模型在未知退化情况下的泛化能力。为了实现这一目标，模型应仅关注图像内容相关的特征而非过度拟合退化特性。近年来，诸如Dropout和特征对齐等众多方法被提出以抑制模型过度拟合退化的自然倾向，并取得了显著成果。然而，这些方法假定模型对所有退化类型（如模糊、噪声、JPEG压缩）都存在过度拟合现象，而本文通过仔细研究发现，模型主要对噪声过度拟合，这主要归因于其与其它退化类型不同的退化模式。本文提出了一种针对性的特征去噪框架，包括噪声检测和去噪模块。我们提出的方法提供了一种通用的解决方案，可以无缝集成到现有的超分辨率模型中，无需修改架构。实验结果表明，该框架在五种传统基准和数据集上的性能优于之前的正则化方法，涵盖了合成和真实场景。', 'title_zh': '不是所有的退化都平等：一种针对图像超分辨率的一致性特征去噪框架'}
{'arxiv_id': 'arXiv:2509.14827', 'title': 'Template-Based Cortical Surface Reconstruction with Minimal Energy Deformation', 'authors': 'Patrick Madlindl, Fabian Bongratz, Christian Wachinger', 'link': 'https://arxiv.org/abs/2509.14827', 'abstract': 'Cortical surface reconstruction (CSR) from magnetic resonance imaging (MRI) is fundamental to neuroimage analysis, enabling morphological studies of the cerebral cortex and functional brain mapping. Recent advances in learning-based CSR have dramatically accelerated processing, allowing for reconstructions through the deformation of anatomical templates within seconds. However, ensuring the learned deformations are optimal in terms of deformation energy and consistent across training runs remains a particular challenge. In this work, we design a Minimal Energy Deformation (MED) loss, acting as a regularizer on the deformation trajectories and complementing the widely used Chamfer distance in CSR. We incorporate it into the recent V2C-Flow model and demonstrate considerable improvements in previously neglected training consistency and reproducibility without harming reconstruction accuracy and topological correctness.', 'abstract_zh': '从磁共振成像（MRI）构建皮层表面的最小能量变形损失（MED）正则化在神经影像分析中的应用', 'title_zh': '基于模板的脑皮层表面重构与最小能量变形'}
{'arxiv_id': 'arXiv:2509.14574', 'title': 'Do Vision-Language Models See Urban Scenes as People Do? An Urban Perception Benchmark', 'authors': 'Rashid Mushkani', 'link': 'https://arxiv.org/abs/2509.14574', 'abstract': "Understanding how people read city scenes can inform design and planning. We introduce a small benchmark for testing vision-language models (VLMs) on urban perception using 100 Montreal street images, evenly split between photographs and photorealistic synthetic scenes. Twelve participants from seven community groups supplied 230 annotation forms across 30 dimensions mixing physical attributes and subjective impressions. French responses were normalized to English. We evaluated seven VLMs in a zero-shot setup with a structured prompt and deterministic parser. We use accuracy for single-choice items and Jaccard overlap for multi-label items; human agreement uses Krippendorff's alpha and pairwise Jaccard. Results suggest stronger model alignment on visible, objective properties than subjective appraisals. The top system (claude-sonnet) reaches macro 0.31 and mean Jaccard 0.48 on multi-label items. Higher human agreement coincides with better model scores. Synthetic images slightly lower scores. We release the benchmark, prompts, and harness for reproducible, uncertainty-aware evaluation in participatory urban analysis.", 'abstract_zh': "了解人们如何阅读城市场景可以指导设计和规划。我们介绍了一个基于100张蒙特利尔街道图像的小型基准，这些图像均匀分为摄影作品和照片级真实合成场景，用于测试视觉-语言模型（VLMs）的城市感知能力。来自七个社区群体的十二名参与者提供了涵盖30个维度的230份注释表单，这些维度结合了物理属性和主观印象。法语回应被归一化为英语。我们在零样本设置下评估了七种VLMs，使用结构化提示和确定性解析器。我们使用准确率评估单选项目，使用Jaccard重叠评估多标签项目；人类一致性使用Krippendorff's alpha和成对Jaccard。结果表明，模型在可见的客观属性上表现出更强的对齐，而在主观评估上则不然。顶级系统（claude-sonnet）在多标签项目的宏观分数达到0.31，平均Jaccard重叠达到0.48。更高的手动一致性与更好的模型得分相关。合成图像得分略低。我们发布了基准、提示和框架，以便在参与式城市分析中进行可重复、意识不确定性的评估。", 'title_zh': '视觉-语言模型如何感知城市场景？一项城市感知基准测试'}
{'arxiv_id': 'arXiv:2509.14571', 'title': 'VisMoDAl: Visual Analytics for Evaluating and Improving Corruption Robustness of Vision-Language Models', 'authors': 'Huanchen Wang, Wencheng Zhang, Zhiqiang Wang, Zhicong Lu, Yuxin Ma', 'link': 'https://arxiv.org/abs/2509.14571', 'abstract': 'Vision-language (VL) models have shown transformative potential across various critical domains due to their capability to comprehend multi-modal information. However, their performance frequently degrades under distribution shifts, making it crucial to assess and improve robustness against real-world data corruption encountered in practical applications. While advancements in VL benchmark datasets and data augmentation (DA) have contributed to robustness evaluation and improvement, there remain challenges due to a lack of in-depth comprehension of model behavior as well as the need for expertise and iterative efforts to explore data patterns. Given the achievement of visualization in explaining complex models and exploring large-scale data, understanding the impact of various data corruption on VL models aligns naturally with a visual analytics approach. To address these challenges, we introduce VisMoDAl, a visual analytics framework designed to evaluate VL model robustness against various corruption types and identify underperformed samples to guide the development of effective DA strategies. Grounded in the literature review and expert discussions, VisMoDAl supports multi-level analysis, ranging from examining performance under specific corruptions to task-driven inspection of model behavior and corresponding data slice. Unlike conventional works, VisMoDAl enables users to reason about the effects of corruption on VL models, facilitating both model behavior understanding and DA strategy formulation. The utility of our system is demonstrated through case studies and quantitative evaluations focused on corruption robustness in the image captioning task.', 'abstract_zh': 'Vision-language (VL)模型在多种关键领域展现了 transformational 的潜力，得益于其多模态信息的理解能力。然而，其性能在数据分布变化下会下降，因此评估和提高其对实际应用中数据污染的鲁棒性变得至关重要。尽管已经取得了视觉语言基准数据集和数据增强（DA）的进步，但在模型行为深入理解不足以及需要专业知识和迭代研究以探索数据模式的情况下，仍然存在挑战。鉴于可视化在解释复杂模型和探索大规模数据方面的成就，理解各种数据污染对Vision-language（VL）模型的影响自然地与视觉分析方法相契合。为了应对这些挑战，我们提出了一种名为VisMoDAl的视觉分析框架，旨在评估VL模型在不同污染类型下的鲁棒性，并识别表现不佳的样本以指导有效的数据增强策略的发展。VisMoDAl基于文献综述和专家讨论，支持多级分析，从特定污染下的性能检查到任务驱动的模型行为和相应数据切片的检查。与传统的研究工作不同，VisMoDAl使用户能够分析污染对Vision-language模型的影响，从而促进模型行为的理解和数据增强策略的制定。通过图像字幕任务中抗污染鲁棒性的案例研究和定量评估，证明了我们系统的实用性。', 'title_zh': 'VisMoDAl: 视觉分析以评估和提升视觉语言模型的腐败鲁棒性'}
