{'arxiv_id': 'arXiv:2503.13080', 'title': 'Vision-based automatic fruit counting with UAV', 'authors': 'Hubert Szolc, Mateusz Wasala, Remigiusz Mietla, Kacper Iwicki, Tomasz Kryjak', 'link': 'https://arxiv.org/abs/2503.13080', 'abstract': 'The use of unmanned aerial vehicles (UAVs) for smart agriculture is becoming increasingly popular. This is evidenced by recent scientific works, as well as the various competitions organised on this topic. Therefore, in this work we present a system for automatic fruit counting using UAVs. To detect them, our solution uses a vision algorithm that processes streams from an RGB camera and a depth sensor using classical image operations. Our system also allows the planning and execution of flight trajectories, taking into account the minimisation of flight time and distance covered. We tested the proposed solution in simulation and obtained an average score of 87.27/100 points from a total of 500 missions. We also submitted it to the UAV Competition organised as part of the ICUAS 2024 conference, where we achieved an average score of 84.83/100 points, placing 6th in a field of 23 teams and advancing to the finals.', 'abstract_zh': '无人机在智能农业中的果实自动计数系统研究', 'title_zh': '基于视觉的无人机自动水果计数'}
{'arxiv_id': 'arXiv:2503.12768', 'title': 'Dynamic-Dark SLAM: RGB-Thermal Cooperative Robot Vision Strategy for Multi-Person Tracking in Both Well-Lit and Low-Light Scenes', 'authors': 'Tatsuro Sakai, Kanji Tanaka, Jonathan Tay Yu Liang, Muhammad Adil Luqman, Daiki Iwata', 'link': 'https://arxiv.org/abs/2503.12768', 'abstract': 'In robot vision, thermal cameras have significant potential for recognizing humans even in complete darkness. However, their application to multi-person tracking (MPT) has lagged due to data scarcity and difficulties in individual identification. In this study, we propose a cooperative MPT system that utilizes co-located RGB and thermal cameras, using pseudo-annotations (bounding boxes + person IDs) to train RGB and T trackers. Evaluation experiments demonstrate that the T tracker achieves remarkable performance in both bright and dark scenes. Furthermore, results suggest that a tracker-switching approach using a binary brightness classifier is more suitable than a tracker-fusion approach for information integration. This study marks a crucial first step toward ``Dynamic-Dark SLAM," enabling effective recognition, understanding, and reconstruction of individuals, occluding objects, and traversable areas in dynamic environments, both bright and dark.', 'abstract_zh': '在机器人视觉中，热成像摄像头在完全黑暗环境中识别人类具有巨大的潜力。然而，其在多人跟踪（MPT）中的应用因数据稀缺和个人识别困难而滞后。本研究提出了一种使用共驻RGB和热成像摄像头的合作MPT系统，通过伪标注（边界框+人员ID）训练RGB和T跟踪器。评估实验表明，T跟踪器在明亮和黑暗场景中均表现出色。此外，结果表明，使用二元亮度分类器的跟踪器切换方法比跟踪器融合方法更适合信息集成。本研究标志着朝着“动态黑暗SLAM”的重要第一步，使得在明暗交替的动态环境中有效识别、理解和重建个体、遮挡物和可通行区域成为可能。', 'title_zh': '动态暗视觉SLAM：在明暗场景下多目标跟踪的RGB-热成像协同机器人视觉策略'}
{'arxiv_id': 'arXiv:2503.12660', 'title': 'KISS-SLAM: A Simple, Robust, and Accurate 3D LiDAR SLAM System With Enhanced Generalization Capabilities', 'authors': 'Tiziano Guadagnino, Benedikt Mersch, Saurabh Gupta, Ignacio Vizzo, Giorgio Grisetti, Cyrill Stachniss', 'link': 'https://arxiv.org/abs/2503.12660', 'abstract': 'Robust and accurate localization and mapping of an environment using laser scanners, so-called LiDAR SLAM, is essential to many robotic applications. Early 3D LiDAR SLAM methods often exploited additional information from IMU or GNSS sensors to enhance localization accuracy and mitigate drift. Later, advanced systems further improved the estimation at the cost of a higher runtime and complexity. This paper explores the limits of what can be achieved with a LiDAR-only SLAM approach while following the "Keep It Small and Simple" (KISS) principle. By leveraging this minimalistic design principle, our system, KISS-SLAM, archives state-of-the-art performances in pose accuracy while requiring little to no parameter tuning for deployment across diverse environments, sensors, and motion profiles. We follow best practices in graph-based SLAM and build upon LiDAR odometry to compute the relative motion between scans and construct local maps of the environment. To correct drift, we match local maps and optimize the trajectory in a pose graph optimization step. The experimental results demonstrate that this design achieves competitive performance while reducing complexity and reliance on additional sensor modalities. By prioritizing simplicity, this work provides a new strong baseline for LiDAR-only SLAM and a high-performing starting point for future research. Further, our pipeline builds consistent maps that can be used directly for further downstream tasks like navigation. Our open-source system operates faster than the sensor frame rate in all presented datasets and is designed for real-world scenarios.', 'abstract_zh': '仅使用激光雷达进行鲁棒且精确的环境定位与建图：遵循“保持简单和简约”原则的LiDAR-only SLAM方法探索', 'title_zh': 'KISS-SLAM：一种具有增强泛化能力的简单、稳健且精确的3D LiDAR SLAM系统'}
{'arxiv_id': 'arXiv:2503.12549', 'title': 'Grasping Partially Occluded Objects Using Autoencoder-Based Point Cloud Inpainting', 'authors': 'Alexander Koebler, Ralf Gross, Florian Buettner, Ingo Thon', 'link': 'https://arxiv.org/abs/2503.12549', 'abstract': "Flexible industrial production systems will play a central role in the future of manufacturing due to higher product individualization and customization. A key component in such systems is the robotic grasping of known or unknown objects in random positions. Real-world applications often come with challenges that might not be considered in grasping solutions tested in simulation or lab settings. Partial occlusion of the target object is the most prominent. Examples of occlusion can be supporting structures in the camera's field of view, sensor imprecision, or parts occluding each other due to the production process. In all these cases, the resulting lack of information leads to shortcomings in calculating grasping points. In this paper, we present an algorithm to reconstruct the missing information. Our inpainting solution facilitates the real-world utilization of robust object matching approaches for grasping point calculation. We demonstrate the benefit of our solution by enabling an existing grasping system embedded in a real-world industrial application to handle occlusions in the input. With our solution, we drastically decrease the number of objects discarded by the process.", 'abstract_zh': '柔性的工业生产系统将在未来的制造业中发挥核心作用，由于产品个性化和定制化程度提高。此类系统的关键组件是在随机位置抓取已知或未知物体的机器人抓取技术。真实世界的应用常常伴随着在模拟或实验室环境中测试的抓取解决方案中未考虑的挑战。目标物体的部分遮挡是最突出的问题。遮挡可以是由相机视野中的支持结构、传感器精度不足或生产过程中相互遮挡的部件引起。在所有这些情况下，由于缺乏信息而导致的不足会影响抓取点的计算。本文提出了一种算法来恢复缺失的信息。我们的 inpainting 解决方案促进了鲁棒对象匹配方法在抓取点计算中在真实世界中的应用。我们通过使嵌入真实工业应用的现有抓取系统能够处理输入中的遮挡，展示了我们解决方案的优势。通过我们的解决方案，我们大大减少了被过程抛弃的物体的数量。', 'title_zh': '基于自编码器的点云修复用于遮挡物体的抓取'}
{'arxiv_id': 'arXiv:2503.12170', 'title': 'DiffAD: A Unified Diffusion Modeling Approach for Autonomous Driving', 'authors': 'Tao Wang, Cong Zhang, Xingguang Qu, Kun Li, Weiwei Liu, Chang Huang', 'link': 'https://arxiv.org/abs/2503.12170', 'abstract': "End-to-end autonomous driving (E2E-AD) has rapidly emerged as a promising approach toward achieving full autonomy. However, existing E2E-AD systems typically adopt a traditional multi-task framework, addressing perception, prediction, and planning tasks through separate task-specific heads. Despite being trained in a fully differentiable manner, they still encounter issues with task coordination, and the system complexity remains high. In this work, we introduce DiffAD, a novel diffusion probabilistic model that redefines autonomous driving as a conditional image generation task. By rasterizing heterogeneous targets onto a unified bird's-eye view (BEV) and modeling their latent distribution, DiffAD unifies various driving objectives and jointly optimizes all driving tasks in a single framework, significantly reducing system complexity and harmonizing task coordination. The reverse process iteratively refines the generated BEV image, resulting in more robust and realistic driving behaviors. Closed-loop evaluations in Carla demonstrate the superiority of the proposed method, achieving a new state-of-the-art Success Rate and Driving Score. The code will be made publicly available.", 'abstract_zh': '端到端自主驾驶（E2E-AD）作为一种实现全自主驾驶的有前途的方法迅速 emergence。然而，现有的 E2E-AD 系统通常采用传统的多任务框架，分别通过特定任务的头部来解决感知、预测和规划任务。尽管是通过完全可微的方式进行训练，它们仍然面临任务协调问题，且系统复杂性仍然很高。在本工作中，我们提出了 DiffAD，一种新颖的扩散概率模型，重新定义自主驾驶为条件图像生成任务。通过将异构目标 rasterize 到统一的鸟瞰图（BEV）并建模其潜在分布，DiffAD 统一了各种驾驶目标，并在一个框架中联合优化所有驾驶任务，显著降低了系统复杂性并协调了任务间的协调。反转过程逐迭代细化生成的 BEV 图像，从而产生更稳健且真实的驾驶行为。在 Carla 中进行的闭环评估表明，所提出的方法具有优越性，实现了新的成功率（Success Rate）和驾驶评分（Driving Score）的最新成果。代码将公开发布。', 'title_zh': 'DiffAD: 自动驾驶统一扩散建模方法'}
{'arxiv_id': 'arXiv:2503.13430', 'title': 'AugMapNet: Improving Spatial Latent Structure via BEV Grid Augmentation for Enhanced Vectorized Online HD Map Construction', 'authors': 'Thomas Monninger, Md Zafar Anwar, Stanislaw Antol, Steffen Staab, Sihao Ding', 'link': 'https://arxiv.org/abs/2503.13430', 'abstract': "Autonomous driving requires an understanding of the infrastructure elements, such as lanes and crosswalks. To navigate safely, this understanding must be derived from sensor data in real-time and needs to be represented in vectorized form. Learned Bird's-Eye View (BEV) encoders are commonly used to combine a set of camera images from multiple views into one joint latent BEV grid. Traditionally, from this latent space, an intermediate raster map is predicted, providing dense spatial supervision but requiring post-processing into the desired vectorized form. More recent models directly derive infrastructure elements as polylines using vectorized map decoders, providing instance-level information. Our approach, Augmentation Map Network (AugMapNet), proposes latent BEV grid augmentation, a novel technique that significantly enhances the latent BEV representation. AugMapNet combines vector decoding and dense spatial supervision more effectively than existing architectures while remaining as straightforward to integrate and as generic as auxiliary supervision. Experiments on nuScenes and Argoverse2 datasets demonstrate significant improvements in vectorized map prediction performance up to 13.3% over the StreamMapNet baseline on 60m range and greater improvements on larger ranges. We confirm transferability by applying our method to another baseline and find similar improvements. A detailed analysis of the latent BEV grid confirms a more structured latent space of AugMapNet and shows the value of our novel concept beyond pure performance improvement. The code will be released soon.", 'abstract_zh': '自主驾驶需要理解基础设施元素，如车道和人行横道。为了安全导航，这种理解必须从实时传感器数据中提取，并以矢量化形式表示。Learned 鸟瞰视图（BEV）编码器常用于将多个视角的相机图像结合成一个联合潜在BEV网格。传统上，从这个潜在空间预测出一个中间栅格地图，提供密集的空间监督，但需要后处理成所需矢量化形式。最近的模型直接使用矢量化地图解码器提取基础设施元素为多段线，提供实例级信息。我们的方法，增强地图网络（AugMapNet），提出潜在BEV网格增强这一新颖技术，显著提升了潜在BEV表示。AugMapNet比现有架构更有效地结合矢量化解码和密集空间监督，同时保持易于集成和通用性，如同辅助监督一样。在nuScenes和Argoverse2数据集上的实验表明，AugMapNet在60m范围内的矢量化地图预测性能比StreamMapNet基线提高了13.3%，在更大范围内性能提升更为显著。我们通过将该方法应用于另一个基线确认其实用性，发现相似的提升效果。详细的潜在BEV网格分析证实了AugMapNet具有更结构化的潜在空间，并展示了我们新概念的价值远超单纯性能提升。代码即将发布。', 'title_zh': 'AugMapNet: 通过BEV网格增强改进空间隐含结构以增强向量在线高清地图构建'}
{'arxiv_id': 'arXiv:2503.13188', 'title': '3D Hierarchical Panoptic Segmentation in Real Orchard Environments Across Different Sensors', 'authors': 'Matteo Sodano, Federico Magistri, Elias Marks, Fares Hosn, Aibek Zurbayev, Rodrigo Marcuzzi, Meher V. R. Malladi, Jens Behley, Cyrill Stachniss', 'link': 'https://arxiv.org/abs/2503.13188', 'abstract': "Crop yield estimation is a relevant problem in agriculture, because an accurate crop yield estimate can support farmers' decisions on harvesting or precision intervention. Robots can help to automate this process. To do so, they need to be able to perceive the surrounding environment to identify target objects. In this paper, we introduce a novel approach to address the problem of hierarchical panoptic segmentation of apple orchards on 3D data from different sensors. Our approach is able to simultaneously provide semantic segmentation, instance segmentation of trunks and fruits, and instance segmentation of plants (a single trunk with its fruits). This allows us to identify relevant information such as individual plants, fruits, and trunks, and capture the relationship among them, such as precisely estimate the number of fruits associated to each tree in an orchard. Additionally, to efficiently evaluate our approach for hierarchical panoptic segmentation, we provide a dataset designed specifically for this task. Our dataset is recorded in Bonn in a real apple orchard with a variety of sensors, spanning from a terrestrial laser scanner to a RGB-D camera mounted on different robotic platforms. The experiments show that our approach surpasses state-of-the-art approaches in 3D panoptic segmentation in the agricultural domain, while also providing full hierarchical panoptic segmentation. Our dataset has been made publicly available at this https URL. We will provide the open-source implementation of our approach and public competiton for hierarchical panoptic segmentation on the hidden test sets upon paper acceptance.", 'abstract_zh': '基于多传感器3D数据的果园分层全景分割方法及其应用', 'title_zh': '跨不同传感器在真实果园环境中的3D分层全景分割'}
{'arxiv_id': 'arXiv:2503.12968', 'title': 'OptiPMB: Enhancing 3D Multi-Object Tracking with Optimized Poisson Multi-Bernoulli Filtering', 'authors': 'Guanhua Ding, Yuxuan Xia, Runwei Guan, Qinchen Wu, Tao Huang, Weiping Ding, Jinping Sun, Guoqiang Mao', 'link': 'https://arxiv.org/abs/2503.12968', 'abstract': 'Accurate 3D multi-object tracking (MOT) is crucial for autonomous driving, as it enables robust perception, navigation, and planning in complex environments. While deep learning-based solutions have demonstrated impressive 3D MOT performance, model-based approaches remain appealing for their simplicity, interpretability, and data efficiency. Conventional model-based trackers typically rely on random vector-based Bayesian filters within the tracking-by-detection (TBD) framework but face limitations due to heuristic data association and track management schemes. In contrast, random finite set (RFS)-based Bayesian filtering handles object birth, survival, and death in a theoretically sound manner, facilitating interpretability and parameter tuning. In this paper, we present OptiPMB, a novel RFS-based 3D MOT method that employs an optimized Poisson multi-Bernoulli (PMB) filter while incorporating several key innovative designs within the TBD framework. Specifically, we propose a measurement-driven hybrid adaptive birth model for improved track initialization, employ adaptive detection probability parameters to effectively maintain tracks for occluded objects, and optimize density pruning and track extraction modules to further enhance overall tracking performance. Extensive evaluations on nuScenes and KITTI datasets show that OptiPMB achieves superior tracking accuracy compared with state-of-the-art methods, thereby establishing a new benchmark for model-based 3D MOT and offering valuable insights for future research on RFS-based trackers in autonomous driving.', 'abstract_zh': '基于随机有限集的精确3D多目标跟踪方法 OptiPMB', 'title_zh': 'OptiPMB: 优化泊松多伯努利滤波以增强三维多目标跟踪'}
{'arxiv_id': 'arXiv:2503.12814', 'title': 'Versatile Physics-based Character Control with Hybrid Latent Representation', 'authors': 'Jinseok Bae, Jungdam Won, Donggeun Lim, Inwoo Hwang, Young Min Kim', 'link': 'https://arxiv.org/abs/2503.12814', 'abstract': 'We present a versatile latent representation that enables physically simulated character to efficiently utilize motion priors. To build a powerful motion embedding that is shared across multiple tasks, the physics controller should employ rich latent space that is easily explored and capable of generating high-quality motion. We propose integrating continuous and discrete latent representations to build a versatile motion prior that can be adapted to a wide range of challenging control tasks. Specifically, we build a discrete latent model to capture distinctive posterior distribution without collapse, and simultaneously augment the sampled vector with the continuous residuals to generate high-quality, smooth motion without jittering. We further incorporate Residual Vector Quantization, which not only maximizes the capacity of the discrete motion prior, but also efficiently abstracts the action space during the task learning phase. We demonstrate that our agent can produce diverse yet smooth motions simply by traversing the learned motion prior through unconditional motion generation. Furthermore, our model robustly satisfies sparse goal conditions with highly expressive natural motions, including head-mounted device tracking and motion in-betweening at irregular intervals, which could not be achieved with existing latent representations.', 'abstract_zh': '我们提出了一种通用的潜在表示，使物理模拟角色能高效利用运动先验。为了构建一个强大的共享多任务的运动嵌入，物理控制器应使用一个易于探索且能生成高质量运动的丰富潜在空间。我们提出将连续和离散的潜在表示集成起来，构建一个通用的运动先验，该先验能够适应一系列具有挑战性的控制任务。具体而言，我们构建了一个离散潜在模型来捕捉独特的后验分布而不发生退化，并同时通过添加连续残差来增强采样向量，生成高质量且平滑的运动，而无闪烁现象。我们进一步引入了残差向量量化方法，不仅最大化了离散运动先验的能力，还在任务学习阶段高效地抽象了动作空间。我们证明，仅通过无条件运动生成来遍历学习到的运动先验，我们的代理就能产生多样且平滑的运动。此外，我们的模型能够稳健地满足稀疏目标条件，并生成具有高度表现力的自然运动，包括头戴设备跟踪和不规则间隔的运动插值，这是现有潜在表示无法实现的。', 'title_zh': '基于混合潜表示的多功能物理驱动角色控制'}
{'arxiv_id': 'arXiv:2503.12419', 'title': 'EgoEvGesture: Gesture Recognition Based on Egocentric Event Camera', 'authors': 'Luming Wang, Hao Shi, Xiaoting Yin, Kailun Yang, Kaiwei Wang', 'link': 'https://arxiv.org/abs/2503.12419', 'abstract': 'Egocentric gesture recognition is a pivotal technology for enhancing natural human-computer interaction, yet traditional RGB-based solutions suffer from motion blur and illumination variations in dynamic scenarios. While event cameras show distinct advantages in handling high dynamic range with ultra-low power consumption, existing RGB-based architectures face inherent limitations in processing asynchronous event streams due to their synchronous frame-based nature. Moreover, from an egocentric perspective, event cameras record data that include events generated by both head movements and hand gestures, thereby increasing the complexity of gesture recognition. To address this, we propose a novel network architecture specifically designed for event data processing, incorporating (1) a lightweight CNN with asymmetric depthwise convolutions to reduce parameters while preserving spatiotemporal features, (2) a plug-and-play state-space model as context block that decouples head movement noise from gesture dynamics, and (3) a parameter-free Bins-Temporal Shift Module (BSTM) that shifts features along bins and temporal dimensions to fuse sparse events efficiently. We further build the EgoEvGesture dataset, the first large-scale dataset for egocentric gesture recognition using event cameras. Experimental results demonstrate that our method achieves 62.7% accuracy in heterogeneous testing with only 7M parameters, 3.1% higher than state-of-the-art approaches. Notable misclassifications in freestyle motions stem from high inter-personal variability and unseen test patterns differing from training data. Moreover, our approach achieved a remarkable accuracy of 96.97% on DVS128 Gesture, demonstrating strong cross-dataset generalization capability. The dataset and models are made publicly available at this https URL.', 'abstract_zh': '自中心手势识别是提升自然人机交互的关键技术，但传统的基于RGB的方法在动态场景中受到运动模糊和光照变化的影响。虽然事件相机在处理高动态范围时表现出色且能耗极低，但现有的基于RGB的架构由于其基于同步帧的方式，在处理异步事件流时存在固有的局限性。此外，从自中心视角来看，事件相机记录的数据包括由头部运动和手部手势共同生成的事件，这增加了手势识别的复杂性。为了解决这一问题，我们提出了一种专为事件数据处理设计的新颖网络架构，包括（1）一种轻量级的CNN，采用不对称深度可分离卷积以减少参数同时保留时空特征，（2）一种可插拔的状态空间模型作为上下文块，可以解耦头部运动噪声与手势动力学，以及（3）一种无需参数的Bins-Time Shift模块（BSTM），沿 bins 和时间维度平移特征以高效融合稀疏事件。我们还构建了EgoEvGesture数据集，这是第一个用于事件相机下自中心手势识别的大规模数据集。实验结果表明，我们的方法在异构测试中达到了62.7%的准确率，比现有最佳方法高3.1%。自由式动作中的显著误分类主要是由于高个体间变异性以及不同训练数据的未知测试模式。此外，我们的方法在DVS128 Gesture数据集上达到了96.97%的准确率，显示出强大的跨数据集泛化能力。数据集和模型已公开发布。', 'title_zh': '基于第一人称事件相机的手势识别：EgoEvGesture'}
{'arxiv_id': 'arXiv:2503.11958', 'title': 'CHOrD: Generation of Collision-Free, House-Scale, and Organized Digital Twins for 3D Indoor Scenes with Controllable Floor Plans and Optimal Layouts', 'authors': 'Chong Su, Yingbin Fu, Zheyuan Hu, Jing Yang, Param Hanji, Shaojun Wang, Xuan Zhao, Cengiz Öztireli, Fangcheng Zhong', 'link': 'https://arxiv.org/abs/2503.11958', 'abstract': 'We introduce CHOrD, a novel framework for scalable synthesis of 3D indoor scenes, designed to create house-scale, collision-free, and hierarchically structured indoor digital twins. In contrast to existing methods that directly synthesize the scene layout as a scene graph or object list, CHOrD incorporates a 2D image-based intermediate layout representation, enabling effective prevention of collision artifacts by successfully capturing them as out-of-distribution (OOD) scenarios during generation. Furthermore, unlike existing methods, CHOrD is capable of generating scene layouts that adhere to complex floor plans with multi-modal controls, enabling the creation of coherent, house-wide layouts robust to both geometric and semantic variations in room structures. Additionally, we propose a novel dataset with expanded coverage of household items and room configurations, as well as significantly improved data quality. CHOrD demonstrates state-of-the-art performance on both the 3D-FRONT and our proposed datasets, delivering photorealistic, spatially coherent indoor scene synthesis adaptable to arbitrary floor plan variations.', 'abstract_zh': '我们提出CHOrD，一种用于大规模合成3D室内场景的新型框架，旨在创建与房屋规模相符、无碰撞且具有分层结构的室内数字孪生。与现有直接将场景布局合成为空间图或对象列表的方法不同，CHOrD 结合了基于2D图像的中间布局表示，使其能够在生成过程中成功地将碰撞伪影捕获为离分布（OOD）场景，从而有效防止碰撞伪影。此外，与现有方法不同，CHOrD 能够生成符合复杂楼层平面图且具有多模态控制的场景布局，从而实现对房间结构几何和语义变化具有鲁棒性的连贯、全屋范围的布局。此外，我们提出了一种新的数据集，该数据集扩展了家用物品和房间配置的涵盖范围，并且数据质量显著提高。CHOrD 在3D-FRONT数据集和我们提出的数据集上均表现出现有的先进性能，能够生成适应任意楼层平面图变化的逼真、空间连贯的室内场景合成。', 'title_zh': 'CHOrD: 生成无碰撞、家庭规模且组织有序的3D室内场景数字孪生，具有可控的楼层平面图和最优布局'}
{'arxiv_id': 'arXiv:2503.11801', 'title': 'Diffuse-CLoC: Guided Diffusion for Physics-based Character Look-ahead Control', 'authors': 'Xiaoyu Huang, Takara Truong, Yunbo Zhang, Fangzhou Yu, Jean Pierre Sleiman, Jessica Hodgins, Koushil Sreenath, Farbod Farshidian', 'link': 'https://arxiv.org/abs/2503.11801', 'abstract': 'We present Diffuse-CLoC, a guided diffusion framework for physics-based look-ahead control that enables intuitive, steerable, and physically realistic motion generation. While existing kinematics motion generation with diffusion models offer intuitive steering capabilities with inference-time conditioning, they often fail to produce physically viable motions. In contrast, recent diffusion-based control policies have shown promise in generating physically realizable motion sequences, but the lack of kinematics prediction limits their steerability. Diffuse-CLoC addresses these challenges through a key insight: modeling the joint distribution of states and actions within a single diffusion model makes action generation steerable by conditioning it on the predicted states. This approach allows us to leverage established conditioning techniques from kinematic motion generation while producing physically realistic motions. As a result, we achieve planning capabilities without the need for a high-level planner. Our method handles a diverse set of unseen long-horizon downstream tasks through a single pre-trained model, including static and dynamic obstacle avoidance, motion in-betweening, and task-space control. Experimental results show that our method significantly outperforms the traditional hierarchical framework of high-level motion diffusion and low-level tracking.', 'abstract_zh': 'Diffuse-CLoC：一种用于物理导向前瞻控制的扩散框架，实现直观、可控且物理真实的运动生成', 'title_zh': '基于物理的角色前瞻控制引导扩散：Diffuse-CLoC'}
{'arxiv_id': 'arXiv:2503.13169', 'title': 'Collaborative AI Enhances Image Understanding in Materials Science', 'authors': 'Ruoyan Avery Yin, Zhichu Ren, Zongyou Yin, Zhen Zhang, So Yeon Kim, Chia-Wei Hsu, Ju Li', 'link': 'https://arxiv.org/abs/2503.13169', 'abstract': 'The Copilot for Real-world Experimental Scientist (CRESt) system empowers researchers to control autonomous laboratories through conversational AI, providing a seamless interface for managing complex experimental workflows. We have enhanced CRESt by integrating a multi-agent collaboration mechanism that utilizes the complementary strengths of the ChatGPT and Gemini models for precise image analysis in materials science. This innovative approach significantly improves the accuracy of experimental outcomes by fostering structured debates between the AI models, which enhances decision-making processes in materials phase analysis. Additionally, to evaluate the generalizability of this approach, we tested it on a quantitative task of counting particles. Here, the collaboration between the AI models also led to improved results, demonstrating the versatility and robustness of this method. By harnessing this dual-AI framework, this approach stands as a pioneering method for enhancing experimental accuracy and efficiency in materials research, with applications extending beyond CRESt to broader scientific experimentation and analysis.', 'abstract_zh': '.real-world实验科学家辅佐系统（CRESt）通过对话式AI赋能研究人员控制自主实验室，提供管理复杂实验流程的无缝界面。我们通过集成多智能体协作机制，结合ChatGPT和Gemini模型的互补优势，增强了CRESt，以精确进行材料科学中的图像分析。这种创新方法通过促进AI模型之间的结构化辩论，显著提高了材料相分析中的决策过程和实验结果准确性。此外，为了评估该方法的普遍适用性，我们将其应用于颗粒计数的定量任务，结果显示AI模型之间的协作也带来了更好的结果，展示了该方法的通用性和鲁棒性。通过利用这一双AI框架，该方法成为提升材料研究实验准确性与效率的开创性方法，其应用超越了CRESt，扩展到更广泛的科学研究和分析。', 'title_zh': '协作AI增强材料科学中的图像理解'}
{'arxiv_id': 'arXiv:2503.13444', 'title': 'VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning', 'authors': 'Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, Mike Zheng Shou', 'link': 'https://arxiv.org/abs/2503.13444', 'abstract': 'Videos, with their unique temporal dimension, demand precise grounded understanding, where answers are directly linked to visual, interpretable evidence. Despite significant breakthroughs in reasoning capabilities within Large Language Models, multi-modal reasoning - especially for videos - remains unexplored. In this work, we introduce VideoMind, a novel video-language agent designed for temporal-grounded video understanding. VideoMind incorporates two key innovations: (i) We identify essential capabilities for video temporal reasoning and develop a role-based agentic workflow, including a planner for coordinating different roles, a grounder for temporal localization, a verifier to assess temporal interval accuracy, and an answerer for question-answering. (ii) To efficiently integrate these diverse roles, we propose a novel Chain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA adaptors while avoiding the overhead of multiple models, thus balancing efficiency and flexibility. Extensive experiments on 14 public benchmarks demonstrate that our agent achieves state-of-the-art performance on diverse video understanding tasks, including 3 on grounded video question-answering, 6 on video temporal grounding, and 5 on general video question-answering, underscoring its effectiveness in advancing video agent and long-form temporal reasoning.', 'abstract_zh': '基于多模态推理的视频语义理解agents：VideoMind及其在视频理解任务中的应用', 'title_zh': 'VideoMind: 一种长视频推理链路模型代理'}
{'arxiv_id': 'arXiv:2503.13434', 'title': 'BlobCtrl: A Unified and Flexible Framework for Element-level Image Generation and Editing', 'authors': 'Yaowei Li, Lingen Li, Zhaoyang Zhang, Xiaoyu Li, Guangzhi Wang, Hongxiang Li, Xiaodong Cun, Ying Shan, Yuexian Zou', 'link': 'https://arxiv.org/abs/2503.13434', 'abstract': 'Element-level visual manipulation is essential in digital content creation, but current diffusion-based methods lack the precision and flexibility of traditional tools. In this work, we introduce BlobCtrl, a framework that unifies element-level generation and editing using a probabilistic blob-based representation. By employing blobs as visual primitives, our approach effectively decouples and represents spatial location, semantic content, and identity information, enabling precise element-level manipulation. Our key contributions include: 1) a dual-branch diffusion architecture with hierarchical feature fusion for seamless foreground-background integration; 2) a self-supervised training paradigm with tailored data augmentation and score functions; and 3) controllable dropout strategies to balance fidelity and diversity. To support further research, we introduce BlobData for large-scale training and BlobBench for systematic evaluation. Experiments show that BlobCtrl excels in various element-level manipulation tasks while maintaining computational efficiency, offering a practical solution for precise and flexible visual content creation. Project page: this https URL', 'abstract_zh': '元素级视觉操控在数字内容创作中至关重要，但当前基于弥散的方法缺乏传统工具的精准性和灵活性。本文介绍了一种名为BlobCtrl的框架，该框架利用概率性的blob表示法统一了元素级生成和编辑。通过使用blobs作为视觉基本元素，我们的方法有效解耦并表示空间位置、语义内容和身份信息，从而实现精确的元素级操控。我们的主要贡献包括：1）包含分层特征融合的双分支弥散架构，以实现无缝的前景-背景集成；2）自监督训练范式，包括定制的数据增强和评分函数；3）可控的 dropout 策略以平衡保真度和多样性。为支持进一步研究，我们介绍了BlobData用于大规模训练，BlobBench用于系统性评估。实验结果显示，BlobCtrl在多种元素级操控任务中表现出色，同时保持计算效率，提供了一种实用的精准和灵活的视觉内容创作解决方案。项目页面：这个 https URL', 'title_zh': 'BlobCtrl：统一且灵活的元素级图像生成与编辑框架'}
{'arxiv_id': 'arXiv:2503.13377', 'title': 'TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM', 'authors': 'Ye Wang, Boshen Xu, Zihao Yue, Zihan Xiao, Ziheng Wang, Liang Zhang, Dingyi Yang, Wenxuan Wang, Qin Jin', 'link': 'https://arxiv.org/abs/2503.13377', 'abstract': 'We introduce TimeZero, a reasoning-guided LVLM designed for the temporal video grounding (TVG) task. This task requires precisely localizing relevant video segments within long videos based on a given language query. TimeZero tackles this challenge by extending the inference process, enabling the model to reason about video-language relationships solely through reinforcement learning. To evaluate the effectiveness of TimeZero, we conduct experiments on two benchmarks, where TimeZero achieves state-of-the-art performance on Charades-STA. Code is available at this https URL.', 'abstract_zh': '我们介绍了TimeZero，一种用于时间视频定位（TVG）任务的推理指导型LVLM。该任务要求根据给定的自然语言查询，精确地在长视频中定位相关的视频片段。TimeZero通过扩展推理过程，仅通过强化学习来处理视频-语言关系，从而应对这一挑战。为了评估TimeZero的有效性，我们在两个基准上进行了实验，其中TimeZero在Charades-STA上达到了最先进的性能。代码可在以下链接获得：this https URL。', 'title_zh': 'TimeZero: 基于推理引导的LVLM时间视频定位'}
{'arxiv_id': 'arXiv:2503.13309', 'title': 'Integrating AI for Human-Centric Breast Cancer Diagnostics: A Multi-Scale and Multi-View Swin Transformer Framework', 'authors': 'Farnoush Bayatmakou, Reza Taleei, Milad Amir Toutounchian, Arash Mohammadi', 'link': 'https://arxiv.org/abs/2503.13309', 'abstract': "Despite advancements in Computer-Aided Diagnosis (CAD) systems, breast cancer remains one of the leading causes of cancer-related deaths among women worldwide. Recent breakthroughs in Artificial Intelligence (AI) have shown significant promise in development of advanced Deep Learning (DL) architectures for breast cancer diagnosis through mammography. In this context, the paper focuses on the integration of AI within a Human-Centric workflow to enhance breast cancer diagnostics. Key challenges are, however, largely overlooked such as reliance on detailed tumor annotations and susceptibility to missing views, particularly during test time. To address these issues, we propose a hybrid, multi-scale and multi-view Swin Transformer-based framework (MSMV-Swin) that enhances diagnostic robustness and accuracy. The proposed MSMV-Swin framework is designed to work as a decision-support tool, helping radiologists analyze multi-view mammograms more effectively. More specifically, the MSMV-Swin framework leverages the Segment Anything Model (SAM) to isolate the breast lobe, reducing background noise and enabling comprehensive feature extraction. The multi-scale nature of the proposed MSMV-Swin framework accounts for tumor-specific regions as well as the spatial characteristics of tissues surrounding the tumor, capturing both localized and contextual information. The integration of contextual and localized data ensures that MSMV-Swin's outputs align with the way radiologists interpret mammograms, fostering better human-AI interaction and trust. A hybrid fusion structure is then designed to ensure robustness against missing views, a common occurrence in clinical practice when only a single mammogram view is available.", 'abstract_zh': '尽管计算机辅助诊断（CAD）系统取得了进展，乳腺癌仍然是一类全球女性癌症相关死亡的主要原因。近期人工智能（AI）的突破性进展显示了通过乳腺X光摄影进行乳腺癌诊断的高级深度学习（DL）架构的巨大潜力。在此背景下，本文关注将AI集成到以人类为中心的工作流程中，以增强乳腺癌诊断。然而，关键挑战，如对详细肿瘤注释的依赖以及在测试时容易遗漏视图，往往被忽视。为解决这些问题，我们提出了一种混合、多尺度和多视图SwinTransformer框架（MSMV-Swin），以增强诊断的稳健性和准确性。提出的MSMV-Swin框架设计为辅助决策工具，帮助放射科医生更有效地分析多视图乳腺X光摄影图像。具体而言，MSMV-Swin框架利用Segment Anything Model（SAM）隔离乳腺小叶，减少背景噪音并实现全面的特征提取。所提出MSMV-Swin框架的多尺度特性考虑了肿瘤特异性区域以及肿瘤周围组织的空间特性，捕获局部和上下文信息。通过结合上下文和局部数据，MSMV-Swin的输出能够更好地与放射科医生解读乳腺X光摄影图像的方式保持一致，促进更好的人机交互和信任。为了确保在仅有一张视图可用的临床实践中对缺失视图具有鲁棒性，我们设计了一种混合融合结构。', 'title_zh': '基于多尺度多视图Swin Transformer的人工智能集成在人体中心乳腺癌诊断中的应用'}
{'arxiv_id': 'arXiv:2503.13277', 'title': 'Artificial Intelligence-Driven Prognostic Classification of COVID-19 Using Chest X-rays: A Deep Learning Approach', 'authors': 'Alfred Simbun, Suresh Kumar', 'link': 'https://arxiv.org/abs/2503.13277', 'abstract': "Background: The COVID-19 pandemic has overwhelmed healthcare systems, emphasizing the need for AI-driven tools to assist in rapid and accurate patient prognosis. Chest X-ray imaging is a widely available diagnostic tool, but existing methods for prognosis classification lack scalability and efficiency. Objective: This study presents a high-accuracy deep learning model for classifying COVID-19 severity (Mild, Moderate, and Severe) using Chest X-ray images, developed on Microsoft Azure Custom Vision. Methods: Using a dataset of 1,103 confirmed COVID-19 X-ray images from AIforCOVID, we trained and validated a deep learning model leveraging Convolutional Neural Networks (CNNs). The model was evaluated on an unseen dataset to measure accuracy, precision, and recall. Results: Our model achieved an average accuracy of 97%, with specificity of 99%, sensitivity of 87%, and an F1-score of 93.11%. When classifying COVID-19 severity, the model achieved accuracies of 89.03% (Mild), 95.77% (Moderate), and 81.16% (Severe). These results demonstrate the model's potential for real-world clinical applications, aiding in faster decision-making and improved resource allocation. Conclusion: AI-driven prognosis classification using deep learning can significantly enhance COVID-19 patient management, enabling early intervention and efficient triaging. Our study provides a scalable, high-accuracy AI framework for integrating deep learning into routine clinical workflows. Future work should focus on expanding datasets, external validation, and regulatory compliance to facilitate clinical adoption.", 'abstract_zh': '背景：COVID-19大流行已使医疗保健系统不堪重负，突显了需要AI驱动工具来辅助快速准确的患者预后。胸部X光成像是一种广泛应用的诊断工具，但现有的预后分类方法缺乏可扩展性和效率。目的：本研究提出了一种高精度的深度学习模型，用于通过胸部X光图像分类COVID-19严重程度（轻度、中度和重度），该模型基于Microsoft Azure Custom Vision开发。方法：使用来自AIforCOVID的1,103张确认的COVID-19 X光图像数据集，我们运用卷积神经网络（CNNs）训练和验证了一个深度学习模型。该模型在未见过的数据集上进行评估，以测量准确率、精确率和召回率。结果：我们的模型实现了平均97%的准确率，特异性为99%，敏感性为87%，F1分数为93.11%。在分类COVID-19严重程度时，轻度为89.03%、中度为95.77%、重度为81.16%。这些结果表明该模型在实际临床应用中的潜力，有助于更快的决策和资源分配。结论：利用深度学习进行AI驱动的预后分类可以显著提高COVID-19患者的管理能力，使早期干预和高效分诊成为可能。本研究提供了将深度学习纳入常规临床工作流的可扩展、高精度AI框架。未来的工作应重点关注数据集的扩展、外部验证和法规合规性，以促进临床应用。', 'title_zh': '基于胸部X光的人工智能驱动的COVID-19预后分类：一种深度学习方法'}
{'arxiv_id': 'arXiv:2503.13211', 'title': 'MedLoRD: A Medical Low-Resource Diffusion Model for High-Resolution 3D CT Image Synthesis', 'authors': 'Marvin Seyfarth, Salman Ul Hassan Dar, Isabelle Ayx, Matthias Alexander Fink, Stefan O. Schoenberg, Hans-Ulrich Kauczor, Sandy Engelhardt', 'link': 'https://arxiv.org/abs/2503.13211', 'abstract': 'Advancements in AI for medical imaging offer significant potential. However, their applications are constrained by the limited availability of data and the reluctance of medical centers to share it due to patient privacy concerns. Generative models present a promising solution by creating synthetic data as a substitute for real patient data. However, medical images are typically high-dimensional, and current state-of-the-art methods are often impractical for computational resource-constrained healthcare environments. These models rely on data sub-sampling, raising doubts about their feasibility and real-world applicability. Furthermore, many of these models are evaluated on quantitative metrics that alone can be misleading in assessing the image quality and clinical meaningfulness of the generated images. To address this, we introduce MedLoRD, a generative diffusion model designed for computational resource-constrained environments. MedLoRD is capable of generating high-dimensional medical volumes with resolutions up to 512$\\times$512$\\times$256, utilizing GPUs with only 24GB VRAM, which are commonly found in standard desktop workstations. MedLoRD is evaluated across multiple modalities, including Coronary Computed Tomography Angiography and Lung Computed Tomography datasets. Extensive evaluations through radiological evaluation, relative regional volume analysis, adherence to conditional masks, and downstream tasks show that MedLoRD generates high-fidelity images closely adhering to segmentation mask conditions, surpassing the capabilities of current state-of-the-art generative models for medical image synthesis in computational resource-constrained environments.', 'abstract_zh': '医学影像领域中人工智能的进步提供了巨大潜力，但由于数据可用性有限以及医疗中心因患者隐私问题不愿共享数据，其应用受到了限制。生成模型为这一问题提供了一种有前景的解决方案，通过生成合成数据来替代真实的患者数据。然而，医学图像通常具有高维度，当前最先进的方法对于计算资源受限的医疗保健环境来说往往不切实际。这些模型依赖于数据子采样，这引发了人们对它们可行性和实际应用性的怀疑。此外，许多模型仅通过定量指标进行评估，这些指标在评估生成图像的质量和临床意义时可能会具有误导性。为了解决这一问题，我们引入了MedLoRD，这是一种针对计算资源受限环境设计的生成扩散模型。MedLoRD 能够生成分辨率为 512×512×256 的高维度医学体素，并仅使用具有 24GB VRAM 的 GPU 实现，这在标准台式工作站中较为普遍。MedLoRD 在冠状动脉计算机断层扫描血管造影和肺部计算机断层扫描等不同模态的数据集上进行了评估。通过放射学评估、相对区域体积分析、条件掩码遵守情况以及下游任务等多种评估方法表明，MedLoRD 生成的高保真图像严格符合分割掩码条件，并在计算资源受限的环境中超越了当前最先进的医学图像生成模型的性能。', 'title_zh': 'MedLoRD: 一种用于高分辨率3D CT图像合成的医疗低资源扩散模型'}
{'arxiv_id': 'arXiv:2503.13139', 'title': 'Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding', 'authors': 'Weiyu Guo, Ziyang Chen, Shaoguang Wang, Jianxiang He, Yijie Xu, Jinhui Ye, Ying Sun, Hui Xiong', 'link': 'https://arxiv.org/abs/2503.13139', 'abstract': "Understanding long video content is a complex endeavor that often relies on densely sampled frame captions or end-to-end feature selectors, yet these techniques commonly overlook the logical relationships between textual queries and visual elements. In practice, computational constraints necessitate coarse frame subsampling, a challenge analogous to ``finding a needle in a haystack.'' To address this issue, we introduce a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search. Specifically, we systematically define four fundamental logical dependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute dependency, and 4) causal order. These relations dynamically update frame sampling distributions through an iterative refinement process, enabling context-aware identification of semantically critical frames tailored to specific query requirements. Our method establishes new SOTA performance on the manually annotated benchmark in key-frame selection metrics. Furthermore, when applied to downstream video question-answering tasks, the proposed approach demonstrates the best performance gains over existing methods on LongVideoBench and Video-MME, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning. The code will be publicly available.", 'abstract_zh': '理解长视频内容是一项复杂的工作，通常依赖于密集采样的帧字幕或端到端特征选择器，但这些技术经常忽视文本查询与视觉元素之间的逻辑关系。在实践中，计算限制要求进行粗略的帧采样，这是一个类似于“在haystack中找针”的挑战。为了解决这个问题，我们提出了一种语义驱动的搜索框架，将关键帧选择重新表述为视觉语义-逻辑搜索的框架。具体而言，我们系统地定义了四种基本的逻辑依赖关系：1) 空间共现，2) 时间临近，3) 属性依赖，4) 因果顺序。这些关系通过迭代优化过程动态更新帧采样分布，从而使系统能够根据特定查询要求识别语义关键帧。我们的方法在关键帧选择基准上的手动标注指标中建立了新的SOTA性能。此外，在下游视频问答任务中，提出的方法在LongVideoBench和Video-MME上展示了比现有方法的最佳性能提升，验证了其在文本查询与视觉-时间推理之间逻辑鸿沟方面的有效性。代码将公开可用。', 'title_zh': '框架中的逻辑：通过视觉语义逻辑验证进行长视频中动态关键帧搜索'}
{'arxiv_id': 'arXiv:2503.13025', 'title': 'PoseSyn: Synthesizing Diverse 3D Pose Data from In-the-Wild 2D Data', 'authors': 'ChangHee Yang, Hyeonseop Song, Seokhun Choi, Seungwoo Lee, Jaechul Kim, Hoseok Do', 'link': 'https://arxiv.org/abs/2503.13025', 'abstract': "Despite considerable efforts to enhance the generalization of 3D pose estimators without costly 3D annotations, existing data augmentation methods struggle in real world scenarios with diverse human appearances and complex poses. We propose PoseSyn, a novel data synthesis framework that transforms abundant in the wild 2D pose dataset into diverse 3D pose image pairs. PoseSyn comprises two key components: Error Extraction Module (EEM), which identifies challenging poses from the 2D pose datasets, and Motion Synthesis Module (MSM), which synthesizes motion sequences around the challenging poses. Then, by generating realistic 3D training data via a human animation model aligned with challenging poses and appearances PoseSyn boosts the accuracy of various 3D pose estimators by up to 14% across real world benchmarks including various backgrounds and occlusions, challenging poses, and multi view scenarios. Extensive experiments further confirm that PoseSyn is a scalable and effective approach for improving generalization without relying on expensive 3D annotations, regardless of the pose estimator's model size or design.", 'abstract_zh': '尽管在无需昂贵3D标注的情况下提升3D姿态估计器的泛化能力方面付出了大量努力，现有的数据增强方法在包含多样化人类外观和复杂姿态的真实世界场景中表现不佳。我们提出了一种名为PoseSyn的新颖数据合成框架，能够将野生物2D姿态数据集转换为多样化的3D姿态图像对。PoseSyn主要包括两个关键组件：错误提取模块（EEM），用于从2D姿态数据集中识别具有挑战性的姿态；以及运动合成模块（MSM），用于围绕这些具有挑战性的姿态合成运动序列。通过使用与具有挑战性的姿态和外观相匹配的人类动画模型生成真实的3D训练数据，PoseSyn在包含各种背景、遮挡、具有挑战性姿态及多视图场景的真实世界基准测试中，能够将各种3D姿态估计器的准确性提升高达14%。进一步的实验还证实，PoseSyn是一种可扩展且有效的方案，能够在无需依赖昂贵3D标注的情况下提高泛化能力，无论姿态估计器的模型大小或设计如何。', 'title_zh': 'PoseSyn: 从野生状态的2D数据合成多样化的3D姿态数据'}
{'arxiv_id': 'arXiv:2503.12964', 'title': 'Training Video Foundation Models with NVIDIA NeMo', 'authors': 'Zeeshan Patel, Ethan He, Parth Mannan, Xiaowei Ren, Ryan Wolf, Niket Agarwal, Jacob Huffman, Zhuoyao Wang, Carl Wang, Jack Chang, Yan Bai, Tommy Huang, Linnan Wang, Sahil Jain, Shanmugam Ramasamy, Joseph Jennings, Ekaterina Sirazitdinova, Oleg Sudakov, Mingyuan Ma, Bobby Chen, Forrest Lin, Hao Wang, Vasanth Rao Naik Sabavat, Sriharsha Niverty, Rong Ou, Pallab Bhattacharya, David Page, Nima Tajbakhsh, Ashwath Aithal', 'link': 'https://arxiv.org/abs/2503.12964', 'abstract': 'Video Foundation Models (VFMs) have recently been used to simulate the real world to train physical AI systems and develop creative visual experiences. However, there are significant challenges in training large-scale, high quality VFMs that can generate high-quality videos. We present a scalable, open-source VFM training pipeline with NVIDIA NeMo, providing accelerated video dataset curation, multimodal data loading, and parallelized video diffusion model training and inference. We also provide a comprehensive performance analysis highlighting best practices for efficient VFM training and inference.', 'abstract_zh': '大规模高质量视频基础模型的可扩展开源训练管道：基于NVIDIA NeMo的技术与性能分析', 'title_zh': '使用NVIDIA NeMo训练视频基础模型'}
{'arxiv_id': 'arXiv:2503.12855', 'title': 'VITED: Video Temporal Evidence Distillation', 'authors': 'Yujie Lu, Yale Song, William Wang, Lorenzo Torresani, Tushar Nagarajan', 'link': 'https://arxiv.org/abs/2503.12855', 'abstract': 'We investigate complex video question answering via chain-of-evidence reasoning -- identifying sequences of temporal spans from multiple relevant parts of the video, together with visual evidence within them. Existing models struggle with multi-step reasoning as they uniformly sample a fixed number of frames, which can miss critical evidence distributed nonuniformly throughout the video. Moreover, they lack the ability to temporally localize such evidence in the broader context of the full video, which is required for answering complex questions. We propose a framework to enhance existing VideoQA datasets with evidence reasoning chains, automatically constructed by searching for optimal intervals of interest in the video with supporting evidence, that maximizes the likelihood of answering a given question. We train our model (VITED) to generate these evidence chains directly, enabling it to both localize evidence windows as well as perform multi-step reasoning across them in long-form video content. We show the value of our evidence-distilled models on a suite of long video QA benchmarks where we outperform state-of-the-art approaches that lack evidence reasoning capabilities.', 'abstract_zh': '通过链式证据推理进行复杂视频问答：增强证据推理链条以实现多步推理和时空局部化', 'title_zh': 'VITED: 视频时间证据精炼'}
{'arxiv_id': 'arXiv:2503.12834', 'title': 'PASTA: Part-Aware Sketch-to-3D Shape Generation with Text-Aligned Prior', 'authors': 'Seunggwan Lee, Hwanhee Jung, Byoungsoo Koh, Qixing Huang, Sangho Yoon, Sangpil Kim', 'link': 'https://arxiv.org/abs/2503.12834', 'abstract': 'A fundamental challenge in conditional 3D shape generation is to minimize the information loss and maximize the intention of user input. Existing approaches have predominantly focused on two types of isolated conditional signals, i.e., user sketches and text descriptions, each of which does not offer flexible control of the generated shape. In this paper, we introduce PASTA, the flexible approach that seamlessly integrates a user sketch and a text description for 3D shape generation. The key idea is to use text embeddings from a vision-language model to enrich the semantic representation of sketches. Specifically, these text-derived priors specify the part components of the object, compensating for missing visual cues from ambiguous sketches. In addition, we introduce ISG-Net which employs two types of graph convolutional networks: IndivGCN, which processes fine-grained details, and PartGCN, which aggregates these details into parts and refines the structure of objects. Extensive experiments demonstrate that PASTA outperforms existing methods in part-level editing and achieves state-of-the-art results in sketch-to-3D shape generation.', 'abstract_zh': '条件3D形状生成中的根本挑战是最大程度地减少信息损失并最大化用户输入的意图。现有方法主要关注两种孤立的条件信号，即用户草图和文本描述，每种方法都无法灵活控制生成的形状。在本文中，我们提出了PASTA，这是一种灵活的方法，可以无缝地将用户草图和文本描述整合到3D形状生成中。核心思想是使用视觉-语言模型的文本嵌入来丰富草图的语义表示。具体而言，这些文本衍生的先验指定了对象的部分组件，弥补了模糊草图中缺失的视觉线索。此外，我们引入了ISG-Net，该网络采用了两种类型的图卷积网络：IndivGCN，用于处理细粒度细节；PartGCN，将这些细节聚合到部分中并细化对象结构。广泛实验表明，PASTA在部分级编辑方面优于现有方法，并在草图到3D形状生成方面取得了最先进的成果。', 'title_zh': 'PASTA：部分意识的文本对齐先验素描到3D形状生成'}
{'arxiv_id': 'arXiv:2503.12781', 'title': 'SAM2 for Image and Video Segmentation: A Comprehensive Survey', 'authors': 'Zhang Jiaxing, Tang Hao', 'link': 'https://arxiv.org/abs/2503.12781', 'abstract': "Despite significant advances in deep learning for image and video segmentation, existing models continue to face challenges in cross-domain adaptability and generalization. Image and video segmentation are fundamental tasks in computer vision with wide-ranging applications in healthcare, agriculture, industrial inspection, and autonomous driving. With the advent of large-scale foundation models, SAM2 - an improved version of SAM (Segment Anything Model)has been optimized for segmentation tasks, demonstrating enhanced performance in complex scenarios. However, SAM2's adaptability and limitations in specific domains require further investigation. This paper systematically analyzes the application of SAM2 in image and video segmentation and evaluates its performance in various fields. We begin by introducing the foundational concepts of image segmentation, categorizing foundation models, and exploring the technical characteristics of SAM and SAM2. Subsequently, we delve into SAM2's applications in static image and video segmentation, emphasizing its performance in specialized areas such as medical imaging and the challenges of cross-domain adaptability. As part of our research, we reviewed over 200 related papers to provide a comprehensive analysis of the topic. Finally, the paper highlights the strengths and weaknesses of SAM2 in segmentation tasks, identifies the technical challenges it faces, and proposes future development directions. This review provides valuable insights and practical recommendations for optimizing and applying SAM2 in real-world scenarios.", 'abstract_zh': '尽管深度学习在图像和视频分割领域取得了显著进展，现有模型仍在跨域适应性和泛化能力方面面临挑战。图像和视频分割是计算机视觉中的基本任务，广泛应用于医疗、农业、工业检测和自动驾驶等领域。随着大规模基础模型的出现，SAM2作为一种改进的SAM（Segment Anything Model）版本，已被优化用于分割任务，并在复杂场景中展示了增强的性能。然而，SAM2在特定领域的适应性和局限性仍需进一步研究。本文系统分析了SAM2在图像和视频分割中的应用，并评估其在各个领域的表现。我们首先介绍了图像分割的基础概念，分类基础模型，并探讨了SAM和SAM2的技术特点。随后，我们深入探讨了SAM2在静态图像和视频分割中的应用，强调了其在医学成像等特定领域的性能以及跨域适应性的挑战。作为我们研究的一部分，我们回顾了超过200篇相关论文，提供了全面的分析。最后，本文指出了SAM2在分割任务中的优势和不足，识别了其面临的技术挑战，并提出了未来发展方向。这篇综述为优化和应用于实际场景中的SAM2提供了有价值的见解和实用建议。', 'title_zh': 'SAM2在图像与视频分割中的综合调研'}
{'arxiv_id': 'arXiv:2503.12780', 'title': 'LangDA: Building Context-Awareness via Language for Domain Adaptive Semantic Segmentation', 'authors': 'Chang Liu, Bavesh Balaji, Saad Hossain, C Thomas, Kwei-Herng Lai, Raviteja Vemulapalli, Alexander Wong, Sirisha Rambhatla', 'link': 'https://arxiv.org/abs/2503.12780', 'abstract': 'Unsupervised domain adaptation for semantic segmentation (DASS) aims to transfer knowledge from a label-rich source domain to a target domain with no labels. Two key approaches in DASS are (1) vision-only approaches using masking or multi-resolution crops, and (2) language-based approaches that use generic class-wise prompts informed by target domain (e.g. "a {snowy} photo of a {class}"). However, the former is susceptible to noisy pseudo-labels that are biased to the source domain. The latter does not fully capture the intricate spatial relationships of objects -- key for dense prediction tasks. To this end, we propose LangDA. LangDA addresses these challenges by, first, learning contextual relationships between objects via VLM-generated scene descriptions (e.g. "a pedestrian is on the sidewalk, and the street is lined with buildings."). Second, LangDA aligns the entire image features with text representation of this context-aware scene caption and learns generalized representations via text. With this, LangDA sets the new state-of-the-art across three DASS benchmarks, outperforming existing methods by 2.6%, 1.4% and 3.9%.', 'abstract_zh': '无监督域适应Semantic分割（DASS）旨在将标记丰富的源域知识转移到无标签的目标域。DASS中的两种关键方法包括（1）仅视觉方法，使用遮罩或多分辨率剪辑，以及（2）基于语言的方法，使用由目标域启发的一般类别标签提示（例如，“一张{雪景}的{类别}照片”）。然而，前者容易受到偏向源域的噪声伪标签的影响。后者未能完全捕捉到对象的复杂空间关系——这对于密集预测任务至关重要。为了解决这些问题，我们提出了LangDA。LangDA通过首先利用VLM生成的场景描述学习对象之间的上下文关系（例如，“一个行人走在人行道上，街道两旁是建筑物。”），来解决这些挑战。其次，LangDA将整个图像特征与基于此上下文场景描述的文字表示对齐，并通过文字学习通用表示。通过这种方式，LangDA在三个DASS基准测试中设定了新的最佳性能，分别优于现有方法2.6%、1.4%和3.9%。', 'title_zh': 'LangDA: 通过语言构建上下文感知能力的领域自适应语义分割'}
{'arxiv_id': 'arXiv:2503.12617', 'title': 'Scaling Semantic Categories: Investigating the Impact on Vision Transformer Labeling Performance', 'authors': 'Anthony Lamelas, Harrison Muchnic', 'link': 'https://arxiv.org/abs/2503.12617', 'abstract': "This study explores the impact of scaling semantic categories on the image classification performance of vision transformers (ViTs). In this specific case, the CLIP server provided by Jina AI is used for experimentation. The research hypothesizes that as the number of ground truth and artificially introduced semantically equivalent categories increases, the labeling accuracy of ViTs improves until a theoretical maximum or limit is reached. A wide variety of image datasets were chosen to test this hypothesis. These datasets were processed through a custom function in Python designed to evaluate the model's accuracy, with adjustments being made to account for format differences between datasets. By exponentially introducing new redundant categories, the experiment assessed accuracy trends until they plateaued, decreased, or fluctuated inconsistently. The findings show that while semantic scaling initially increases model performance, the benefits diminish or reverse after surpassing a critical threshold, providing insight into the limitations and possible optimization of category labeling strategies for ViTs.", 'abstract_zh': '本研究探讨了扩大语义类别对视觉变换器（ViTs）图像分类性能的影响。具体而言，使用Jina AI提供的CLIP服务器进行实验。研究假设随着真实和人工引入的语义等价类别数量的增加，ViTs的标签准确性会提高，直至达到理论上的最大值或上限。多种图像数据集被选择以测试这一假设。这些数据集经过自定义的Python函数处理，以评估模型的准确性，并对不同数据集的格式差异进行了调整。通过指数引入新的冗余类别，实验评估了准确性趋势，直到它们趋于平稳、下降或波动不一致。研究发现，尽管语义扩展初期提高了模型性能，但在超过某一临界阈值后，其益处会减弱或逆转，为ViTs的类别标记策略的局限性和潜在优化提供了见解。', 'title_zh': '扩展语义类别：探究对视觉变换器标签性能的影响'}
{'arxiv_id': 'arXiv:2503.12595', 'title': 'Point Cloud Based Scene Segmentation: A Survey', 'authors': 'Dan Halperin, Niklas Eisl', 'link': 'https://arxiv.org/abs/2503.12595', 'abstract': 'Autonomous driving is a safety-critical application, and it is therefore a top priority that the accompanying assistance systems are able to provide precise information about the surrounding environment of the vehicle. Tasks such as 3D Object Detection deliver an insufficiently detailed understanding of the surrounding scene because they only predict a bounding box for foreground objects. In contrast, 3D Semantic Segmentation provides richer and denser information about the environment by assigning a label to each individual point, which is of paramount importance for autonomous driving tasks, such as navigation or lane changes. To inspire future research, in this review paper, we provide a comprehensive overview of the current state-of-the-art methods in the field of Point Cloud Semantic Segmentation for autonomous driving. We categorize the approaches into projection-based, 3D-based and hybrid methods. Moreover, we discuss the most important and commonly used datasets for this task and also emphasize the importance of synthetic data to support research when real-world data is limited. We further present the results of the different methods and compare them with respect to their segmentation accuracy and efficiency.', 'abstract_zh': '自主驾驶是一种安全关键型应用，因此，伴随的辅助系统必须能够提供关于车辆周围环境的精确信息。与3D物体检测相比，3D语义分割通过为每个独立点分配标签，提供了更加丰富和密集的环境信息，这对于自动驾驶任务如导航或换道至关重要。为了激励未来的研究，在这篇综述性论文中，我们提供了点云语义分割领域当前先进技术的全面概述。我们将方法分为投影型、3D型和混合型。此外，我们讨论了该任务中最重要的常用数据集，并强调在现实数据有限时，合成数据的重要性。我们进一步展示了不同方法的结果，并从分割准确性和效率的角度进行了比较。', 'title_zh': '基于点云的场景分割：一个综述'}
{'arxiv_id': 'arXiv:2503.12593', 'title': 'Fourier-Based 3D Multistage Transformer for Aberration Correction in Multicellular Specimens', 'authors': 'Thayer Alshaabi, Daniel E. Milkie, Gaoxiang Liu, Cyna Shirazinejad, Jason L. Hong, Kemal Achour, Frederik Görlitz, Ana Milunovic-Jevtic, Cat Simmons, Ibrahim S. Abuzahriyeh, Erin Hong, Samara Erin Williams, Nathanael Harrison, Evan Huang, Eun Seok Bae, Alison N. Killilea, David G. Drubin, Ian A. Swinburne, Srigokul Upadhyayula, Eric Betzig', 'link': 'https://arxiv.org/abs/2503.12593', 'abstract': 'High-resolution tissue imaging is often compromised by sample-induced optical aberrations that degrade resolution and contrast. While wavefront sensor-based adaptive optics (AO) can measure these aberrations, such hardware solutions are typically complex, expensive to implement, and slow when serially mapping spatially varying aberrations across large fields of view. Here, we introduce AOViFT (Adaptive Optical Vision Fourier Transformer) -- a machine learning-based aberration sensing framework built around a 3D multistage Vision Transformer that operates on Fourier domain embeddings. AOViFT infers aberrations and restores diffraction-limited performance in puncta-labeled specimens with substantially reduced computational cost, training time, and memory footprint compared to conventional architectures or real-space networks. We validated AOViFT on live gene-edited zebrafish embryos, demonstrating its ability to correct spatially varying aberrations using either a deformable mirror or post-acquisition deconvolution. By eliminating the need for the guide star and wavefront sensing hardware and simplifying the experimental workflow, AOViFT lowers technical barriers for high-resolution volumetric microscopy across diverse biological samples.', 'abstract_zh': '基于自适应光学视觉傅里叶变换的高分辨率组织成像', 'title_zh': '基于傅里叶变换的三维多阶段变压器在多细胞标本 aberration 矫正中的应用'}
{'arxiv_id': 'arXiv:2503.12575', 'title': 'BalancedDPO: Adaptive Multi-Metric Alignment', 'authors': 'Dipesh Tamboli, Souradip Chakraborty, Aditya Malusare, Biplab Banerjee, Amrit Singh Bedi, Vaneet Aggarwal', 'link': 'https://arxiv.org/abs/2503.12575', 'abstract': 'Text-to-image (T2I) diffusion models have made remarkable advancements, yet aligning them with diverse preferences remains a persistent challenge. Current methods often optimize single metrics or depend on narrowly curated datasets, leading to overfitting and limited generalization across key visual quality metrics. We present BalancedDPO, a novel extension of Direct Preference Optimization (DPO) that addresses these limitations by simultaneously aligning T2I diffusion models with multiple metrics, including human preference, CLIP score, and aesthetic quality. Our key novelty lies in aggregating consensus labels from diverse metrics in the preference distribution space as compared to existing reward mixing approaches, enabling robust and scalable multi-metric alignment while maintaining the simplicity of the standard DPO pipeline that we refer to as BalancedDPO. Our evaluations on the Pick-a-Pic, PartiPrompt and HPD datasets show that BalancedDPO achieves state-of-the-art results, outperforming existing approaches across all major metrics. BalancedDPO improves the average win rates by 15%, 7.1%, and 10.3% on Pick-a-pic, PartiPrompt and HPD, respectively, from the DiffusionDPO.', 'abstract_zh': 'Text-to-image (T2I) 演化模型取得了显著进步，但与多样化的偏好对齐仍然是一项持续的挑战。现有方法往往优化单一指标或依赖于狭隘定制的数据集，导致在关键视觉质量指标上的过拟合和有限泛化能力。我们提出了一种名为 BalancedDPO 的 Direct Preference Optimization (DPO) 的新型扩展方法，通过同时与多个指标对齐 T2I 演化模型，包括人类偏好、CLIP 分数和美学质量，来解决这些限制。我们的主要创新之处在于在偏好分布空间中汇总来自多种指标的共识标签，相比于现有的奖励混合方法，这使得多指标对齐更加稳健和可扩展，同时保持标准 DPO 管道的简单性，我们称之为 BalancedDPO。我们在 Pick-a-Pic、PartiPrompt 和 HPD 数据集上的评估显示，BalancedDPO 达到了最先进的效果，在所有主要指标上均优于现有方法。与 DiffusionDPO 相比，BalancedDPO 分别在 Pick-a-Pic、PartiPrompt 和 HPD 上的平均胜率提高了 15%、7.1% 和 10.3%。', 'title_zh': 'BalancedDPO: 自适应多指标对齐'}
{'arxiv_id': 'arXiv:2503.12572', 'title': 'Deblur Gaussian Splatting SLAM', 'authors': 'Francesco Girlanda, Denys Rozumnyi, Marc Pollefeys, Martin R. Oswald', 'link': 'https://arxiv.org/abs/2503.12572', 'abstract': 'We present Deblur-SLAM, a robust RGB SLAM pipeline designed to recover sharp reconstructions from motion-blurred inputs. The proposed method bridges the strengths of both frame-to-frame and frame-to-model approaches to model sub-frame camera trajectories that lead to high-fidelity reconstructions in motion-blurred settings. Moreover, our pipeline incorporates techniques such as online loop closure and global bundle adjustment to achieve a dense and precise global trajectory. We model the physical image formation process of motion-blurred images and minimize the error between the observed blurry images and rendered blurry images obtained by averaging sharp virtual sub-frame images. Additionally, by utilizing a monocular depth estimator alongside the online deformation of Gaussians, we ensure precise mapping and enhanced image deblurring. The proposed SLAM pipeline integrates all these components to improve the results. We achieve state-of-the-art results for sharp map estimation and sub-frame trajectory recovery both on synthetic and real-world blurry input data.', 'abstract_zh': '我们提出Deblur-SLAM，一种用于从运动模糊输入中恢复清晰重构的鲁棒RGB SLAM管道。该提出的方法结合了帧到帧和帧到模型方法的优点，用于建模亚帧相机轨迹，以在运动模糊场景中实现高保真重构。此外，我们的管道 Incorporates 技术如在线环闭合和全局束调整，以实现密集和精确的全局轨迹。我们建模了运动模糊图像的物理成像过程，并最小化了观测模糊图像与通过平均清晰虚拟亚帧图像获得的渲染模糊图像之间的误差。此外，通过利用单目深度估计器与高斯的在线变形，我们确保精确的映射和增强的图像去模糊。提出的SLAM管道将所有这些组件集成起来以提高结果。我们在这项工作中不仅在合成的，而且在真实的模糊输入数据中达到最先进的结果，用于清晰地图估计和亚帧轨迹恢复。', 'title_zh': '去模糊高斯体素SLAM'}
{'arxiv_id': 'arXiv:2503.12484', 'title': 'SING: Semantic Image Communications using Null-Space and INN-Guided Diffusion Models', 'authors': 'Jiakang Chen, Selim F. Yilmaz, Di You, Pier Luigi Dragotti, Deniz Gündüz', 'link': 'https://arxiv.org/abs/2503.12484', 'abstract': 'Joint source-channel coding systems based on deep neural networks (DeepJSCC) have recently demonstrated remarkable performance in wireless image transmission. Existing methods primarily focus on minimizing distortion between the transmitted image and the reconstructed version at the receiver, often overlooking perceptual quality. This can lead to severe perceptual degradation when transmitting images under extreme conditions, such as low bandwidth compression ratios (BCRs) and low signal-to-noise ratios (SNRs). In this work, we propose SING, a novel two-stage JSCC framework that formulates the recovery of high-quality source images from corrupted reconstructions as an inverse problem. Depending on the availability of information about the DeepJSCC encoder/decoder and the channel at the receiver, SING can either approximate the stochastic degradation as a linear transformation, or leverage invertible neural networks (INNs) for precise modeling. Both approaches enable the seamless integration of diffusion models into the reconstruction process, enhancing perceptual quality. Experimental results demonstrate that SING outperforms DeepJSCC and other approaches, delivering superior perceptual quality even under extremely challenging conditions, including scenarios with significant distribution mismatches between the training and test data.', 'abstract_zh': '基于深度神经网络的联合源信道编码系统（DeepJSCC）在无线图像传输中 recently demonstrated remarkable performance.现有的方法主要侧重于最小化传输图像和接收端重构版本之间的失真，往往忽视了感知质量。这可能导致在极端条件下（如低带宽压缩比和低信噪比）传输图像时感知质量严重下降。在本文中，我们提出了一种新颖的两阶段JSCC框架SING，将从受污染的重构中恢复高质量源图像的问题转化为逆问题。依据接收端关于DeepJSCC编码器/解码器及信道信息的可用性，SING可以将随机退化近似为线性变换，或者利用可逆神经网络（INNs）进行精确建模。这两种方法都使扩散模型能够无缝集成到恢复过程中，从而提高感知质量。实验结果表明，SING在包括训练数据和测试数据分布具有显著差异的极端条件下，优于DeepJSCC和其他方法，提供了更优的感知质量。', 'title_zh': 'SING: 基于 null-space 和 INN 引导扩散模型的_semantic_图像通信'}
{'arxiv_id': 'arXiv:2503.12447', 'title': 'Causality Model for Semantic Understanding on Videos', 'authors': 'Li Yicong', 'link': 'https://arxiv.org/abs/2503.12447', 'abstract': 'After a decade of prosperity, the development of video understanding has reached a critical juncture, where the sole reliance on massive data and complex architectures is no longer a one-size-fits-all solution to all situations. The presence of ubiquitous data imbalance hampers DNNs from effectively learning the underlying causal mechanisms, leading to significant performance drops when encountering distribution shifts, such as long-tail imbalances and perturbed imbalances. This realization has prompted researchers to seek alternative methodologies to capture causal patterns in video data. To tackle these challenges and increase the robustness of DNNs, causal modeling emerged as a principle to discover the true causal patterns behind the observed correlations. This thesis focuses on the domain of semantic video understanding and explores the potential of causal modeling to advance two fundamental tasks: Video Relation Detection (VidVRD) and Video Question Answering (VideoQA).', 'abstract_zh': '在经历了十年的繁荣发展之后，视频理解的开发已到达一个关键点，仅仅依赖大量数据和复杂架构已不再是一劳永逸的解决方案。普遍存在数据失衡阻碍了深度神经网络（DNNs）有效地学习潜在的因果机制，导致在遭遇分布转移，如长尾失衡和扰动失衡时性能显著下降。这一认识促使研究人员寻找新的方法来捕获视频数据中的因果模式。为解决这些挑战并提高DNNs的稳健性，因果建模作为一种原则应运而生，以发现观测关联背后的真正因果模式。本文集中于语义视频理解领域，探讨因果建模如何推进两项基本任务：视频关系检测（VidVRD）和视频问答（VideoQA）。', 'title_zh': '视频语义理解的因果模型'}
{'arxiv_id': 'arXiv:2503.12356', 'title': 'Localized Concept Erasure for Text-to-Image Diffusion Models Using Training-Free Gated Low-Rank Adaptation', 'authors': 'Byung Hyun Lee, Sungjin Lim, Se Young Chun', 'link': 'https://arxiv.org/abs/2503.12356', 'abstract': 'Fine-tuning based concept erasing has demonstrated promising results in preventing generation of harmful contents from text-to-image diffusion models by removing target concepts while preserving remaining concepts. To maintain the generation capability of diffusion models after concept erasure, it is necessary to remove only the image region containing the target concept when it locally appears in an image, leaving other regions intact. However, prior arts often compromise fidelity of the other image regions in order to erase the localized target concept appearing in a specific area, thereby reducing the overall performance of image generation. To address these limitations, we first introduce a framework called localized concept erasure, which allows for the deletion of only the specific area containing the target concept in the image while preserving the other regions. As a solution for the localized concept erasure, we propose a training-free approach, dubbed Gated Low-rank adaptation for Concept Erasure (GLoCE), that injects a lightweight module into the diffusion model. GLoCE consists of low-rank matrices and a simple gate, determined only by several generation steps for concepts without training. By directly applying GLoCE to image embeddings and designing the gate to activate only for target concepts, GLoCE can selectively remove only the region of the target concepts, even when target and remaining concepts coexist within an image. Extensive experiments demonstrated GLoCE not only improves the image fidelity to text prompts after erasing the localized target concepts, but also outperforms prior arts in efficacy, specificity, and robustness by large margin and can be extended to mass concept erasure.', 'abstract_zh': '基于细粒度概念消除的微调在预防文本到图像扩散模型生成有害内容方面表现出有 promising 的结果，通过移除目标概念同时保留其余概念。为在概念消除后维持扩散模型的生成能力，在图像中局部出现目标概念时仅需移除包含目标概念的图像区域，而不影响其他区域。然而，现有方法往往为了消除特定区域的局部目标概念而牺牲其他图像区域的保真度，从而降低了整体图像生成性能。为解决这些限制，我们首先引入了一种称为局部概念消除的框架，该框架允许仅删除图像中包含目标概念的特定区域，而不影响其他区域。为实现局部概念消除，我们提出了一种无需训练的解决方案，名为门控低秩适应的概念消除（GLoCE），该方法将一个轻量级模块注入扩散模型。GLoCE 由低秩矩阵和一个仅由几个生成步骤确定的简单门控机制组成。通过直接将 GLoCE 应用于图像嵌入并向门控机制设计只在目标概念时激活，GLoCE 可以仅选择性地移除目标概念区域，即使目标概念和剩余概念在图像中共存。广泛实验表明，GLoCE 不仅在移除局部目标概念后提高了图像对文本提示的保真度，还在有效性、特异性和鲁棒性方面显著优于现有方法，可以扩展到大规模概念消除。', 'title_zh': '基于训练-free门控低秩适应的局部概念擦除文本到图像扩散模型'}
{'arxiv_id': 'arXiv:2503.12307', 'title': 'Swift4D:Adaptive divide-and-conquer Gaussian Splatting for compact and efficient reconstruction of dynamic scene', 'authors': 'Jiahao Wu, Rui Peng, Zhiyan Wang, Lu Xiao, Luyang Tang, Jinbo Yan, Kaiqiang Xiong, Ronggang Wang', 'link': 'https://arxiv.org/abs/2503.12307', 'abstract': 'Novel view synthesis has long been a practical but challenging task, although the introduction of numerous methods to solve this problem, even combining advanced representations like 3D Gaussian Splatting, they still struggle to recover high-quality results and often consume too much storage memory and training time. In this paper we propose Swift4D, a divide-and-conquer 3D Gaussian Splatting method that can handle static and dynamic primitives separately, achieving a good trade-off between rendering quality and efficiency, motivated by the fact that most of the scene is the static primitive and does not require additional dynamic properties. Concretely, we focus on modeling dynamic transformations only for the dynamic primitives which benefits both efficiency and quality. We first employ a learnable decomposition strategy to separate the primitives, which relies on an additional parameter to classify primitives as static or dynamic. For the dynamic primitives, we employ a compact multi-resolution 4D Hash mapper to transform these primitives from canonical space into deformation space at each timestamp, and then mix the static and dynamic primitives to produce the final output. This divide-and-conquer method facilitates efficient training and reduces storage redundancy. Our method not only achieves state-of-the-art rendering quality while being 20X faster in training than previous SOTA methods with a minimum storage requirement of only 30MB on real-world datasets. Code is available at this https URL.', 'abstract_zh': '基于分而治之的3D高斯点云合成方法Swift4D', 'title_zh': 'Swift4D：自适应分而治之高斯点云化及其在动态场景紧凑高效重建中的应用'}
{'arxiv_id': 'arXiv:2503.12034', 'title': 'Real-Time Manipulation Action Recognition with a Factorized Graph Sequence Encoder', 'authors': 'Enes Erdogan, Eren Erdal Aksoy, Sanem Sariel', 'link': 'https://arxiv.org/abs/2503.12034', 'abstract': 'Recognition of human manipulation actions in real-time is essential for safe and effective human-robot interaction and collaboration. The challenge lies in developing a model that is both lightweight enough for real-time execution and capable of generalization. While some existing methods in the literature can run in real-time, they struggle with temporal scalability, i.e., they fail to adapt to long-duration manipulations effectively. To address this, leveraging the generalizable scene graph representations, we propose a new Factorized Graph Sequence Encoder network that not only runs in real-time but also scales effectively in the temporal dimension, thanks to its factorized encoder architecture. Additionally, we introduce Hand Pooling operation, a simple pooling operation for more focused extraction of the graph-level embeddings. Our model outperforms the previous state-of-the-art real-time approach, achieving a 14.3\\% and 5.6\\% improvement in F1-macro score on the KIT Bimanual Action (Bimacs) Dataset and Collaborative Action (CoAx) Dataset, respectively. Moreover, we conduct an extensive ablation study to validate our network design choices. Finally, we compare our model with its architecturally similar RGB-based model on the Bimacs dataset and show the limitations of this model in contrast to ours on such an object-centric manipulation dataset.', 'abstract_zh': '实时识别人类操作动作对于安全有效的机器人交互与协作至关重要。挑战在于开发一种既轻量级以支持实时执行又具有泛化能力的模型。虽然文献中的一些现有方法可以实时运行，但在时间维度的可扩展性上存在困难，即它们不能有效地适应长时间的操作。为此，我们利用可泛化的场景图表示，提出了一种新的因子化图序列编码网络，该网络不仅能够实时运行，还能够在时间维度上有效扩展，得益于其因子化的编码架构。此外，我们引入了手部聚合操作，这是一种更专注于图级嵌入提取的简单聚合并操作。我们的模型在KIT双臂动作（Bimacs）数据集和协作动作（CoAx）数据集上的F1-宏观分数上分别超过了之前的最先进实时方法14.3%和5.6%。此外，我们进行了广泛的消融研究以验证我们的网络设计选择。最后，我们在Bimacs数据集上将我们的模型与其架构相似的RGB基模型进行了比较，展示了该模型在这种以物体为中心的操作数据集上的局限性，相比之下，我们的模型表现更好。', 'title_zh': '实时操作动作识别的因子化图序列编码器'}
{'arxiv_id': 'arXiv:2503.12018', 'title': 'Compose Your Aesthetics: Empowering Text-to-Image Models with the Principles of Art', 'authors': 'Zhe Jin, Tat-Seng Chua', 'link': 'https://arxiv.org/abs/2503.12018', 'abstract': 'Text-to-Image (T2I) diffusion models (DM) have garnered widespread adoption due to their capability in generating high-fidelity outputs and accessibility to anyone able to put imagination into words. However, DMs are often predisposed to generate unappealing outputs, much like the random images on the internet they were trained on. Existing approaches to address this are founded on the implicit premise that visual aesthetics is universal, which is limiting. Aesthetics in the T2I context should be about personalization and we propose the novel task of aesthetics alignment which seeks to align user-specified aesthetics with the T2I generation output. Inspired by how artworks provide an invaluable perspective to approach aesthetics, we codify visual aesthetics using the compositional framework artists employ, known as the Principles of Art (PoA). To facilitate this study, we introduce CompArt, a large-scale compositional art dataset building on top of WikiArt with PoA analysis annotated by a capable Multimodal LLM. Leveraging the expressive power of LLMs and training a lightweight and transferrable adapter, we demonstrate that T2I DMs can effectively offer 10 compositional controls through user-specified PoA conditions. Additionally, we design an appropriate evaluation framework to assess the efficacy of our approach.', 'abstract_zh': '文本到图像（T2I）扩散模型（DM）因其生成高保真输出的能力和任何能够用文字表达想象的人都能使用的特点而广受欢迎。然而，DMs往往会生成不令人满意的输出，类似于它们训练过程中遇到的互联网上的随机图像。现有的解决方法基于视觉美学具有普遍性的隐含前提，这限制了它们的应用。在T2I的语境中，美学应该是个性化的，我们提出了一个新的任务——美学对齐，旨在将用户指定的美学与T2I生成的输出进行对齐。受艺术作品如何提供美学视角的影响，我们使用艺术家所使用的构图框架——艺术原则（PoA）来编码视觉美学。为了促进这项研究，我们引入了CompArt，一个基于WikiArt的大规模构图艺术数据集，并通过一个有能力的多模态LLM进行了PoA分析的注释。借助LLMs的表达能力并训练一个轻量级且可转移的适配器，我们证明了T2I DMs可以通过用户指定的PoA条件有效提供10个构图控制。此外，我们设计了一个合适的评估框架来评估我们方法的有效性。', 'title_zh': '构建你的美学：赋予文本-to-图像模型艺术原理'}
{'arxiv_id': 'arXiv:2503.11995', 'title': 'Fraesormer: Learning Adaptive Sparse Transformer for Efficient Food Recognition', 'authors': 'Shun Zou, Yi Zou, Mingya Zhang, Shipeng Luo, Zhihao Chen, Guangwei Gao', 'link': 'https://arxiv.org/abs/2503.11995', 'abstract': 'In recent years, Transformer has witnessed significant progress in food recognition. However, most existing approaches still face two critical challenges in lightweight food recognition: (1) the quadratic complexity and redundant feature representation from interactions with irrelevant tokens; (2) static feature recognition and single-scale representation, which overlook the unstructured, non-fixed nature of food images and the need for multi-scale features. To address these, we propose an adaptive and efficient sparse Transformer architecture (Fraesormer) with two core designs: Adaptive Top-k Sparse Partial Attention (ATK-SPA) and Hierarchical Scale-Sensitive Feature Gating Network (HSSFGN). ATK-SPA uses a learnable Gated Dynamic Top-K Operator (GDTKO) to retain critical attention scores, filtering low query-key matches that hinder feature aggregation. It also introduces a partial channel mechanism to reduce redundancy and promote expert information flow, enabling local-global collaborative modeling. HSSFGN employs gating mechanism to achieve multi-scale feature representation, enhancing contextual semantic information. Extensive experiments show that Fraesormer outperforms state-of-the-art methods. code is available at this https URL.', 'abstract_zh': '近年来，Transformer在食品识别方面取得了显著进展。然而，现有的轻量化食品识别方法仍面临两大关键挑战：(1) 与无关令牌交互带来的二次复杂度和冗余特征表示；(2) 静态特征识别和单尺度表示，忽视了食品图像的非结构化、非固定性质以及多尺度特征的需求。为应对这些挑战，我们提出了一种自适应高效的稀疏Transformer架构（Fraesormer），其包含两大核心设计：自适应Top-k稀疏部分注意机制（ATK-SPA）和层次化尺度敏感特征门控网络（HSSFGN）。ATK-SPA通过可学习的门控动态Top-K运算符（GDTKO）保留关键注意分数，过滤妨碍特征聚合的低匹配查询-键对，并引入部分通道机制以减少冗余并促进专家信息流，实现局部-全局协作建模。HSSFGN通过门控机制实现多尺度特征表示，增强上下文语义信息。大量实验表明，Fraesormer优于现有最先进的方法。代码可在此处获取。', 'title_zh': 'Fraesormer: 学习自适应稀疏变换器以实现高效的食品识别'}
{'arxiv_id': 'arXiv:2503.11937', 'title': 'Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder', 'authors': 'Wonwoong Cho, Yan-Ying Chen, Matthew Klenk, David I. Inouye, Yanxia Zhang', 'link': 'https://arxiv.org/abs/2503.11937', 'abstract': 'Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in generating high quality images. However, enabling precise control of continuous attributes, especially multiple attributes simultaneously, in a new domain (e.g., numeric values like eye openness or car width) with text-only guidance remains a significant challenge. To address this, we introduce the Attribute (Att) Adapter, a novel plug-and-play module designed to enable fine-grained, multi-attributes control in pretrained diffusion models. Our approach learns a single control adapter from a set of sample images that can be unpaired and contain multiple visual attributes. The Att-Adapter leverages the decoupled cross attention module to naturally harmonize the multiple domain attributes with text conditioning. We further introduce Conditional Variational Autoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the diverse nature of the visual world. Evaluations on two public datasets show that Att-Adapter outperforms all LoRA-based baselines in controlling continuous attributes. Additionally, our method enables a broader control range and also improves disentanglement across multiple attributes, surpassing StyleGAN-based techniques. Notably, Att-Adapter is flexible, requiring no paired synthetic data for training, and is easily scalable to multiple attributes within a single model.', 'abstract_zh': '文本到图像（T2I）扩散模型在生成高质量图像方面取得了显著性能。然而，仅通过文本指导在新领域（例如，如眼球睁开度或汽车宽度这样的数值值）中精确控制连续属性，尤其是同时控制多个属性，仍然是一个重大挑战。为此，我们引入了属性（Att）适配器，这是一种新型即插即用模块，设计用于在预训练扩散模型中启用细粒度、多属性控制。我们的方法从一组不配对的包含多个视觉属性的样本图像中学习一个单一的控制适配器。Att-Adapter 利用解耦交叉注意力模块自然地将多个领域属性与文本条件协调起来。我们进一步在Att-Adapter中引入条件变分自编码器（CVAE）以减轻过拟合，适应视觉世界的多样性质。在两个公开数据集上的评估显示，Att-Adapter 在控制连续属性方面优于所有基于LoRA的基本模型。此外，我们的方法能够实现更广泛的控制范围，并在多个属性上提高了分离度，超越了基于StyleGAN的技术。值得注意的是，Att-Adapter 具有灵活性，无需配对的合成数据即可进行训练，并且可以轻松扩展到单个模型内的多个属性。', 'title_zh': 'Att-Adapter: 一种通过条件变分自编码器实现的稳健而精确的领域特定多属性T2I扩散适配器'}
{'arxiv_id': 'arXiv:2503.11906', 'title': 'A Survey on SAR ship classification using Deep Learning', 'authors': 'Ch Muhammad Awais, Marco Reggiannini, Davide Moroni, Emanuele Salerno', 'link': 'https://arxiv.org/abs/2503.11906', 'abstract': 'Deep learning (DL) has emerged as a powerful tool for Synthetic Aperture Radar (SAR) ship classification. This survey comprehensively analyzes the diverse DL techniques employed in this domain. We identify critical trends and challenges, highlighting the importance of integrating handcrafted features, utilizing public datasets, data augmentation, fine-tuning, explainability techniques, and fostering interdisciplinary collaborations to improve DL model performance. This survey establishes a first-of-its-kind taxonomy for categorizing relevant research based on DL models, handcrafted feature use, SAR attribute utilization, and the impact of fine-tuning. We discuss the methodologies used in SAR ship classification tasks and the impact of different techniques. Finally, the survey explores potential avenues for future research, including addressing data scarcity, exploring novel DL architectures, incorporating interpretability techniques, and establishing standardized performance metrics. By addressing these challenges and leveraging advancements in DL, researchers can contribute to developing more accurate and efficient ship classification systems, ultimately enhancing maritime surveillance and related applications.', 'abstract_zh': '深学习（DL）已发展成为合成孔径雷达（SAR）船舶分类的强大工具。本文综述了该领域中 diverse DL 技术的多样应用，并分析了关键趋势和挑战，强调了集成手工艺特征、利用公开数据集、数据增强、微调、解释性技术以及促进跨学科合作以提高 DL 模型性能的重要性。本文建立了第一种基于 DL 模型、手工艺特征使用、SAR 属性利用以及微调影响的分类税onomic框架。本文讨论了 SAR 船舶分类任务中使用的不同方法和技术的影响。最后，本文探讨了未来研究的潜在方向，包括解决数据稀缺性、探索新的 DL 架构、结合可解释性技术以及建立标准化性能指标。通过应对这些挑战并利用 DL 的进步，研究人员可以贡献于开发更准确和高效的船舶分类系统，最终增强海上监视及相关应用。', 'title_zh': 'SAR船舶分类的深度学习综述'}
{'arxiv_id': 'arXiv:2503.11905', 'title': 'Upcycling Text-to-Image Diffusion Models for Multi-Task Capabilities', 'authors': 'Ruchika Chavhan, Abhinav Mehrotra, Malcolm Chadwick, Alberto Gil Ramos, Luca Morreale, Mehdi Noroozi, Sourav Bhattacharya', 'link': 'https://arxiv.org/abs/2503.11905', 'abstract': 'Text-to-image synthesis has witnessed remarkable advancements in recent years. Many attempts have been made to adopt text-to-image models to support multiple tasks. However, existing approaches typically require resource-intensive re-training or additional parameters to accommodate for the new tasks, which makes the model inefficient for on-device deployment. We propose Multi-Task Upcycling (MTU), a simple yet effective recipe that extends the capabilities of a pre-trained text-to-image diffusion model to support a variety of image-to-image generation tasks. MTU replaces Feed-Forward Network (FFN) layers in the diffusion model with smaller FFNs, referred to as experts, and combines them with a dynamic routing mechanism. To the best of our knowledge, MTU is the first multi-task diffusion modeling approach that seamlessly blends multi-tasking with on-device compatibility, by mitigating the issue of parameter inflation. We show that the performance of MTU is on par with the single-task fine-tuned diffusion models across several tasks including image editing, super-resolution, and inpainting, while maintaining similar latency and computational load (GFLOPs) as the single-task fine-tuned models.', 'abstract_zh': '多任务.upcycling（MTU）：一种简单有效的预训练文本到图像扩散模型多任务扩展方法', 'title_zh': '循环利用文本到图像扩散模型以实现多任务能力'}
{'arxiv_id': 'arXiv:2503.11851', 'title': 'DCAT: Dual Cross-Attention Fusion for Disease Classification in Radiological Images with Uncertainty Estimation', 'authors': 'Jutika Borah, Hidam Kumarjit Singh', 'link': 'https://arxiv.org/abs/2503.11851', 'abstract': 'Accurate and reliable image classification is crucial in radiology, where diagnostic decisions significantly impact patient outcomes. Conventional deep learning models tend to produce overconfident predictions despite underlying uncertainties, potentially leading to misdiagnoses. Attention mechanisms have emerged as powerful tools in deep learning, enabling models to focus on relevant parts of the input data. Combined with feature fusion, they can be effective in addressing uncertainty challenges. Cross-attention has become increasingly important in medical image analysis for capturing dependencies across features and modalities. This paper proposes a novel dual cross-attention fusion model for medical image analysis by addressing key challenges in feature integration and interpretability. Our approach introduces a bidirectional cross-attention mechanism with refined channel and spatial attention that dynamically fuses feature maps from EfficientNetB4 and ResNet34 leveraging multi-network contextual dependencies. The refined features through channel and spatial attention highlights discriminative patterns crucial for accurate classification. The proposed model achieved AUC of 99.75%, 100%, 99.93% and 98.69% and AUPR of 99.81%, 100%, 99.97%, and 96.36% on Covid-19, Tuberculosis, Pneumonia Chest X-ray images and Retinal OCT images respectively. The entropy values and several high uncertain samples give an interpretable visualization from the model enhancing transparency. By combining multi-scale feature extraction, bidirectional attention and uncertainty estimation, our proposed model strongly impacts medical image analysis.', 'abstract_zh': '准确可靠的医学图像分类对于放射学至关重要，其中诊断决策显著影响患者预后。传统的深度学习模型往往会对带有潜在不确定性的预测过于自信，这可能导致误诊。注意力机制已成为深度学习中的强大工具，能够使模型专注于输入数据的相关部分。结合特征融合，它们可以在应对不确定性挑战时发挥有效作用。交叉注意力在医学图像分析中变得越来越重要，用于捕捉特征和模态之间的依赖关系。本文提出了一种新颖的双向交叉注意力融合模型，通过解决特征集成和可解释性中的关键挑战，用于医学图像分析。该方法引入了一种基于EfficientNetB4和ResNet34的双向交叉注意力机制，并利用多网络上下文依赖性动态融合特征图。通过通道和空间注意力精炼特征，突显了对于准确分类至关重要的判别模式。该提出的模型在COVID-19、肺结核、肺炎胸片图像和视网膜OCT图像上的AUC分别为99.75%、100%、99.93%和98.69%，AUPR分别为99.81%、100%、99.97%和96.36%。熵值和多个高不确定样本提供了可解释的可视化效果，增强透明度。通过结合多尺度特征提取、双向注意力和不确定性估计，我们提出的模型对医学图像分析产生了重大影响。', 'title_zh': 'DCAT：双交叉注意力融合在放射影像疾病分类中的不确定性估计'}
{'arxiv_id': 'arXiv:2503.11846', 'title': 'From Pixels to Histopathology: A Graph-Based Framework for Interpretable Whole Slide Image Analysis', 'authors': 'Alexander Weers, Alexander H. Berger, Laurin Lux, Peter Schüffler, Daniel Rueckert, Johannes C. Paetzold', 'link': 'https://arxiv.org/abs/2503.11846', 'abstract': "The histopathological classification of whole-slide images (WSIs) is a fundamental task in digital pathology; yet it requires extensive time and expertise from specialists. While deep learning methods show promising results, they typically process WSIs by dividing them into artificial patches, which inherently prevents a network from learning from the entire image context, disregards natural tissue structures and compromises interpretability. Our method overcomes this limitation through a novel graph-based framework that constructs WSI graph representations. The WSI-graph efficiently captures essential histopathological information in a compact form. We build tissue representations (nodes) that follow biological boundaries rather than arbitrary patches all while providing interpretable features for explainability. Through adaptive graph coarsening guided by learned embeddings, we progressively merge regions while maintaining discriminative local features and enabling efficient global information exchange. In our method's final step, we solve the diagnostic task through a graph attention network. We empirically demonstrate strong performance on multiple challenging tasks such as cancer stage classification and survival prediction, while also identifying predictive factors using Integrated Gradients. Our implementation is publicly available at this https URL", 'abstract_zh': '全slide图像（WSI）的组织病理学分类是数字病理学中的一个基本任务；但需要大量时间和专家知识。虽然深度学习方法显示出前景，但它们通常通过将WSI划分为人工片段来处理图像，这固有地防止网络从整个图像上下文中学习，忽视了自然组织结构并损害可解释性。我们的方法通过一种新型的图基构建框架克服了这一限制，构造了WSI图表示。WSI图以紧凑的形式高效地捕捉重要组织病理学信息。我们构建了遵循生物边界组织表示（节点），提供了可解释特征以增强可解释性。通过由学习嵌入引导的自适应图粗化，我们逐步合并区域，同时保持区分局部特征并实现高效的全局信息交换。在我们方法的最后一歩，我们通过图注意力网络解决诊断任务。我们在多个具有挑战性的任务中，如癌症分期分类和生存预测，展示了强大的性能，并使用集成梯度识别预测因素。我们的实现已公开发布在以下网址。', 'title_zh': '从像素到病理组织：一种基于图的可解释全视野图像分析框架'}
{'arxiv_id': 'arXiv:2503.11794', 'title': 'Semantic-Clipping: Efficient Vision-Language Modeling with Semantic-Guidedd Visual Selection', 'authors': 'Bangzheng Li, Fei Wang, Wenxuan Zhou, Nan Xu, Ben Zhou, Sheng Zhang, Hoifung Poon, Muhao Chen', 'link': 'https://arxiv.org/abs/2503.11794', 'abstract': 'Vision-Language Models (VLMs) leverage aligned visual encoders to transform images into visual tokens, allowing them to be processed similarly to text by the backbone large language model (LLM). This unified input paradigm enables VLMs to excel in vision-language tasks such as visual question answering (VQA). To improve fine-grained visual reasoning, recent advancements in vision-language modeling introduce image cropping techniques that feed all encoded sub-images into the model. However, this approach significantly increases the number of visual tokens, leading to inefficiency and potential distractions for the LLM. To address the generalization challenges of image representation in VLMs, we propose a lightweight, universal framework that seamlessly integrates with existing VLMs to enhance their ability to process finegrained details. Our method leverages textual semantics to identify key visual areas, improving VQA performance without requiring any retraining of the VLM. Additionally, it incorporates textual signals into the visual encoding process, enhancing both efficiency and effectiveness. The proposed method, SEMCLIP, strengthens the visual understanding of a 7B VLM, LLaVA-1.5 by 3.3% on average across 7 benchmarks, and particularly by 5.3% on the challenging detailed understanding benchmark V*.', 'abstract_zh': 'Vision-Language模型（VLMs）利用对齐的视觉编码器将图像转换为视觉标记，使其能够像文本一样被骨干大规模语言模型（LLM）处理。这种统一的输入范式使VLMs在视觉问答（VQA）等视觉-语言任务中表现出色。为了改进精细视觉推理，最近的视觉-语言建模进展引入了图像裁剪技术，将所有编码的子图像输入到模型中。然而，这种方法大幅增加了视觉标记的数量，导致效率低下并可能分散LLM的注意力。为了解决视觉表示在VLMs中的泛化挑战，我们提出了一种轻量级的通用框架，可以无缝集成到现有的VLMs中，增强其处理细粒度细节的能力。该方法利用文本语义识别关键视觉区域，无需对VLM进行任何重新训练即可提高VQA性能。此外，该方法还将文本信号融入视觉编码过程中，提高了效率和有效性。所提出的方法SEMCLIP在7个基准测试中平均将7B VLM LLaVA-1.5的视觉理解能力增强3.3%，特别是在具有挑战性的详细理解基准测试V*中提高了5.3%。', 'title_zh': '语义剪枝：基于语义导向视觉选择的高效视觉-语言建模'}
{'arxiv_id': 'arXiv:2503.11720', 'title': 'Fine-Tuning Diffusion Generative Models via Rich Preference Optimization', 'authors': 'Hanyang Zhao, Haoxian Chen, Yucheng Guo, Genta Indra Winata, Tingting Ou, Ziyu Huang, David D. Yao, Wenpin Tang', 'link': 'https://arxiv.org/abs/2503.11720', 'abstract': 'We introduce Rich Preference Optimization (RPO), a novel pipeline that leverages rich feedback signals to improve the curation of preference pairs for fine-tuning text-to-image diffusion models. Traditional methods, like Diffusion-DPO, often rely solely on reward model labeling, which can be opaque, offer limited insights into the rationale behind preferences, and are prone to issues such as reward hacking or overfitting. In contrast, our approach begins with generating detailed critiques of synthesized images to extract reliable and actionable image editing instructions. By implementing these instructions, we create refined images, resulting in synthetic, informative preference pairs that serve as enhanced tuning datasets. We demonstrate the effectiveness of our pipeline and the resulting datasets in fine-tuning state-of-the-art diffusion models.', 'abstract_zh': 'Rich Preference Optimization: 一种利用丰富反馈信号改进文本到图像扩散模型微调偏序对策管stype', 'title_zh': '通过丰富的偏好优化 fine-tuning 扩散生成模型'}
