{'arxiv_id': 'arXiv:2507.07017', 'title': 'First Return, Entropy-Eliciting Explore', 'authors': 'Tianyu Zheng, Tianshun Xing, Qingshui Gu, Taoran Liang, Xingwei Qu, Xin Zhou, Yizhi Li, Zhoufutu Wen, Chenghua Lin, Wenhao Huang, Qian Liu, Ge Zhang, Zejun Ma', 'link': 'https://arxiv.org/abs/2507.07017', 'abstract': "Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning abilities of Large Language Models (LLMs) but it struggles with unstable exploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a structured exploration framework that identifies high-uncertainty decision points in reasoning trajectories and performs targeted rollouts to construct semantically grounded intermediate feedback. Our method provides targeted guidance without relying on dense supervision. Empirical results on mathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable training, produces longer and more coherent responses, and increases the proportion of fully correct trajectories. These results highlight the framework's effectiveness in improving LLM reasoning through more robust and structured exploration.", 'abstract_zh': '可验证奖励强化学习（RLVR）提高了大型语言模型（LLMs）的推理能力，但面临着不稳定探索的挑战。我们提出了一种结构化探索框架FR3E（首次返回，熵激发探索），该框架识别推理轨迹中的高不确定性决策点，并进行有针对性的 rollout，构建语义上连贯的中间反馈。该方法提供有针对性的指导，而不依赖密集的监督。在数学推理基准测试（AIME24）上的实验证明，FR3E 促进了更稳定的训练，产生了更长且更连贯的回答，并增加了完全正确的轨迹的比例。这些结果突显了该框架通过更稳健和结构化的探索来提高LLM推理能力的有效性。', 'title_zh': '首次返回, 信息探索性搜索'}
{'arxiv_id': 'arXiv:2507.06993', 'title': 'The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced Planning, Navigation, and Dynamic Adaptation', 'authors': 'Jieren Deng, Aleksandar Cvetkovic, Pak Kiu Chung, Dragomir Yankov, Chiqun Zhang', 'link': 'https://arxiv.org/abs/2507.06993', 'abstract': 'Traditional travel-planning systems are often static and fragmented, leaving them ill-equipped to handle real-world complexities such as evolving environmental conditions and unexpected itinerary disruptions. In this paper, we identify three gaps between existing service providers causing frustrating user experience: intelligent trip planning, precision "last-100-meter" navigation, and dynamic itinerary adaptation. We propose three cooperative agents: a Travel Planning Agent that employs grid-based spatial grounding and map analysis to help resolve complex multi-modal user queries; a Destination Assistant Agent that provides fine-grained guidance for the final navigation leg of each journey; and a Local Discovery Agent that leverages image embeddings and Retrieval-Augmented Generation (RAG) to detect and respond to trip plan disruptions. With evaluations and experiments, our system demonstrates substantial improvements in query interpretation, navigation accuracy, and disruption resilience, underscoring its promise for applications from urban exploration to emergency response.', 'abstract_zh': '现有的旅行规划系统往往静态且碎片化，难以应对诸如环境变化和行程意外中断等现实世界复杂性。本文识别了现有服务提供商之间的三个差距，导致用户体验不佳：智能行程规划、精准“最后100米”导航以及动态行程适应。我们提出了三种协作代理：旅行规划代理，利用基于网格的空间定位和地图分析来解决复杂的多模态用户查询；目的地助手代理，为每段旅程的最终导航提供精细指导；本地发现代理，利用图像嵌入和检索增强生成（RAG）来检测和响应行程计划中断。通过评估和实验，我们的系统在查询解释、导航精度和应对中断能力方面显示出显著改进，证明了其在从城市探索到应急响应等应用中的潜力。', 'title_zh': '用户导向的地理体验：一种由LLM驱动的增强规划、导航和动态适应框架'}
{'arxiv_id': 'arXiv:2507.06396', 'title': 'Representing Prompting Patterns with PDL: Compliance Agent Case Study', 'authors': 'Mandana Vaziri, Louis Mandel, Yuji Watanabe, Hirokuni Kitahara, Martin Hirzel, Anca Sailer', 'link': 'https://arxiv.org/abs/2507.06396', 'abstract': "Prompt engineering for LLMs remains complex, with existing frameworks either hiding complexity behind restrictive APIs or providing inflexible canned patterns that resist customization -- making sophisticated agentic programming challenging. We present the Prompt Declaration Language (PDL), a novel approach to prompt representation that tackles this fundamental complexity by bringing prompts to the forefront, enabling manual and automatic prompt tuning while capturing the composition of LLM calls together with rule-based code and external tools. By abstracting away the plumbing for such compositions, PDL aims at improving programmer productivity while providing a declarative representation that is amenable to optimization. This paper demonstrates PDL's utility through a real-world case study of a compliance agent. Tuning the prompting pattern of this agent yielded up to 4x performance improvement compared to using a canned agent and prompt pattern.", 'abstract_zh': 'LLM范例工程 remains complex, with existing frameworks either hiding complexity behind restrictive APIs or providing inflexible canned patterns that resist customization -- making sophisticated agentic programming challenging. We present the Prompt Declaration Language (PDL), a novel approach to prompt representation that tackles this fundamental complexity by bringing prompts to the forefront, enabling manual and automatic prompt tuning while capturing the composition of LLM calls together with rule-based code and external tools. By abstracting away the plumbing for such compositions, PDL aims at improving programmer productivity while providing a declarative representation that is amenable to optimization. This paper demonstrates PDL’s utility through a real-world case study of a compliance agent. Tuning the prompting pattern of this agent yielded up to 4x performance improvement compared to using a canned agent and prompt pattern.', 'title_zh': '使用PDL表示提示模式：合规代理案例研究'}
{'arxiv_id': 'arXiv:2507.07024', 'title': 'FlexOlmo: Open Language Models for Flexible Data Use', 'authors': 'Weijia Shi, Akshita Bhagia, Kevin Farhat, Niklas Muennighoff, Pete Walsh, Jacob Morrison, Dustin Schwenk, Shayne Longpre, Jake Poznanski, Allyson Ettinger, Daogao Liu, Margaret Li, Dirk Groeneveld, Mike Lewis, Wen-tau Yih, Luca Soldaini, Kyle Lo, Noah A. Smith, Luke Zettlemoyer, Pang Wei Koh, Hannaneh Hajishirzi, Ali Farhadi, Sewon Min', 'link': 'https://arxiv.org/abs/2507.07024', 'abstract': "We introduce FlexOlmo, a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. We evaluate models with up to 37 billion parameters (20 billion active) on 31 diverse downstream tasks. We show that a general expert trained on public data can be effectively combined with independently trained experts from other data owners, leading to an average 41% relative improvement while allowing users to opt out of certain data based on data licensing or permission requirements. Our approach also outperforms prior model merging methods by 10.1% on average and surpasses the standard MoE trained without data restrictions using the same training FLOPs. Altogether, this research presents a solution for both data owners and researchers in regulated industries with sensitive or protected data. FlexOlmo enables benefiting from closed data while respecting data owners' preferences by keeping their data local and supporting fine-grained control of data access during inference.", 'abstract_zh': 'FlexOlmo：支持分布式训练和数据灵活推理的新语言模型', 'title_zh': 'FlexOlmo: 开放语言模型的灵活数据使用'}
{'arxiv_id': 'arXiv:2507.06992', 'title': 'MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology Report Generation', 'authors': 'Qilong Xing, Zikai Song, Youjia Zhang, Na Feng, Junqing Yu, Wei Yang', 'link': 'https://arxiv.org/abs/2507.06992', 'abstract': 'Despite significant advancements in adapting Large Language Models (LLMs) for radiology report generation (RRG), clinical adoption remains challenging due to difficulties in accurately mapping pathological and anatomical features to their corresponding text descriptions. Additionally, semantic agnostic feature extraction further hampers the generation of accurate diagnostic reports. To address these challenges, we introduce Medical Concept Aligned Radiology Report Generation (MCA-RG), a knowledge-driven framework that explicitly aligns visual features with distinct medical concepts to enhance the report generation process. MCA-RG utilizes two curated concept banks: a pathology bank containing lesion-related knowledge, and an anatomy bank with anatomical descriptions. The visual features are aligned with these medical concepts and undergo tailored enhancement. We further propose an anatomy-based contrastive learning procedure to improve the generalization of anatomical features, coupled with a matching loss for pathological features to prioritize clinically relevant regions. Additionally, a feature gating mechanism is employed to filter out low-quality concept features. Finally, the visual features are corresponding to individual medical concepts, and are leveraged to guide the report generation process. Experiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate that MCA-RG achieves superior performance, highlighting its effectiveness in radiology report generation.', 'abstract_zh': '面向医学概念对齐的放射报告生成（MCA-RG）：一种知识驱动的框架以增强放射报告生成过程', 'title_zh': 'MCA-RG: 通过医学概念对齐增强LLMs的放射报告生成'}
{'arxiv_id': 'arXiv:2507.06909', 'title': 'MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal Prediction', 'authors': 'Xiao Wang, Jiahuan Pei, Diancheng Shui, Zhiguang Han, Xin Sun, Dawei Zhu, Xiaoyu Shen', 'link': 'https://arxiv.org/abs/2507.06909', 'abstract': 'Legal judgment prediction offers a compelling method to aid legal practitioners and researchers. However, the research question remains relatively under-explored: Should multiple defendants and charges be treated separately in LJP? To address this, we introduce a new dataset namely multi-person multi-charge prediction (MPMCP), and seek the answer by evaluating the performance of several prevailing legal large language models (LLMs) on four practical legal judgment scenarios: (S1) single defendant with a single charge, (S2) single defendant with multiple charges, (S3) multiple defendants with a single charge, and (S4) multiple defendants with multiple charges. We evaluate the dataset across two LJP tasks, i.e., charge prediction and penalty term prediction. We have conducted extensive experiments and found that the scenario involving multiple defendants and multiple charges (S4) poses the greatest challenges, followed by S2, S3, and S1. The impact varies significantly depending on the model. For example, in S4 compared to S1, InternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD, while Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD. Our dataset and code are available at this https URL.', 'abstract_zh': '多被告人多罪名法律判决预测：一个新的研究视角与实证分析', 'title_zh': '多司法：一个多当事人、多罪名的中文法律预测数据集'}
{'arxiv_id': 'arXiv:2507.06892', 'title': 'Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model', 'authors': 'Jing Liang, Hongyao Tang, Yi Ma, Jinyi Liu, Yan Zheng, Shuyue Hu, Lei Bai, Jianye Hao', 'link': 'https://arxiv.org/abs/2507.06892', 'abstract': "Reinforcement Learning (RL) has demonstrated its potential to improve the reasoning ability of Large Language Models (LLMs). One major limitation of most existing Reinforcement Finetuning (RFT) methods is that they are on-policy RL in nature, i.e., data generated during the past learning process is not fully utilized. This inevitably comes at a significant cost of compute and time, posing a stringent bottleneck on continuing economic and efficient scaling. To this end, we launch the renaissance of off-policy RL and propose Reincarnating Mix-policy Proximal Policy Gradient (ReMix), a general approach to enable on-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix consists of three major components: (1) Mix-policy proximal policy gradient with an increased Update-To-Data (UTD) ratio for efficient training; (2) KL-Convex policy constraint to balance the trade-off between stability and flexibility; (3) Policy reincarnation to achieve a seamless transition from efficient early-stage learning to steady asymptotic improvement. In our experiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base models. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with 0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B model) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math reasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and MATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level performance with an over 30x to 450x reduction in training cost in terms of rollout data volume. In addition, we reveal insightful findings via multifaceted analysis, including the implicit preference for shorter responses due to the Whipping Effect of off-policy discrepancy, the collapse mode of self-reflection behavior under the presence of severe off-policyness, etc.", 'abstract_zh': "强化学习（RL）在提升大型语言模型（LLMs）的推理能力方面展现了其潜力。大多数现有的强化调优（RFT）方法的主要局限是它们本质上是在线策略RL，即过去学习过程中生成的数据未被充分利用。这不可避免地导致了显著的计算和时间成本增加，成为继续经济高效扩展的严苛瓶颈。为此，我们重启了离线策略RL的复兴，并提出了Reincarnating Mix-policy Proximal Policy Gradient（ReMix），一种使如PPO和GRPO等在线策略RFT方法能够利用离线数据的通用方法。ReMix包括三大组成部分：（1）具有增加的Update-To-Data（UTD）比的Mix-policy近端策略梯度，以实现高效的训练；（2）KL-凸规划策约束，以平衡稳定性和灵活性之间的贸易关系；（3）策略转世，以实现从高效的初始阶段学习到稳定渐进改进的无缝过渡。在我们的实验中，我们基于PPO、GRPO和1.5B、7B基模型训练了一系列ReMix模型。对于1.5B模型，ReMix展示了52.10%的平均Pass@1准确率，使用了0.079M响应回放、350训练步骤；对于7B模型，ReMix达到了63.27%和64.39%的准确率，使用了0.007M/0.011M响应回放、50/75训练步骤，这在五个数学推理基准（即AIME'24、AMC'23、Minerva、OlympiadBench和MATH500）上进行了测试。与最近15个先进模型相比，ReMix在回放数据量方面展示了SOTA水平的表现，训练成本减少了30至450倍。此外，我们通过多方面的分析揭示了有趣的发现，包括由于离线偏差引起的鞭打效应偏好较短的响应，以及在严重离策略情况下的自我反思行为崩溃模式等。", 'title_zh': '挤干浸湿的海绵：高效的离策 reinforcement fine-tuningfor 大型语言模型'}
{'arxiv_id': 'arXiv:2507.06850', 'title': 'The Dark Side of LLMs Agent-based Attacks for Complete Computer Takeover', 'authors': 'Matteo Lupinacci, Francesco Aurelio Pironti, Francesco Blefari, Francesco Romeo, Luigi Arena, Angelo Furfaro', 'link': 'https://arxiv.org/abs/2507.06850', 'abstract': 'The rapid adoption of Large Language Model (LLM) agents and multi-agent systems enables unprecedented capabilities in natural language processing and generation. However, these systems have introduced unprecedented security vulnerabilities that extend beyond traditional prompt injection attacks. This paper presents the first comprehensive evaluation of LLM agents as attack vectors capable of achieving complete computer takeover through the exploitation of trust boundaries within agentic AI systems where autonomous entities interact and influence each other. We demonstrate that adversaries can leverage three distinct attack surfaces - direct prompt injection, RAG backdoor attacks, and inter-agent trust exploitation - to coerce popular LLMs (including GPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing malware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals an alarming vulnerability hierarchy: while 41.2% of models succumb to direct prompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical 82.4% can be compromised through inter-agent trust exploitation. Notably, we discovered that LLMs which successfully resist direct malicious commands will execute identical payloads when requested by peer agents, revealing a fundamental flaw in current multi-agent security models. Our findings demonstrate that only 5.9% of tested models (1/17) proved resistant to all attack vectors, with the majority exhibiting context-dependent security behaviors that create exploitable blind spots. Our findings also highlight the need to increase awareness and research on the security risks of LLMs, showing a paradigm shift in cybersecurity threats, where AI tools themselves become sophisticated attack vectors.', 'abstract_zh': '大型语言模型代理和多代理系统的快速采纳为自然语言处理和生成提供了前所未有的能力。然而，这些系统引入了超出传统提示注入攻击的前所未有的安全漏洞。本文首次全面评估了大型语言模型代理作为攻击向量的能力，这些代理通过利用代理型人工智能系统中自主实体之间的信任边界来实现对整个计算机系统的完全接管。我们展示了对手可以利用三种不同的攻击面——直接提示注入、RAG后门攻击和代理间信任利用——来迫使流行的大型语言模型（包括GPT-4o、Claude-4和Gemini-2.5）自主安装和执行恶意软件。对17个最先进的大型语言模型的评估揭示了一个令人惊讶的漏洞层级：41.2%的模型对直接提示注入易受攻击，52.9%的模型对RAG后门攻击易受攻击，而至关重要的82.4%的模型可以通过代理间信任利用被妥协。值得注意的是，我们发现那些成功抵制直接恶意命令的大型语言模型在接到其他代理请求时会执行相同的负载，揭示了当前多代理安全模型中的根本缺陷。我们的研究结果表明，只有5.9%（1/17）的测试模型对所有攻击向量都具有抵抗力，大多数模型表现出上下文相关的安全行为，这些行为创造了可利用的安全盲点。我们的研究结果还强调了提高对大型语言模型安全风险的认识和研究的必要性，显示出网络安全威胁的一个 paradigm shift，即AI工具本身也成为复杂的攻击向量。', 'title_zh': 'LLMs的暗面：基于代理的攻击对完全计算机接管'}
{'arxiv_id': 'arXiv:2507.06803', 'title': 'Text to model via SysML: Automated generation of dynamical system computational models from unstructured natural language text via enhanced System Modeling Language diagrams', 'authors': 'Matthew Anderson Hendricks, Alice Cicirello', 'link': 'https://arxiv.org/abs/2507.06803', 'abstract': 'This paper contributes to speeding up the design and deployment of engineering dynamical systems by proposing a strategy for exploiting domain and expert knowledge for the automated generation of dynamical system computational model starting from a corpus of document relevant to the dynamical system of interest and an input document describing the specific system. This strategy is implemented in five steps and, crucially, it uses system modeling language diagrams (SysML) to extract accurate information about the dependencies, attributes, and operations of components. Natural Language Processing (NLP) strategies and Large Language Models (LLMs) are employed in specific tasks to improve intermediate outputs of the SySML diagrams automated generation, such as: list of key nouns; list of extracted relationships; list of key phrases and key relationships; block attribute values; block relationships; and BDD diagram generation. The applicability of automated SysML diagram generation is illustrated with different case studies. The computational models of complex dynamical systems from SysML diagrams are then obtained via code generation and computational model generation steps. In the code generation step, NLP strategies are used for summarization, while LLMs are used for validation only. The proposed approach is not limited to a specific system, domain, or computational software. The applicability of the proposed approach is shown via an end-to-end example from text to model of a simple pendulum, showing improved performance compared to results yielded by LLMs only.', 'abstract_zh': '本文通过提议一种策略，利用领域和专家知识自动生成动态系统计算模型，从而加速工程动态系统的设计和部署，该策略基于与感兴趣的动态系统相关的文档集和描述特定系统的输入文档实施。该策略分为五个步骤，并且关键的是，它使用系统建模语言图表（SysML）来提取关于组件之间的依赖关系、属性和操作的准确信息。通过自然语言处理（NLP）策略和大型语言模型（LLMs）在特定任务中的应用，改进了SysML图表自动生成的中间输出，如关键名词列表；提取的关系列表；关键短语和关键关系列表；模块属性值；模块关系；以及BDD图表生成。自动化SysML图表生成的应用性通过不同的案例研究进行了说明。通过代码生成和计算模型生成步骤，从SysML图表中获得了复杂动态系统的计算模型。在代码生成步骤中，NLP策略用于总结，而LLMs仅用于验证。所提出的方法不限于特定的系统、领域或计算软件。所提出的方法的适用性通过从文本到简单摆锤模型的端到端示例进行了展示，显示出与仅由LLMs生成的结果相比的改进性能。', 'title_zh': '通过SysML将文本转换为模型：基于增强的System Modeling Language图 автомат化生成动态系统计算模型从非结构化的自然语言文本'}
{'arxiv_id': 'arXiv:2507.06795', 'title': 'Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining: Method, Evaluation and Applications', 'authors': 'Seonwu Kim, Yohan Na, Kihun Kim, Hanhee Cho, Geun Lim, Mintae Kim, Seongik Park, Ki Hyun Kim, Youngsub Han, Byoung-Ki Jeon', 'link': 'https://arxiv.org/abs/2507.06795', 'abstract': 'The emergence of open-source large language models (LLMs) has expanded opportunities for enterprise applications; however, many organizations still lack the infrastructure to deploy and maintain large-scale models. As a result, small LLMs (sLLMs) have become a practical alternative, despite their inherent performance limitations. While Domain Adaptive Continual Pretraining (DACP) has been previously explored as a method for domain adaptation, its utility in commercial applications remains under-examined. In this study, we validate the effectiveness of applying a DACP-based recipe across diverse foundation models and service domains. Through extensive experiments and real-world evaluations, we demonstrate that DACP-applied sLLMs achieve substantial gains in target domain performance while preserving general capabilities, offering a cost-efficient and scalable solution for enterprise-level deployment.', 'abstract_zh': '开源大型语言模型的出现扩展了企业应用的机会；然而，许多组织仍然缺乏部署和维护大规模模型的基础设施。因此，小型语言模型（sLLMs）成为一种实用的替代方案，尽管它们存在固有的性能限制。虽然领域适应连续预训练（DACP）曾作为一种领域适应的方法被探索过，但其在商业应用中的实用性仍然尚未充分 examination。在本研究中，我们验证了基于 DACP 的方法在不同基础模型和服务领域中的有效性。通过广泛的实验和实际评价，我们证明了应用 DACP 的 sLLMs 在目标领域的性能取得了显著提升，同时保留了一般能力，提供了一种成本效益高且可扩展的面向企业的部署解决方案。', 'title_zh': '通过领域适应连续预训练提高工业领域的sLLMs效率：方法、评估与应用'}
{'arxiv_id': 'arXiv:2507.06734', 'title': 'Civil Society in the Loop: Feedback-Driven Adaptation of (L)LM-Assisted Classification in an Open-Source Telegram Monitoring Tool', 'authors': 'Milena Pustet, Elisabeth Steffen, Helena Mihaljević, Grischa Stanjek, Yannis Illies', 'link': 'https://arxiv.org/abs/2507.06734', 'abstract': "The role of civil society organizations (CSOs) in monitoring harmful online content is increasingly crucial, especially as platform providers reduce their investment in content moderation. AI tools can assist in detecting and monitoring harmful content at scale. However, few open-source tools offer seamless integration of AI models and social media monitoring infrastructures. Given their thematic expertise and contextual understanding of harmful content, CSOs should be active partners in co-developing technological tools, providing feedback, helping to improve models, and ensuring alignment with stakeholder needs and values, rather than as passive 'consumers'. However, collaborations between the open source community, academia, and civil society remain rare, and research on harmful content seldom translates into practical tools usable by civil society actors. This work in progress explores how CSOs can be meaningfully involved in an AI-assisted open-source monitoring tool of anti-democratic movements on Telegram, which we are currently developing in collaboration with CSO stakeholders.", 'abstract_zh': '民间社会组织（CSOs）在监控有害网络内容中的作用日益重要，尤其是在平台提供商减少内容审核投资的情况下。AI工具可以协助大规模检测和监控有害内容。然而，很少有开源工具能够无缝集成AI模型和社会媒体监控基础设施。鉴于CSOs在主题专业知识和有害内容情境理解方面的优势，他们应作为积极合作伙伴，在共同开发技术工具、提供反馈、改进模型以及确保符合利益相关方需求和价值观方面发挥作用，而非仅仅作为被动的“消费者”。然而，开源社区、学术界与民间社会组织之间的合作仍然很少见，有关有害内容的研究很少能够转化为可被民间社会组织使用的实际工具。本研究旨在探索CSOs如何在我们正在与CSO利益相关方合作开发的一个用于监控Telegram上的反民主运动的AI辅助开源监控工具中实现有意义的参与。', 'title_zh': '公民社会在环中: 反馈驱动的（L）LM辅助分类在开源电报监控工具中的自适应调整'}
{'arxiv_id': 'arXiv:2507.06715', 'title': 'CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and Context Aware Text Generation with LLMs', 'authors': 'Garapati Keerthana, Manik Gupta', 'link': 'https://arxiv.org/abs/2507.06715', 'abstract': 'Large language models (LLMs), including zero-shot and few-shot paradigms, have shown promising capabilities in clinical text generation. However, real-world applications face two key challenges: (1) patient data is highly unstructured, heterogeneous, and scattered across multiple note types and (2) clinical notes are often long and semantically dense, making naive prompting infeasible due to context length constraints and the risk of omitting clinically relevant information.\nWe introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a domain-specific framework for structured and clinically grounded text generation using LLMs. It incorporates a novel hierarchical chunking strategy that respects clinical document structure and introduces a task-specific dual-stage retrieval mechanism. The global stage identifies relevant note types using evidence-based queries, while the local stage extracts high-value content within those notes creating relevance at both document and section levels.\nWe apply the system to generate structured progress notes for individual hospital visits using 15 clinical note types from the MIMIC-III dataset. Experiments show that it preserves temporal and semantic alignment across visits, achieving an average alignment score of 87.7%, surpassing the 80.7% baseline from real clinician-authored notes. The generated outputs also demonstrate high consistency across LLMs, reinforcing deterministic behavior essential for reproducibility, reliability, and clinical trust.', 'abstract_zh': '基于临床指导的检索增强生成框架（CLI-RAG）：面向临床文本结构化生成的领域特定框架', 'title_zh': 'CLI-RAG：一种基于检索增强的临床结构化和上下文感知文本生成框架（使用大型语言模型）'}
{'arxiv_id': 'arXiv:2507.06674', 'title': 'Exploring State-Space-Model based Language Model in Music Generation', 'authors': 'Wei-Jaw Lee, Fang-Chih Hsieh, Xuanjun Chen, Fang-Duo Tsai, Yi-Hsuan Yang', 'link': 'https://arxiv.org/abs/2507.06674', 'abstract': 'The recent surge in State Space Models (SSMs), particularly the emergence of Mamba, has established them as strong alternatives or complementary modules to Transformers across diverse domains. In this work, we aim to explore the potential of Mamba-based architectures for text-to-music generation. We adopt discrete tokens of Residual Vector Quantization (RVQ) as the modeling representation and empirically find that a single-layer codebook can capture semantic information in music. Motivated by this observation, we focus on modeling a single-codebook representation and adapt SiMBA, originally designed as a Mamba-based encoder, to function as a decoder for sequence modeling. We compare its performance against a standard Transformer-based decoder. Our results suggest that, under limited-resource settings, SiMBA achieves much faster convergence and generates outputs closer to the ground truth. This demonstrates the promise of SSMs for efficient and expressive text-to-music generation. We put audio examples on Github.', 'abstract_zh': '最近State Space Models（SSMs）的崛起，尤其是Mamba的出现，已确立它们作为Transformer在多个领域中的有力替代方案或补充模块的地位。本工作旨在探索基于Mamba的架构在文本到音乐生成中的潜力。我们采用残差矢量量化（RVQ）的离散令牌作为建模表示，并实验证明单层码本可以捕捉音乐中的语义信息。受此观察的启发，我们将重点放在建模单码本表示上，并将最初作为Mamba基编码器设计的SiMBA适应性地转化为序列建模的解码器。我们将其性能与标准的Transformer基解码器进行比较。结果显示，在资源受限的设置下，SiMBA在收敛速度上更快，并生成与真实值更接近的输出。这表明SSMs在高效且富有表现力的文本到音乐生成中具有潜力。我们在Github上提供了音频示例。', 'title_zh': '基于状态空间模型的语言生成在音乐创作中的探索'}
{'arxiv_id': 'arXiv:2507.06658', 'title': 'Elite Polarization in European Parliamentary Speeches: a Novel Measurement Approach Using Large Language Models', 'authors': 'Gennadii Iakovlev', 'link': 'https://arxiv.org/abs/2507.06658', 'abstract': 'This project introduces a new measure of elite polarization via actor and subject detection using artificial intelligence. I identify when politicians mention one another in parliamentary speeches, note who is speaking and who is being addressed, and assess the emotional temperature behind these evaluations. This maps how elites evaluate their various out-parties, allowing us to create an index of mutual out-party hostility, that is, elite polarization. While I analyzed polarization data over the past four decades for the UK, and two decades for Hungary and Italy, my approach lays the groundwork for a twenty-year, EU-wide time-series dataset on elite polarization. I obtain the results that can be aggregated by party and quarter. The resulting index demonstrates a good face validity: it reacts to events such as electoral campaigns, country- and party-level crises, and to parties losing and assuming power.', 'abstract_zh': '本研究通过使用人工智能进行行为者和主题检测，提出了一种新的精英极化衡量方法。我识别出政治家在议会演讲中提到彼此的情况，记录发言者和被提及者，并评估这些评价背后的情感温度。这有助于映射精英对其各种反对派的评价方式，从而构建一个互对方针敌对指数，即精英极化。我分析了过去四十年的英国、以及过去二十年的匈牙利和意大利的极化数据，并提出了一个二十年期的欧盟范围内的精英极化时间序列数据集。结果可以按政党和季度进行汇总。所得指数具有良好的表面效度：它对诸如选举运动、国家和政党层面的危机以及政党失去和获得权力等事件作出反应。', 'title_zh': '欧洲议会演讲中的精英极化：一种新型的大语言模型测量方法'}
{'arxiv_id': 'arXiv:2507.06623', 'title': 'Expediting data extraction using a large language model (LLM) and scoping review protocol: a methodological study within a complex scoping review', 'authors': 'James Stewart-Evans, Emma Wilson, Tessa Langley, Andrew Prayle, Angela Hands, Karen Exley, Jo Leonardi-Bee', 'link': 'https://arxiv.org/abs/2507.06623', 'abstract': 'The data extraction stages of reviews are resource-intensive, and researchers may seek to expediate data extraction using online (large language models) LLMs and review protocols. Claude 3.5 Sonnet was used to trial two approaches that used a review protocol to prompt data extraction from 10 evidence sources included in a case study scoping review. A protocol-based approach was also used to review extracted data. Limited performance evaluation was undertaken which found high accuracy for the two extraction approaches (83.3% and 100%) when extracting simple, well-defined citation details; accuracy was lower (9.6% and 15.8%) when extracting more complex, subjective data items. Considering all data items, both approaches had precision >90% but low recall (<25%) and F1 scores (<40%). The context of a complex scoping review, open response types and methodological approach likely impacted performance due to missed and misattributed data. LLM feedback considered the baseline extraction accurate and suggested minor amendments: four of 15 (26.7%) to citation details and 8 of 38 (21.1%) to key findings data items were considered to potentially add value. However, when repeating the process with a dataset featuring deliberate errors, only 2 of 39 (5%) errors were detected. Review-protocol-based methods used for expediency require more robust performance evaluation across a range of LLMs and review contexts with comparison to conventional prompt engineering approaches. We recommend researchers evaluate and report LLM performance if using them similarly to conduct data extraction or review extracted data. LLM feedback contributed to protocol adaptation and may assist future review protocol drafting.', 'abstract_zh': '基于审查协议的数据提取阶段耗时且资源密集，研究者可能寻求通过在线大型语言模型（LLM）和审查协议来加速数据提取。使用Claude 3.5 Sonnet试验了两种方法，这些方法利用审查协议从案例研究范围审查中包含的10个证据来源中提示数据提取。还使用基于审查协议的方法审查提取的数据。进行了有限的性能评估，发现两种提取方法在提取简单明确的引用细节时准确率高（分别为83.3%和100%），但在提取更复杂、主观的数据项时准确率较低（分别为9.6%和15.8%）。考虑到所有数据项，两种方法的精确度均超过90%，但召回率较低（均低于25%），F1分数较低（均低于40%）。复杂范围审查的背景、开放式的回答类型和方法学方法可能影响了性能，导致数据被遗漏或错误归因。LLM反馈认为基线提取准确，并建议进行细微调整：15个中4个（26.7%）是关于引用细节的，38个中8个（21.1%）是关于关键发现的数据项可能增加价值。然而，在使用包含故意错误的数据集重复此过程时，仅检测到39个中2个（5%）的错误。为了提高效率而使用的基于审查协议的方法需要在一系列LLM和审查背景下进行更 robust 的性能评估，并与传统提示工程技术方法进行比较。我们建议研究人员在使用LLM进行数据提取或审查提取数据时评估和报告LLM性能。LLM反馈有助于审查协议的适应，并可能有助于未来审查协议的起草。', 'title_zh': '使用大规模语言模型（LLM）和范围审查协议加速数据提取：复杂范围审查中的方法学研究'}
{'arxiv_id': 'arXiv:2507.06573', 'title': 'From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via Progressive Optimization', 'authors': 'Xinjie Chen, Minpeng Liao, Guoxin Chen, Chengxi Li, Biao Fu, Kai Fan, Xinggao Liu', 'link': 'https://arxiv.org/abs/2507.06573', 'abstract': "Reinforcement learning with verifiable rewards (RLVR) has recently advanced the reasoning capabilities of large language models (LLMs). While prior work has emphasized algorithmic design, data curation, and reward shaping, we investigate RLVR from a sample-centric perspective and introduce LPPO (Learning-Progress and Prefix-guided Optimization), a framework of progressive optimization techniques. Our work addresses a critical question: how to best leverage a small set of trusted, high-quality demonstrations, rather than simply scaling up data volume. First, motivated by how hints aid human problem-solving, we propose prefix-guided sampling, an online data augmentation method that incorporates partial solution prefixes from expert demonstrations to guide the policy, particularly for challenging instances. Second, inspired by how humans focus on important questions aligned with their current capabilities, we introduce learning-progress weighting, a dynamic strategy that adjusts each training sample's influence based on model progression. We estimate sample-level learning progress via an exponential moving average of per-sample pass rates, promoting samples that foster learning and de-emphasizing stagnant ones. Experiments on mathematical-reasoning benchmarks demonstrate that our methods outperform strong baselines, yielding faster convergence and a higher performance ceiling.", 'abstract_zh': '可验证奖励的强化学习（RLVR） recently advanced 大型语言模型（LLMs）的推理能力。从采样中心视角探究 RLVR 并引入 LPPO（Learning-Progress 和 Prefix-guided 优化）框架：渐进优化技术。我们的研究解决了一个关键问题：如何最好地利用少量可靠的高质量示范，而不是简单地扩大数据量。首先，受到提示有助于人类问题解决的启发，我们提出了 prefix-guided 抽样，这是一种在线数据增强方法，通过结合专家示范中的部分解题前缀来引导策略，特别是对于具有挑战性的实例。其次，受到人类关注与其当前能力相符的重要问题的启发，我们引入了 learning-progress 加权，这是一种动态策略，根据模型进展调整每个训练样本的影响。我们通过每个样本通过率的指数移动平均来估计样本级的学习进展，促进有利于学习的样本并淡化停滞的样本。在数学推理基准测试中的实验表明，我们的方法优于强基准方法，实现了更快的收敛并提高了性能上限。', 'title_zh': '从数据为中心到样本为中心：通过渐进优化增强大规模语言模型推理能力'}
{'arxiv_id': 'arXiv:2507.06558', 'title': 'The Primacy of Magnitude in Low-Rank Adaptation', 'authors': 'Zicheng Zhang, Haoran Li, Yifeng Zhang, Guoqiang Gong, Jiaxing Wang, Pengzhang Liu, Qixia Jiang, Junxing Hu', 'link': 'https://arxiv.org/abs/2507.06558', 'abstract': 'Low-Rank Adaptation (LoRA) offers a parameter-efficient paradigm for tuning large models. While recent spectral initialization methods improve convergence and performance over the naive "Noise & Zeros" scheme, their extra computational and storage overhead undermines efficiency. In this paper, we establish update magnitude as the fundamental driver of LoRA performance and propose LoRAM, a magnitude-driven "Basis & Basis" initialization scheme that matches spectral methods without their inefficiencies. Our key contributions are threefold: (i) Magnitude of weight updates determines convergence. We prove low-rank structures intrinsically bound update magnitudes, unifying hyperparameter tuning in learning rate, scaling factor, and initialization as mechanisms to optimize magnitude regulation. (ii) Spectral initialization succeeds via magnitude amplification. We demystify that the presumed knowledge-driven benefit of the spectral component essentially arises from the boost in the weight update magnitude. (iii) A novel and compact initialization strategy, LoRAM, scales deterministic orthogonal bases using pretrained weight magnitudes to simulate spectral gains. Extensive experiments show that LoRAM serves as a strong baseline, retaining the full efficiency of LoRA while matching or outperforming spectral initialization across benchmarks.', 'abstract_zh': 'LoRA的幅度驱动初始化方案：LoRAM及其优越性', 'title_zh': '低秩适应中的幅度优先性'}
{'arxiv_id': 'arXiv:2507.06528', 'title': 'InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior', 'authors': 'Huisheng Wang, Zhuoshi Pan, Hangjing Zhang, Mingxiao Liu, Hanqing Gao, H. Vicky Zhao', 'link': 'https://arxiv.org/abs/2507.06528', 'abstract': 'Aligning Large Language Models (LLMs) with investor decision-making processes under herd behavior is a critical challenge in behavioral finance, which grapples with a fundamental limitation: the scarcity of real-user data needed for Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM outputs and human behavioral patterns, its reliance on massive authentic data imposes substantial collection costs and privacy risks. We propose InvestAlign, a novel framework that constructs high-quality SFT datasets by leveraging theoretical solutions to similar and simple optimal investment problems rather than complex scenarios. Our theoretical analysis demonstrates that training LLMs with InvestAlign-generated data achieves faster parameter convergence than using real-user data, suggesting superior learning efficiency. Furthermore, we develop InvestAgent, an LLM agent fine-tuned with InvestAlign, which demonstrates significantly closer alignment to real-user data than pre-SFT models in both simple and complex investment problems. This highlights our proposed InvestAlign as a promising approach with the potential to address complex optimal investment problems and align LLMs with investor decision-making processes under herd behavior. Our code is publicly available at this https URL.', 'abstract_zh': '利用理论最优投资解决方案构建高质量监督微调数据集以缓解大型语言模型与投资者决策过程在羊群行为下的不一致性：一种行为金融中的关键挑战', 'title_zh': 'InvestAlign：克服 herd 行为下投资决策过程与大规模语言模型对齐中的数据稀缺性问题'}
{'arxiv_id': 'arXiv:2507.06520', 'title': 'Gradientsys: A Multi-Agent LLM Scheduler with ReAct Orchestration', 'authors': 'Xinyuan Song, Zeyu Wang, Siyi Wu, Tianyu Shi, Lynn Ai', 'link': 'https://arxiv.org/abs/2507.06520', 'abstract': 'We present Gradientsys, a next-generation multi-agent scheduling framework that coordinates diverse specialized AI agents using a typed Model-Context Protocol (MCP) and a ReAct-based dynamic planning loop. At its core, Gradientsys employs an LLM-powered scheduler for intelligent one-to-many task dispatch, enabling parallel execution of heterogeneous agents such as PDF parsers, web search modules, GUI controllers, and web builders. The framework supports hybrid synchronous/asynchronous execution, respects agent capacity constraints, and incorporates a robust retry-and-replan mechanism to handle failures gracefully. To promote transparency and trust, Gradientsys includes an observability layer streaming real-time agent activity and intermediate reasoning via Server-Sent Events (SSE). We offer an architectural overview and evaluate Gradientsys against existing frameworks in terms of extensibility, scheduling topology, tool reusability, parallelism, and observability. Experiments on the GAIA general-assistant benchmark show that Gradientsys achieves higher task success rates with reduced latency and lower API costs compared to a MinionS-style baseline, demonstrating the strength of its LLM-driven multi-agent orchestration.', 'abstract_zh': 'Gradientsys：一种基于类型化模型-上下文协议和ReAct动态规划环的下一代多Agent调度框架', 'title_zh': 'Gradientsys: 一个多代理LLM调度器带ReAct编排'}
{'arxiv_id': 'arXiv:2507.06512', 'title': 'Towards LLM-based Root Cause Analysis of Hardware Design Failures', 'authors': 'Siyu Qiu, Muzhi Wang, Raheel Afsharmazayejani, Mohammad Moradi Shahmiri, Benjamin Tan, Hammond Pearce', 'link': 'https://arxiv.org/abs/2507.06512', 'abstract': "With advances in large language models (LLMs), new opportunities have emerged to develop tools that support the digital hardware design process. In this work, we explore how LLMs can assist with explaining the root cause of design issues and bugs that are revealed during synthesis and simulation, a necessary milestone on the pathway towards widespread use of LLMs in the hardware design process and for hardware security analysis. We find promising results: for our corpus of 34 different buggy scenarios, OpenAI's o3-mini reasoning model reached a correct determination 100% of the time under pass@5 scoring, with other state of the art models and configurations usually achieving more than 80% performance and more than 90% when assisted with retrieval-augmented generation.", 'abstract_zh': '随着大规模语言模型（LLMs）的进步，出现了开发支持数字硬件设计过程的新工具的机会。在本研究中，我们探讨了LLMs如何在综合和仿真过程中协助解释设计问题和故障的根本原因，这是走向在硬件设计过程和硬件安全分析中广泛使用LLMs的必要里程碑。我们取得了令人鼓舞的结果：对于34种不同的故障场景，OpenAI的o3-mini推理模型在pass@5评分下的正确判定率为100%，其他最先进的模型和配置通常实现超过80%的性能，而在检索增强生成的帮助下，这一性能通常超过90%。', 'title_zh': '基于大规模语言模型的硬件设计失效根本原因分析'}
{'arxiv_id': 'arXiv:2507.06507', 'title': 'GR-LLMs: Recent Advances in Generative Recommendation Based on Large Language Models', 'authors': 'Zhen Yang, Haitao Lin, Jiawei xue, Ziji Zhang', 'link': 'https://arxiv.org/abs/2507.06507', 'abstract': 'In the past year, Generative Recommendations (GRs) have undergone substantial advancements, especially in leveraging the powerful sequence modeling and reasoning capabilities of Large Language Models (LLMs) to enhance overall recommendation performance. LLM-based GRs are forming a new paradigm that is distinctly different from discriminative recommendations, showing strong potential to replace traditional recommendation systems heavily dependent on complex hand-crafted features. In this paper, we provide a comprehensive survey aimed at facilitating further research of LLM-based GRs. Initially, we outline the general preliminaries and application cases of LLM-based GRs. Subsequently, we introduce the main considerations when LLM-based GRs are applied in real industrial scenarios. Finally, we explore promising directions for LLM-based GRs. We hope that this survey contributes to the ongoing advancement of the GR domain.', 'abstract_zh': '过去一年中，生成性推荐（GRs）取得了显著进展，特别是在利用大型语言模型（LLMs）的强大序列建模和推理能力提升整体推荐性能方面。基于LLM的GRs形成了与判别性推荐截然不同的新范式，展现出替代高度依赖复杂手工特征的传统推荐系统的强大潜力。本文旨在对基于LLM的GRs进行全面综述，以促进相关研究。首先，我们概述了基于LLM的GRs的一般预备知识和应用案例。随后，介绍了在实际工业场景中应用基于LLM的GRs时需要考虑的主要因素。最后，探讨了基于LLM的GRs的发展方向。我们希望这篇综述能够推动生成性推荐领域的发展。', 'title_zh': 'GR-LLMs: 大语言模型驱动的生成性推荐 Recent Advances'}
{'arxiv_id': 'arXiv:2507.06506', 'title': 'Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings', 'authors': 'Russell Taylor, Benjamin Herbert, Michael Sana', 'link': 'https://arxiv.org/abs/2507.06506', 'abstract': "Translating wordplay across languages presents unique challenges that have long confounded both professional human translators and machine translation systems. This research proposes a novel approach for translating puns from English to French by combining state-of-the-art large language models with specialized techniques for wordplay generation.\nOur methodology employs a three-stage approach. First, we establish a baseline using multiple frontier large language models with feedback based on a new contrastive learning dataset. Second, we implement a guided chain-of-thought pipeline with combined phonetic-semantic embeddings. Third, we implement a multi-agent generator-discriminator framework for evaluating and regenerating puns with feedback.\nMoving beyond the limitations of literal translation, our methodology's primary objective is to capture the linguistic creativity and humor of the source text wordplay, rather than simply duplicating its vocabulary. Our best runs earned first and second place in the CLEF JOKER 2025 Task 2 competition where they were evaluated manually by expert native French speakers.\nThis research addresses a gap between translation studies and computational linguistics by implementing linguistically-informed techniques for wordplay translation, advancing our understanding of how language models can be leveraged to handle the complex interplay between semantic ambiguity, phonetic similarity, and the implicit cultural and linguistic awareness needed for successful humor.", 'abstract_zh': '跨语言翻译字面游戏 presents unique challenges that have long confounded both professional human translators and machine translation systems. This research proposes a novel approach for translating puns from English to French by combining state-of-the-art large language models with specialized techniques for wordplay generation.', 'title_zh': '妙趣横生：基于对比学习和音义嵌入的多agent词语玩法规则翻译'}
{'arxiv_id': 'arXiv:2507.06323', 'title': 'Bridging AI and Software Security: A Comparative Vulnerability Assessment of LLM Agent Deployment Paradigms', 'authors': 'Tarek Gasmi, Ramzi Guesmi, Ines Belhadj, Jihene Bennaceur', 'link': 'https://arxiv.org/abs/2507.06323', 'abstract': 'Large Language Model (LLM) agents face security vulnerabilities spanning AI-specific and traditional software domains, yet current research addresses these separately. This study bridges this gap through comparative evaluation of Function Calling architecture and Model Context Protocol (MCP) deployment paradigms using a unified threat classification framework. We tested 3,250 attack scenarios across seven language models, evaluating simple, composed, and chained attacks targeting both AI-specific threats (prompt injection) and software vulnerabilities (JSON injection, denial-of-service). Function Calling showed higher overall attack success rates (73.5% vs 62.59% for MCP), with greater system-centric vulnerability while MCP exhibited increased LLM-centric exposure. Attack complexity dramatically amplified effectiveness, with chained attacks achieving 91-96% success rates. Counterintuitively, advanced reasoning models demonstrated higher exploitability despite better threat detection. Results demonstrate that architectural choices fundamentally reshape threat landscapes. This work establishes methodological foundations for cross-domain LLM agent security assessment and provides evidence-based guidance for secure deployment. Code and experimental materials are available at https: // github. com/ theconsciouslab-ai/llm-agent-security.', 'abstract_zh': '大型语言模型（LLM）代理在AI特定和传统软件领域面临安全漏洞，当前研究对此分别进行探讨。本研究通过使用统一的威胁分类框架对比评估功能调用架构和模型上下文协议（MCP）部署范式的安全性，弥合了这一差距。我们在七个语言模型中测试了3250个攻击场景，评估了针对AI特定威胁（提示注入）和软件漏洞（JSON注入、服务拒绝）的简单、组合和链式攻击。功能调用的整体攻击成功率较高（73.5%比MCP的62.59%），显示出更大的系统中心化漏洞，而MCP则表现出更高的LLM中心化曝露。攻击复杂性极大地提升了效果，链式攻击的成功率达到了91-96%。令人意想不到的是，高级推理模型在显示出更高利用性的同时，对威胁检测更为有效。研究成果表明，架构选择从根本上重塑了威胁景观。本研究为跨域LLM代理安全评估奠定了方法论基础，并提供了基于证据的指导，以实现安全部署。相关代码和实验材料可在https://github.com/theconsciouslab-ai/llm-agent-security获得。', 'title_zh': '人工智能与软件安全融合：大规模语言模型代理部署范式的相对漏洞评估'}
{'arxiv_id': 'arXiv:2507.06310', 'title': 'Too Human to Model:The Uncanny Valley of LLMs in Social Simulation -- When Generative Language Agents Misalign with Modelling Principles', 'authors': 'Yongchao Zeng, Calum Brown, Mark Rounsevell', 'link': 'https://arxiv.org/abs/2507.06310', 'abstract': 'Large language models (LLMs) have been increasingly used to build agents in social simulation because of their impressive abilities to generate fluent, contextually coherent dialogues. Such abilities can enhance the realism of models. However, the pursuit of realism is not necessarily compatible with the epistemic foundation of modelling. We argue that LLM agents, in many regards, are too human to model: they are too expressive, detailed and intractable to be consistent with the abstraction, simplification, and interpretability typically demanded by modelling. Through a model-building thought experiment that converts the Bass diffusion model to an LLM-based variant, we uncover five core dilemmas: a temporal resolution mismatch between natural conversation and abstract time steps; the need for intervention in conversations while avoiding undermining spontaneous agent outputs; the temptation to introduce rule-like instructions in prompts while maintaining conversational naturalness; the tension between role consistency and role evolution across time; and the challenge of understanding emergence, where system-level patterns become obscured by verbose micro textual outputs. These dilemmas steer the LLM agents towards an uncanny valley: not abstract enough to clarify underlying social mechanisms, while not natural enough to represent realistic human behaviour. This exposes an important paradox: the realism of LLM agents can obscure, rather than clarify, social dynamics when misapplied. We tease out the conditions in which LLM agents are ideally suited: where system-level emergence is not the focus, linguistic nuances and meaning are central, interactions unfold in natural time, and stable role identity is more important than long-term behavioural evolution. We call for repositioning LLM agents in the ecosystem of social simulation for future applications.', 'abstract_zh': '大型语言模型（LLMs）在社会模拟中的应用因其生成流畅且上下文连贯对话的能力而日益增多。这种能力能够增强模型的现实感。然而，追求现实感并不一定与建模的哲学基础相兼容。我们argue认为，LLM代理在许多方面过于人性化，难以与建模通常要求的抽象化、简化和可解释性保持一致。通过将巴拉斯扩散模型转化为基于LLM的变体的建模思想实验，我们揭示了五个核心困境：自然对话与抽象时间步骤之间的时间分辨率不匹配；在干预对话的同时避免削弱代理自发输出的需求；在保持对话自然性的同时引入规则性的指令；角色一致性与随时间变化的角色发展之间的张力；以及理解涌现的挑战，其中系统级模式被冗长的微观文本输出所掩盖。这些困境将LLM代理推向了拟真谷：既不够抽象以澄清潜在的社会机制，也不够自然以代表现实的人类行为。这揭示了一个重要的悖论：当LLM代理被误用时，其现实感反而会模糊社会动态而非澄清它们。我们阐述了LLM代理最合适的条件，在这些条件下，系统级的涌现不是重点，语言细微差别和意义是中心，交互在自然时间中展开，并且稳定的角色身份比长期行为演变更为重要。我们呼吁在社会模拟的生态系统中重新定位LLM代理，以期未来应用。', 'title_zh': '难以模型化的太人性的方面：生成语言代理在社会模拟中的谷�第一季度现象——当生成语言代理与建模原则产生偏差时'}
{'arxiv_id': 'arXiv:2507.06306', 'title': 'Humans overrely on overconfident language models, across languages', 'authors': 'Neil Rathi, Dan Jurafsky, Kaitlyn Zhou', 'link': 'https://arxiv.org/abs/2507.06306', 'abstract': "As large language models (LLMs) are deployed globally, it is crucial that their responses are calibrated across languages to accurately convey uncertainty and limitations. Previous work has shown that LLMs are linguistically overconfident in English, leading users to overrely on confident generations. However, the usage and interpretation of epistemic markers (e.g., 'It's definitely,' 'I think') can differ sharply across languages. Here, we study the risks of multilingual linguistic (mis)calibration, overconfidence, and overreliance across five languages to evaluate the safety of LLMs in a global context.\nWe find that overreliance risks are high across all languages. We first analyze the distribution of LLM-generated epistemic markers, and observe that while LLMs are cross-linguistically overconfident, they are also sensitive to documented linguistic variation. For example, models generate the most markers of uncertainty in Japanese and the most markers of certainty in German and Mandarin. We then measure human reliance rates across languages, finding that while users strongly rely on confident LLM generations in all languages, reliance behaviors differ cross-linguistically: for example, users rely significantly more on expressions of uncertainty in Japanese than in English. Taken together, these results indicate high risk of reliance on overconfident model generations across languages. Our findings highlight the challenges of multilingual linguistic calibration and stress the importance of culturally and linguistically contextualized model safety evaluations.", 'abstract_zh': '随着大规模语言模型在全球范围内的部署，校准其跨语言响应以准确传达不确定性与限制至关重要。我们研究了五种语言中多语言语言（误）校准、过度自信及过度依赖的风险，以评估全球范围内大型语言模型的安全性。我们发现，所有语言中的过度依赖风险都很高。我们首先分析了LLM生成的表征不确定性标记的分布，观察到虽然LLM在跨语言使用中表现出过度自信，但它们对记录的语言变化也非常敏感。例如，模型在日本生成了最多的不确定性标记，在德国和 Mandarin 中生成了最多的确定性标记。然后我们衡量了不同语言中人类的依赖率，发现虽然用户在所有语言中都强烈依赖于LLM的生成内容，但依赖行为在不同语言中有所不同：例如，用户对不确定性的表达在日本的依赖程度明显高于英语。综上所述，这些结果表明语言之间过度自信模型生成的内容存在较高的依赖风险。我们的研究突出了多语言语言校准的挑战，并强调了进行文化与语言背景下的模型安全性评估的重要性。', 'title_zh': '人类过度依赖于语言模型的高枕无忧表述，跨越语言界限。'}
{'arxiv_id': 'arXiv:2507.06282', 'title': 'The bitter lesson of misuse detection', 'authors': 'Hadrien Mariaccia, Charbel-Raphaël Segerie, Diego Dorn', 'link': 'https://arxiv.org/abs/2507.06282', 'abstract': 'Prior work on jailbreak detection has established the importance of adversarial robustness for LLMs but has largely focused on the model ability to resist adversarial inputs and to output safe content, rather than the effectiveness of external supervision systems. The only public and independent benchmark of these guardrails to date evaluates a narrow set of supervisors on limited scenarios. Consequently, no comprehensive public benchmark yet verifies how well supervision systems from the market perform under realistic, diverse attacks. To address this, we introduce BELLS, a Benchmark for the Evaluation of LLM Supervision Systems. The framework is two dimensional: harm severity (benign, borderline, harmful) and adversarial sophistication (direct vs. jailbreak) and provides a rich dataset covering 3 jailbreak families and 11 harm categories. Our evaluations reveal drastic limitations of specialized supervision systems. While they recognize some known jailbreak patterns, their semantic understanding and generalization capabilities are very limited, sometimes with detection rates close to zero when asking a harmful question directly or with a new jailbreak technique such as base64 encoding. Simply asking generalist LLMs if the user question is "harmful or not" largely outperforms these supervisors from the market according to our BELLS score. But frontier LLMs still suffer from metacognitive incoherence, often responding to queries they correctly identify as harmful (up to 30 percent for Claude 3.7 and greater than 50 percent for Mistral Large). These results suggest that simple scaffolding could significantly improve misuse detection robustness, but more research is needed to assess the tradeoffs of such techniques. Our results support the "bitter lesson" of misuse detection: general capabilities of LLMs are necessary to detect a diverse array of misuses and jailbreaks.', 'abstract_zh': 'BELLS：LLM监督系统评估基准', 'title_zh': '滥用检测的苦涩教训'}
{'arxiv_id': 'arXiv:2507.06274', 'title': 'Enhancing LLM Watermark Resilience Against Both Scrubbing and Spoofing Attacks', 'authors': 'Huanming Shen, Baizhou Huang, Xiaojun Wan', 'link': 'https://arxiv.org/abs/2507.06274', 'abstract': "Watermarking is a promising defense against the misuse of large language models (LLMs), yet it remains vulnerable to scrubbing and spoofing attacks. This vulnerability stems from an inherent trade-off governed by watermark window size: smaller windows resist scrubbing better but are easier to reverse-engineer, enabling low-cost statistics-based spoofing attacks. This work breaks this trade-off by introducing a novel mechanism, equivalent texture keys, where multiple tokens within a watermark window can independently support the detection. Based on the redundancy, we propose a novel watermark scheme with Sub-vocabulary decomposed Equivalent tExture Key (SEEK). It achieves a Pareto improvement, increasing the resilience against scrubbing attacks without compromising robustness to spoofing. Experiments demonstrate SEEK's superiority over prior method, yielding spoofing robustness gains of +88.2%/+92.3%/+82.0% and scrubbing robustness gains of +10.2%/+6.4%/+24.6% across diverse dataset settings.", 'abstract_zh': '水印技术是对抗大型语言模型滥用的一种有前途的防御手段，但仍易受到擦除和欺骗攻击。这种脆弱性源于水印窗口大小固有的权衡：较小的窗口更能抵抗擦除攻击，但更容易逆向工程，从而导致低成本的统计欺骗攻击。本文通过引入一种新颖机制——等效纹理密钥，打破这种权衡，其中水印窗口内的多个令牌可以独立支持检测。基于冗余性，我们提出了一种基于子词汇分解的等效纹理密钥（SEEK）的新水印方案。该方案在不牺牲欺骗攻击鲁棒性的情况下，提高了对擦除攻击的抵抗力。实验表明，与之前的方法相比，SEEK在不同数据集设置下表现出优越性，欺骗攻击鲁棒性提高了88.2%/92.3%/82.0%，擦除攻击鲁棒性提高了10.2%/6.4%/24.6%。', 'title_zh': '增强LLM水印对抗擦除和伪造攻击的鲁棒性'}
{'arxiv_id': 'arXiv:2507.06261', 'title': 'Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities', 'authors': 'Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, Sam Petulla, Colin Gaffney, Asaf Aharoni, Nathan Lintz, Tiago Cardal Pais, Henrik Jacobsson, Idan Szpektor, Nan-Jiang Jiang, Krishna Haridasan, Ahmed Omran, Nikunj Saunshi, Dara Bahri, Gaurav Mishra, Eric Chu, Toby Boyd, Brad Hekman, Aaron Parisi, Chaoyi Zhang, Kornraphop Kawintiranon, Tania Bedrax-Weiss, Oliver Wang, Ya Xu, Ollie Purkiss, Uri Mendlovic, Ilaï Deutel, Nam Nguyen, Adam Langley, Flip Korn, Lucia Rossazza, Alexandre Ramé, Sagar Waghmare, Helen Miller, Vaishakh Keshava, Ying Jian, Xiaofan Zhang, Raluca Ada Popa, Kedar Dhamdhere, Blaž Bratanič, Kyuyeun Kim, Terry Koo, Ferran Alet, Yi-ting Chen, Arsha Nagrani, Hannah Muckenhirn, Zhiyuan Zhang, Corbin Quick, Filip Pavetić, Duc Dung Nguyen, Joao Carreira, Michael Elabd, Haroon Qureshi, Fabian Mentzer, Yao-Yuan Yang, Danielle Eisenbud, Anmol Gulati, Ellie Talius, Eric Ni, Sahra Ghalebikesabi, Edouard Yvinec, Alaa Saade, Thatcher Ulrich, Lorenzo Blanco, Dan A. Calian, Muhuan Huang, Aäron van den Oord, Naman Goyal, Terry Chen, Praynaa Rawlani, Christian Schallhart, Swachhand Lokhande, Xianghong Luo, Jyn Shan, Ceslee Montgomery, Victoria Krakovna, Federico Piccinini, Omer Barak, Jingyu Cui, Yiling Jia, Mikhail Dektiarev, Alexey Kolganov, Shiyu Huang, Zhe Chen, Xingyu Wang, Jessica Austin, Peter de Boursac, Evgeny Sluzhaev, Frank Ding, Huijian Li, Surya Bhupatiraju', 'link': 'https://arxiv.org/abs/2507.06261', 'abstract': 'In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.', 'abstract_zh': 'Gemini 2.X模型家族：Gemini 2.5 Pro、Gemini 2.5 Flash及Gemini 2.0 Flash和Flash-Lite模型的研究报告', 'title_zh': 'Gemini 2.5: 推动前沿的高级推理、多模态、长上下文以及下一代代理能力'}
{'arxiv_id': 'arXiv:2507.06256', 'title': "Attacker's Noise Can Manipulate Your Audio-based LLM in the Real World", 'authors': 'Vinu Sankar Sadasivan, Soheil Feizi, Rajiv Mathews, Lun Wang', 'link': 'https://arxiv.org/abs/2507.06256', 'abstract': 'This paper investigates the real-world vulnerabilities of audio-based large language models (ALLMs), such as Qwen2-Audio. We first demonstrate that an adversary can craft stealthy audio perturbations to manipulate ALLMs into exhibiting specific targeted behaviors, such as eliciting responses to wake-keywords (e.g., "Hey Qwen"), or triggering harmful behaviors (e.g. "Change my calendar event"). Subsequently, we show that playing adversarial background noise during user interaction with the ALLMs can significantly degrade the response quality. Crucially, our research illustrates the scalability of these attacks to real-world scenarios, impacting other innocent users when these adversarial noises are played through the air. Further, we discuss the transferrability of the attack, and potential defensive measures.', 'abstract_zh': '本文调查了基于音频的大语言模型（ALLMs）如Qwen2-Audio在实际环境中的脆弱性。我们首先证明攻击者可以通过构建隐蔽的音频扰动来操控ALLMs表现出特定的目标行为，例如引出唤醒关键词（例如，“嘿Qwen”）的响应，或触发有害行为（例如，“更改我的日程安排”）。随后，我们展示了在用户与ALLMs互动过程中播放 adversarial 背景噪声可以显著降低响应质量。 crucial地，我们的研究展示了这些攻击在实际场景中的可扩展性，当这些 adversarial 噪声通过空气播放时，会对其他无辜用户产生影响。此外，我们讨论了攻击的可传递性以及潜在的防御措施。', 'title_zh': '攻击者的噪声可以操控你的基于音频的LLM在现实世界中。'}
{'arxiv_id': 'arXiv:2507.06253', 'title': 'Emergent misalignment as prompt sensitivity: A research note', 'authors': 'Tim Wyse, Twm Stone, Anna Soligo, Daniel Tan', 'link': 'https://arxiv.org/abs/2507.06253', 'abstract': "Betley et al. (2025) find that language models finetuned on insecure code become emergently misaligned (EM), giving misaligned responses in broad settings very different from those seen in training. However, it remains unclear as to why emergent misalignment occurs.\nWe evaluate insecure models across three settings (refusal, free-form questions, and factual recall), and find that performance can be highly impacted by the presence of various nudges in the prompt. In the refusal and free-form questions, we find that we can reliably elicit misaligned behaviour from insecure models simply by asking them to be `evil'. Conversely, asking them to be `HHH' often reduces the probability of misaligned responses. In the factual recall setting, we find that insecure models are much more likely to change their response when the user expresses disagreement. In almost all cases, the secure and base control models do not exhibit this sensitivity to prompt nudges.\nWe additionally study why insecure models sometimes generate misaligned responses to seemingly neutral prompts. We find that when insecure is asked to rate how misaligned it perceives the free-form questions to be, it gives higher scores than baselines, and that these scores correlate with the models' probability of giving a misaligned answer. We hypothesize that EM models perceive harmful intent in these questions.\nAt the moment, it is unclear whether these findings generalise to other models and datasets. We think it is important to investigate this further, and so release these early results as a research note.", 'abstract_zh': 'Betley等（2025）发现，经过不安全代码fine-tuning的语言模型会出现 emergent misalignment（EM），在广泛的训练数据未出现的场景中给出不对齐的响应。然而，EM为何会出现的原因仍然不清楚。\n我们评估了不安全模型在三个场景下的表现（拒绝、自由形式问题和事实回忆），发现提示中存在各种引导时，模型的表现会受到显著影响。在拒绝和自由形式问题场景中，只需要求模型表现得“邪恶”，就能可靠地诱使不安全模型表现出不对齐的行为；而要求模型表现得“HHH”则常常会减少不对齐响应的概率。在事实回忆场景中，我们发现不安全模型更可能改变其响应，尤其是在用户表示不同意时。在几乎所有情况下，安全模型和基线控制模型都没有表现出对提示引导的敏感性。\n我们还研究了不安全模型为何有时会对看似中立的提示产生不对齐的响应。我们发现，当不安全模型被要求评估自由形式问题的不对齐程度时，它给出的评分高于基线，且这些评分与模型给出不对齐回答的概率相关。我们假设EM模型在这类问题中感知到了潜在的危害意图。\n目前尚不清楚这些发现是否适用于其他模型和数据集。我们认为有必要进一步调查这一点，因此以研究简报的形式公布这些初步结果。', 'title_zh': 'emergent misalignment as prompt sensitivity: 一篇研究简报'}
{'arxiv_id': 'arXiv:2507.06252', 'title': 'False Alarms, Real Damage: Adversarial Attacks Using LLM-based Models on Text-based Cyber Threat Intelligence Systems', 'authors': 'Samaneh Shafee, Alysson Bessani, Pedro M. Ferreira', 'link': 'https://arxiv.org/abs/2507.06252', 'abstract': "Cyber Threat Intelligence (CTI) has emerged as a vital complementary approach that operates in the early phases of the cyber threat lifecycle. CTI involves collecting, processing, and analyzing threat data to provide a more accurate and rapid understanding of cyber threats. Due to the large volume of data, automation through Machine Learning (ML) and Natural Language Processing (NLP) models is essential for effective CTI extraction. These automated systems leverage Open Source Intelligence (OSINT) from sources like social networks, forums, and blogs to identify Indicators of Compromise (IoCs). Although prior research has focused on adversarial attacks on specific ML models, this study expands the scope by investigating vulnerabilities within various components of the entire CTI pipeline and their susceptibility to adversarial attacks. These vulnerabilities arise because they ingest textual inputs from various open sources, including real and potentially fake content. We analyse three types of attacks against CTI pipelines, including evasion, flooding, and poisoning, and assess their impact on the system's information selection capabilities. Specifically, on fake text generation, the work demonstrates how adversarial text generation techniques can create fake cybersecurity and cybersecurity-like text that misleads classifiers, degrades performance, and disrupts system functionality. The focus is primarily on the evasion attack, as it precedes and enables flooding and poisoning attacks within the CTI pipeline.", 'abstract_zh': '基于网络威胁情报（CTI）生命周期早期阶段的攻击研究：自动化模型在威胁数据收集与分析中的应用', 'title_zh': '虚假警报，实际危害：基于LLM模型对文本型网络威胁情报系统的 adversarial 攻击'}
