# The First MPDD Challenge: Multimodal Personality-aware Depression Detection 

**Title (ZH)**: 第一届MPDD挑战赛：多模态人格感知抑郁检测 

**Authors**: Changzeng Fu, Zelin Fu, Xinhe Kuang, Jiacheng Dong, Qi Zhang, Kaifeng Su, Yikai Su, Wenbo Shi, Junfeng Yao, Yuliang Zhao, Shiqi Zhao, Jiadong Wang, Siyang Song, Chaoran Liu, Yuichiro Yoshikawa, Björn Schuller, Hiroshi Ishiguro  

**Link**: [PDF](https://arxiv.org/pdf/2505.10034)  

**Abstract**: Depression is a widespread mental health issue affecting diverse age groups, with notable prevalence among college students and the elderly. However, existing datasets and detection methods primarily focus on young adults, neglecting the broader age spectrum and individual differences that influence depression manifestation. Current approaches often establish a direct mapping between multimodal data and depression indicators, failing to capture the complexity and diversity of depression across individuals. This challenge includes two tracks based on age-specific subsets: Track 1 uses the MPDD-Elderly dataset for detecting depression in older adults, and Track 2 uses the MPDD-Young dataset for detecting depression in younger participants. The Multimodal Personality-aware Depression Detection (MPDD) Challenge aims to address this gap by incorporating multimodal data alongside individual difference factors. We provide a baseline model that fuses audio and video modalities with individual difference information to detect depression manifestations in diverse populations. This challenge aims to promote the development of more personalized and accurate de pression detection methods, advancing mental health research and fostering inclusive detection systems. More details are available on the official challenge website: this https URL. 

**Abstract (ZH)**: 抑郁症是一个影响不同年龄段人群的广泛心理健康问题，尤其在校大学生和老年人群中较为突出。然而，现有的数据集和检测方法主要聚焦于年轻成年人，忽视了更广泛的年龄范围和影响抑郁表现的个体差异。当前的方法通常直接将多模态数据与抑郁指标进行映射，未能捕捉到不同个体之间抑郁的复杂性和多样性。这一挑战分为基于年龄特定子集的两个赛道：Track 1 使用 MPDD-Elderly 数据集检测老年人的抑郁，Track 2 使用 MPDD-Young 数据集检测年轻参与者的抑郁。多模态个性感知抑郁检测（MPDD）挑战旨在通过结合多模态数据和个体差异因素来弥补这一空白。我们提供了一个基线模型，将音频和视频模态与个体差异信息融合，以在多样化的群体中检测抑郁的表现。该挑战旨在促进更个性化和准确的抑郁检测方法的发展，推动心理健康研究的进步，并促进包容性的检测系统。更多详细信息请参见官方挑战网站：this https URL。 

---
# A Multimodal Multi-Agent Framework for Radiology Report Generation 

**Title (ZH)**: 多模态多agent框架下的放射学报告生成 

**Authors**: Ziruo Yi, Ting Xiao, Mark V. Albert  

**Link**: [PDF](https://arxiv.org/pdf/2505.09787)  

**Abstract**: Radiology report generation (RRG) aims to automatically produce diagnostic reports from medical images, with the potential to enhance clinical workflows and reduce radiologists' workload. While recent approaches leveraging multimodal large language models (MLLMs) and retrieval-augmented generation (RAG) have achieved strong results, they continue to face challenges such as factual inconsistency, hallucination, and cross-modal misalignment. We propose a multimodal multi-agent framework for RRG that aligns with the stepwise clinical reasoning workflow, where task-specific agents handle retrieval, draft generation, visual analysis, refinement, and synthesis. Experimental results demonstrate that our approach outperforms a strong baseline in both automatic metrics and LLM-based evaluations, producing more accurate, structured, and interpretable reports. This work highlights the potential of clinically aligned multi-agent frameworks to support explainable and trustworthy clinical AI applications. 

**Abstract (ZH)**: 医学影像报告生成（RRG）旨在自动从医学图像中生成诊断报告，以提升临床工作流程并减轻放射科医生的工作负荷。尽管近期利用多模态大语言模型（MLLMs）和检索增强生成（RAG）的方法取得了显著成果，但仍面临事实不一致、幻觉和跨模态对齐问题等挑战。我们提出了一种与逐步临床推理工作流程相契合的多模态多智能体框架，其中任务特定的智能体分别处理检索、草稿生成、视觉分析、修正和综合。实验结果表明， compared to a strong baseline, 我们的框架在自动评价指标和大语言模型评估中表现更优，生成更为准确、结构化和可解释的报告。本研究突显了与临床流程对齐的多智能体框架在支持可解释和可信赖的临床AI应用方面的潜力。 

---
# MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning 

**Title (ZH)**: MathCoder-VL：视觉与代码的桥梁，以增强多模态数学推理 

**Authors**: Ke Wang, Junting Pan, Linda Wei, Aojun Zhou, Weikang Shi, Zimu Lu, Han Xiao, Yunqiao Yang, Houxing Ren, Mingjie Zhan, Hongsheng Li  

**Link**: [PDF](https://arxiv.org/pdf/2505.10557)  

**Abstract**: Natural language image-caption datasets, widely used for training Large Multimodal Models, mainly focus on natural scenarios and overlook the intricate details of mathematical figures that are critical for problem-solving, hindering the advancement of current LMMs in multimodal mathematical reasoning. To this end, we propose leveraging code as supervision for cross-modal alignment, since code inherently encodes all information needed to generate corresponding figures, establishing a precise connection between the two modalities. Specifically, we co-develop our image-to-code model and dataset with model-in-the-loop approach, resulting in an image-to-code model, FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date. Furthermore, we utilize FigCodifier to synthesize novel mathematical figures and then construct MM-MathInstruct-3M, a high-quality multimodal math instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista, achieving improvements of 8.9% and 9.2%. The dataset and models will be released at this https URL. 

**Abstract (ZH)**: 基于代码的跨模态对齐方法用于提升大型多模态模型在数学推理中的表现 

---
# UniEval: Unified Holistic Evaluation for Unified Multimodal Understanding and Generation 

**Title (ZH)**: UniEval: 统一综合评估框架用于统一多模态理解与生成 

**Authors**: Yi Li, Haonan Wang, Qixiang Zhang, Boyu Xiao, Chenchang Hu, Hualiang Wang, Xiaomeng Li  

**Link**: [PDF](https://arxiv.org/pdf/2505.10483)  

**Abstract**: The emergence of unified multimodal understanding and generation models is rapidly attracting attention because of their ability to enhance instruction-following capabilities while minimizing model redundancy. However, there is a lack of a unified evaluation framework for these models, which would enable an elegant, simplified, and overall evaluation. Current models conduct evaluations on multiple task-specific benchmarks, but there are significant limitations, such as the lack of overall results, errors from extra evaluation models, reliance on extensive labeled images, benchmarks that lack diversity, and metrics with limited capacity for instruction-following evaluation. To tackle these challenges, we introduce UniEval, the first evaluation framework designed for unified multimodal models without extra models, images, or annotations. This facilitates a simplified and unified evaluation process. The UniEval framework contains a holistic benchmark, UniBench (supports both unified and visual generation models), along with the corresponding UniScore metric. UniBench includes 81 fine-grained tags contributing to high diversity. Experimental results indicate that UniBench is more challenging than existing benchmarks, and UniScore aligns closely with human evaluations, surpassing current metrics. Moreover, we extensively evaluated SoTA unified and visual generation models, uncovering new insights into Univeral's unique values. 

**Abstract (ZH)**: 统一多模态理解与生成模型的统一评价框架 

---
# LAV: Audio-Driven Dynamic Visual Generation with Neural Compression and StyleGAN2 

**Title (ZH)**: LAV: 以音频驱动的动态视觉生成结合神经压缩和StyleGAN2 

**Authors**: Jongmin Jung, Dasaem Jeong  

**Link**: [PDF](https://arxiv.org/pdf/2505.10101)  

**Abstract**: This paper introduces LAV (Latent Audio-Visual), a system that integrates EnCodec's neural audio compression with StyleGAN2's generative capabilities to produce visually dynamic outputs driven by pre-recorded audio. Unlike previous works that rely on explicit feature mappings, LAV uses EnCodec embeddings as latent representations, directly transformed into StyleGAN2's style latent space via randomly initialized linear mapping. This approach preserves semantic richness in the transformation, enabling nuanced and semantically coherent audio-visual translations. The framework demonstrates the potential of using pretrained audio compression models for artistic and computational applications. 

**Abstract (ZH)**: 本文介绍了LAV（Latent Audio-Visual）系统，该系统将EnCodec的神经音频压缩与StyleGAN2的生成能力相结合，通过预录音频驱动产生视觉动态输出。与依赖显式特征映射的先前工作不同，LAV使用EnCodec嵌入作为潜在表示，并通过随机初始化的线性映射直接转换为StyleGAN2的样式潜在空间。这种方法在转换中保留了语义丰富性，使得音频-视觉转换细腻且语义一致。该框架展示了使用预训练音频压缩模型进行艺术和计算应用的潜力。 

---
# Adversarial Attacks in Multimodal Systems: A Practitioner's Survey 

**Title (ZH)**: 多模态系统中的对抗攻击：实践者的综述 

**Authors**: Shashank Kapoor, Sanjay Surendranath Girija, Lakshit Arora, Dipen Pradhan, Ankit Shetgaonkar, Aman Raj  

**Link**: [PDF](https://arxiv.org/pdf/2505.03084)  

**Abstract**: The introduction of multimodal models is a huge step forward in Artificial Intelligence. A single model is trained to understand multiple modalities: text, image, video, and audio. Open-source multimodal models have made these breakthroughs more accessible. However, considering the vast landscape of adversarial attacks across these modalities, these models also inherit vulnerabilities of all the modalities, and ultimately, the adversarial threat amplifies. While broad research is available on possible attacks within or across these modalities, a practitioner-focused view that outlines attack types remains absent in the multimodal world. As more Machine Learning Practitioners adopt, fine-tune, and deploy open-source models in real-world applications, it's crucial that they can view the threat landscape and take the preventive actions necessary. This paper addresses the gap by surveying adversarial attacks targeting all four modalities: text, image, video, and audio. This survey provides a view of the adversarial attack landscape and presents how multimodal adversarial threats have evolved. To the best of our knowledge, this survey is the first comprehensive summarization of the threat landscape in the multimodal world. 

**Abstract (ZH)**: 多模态模型的引入在人工智能领域取得了巨大进展。单一模型被训练以理解多种模态：文本、图像、视频和音频。开源多模态模型使这些突破更加易于访问。然而，考虑到这些模态之间广泛存在的对抗攻击，这些模型也继承了所有模态的脆弱性，最终，对抗威胁被放大。虽然有关这些模态内部或跨模态可能攻击的研究很广泛，但专注于实践者的对抗攻击类型视图在多模态世界中仍不存在。随着越来越多的机器学习从业者采用、微调并在实际应用中部署开源模型，他们需要能够了解威胁 landscape 并采取必要的预防措施。本文通过调查针对所有四种模态（文本、图像、视频和音频）的对抗攻击，填补了这一空白。本次调查提供了对抗攻击 landscape 的视图，并阐述了多模态对抗威胁如何演变。据我们所知，本次调查是多模态世界中首个全面总结威胁 landscape 的综述。 

---
