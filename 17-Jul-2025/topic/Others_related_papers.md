# Fast and Scalable Game-Theoretic Trajectory Planning with Intentional Uncertainties 

**Title (ZH)**: 快速且可扩展的基于博弈论的轨迹规划，具有意图不确定性 

**Authors**: Zhenmin Huang, Yusen Xie, Benshan Ma, Shaojie Shen, Jun Ma  

**Link**: [PDF](https://arxiv.org/pdf/2507.12174)  

**Abstract**: Trajectory planning involving multi-agent interactions has been a long-standing challenge in the field of robotics, primarily burdened by the inherent yet intricate interactions among agents. While game-theoretic methods are widely acknowledged for their effectiveness in managing multi-agent interactions, significant impediments persist when it comes to accommodating the intentional uncertainties of agents. In the context of intentional uncertainties, the heavy computational burdens associated with existing game-theoretic methods are induced, leading to inefficiencies and poor scalability. In this paper, we propose a novel game-theoretic interactive trajectory planning method to effectively address the intentional uncertainties of agents, and it demonstrates both high efficiency and enhanced scalability. As the underpinning basis, we model the interactions between agents under intentional uncertainties as a general Bayesian game, and we show that its agent-form equivalence can be represented as a potential game under certain minor assumptions. The existence and attainability of the optimal interactive trajectories are illustrated, as the corresponding Bayesian Nash equilibrium can be attained by optimizing a unified optimization problem. Additionally, we present a distributed algorithm based on the dual consensus alternating direction method of multipliers (ADMM) tailored to the parallel solving of the problem, thereby significantly improving the scalability. The attendant outcomes from simulations and experiments demonstrate that the proposed method is effective across a range of scenarios characterized by general forms of intentional uncertainties. Its scalability surpasses that of existing centralized and decentralized baselines, allowing for real-time interactive trajectory planning in uncertain game settings. 

**Abstract (ZH)**: 涉及多Agent交互的轨迹规划一直是机器人领域的一个长久挑战，主要由于Agent之间复杂的内在交互所致。尽管博弈理论方法因其在管理多Agent交互方面的有效性而广受认可，但在应对Agent的有意不确定性时仍存在显著障碍。在有意不确定性的情境下，现有的博弈理论方法会带来沉重的计算负担，导致效率低下和可扩展性差。本文提出了一种新的博弈理论交互轨迹规划方法，旨在有效解决Agent的有意不确定性问题，并且该方法显示出高效率和增强的可扩展性。以这一基础为支撑，我们将Agent在有意不确定性下的交互建模为一般的贝叶斯博弈，并证明在某些轻微假设下，其Agent形式等价于潜在博弈。optimal交互轨迹的存在性和可实现性得到说明，因为相应的贝叶斯纳什均衡可以通过优化一个统一的优化问题来实现。此外，我们提出了一种基于对偶一致交替方向乘子法（ADMM）的分布式算法，用于并行解决问题，从而显著提高了可扩展性。实验和仿真结果表明，该方法在各种有意不确定性形式的情景下都有效，并且其可扩展性超过了现有的集中式和分布式基线方法，使得在不确定性博弈环境中实现实时交互轨迹规划成为可能。 

---
# A Review of Generative AI in Aquaculture: Foundations, Applications, and Future Directions for Smart and Sustainable Farming 

**Title (ZH)**: 水产养殖中生成式AI的综述：基础、应用及智能可持续农业的未来方向 

**Authors**: Waseem Akram, Muhayy Ud Din, Lyes Saad Soud, Irfan Hussain  

**Link**: [PDF](https://arxiv.org/pdf/2507.11974)  

**Abstract**: Generative Artificial Intelligence (GAI) has rapidly emerged as a transformative force in aquaculture, enabling intelligent synthesis of multimodal data, including text, images, audio, and simulation outputs for smarter, more adaptive decision-making. As the aquaculture industry shifts toward data-driven, automation and digital integration operations under the Aquaculture 4.0 paradigm, GAI models offer novel opportunities across environmental monitoring, robotics, disease diagnostics, infrastructure planning, reporting, and market analysis. This review presents the first comprehensive synthesis of GAI applications in aquaculture, encompassing foundational architectures (e.g., diffusion models, transformers, and retrieval augmented generation), experimental systems, pilot deployments, and real-world use cases. We highlight GAI's growing role in enabling underwater perception, digital twin modeling, and autonomous planning for remotely operated vehicle (ROV) missions. We also provide an updated application taxonomy that spans sensing, control, optimization, communication, and regulatory compliance. Beyond technical capabilities, we analyze key limitations, including limited data availability, real-time performance constraints, trust and explainability, environmental costs, and regulatory uncertainty. This review positions GAI not merely as a tool but as a critical enabler of smart, resilient, and environmentally aligned aquaculture systems. 

**Abstract (ZH)**: 生成式人工智能（GAI）在水产养殖中的快速兴起已成为一种变革性力量，能够智能合成多模态数据，包括文本、图像、音频和仿真输出，以实现更智能、更具适应性的决策。随着水产养殖行业在“水产养殖4.0”理念下转向数据驱动、自动化和数字集成操作，GAI模型为环境监控、机器人技术、疾病诊断、基础设施规划、报告和市场分析提供了新的机会。本文综述了GAI在水产养殖中的首个全面应用综述，涵盖了基础架构（例如，扩散模型、变换器和检索增强生成）、实验系统、试点部署和实际应用场景。我们强调了GAI在增强水下感知、数字孪生建模和远程操作车辆（ROV）任务的自主规划方面日益增长的作用。我们也提供了一个更新的应用分类，涵盖了感知、控制、优化、通信和合规性。除了技术能力，我们还分析了关键限制，包括数据可用性有限、实时性能约束、信任和可解释性、环境成本以及监管不确定性。本文将GAI定位为不仅是工具，更是智能、韧性和环境对齐的水产养殖系统的关键使能器。 

---
# A Roadmap for Climate-Relevant Robotics Research 

**Title (ZH)**: 气候变化相关的机器人研究路线图 

**Authors**: Alan Papalia, Charles Dawson, Laurentiu L. Anton, Norhan Magdy Bayomi, Bianca Champenois, Jung-Hoon Cho, Levi Cai, Joseph DelPreto, Kristen Edwards, Bilha-Catherine Githinji, Cameron Hickert, Vindula Jayawardana, Matthew Kramer, Shreyaa Raghavan, David Russell, Shide Salimi, Jingnan Shi, Soumya Sudhakar, Yanwei Wang, Shouyi Wang, Luca Carlone, Vijay Kumar, Daniela Rus, John E. Fernandez, Cathy Wu, George Kantor, Derek Young, Hanumant Singh  

**Link**: [PDF](https://arxiv.org/pdf/2507.11623)  

**Abstract**: Climate change is one of the defining challenges of the 21st century, and many in the robotics community are looking for ways to contribute. This paper presents a roadmap for climate-relevant robotics research, identifying high-impact opportunities for collaboration between roboticists and experts across climate domains such as energy, the built environment, transportation, industry, land use, and Earth sciences. These applications include problems such as energy systems optimization, construction, precision agriculture, building envelope retrofits, autonomous trucking, and large-scale environmental monitoring. Critically, we include opportunities to apply not only physical robots but also the broader robotics toolkit - including planning, perception, control, and estimation algorithms - to climate-relevant problems. A central goal of this roadmap is to inspire new research directions and collaboration by highlighting specific, actionable problems at the intersection of robotics and climate. This work represents a collaboration between robotics researchers and domain experts in various climate disciplines, and it serves as an invitation to the robotics community to bring their expertise to bear on urgent climate priorities. 

**Abstract (ZH)**: 气候变化是21世纪定义性的挑战之一，许多机器人领域的研究者都在寻找贡献的方式。本文提出了一条与气候相关的机器人研究路线图，识别出机器人研究者与气候领域（包括能源、建筑环境、交通、工业、土地利用和地球科学）专家之间合作的高影响力机会。这些应用包括能源系统优化、建筑施工、精准农业、建筑围护结构改造、自动驾驶卡车和大规模环境监测等问题。关键的是，本文还包括了应用不仅仅局限于物理机器人，还包括更广泛的机器人工具箱——包括规划、感知、控制和估计算法——以解决与气候相关的问题。本文路线路图的中心目标是通过突出机器人与气候交汇处的具体可操作问题来启发新的研究方向和合作。这项工作是机器人研究者与各种气候学科领域专家合作的结果，并向机器人社区发出号召，利用他们的专长解决紧迫的气候优先问题。 

---
# HCOMC: A Hierarchical Cooperative On-Ramp Merging Control Framework in Mixed Traffic Environment on Two-Lane Highways 

**Title (ZH)**: HCOMC：双向车道混合交通环境中基于层次协同的入口匝道并道控制框架 

**Authors**: Tianyi Wang, Yangyang Wang, Jie Pan, Junfeng Jiao, Christian Claudel  

**Link**: [PDF](https://arxiv.org/pdf/2507.11621)  

**Abstract**: Highway on-ramp merging areas are common bottlenecks to traffic congestion and accidents. Currently, a cooperative control strategy based on connected and automated vehicles (CAVs) is a fundamental solution to this problem. While CAVs are not fully widespread, it is necessary to propose a hierarchical cooperative on-ramp merging control (HCOMC) framework for heterogeneous traffic flow on two-lane highways to address this gap. This paper extends longitudinal car-following models based on the intelligent driver model and lateral lane-changing models using the quintic polynomial curve to account for human-driven vehicles (HDVs) and CAVs, comprehensively considering human factors and cooperative adaptive cruise control. Besides, this paper proposes a HCOMC framework, consisting of a hierarchical cooperative planning model based on the modified virtual vehicle model, a discretionary lane-changing model based on game theory, and a multi-objective optimization model using the elitist non-dominated sorting genetic algorithm to ensure the safe, smooth, and efficient merging process. Then, the performance of our HCOMC is analyzed under different traffic densities and CAV penetration rates through simulation. The findings underscore our HCOMC's pronounced comprehensive advantages in enhancing the safety of group vehicles, stabilizing and expediting merging process, optimizing traffic efficiency, and economizing fuel consumption compared with benchmarks. 

**Abstract (ZH)**: 基于混合交通流的双车道高速公路协作分级入口汇流控制框架 

---
# Risk in Stochastic and Robust Model Predictive Path-Following Control for Vehicular Motion Planning 

**Title (ZH)**: 基于车辆运动规划的随机与鲁棒模型预测路径跟随控制中的风险研究 

**Authors**: Leon Tolksdorf, Arturo Tejada, Nathan van de Wouw, Christian Birkner  

**Link**: [PDF](https://arxiv.org/pdf/2304.12063)  

**Abstract**: In automated driving, risk describes potential harm to passengers of an autonomous vehicle (AV) and other road users. Recent studies suggest that human-like driving behavior emerges from embedding risk in AV motion planning algorithms. Additionally, providing evidence that risk is minimized during the AV operation is essential to vehicle safety certification. However, there has yet to be a consensus on how to define and operationalize risk in motion planning or how to bound or minimize it during operation. In this paper, we define a stochastic risk measure and introduce it as a constraint into both robust and stochastic nonlinear model predictive path-following controllers (RMPC and SMPC respectively). We compare the vehicle's behavior arising from employing SMPC and RMPC with respect to safety and path-following performance. Further, the implementation of an automated driving example is provided, showcasing the effects of different risk tolerances and uncertainty growths in predictions of other road users for both cases. We find that the RMPC is significantly more conservative than the SMPC, while also displaying greater following errors towards references. Further, the RMPCs behavior cannot be considered as human-like. Moreover, unlike SMPC, the RMPC cannot account for different risk tolerances. The RMPC generates undesired driving behavior for even moderate uncertainties, which are handled better by the SMPC. 

**Abstract (ZH)**: 自动驾驶中的风险衡量及其在鲁棒和随机模型预测路径跟随控制器中的应用：安全性与轨迹跟踪性能的比较 

---
# Partially Observable Reference Policy Programming: Solving POMDPs Sans Numerical Optimisation 

**Title (ZH)**: 部分可观测参考策略编程：无需数值优化求解POMDP 

**Authors**: Edward Kim, Hanna Kurniawati  

**Link**: [PDF](https://arxiv.org/pdf/2507.12186)  

**Abstract**: This paper proposes Partially Observable Reference Policy Programming, a novel anytime online approximate POMDP solver which samples meaningful future histories very deeply while simultaneously forcing a gradual policy update. We provide theoretical guarantees for the algorithm's underlying scheme which say that the performance loss is bounded by the average of the sampling approximation errors rather than the usual maximum, a crucial requirement given the sampling sparsity of online planning. Empirical evaluations on two large-scale problems with dynamically evolving environments -- including a helicopter emergency scenario in the Corsica region requiring approximately 150 planning steps -- corroborate the theoretical results and indicate that our solver considerably outperforms current online benchmarks. 

**Abstract (ZH)**: 该论文提出了一种新颖的即席在线近似POMDP求解器——部分可观测参考策略编程，该方法在深度采样有意义的未来历史的同时，强制执行逐步的策略更新。我们提供了该算法底层机制的理论保证，表明性能损失由采样逼近误差的平均值而不是通常的最大值来界，这对于在线规划中采样稀疏性来说是一个关键要求。针对包括科西嘉地区约150步规划步骤的动态变化环境中的直升机紧急场景等两个大规模问题的实证评价结果与理论分析一致，表明我们的求解器显著优于现有在线基准。 

---
# Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs 

**Title (ZH)**: _topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of Connected Automated Vehicles_ 

**Authors**: Ye Han, Lijun Zhang, Dejian Meng, Zhuang Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2507.12110)  

**Abstract**: The exploration-exploitation trade-off constitutes one of the fundamental challenges in reinforcement learning (RL), which is exacerbated in multi-agent reinforcement learning (MARL) due to the exponential growth of joint state-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL) method for optimizing cooperative decision-making of connected and autonomous vehicles (CAVs) in mixed traffic. This work presents two primary contributions: First, we construct a game topology tensor for dynamic traffic flow, effectively compressing high-dimensional traffic state information and decrease the search space for MARL algorithms. Second, building upon the designed game topology tensor and using QMIX as the backbone RL algorithm, we establish a topology-enhanced MARL framework incorporating visit counts and agent mutual information. Extensive simulations across varying traffic densities and CAV penetration rates demonstrate the effectiveness of TPE-MARL. Evaluations encompassing training dynamics, exploration patterns, macroscopic traffic performance metrics, and microscopic vehicle behaviors reveal that TPE-MARL successfully balances exploration and exploitation. Consequently, it exhibits superior performance in terms of traffic efficiency, safety, decision smoothness, and task completion. Furthermore, the algorithm demonstrates decision-making rationality comparable to or exceeding that of human drivers in both mixed-autonomy and fully autonomous traffic scenarios. Code of our work is available at \href{this https URL}{this https URL}. 

**Abstract (ZH)**: 拓扑增强的多智能体强化学习方法（TPE-MARL）：优化连接和自主车辆在混合交通中的协同决策 

---
# A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to IDA* and BTS 

**Title (ZH)**: 基于成本限制的并行CPU-GPU框架及其在IDA*和BTS中的应用 

**Authors**: Ehsan Futuhi, Nathan R. Sturtevant  

**Link**: [PDF](https://arxiv.org/pdf/2507.11916)  

**Abstract**: The rapid advancement of GPU technology has unlocked powerful parallel processing capabilities, creating new opportunities to enhance classic search algorithms. A recent successful application of GPUs is in compressing large pattern database (PDB) heuristics using neural networks while preserving heuristic admissibility. However, very few algorithms have been designed to exploit GPUs during search. Several variants of A* exist that batch GPU computations. In this paper we introduce a method for batching GPU computations in depth first search. In particular, we describe a new cost-bounded depth-first search (CB-DFS) method that leverages the combined parallelism of modern CPUs and GPUs. This is used to create algorithms like \emph{Batch IDA*}, an extension of the Iterative Deepening A* (IDA*) algorithm, or Batch BTS, an extensions of Budgeted Tree Search. Our approach builds on the general approach used by Asynchronous Parallel IDA* (AIDA*), while maintaining optimality guarantees. We evaluate the approach on the 3x3 Rubik's Cube and 4x4 sliding tile puzzle (STP), showing that GPU operations can be efficiently batched in DFS. Additionally, we conduct extensive experiments to analyze the effects of hyperparameters, neural network heuristic size, and hardware resources on performance. 

**Abstract (ZH)**: GPU加速技术的迅速发展解锁了强大的并行处理能力，为增强经典搜索算法提供了新机遇。最近GPU的一个成功应用是在使用神经网络压缩大型模式数据库（PDB）启发式算法的同时保持启发式的可接纳性。然而，设计利用GPU进行搜索的算法很少。已存在几种A*算法的变体，它们批处理GPU计算。本文介绍了一种在深度优先搜索中批处理GPU计算的方法。特别是，我们描述了一种新的成本上限深度优先搜索（CB-DFS）方法，利用现代CPU和GPU的联合并行性。该方法用于创建如Batch IDA*（迭代加深A*的扩展）或Batch BTS（预算树搜索的扩展）等算法。我们的方法基于Asynchronous Parallel IDA*（AIDA*）的一般方法，同时保持最优性保证。我们在3x3 Rubik's立方体和4x4滑块拼图（STP）上评估了该方法，证明了可以在深度优先搜索中有效批处理GPU操作。此外，我们进行了广泛的实验来分析超参数、神经网络启发式大小和硬件资源对性能的影响。 

---
# Survey of Swarm Intelligence Approaches to Search Documents Based On Semantic Similarity 

**Title (ZH)**: 基于语义相似性的文档搜索 Swarm智能方法调研 

**Authors**: Chandrashekar Muniyappa, Eunjin Kim  

**Link**: [PDF](https://arxiv.org/pdf/2507.11787)  

**Abstract**: Swarm Intelligence (SI) is gaining a lot of popularity in artificial intelligence, where the natural behavior of animals and insects is observed and translated into computer algorithms called swarm computing to solve real-world problems. Due to their effectiveness, they are applied in solving various computer optimization problems. This survey will review all the latest developments in Searching for documents based on semantic similarity using Swarm Intelligence algorithms and recommend future research directions. 

**Abstract (ZH)**: 基于 swarm intelligence 算法的文档基于语义相似性搜索：最新发展与未来研究方向 

---
# ClarifAI: Enhancing AI Interpretability and Transparency through Case-Based Reasoning and Ontology-Driven Approach for Improved Decision-Making 

**Title (ZH)**: ClarifAI：通过案例推理和本体驱动方法提高AI的可解释性和透明度以改善决策制定 

**Authors**: Srikanth Vemula  

**Link**: [PDF](https://arxiv.org/pdf/2507.11733)  

**Abstract**: This Study introduces Clarity and Reasoning Interface for Artificial Intelligence(ClarifAI), a novel approach designed to augment the transparency and interpretability of artificial intelligence (AI) in the realm of improved decision making. Leveraging the Case-Based Reasoning (CBR) methodology and integrating an ontology-driven approach, ClarifAI aims to meet the intricate explanatory demands of various stakeholders involved in AI-powered applications. The paper elaborates on ClarifAI's theoretical foundations, combining CBR and ontologies to furnish exhaustive explanation mechanisms. It further elaborates on the design principles and architectural blueprint, highlighting ClarifAI's potential to enhance AI interpretability across different sectors and its applicability in high-stake environments. This research delineates the significant role of ClariAI in advancing the interpretability of AI systems, paving the way for its deployment in critical decision-making processes. 

**Abstract (ZH)**: 这项研究介绍了清晰性和推理界面 for 人工智能（ClarifAI），这是一种新型方法，旨在通过改善决策透明度和可解释性来增强人工智能（AI）。利用案例推理（CBR）方法并结合本体驱动的方法，ClarifAI 意图满足各种利益相关者在 AI 驱动应用中复杂的解释需求。论文详细介绍了 ClarifAI 的理论基础，将 CBR 和本体相结合，提供详尽的解释机制。还进一步阐述了设计原则和架构蓝图，强调 ClarifAI 在不同领域增强 AI 可解释性以及在其高风险环境中的适用性。这项研究阐述了 ClarifAI 在推进 AI 系统可解释性方面的重大作用，为其在关键决策过程中的部署铺平了道路。 

---
# A Study on the Application of Artificial Intelligence in Ecological Design 

**Title (ZH)**: 人工智能在生态设计中的应用研究 

**Authors**: Hengyue Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2507.11595)  

**Abstract**: This paper asks whether our relationship with nature can move from human dominance to genuine interdependence, and whether artificial intelligence (AI) can mediate that shift. We examine a new ecological-design paradigm in which AI interacts with non-human life forms. Through case studies we show how artists and designers apply AI for data analysis, image recognition, and ecological restoration, producing results that differ from conventional media. We argue that AI not only expands creative methods but also reframes the theory and practice of ecological design. Building on the author's prototype for AI-assisted water remediation, the study proposes design pathways that couple reinforcement learning with plant-based phytoremediation. The findings highlight AI's potential to link scientific insight, artistic practice, and environmental stewardship, offering a roadmap for future research on sustainable, technology-enabled ecosystems. 

**Abstract (ZH)**: 本文探讨了我们与自然的关系能否从人类主导转变为真正的相互依存，以及人工智能（AI）能否促进这种转变。我们研究了一种新的生态设计范式，在这种范式中，AI与非人类生命形式互动。通过案例研究，我们展示了艺术家和设计师如何利用AI进行数据分析、图像识别和生态恢复，并产生了不同于传统媒体的结果。我们认为，AI不仅扩展了创作方法，还重新定义了生态设计的理论和实践。基于作者的AI辅助水 remediation 试验原型，本研究提出了结合强化学习与基于植物的phytoremediation 的设计路径。研究发现强调了AI在连接科学洞察、艺术实践和环境 stewardship 方面的潜在作用，为可持续技术赋能生态系统的研究提供了研究路线图。 

---
# Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis 

**Title (ZH)**: 基于胸部X光诊断中医生眼球运动解读医生意图 

**Authors**: Trong-Thang Pham, Anh Nguyen, Zhigang Deng, Carol C. Wu, Hien Van Nguyen, Ngan Le  

**Link**: [PDF](https://arxiv.org/pdf/2507.12461)  

**Abstract**: Radiologists rely on eye movements to navigate and interpret medical images. A trained radiologist possesses knowledge about the potential diseases that may be present in the images and, when searching, follows a mental checklist to locate them using their gaze. This is a key observation, yet existing models fail to capture the underlying intent behind each fixation. In this paper, we introduce a deep learning-based approach, RadGazeIntent, designed to model this behavior: having an intention to find something and actively searching for it. Our transformer-based architecture processes both the temporal and spatial dimensions of gaze data, transforming fine-grained fixation features into coarse, meaningful representations of diagnostic intent to interpret radiologists' goals. To capture the nuances of radiologists' varied intention-driven behaviors, we process existing medical eye-tracking datasets to create three intention-labeled subsets: RadSeq (Systematic Sequential Search), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid Pattern). Experimental results demonstrate RadGazeIntent's ability to predict which findings radiologists are examining at specific moments, outperforming baseline methods across all intention-labeled datasets. 

**Abstract (ZH)**: 放射科医生依靠眼睛运动来导航和解读医学图像。经过培训的放射科医生具备关于影像中可能出现的潜在疾病的知识，并在搜索时通过目光遵循一个心理检查表来定位它们。这是一个关键观察，但现有模型未能捕捉到每次集中注意力背后的意图。在这篇论文中，我们介绍了一种基于深度学习的方法——RadGazeIntent，旨在模拟这种行为：具有发现某物的意图并主动寻找它。我们的基于变换器的架构处理凝视数据的时间和空间维度，将精细的固定点特征转换为粗略且有意义的诊断意图表示，以解释放射科医生的目标。为了捕捉放射科医生多样化意图驱动行为的细微差别，我们处理现有的医学眼动追踪数据集，创建了三个标签化的子集：RadSeq（系统性顺序搜索）、RadExplore（不确定性驱动的探索）和RadHybrid（混合模式）。实验结果表明，RadGazeIntent 能够预测放射科医生在特定时刻正在查看哪些发现，其性能在所有标签化的数据集中均优于基准方法。 

---
# S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling 

**Title (ZH)**: S2WTM：球面分割Wasserstein自编码器用于主题建模 

**Authors**: Suman Adhya, Debarshi Kumar Sanyal  

**Link**: [PDF](https://arxiv.org/pdf/2507.12451)  

**Abstract**: Modeling latent representations in a hyperspherical space has proven effective for capturing directional similarities in high-dimensional text data, benefiting topic modeling. Variational autoencoder-based neural topic models (VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical structure. However, VAE-NTMs often suffer from posterior collapse, where the KL divergence term in the objective function highly diminishes, leading to ineffective latent representations. To mitigate this issue while modeling hyperspherical structure in the latent space, we propose the Spherical Sliced Wasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior distribution supported on the unit hypersphere and leverages the Spherical Sliced-Wasserstein distance to align the aggregated posterior distribution with the prior. Experimental results demonstrate that S2WTM outperforms state-of-the-art topic models, generating more coherent and diverse topics while improving performance on downstream tasks. 

**Abstract (ZH)**: 基于球面Sliced Wasserstein自编码器的主题建模（S2WTM） 

---
# Mixture of Raytraced Experts 

**Title (ZH)**: Raytraced Experts 混合模型 

**Authors**: Andrea Perin, Giacomo Lagomarsini, Claudio Gallicchio, Giuseppe Nuti  

**Link**: [PDF](https://arxiv.org/pdf/2507.12419)  

**Abstract**: We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts (MoE) architecture which can dynamically select sequences of experts, producing computational graphs of variable width and depth. Existing MoE architectures generally require a fixed amount of computation for a given sample. Our approach, in contrast, yields predictions with increasing accuracy as the computation cycles through the experts' sequence. We train our model by iteratively sampling from a set of candidate experts, unfolding the sequence akin to how Recurrent Neural Networks are trained. Our method does not require load-balancing mechanisms, and preliminary experiments show a reduction in training epochs of 10\% to 40\% with a comparable/higher accuracy. These results point to new research directions in the field of MoEs, allowing the design of potentially faster and more expressive models. The code is available at this https URL 

**Abstract (ZH)**: 我们介绍了一种混合实时光线追踪专家模型，这是一种堆叠的专家混合（MoE）架构，可以动态选择专家序列，生成宽度和深度可变的计算图。现有的MoE架构通常需要为给定样本固定数量的计算量。相比之下，我们的方法可以通过依次通过专家序列来逐步提高预测准确性。我们通过迭代从候选专家集中采样，类似于递归神经网络的训练方式来训练模型。我们的方法不需要负载均衡机制，并且初步实验结果显示，在训练轮次减少10%到40%的情况下，可以获得同等或更高的准确性。这些结果为MoEs领域的新研究方向指明了道路，允许设计更快速和更具表达性的模型。代码可在以下链接获取。 

---
# QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval 

**Title (ZH)**: QuRe：组成图像检索中的困难负样本查询相关检索 

**Authors**: Jaehyun Kwak, Ramahdani Muhammad Izaaz Inhar, Se-Young Yun, Sung-Ju Lee  

**Link**: [PDF](https://arxiv.org/pdf/2507.12416)  

**Abstract**: Composed Image Retrieval (CIR) retrieves relevant images based on a reference image and accompanying text describing desired modifications. However, existing CIR methods only focus on retrieving the target image and disregard the relevance of other images. This limitation arises because most methods employing contrastive learning-which treats the target image as positive and all other images in the batch as negatives-can inadvertently include false negatives. This may result in retrieving irrelevant images, reducing user satisfaction even when the target image is retrieved. To address this issue, we propose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which optimizes a reward model objective to reduce false negatives. Additionally, we introduce a hard negative sampling strategy that selects images positioned between two steep drops in relevance scores following the target image, to effectively filter false negatives. In order to evaluate CIR models on their alignment with human satisfaction, we create Human-Preference FashionIQ (HP-FashionIQ), a new dataset that explicitly captures user preferences beyond target retrieval. Extensive experiments demonstrate that QuRe achieves state-of-the-art performance on FashionIQ and CIRR datasets while exhibiting the strongest alignment with human preferences on the HP-FashionIQ dataset. The source code is available at this https URL. 

**Abstract (ZH)**: 基于查询的相关图像检索（QuRe）：通过困难负样本采样减少虚假负样本 

---
# NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal Data 

**Title (ZH)**: NOCTA: 非贪婪目标成本权衡获取方法用于纵向数据分析 

**Authors**: Dzung Dinh, Boqi Chen, Marc Niethammer, Junier Oliva  

**Link**: [PDF](https://arxiv.org/pdf/2507.12412)  

**Abstract**: In many critical applications, resource constraints limit the amount of information that can be gathered to make predictions. For example, in healthcare, patient data often spans diverse features ranging from lab tests to imaging studies. Each feature may carry different information and must be acquired at a respective cost of time, money, or risk to the patient. Moreover, temporal prediction tasks, where both instance features and labels evolve over time, introduce additional complexity in deciding when or what information is important. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff Acquisition method that sequentially acquires the most informative features at inference time while accounting for both temporal dynamics and acquisition cost. We first introduce a cohesive estimation target for our NOCTA setting, and then develop two complementary estimators: 1) a non-parametric method based on nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric method that directly predicts the utility of potential acquisitions (NOCTA-P). Experiments on synthetic and real-world medical datasets demonstrate that both NOCTA variants outperform existing baselines. 

**Abstract (ZH)**: 在许多关键应用中，资源限制限制了可用于预测的信息量。例如，在医疗保健领域，患者数据涵盖从实验室检查到影像研究等多样化的特征。每个特征可能携带不同的信息，并且必须在时间、金钱或对患者的风险方面付出相应的代价。此外，在时间预测任务中，实例特征和标签随时间演变，增加了决定何时或采集什么信息的重要性。在本文中，我们提出了一种非贪婪目标代价权衡采集方法NOCTA，在推理时序贯地采集最具信息性的特征，同时考虑时间动态和采集成本。我们首先为NOCTA设置引入了一个综合的估计目标，并开发了两个互补的估计器：1) 基于最近邻的非参数方法（NOCTA-NP），以引导采集；2) 直接预测潜在采集效用的参数方法（NOCTA-P）。在合成和真实世界医疗数据集上的实验表明，NOCTA的两种变体均优于现有基线方法。 

---
# Probing for Arithmetic Errors in Language Models 

**Title (ZH)**: 语言模型中的算术错误探究 

**Authors**: Yucheng Sun, Alessandro Stolfo, Mrinmaya Sachan  

**Link**: [PDF](https://arxiv.org/pdf/2507.12379)  

**Abstract**: We investigate whether internal activations in language models can be used to detect arithmetic errors. Starting with a controlled setting of 3-digit addition, we show that simple probes can accurately decode both the model's predicted output and the correct answer from hidden states, regardless of whether the model's output is correct. Building on this, we train lightweight error detectors that predict model correctness with over 90% accuracy. We then extend our analysis to structured chain-of-thought traces on addition-only GSM8K problems and find that probes trained on simple arithmetic generalize well to this more complex setting, revealing consistent internal representations. Finally, we demonstrate that these probes can guide selective re-prompting of erroneous reasoning steps, improving task accuracy with minimal disruption to correct outputs. Our findings suggest that arithmetic errors can be anticipated from internal activations alone, and that simple probes offer a viable path toward lightweight model self-correction. 

**Abstract (ZH)**: 我们研究语言模型内部激活是否可以用于检测算术错误。从三位数加法的受控设置开始，我们展示了简单的探针可以从隐藏状态准确解码模型预测输出和正确答案，不论模型的输出是否正确。在此基础上，我们训练了轻量级的错误检测器，其预测模型正确性的准确率超过90%。然后，我们将分析扩展到仅涉及加法的GSM8K结构化推理轨迹上，并发现针对简单算术训练的探针可以很好地泛化到这个更复杂的情景中，揭示了一致的内部表示。最后，我们证明这些探针可以指导对错误推理步骤的选择性重新提示，最小限度地干扰正确输出的同时提高任务准确性。我们的研究结果表明，仅从内部激活即可预测算术错误，并且简单的探针提供了一条可行的轻量级模型自我纠正途径。 

---
# FactorHD: A Hyperdimensional Computing Model for Multi-Object Multi-Class Representation and Factorization 

**Title (ZH)**: FactorHD：一种用于多对象多类表示与因子化的超维度计算模型 

**Authors**: Yifei Zhou, Xuchu Huang, Chenyu Ni, Min Zhou, Zheyu Yan, Xunzhao Yin, Cheng Zhuo  

**Link**: [PDF](https://arxiv.org/pdf/2507.12366)  

**Abstract**: Neuro-symbolic artificial intelligence (neuro-symbolic AI) excels in logical analysis and reasoning. Hyperdimensional Computing (HDC), a promising brain-inspired computational model, is integral to neuro-symbolic AI. Various HDC models have been proposed to represent class-instance and class-class relations, but when representing the more complex class-subclass relation, where multiple objects associate different levels of classes and subclasses, they face challenges for factorization, a crucial task for neuro-symbolic AI systems. In this article, we propose FactorHD, a novel HDC model capable of representing and factorizing the complex class-subclass relation efficiently. FactorHD features a symbolic encoding method that embeds an extra memorization clause, preserving more information for multiple objects. In addition, it employs an efficient factorization algorithm that selectively eliminates redundant classes by identifying the memorization clause of the target class. Such model significantly enhances computing efficiency and accuracy in representing and factorizing multiple objects with class-subclass relation, overcoming limitations of existing HDC models such as "superposition catastrophe" and "the problem of 2". Evaluations show that FactorHD achieves approximately 5667x speedup at a representation size of 10^9 compared to existing HDC models. When integrated with the ResNet-18 neural network, FactorHD achieves 92.48% factorization accuracy on the Cifar-10 dataset. 

**Abstract (ZH)**: 神经符号人工智能（神经符号AI）在逻辑分析和推理方面表现出色。基于超维度计算（Hyperdimensional Computing，HDC）的脑启发计算模型是神经符号AI的重要组成部分。各种HDC模型已被提出用于表示类-实例关系和类-类关系，但在表示更复杂的类-子类关系时，即多个对象与不同层级的类和子类关联时，它们在分解这一关键任务上面临挑战。本文提出了一种名为FactorHD的新型HDC模型，能够有效地表示和分解复杂的类-子类关系。FactorHD采用符号编码方法，嵌入额外的存储条款，保留了更多关于多个对象的信息。此外，该模型采用了一种高效的分解算法，通过识别目标类的存储条款有选择地消除冗余类。该模型在表示和分解具有类-子类关系的多个对象时显著提高了计算效率和准确性，克服了现有HDC模型诸如“叠加灾难”和“2问题”等限制。评价结果显示，FactorHD在表示规模为10^9的情况下相比于现有HDC模型实现了约5667倍的速度提升。当与ResNet-18神经网络结合使用时，FactorHD在Cifar-10数据集上实现了92.48%的分解准确率。 

---
# Cluster Contrast for Unsupervised Visual Representation Learning 

**Title (ZH)**: 无监督视觉表示学习的聚类对比方法 

**Authors**: Nikolaos Giakoumoglou, Tania Stathaki  

**Link**: [PDF](https://arxiv.org/pdf/2507.12359)  

**Abstract**: We introduce Cluster Contrast (CueCo), a novel approach to unsupervised visual representation learning that effectively combines the strengths of contrastive learning and clustering methods. Inspired by recent advancements, CueCo is designed to simultaneously scatter and align feature representations within the feature space. This method utilizes two neural networks, a query and a key, where the key network is updated through a slow-moving average of the query outputs. CueCo employs a contrastive loss to push dissimilar features apart, enhancing inter-class separation, and a clustering objective to pull together features of the same cluster, promoting intra-class compactness. Our method achieves 91.40% top-1 classification accuracy on CIFAR-10, 68.56% on CIFAR-100, and 78.65% on ImageNet-100 using linear evaluation with a ResNet-18 backbone. By integrating contrastive learning with clustering, CueCo sets a new direction for advancing unsupervised visual representation learning. 

**Abstract (ZH)**: 我们介绍了簇对比（CueCo），这是一种新颖的无监督视觉表示学习方法，有效地结合了对比学习和聚类方法的优点。受 recent 进展的启发，CueCo 设计用于同时在特征空间中分散和对齐特征表示。该方法使用两个神经网络——查询和键，其中键网络通过查询输出的慢移动平均值进行更新。CueCo 使用对比损失来推动不相似的特征分开，增强类间分离，使用聚类目标将相同簇的特征拉近，促进类内紧凑性。我们的方法在使用 ResNet-18 作为骨干网络的线性评估中，CIFAR-10 的 top-1 分类准确率为 91.40%，CIFAR-100 为 68.56%，ImageNet-100 为 78.65%。通过将对比学习与聚类相结合，CueCo 为推进无监督视觉表示学习设定了新的方向。 

---
# Neural Polar Decoders for Deletion Channels 

**Title (ZH)**: 神经极化解码器用于删除信道 

**Authors**: Ziv Aharoni, Henry D. Pfister  

**Link**: [PDF](https://arxiv.org/pdf/2507.12329)  

**Abstract**: This paper introduces a neural polar decoder (NPD) for deletion channels with a constant deletion rate. Existing polar decoders for deletion channels exhibit high computational complexity of $O(N^4)$, where $N$ is the block length. This limits the application of polar codes for deletion channels to short-to-moderate block lengths. In this work, we demonstrate that employing NPDs for deletion channels can reduce the computational complexity. First, we extend the architecture of the NPD to support deletion channels. Specifically, the NPD architecture consists of four neural networks (NNs), each replicating fundamental successive cancellation (SC) decoder operations. To support deletion channels, we change the architecture of only one. The computational complexity of the NPD is $O(AN\log N)$, where the parameter $A$ represents a computational budget determined by the user and is independent of the channel. We evaluate the new extended NPD for deletion channels with deletion rates $\delta\in\{0.01, 0.1\}$ and we verify the NPD with the ground truth given by the trellis decoder by Tal et al. We further show that due to the reduced complexity of the NPD, we are able to incorporate list decoding and further improve performance. We believe that the extended NPD presented here could have applications in future technologies like DNA storage. 

**Abstract (ZH)**: 一种用于恒定删除率信道的神经极化解码器 

---
# Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models 

**Title (ZH)**: 面向高保真度和高效生产扩散模型的组合离散潜码 

**Authors**: Samuel Lavoie, Michael Noukhovitch, Aaron Courville  

**Link**: [PDF](https://arxiv.org/pdf/2507.12318)  

**Abstract**: We argue that diffusion models' success in modeling complex distributions is, for the most part, coming from their input conditioning. This paper investigates the representation used to condition diffusion models from the perspective that ideal representations should improve sample fidelity, be easy to generate, and be compositional to allow out-of-training samples generation. We introduce Discrete Latent Code (DLC), an image representation derived from Simplicial Embeddings trained with a self-supervised learning objective. DLCs are sequences of discrete tokens, as opposed to the standard continuous image embeddings. They are easy to generate and their compositionality enables sampling of novel images beyond the training distribution. Diffusion models trained with DLCs have improved generation fidelity, establishing a new state-of-the-art for unconditional image generation on ImageNet. Additionally, we show that composing DLCs allows the image generator to produce out-of-distribution samples that coherently combine the semantics of images in diverse ways. Finally, we showcase how DLCs can enable text-to-image generation by leveraging large-scale pretrained language models. We efficiently finetune a text diffusion language model to generate DLCs that produce novel samples outside of the image generator training distribution. 

**Abstract (ZH)**: 我们argue rằng diffusion模型在建模复杂分布方面的成功主要归因于它们的输入条件。本文从理想表示应提高样本保真度、易于生成且具有可组合性以生成训练分布外样本的角度，探讨了用于条件化diffusion模型的表示方法。我们引入了离散潜码（DLC），这是一种源自Simplicial Embeddings并通过自我监督学习目标训练得到的图像表示方法。DLC是离散标记的序列，相比于标准的连续图像嵌入，它们更易于生成且其可组合性使得能够生成训练分布外的新颖图像。使用DLC训练的diffusion模型在图像生成保真度方面有所提升，建立了在ImageNet上无条件图像生成的新state-of-the-art。此外，我们展示通过组合DLC可以使图像生成器产生符合内部一致性的跨类别图像样本。最后，我们展示了DLC如何通过利用大规模预训练语言模型实现图文生成。我们高效地微调了一种文本diffusion语言模型以生成DLC，从而产生图像生成器训练分布外的新颖样本。 

---
# PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning 

**Title (ZH)**: PROL：基于提示在线学习的无回放流式数据连续学习 

**Authors**: M. Anwar Ma'sum, Mahardhika Pratama, Savitha Ramasamy, Lin Liu, Habibullah Habibullah, Ryszard Kowalczyk  

**Link**: [PDF](https://arxiv.org/pdf/2507.12305)  

**Abstract**: The data privacy constraint in online continual learning (OCL), where the data can be seen only once, complicates the catastrophic forgetting problem in streaming data. A common approach applied by the current SOTAs in OCL is with the use of memory saving exemplars or features from previous classes to be replayed in the current task. On the other hand, the prompt-based approach performs excellently in continual learning but with the cost of a growing number of trainable parameters. The first approach may not be applicable in practice due to data openness policy, while the second approach has the issue of throughput associated with the streaming data. In this study, we propose a novel prompt-based method for online continual learning that includes 4 main components: (1) single light-weight prompt generator as a general knowledge, (2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model (PTM) generalization preserving, and (4) hard-soft updates mechanism. Our proposed method achieves significantly higher performance than the current SOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity analysis shows that our method requires a relatively smaller number of parameters and achieves moderate training time, inference time, and throughput. For further study, the source code of our method is available at this https URL. 

**Abstract (ZH)**: 在线持续学习中的数据隐私约束 complicates 灾难性遗忘问题，在此类学习中数据仅可见一次。当前的SOTA方法通常使用节省内存的先前类别的示例或特征在当前任务中重新播放。另一方面，基于提示的方法在持续学习中表现出色，但代价是可训练参数数量的增长。由于数据开放政策，第一种方法可能在实践中不可行，而第二种方法与流式数据相关联的吞吐量问题。在这项研究中，我们提出了一种新的基于提示的在线持续学习方法，包含四个主要组成部分：（1）单个轻量级提示生成器作为通用知识，（2）可训练的比例移位器作为特定知识，（3）预训练模型泛化的保持，（4）硬软更新机制。我们提出的方法在CIFAR100、ImageNet-R、ImageNet-A和CUB数据集上的性能显著优于当前的SOTA方法。我们的时间复杂性分析表明，我们的方法需要相对较少的参数，并实现中等程度的训练时间、推理时间和吞吐量。对于我们提出的方法的源代码，可以在以下网址获取。 

---
# Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants 

**Title (ZH)**: 基于站点的逐层Fine-Tuning与渐进层冻结：面向极早早产儿Day-1胸部X光片的Bronchopulmonary Dysplasia稳健预测 

**Authors**: Sybelle Goedicke-Fritz, Michelle Bous, Annika Engel, Matthias Flotho, Pascal Hirsch, Hannah Wittig, Dino Milanovic, Dominik Mohr, Mathias Kaspar, Sogand Nemat, Dorothea Kerner, Arno Bücker, Andreas Keller, Sascha Meyer, Michael Zemlin, Philipp Flotho  

**Link**: [PDF](https://arxiv.org/pdf/2507.12269)  

**Abstract**: Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of extremely low birth weight infants. Defined by oxygen dependence at 36 weeks postmenstrual age, it causes lifelong respiratory complications. However, preventive interventions carry severe risks, including neurodevelopmental impairment, ventilator-induced lung injury, and systemic complications. Therefore, early BPD prognosis and prediction of BPD outcome is crucial to avoid unnecessary toxicity in low risk infants. Admission radiographs of extremely preterm infants are routinely acquired within 24h of life and could serve as a non-invasive prognostic tool. In this work, we developed and investigated a deep learning approach using chest X-rays from 163 extremely low-birth-weight infants ($\leq$32 weeks gestation, 401-999g) obtained within 24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult chest radiographs, employing progressive layer freezing with discriminative learning rates to prevent overfitting and evaluated a CutMix augmentation and linear probing. For moderate/severe BPD outcome prediction, our best performing model with progressive freezing, linear probing and CutMix achieved an AUROC of 0.78 $\pm$ 0.10, balanced accuracy of 0.69 $\pm$ 0.10, and an F1-score of 0.67 $\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet initialization (p = 0.031) which confirms domain-specific pretraining to be important for BPD outcome prediction. Routine IRDS grades showed limited prognostic value (AUROC 0.57 $\pm$ 0.11), confirming the need of learned markers. Our approach demonstrates that domain-specific pretraining enables accurate BPD prediction from routine day-1 radiographs. Through progressive freezing and linear probing, the method remains computationally feasible for site-level implementation and future federated learning deployments. 

**Abstract (ZH)**: 极低出生体重儿肺部发育不良(BPD)的胸部X射线早期预测：基于领域特定预训练的深度学习方法 

---
# A Framework for Nonstationary Gaussian Processes with Neural Network Parameters 

**Title (ZH)**: 非平稳高斯过程的神经网络参数框架 

**Authors**: Zachary James, Joseph Guinness  

**Link**: [PDF](https://arxiv.org/pdf/2507.12262)  

**Abstract**: Gaussian processes have become a popular tool for nonparametric regression because of their flexibility and uncertainty quantification. However, they often use stationary kernels, which limit the expressiveness of the model and may be unsuitable for many datasets. We propose a framework that uses nonstationary kernels whose parameters vary across the feature space, modeling these parameters as the output of a neural network that takes the features as input. The neural network and Gaussian process are trained jointly using the chain rule to calculate derivatives. Our method clearly describes the behavior of the nonstationary parameters and is compatible with approximation methods for scaling to large datasets. It is flexible and easily adapts to different nonstationary kernels without needing to redesign the optimization procedure. Our methods are implemented with the GPyTorch library and can be readily modified. We test a nonstationary variance and noise variant of our method on several machine learning datasets and find that it achieves better accuracy and log-score than both a stationary model and a hierarchical model approximated with variational inference. Similar results are observed for a model with only nonstationary variance. We also demonstrate our approach's ability to recover the nonstationary parameters of a spatial dataset. 

**Abstract (ZH)**: Gaussian过程已成为非参数回归的一个流行工具，这是因为它们的灵活性和不确定性量化。然而，它们通常使用stationary内核，这限制了模型的表达能力，并可能不适合许多数据集。我们提出了一种框架，该框架使用非stationary内核，其参数在特征空间中变化，并将这些参数建模为神经网络的输出，该神经网络将特征作为输入。神经网络和Gaussian过程通过链式规则计算导数进行联合训练。我们的方法清晰地描述了非stationary参数的行为，并且与用于处理大规模数据集的近似方法兼容。该方法具有灵活性，无需重新设计优化程序即可轻松适应不同类型的非stationary内核。我们使用GPyTorch库实现这些方法，并可以方便地进行修改。我们对几个机器学习数据集测试了我们的非stationary方差和噪声变体方法，发现它在准确性和log-分数方面优于stationary模型和用变分推断近似的分层模型。具有仅非stationary方差的模型也观察到了类似的成果。我们还展示了我们的方法恢复空间数据集中非stationary参数的能力。 

---
# Looking for Fairness in Recommender Systems 

**Title (ZH)**: 在推荐系统中寻找公平性 

**Authors**: Cécile Logé  

**Link**: [PDF](https://arxiv.org/pdf/2507.12242)  

**Abstract**: Recommender systems can be found everywhere today, shaping our everyday experience whenever we're consuming content, ordering food, buying groceries online, or even just reading the news. Let's imagine we're in the process of building a recommender system to make content suggestions to users on social media. When thinking about fairness, it becomes clear there are several perspectives to consider: the users asking for tailored suggestions, the content creators hoping for some limelight, and society at large, navigating the repercussions of algorithmic recommendations. A shared fairness concern across all three is the emergence of filter bubbles, a side-effect that takes place when recommender systems are almost "too good", making recommendations so tailored that users become inadvertently confined to a narrow set of opinions/themes and isolated from alternative ideas. From the user's perspective, this is akin to manipulation. From the small content creator's perspective, this is an obstacle preventing them access to a whole range of potential fans. From society's perspective, the potential consequences are far-reaching, influencing collective opinions, social behavior and political decisions. How can our recommender system be fine-tuned to avoid the creation of filter bubbles, and ensure a more inclusive and diverse content landscape? Approaching this problem involves defining one (or more) performance metric to represent diversity, and tweaking our recommender system's performance through the lens of fairness. By incorporating this metric into our evaluation framework, we aim to strike a balance between personalized recommendations and the broader societal goal of fostering rich and varied cultures and points of view. 

**Abstract (ZH)**: 推荐系统无处不在：构建社交媒体内容推荐系统中的公平性考量与实现 

---
# Sparse Autoencoders for Sequential Recommendation Models: Interpretation and Flexible Control 

**Title (ZH)**: 稀疏自编码器在序列推荐模型中的应用：解释与灵活控制 

**Authors**: Anton Klenitskiy, Konstantin Polev, Daria Denisova, Alexey Vasilev, Dmitry Simakov, Gleb Gusev  

**Link**: [PDF](https://arxiv.org/pdf/2507.12202)  

**Abstract**: Many current state-of-the-art models for sequential recommendations are based on transformer architectures. Interpretation and explanation of such black box models is an important research question, as a better understanding of their internals can help understand, influence, and control their behavior, which is very important in a variety of real-world applications. Recently sparse autoencoders (SAE) have been shown to be a promising unsupervised approach for extracting interpretable features from language models. These autoencoders learn to reconstruct hidden states of the transformer's internal layers from sparse linear combinations of directions in their activation space.
This paper is focused on the application of SAE to the sequential recommendation domain. We show that this approach can be successfully applied to the transformer trained on a sequential recommendation task: learned directions turn out to be more interpretable and monosemantic than the original hidden state dimensions. Moreover, we demonstrate that the features learned by SAE can be used to effectively and flexibly control the model's behavior, providing end-users with a straightforward method to adjust their recommendations to different custom scenarios and contexts. 

**Abstract (ZH)**: 基于稀疏自编码器的顺序推荐领域应用研究 

---
# Quantize More, Lose Less: Autoregressive Generation from Residually Quantized Speech Representations 

**Title (ZH)**: 量化更多，损失更少：基于残余量化语音表示的自回归生成 

**Authors**: Yichen Han, Xiaoyang Hao, Keming Chen, Weibo Xiong, Jun He, Ruonan Zhang, Junjie Cao, Yue Liu, Bowen Li, Dongrui Zhang, Hui Xia, Huilei Fu, Kai Jia, Kaixuan Guo, Mingli Jin, Qingyun Meng, Ruidong Ma, Ruiqian Fang, Shaotong Guo, Xuhui Li, Yang Xiang, Ying Zhang, Yulong Liu, Yunfeng Li, Yuyi Zhang, Yuze Zhou, Zhen Wang, Zhaowen Chen  

**Link**: [PDF](https://arxiv.org/pdf/2507.12197)  

**Abstract**: Text-to-speech (TTS) synthesis has seen renewed progress under the discrete modeling paradigm. Existing autoregressive approaches often rely on single-codebook representations, which suffer from significant information loss. Even with post-hoc refinement techniques such as flow matching, these methods fail to recover fine-grained details (e.g., prosodic nuances, speaker-specific timbres), especially in challenging scenarios like singing voice or music synthesis. We propose QTTS, a novel TTS framework built upon our new audio codec, QDAC. The core innovation of QDAC lies in its end-to-end training of an ASR-based auto-regressive network with a GAN, which achieves superior semantic feature disentanglement for scalable, near-lossless compression. QTTS models these discrete codes using two innovative strategies: the Hierarchical Parallel architecture, which uses a dual-AR structure to model inter-codebook dependencies for higher-quality synthesis, and the Delay Multihead approach, which employs parallelized prediction with a fixed delay to accelerate inference speed. Our experiments demonstrate that the proposed framework achieves higher synthesis quality and better preserves expressive content compared to baseline. This suggests that scaling up compression via multi-codebook modeling is a promising direction for high-fidelity, general-purpose speech and audio generation. 

**Abstract (ZH)**: 基于离散建模范式的文本到speech合成已取得新的进展。现有自回归方法通常依赖单码本表示，存在显著信息丢失的问题。即使采用流匹配等后处理技术，这些方法在如歌声合成或音乐合成等挑战场景下，也无法恢复细微的语音特征（如韵律细节、特定讲话者音色），我们提出了QTTS，一种基于我们新音频编解码器QDAC的新型文本到speech框架。QDAC的核心创新在于将基于ASR的自回归网络与GAN结合进行端到端训练，从而实现更优的语义特征分离，以实现可扩展且近乎无损的压缩。QTTS通过两种创新策略来建模这些离散代码：层次并行架构，该架构采用双自回归结构以更好地建模跨码本依赖关系，提升合成质量；延迟多头方法，该方法通过并行预测结合固定延迟来加速推理速度。我们的实验表明，所提出的框架在合成质量和保持表达内容方面优于基线，这表明通过多码本建模放大压缩是高质量、通用语音和音频生成的一个有希望的方向。 

---
# Selective Quantization Tuning for ONNX Models 

**Title (ZH)**: ONNX模型的选择性量化调优 

**Authors**: Nikolaos Louloudakis, Ajitha Rajan  

**Link**: [PDF](https://arxiv.org/pdf/2507.12196)  

**Abstract**: Quantization is a process that reduces the precision of deep neural network models to lower model size and computational demands, often at the cost of accuracy. However, fully quantized models may exhibit sub-optimal performance below acceptable levels and face deployment challenges on low-end hardware accelerators due to practical constraints. To address these issues, quantization can be selectively applied to only a subset of layers, but selecting which layers to exclude is non-trivial. To this direction, we propose TuneQn, a suite enabling selective quantization, deployment and execution of ONNX models across various CPU and GPU devices, combined with profiling and multi-objective optimization. TuneQn generates selectively quantized ONNX models, deploys them on different hardware, measures performance on metrics like accuracy and size, performs Pareto Front minimization to identify the best model candidate and visualizes the results. To demonstrate the effectiveness of TuneQn, we evaluated TuneQn on four ONNX models with two quantization settings across CPU and GPU devices. As a result, we demonstrated that our utility effectively performs selective quantization and tuning, selecting ONNX model candidates with up to a $54.14$% reduction in accuracy loss compared to the fully quantized model, and up to a $72.9$% model size reduction compared to the original model. 

**Abstract (ZH)**: TuneQn：一种针对ONNX模型的选择性量化、部署和执行套件，结合了性能分析和多目标优化 

---
# BenchRL-QAS: Benchmarking reinforcement learning algorithms for quantum architecture search 

**Title (ZH)**: BenchRL-QAS: 量子架构搜索中强化学习算法的基准测试 

**Authors**: Azhar Ikhtiarudin, Aditi Das, Param Thakkar, Akash Kundu  

**Link**: [PDF](https://arxiv.org/pdf/2507.12189)  

**Abstract**: We introduce BenchRL-QAS, a unified benchmarking framework for systematically evaluating reinforcement learning (RL) algorithms in quantum architecture search (QAS) across diverse variational quantum algorithm tasks and system sizes ranging from 2- to 8-qubit. Our study benchmarks nine RL agents including both value-based and policy-gradient methods on representative quantum problems such as variational quantum eigensolver, variational quantum state diagonalization, quantum classification, and state preparation, spanning both noiseless and realistic noisy regimes. We propose a weighted ranking metric that balances accuracy, circuit depth, gate count, and computational efficiency, enabling fair and comprehensive comparison. Our results first reveal that RL-based quantum classifier outperforms baseline variational classifiers. Then we conclude that no single RL algorithm is universally optimal when considering a set of QAS tasks; algorithmic performance is highly context-dependent, varying with task structure, qubit count, and noise. This empirical finding provides strong evidence for the "no free lunch" principle in RL-based quantum circuit design and highlights the necessity of tailored algorithm selection and systematic benchmarking for advancing quantum circuit synthesis. This work represents the most comprehensive RL-QAS benchmarking effort to date, and BenchRL-QAS along with all experimental data are made publicly available to support reproducibility and future research this https URL. 

**Abstract (ZH)**: BenchRL-QAS：量子架构搜索中基于强化学习的统一基准框架 

---
# PRISM: Distributed Inference for Foundation Models at Edge 

**Title (ZH)**: PRISM: 边缘端基础模型分布式推理 

**Authors**: Muhammad Azlan Qazi, Alexandros Iosifidis, Qi Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2507.12145)  

**Abstract**: Foundation models (FMs) have achieved remarkable success across a wide range of applications, from image classification to natural langurage processing, but pose significant challenges for deployment at edge. This has sparked growing interest in developing practical and efficient strategies for bringing foundation models to edge environments. In this work, we propose PRISM, a communication-efficient and compute-aware strategy for distributed Transformer inference on edge devices. Our method leverages a Segment Means representation to approximate intermediate output features, drastically reducing inter-device communication. Additionally, we restructure the self-attention mechanism to eliminate redundant computations caused by per-device Key/Value calculation in position-wise partitioning and design a partition-aware causal masking scheme tailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2 across diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and CBT. Our results demonstrate substantial reductions in communication overhead (up to 99.2% for BERT at compression rate CR = 128) and per-device computation (51.24% for BERT at the same setting), with only minor accuracy degradation. This method offers a scalable and practical solution for deploying foundation models in distributed resource-constrained environments. 

**Abstract (ZH)**: PRISM：适用于边缘设备的通信高效且计算感知的分布式Transformer推理策略 

---
# Non-Adaptive Adversarial Face Generation 

**Title (ZH)**: 非自适应对抗面部生成 

**Authors**: Sunpill Kim, Seunghun Paik, Chanwoo Hwang, Minsu Kim, Jae Hong Seo  

**Link**: [PDF](https://arxiv.org/pdf/2507.12107)  

**Abstract**: Adversarial attacks on face recognition systems (FRSs) pose serious security and privacy threats, especially when these systems are used for identity verification. In this paper, we propose a novel method for generating adversarial faces-synthetic facial images that are visually distinct yet recognized as a target identity by the FRS. Unlike iterative optimization-based approaches (e.g., gradient descent or other iterative solvers), our method leverages the structural characteristics of the FRS feature space. We figure out that individuals sharing the same attribute (e.g., gender or race) form an attributed subsphere. By utilizing such subspheres, our method achieves both non-adaptiveness and a remarkably small number of queries. This eliminates the need for relying on transferability and open-source surrogate models, which have been a typical strategy when repeated adaptive queries to commercial FRSs are impossible. Despite requiring only a single non-adaptive query consisting of 100 face images, our method achieves a high success rate of over 93% against AWS's CompareFaces API at its default threshold. Furthermore, unlike many existing attacks that perturb a given image, our method can deliberately produce adversarial faces that impersonate the target identity while exhibiting high-level attributes chosen by the adversary. 

**Abstract (ZH)**: 对抗攻击对面部识别系统（FRS）的威胁：一种基于结构特征的生成对抗性面部的方法 

---
# BOOKCOREF: Coreference Resolution at Book Scale 

**Title (ZH)**: BOOKCOREF：图书规模的共指解析 

**Authors**: Giuliano Martinelli, Tommaso Bonomo, Pere-Lluís Huguet Cabot, Roberto Navigli  

**Link**: [PDF](https://arxiv.org/pdf/2507.12075)  

**Abstract**: Coreference Resolution systems are typically evaluated on benchmarks containing small- to medium-scale documents. When it comes to evaluating long texts, however, existing benchmarks, such as LitBank, remain limited in length and do not adequately assess system capabilities at the book scale, i.e., when co-referring mentions span hundreds of thousands of tokens. To fill this gap, we first put forward a novel automatic pipeline that produces high-quality Coreference Resolution annotations on full narrative texts. Then, we adopt this pipeline to create the first book-scale coreference benchmark, BOOKCOREF, with an average document length of more than 200,000 tokens. We carry out a series of experiments showing the robustness of our automatic procedure and demonstrating the value of our resource, which enables current long-document coreference systems to gain up to +20 CoNLL-F1 points when evaluated on full books. Moreover, we report on the new challenges introduced by this unprecedented book-scale setting, highlighting that current models fail to deliver the same performance they achieve on smaller documents. We release our data and code to encourage research and development of new book-scale Coreference Resolution systems at this https URL. 

**Abstract (ZH)**: 核心参考决议系统通常在包含小型到中型规模文档的基准上进行评估。然而，在评估长文本时，现有的基准，如LitBank，在长度上仍然有限，并不能充分评估系统在图书规模下的能力，即当共指mention跨越数十万token时。为了填补这一空白，我们首先提出了一种新颖的自动管道，用于生成高质量的核心参考决议注释，覆盖完整的叙述文本。然后，我们采用此管道创建了首个图书规模的核心参考基准BOOKCOREF，其文档平均长度超过200,000token。我们进行了一系列实验，展示了我们自动流程的稳健性，并证明了我们资源的价值，这对于评估完整书籍的核心参考系统性能提高了多达+20 CoNLL-F1点。此外，我们报告了由此开创的图书规模设置引入的新挑战，指出当前模型在小型文档上的表现无法在大文档中达到相同水平。我们在此发布我们的数据和代码，以鼓励对新的图书规模核心参考决议系统的研究和发展：https://yourlinkhere.com 

---
# StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features 

**Title (ZH)**: StylOch 在 PAN 上：基于频率的体式特征的梯度提升树 

**Authors**: Jeremi K. Ochab, Mateusz Matias, Tymoteusz Boba, Tomasz Walkowiak  

**Link**: [PDF](https://arxiv.org/pdf/2507.12064)  

**Abstract**: This submission to the binary AI detection task is based on a modular stylometric pipeline, where: public spaCy models are used for text preprocessing (including tokenisation, named entity recognition, dependency parsing, part-of-speech tagging, and morphology annotation) and extracting several thousand features (frequencies of n-grams of the above linguistic annotations); light-gradient boosting machines are used as the classifier. We collect a large corpus of more than 500 000 machine-generated texts for the classifier's training. We explore several parameter options to increase the classifier's capacity and take advantage of that training set. Our approach follows the non-neural, computationally inexpensive but explainable approach found effective previously. 

**Abstract (ZH)**: 基于模块化文体学流水线的二进制AI检测任务提交 

---
# Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery 

**Title (ZH)**: 基于视图内和视图间相关性的多视图新型类发现 

**Authors**: Xinhang Wan, Jiyuan Liu, Qian Qu, Suyuan Liu, Chuyu Zhang, Fangdi Wang, Xinwang Liu, En Zhu, Kunlun He  

**Link**: [PDF](https://arxiv.org/pdf/2507.12029)  

**Abstract**: In this paper, we address the problem of novel class discovery (NCD), which aims to cluster novel classes by leveraging knowledge from disjoint known classes. While recent advances have made significant progress in this area, existing NCD methods face two major limitations. First, they primarily focus on single-view data (e.g., images), overlooking the increasingly common multi-view data, such as multi-omics datasets used in disease diagnosis. Second, their reliance on pseudo-labels to supervise novel class clustering often results in unstable performance, as pseudo-label quality is highly sensitive to factors such as data noise and feature dimensionality. To address these challenges, we propose a novel framework named Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery (IICMVNCD), which is the first attempt to explore NCD in multi-view setting so far. Specifically, at the intra-view level, leveraging the distributional similarity between known and novel classes, we employ matrix factorization to decompose features into view-specific shared base matrices and factor matrices. The base matrices capture distributional consistency among the two datasets, while the factor matrices model pairwise relationships between samples. At the inter-view level, we utilize view relationships among known classes to guide the clustering of novel classes. This includes generating predicted labels through the weighted fusion of factor matrices and dynamically adjusting view weights of known classes based on the supervision loss, which are then transferred to novel class learning. Experimental results validate the effectiveness of our proposed approach. 

**Abstract (ZH)**: 基于 intra-view 和 inter-view 相关性的多视角新型类发现 (IICMVNCD) 

---
# DUSE: A Data Expansion Framework for Low-resource Automatic Modulation Recognition based on Active Learning 

**Title (ZH)**: DUSE：一种基于主动学习的稀少资源自动调制识别数据扩展框架 

**Authors**: Yao Lu, Hongyu Gao, Zhuangzhi Chen, Dongwei Xu, Yun Lin, Qi Xuan, Guan Gui  

**Link**: [PDF](https://arxiv.org/pdf/2507.12011)  

**Abstract**: Although deep neural networks have made remarkable achievements in the field of automatic modulation recognition (AMR), these models often require a large amount of labeled data for training. However, in many practical scenarios, the available target domain data is scarce and difficult to meet the needs of model training. The most direct way is to collect data manually and perform expert annotation, but the high time and labor costs are unbearable. Another common method is data augmentation. Although it can enrich training samples to a certain extent, it does not introduce new data and therefore cannot fundamentally solve the problem of data scarcity. To address these challenges, we introduce a data expansion framework called Dynamic Uncertainty-driven Sample Expansion (DUSE). Specifically, DUSE uses an uncertainty scoring function to filter out useful samples from relevant AMR datasets and employs an active learning strategy to continuously refine the scorer. Extensive experiments demonstrate that DUSE consistently outperforms 8 coreset selection baselines in both class-balance and class-imbalance settings. Besides, DUSE exhibits strong cross-architecture generalization for unseen models. 

**Abstract (ZH)**: 动态不确定性驱动样本扩展框架（DUSE）在自动调制识别领域的数据扩展 

---
# Formal Verification of Neural Certificates Done Dynamically 

**Title (ZH)**: 动态验证神经证书的形式化验证 

**Authors**: Thomas A. Henzinger, Konstantin Kueffner, Emily Yu  

**Link**: [PDF](https://arxiv.org/pdf/2507.11987)  

**Abstract**: Neural certificates have emerged as a powerful tool in cyber-physical systems control, providing witnesses of correctness. These certificates, such as barrier functions, often learned alongside control policies, once verified, serve as mathematical proofs of system safety. However, traditional formal verification of their defining conditions typically faces scalability challenges due to exhaustive state-space exploration. To address this challenge, we propose a lightweight runtime monitoring framework that integrates real-time verification and does not require access to the underlying control policy. Our monitor observes the system during deployment and performs on-the-fly verification of the certificate over a lookahead region to ensure safety within a finite prediction horizon. We instantiate this framework for ReLU-based control barrier functions and demonstrate its practical effectiveness in a case study. Our approach enables timely detection of safety violations and incorrect certificates with minimal overhead, providing an effective but lightweight alternative to the static verification of the certificates. 

**Abstract (ZH)**: 神经证书已成为物理信息系统控制的强大工具，提供了正确性的证据。这些证书，例如边界函数，通常与控制策略一同学习，验证后成为系统安全性的数学证明。然而，传统形式验证通常由于耗尽状态空间的探索而面临可扩展性挑战。为了应对这一挑战，我们提出了一种轻量级的运行时监控框架，该框架集成实时验证且无需访问底层控制策略。该监控器在系统部署期间进行观察，并对预视窗口内的证书即时进行验证，以确保在有限的预测窗口内系统的安全性。我们为基于ReLU的控制边界函数实例化了该框架，并在案例研究中证明了其实用有效性。我们的方法能够及时检测安全违规和不正确的证书，且具有最小的开销，提供了一种有效但轻量级的证书静态验证替代方案。 

---
# Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation 

**Title (ZH)**: 面向毒有害内容的少样本提示生成方法在低资源新加坡英语翻译中的应用 

**Authors**: Ziyu Ge, Gabriel Chua, Leanne Tan, Roy Ka-Wei Lee  

**Link**: [PDF](https://arxiv.org/pdf/2507.11966)  

**Abstract**: As online communication increasingly incorporates under-represented languages and colloquial dialects, standard translation systems often fail to preserve local slang, code-mixing, and culturally embedded markers of harmful speech. Translating toxic content between low-resource language pairs poses additional challenges due to scarce parallel data and safety filters that sanitize offensive expressions. In this work, we propose a reproducible, two-stage framework for toxicity-preserving translation, demonstrated on a code-mixed Singlish safety corpus. First, we perform human-verified few-shot prompt engineering: we iteratively curate and rank annotator-selected Singlish-target examples to capture nuanced slang, tone, and toxicity. Second, we optimize model-prompt pairs by benchmarking several large language models using semantic similarity via direct and back-translation. Quantitative human evaluation confirms the effectiveness and efficiency of our pipeline. Beyond improving translation quality, our framework contributes to the safety of multicultural LLMs by supporting culturally sensitive moderation and benchmarking in low-resource contexts. By positioning Singlish as a testbed for inclusive NLP, we underscore the importance of preserving sociolinguistic nuance in real-world applications such as content moderation and regional platform governance. 

**Abstract (ZH)**: 随着在线通信越来越多地 Incorporating 偏言僻语和口语方言，标准翻译系统往往无法保留当地俚语、代码混合和文化嵌入的有害言论标记。由于低资源语言对之间的平行数据稀缺以及会过滤冒犯性表达的安全过滤器，翻译低资源语言对之间的有害内容带来了额外的挑战。在本文中，我们提出了一种可重现的两阶段框架，用于保留有害内容的翻译，并在混合方言的安全语料库上进行了展示。首先，我们进行人工验证的小样本提示工程：我们迭代地整理和排名注释者选择的Singlish目标示例，以捕捉细微的俚语、语气和有害内容。第二，我们通过多种大型语言模型的基准测试，使用语义相似性通过直接翻译和反向翻译优化模型-提示对。定量的人类评估证实了我们管道的有效性和效率。除了改善翻译质量，我们的框架还通过提供文化敏感的调节和支持低资源情境下的基准测试，促进了多文化大语言模型的安全性。通过将Singlish作为包容性自然语言处理的试验平台，我们强调了在内容调节和区域平台治理等实际应用中保留社会语言学细腻之处的重要性。 

---
# Kevin: Multi-Turn RL for Generating CUDA Kernels 

**Title (ZH)**: Kevin: 多轮强化学习生成 CUDA 内核 

**Authors**: Carlo Baronio, Pietro Marsella, Ben Pan, Simon Guo, Silas Alberti  

**Link**: [PDF](https://arxiv.org/pdf/2507.11948)  

**Abstract**: Writing GPU kernels is a challenging task and critical for AI systems' efficiency. It is also highly iterative: domain experts write code and improve performance through execution feedback. Moreover, it presents verifiable rewards like correctness and speedup, making it a natural environment to apply Reinforcement Learning (RL). To explicitly incorporate the iterative nature of this process into training, we develop a flexible multi-turn RL recipe that addresses unique challenges encountered in real-world settings, such as learning from long trajectories and effective reward attribution across turns. We present Kevin - K(ernel D)evin, the first model trained with multi-turn RL for CUDA kernel generation and optimization. In our evaluation setup, Kevin shows significant gains over its base model (QwQ-32B), improving correctness of generated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to 1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini (0.78x). Finally, we study its behavior across test-time scaling axes: we found scaling serial refinement more beneficial than parallel sampling. In particular, when given more refinement turns, Kevin shows a higher rate of improvement. 

**Abstract (ZH)**: GPU内核编写是一项具有挑战性的工作，对于AI系统的效率至关重要。它也是高度迭代的：领域专家通过执行反馈编写代码并提高性能。此外，它提供了可验证的奖励，如正确性和加速比，使其成为一个自然的应用强化学习（RL）的环境。为了将这一过程的迭代性质明确地融入训练中，我们开发了一个灵活的多轮RL方法，以解决实际场景中遇到的独特挑战，如学习长轨迹和跨轮次的有效奖励归因。我们介绍了K(ernel D)evin（Kevin），这是第一个使用多轮RL训练的CUDA内核生成和优化模型。在我们的评估设置中，Kevin在生成内核的正确性和平均加速比方面显著优于其基模型（QwQ-32B），并在纯CUDA中将生成内核的正确性从56%提高到82%，平均加速比从基线（PyTorch Eager）的0.53倍提高到1.10倍，并超越了前沿模型o4-mini（0.78倍）。最后，我们研究了其在测试时扩展会话轴的行为：我们发现串行细化比并行采样更有益，特别是在获得更多细化轮次时，Kevin显示出更高的改进率。 

---
# POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering 

**Title (ZH)**: 多模态图表问答基准：多语言视觉-语言模型评估 

**Authors**: Yichen Xu, Liangyu Chen, Liang Zhang, Wenxuan Wang, Qin Jin  

**Link**: [PDF](https://arxiv.org/pdf/2507.11939)  

**Abstract**: Charts are a universally adopted medium for interpreting and communicating data. However, existing chart understanding benchmarks are predominantly English-centric, limiting their accessibility and applicability to global audiences. In this paper, we present PolyChartQA, the first large-scale multilingual chart question answering benchmark covering 22,606 charts and 26,151 question-answering pairs across 10 diverse languages. PolyChartQA is built using a decoupled pipeline that separates chart data from rendering code, allowing multilingual charts to be flexibly generated by simply translating the data and reusing the code. We leverage state-of-the-art LLM-based translation and enforce rigorous quality control in the pipeline to ensure the linguistic and semantic consistency of the generated multilingual charts. PolyChartQA facilitates systematic evaluation of multilingual chart understanding. Experiments on both open- and closed-source large vision-language models reveal a significant performance gap between English and other languages, especially low-resource ones with non-Latin scripts. This benchmark lays a foundation for advancing globally inclusive vision-language models. 

**Abstract (ZH)**: 图表是一种广泛采用的数据解释与交流工具。然而，现有的图表理解基准主要以英语为中心，限制了其对全球受众的访问性和适用性。本文介绍了PolyChartQA，这是一个涵盖22,606幅图表和26,151个问答对、包含10种不同语言的首个大规模多语言图表问答基准。PolyChartQA 使用了一个解耦的管道，将图表数据与渲染代码分离，允许通过简单翻译数据并重用代码来灵活生成多语言图表。我们利用最先进的基于LLM的翻译，并在管道中实施严格的质量控制，以确保生成的多语言图表在语言和语义上的一致性。PolyChartQA 促进了多语言图表理解的系统评估。在开源和闭源大型视觉-语言模型上的实验揭示了英语与其他语言之间，尤其是使用非拉丁字符的低资源语言之间存在显著的表现差异。这个基准为推动全球包容性的视觉-语言模型的发展奠定了基础。 

---
# A Survey of Deep Learning for Geometry Problem Solving 

**Title (ZH)**: 深度学习在几何问题求解领域的研究综述 

**Authors**: Jianzhe Ma, Wenxuan Wang, Qin Jin  

**Link**: [PDF](https://arxiv.org/pdf/2507.11936)  

**Abstract**: Geometry problem solving is a key area of mathematical reasoning, which is widely involved in many important fields such as education, mathematical ability assessment of artificial intelligence, and multimodal ability assessment. In recent years, the rapid development of deep learning technology, especially the rise of multimodal large language models, has triggered a widespread research boom. This paper provides a survey of the applications of deep learning in geometry problem solving, including (i) a comprehensive summary of the relevant tasks in geometry problem solving; (ii) a thorough review of related deep learning methods; (iii) a detailed analysis of evaluation metrics and methods; and (iv) a critical discussion of the current challenges and future directions that can be explored. Our goal is to provide a comprehensive and practical reference of deep learning for geometry problem solving to promote further developments in this field. We create a continuously updated list of papers on GitHub: this https URL. 

**Abstract (ZH)**: 深度学习在几何问题求解中的应用综述：包括（i）几何问题求解相关任务的全面总结；（ii）相关深度学习方法的彻底审查；（iii）评价指标和方法的详细分析；以及（iv）当前挑战和未来方向的批判性讨论。我们的目标是为几何问题求解提供一个全面和实用的深度学习参考，促进该领域的进一步发展。我们在GitHub上创建了一个不断更新的论文列表：this https URL。 

---
# Native-AI Empowered Scalable Architectures and Solutions for Future Non-Terrestrial Networks: An Overview 

**Title (ZH)**: 基于本地人工智能赋能的面向未来非地基网络的可扩展架构与解决方案：概览 

**Authors**: Jikang Deng, Fizza Hassan, Hui Zhou, Saad Al-Ahmadi, Mohamed-Slim Alouini, Daniel B. Da Costa  

**Link**: [PDF](https://arxiv.org/pdf/2507.11935)  

**Abstract**: As the path toward 6G networks is being charted, the emerging applications have motivated evolutions of network architectures to realize the efficient, reliable, and flexible wireless networks. Among the potential architectures, the non-terrestrial network (NTN) and open radio access network (ORAN) have received increasing interest from both academia and industry. Although the deployment of NTNs ensures coverage, enhances spectral efficiency, and improves the resilience of wireless networks. The high altitude and mobility of NTN present new challenges in the development and operations (DevOps) lifecycle, hindering intelligent and scalable network management due to the lack of native artificial intelligence (AI) capability. With the advantages of ORAN in disaggregation, openness, virtualization, and intelligence, several works propose integrating ORAN principles into the NTN, focusing mainly on ORAN deployment options based on transparent and regenerative systems. However, a holistic view of how to effectively combine ORAN and NTN throughout the DevOps lifecycle is still missing, especially regarding how intelligent ORAN addresses the scalability challenges in NTN. Motivated by this, in this paper, we first provide the background knowledge about ORAN and NTN, outline the state-of-the-art research on ORAN for NTNs, and present the DevOps challenges that motivate the adoption of ORAN solutions. We then propose the ORAN-based NTN framework, discussing its features and architectures in detail. These include the discussion about flexible fronthaul split, RAN intelligent controllers (RICs) enhancement for distributed learning, scalable deployment architecture, and multi-domain service management. Finally, the future research directions, including combinations of the ORAN-based NTN framework and other enabling technologies and schemes, as well as the candidate use cases, are highlighted. 

**Abstract (ZH)**: 随着向6G网络的发展路径被勾画，新兴应用促使网络架构演进以实现高效、可靠和灵活的无线网络。在潜在架构中，非地基网络（NTN）和开放无线接入网络（ORAN）受到了学术界和产业界的广泛关注。尽管部署NTN确保了覆盖范围、提升了频谱效率并改善了无线网络的韧性，但NTN的高海拔和高移动性给开发和运维（DevOps）生命周期带来了新的挑战，由于缺乏内置的人工智能（AI）能力，这阻碍了智能和可扩展网络管理的实现。利用ORAN在解耦、开放性、虚拟化和智能化方面的优势，一些研究提出了将ORAN原则集成到NTN中的方案，主要集中在基于透明和再生系统的ORAN部署选项上。然而，有关如何在DevOps生命周期中有效地结合ORAN和NTN的全面观点仍然缺失，尤其是在智能ORAN如何应对NTN的可扩展性挑战方面。受此驱动，在本文中，我们首先提供关于ORAN和NTN的背景知识，概述了针对NTN的ORAN的最新研究，并阐述了促使采用ORAN解决方案的DevOps挑战。然后，我们提出了基于ORAN的NTN框架，详细讨论了其特点和架构，包括灵活前传分割、RICs增强以支持分布式学习、可扩展部署架构以及多域服务管理。最后，我们指出了未来研究的方向，包括基于ORAN的 NTN框架与其他使能技术和方案的结合，以及潜在的应用案例。 

---
# Interactive Hybrid Rice Breeding with Parametric Dual Projection 

**Title (ZH)**: 参数双投影驱动的互动水稻杂交育种 

**Authors**: Changjian Chen, Pengcheng Wang, Fei Lyu, Zhuo Tang, Li Yang, Long Wang, Yong Cai, Feng Yu, Kenli Li  

**Link**: [PDF](https://arxiv.org/pdf/2507.11848)  

**Abstract**: Hybrid rice breeding crossbreeds different rice lines and cultivates the resulting hybrids in fields to select those with desirable agronomic traits, such as higher yields. Recently, genomic selection has emerged as an efficient way for hybrid rice breeding. It predicts the traits of hybrids based on their genes, which helps exclude many undesired hybrids, largely reducing the workload of field cultivation. However, due to the limited accuracy of genomic prediction models, breeders still need to combine their experience with the models to identify regulatory genes that control traits and select hybrids, which remains a time-consuming process. To ease this process, in this paper, we proposed a visual analysis method to facilitate interactive hybrid rice breeding. Regulatory gene identification and hybrid selection naturally ensemble a dual-analysis task. Therefore, we developed a parametric dual projection method with theoretical guarantees to facilitate interactive dual analysis. Based on this dual projection method, we further developed a gene visualization and a hybrid visualization to verify the identified regulatory genes and hybrids. The effectiveness of our method is demonstrated through the quantitative evaluation of the parametric dual projection method, identified regulatory genes and desired hybrids in the case study, and positive feedback from breeders. 

**Abstract (ZH)**: 杂交水稻育种将不同的水稻品系进行杂交，并在田间种植杂交后代，选择具有desired agronomic traits（如高产量）的水稻。近年来，基因组选择已成为杂交水稻育种的有效方法。它基于基因预测杂交后代的性状，有助于排除许多不 desired的杂交后代，大大减少了田间种植的工作量。然而，由于基因组预测模型的准确性有限，育种者仍需结合经验和模型来识别控制性状的调控基因并选择杂交后代，这仍然是一个耗时的过程。为了简化这一过程，本文提出了一种可视化分析方法，以促进交互式杂交水稻育种。调控基因识别和杂交选择自然构成一项双重分析任务。因此，我们开发了一种带有理论保证的参数双重投影方法，以促进交互式双重分析。基于这一双重投影方法，我们进一步开发了一种基因可视化和杂交可视化，以验证识别出的调控基因和理想的杂交后代。通过案例研究中的定量评估、识别出的调控基因和期望的杂交后代的有效性，以及育种者的正面反馈，证明了我们方法的有效性。 

---
# MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory 

**Title (ZH)**: MNIST-Gen: 基于层次语义、强化学习和范畴论的模块化MNIST样式数据集生成 

**Authors**: Pouya Shaeri, Arash Karimi, Ariane Middel  

**Link**: [PDF](https://arxiv.org/pdf/2507.11821)  

**Abstract**: Neural networks are often benchmarked using standard datasets such as MNIST, FashionMNIST, or other variants of MNIST, which, while accessible, are limited to generic classes such as digits or clothing items. For researchers working on domain-specific tasks, such as classifying trees, food items, or other real-world objects, these data sets are insufficient and irrelevant. Additionally, creating and publishing a custom dataset can be time consuming, legally constrained, or beyond the scope of individual projects. We present MNIST-Gen, an automated, modular, and adaptive framework for generating MNIST-style image datasets tailored to user-specified categories using hierarchical semantic categorization. The system combines CLIP-based semantic understanding with reinforcement learning and human feedback to achieve intelligent categorization with minimal manual intervention. Our hierarchical approach supports complex category structures with semantic characteristics, enabling fine-grained subcategorization and multiple processing modes: individual review for maximum control, smart batch processing for large datasets, and fast batch processing for rapid creation. Inspired by category theory, MNIST-Gen models each data transformation stage as a composable morphism, enhancing clarity, modularity, and extensibility. As proof of concept, we generate and benchmark two novel datasets-\textit{Tree-MNIST} and \textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing task-specific evaluation data while achieving 85\% automatic categorization accuracy and 80\% time savings compared to manual approaches. 

**Abstract (ZH)**: MNIST-Gen：一种基于层次语义分类的自动、模块化和适应性强的MNIST样式图像数据集生成框架 

---
# CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for Learning with Noisy Labels 

**Title (ZH)**: CLID-MU：基于跨层信息分歧的元更新策略学习 noisy 标签数据 

**Authors**: Ruofan Hu, Dongyu Zhang, Huayi Zhang, Elke Rundensteiner  

**Link**: [PDF](https://arxiv.org/pdf/2507.11807)  

**Abstract**: Learning with noisy labels (LNL) is essential for training deep neural networks with imperfect data. Meta-learning approaches have achieved success by using a clean unbiased labeled set to train a robust model. However, this approach heavily depends on the availability of a clean labeled meta-dataset, which is difficult to obtain in practice. In this work, we thus tackle the challenge of meta-learning for noisy label scenarios without relying on a clean labeled dataset. Our approach leverages the data itself while bypassing the need for labels. Building on the insight that clean samples effectively preserve the consistency of related data structures across the last hidden and the final layer, whereas noisy samples disrupt this consistency, we design the Cross-layer Information Divergence-based Meta Update Strategy (CLID-MU). CLID-MU leverages the alignment of data structures across these diverse feature spaces to evaluate model performance and use this alignment to guide training. Experiments on benchmark datasets with varying amounts of labels under both synthetic and real-world noise demonstrate that CLID-MU outperforms state-of-the-art methods. The code is released at this https URL. 

**Abstract (ZH)**: 基于嘈杂标签的元学习：不依赖干净标签数据的交叉层信息分歧元更新策略 

---
# Fragment size density estimator for shrinkage-induced fracture based on a physics-informed neural network 

**Title (ZH)**: 基于物理支配神经网络的收缩诱导破裂尺寸密度估计 

**Authors**: Shin-ichi Ito  

**Link**: [PDF](https://arxiv.org/pdf/2507.11799)  

**Abstract**: This paper presents a neural network (NN)-based solver for an integro-differential equation that models shrinkage-induced fragmentation. The proposed method directly maps input parameters to the corresponding probability density function without numerically solving the governing equation, thereby significantly reducing computational costs. Specifically, it enables efficient evaluation of the density function in Monte Carlo simulations while maintaining accuracy comparable to or even exceeding that of conventional finite difference schemes. Validatation on synthetic data demonstrates both the method's computational efficiency and predictive reliability. This study establishes a foundation for the data-driven inverse analysis of fragmentation and suggests the potential for extending the framework beyond pre-specified model structures. 

**Abstract (ZH)**: 基于神经网络的积分微分方程碎裂建模求解器：无数值求解 governing 方程直接映射输入参数至对应的概率密度函数，显著降低计算成本 

---
# Foundation Models for Brain Signals: A Critical Review of Current Progress and Future Directions 

**Title (ZH)**: 脑信号的基石模型：现有进展与未来方向的批判性review 

**Authors**: Gayal Kuruppu, Neeraj Wagh, Yogatheesan Varatharajah  

**Link**: [PDF](https://arxiv.org/pdf/2507.11783)  

**Abstract**: Patterns of electrical brain activity recorded via electroencephalography (EEG) offer immense value for scientific and clinical investigations. The inability of supervised EEG encoders to learn robust EEG patterns and their over-reliance on expensive signal annotations have sparked a transition towards general-purpose self-supervised EEG encoders, i.e., EEG foundation models (EEG-FMs), for robust and scalable EEG feature extraction. However, the real-world readiness of early EEG-FMs and the rubric for long-term research progress remain unclear. A systematic and comprehensive review of first-generation EEG-FMs is therefore necessary to understand the current state-of-the-art and identify key directions for future EEG-FMs. To that end, this study reviews 10 early EEG-FMs and presents a critical synthesis of their methodology, empirical findings, and outstanding research gaps. We find that most EEG-FMs adopt a sequence-based modeling scheme that relies on transformer-based backbones and the reconstruction of masked sequences for self-supervision. However, model evaluations remain heterogeneous and largely limited, making it challenging to assess their practical off-the-shelf utility. In addition to adopting standardized and realistic evaluations, future work should demonstrate more substantial scaling effects and make principled and trustworthy choices throughout the EEG representation learning pipeline. We believe that developing benchmarks, software tools, technical methodologies, and applications in collaboration with domain experts may further advance the translational utility and real-world adoption of EEG-FMs. 

**Abstract (ZH)**: 基于电生理活动的早期脑机接口基础模型：现状与未来方向 

---
# Predicting Delayed Trajectories Using Network Features: A Study on the Dutch Railway Network 

**Title (ZH)**: 使用网络特征预测延迟轨迹：以荷兰铁路网络为例的研究 

**Authors**: Merel Kampere, Ali Mohammed Mansoor Alsahag  

**Link**: [PDF](https://arxiv.org/pdf/2507.11776)  

**Abstract**: The Dutch railway network is one of the busiest in the world, with delays being a prominent concern for the principal passenger railway operator NS. This research addresses a gap in delay prediction studies within the Dutch railway network by employing an XGBoost Classifier with a focus on topological features. Current research predominantly emphasizes short-term predictions and neglects the broader network-wide patterns essential for mitigating ripple effects. This research implements and improves an existing methodology, originally designed to forecast the evolution of the fast-changing US air network, to predict delays in the Dutch Railways. By integrating Node Centrality Measures and comparing multiple classifiers like RandomForest, DecisionTree, GradientBoosting, AdaBoost, and LogisticRegression, the goal is to predict delayed trajectories. However, the results reveal limited performance, especially in non-simultaneous testing scenarios, suggesting the necessity for more context-specific adaptations. Regardless, this research contributes to the understanding of transportation network evaluation and proposes future directions for developing more robust predictive models for delays. 

**Abstract (ZH)**: 荷兰铁路网络是世界上 busiest 的之一，对于主要的旅客铁路运营商 NS 来说，延误是一个重要的关注点。本研究通过采用基于拓扑特征的 XGBoost 分类器填补了荷兰铁路网络延误预测研究中的空白。当前的研究主要侧重于短期预测，并忽视了对缓解连锁反应至关重要的广泛网络模式。本研究将一种先前设计用于预测快速变化的美国航空网络演变的方法实施并改进，以预测荷兰铁路的延误。通过整合节点中心性度量并比较多种分类器（如随机森林、决策树、梯度提升、自适应提升和支持向量机），本研究旨在预测延误轨迹。然而，结果表明，在非同时测试场景中性能有限，这表明需要更具体的上下文适应性调整。尽管如此，本研究为交通网络评估的理解做出了贡献，并提出了开发更稳健的延误预测模型的未来方向。 

---
# Challenges in GenAI and Authentication: a scoping review 

**Title (ZH)**: GenAI和身份认证领域的挑战：一项范围审查 

**Authors**: Wesley dos Reis Bezerra, Lais Machado Bezerra, Carlos Becker Westphall  

**Link**: [PDF](https://arxiv.org/pdf/2507.11775)  

**Abstract**: Authentication and authenticity have been a security challenge since the beginning of information sharing, especially in the context of digital information. With the advancement of generative artificial intelligence, these challenges have evolved, demanding a more up-to-date analysis of their impacts on society and system security. This work presents a scoping review that analyzed 88 documents from the IEEExplorer, Scopus, and ACM databases, promoting an analysis of the resulting portfolio through six guiding questions focusing on the most relevant work, challenges, attack surfaces, threats, proposed solutions, and gaps. Finally, the portfolio articles are analyzed through this guiding research lens and also receive individualized analysis. The results consistently outline the challenges, gaps, and threats related to images, text, audio, and video, thereby supporting new research in the areas of authentication and generative artificial intelligence. 

**Abstract (ZH)**: 自信息共享之初，认证与真实性就是安全挑战，尤其是在数字信息的背景下。随着生成式人工智能的发展，这些挑战已演变，迫切需要对它们对社会和系统安全的影响进行更最新的分析。本文通过分析IEEEExplorer、Scopus和ACM数据库中的88份文档，提出了一项范围审查，通过六项引导问题聚焦于最具相关性的工作、挑战、攻击面、威胁、提出的解决方案以及空白，最终通过这个引导研究视角来分析这些文献，并进行个别分析。结果一致地概述了与图像、文本、音频和视频相关的挑战、空白和威胁，从而支持在认证和生成式人工智能领域的新的研究。 

---
# Small Data Explainer -- The impact of small data methods in everyday life 

**Title (ZH)**: 小数据解释者——小数据方法对日常生活的影响 

**Authors**: Maren Hackenberg, Sophia G. Connor, Fabian Kabus, June Brawner, Ella Markham, Mahi Hardalupas, Areeq Chowdhury, Rolf Backofen, Anna Köttgen, Angelika Rohde, Nadine Binder, Harald Binder, Collaborative Research Center 1597 Small Data  

**Link**: [PDF](https://arxiv.org/pdf/2507.11773)  

**Abstract**: The emergence of breakthrough artificial intelligence (AI) techniques has led to a renewed focus on how small data settings, i.e., settings with limited information, can benefit from such developments. This includes societal issues such as how best to include under-represented groups in data-driven policy and decision making, or the health benefits of assistive technologies such as wearables. We provide a conceptual overview, in particular contrasting small data with big data, and identify common themes from exemplary case studies and application areas. Potential solutions are described in a more detailed technical overview of current data analysis and modelling techniques, highlighting contributions from different disciplines, such as knowledge-driven modelling from statistics and data-driven modelling from computer science. By linking application settings, conceptual contributions and specific techniques, we highlight what is already feasible and suggest what an agenda for fully leveraging small data might look like. 

**Abstract (ZH)**: 突破性人工智能技术的出现促使人们重新关注小数据环境下的潜在益处，特别是这些技术如何在信息有限的情况下发挥作用。这包括社会问题，如如何更好地将代表性不足的群体纳入数据驱动的政策和决策，或可穿戴设备等辅助技术在健康方面的益处。本文提供了一个概念性概述，尤其是小数据与大数据的对比，并从典型案例和应用领域中识别出共同主题。在更详细的技术概述中描述了潜在解决方案，强调来自不同学科的贡献，如统计学的知识驱动建模和计算机科学的数据驱动建模。通过将应用情境、概念性贡献和技术具体方法联系起来，本文突显了当前已实现的能力，并提出了充分利用小数据的议程可能是什么样的。 

---
# Survey of Genetic and Differential Evolutionary Algorithm Approaches to Search Documents Based On Semantic Similarity 

**Title (ZH)**: 基于语义相似性的文档搜索中遗传和差分进化算法综述 

**Authors**: Chandrashekar Muniyappa, Eunjin Kim  

**Link**: [PDF](https://arxiv.org/pdf/2507.11751)  

**Abstract**: Identifying similar documents within extensive volumes of data poses a significant challenge. To tackle this issue, researchers have developed a variety of effective distributed computing techniques. With the advancement of computing power and the rise of big data, deep neural networks and evolutionary computing algorithms such as genetic algorithms and differential evolution algorithms have achieved greater success. This survey will explore the most recent advancements in the search for documents based on their semantic text similarity, focusing on genetic and differential evolutionary computing algorithms. 

**Abstract (ZH)**: 在大量数据中识别相似文档面临显著挑战。为应对这一问题，研究人员开发了多种有效的分布式计算技术。随着计算能力的提升和大数据的兴起，基于语义文本相似性的文档搜索取得了更大进展，尤其得益于遗传算法和差分进化算法等进化计算算法的应用。本综述将探讨这些算法在基于语义文本相似性的文档搜索领域的最新进展。 

---
# Globalization for Scalable Short-term Load Forecasting 

**Title (ZH)**: 全球化短时负荷预测-scalability方法 

**Authors**: Amirhossein Ahmadi, Hamidreza Zareipour, Henry Leung  

**Link**: [PDF](https://arxiv.org/pdf/2507.11729)  

**Abstract**: Forecasting load in power transmission networks is essential across various hierarchical levels, from the system level down to individual points of delivery (PoD). While intuitive and locally accurate, traditional local forecasting models (LFMs) face significant limitations, particularly in handling generalizability, overfitting, data drift, and the cold start problem. These methods also struggle with scalability, becoming computationally expensive and less efficient as the network's size and data volume grow. In contrast, global forecasting models (GFMs) offer a new approach to enhance prediction generalizability, scalability, accuracy, and robustness through globalization and cross-learning. This paper investigates global load forecasting in the presence of data drifts, highlighting the impact of different modeling techniques and data heterogeneity. We explore feature-transforming and target-transforming models, demonstrating how globalization, data heterogeneity, and data drift affect each differently. In addition, we examine the role of globalization in peak load forecasting and its potential for hierarchical forecasting. To address data heterogeneity and the balance between globality and locality, we propose separate time series clustering (TSC) methods, introducing model-based TSC for feature-transforming models and new weighted instance-based TSC for target-transforming models. Through extensive experiments on a real-world dataset of Alberta's electricity load, we demonstrate that global target-transforming models consistently outperform their local counterparts, especially when enriched with global features and clustering techniques. In contrast, global feature-transforming models face challenges in balancing local and global dynamics, often requiring TSC to manage data heterogeneity effectively. 

**Abstract (ZH)**: 在数据漂移情况下进行电力传输网络负荷的全球预报：不同建模技术与数据异质性的影响 

---
# Subgraph Generation for Generalizing on Out-of-Distribution Links 

**Title (ZH)**: 生成子图以泛化处理分布外链接 

**Authors**: Jay Revolinsky, Harry Shomer, Jiliang Tang  

**Link**: [PDF](https://arxiv.org/pdf/2507.11710)  

**Abstract**: Graphs Neural Networks (GNNs) demonstrate high-performance on the link prediction (LP) task. However, these models often rely on all dataset samples being drawn from the same distribution. In addition, graph generative models (GGMs) show a pronounced ability to generate novel output graphs. Despite this, GGM applications remain largely limited to domain-specific tasks. To bridge this gap, we propose FLEX as a GGM framework which leverages two mechanism: (1) structurally-conditioned graph generation, and (2) adversarial co-training between an auto-encoder and GNN. As such, FLEX ensures structural-alignment between sample distributions to enhance link-prediction performance in out-of-distribution (OOD) scenarios. Notably, FLEX does not require expert knowledge to function in different OOD scenarios. Numerous experiments are conducted in synthetic and real-world OOD settings to demonstrate FLEX's performance-enhancing ability, with further analysis for understanding the effects of graph data augmentation on link structures. The source code is available here: this https URL. 

**Abstract (ZH)**: 基于图神经网络的结构条件化图生成框架FLEX在跨分布场景下的链接预测性能提升 

---
# Time series classification of satellite data using LSTM networks: an approach for predicting leaf-fall to minimize railroad traffic disruption 

**Title (ZH)**: 使用LSTM网络的卫星数据时间序列分类：一种预测落叶以最小化铁路交通中断的方法 

**Authors**: Hein de Wilde, Ali Mohammed Mansoor Alsahag, Pierre Blanchet  

**Link**: [PDF](https://arxiv.org/pdf/2507.11702)  

**Abstract**: Railroad traffic disruption as a result of leaf-fall cost the UK rail industry over 300 million per year and measures to mitigate such disruptions are employed on a large scale, with 1.67 million kilometers of track being treated in the UK in 2021 alone. Therefore, the ability to anticipate the timing of leaf-fall would offer substantial benefits for rail network operators, enabling the efficient scheduling of such mitigation measures. However, current methodologies for predicting leaf-fall exhibit considerable limitations in terms of scalability and reliability. This study endeavors to devise a prediction system that leverages specialized prediction methods and the latest satellite data sources to generate both scalable and reliable insights into leaf-fall timings. An LSTM network trained on ground-truth leaf-falling data combined with multispectral and meteorological satellite data demonstrated a root-mean-square error of 6.32 days for predicting the start of leaf-fall and 9.31 days for predicting the end of leaf-fall. The model, which improves upon previous work on the topic, offers promising opportunities for the optimization of leaf mitigation measures in the railway industry and the improvement of our understanding of complex ecological systems. 

**Abstract (ZH)**: 铁路因落叶导致的交通中断每年给英国铁路行业造成超过3亿英镑的损失，为减轻此类中断，英国2021年 alone 处理了167万 kilometers 的轨道。因此，能够预测落叶时机的能力将为铁路网络运营者带来显著益处，有助于高效安排此类缓解措施。然而，当前的落叶预测方法在可扩展性和可靠性方面存在显著局限。本研究旨在利用专门的预测方法和最新的卫星数据源，开发一种既能提供可扩展性和可靠性又能预测落叶时机的预测系统。使用地面真实落叶数据训练的LSTM网络结合多光谱和气象卫星数据，预测落叶开始和结束的时间分别显示了6.32天和9.31天的均方根误差。该模型在该领域的工作基础上有所改进，为铁路行业的落叶缓解措施优化和对复杂生态系统理解的提高提供了前景。 

---
# PGT-I: Scaling Spatiotemporal GNNs with Memory-Efficient Distributed Training 

**Title (ZH)**: PGT-I：使用内存高效分布式训练扩展时空GNNs 

**Authors**: Seth Ockerman, Amal Gueroudji, Tanwi Mallick, Yixuan He, Line Pouchard, Robert Ross, Shivaram Venkataraman  

**Link**: [PDF](https://arxiv.org/pdf/2507.11683)  

**Abstract**: Spatiotemporal graph neural networks (ST-GNNs) are powerful tools for modeling spatial and temporal data dependencies. However, their applications have been limited primarily to small-scale datasets because of memory constraints. While distributed training offers a solution, current frameworks lack support for spatiotemporal models and overlook the properties of spatiotemporal data. Informed by a scaling study on a large-scale workload, we present PyTorch Geometric Temporal Index (PGT-I), an extension to PyTorch Geometric Temporal that integrates distributed data parallel training and two novel strategies: index-batching and distributed-index-batching. Our index techniques exploit spatiotemporal structure to construct snapshots dynamically at runtime, significantly reducing memory overhead, while distributed-index-batching extends this approach by enabling scalable processing across multiple GPUs. Our techniques enable the first-ever training of an ST-GNN on the entire PeMS dataset without graph partitioning, reducing peak memory usage by up to 89\% and achieving up to a 13.1x speedup over standard DDP with 128 GPUs. 

**Abstract (ZH)**: 基于图的时空神经网络（ST-GNNs）是建模时空数据依赖性的强大工具。然而，由于内存限制，它们的应用主要局限于小规模数据集。虽然分布式训练提供了一种解决方案，但现有的框架缺乏对时空模型的支持，忽略了时空数据的特性。基于大规模工作负载的扩展研究，我们提出了PyTorch Geometric Temporal Index（PGT-I），这是对PyTorch Geometric Temporal的扩展，集成了分布式数据并行训练以及两种新型策略：索引批量处理和分布式索引批量处理。我们的索引技术利用时空结构在运行时动态构建快照，显著减少了内存开销，而分布式索引批量处理进一步扩展了这一方法，实现了跨多个GPU的可扩展处理。我们的技术使得首次在无需图分区的情况下对整个PeMS数据集训练ST-GNN，峰值内存使用量最多减少89%，并比128块GPU上的标准DDP快13.1倍。 

---
# Counting Answer Sets of Disjunctive Answer Set Programs 

**Title (ZH)**: 计算析取回答集程序的回答集个数 

**Authors**: Mohimenul Kabir, Supratik Chakraborty, Kuldeep S Meel  

**Link**: [PDF](https://arxiv.org/pdf/2507.11655)  

**Abstract**: Answer Set Programming (ASP) provides a powerful declarative paradigm for knowledge representation and reasoning. Recently, counting answer sets has emerged as an important computational problem with applications in probabilistic reasoning, network reliability analysis, and other domains. This has motivated significant research into designing efficient ASP counters. While substantial progress has been made for normal logic programs, the development of practical counters for disjunctive logic programs remains challenging.
We present SharpASP-SR, a novel framework for counting answer sets of disjunctive logic programs based on subtractive reduction to projected propositional model counting. Our approach introduces an alternative characterization of answer sets that enables efficient reduction while ensuring that intermediate representations remain of polynomial size. This allows SharpASP-SR to leverage recent advances in projected model counting technology. Through extensive experimental evaluation on diverse benchmarks, we demonstrate that SharpASP-SR significantly outperforms existing counters on instances with large answer set counts. Building on these results, we develop a hybrid counting approach that combines enumeration techniques with SharpASP-SR to achieve state-of-the-art performance across the full spectrum of disjunctive programs. 

**Abstract (ZH)**: 基于减法约简到投影命题模型计数的SharpASP-SR：可处理析取逻辑程序的答案集计数新框架 

---
# Tracing the Path to Grokking: Embeddings, Dropout, and Network Activation 

**Title (ZH)**: 追踪grokking之路：嵌入式表示、 Dropout与网络激活 

**Authors**: Ahmed Salah, David Yevick  

**Link**: [PDF](https://arxiv.org/pdf/2507.11645)  

**Abstract**: Grokking refers to delayed generalization in which the increase in test accuracy of a neural network occurs appreciably after the improvement in training accuracy This paper introduces several practical metrics including variance under dropout, robustness, embedding similarity, and sparsity measures, that can forecast grokking behavior. Specifically, the resilience of neural networks to noise during inference is estimated from a Dropout Robustness Curve (DRC) obtained from the variation of the accuracy with the dropout rate as the model transitions from memorization to generalization. The variance of the test accuracy under stochastic dropout across training checkpoints further exhibits a local maximum during the grokking. Additionally, the percentage of inactive neurons decreases during generalization, while the embeddings tend to a bimodal distribution independent of initialization that correlates with the observed cosine similarity patterns and dataset symmetries. These metrics additionally provide valuable insight into the origin and behaviour of grokking. 

**Abstract (ZH)**: Grokking延迟泛化现象中神经网络测试准确度在训练准确度提升之后显著提高的行为。本论文引入了若干实用的评价指标，包括丢弃dropout下的方差、鲁棒性、嵌入相似性和稀疏性度量，以预测grokking行为。具体而言，通过从模型从记忆过渡到泛化过程中准确度随dropout率变化得到的丢弃鲁棒性曲线（DRC）估计神经网络在推断过程中的抗噪性。训练检查点下测试准确度在dropout下的方差进一步在grokking期间显示出局部最大值。此外，在泛化过程中，不活跃神经元的比例下降，而嵌入趋向于与初始化无关、独立于数据集对称性的双模分布，这种分布与观察到的余弦相似性模式和数据集对称性相关。这些指标还提供了关于grokking的起源和行为的宝贵见解。 

---
# Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders 

**Title (ZH)**: 使用变分自编码器的可解释性直肠癌MRI淋巴结转移预测 

**Authors**: Benjamin Keel, Aaron Quyn, David Jayne, Maryam Mohsin, Samuel D. Relton  

**Link**: [PDF](https://arxiv.org/pdf/2507.11638)  

**Abstract**: Effective treatment for rectal cancer relies on accurate lymph node metastasis (LNM) staging. However, radiological criteria based on lymph node (LN) size, shape and texture morphology have limited diagnostic accuracy. In this work, we investigate applying a Variational Autoencoder (VAE) as a feature encoder model to replace the large pre-trained Convolutional Neural Network (CNN) used in existing approaches. The motivation for using a VAE is that the generative model aims to reconstruct the images, so it directly encodes visual features and meaningful patterns across the data. This leads to a disentangled and structured latent space which can be more interpretable than a CNN. Models are deployed on an in-house MRI dataset with 168 patients who did not undergo neo-adjuvant treatment. The post-operative pathological N stage was used as the ground truth to evaluate model predictions. Our proposed model 'VAE-MLP' achieved state-of-the-art performance on the MRI dataset, with cross-validated metrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85 +/- 0.05. Code is available at: this https URL. 

**Abstract (ZH)**: 有效的直肠癌治疗依赖于准确的淋巴结转移（LNM）分期。然而，基于淋巴结（LN）大小、形状和纹理形态的放射学标准在诊断准确性方面有限。本研究中，我们探索使用变分自编码器（VAE）作为特征编码模型，以替代现有方法中使用的大型预训练卷积神经网络（CNN）。使用VAE的原因在于生成模型旨在重构图像，因此可以直接编码视觉特征和有意义的模式，从而产生更易于解释的分离和结构化的潜在空间。该模型在未接受新辅助治疗的168名患者的院内MRI数据集上部署，并使用术后病理N分期作为ground truth评估模型预测。我们提出的模型'VAE-MLP'在MRI数据集上实现了最先进的性能，交叉验证指标包括AUC 0.86 ± 0.05、敏感性0.79 ± 0.06和特异性0.85 ± 0.05。代码可在以下链接获取：this https URL。 

---
# JSQA: Speech Quality Assessment with Perceptually-Inspired Contrastive Pretraining Based on JND Audio Pairs 

**Title (ZH)**: JSQA：基于JND音频配对的知觉启发对比预训练的语音质量评估 

**Authors**: Junyi Fan, Donald Williamson  

**Link**: [PDF](https://arxiv.org/pdf/2507.11636)  

**Abstract**: Speech quality assessment (SQA) is often used to learn a mapping from a high-dimensional input space to a scalar that represents the mean opinion score (MOS) of the perceptual speech quality. Learning such a mapping is challenging for many reasons, but largely because MOS exhibits high levels of inherent variance due to perceptual and experimental-design differences. Many solutions have been proposed, but many approaches do not properly incorporate perceptual factors into their learning algorithms (beyond the MOS label), which could lead to unsatisfactory results. To this end, we propose JSQA, a two-stage framework that pretrains an audio encoder using perceptually-guided contrastive learning on just noticeable difference (JND) pairs, followed by fine-tuning for MOS prediction. We first generate pairs of audio data within JND levels, which are then used to pretrain an encoder to leverage perceptual quality similarity information and map it into an embedding space. The JND pairs come from clean LibriSpeech utterances that are mixed with background noise from CHiME-3, at different signal-to-noise ratios (SNRs). The encoder is later fine-tuned with audio samples from the NISQA dataset for MOS prediction. Experimental results suggest that perceptually-inspired contrastive pretraining significantly improves the model performance evaluated by various metrics when compared against the same network trained from scratch without pretraining. These findings suggest that incorporating perceptual factors into pretraining greatly contributes to the improvement in performance for SQA. 

**Abstract (ZH)**: 基于感知的对比预训练的语音质量评估（JSQA） 

---
# Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation 

**Title (ZH)**: 跨语言少样本学习在增量适应下的波斯语情感分析 

**Authors**: Farideh Majidi, Ziaeddin Beheshtifard  

**Link**: [PDF](https://arxiv.org/pdf/2507.11634)  

**Abstract**: This research examines cross-lingual sentiment analysis using few-shot learning and incremental learning methods in Persian. The main objective is to develop a model capable of performing sentiment analysis in Persian using limited data, while getting prior knowledge from high-resource languages. To achieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and DistilBERT) were employed, which were fine-tuned using few-shot and incremental learning approaches on small samples of Persian data from diverse sources, including X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled the models to learn from a broad range of contexts. Experimental results show that the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96% accuracy on Persian sentiment analysis. These findings highlight the effectiveness of combining few-shot learning and incremental learning with multilingual pre-trained models. 

**Abstract (ZH)**: 本研究使用少量学习和增量学习方法对波斯语进行跨语言情感分析，旨在开发一种能够在有限数据下进行波斯语情感分析的模型，并从中获得高资源语言的知识。研究采用了三种预先训练的多语言模型（XLM-RoBERTa、mDeBERTa和DistilBERT），这些模型利用少量来自不同来源（包括X、Instagram、Digikala、Snappfood和Taaghche）的波斯语数据进行微调，采用少量学习和增量学习方法。这种多样性使模型能够从广泛的语境中学习。实验结果表明，mDeBERTa和XLM-RoBERTa取得了高性能，波斯语情感分析准确率达到96%。这些发现强调了将少量学习、增量学习与多语言预训练模型结合使用的有效性。 

---
# Learning Representations of Event Time Series with Sparse Autoencoders for Anomaly Detection, Similarity Search, and Unsupervised Classification 

**Title (ZH)**: 使用稀疏自动编码器学习事件时间序列表示以进行异常检测、相似性搜索和无监督分类 

**Authors**: Steven Dillmann, Juan Rafael Martínez-Galarza  

**Link**: [PDF](https://arxiv.org/pdf/2507.11620)  

**Abstract**: Event time series are sequences of discrete events occurring at irregular time intervals, each associated with a domain-specific observational modality. They are common in domains such as high-energy astrophysics, computational social science, cybersecurity, finance, healthcare, neuroscience, and seismology. Their unstructured and irregular structure poses significant challenges for extracting meaningful patterns and identifying salient phenomena using conventional techniques. We propose novel two- and three-dimensional tensor representations for event time series, coupled with sparse autoencoders that learn physically meaningful latent representations. These embeddings support a variety of downstream tasks, including anomaly detection, similarity-based retrieval, semantic clustering, and unsupervised classification. We demonstrate our approach on a real-world dataset from X-ray astronomy, showing that these representations successfully capture temporal and spectral signatures and isolate diverse classes of X-ray transients. Our framework offers a flexible, scalable, and generalizable solution for analyzing complex, irregular event time series across scientific and industrial domains. 

**Abstract (ZH)**: 事件时间序列是由在不规则时间间隔发生的离散事件组成的序列，每个事件都与特定领域的观测模态相关。它们在高能天体物理学、计算社会科学、网络安全、金融、医疗保健、神经科学和地震学等领域很常见。它们的无结构和不规则结构给使用传统技术提取有意义的模式和识别显著现象带来了重大挑战。我们提出了用于事件时间序列的新型二维和三维张量表示，并结合了稀疏自编码器以学习物理上有意义的潜在表示。这些嵌入支持多种下游任务，包括异常检测、基于相似性的检索、语义聚类和无监督分类。我们在X射线天文的实际数据集上展示了我们的方法，表明这些表示成功捕获了时间和频谱特征，并隔离了不同的X射线瞬变类。我们的框架为跨科学和技术领域分析复杂、不规则的事件时间序列提供了灵活、可扩展和通用的解决方案。 

---
# AI, Humans, and Data Science: Optimizing Roles Across Workflows and the Workforce 

**Title (ZH)**: AI、人类与数据科学：优化工作流与 workforce 中的角色 

**Authors**: Richard Timpone, Yongwei Yang  

**Link**: [PDF](https://arxiv.org/pdf/2507.11597)  

**Abstract**: AI is transforming research. It is being leveraged to construct surveys, synthesize data, conduct analysis, and write summaries of the results. While the promise is to create efficiencies and increase quality, the reality is not always as clear cut. Leveraging our framework of Truth, Beauty, and Justice (TBJ) which we use to evaluate AI, machine learning and computational models for effective and ethical use (Taber and Timpone 1997; Timpone and Yang 2024), we consider the potential and limitation of analytic, generative, and agentic AI to augment data scientists or take on tasks traditionally done by human analysts and researchers. While AI can be leveraged to assist analysts in their tasks, we raise some warnings about push-button automation. Just as earlier eras of survey analysis created some issues when the increased ease of using statistical software allowed researchers to conduct analyses they did not fully understand, the new AI tools may create similar but larger risks. We emphasize a human-machine collaboration perspective (Daugherty and Wilson 2018) throughout the data science workflow and particularly call out the vital role that data scientists play under VUCA decision areas. We conclude by encouraging the advance of AI tools to complement data scientists but advocate for continued training and understanding of methods to ensure the substantive value of research is fully achieved by applying, interpreting, and acting upon results most effectively and ethically. 

**Abstract (ZH)**: AI正在变革研究。它被用于构建调查、合成数据、开展分析以及撰写结果总结。尽管承诺是提高效率和提高质量，实际情况往往并非如此清晰。利用我们用于评估AI、机器学习和计算模型的有效性和伦理使用的Truth、Beauty、Justice（TBJ）框架（Taber和Timpone 1997；Timpone和Yang 2024），我们探讨了分析型、生成型和自主型AI在增强数据科学家能力或承担传统上由人类分析师和研究人员完成的任务方面的潜力和局限性。虽然AI可以用于协助分析师完成任务，但我们提出了关于一键自动化可能带来的一些警告。正如统计软件使用便捷带来的统计分析问题一样，新的AI工具可能会带来类似但更大的风险。我们全程强调人机协作的观点（Daugherty和Wilson 2018），特别是在VUCA决策领域明确数据科学家的关键作用。我们总结时鼓励AI工具的发展应补充数据科学家的功能，同时倡导继续培训和理解方法，以确保研究的实际价值通过最有效和伦理的方式应用、解释和采取行动得到充分实现。 

---
# SToFM: a Multi-scale Foundation Model for Spatial Transcriptomics 

**Title (ZH)**: SToFM：空间转录组学的多尺度基础模型 

**Authors**: Suyuan Zhao, Yizhen Luo, Ganbo Yang, Yan Zhong, Hao Zhou, Zaiqing Nie  

**Link**: [PDF](https://arxiv.org/pdf/2507.11588)  

**Abstract**: Spatial Transcriptomics (ST) technologies provide biologists with rich insights into single-cell biology by preserving spatial context of cells. Building foundational models for ST can significantly enhance the analysis of vast and complex data sources, unlocking new perspectives on the intricacies of biological tissues. However, modeling ST data is inherently challenging due to the need to extract multi-scale information from tissue slices containing vast numbers of cells. This process requires integrating macro-scale tissue morphology, micro-scale cellular microenvironment, and gene-scale gene expression profile. To address this challenge, we propose SToFM, a multi-scale Spatial Transcriptomics Foundation Model. SToFM first performs multi-scale information extraction on each ST slice, to construct a set of ST sub-slices that aggregate macro-, micro- and gene-scale information. Then an SE(2) Transformer is used to obtain high-quality cell representations from the sub-slices. Additionally, we construct \textbf{SToCorpus-88M}, the largest high-resolution spatial transcriptomics corpus for pretraining. SToFM achieves outstanding performance on a variety of downstream tasks, such as tissue region semantic segmentation and cell type annotation, demonstrating its comprehensive understanding of ST data 

**Abstract (ZH)**: 空间转录组学（ST）技术通过保留细胞的空间上下文为生物学家提供了丰富的单细胞生物学见解。构建ST的基础模型可以显著增强对海量复杂数据源的分析，揭示生物组织复杂性的新视角。然而，建模ST数据由于需要从包含大量细胞的组织切片中提取多尺度信息而具有固有挑战性。这一过程需要整合宏观尺度的组织形态、微观尺度的细胞微环境以及基因尺度的基因表达谱。为了解决这一挑战，我们提出了一种多尺度空间转录组学基础模型SToFM。SToFM首先在每个ST切片上进行多尺度信息提取，构建包含宏观、微观和基因尺度信息的ST子切片集。然后使用SE(2)变压器从子切片中获取高质量的细胞表示。此外，我们构建了SToCorpus-88M，这是用于预训练的最大高分辨率空间转录组学数据集。SToFM在组织区域语义分割和细胞类型注释等多种下游任务上取得了优异性能，显示出其全面理解ST数据的能力。 

---
# Distribution-Free Uncertainty-Aware Virtual Sensing via Conformalized Neural Operators 

**Title (ZH)**: 基于同验化神经运算器的分布无关的不确定性感知虚拟传感 

**Authors**: Kazuma Kobayashi, Shailesh Garg, Farid Ahmed, Souvik Chakraborty, Syed Bahauddin Alam  

**Link**: [PDF](https://arxiv.org/pdf/2507.11574)  

**Abstract**: Robust uncertainty quantification (UQ) remains a critical barrier to the safe deployment of deep learning in real-time virtual sensing, particularly in high-stakes domains where sparse, noisy, or non-collocated sensor data are the norm. We introduce the Conformalized Monte Carlo Operator (CMCO), a framework that transforms neural operator-based virtual sensing with calibrated, distribution-free prediction intervals. By unifying Monte Carlo dropout with split conformal prediction in a single DeepONet architecture, CMCO achieves spatially resolved uncertainty estimates without retraining, ensembling, or custom loss design. Our method addresses a longstanding challenge: how to endow operator learning with efficient and reliable UQ across heterogeneous domains. Through rigorous evaluation on three distinct applications: turbulent flow, elastoplastic deformation, and global cosmic radiation dose estimation-CMCO consistently attains near-nominal empirical coverage, even in settings with strong spatial gradients and proxy-based sensing. This breakthrough offers a general-purpose, plug-and-play UQ solution for neural operators, unlocking real-time, trustworthy inference in digital twins, sensor fusion, and safety-critical monitoring. By bridging theory and deployment with minimal computational overhead, CMCO establishes a new foundation for scalable, generalizable, and uncertainty-aware scientific machine learning. 

**Abstract (ZH)**: 稳健的不确定性量化（UQ）仍然是在实时虚拟传感中安全部署深度学习的關鍵障礙，特別是在高風險領域中，該領域以稀疏、噪聲或非共址傳感器數據為常態。我們介紹了一種稱為統成型蒙特卡洛操作符（CMCO）的框架，該框架通過校准的、非特定分布的预测区间，将基于神经操作员的虚拟传感转化为具有空间解析的不确定性估计。CMCO通过在统一的DeepONet架构中结合蒙特卡洛丢弃和分割一致预测，无需重新训练、集成或自定义损失设计即可实现不确定性估计。我们的方法解决了长期存在的挑战：如何在异构领域中赋予操作学习高效且可靠的不确定性量化。通过在湍流流动、弹塑性变形和全球宇宙辐射剂量估计等三个不同应用中进行严格的评估，CMCO在具有强烈空间梯度和代理式传感的情况下，始终能够实现接近名义的实证覆盖率。这一突破为神经操作员提供了通用、即插即用的不确定性量化解决方案，解锁了数字孪生、传感器融合和关键安全监测中的实时、可信赖的推理。通过最小的计算开销实现理论与部署的结合，CMCO奠定了可扩展、泛化且不确定性意识的科学机器学习的新基础。 

---
# SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery 

**Title (ZH)**: SurgeryLSTM：一种时间意识的神经网络模型，用于脊柱手术后的准确可解释住院日预测 

**Authors**: Ha Na Cho, Sairam Sutari, Alexander Lopez, Hansen Bow, Kai Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2507.11570)  

**Abstract**: Objective: To develop and evaluate machine learning (ML) models for predicting length of stay (LOS) in elective spine surgery, with a focus on the benefits of temporal modeling and model interpretability. Materials and Methods: We compared traditional ML models (e.g., linear regression, random forest, support vector machine (SVM), and XGBoost) with our developed model, SurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an attention, using structured perioperative electronic health records (EHR) data. Performance was evaluated using the coefficient of determination (R2), and key predictors were identified using explainable AI. Results: SurgeryLSTM achieved the highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85) and baseline models. The attention mechanism improved interpretability by dynamically identifying influential temporal segments within preoperative clinical sequences, allowing clinicians to trace which events or features most contributed to each LOS prediction. Key predictors of LOS included bone disorder, chronic kidney disease, and lumbar fusion identified as the most impactful predictors of LOS. Discussion: Temporal modeling with attention mechanisms significantly improves LOS prediction by capturing the sequential nature of patient data. Unlike static models, SurgeryLSTM provides both higher accuracy and greater interpretability, which are critical for clinical adoption. These results highlight the potential of integrating attention-based temporal models into hospital planning workflows. Conclusion: SurgeryLSTM presents an effective and interpretable AI solution for LOS prediction in elective spine surgery. Our findings support the integration of temporal, explainable ML approaches into clinical decision support systems to enhance discharge readiness and individualized patient care. 

**Abstract (ZH)**: 目标：开发和评估机器学习（ML）模型以预测择期脊柱手术的住院时间（LOS），重点关注时间模型和模型可解释性的益处。材料与方法：我们将传统的ML模型（如线性回归、随机森林、支持向量机（SVM）和XGBoost）与我们开发的SurgeryLSTM模型进行了比较，SurgeryLSTM是一种带有注意力机制的掩码双向长短期记忆（BiLSTM）模型，使用结构化的围手术期电子健康记录（EHR）数据。性能通过决定系数（R²）进行评估，并使用可解释的AI来识别关键预测因子。结果：SurgeryLSTM实现了最高的预测准确性（R²=0.86），优于XGBoost（R²=0.85）和基线模型。注意力机制通过动态识别预手术临床序列中影响显著的时间段来提高可解释性，使临床医生能够跟踪哪些事件或特征对每个LOS预测贡献最大。关键的LOS预测因子包括骨疾病、慢性肾病和腰椎融合，这些被确定为对LOS影响最大的预测因子。讨论：通过注意力机制进行的时间建模显著提高了LOS预测的准确性，捕捉了患者数据的序列性质。与静态模型不同，SurgeryLSTM提供了更高的准确性和更大的可解释性，这对于临床应用至关重要。这些结果突显了将基于注意力的时间模型整合到医院规划工作流程中的潜力。结论：SurgeryLSTM提供了一种有效且可解释的人工智能解决方案，用于择期脊柱手术的LOS预测。我们的研究结果支持将基于时间的、可解释的机器学习方法整合到临床决策支持系统中，以提高出院准备度和个性化病人护理。 

---
# A Model Aware AIGC Task Offloading Algorithm in IIoT Edge Computing 

**Title (ZH)**: 一种工业物联网边缘计算中模型意识的AIGC任务卸载算法 

**Authors**: Xin Wang, Xiao Huan Li, Xun Wang  

**Link**: [PDF](https://arxiv.org/pdf/2507.11560)  

**Abstract**: The integration of the Industrial Internet of Things (IIoT) with Artificial Intelligence-Generated Content (AIGC) offers new opportunities for smart manufacturing, but it also introduces challenges related to computation-intensive tasks and low-latency demands. Traditional generative models based on cloud computing are difficult to meet the real-time requirements of AIGC tasks in IIoT environments, and edge computing can effectively reduce latency through task offloading. However, the dynamic nature of AIGC tasks, model switching delays, and resource constraints impose higher demands on edge computing environments. To address these challenges, this paper proposes an AIGC task offloading framework tailored for IIoT edge computing environments, considering the latency and energy consumption caused by AIGC model switching for the first time. IIoT devices acted as multi-agent collaboratively offload their dynamic AIGC tasks to the most appropriate edge servers deployed with different generative models. A model aware AIGC task offloading algorithm based on Multi-Agent Deep Deterministic Policy Gradient (MADDPG-MATO) is devised to minimize the latency and energy. Experimental results show that MADDPG-MATO outperforms baseline algorithms, achieving an average reduction of 6.98% in latency, 7.12% in energy consumption, and a 3.72% increase in task completion rate across four sets of experiments with model numbers ranging from 3 to 6, it is demonstrated that the proposed algorithm is robust and efficient in dynamic, high-load IIoT environments. 

**Abstract (ZH)**: 工业互联网（IIoT）与人工智能生成内容（AIGC）的集成为智能制造提供了新机遇，但也带来了与计算密集型任务和低延迟需求相关的新挑战。基于云计算的传统生成模型难以满足IIoT环境中AIGC任务的实时需求，边缘计算可以通过任务卸载有效降低延迟。然而，AIGC任务的动态特性、模型切换延迟和资源约束对边缘计算环境提出了更高要求。为应对这些挑战，本文首次考虑了AIGC模型切换引起的延迟和能耗，提出了一种针对IIoT边缘计算环境的AIGC任务卸载框架。IIoT设备作为多代理协同将动态AIGC任务卸载到部署不同生成模型的最恰当边缘服务器上。基于多代理深度确定性策略梯度（MADDPG-MATO）的模型感知AIGC任务卸载算法被设计用于最小化延迟和能耗。实验结果表明，MADDPG-MATO优于基线算法，在四组实验中，模型数量从3到6的条件下，平均延迟降低了6.98%，能耗降低了7.12%，任务完成率提高了3.72%。研究结果证明，所提出算法在动态、高负载的IIoT环境中既稳健又高效。 

---
# Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models 

**Title (ZH)**: 倒转-DPO：扩散模型的精确高效后训练方法 

**Authors**: Zejian Li, Yize Li, Chenye Meng, Zhongni Liu, Yang Ling, Shengyuan Zhang, Guang Yang, Changyuan Yang, Zhiyuan Yang, Lingyun Sun  

**Link**: [PDF](https://arxiv.org/pdf/2507.11554)  

**Abstract**: Recent advancements in diffusion models (DMs) have been propelled by alignment methods that post-train models to better conform to human preferences. However, these approaches typically require computation-intensive training of a base model and a reward model, which not only incurs substantial computational overhead but may also compromise model accuracy and training efficiency. To address these limitations, we propose Inversion-DPO, a novel alignment framework that circumvents reward modeling by reformulating Direct Preference Optimization (DPO) with DDIM inversion for DMs. Our method conducts intractable posterior sampling in Diffusion-DPO with the deterministic inversion from winning and losing samples to noise and thus derive a new post-training paradigm. This paradigm eliminates the need for auxiliary reward models or inaccurate appromixation, significantly enhancing both precision and efficiency of training. We apply Inversion-DPO to a basic task of text-to-image generation and a challenging task of compositional image generation. Extensive experiments show substantial performance improvements achieved by Inversion-DPO compared to existing post-training methods and highlight the ability of the trained generative models to generate high-fidelity compositionally coherent images. For the post-training of compostitional image geneation, we curate a paired dataset consisting of 11,140 images with complex structural annotations and comprehensive scores, designed to enhance the compositional capabilities of generative models. Inversion-DPO explores a new avenue for efficient, high-precision alignment in diffusion models, advancing their applicability to complex realistic generation tasks. Our code is available at this https URL 

**Abstract (ZH)**: 最近在扩散模型中的进展通过齐次方法在训练后使模型更好地符合人类偏好得到了推动。然而，这些方法通常需要对基础模型和奖励模型进行计算密集型的训练，这不仅会带来显著的计算开销，还可能影响模型准确性和训练效率。为解决这些问题，我们提出了Inversion-DPO，一种新的齐次框架，通过使用DDIM逆运算重新表述直接偏好优化(DPO)来规避奖励建模。我们的方法在扩散模型中使用确定性的逆运算进行难以处理的后验采样，从而从获胜和失败样本到噪声中导出了一个新的训练后范式。该范式消除了对辅助奖励模型或不准确逼近的需求，显著提高了训练的精度和效率。我们将Inversion-DPO应用于文本到图像生成的基本任务和复杂的图像生成挑战任务。广泛的实验证明，Inversion-DPO相比现有训练后方法实现了显著的性能提升，并突显了训练生成模型生成高质量组成一致图像的能力。为了训练后处理组成图像生成，我们制作了一个包含11,140张具有复杂结构注释和全面评分的配对数据集，旨在增强生成模型的组成能力。Inversion-DPO探索了扩散模型中高效、高精度齐次的新途径，促进了其应用于复杂的现实生成任务。我们的代码可在以下链接获得。 

---
# Deformable Dynamic Convolution for Accurate yet Efficient Spatio-Temporal Traffic Prediction 

**Title (ZH)**: 可变形动态卷积实现精确且高效的时空交通预测 

**Authors**: Hyeonseok Jin, Geonmin Kim, Kyungbaek Kim  

**Link**: [PDF](https://arxiv.org/pdf/2507.11550)  

**Abstract**: Spatio-temporal traffic prediction plays a key role in intelligent transportation systems by enabling accurate prediction in complex urban areas. Although not only accuracy but also efficiency for scalability is important, some previous methods struggle to capture heterogeneity such as varying traffic patterns across regions and time periods. Moreover, Graph Neural Networks (GNNs), which are the mainstream of traffic prediction, not only require predefined adjacency matrix, but also limit scalability to large-scale data containing many nodes due to their inherent complexity. To overcome these limitations, we propose Deformable Dynamic Convolution Network (DDCN) for accurate yet efficient traffic prediction. Traditional Convolutional Neural Networks (CNNs) are limited in modeling non-Euclidean spatial structures and spatio-temporal heterogeneity, DDCN overcomes these challenges by dynamically applying deformable filters based on offset. Specifically, DDCN decomposes transformer-style CNN to encoder-decoder structure, and applies proposed approaches to the spatial and spatio-temporal attention blocks of the encoder to emphasize important features. The decoder, composed of feed-forward module, complements the output of the encoder. This novel structure make DDCN can perform accurate yet efficient traffic prediction. In comprehensive experiments on four real-world datasets, DDCN achieves competitive performance, emphasizing the potential and effectiveness of CNN-based approaches for spatio-temporal traffic prediction. 

**Abstract (ZH)**: 空间时间交通预测在智能交通系统中扮演着关键角色，通过在复杂城市区域实现准确预测。虽然准确性和可扩展性的效率同样重要，但一些先前的方法难以捕捉不同区域和时间周期下的异质性。此外，主流行驶中的图神经网络（GNNs）不仅需要预先定义的邻接矩阵，而且由于其固有的复杂性，难以处理包含大量节点的大型数据集。为克服这些限制，我们提出了可变形动态卷积网络（DDCN），以实现准确且高效的交通预测。传统的卷积神经网络（CNNs）在建模非欧几里得空间结构和空间时间异质性方面受到限制，DDCN通过基于偏移动态应用可变形滤波器来克服这些挑战。具体而言，DDCN将transformer风格的CNN分解为编码器-解码器结构，并将所提出的方法应用于编码器的空间和空间时间注意力模块，以强调重要特征。解码器由前馈模块组成，补充编码器的输出。这种新颖结构使DDCN能够实现准确且高效的交通预测。在四个真实世界数据集的全面实验中，DDCN取得了竞争力的表现，强调了基于CNN的方法在空间时间交通预测中的潜力和有效性。 

---
# Fairness Is Not Enough: Auditing Competence and Intersectional Bias in AI-powered Resume Screening 

**Title (ZH)**: 公平性不够：AI驱动的简历筛选中的能力审计与交叉偏见审查 

**Authors**: Kevin T Webster  

**Link**: [PDF](https://arxiv.org/pdf/2507.11548)  

**Abstract**: The increasing use of generative AI for resume screening is predicated on the assumption that it offers an unbiased alternative to biased human decision-making. However, this belief fails to address a critical question: are these AI systems fundamentally competent at the evaluative tasks they are meant to perform? This study investigates the question of competence through a two-part audit of eight major AI platforms. Experiment 1 confirmed complex, contextual racial and gender biases, with some models penalizing candidates merely for the presence of demographic signals. Experiment 2, which evaluated core competence, provided a critical insight: some models that appeared unbiased were, in fact, incapable of performing a substantive evaluation, relying instead on superficial keyword matching. This paper introduces the "Illusion of Neutrality" to describe this phenomenon, where an apparent lack of bias is merely a symptom of a model's inability to make meaningful judgments. This study recommends that organizations and regulators adopt a dual-validation framework, auditing AI hiring tools for both demographic bias and demonstrable competence to ensure they are both equitable and effective. 

**Abstract (ZH)**: 生成式AI在简历筛选中的广泛应用假定其提供了无偏见的人类决策的替代方案。然而，这种信念未能回答一个关键问题：这些AI系统本质上是否具备完成所期望评估任务的能力？本研究通过审计八大主要AI平台的两个部分，调查了这一能力问题。实验1确认了复杂的、情境化的种族和性别偏见，一些模型仅因候选人的某些 demographics 信号而对其扣分。实验2评估核心能力，揭示了一个关键洞察：一些看似无偏见的模型实际上无法进行实质性的评估，而是依赖表面的关键词匹配。本文将这种现象称为“中立幻象”，即表面无偏见仅仅是模型无法做出有意义判断的症状。本研究建议组织和监管机构采用双重验证框架，审计AI招聘工具的种族偏见和可验证的能力，以确保它们既公平又有效。 

---
# A Review of Generative AI in Computer Science Education: Challenges and Opportunities in Accuracy, Authenticity, and Assessment 

**Title (ZH)**: 计算机科学教育中生成式AI的综述：准确性、真实性与评估面临的挑战与机遇 

**Authors**: Iman Reihanian, Yunfei Hou, Yu Chen, Yifei Zheng  

**Link**: [PDF](https://arxiv.org/pdf/2507.11543)  

**Abstract**: This paper surveys the use of Generative AI tools, such as ChatGPT and Claude, in computer science education, focusing on key aspects of accuracy, authenticity, and assessment. Through a literature review, we highlight both the challenges and opportunities these AI tools present. While Generative AI improves efficiency and supports creative student work, it raises concerns such as AI hallucinations, error propagation, bias, and blurred lines between AI-assisted and student-authored content. Human oversight is crucial for addressing these concerns. Existing literature recommends adopting hybrid assessment models that combine AI with human evaluation, developing bias detection frameworks, and promoting AI literacy for both students and educators. Our findings suggest that the successful integration of AI requires a balanced approach, considering ethical, pedagogical, and technical factors. Future research may explore enhancing AI accuracy, preserving academic integrity, and developing adaptive models that balance creativity with precision. 

**Abstract (ZH)**: 本研究调查了ChatGPT和Claude等生成式AI工具在计算机科学教育中的应用，重点关注准确度、真实性和评估的关键方面。通过文献综述，我们指出这些AI工具带来的挑战和机遇。尽管生成式AI提高了效率并支持学生的创造性工作，但也引发了AI幻觉、错误传播、偏见以及AI辅助内容与学生原创内容界限模糊等担忧。人类监督对于应对这些担忧至关重要。现有文献建议采用将AI与人工评估相结合的混合评估模型、开发偏见检测框架，并促进师生的AI素养。我们的研究结果表明，AI的成功集成需要综合考虑伦理、教学法和技术因素。未来的研究可以探索提高AI准确度、维护学术诚信以及开发既能平衡创造力又能提高精确度的适应性模型。 

---
