# Kaiwu: A Multimodal Manipulation Dataset and Framework for Robot Learning and Human-Robot Interaction 

**Title (ZH)**: .Kaiwu：一种用于机器人学习和人机交互的多模态操作数据集及框架 

**Authors**: Shuo Jiang, Haonan Li, Ruochen Ren, Yanmin Zhou, Zhipeng Wang, Bin He  

**Link**: [PDF](https://arxiv.org/pdf/2503.05231)  

**Abstract**: Cutting-edge robot learning techniques including foundation models and imitation learning from humans all pose huge demands on large-scale and high-quality datasets which constitute one of the bottleneck in the general intelligent robot fields. This paper presents the Kaiwu multimodal dataset to address the missing real-world synchronized multimodal data problems in the sophisticated assembling scenario,especially with dynamics information and its fine-grained labelling. The dataset first provides an integration of human,environment and robot data collection framework with 20 subjects and 30 interaction objects resulting in totally 11,664 instances of integrated actions. For each of the demonstration,hand motions,operation pressures,sounds of the assembling process,multi-view videos, high-precision motion capture information,eye gaze with first-person videos,electromyography signals are all recorded. Fine-grained multi-level annotation based on absolute timestamp,and semantic segmentation labelling are performed. Kaiwu dataset aims to facilitate robot learning,dexterous manipulation,human intention investigation and human-robot collaboration research. 

**Abstract (ZH)**: 基于多模态数据的精密装配场景机器人学习数据集Kaiwu 

---
# VLMs Play StarCraft II: A Benchmark and Multimodal Decision Method 

**Title (ZH)**: VLMs在StarCraft II中的应用：一个基准和多模态决策方法 

**Authors**: Weiyu Ma, Yuqian Fu, Zecheng Zhang, Guohao Li  

**Link**: [PDF](https://arxiv.org/pdf/2503.05383)  

**Abstract**: We introduce VLM-Attention, a multimodal StarCraft II environment that aligns artificial agent perception with the human gameplay experience. Traditional frameworks such as SMAC rely on abstract state representations that diverge significantly from human perception, limiting the ecological validity of agent behavior. Our environment addresses this limitation by incorporating RGB visual inputs and natural language observations that more closely simulate human cognitive processes during gameplay. The VLM-Attention framework consists of three integrated components: (1) a vision-language model enhanced with specialized self-attention mechanisms for strategic unit targeting and battlefield assessment, (2) a retrieval-augmented generation system that leverages domain-specific StarCraft II knowledge to inform tactical decisions, and (3) a dynamic role-based task distribution system that enables coordinated multi-agent behavior. Our experimental evaluation across 21 custom scenarios demonstrates that VLM-based agents powered by foundation models (specifically Qwen-VL and GPT-4o) can execute complex tactical maneuvers without explicit training, achieving comparable performance to traditional MARL methods that require substantial training iterations. This work establishes a foundation for developing human-aligned StarCraft II agents and advances the broader research agenda of multimodal game AI. Our implementation is available at this https URL. 

**Abstract (ZH)**: 我们引入了VLM-Attention，这是一种 multimodal StarCraft II 环境，将人工代理的感知与人类游戏体验对齐。传统的框架如SMAC依赖于与人类感知差异较大的抽象状态表示，限制了代理行为的生态有效性。我们的环境通过引入RGB视觉输入和自然语言观察来解决这一局限性，这些观察更贴近于游戏过程中人类的认知过程。VLM-Attention框架包括三个集成组件：（1）配备专门自注意力机制的视觉-语言模型，用于战略单位目标定位和战场评估；（2）利用特定领域StarCraft II知识的检索增强生成系统，以指导战术决策；（3）一种动态角色任务分配系统，以实现协调的多代理行为。我们在21个定制场景的实验评估表明，基于基础模型（具体为Qwen-VL和GPT-4o）的VLM代理可以执行复杂的战术操作，无需显式训练，达到与需要大量训练迭代的传统MARL方法相当的性能。这项工作为开发与人类对齐的StarCraft II代理奠定了基础，并推动了多模态游戏AI的更广泛研究议程。我们的实现可在此处访问：this https URL。 

---
# FMT:A Multimodal Pneumonia Detection Model Based on Stacking MOE Framework 

**Title (ZH)**: FMT：基于MOE框架的多模态肺炎检测模型 

**Authors**: Jingyu Xu, Yang Wang  

**Link**: [PDF](https://arxiv.org/pdf/2503.05626)  

**Abstract**: Artificial intelligence has shown the potential to improve diagnostic accuracy through medical image analysis for pneumonia diagnosis. However, traditional multimodal approaches often fail to address real-world challenges such as incomplete data and modality loss. In this study, a Flexible Multimodal Transformer (FMT) was proposed, which uses ResNet-50 and BERT for joint representation learning, followed by a dynamic masked attention strategy that simulates clinical modality loss to improve robustness; finally, a sequential mixture of experts (MOE) architecture was used to achieve multi-level decision refinement. After evaluation on a small multimodal pneumonia dataset, FMT achieved state-of-the-art performance with 94% accuracy, 95% recall, and 93% F1 score, outperforming single-modal baselines (ResNet: 89%; BERT: 79%) and the medical benchmark CheXMed (90%), providing a scalable solution for multimodal diagnosis of pneumonia in resource-constrained medical settings. 

**Abstract (ZH)**: 人工智能在通过肺炎诊断的医学图像分析中展示了提升诊断准确性潜力，然而传统多模态方法往往无法解决实际挑战如数据不完整和模态丢失。在此研究中，提出了一种灵活的多模态变换器（FMT），使用ResNet-50和BERT进行联合表示学习，随后采用动态遮蔽注意机制以模拟临床模态丢失从而提高鲁棒性；最后，采用层次专家混合架构实现多重决策精细化。经过对小型多模态肺炎数据集的评估，FMT 达到了94%的准确率、95%的召回率和93%的F1分数，超越了单模态基线（ResNet: 89%; BERT: 79%）和医学基准CheXMed（90%），提供了资源受限医疗环境中多模态肺炎诊断的可扩展解决方案。 

---
# Robust Multimodal Learning for Ophthalmic Disease Grading via Disentangled Representation 

**Title (ZH)**: 基于解耦表示的鲁棒多模态学习在眼科疾病分级中的应用 

**Authors**: Xinkun Wang, Yifang Wang, Senwei Liang, Feilong Tang, Chengzhi Liu, Ming Hu, Chao Hu, Junjun He, Zongyuan Ge, Imran Razzak  

**Link**: [PDF](https://arxiv.org/pdf/2503.05319)  

**Abstract**: This paper discusses how ophthalmologists often rely on multimodal data to improve diagnostic accuracy. However, complete multimodal data is rare in real-world applications due to a lack of medical equipment and concerns about data privacy. Traditional deep learning methods typically address these issues by learning representations in latent space. However, the paper highlights two key limitations of these approaches: (i) Task-irrelevant redundant information (e.g., numerous slices) in complex modalities leads to significant redundancy in latent space representations. (ii) Overlapping multimodal representations make it difficult to extract unique features for each modality. To overcome these challenges, the authors propose the Essence-Point and Disentangle Representation Learning (EDRL) strategy, which integrates a self-distillation mechanism into an end-to-end framework to enhance feature selection and disentanglement for more robust multimodal learning. Specifically, the Essence-Point Representation Learning module selects discriminative features that improve disease grading performance. The Disentangled Representation Learning module separates multimodal data into modality-common and modality-unique representations, reducing feature entanglement and enhancing both robustness and interpretability in ophthalmic disease diagnosis. Experiments on multimodal ophthalmology datasets show that the proposed EDRL strategy significantly outperforms current state-of-the-art methods. 

**Abstract (ZH)**: 本文讨论了眼科医生如何依靠多模态数据以提高诊断准确性。然而，在实际应用中，由于缺乏医疗设备和数据隐私方面的担忧，完整的多模态数据很少见。传统的深度学习方法通常通过在潜在空间中学习表示来解决这些问题。然而，本文强调了这些方法的两个关键局限性：（i）在复杂模态中存在与任务无关的冗余信息（如大量切片），导致潜在空间表示中的显著冗余；（ii）重叠的多模态表示使得难以提取每种模态的独特特征。为克服这些挑战，作者提出了一种称为Essence-Point和解耦表示学习（EDRL）的策略，该策略将自我蒸馏机制集成到端到端框架中，以增强特征选择和解耦，从而提高多模态学习的鲁棒性。具体来说，Essence-Point表示学习模块选择能提高疾病分级性能的判别特征。解耦表示学习模块将多模态数据分离为模态通用和模态独特表示，减少特征纠缠并增强眼部疾病诊断的鲁棒性和可解释性。实验表明，所提出的EDRL策略在多模态眼科数据集上的性能显著优于当前最先进的方法。 

---
# Replicating Human Social Perception in Generative AI: Evaluating the Valence-Dominance Model 

**Title (ZH)**: 在生成式AI中复制人类社会知觉：评价情感主导模型 

**Authors**: Necdet Gurkan, Kimathi Njoki, Jordan W. Suchow  

**Link**: [PDF](https://arxiv.org/pdf/2503.04842)  

**Abstract**: As artificial intelligence (AI) continues to advance--particularly in generative models--an open question is whether these systems can replicate foundational models of human social perception. A well-established framework in social cognition suggests that social judgments are organized along two primary dimensions: valence (e.g., trustworthiness, warmth) and dominance (e.g., power, assertiveness). This study examines whether multimodal generative AI systems can reproduce this valence-dominance structure when evaluating facial images and how their representations align with those observed across world regions. Through principal component analysis (PCA), we found that the extracted dimensions closely mirrored the theoretical structure of valence and dominance, with trait loadings aligning with established definitions. However, many world regions and generative AI models also exhibited a third component, the nature and significance of which warrant further investigation. These findings demonstrate that multimodal generative AI systems can replicate key aspects of human social perception, raising important questions about their implications for AI-driven decision-making and human-AI interactions. 

**Abstract (ZH)**: 随着人工智能（AI）的不断发展——特别是在生成模型方面——一个开放式问题是这些系统是否能够复制人类社会感知的基础模型。社会认知中一个成熟的框架表明，社会判断沿着两个主要维度组织：效价（例如，信任度，温暖）和支配性（例如，权力，主导性）。本研究探讨了多模态生成AI系统在评估面部图像时是否能够重现这一效价-支配性结构，以及它们的表示方式如何与世界各地的观察结果相一致。通过主成分分析（PCA），我们发现提取的维度与理论中的效价和支配性结构高度一致，特性载荷与既定定义相符。然而，许多世界地区和生成AI模型还表现出第三个成分，其性质和意义需要进一步研究。这些发现表明，多模态生成AI系统能够复制人类社会感知的关键方面，这引发了关于其对AI驱动决策和人机交互影响的重要问题。 

---
# Advancing Multimodal In-Context Learning in Large Vision-Language Models with Task-aware Demonstrations 

**Title (ZH)**: 在大型视觉语言模型中通过任务感知演示促进多模态上下文学习 

**Authors**: Yanshu Li  

**Link**: [PDF](https://arxiv.org/pdf/2503.04839)  

**Abstract**: Multimodal in-context learning (ICL) has emerged as a key capability of Large Vision-Language Models (LVLMs), driven by their increasing scale and applicability. Despite its promise, effective ICL in the multimodal setting remains challenging due to the inherent complexity of image-text inputs and the high sensitivity of ICL performance to input configurations. In this work, we shed light on the core mechanism underlying multimodal ICL, identifying task mapping as a crucial factor in configuring robust in-context demonstration (ICD) sequences. Building on these insights, we propose \textit{SabER}, a lightweight yet powerful decoder-only transformer equipped with task-aware attention, which intelligently selects and arranges ICDs from a demonstration library in an autoregressive fashion. This design enables fine-grained feature extraction and cross-modal reasoning, iteratively refining task mapping to generate high-quality ICD sequences. Through extensive experiments covering five LVLMs and nine benchmark datasets, SabER not only demonstrates strong empirical performance, but also provides deeper understanding of how task semantics interact with multimodal ICDs. Our findings highlight the importance of principled ICD sequence configuration and open new avenues to enhance multimodal ICL in a wide range of real-world scenarios. 

**Abstract (ZH)**: 多模态上下文学习（ICL）已成为大型视觉-语言模型（LVLMs）的关键能力，随着模型规模的增加和应用场景的扩展而兴起。尽管前景广阔，但在多模态环境中有效的ICL仍然具有挑战性，这归因于图像-文本输入的内在复杂性以及ICL性能对输入配置的高敏感性。在本文中，我们揭示了多模态ICL的核心机制，并识别任务映射是配置鲁棒的上下文示范（ICD）序列的关键因素。基于这些见解，我们提出了一种轻量级但强大的仅解码器变换器\textit{SabER}，它配备了任务感知注意力机制，并以自回归方式智能选择和排列示范库中的ICD。这种设计使细粒度特征提取和跨模态推理成为可能，迭代优化任务映射以生成高质量的ICD序列。通过涵盖五种LVLM和九个基准数据集的 extensive 实验，\textit{SabER} 不仅展示了强大的实证性能，还加深了对任务语义与多模态ICD之间相互作用的理解。我们的发现强调了原理性ICD序列配置的重要性，并为在广泛的实际场景中增强多模态ICL开辟了新的途径。 

---
# PGAD: Prototype-Guided Adaptive Distillation for Multi-Modal Learning in AD Diagnosis 

**Title (ZH)**: PGAD：原型引导的自适应 distilled 多模态学习在AD诊断中的应用 

**Authors**: Yanfei Li, Teng Yin, Wenyi Shang, Jingyu Liu, Xi Wang, Kaiyang Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2503.04836)  

**Abstract**: Missing modalities pose a major issue in Alzheimer's Disease (AD) diagnosis, as many subjects lack full imaging data due to cost and clinical constraints. While multi-modal learning leverages complementary information, most existing methods train only on complete data, ignoring the large proportion of incomplete samples in real-world datasets like ADNI. This reduces the effective training set and limits the full use of valuable medical data. While some methods incorporate incomplete samples, they fail to effectively address inter-modal feature alignment and knowledge transfer challenges under high missing rates. To address this, we propose a Prototype-Guided Adaptive Distillation (PGAD) framework that directly incorporates incomplete multi-modal data into training. PGAD enhances missing modality representations through prototype matching and balances learning with a dynamic sampling strategy. We validate PGAD on the ADNI dataset with varying missing rates (20%, 50%, and 70%) and demonstrate that it significantly outperforms state-of-the-art approaches. Ablation studies confirm the effectiveness of prototype matching and adaptive sampling, highlighting the potential of our framework for robust and scalable AD diagnosis in real-world clinical settings. 

**Abstract (ZH)**: 缺失模态数据在阿尔茨海默病诊断中构成重大挑战：现有的方法大多仅在完整数据上进行训练，忽略了真实世界数据集中（如ADNI）大量不完整样本的价值，这限制了有效训练集的大小并限制了宝贵医学数据的充分利用。为此，我们提出了一种原型导向自适应蒸馏（PGAD）框架，该框架可以直接将不完整多模态数据纳入训练。PGAD通过原型匹配增强缺失模态表示，并借助动态采样策略平衡学习。我们在不同缺失率（20%，50%，70%）的ADNI数据集上验证了PGAD，并且结果显示其显著优于现有先进方法。消融实验确认了原型匹配和自适应采样的有效性，突显了该框架在真实临床环境中进行鲁棒且可扩展的阿尔茨海默病诊断的潜力。 

---
# RTFusion: A depth estimation network based on multimodal fusion in challenging scenarios 

**Title (ZH)**: RTFusion：基于多模态融合的挑战场景深度估计网络 

**Authors**: Zelin Meng, Takanori Fukao  

**Link**: [PDF](https://arxiv.org/pdf/2503.04821)  

**Abstract**: Depth estimation in complex real-world scenarios is a challenging task, especially when relying solely on a single modality such as visible light or thermal infrared (THR) imagery. This paper proposes a novel multimodal depth estimation model, RTFusion, which enhances depth estimation accuracy and robustness by integrating the complementary strengths of RGB and THR data. The RGB modality provides rich texture and color information, while the THR modality captures thermal patterns, ensuring stability under adverse lighting conditions such as extreme illumination. The model incorporates a unique fusion mechanism, EGFusion, consisting of the Mutual Complementary Attention (MCA) module for cross-modal feature alignment and the Edge Saliency Enhancement Module (ESEM) to improve edge detail preservation. Comprehensive experiments on the MS2 and ViViD++ datasets demonstrate that the proposed model consistently produces high-quality depth maps across various challenging environments, including nighttime, rainy, and high-glare conditions. The experimental results highlight the potential of the proposed method in applications requiring reliable depth estimation, such as autonomous driving, robotics, and augmented reality. 

**Abstract (ZH)**: 复杂现实场景中的深度估计是一项具有挑战性的任务，尤其是在依赖单一模态如可见光或热红外（THR）图像时。本文提出了一种新颖的多模态深度估计模型RTFusion，该模型通过结合RGB和THR数据的互补优势，提高了深度估计的准确性和鲁棒性。RGB模态提供丰富的纹理和颜色信息，而THR模态捕捉热模式，确保在极端光照等恶劣光照条件下保持稳定性。该模型结合了一种独特的融合机制EGFusion，包括用于跨模态特征对齐的Mutual Complementary Attention (MCA) 模块和用于增强边缘细节保留的Edge Saliency Enhancement Module (ESEM)。在MS2和ViViD++数据集上的全面实验显示，所提模型在各种具有挑战性的环境中（包括夜间、雨天和高反射条件）能够生成高质量的深度图。实验结果强调了所提出方法在需要可靠深度估计的应用，如自动驾驶、机器人技术和增强现实中的潜力。 

---
# SuperRAG: Beyond RAG with Layout-Aware Graph Modeling 

**Title (ZH)**: SuperRAG：具有布局意识的图建模超越RAG 

**Authors**: Jeff Yang, Duy-Khanh Vu, Minh-Tien Nguyen, Xuan-Quang Nguyen, Linh Nguyen, Hung Le  

**Link**: [PDF](https://arxiv.org/pdf/2503.04790)  

**Abstract**: This paper introduces layout-aware graph modeling for multimodal RAG. Different from traditional RAG methods that mostly deal with flat text chunks, the proposed method takes into account the relationship of multimodalities by using a graph structure. To do that, a graph modeling structure is defined based on document layout parsing. The structure of an input document is retained with the connection of text chunks, tables, and figures. This representation allows the method to handle complex questions that require information from multimodalities. To confirm the efficiency of the graph modeling, a flexible RAG pipeline is developed using robust components. Experimental results on four benchmark test sets confirm the contribution of the layout-aware modeling for performance improvement of the RAG pipeline. 

**Abstract (ZH)**: 基于布局感知的图建模在多模态RAG中的应用 

---
