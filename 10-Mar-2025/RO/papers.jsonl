{'arxiv_id': 'arXiv:2503.05666', 'title': 'Kinodynamic Model Predictive Control for Energy Efficient Locomotion of Legged Robots with Parallel Elasticity', 'authors': 'Yulun Zhuang, Yichen Wang, Yanran Ding', 'link': 'https://arxiv.org/abs/2503.05666', 'abstract': 'In this paper, we introduce a kinodynamic model predictive control (MPC) framework that exploits unidirectional parallel springs (UPS) to improve the energy efficiency of dynamic legged robots. The proposed method employs a hierarchical control structure, where the solution of MPC with simplified dynamic models is used to warm-start the kinodynamic MPC, which accounts for nonlinear centroidal dynamics and kinematic constraints. The proposed approach enables energy efficient dynamic hopping on legged robots by using UPS to reduce peak motor torques and energy consumption during stance phases. Simulation results demonstrated a 38.8% reduction in the cost of transport (CoT) for a monoped robot equipped with UPS during high-speed hopping. Additionally, preliminary hardware experiments show a 14.8% reduction in energy consumption. Video: this https URL', 'abstract_zh': '本文介绍了一种利用单向平行弹簧（UPS）改善动态腿式机器人能量效率的kinodynamic模型预测控制（MPC）框架。提出的办法采用分层控制结构，其中简化动力学模型的MPC解用于预热odynamics MPC，该方法考虑了非线性质心动态和运动学约束。提出的办法通过使用UPS在支撑阶段减少峰值电机扭矩和能量消耗，从而实现能量高效的动态跳跃。仿真实验表明，配备UPS的单腿机器人在高速跳跃时运输成本减少了38.8%。此外，初步硬件实验显示能量消耗减少了14.8%。视频：这个链接https://这个链接结尾被截断，请访问上述链接以获取视频内容。', 'title_zh': '基于并联弹性性的腿式机器人高效运动的运动动力学模型预测控制'}
{'arxiv_id': 'arXiv:2503.05656', 'title': 'Small-Scale Testbeds for Connected and Automated Vehicles and Robot Swarms: Challenges and a Roadmap', 'authors': 'Jianye Xu, Bassam Alrifaee, Johannes Betz, Armin Mokhtarian, Archak Mittal, Mengchi Cai, Rahul Mangharam, Omar M. Shehata, Catherine M. Elias, Jan-Nico Zaech, Patrick Scheffe, Felix Jahncke, Sangeet Sankaramangalam Ulhas, Kaj Munhoz Arfvidsson', 'link': 'https://arxiv.org/abs/2503.05656', 'abstract': 'This article proposes a roadmap to address the current challenges in small-scale testbeds for Connected and Automated Vehicles (CAVs) and robot swarms. The roadmap is a joint effort of participants in the workshop "1st Workshop on Small-Scale Testbeds for Connected and Automated Vehicles and Robot Swarms," held on June 2 at the IEEE Intelligent Vehicles Symposium (IV) 2024 in Jeju, South Korea. The roadmap contains three parts: 1) enhancing accessibility and diversity, especially for underrepresented communities, 2) sharing best practices for the development and maintenance of testbeds, and 3) connecting testbeds through an abstraction layer to support collaboration. The workshop features eight invited speakers, four contributed papers [1]-[4], and a presentation of a survey paper on testbeds [5]. The survey paper provides an online comparative table of more than 25 testbeds, available at this https URL. The workshop\'s own website is available at this https URL.', 'abstract_zh': '本文提出了一条路线图，以应对连接和自动车辆（CAVs）及机器人蜂群小型试验床目前面临的挑战。该路线图是参加于2024年6月2日在韩国济州举行的IEEE智能汽车研讨会（IV）上举行的“首届连接和自动车辆及机器人蜂群小型试验床研讨会”工作坊的参与者们共同制定的。该路线图包含三个部分：1）提升 accessibility 和多样性，尤其是促进代表性不足的社区，2）分享试验床开发和维护的最佳实践，3）通过抽象层连接试验床以支持协作。研讨会设有八位特邀演讲嘉宾，四篇贡献论文 [1]-[4]，以及一篇关于试验床的综述论文的介绍 [5]。综述论文提供了一份在线比较表，包含超过25个试验床，并可通过此链接访问：https://link. Here. 研讨会的官方网站可通过此链接访问：https://link. Here.', 'title_zh': '面向连接和自动驾驶车辆及机器人 swarm 的小规模试验床：挑战与 roadmap'}
{'arxiv_id': 'arXiv:2503.05652', 'title': 'BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities', 'authors': 'Yunfan Jiang, Ruohan Zhang, Josiah Wong, Chen Wang, Yanjie Ze, Hang Yin, Cem Gokmen, Shuran Song, Jiajun Wu, Li Fei-Fei', 'link': 'https://arxiv.org/abs/2503.05652', 'abstract': "Real-world household tasks present significant challenges for mobile manipulation robots. An analysis of existing robotics benchmarks reveals that successful task performance hinges on three key whole-body control capabilities: bimanual coordination, stable and precise navigation, and extensive end-effector reachability. Achieving these capabilities requires careful hardware design, but the resulting system complexity further complicates visuomotor policy learning. To address these challenges, we introduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for whole-body manipulation in diverse household tasks. Built on a bimanual, wheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body teleoperation interface for data collection and a novel algorithm for learning whole-body visuomotor policies. We evaluate BRS on five challenging household tasks that not only emphasize the three core capabilities but also introduce additional complexities, such as long-range navigation, interaction with articulated and deformable objects, and manipulation in confined spaces. We believe that BRS's integrated robotic embodiment, data collection interface, and learning framework mark a significant step toward enabling real-world whole-body manipulation for everyday household tasks. BRS is open-sourced at this https URL", 'abstract_zh': '真实世界家庭任务对移动操作机器人构成显著挑战。现有机器人基准分析表明，任务成功完成依赖于三项关键的整体身体控制能力：双臂协调、稳定而精确的导航以及广泛的手爪可达性。实现这些能力需要精细的硬件设计，但由此产生的系统复杂性进一步复杂化了视知觉运动策略的学习。为应对这些挑战，我们介绍了BEHAVIOR机器人套件（BRS），这是一个用于多种家庭任务的整体身体操作综合框架。BRS基于一个双臂轮式机器人，具有4自由度的躯干，集成了一个低成本的整体身体远程操作接口以进行数据收集，以及一种新颖的整体身体视知觉运动学习算法。我们在五个具有挑战性的家庭任务中评估了BRS，这些任务不仅强调了三项核心能力，还引入了额外的复杂性，例如远程导航、与关节运动和变形物体的交互以及在受限空间中的操作。我们相信，BRS集成了机器人实体、数据收集接口和学习框架，标志着向为日常生活家庭任务实现真实-world整体身体操作迈出的重要一步。BRS已在该网址开源：this https URL。', 'title_zh': 'BEHAVIOR 机器人套件：简化日常生活家居活动的全身 manipulation 流程'}
{'arxiv_id': 'arXiv:2503.05646', 'title': 'dARt Vinci: Egocentric Data Collection for Surgical Robot Learning at Scale', 'authors': 'Yihao Liu, Yu-Chun Ku, Jiaming Zhang, Hao Ding, Peter Kazanzides, Mehran Armand', 'link': 'https://arxiv.org/abs/2503.05646', 'abstract': "Data scarcity has long been an issue in the robot learning community. Particularly, in safety-critical domains like surgical applications, obtaining high-quality data can be especially difficult. It poses challenges to researchers seeking to exploit recent advancements in reinforcement learning and imitation learning, which have greatly improved generalizability and enabled robots to conduct tasks autonomously. We introduce dARt Vinci, a scalable data collection platform for robot learning in surgical settings. The system uses Augmented Reality (AR) hand tracking and a high-fidelity physics engine to capture subtle maneuvers in primitive surgical tasks: By eliminating the need for a physical robot setup and providing flexibility in terms of time, space, and hardware resources-such as multiview sensors and actuators-specialized simulation is a viable alternative. At the same time, AR allows the robot data collection to be more egocentric, supported by its body tracking and content overlaying capabilities. Our user study confirms the proposed system's efficiency and usability, where we use widely-used primitive tasks for training teleoperation with da Vinci surgical robots. Data throughput improves across all tasks compared to real robot settings by 41% on average. The total experiment time is reduced by an average of 10%. The temporal demand in the task load survey is improved. These gains are statistically significant. Additionally, the collected data is over 400 times smaller in size, requiring far less storage while achieving double the frequency.", 'abstract_zh': '数据不足长期是机器人学习领域的瓶颈问题。特别是在如外科应用等安全性要求高的领域，获取高质量数据尤为困难。这给致力于利用强化学习和模仿学习最新进展的研究人员带来了挑战，这些进展极大地提高了通用性并使机器人能够自主执行任务。我们引入了dARt Vinci，这是一种可扩展的外科手术场景下机器人学习数据采集平台。该系统利用增强现实（AR）手部跟踪和高保真物理引擎来捕捉基础外科任务中的细微操作：通过消除对物理机器人设置的需求，并在时间、空间和硬件资源（如多视角传感器和执行器）方面提供灵活性，专用仿真成为一种可行的替代方案。同时，AR允许机器人数据采集更加以主观视角为中心，得益于其肢体跟踪和内容叠加能力。我们的用户研究证实了所提出系统的高效性和易用性，其中我们使用广为使用的基础任务进行达芬奇外科手术机器人的遥操作训练。与实际机器人设置相比，数据吞吐量在所有任务中平均提高了41%。总实验时间平均缩短了10%。任务负载的时间要求得到了改善。这些增益在统计学上具有显著性。另外，收集的数据量大约小了400倍，需要更少的存储空间，但实现了两倍的频率。', 'title_zh': '达芬奇：以自我中心视角进行外科机器人规模化学习的数据收集'}
{'arxiv_id': 'arXiv:2503.05630', 'title': 'Joint 3D Point Cloud Segmentation using Real-Sim Loop: From Panels to Trees and Branches', 'authors': 'Tian Qiu, Ruiming Du, Nikolai Spine, Lailiang Cheng, Yu Jiang', 'link': 'https://arxiv.org/abs/2503.05630', 'abstract': 'Modern orchards are planted in structured rows with distinct panel divisions to improve management. Accurate and efficient joint segmentation of point cloud from Panel to Tree and Branch (P2TB) is essential for robotic operations. However, most current segmentation methods focus on single instance segmentation and depend on a sequence of deep networks to perform joint tasks. This strategy hinders the use of hierarchical information embedded in the data, leading to both error accumulation and increased costs for annotation and computation, which limits its scalability for real-world applications. In this study, we proposed a novel approach that incorporated a Real2Sim L-TreeGen for training data generation and a joint model (J-P2TB) designed for the P2TB task. The J-P2TB model, trained on the generated simulation dataset, was used for joint segmentation of real-world panel point clouds via zero-shot learning. Compared to representative methods, our model outperformed them in most segmentation metrics while using 40% fewer learnable parameters. This Sim2Real result highlighted the efficacy of L-TreeGen in model training and the performance of J-P2TB for joint segmentation, demonstrating its strong accuracy, efficiency, and generalizability for real-world applications. These improvements would not only greatly benefit the development of robots for automated orchard operations but also advance digital twin technology.', 'abstract_zh': '现代果园中基于结构化行植株分割的点云联合分割方法研究：从面板到树木和枝条（P2TB）的任务', 'title_zh': '使用实-模环路的联合3D点云分割：从面板到树和枝条'}
{'arxiv_id': 'arXiv:2503.05623', 'title': 'Limits of specifiability for sensor-based robotic planning tasks', 'authors': "Basak Sakcak, Dylan A. Shell, Jason M. O'Kane", 'link': 'https://arxiv.org/abs/2503.05623', 'abstract': "There is now a large body of techniques, many based on formal methods, for describing and realizing complex robotics tasks, including those involving a variety of rich goals and time-extended behavior. This paper explores the limits of what sorts of tasks are specifiable, examining how the precise grounding of specifications, that is, whether the specification is given in terms of the robot's states, its actions and observations, its knowledge, or some other information,is crucial to whether a given task can be specified. While prior work included some description of particular choices for this grounding, our contribution treats this aspect as a first-class citizen: we introduce notation to deal with a large class of problems, and examine how the grounding affects what tasks can be posed. The results demonstrate that certain classes of tasks are specifiable under different combinations of groundings.", 'abstract_zh': '现有的技术，许多基于形式化方法，可用于描述和实现复杂的机器人任务，包括涉及多种丰富目标和长时间行为的任务。本文探讨了可描述任务的极限，研究了规范的确切 grounding，即规范是以机器人状态、行动和观察、知识或其他信息的形式给出的，对是否可以描述给定任务至关重要。虽然早期工作对此 grounding 有一些描述，但我们在此将这一方面视为一等公民：我们引入符号来处理一类问题，并探讨 grounding 如何影响可提出的任务类型。结果表明，在不同的 grounding 组合下，某些类别的任务是可以描述的。', 'title_zh': '基于传感器的机器人规划任务的可指谓极限'}
{'arxiv_id': 'arXiv:2503.05619', 'title': 'Learning and generalization of robotic dual-arm manipulation of boxes from demonstrations via Gaussian Mixture Models (GMMs)', 'authors': 'Qian Ying Lee, Suhas Raghavendra Kulkarni, Kenzhi Iskandar Wong, Lin Yang, Bernardo Noronha, Yongjun Wee, Tzu-Yi Hung, Domenico Campolo', 'link': 'https://arxiv.org/abs/2503.05619', 'abstract': 'Learning from demonstration (LfD) is an effective method to teach robots to move and manipulate objects in a human-like manner. This is especially true when dealing with complex robotic systems, such as those with dual arms employed for their improved payload capacity and manipulability. However, a key challenge is in expanding the robotic movements beyond the learned scenarios to adapt to minor and major variations from the specific demonstrations. In this work, we propose a learning and novel generalization approach that adapts the learned Gaussian Mixture Model (GMM)-parameterized policy derived from human demonstrations. Our method requires only a small number of human demonstrations and eliminates the need for a robotic system during the demonstration phase, which can significantly reduce both cost and time. The generalization process takes place directly in the parameter space, leveraging the lower-dimensional representation of GMM parameters. With only three parameters per Gaussian component, this process is computationally efficient and yields immediate results upon request. We validate our approach through real-world experiments involving a dual-arm robotic manipulation of boxes. Starting with just five demonstrations for a single task, our approach successfully generalizes to new unseen scenarios, including new target locations, orientations, and box sizes. These results highlight the practical applicability and scalability of our method for complex manipulations.', 'abstract_zh': '基于演示学习（LfD）在复杂双臂机器人操作中的有效学习与新颖泛化方法', 'title_zh': '基于高斯混合模型（GMMs）从演示学习和推广双臂机器人搬运箱子的能力'}
{'arxiv_id': 'arXiv:2503.05573', 'title': 'InDRiVE: Intrinsic Disagreement based Reinforcement for Vehicle Exploration through Curiosity Driven Generalized World Model', 'authors': 'Feeza Khan Khanzada, Jaerock Kwon', 'link': 'https://arxiv.org/abs/2503.05573', 'abstract': 'Model-based Reinforcement Learning (MBRL) has emerged as a promising paradigm for autonomous driving, where data efficiency and robustness are critical. Yet, existing solutions often rely on carefully crafted, task specific extrinsic rewards, limiting generalization to new tasks or environments. In this paper, we propose InDRiVE (Intrinsic Disagreement based Reinforcement for Vehicle Exploration), a method that leverages purely intrinsic, disagreement based rewards within a Dreamer based MBRL framework. By training an ensemble of world models, the agent actively explores high uncertainty regions of environments without any task specific feedback. This approach yields a task agnostic latent representation, allowing for rapid zero shot or few shot fine tuning on downstream driving tasks such as lane following and collision avoidance. Experimental results in both seen and unseen environments demonstrate that InDRiVE achieves higher success rates and fewer infractions compared to DreamerV2 and DreamerV3 baselines despite using significantly fewer training steps. Our findings highlight the effectiveness of purely intrinsic exploration for learning robust vehicle control behaviors, paving the way for more scalable and adaptable autonomous driving systems.', 'abstract_zh': '基于模型的强化学习(InDRiVE):基于内在分歧的车辆探索', 'title_zh': 'InDRiVE：固有分歧驱动的强化学习在好奇心驱动的通用世界模型中的车辆探索'}
{'arxiv_id': 'arXiv:2503.05569', 'title': 'A-SEE2.0: Active-Sensing End-Effector for Robotic Ultrasound Systems with Dense Contact Surface Perception Enabled Probe Orientation Adjustment', 'authors': 'Yernar Zhetpissov, Xihan Ma, Kehan Yang, Haichong K. Zhang', 'link': 'https://arxiv.org/abs/2503.05569', 'abstract': 'Conventional freehand ultrasound (US) imaging is highly dependent on the skill of the operator, often leading to inconsistent results and increased physical demand on sonographers. Robotic Ultrasound Systems (RUSS) aim to address these limitations by providing standardized and automated imaging solutions, especially in environments with limited access to skilled operators. This paper presents the development of a novel RUSS system that employs dual RGB-D depth cameras to maintain the US probe normal to the skin surface, a critical factor for optimal image quality. Our RUSS integrates RGB-D camera data with robotic control algorithms to maintain orthogonal probe alignment on uneven surfaces without preoperative data. Validation tests using a phantom model demonstrate that the system achieves robust normal positioning accuracy while delivering ultrasound images comparable to those obtained through manual scanning. A-SEE2.0 demonstrates 2.47 ${\\pm}$ 1.25 degrees error for flat surface normal-positioning and 12.19 ${\\pm}$ 5.81 degrees normal estimation error on mannequin surface. This work highlights the potential of A-SEE2.0 to be used in clinical practice by testing its performance during in-vivo forearm ultrasound examinations.', 'abstract_zh': '传统自由手超声成像高度依赖操作者的技能，往往会带来不一致的结果并增加超声技师的体力负担。机器人超声系统（RUSS）旨在通过提供标准化和自动化的成像解决方案来解决这些问题，特别是在限制性操作者技能的环境中。本文介绍了一种采用双RGB-D深度摄像头的新颖RUSS系统，该系统用于保持超声探头与皮肤表面垂直，这对于获得最佳图像质量至关重要。我们的RUSS系统将RGB-D摄像头数据与机器人控制算法相结合，在无需术前数据的情况下，在不平表面上保持探头的正交对准。使用仿真模型进行的验证测试显示，该系统在实现稳健的垂直定位精度的同时，提供的超声图像与手动扫描获得的图像相当。A-SEE2.0在平坦表面的垂直定位误差为2.47 ${\\pm}$ 1.25 度，在人体模型表面的垂直估计误差为12.19 ${\\pm}$ 5.81 度。这项工作通过在活体前臂超声检查中测试其性能，突显了A-SEE2.0在临床实践中的潜在应用价值。', 'title_zh': 'A-SEE2.0: 具有密集接触表面感知的主动传感末端执行器，实现探头姿态调整的机器人超声系统'}
{'arxiv_id': 'arXiv:2503.05539', 'title': 'Accelerating db-A$^\\textbf{*}$ for Kinodynamic Motion Planning Using Diffusion', 'authors': 'Julius Franke, Akmaral Moldagalieva, Pia Hanfeld, Wolfgang Hönig', 'link': 'https://arxiv.org/abs/2503.05539', 'abstract': 'We present a novel approach for generating motion primitives for kinodynamic motion planning using diffusion models. The motions generated by our approach are adapted to each problem instance by utilizing problem-specific parameters, allowing for finding solutions faster and of better quality. The diffusion models used in our approach are trained on randomly cut solution trajectories. These trajectories are created by solving randomly generated problem instances with a kinodynamic motion planner. Experimental results show significant improvements up to 30 percent in both computation time and solution quality across varying robot dynamics such as second-order unicycle or car with trailer.', 'abstract_zh': '使用扩散模型生成适应性运动基本单元的新型方法：针对动力学运动规划的快速高效解决方案', 'title_zh': '使用扩散加速db-A*算法的kinodynamic运动规划'}
{'arxiv_id': 'arXiv:2503.05508', 'title': 'Design, Dynamic Modeling and Control of a 2-DOF Robotic Wrist Actuated by Twisted and Coiled Actuators', 'authors': 'Yunsong Zhang, Xinyu Zhou, Feitian Zhang', 'link': 'https://arxiv.org/abs/2503.05508', 'abstract': 'Robotic wrists play a pivotal role in the functionality of industrial manipulators and humanoid robots, facilitating manipulation and grasping tasks. In recent years, there has been a growing interest in integrating artificial muscle-driven actuators for robotic wrists, driven by advancements in technology offering high energy density, lightweight construction, and compact designs. However, in the study of robotic wrists driven by artificial muscles, dynamic model-based controllers are often overlooked, despite their critical importance for motion analysis and dynamic control of robots. This paper presents a novel design of a two-degree-of-freedom (2-DOF) robotic wrist driven by twisted and coiled actuators (TCA) utilizing a parallel mechanism with a 3RRRR configuration. The proposed robotic wrist is expected to feature lightweight structures and superior motion performance while mitigating friction issues. The Lagrangian dynamic model of the wrist is established, along with a nonlinear model predictive controller (NMPC) designed for trajectory tracking tasks. A prototype of the robotic wrist is developed, and extensive experiments are conducted to validate its superior motion performance and the proposed dynamic model. Subsequently, extensive comparative experiments between NMPC and PID controller were conducted under various operating conditions. The experimental results demonstrate the effectiveness and robustness of the dynamic model-based controller in the motion control of TCA-driven robotic wrists.', 'abstract_zh': '基于人工肌肉驱动的两自由度并联机械手腕的设计与动态控制', 'title_zh': '由扭曲和螺旋执行器驱动的2-DOF机器人手腕的设计、动态建模与控制'}
{'arxiv_id': 'arXiv:2503.05490', 'title': 'Adaptive Neural Unscented Kalman Filter', 'authors': 'Amit Levy, Itzik Klein', 'link': 'https://arxiv.org/abs/2503.05490', 'abstract': 'The unscented Kalman filter is an algorithm capable of handling nonlinear scenarios. Uncertainty in process noise covariance may decrease the filter estimation performance or even lead to its divergence. Therefore, it is important to adjust the process noise covariance matrix in real time. In this paper, we developed an adaptive neural unscented Kalman filter to cope with time-varying uncertainties during platform operation. To this end, we devised ProcessNet, a simple yet efficient end-to-end regression network to adaptively estimate the process noise covariance matrix. We focused on the nonlinear inertial sensor and Doppler velocity log fusion problem in the case of autonomous underwater vehicle navigation. Using a real-world recorded dataset from an autonomous underwater vehicle, we demonstrated our filter performance and showed its advantages over other adaptive and non-adaptive nonlinear filters.', 'abstract_zh': '无迹卡尔曼滤波器是一种能够处理非线性场景的算法。过程噪声协方差中的不确定性可能会降低滤波器的估计性能，甚至导致滤波器发散。因此，实时调整过程噪声协方差矩阵非常重要。在本文中，我们开发了一种自适应神经无迹卡尔曼滤波器，以应对平台运行过程中时间变化的不确定性。为此，我们设计了ProcessNet，这是一种简单且高效的端到端回归网络，可自适应估计过程噪声协方差矩阵。我们专注于自主水下车辆导航中非线性惯性传感器与多普勒速度日志融合的问题。使用自主水下车辆记录的真实数据集，我们展示了该滤波器的性能，并且证明了其相对于其他自适应和非自适应非线性滤波器的优势。', 'title_zh': '自适应神经无迹卡尔曼滤波器'}
{'arxiv_id': 'arXiv:2503.05471', 'title': 'Topology-Driven Trajectory Optimization for Modelling Controllable Interactions Within Multi-Vehicle Scenario', 'authors': 'Changjia Ma, Yi Zhao, Zhongxue Gan, Bingzhao Gao, Wenchao Ding', 'link': 'https://arxiv.org/abs/2503.05471', 'abstract': 'Trajectory optimization in multi-vehicle scenarios faces challenges due to its non-linear, non-convex properties and sensitivity to initial values, making interactions between vehicles difficult to control. In this paper, inspired by topological planning, we propose a differentiable local homotopy invariant metric to model the interactions. By incorporating this topological metric as a constraint into multi-vehicle trajectory optimization, our framework is capable of generating multiple interactive trajectories from the same initial values, achieving controllable interactions as well as supporting user-designed interaction patterns. Extensive experiments demonstrate its superior optimality and efficiency over existing methods. We will release open-source code to advance relative research.', 'abstract_zh': '多车辆场景中的轨迹优化由于其非线性和非凸性质以及对初始值的敏感性而面临挑战，使得车辆之间的交互难以控制。受拓扑规划启发，本文提出了一种可微局部同伦不变度量来建模交互。通过将这种拓扑度量作为约束融入多车辆轨迹优化中，我们的框架能够从相同的初始值生成多个交互轨迹，实现可控的交互并支持用户设计的交互模式。详实验验证了其在最优性和效率上的优越性。我们将开放源代码以促进相关研究。', 'title_zh': '拓扑驱动的轨迹优化方法：多车辆场景中可控交互建模'}
{'arxiv_id': 'arXiv:2503.05425', 'title': 'LiDAR-enhanced 3D Gaussian Splatting Mapping', 'authors': 'Jian Shen, Huai Yu, Ji Wu, Wen Yang, Gui-Song Xia', 'link': 'https://arxiv.org/abs/2503.05425', 'abstract': 'This paper introduces LiGSM, a novel LiDAR-enhanced 3D Gaussian Splatting (3DGS) mapping framework that improves the accuracy and robustness of 3D scene mapping by integrating LiDAR data. LiGSM constructs joint loss from images and LiDAR point clouds to estimate the poses and optimize their extrinsic parameters, enabling dynamic adaptation to variations in sensor alignment. Furthermore, it leverages LiDAR point clouds to initialize 3DGS, providing a denser and more reliable starting points compared to sparse SfM points. In scene rendering, the framework augments standard image-based supervision with depth maps generated from LiDAR projections, ensuring an accurate scene representation in both geometry and photometry. Experiments on public and self-collected datasets demonstrate that LiGSM outperforms comparative methods in pose tracking and scene rendering.', 'abstract_zh': 'LiGSM：一种基于LiDAR的新型3D高斯点云重建（3DGS）建图框架', 'title_zh': 'LiDAR增强的3D高斯绘制映射'}
{'arxiv_id': 'arXiv:2503.05398', 'title': 'Self-Modeling Robots by Photographing', 'authors': 'Kejun Hu, Peng Yu, Ning Tan', 'link': 'https://arxiv.org/abs/2503.05398', 'abstract': 'Self-modeling enables robots to build task-agnostic models of their morphology and kinematics based on data that can be automatically collected, with minimal human intervention and prior information, thereby enhancing machine intelligence. Recent research has highlighted the potential of data-driven technology in modeling the morphology and kinematics of robots. However, existing self-modeling methods suffer from either low modeling quality or excessive data acquisition costs. Beyond morphology and kinematics, texture is also a crucial component of robots, which is challenging to model and remains unexplored. In this work, a high-quality, texture-aware, and link-level method is proposed for robot self-modeling. We utilize three-dimensional (3D) Gaussians to represent the static morphology and texture of robots, and cluster the 3D Gaussians to construct neural ellipsoid bones, whose deformations are controlled by the transformation matrices generated by a kinematic neural network. The 3D Gaussians and kinematic neural network are trained using data pairs composed of joint angles, camera parameters and multi-view images without depth information. By feeding the kinematic neural network with joint angles, we can utilize the well-trained model to describe the corresponding morphology, kinematics and texture of robots at the link level, and render robot images from different perspectives with the aid of 3D Gaussian splatting. Furthermore, we demonstrate that the established model can be exploited to perform downstream tasks such as motion planning and inverse kinematics.', 'abstract_zh': '基于数据的自建模技术使机器人能够构建与任务无关的形态和运动学模型，基于可自动收集的数据，无需大量人工干预和先验信息，从而提升机器智能。近期研究突显了基于数据技术在建模机器人形态和运动学方面的潜力。然而，现有自建模方法要么建模质量较低，要么数据采集成本过高。除了形态和运动学，纹理也是机器人的重要组成部分，其建模具有挑战性且尚未得到探索。本文提出了一种高质量、纹理感知且关节级的机器人自建模方法。我们使用三维高斯分布表示机器人的静态形态和纹理，并聚类三维高斯分布以构建神经椭球骨骼，其变形由由运动学神经网络生成的变换矩阵控制。三维高斯分布和运动学神经网络使用关节角度、相机参数和多视角图像（无深度信息）的数据对进行训练。通过向运动学神经网络输入关节角度，可以利用训练良好的模型在关节级描述机器人的形态、运动学和纹理，并借助三维高斯点绘制以不同视角渲染机器人图像。此外，我们证明建立的模型可用于执行诸如运动规划和逆运动学等下游任务。', 'title_zh': '自建模机器人：通过拍照实现'}
{'arxiv_id': 'arXiv:2503.05316', 'title': 'CoinRobot: Generalized End-to-end Robotic Learning for Physical Intelligence', 'authors': 'Yu Zhao, Huxian Liu, Xiang Chen, Jiankai Sun, Jiahuan Yan, Luhui Hu', 'link': 'https://arxiv.org/abs/2503.05316', 'abstract': 'Physical intelligence holds immense promise for advancing embodied intelligence, enabling robots to acquire complex behaviors from demonstrations. However, achieving generalization and transfer across diverse robotic platforms and environments requires careful design of model architectures, training strategies, and data diversity. Meanwhile existing systems often struggle with scalability, adaptability to heterogeneous hardware, and objective evaluation in real-world settings. We present a generalized end-to-end robotic learning framework designed to bridge this gap. Our framework introduces a unified architecture that supports cross-platform adaptability, enabling seamless deployment across industrial-grade robots, collaborative arms, and novel embodiments without task-specific modifications. By integrating multi-task learning with streamlined network designs, it achieves more robust performance than conventional approaches, while maintaining compatibility with varying sensor configurations and action spaces. We validate our framework through extensive experiments on seven manipulation tasks. Notably, Diffusion-based models trained in our framework demonstrated superior performance and generalizability compared to the LeRobot framework, achieving performance improvements across diverse robotic platforms and environmental conditions.', 'abstract_zh': '物理智能为推进嵌入式智能提供了巨大潜力，使机器人能够通过演示获取复杂行为。然而，要在不同类型的机器人平台和环境中实现泛化和迁移需要精心设计模型架构、训练策略和数据多样性。同时，现有系统在可扩展性、异构硬件的适应性以及现实环境中的目标评估方面经常面临挑战。我们提出了一种通用的端到端机器人学习框架，旨在弥合这一差距。该框架引入了一个统一的架构，支持跨平台适应性，能够在工业级机器人、协作臂和新型实体上无缝部署，无需针对特定任务进行修改。通过结合多任务学习和 streamlined 网络设计，该框架在保持与不同传感器配置和动作空间兼容的同时，实现了比传统方法更为稳健的性能。我们通过在七项操作任务上进行广泛的实验验证了该框架。值得注意的是，采用我们框架训练的扩散模型在多种机器人平台和环境条件下展现出优于 LeRobot 框架的性能和泛化能力。', 'title_zh': 'CoinRobot: 通用端到端机器人学习用于物理智能'}
{'arxiv_id': 'arXiv:2503.05301', 'title': 'A Helping (Human) Hand in Kinematic Structure Estimation', 'authors': 'Adrian Pfisterer, Xing Li, Vito Mengers, Oliver Brock', 'link': 'https://arxiv.org/abs/2503.05301', 'abstract': "Visual uncertainties such as occlusions, lack of texture, and noise present significant challenges in obtaining accurate kinematic models for safe robotic manipulation. We introduce a probabilistic real-time approach that leverages the human hand as a prior to mitigate these uncertainties. By tracking the constrained motion of the human hand during manipulation and explicitly modeling uncertainties in visual observations, our method reliably estimates an object's kinematic model online. We validate our approach on a novel dataset featuring challenging objects that are occluded during manipulation and offer limited articulations for perception. The results demonstrate that by incorporating an appropriate prior and explicitly accounting for uncertainties, our method produces accurate estimates, outperforming two recent baselines by 195% and 140%, respectively. Furthermore, we demonstrate that our approach's estimates are precise enough to allow a robot to manipulate even small objects safely.", 'abstract_zh': '视觉不确定性，如遮挡、缺乏纹理和噪声，给安全的机器人操作获取准确的运动模型带来了巨大挑战。我们提出了一种概率实时方法，利用人类手部动作作为先验知识以减轻这些不确定性。通过在操作过程中跟踪人类手部的受限运动并明确建模视觉观测中的不确定性，该方法可以在线可靠地估算物体的运动模型。我们在一个包含操作过程中被遮挡且感知度受限的挑战性物体的新数据集上验证了该方法。结果显示，通过引入适当的先验知识并明确考虑不确定性，该方法能产生准确的估算结果，分别比两个最近的基线方法提高了195%和140%。此外，我们展示了该方法的估算精度足以使机器人能够安全操作小物体。', 'title_zh': '协助(人类)进行运动结构估计'}
{'arxiv_id': 'arXiv:2503.05274', 'title': 'Evidential Uncertainty Estimation for Multi-Modal Trajectory Prediction', 'authors': 'Sajad Marvi, Christoph Rist, Julian Schmidt, Julian Jordan, Abhinav Valada', 'link': 'https://arxiv.org/abs/2503.05274', 'abstract': 'Accurate trajectory prediction is crucial for autonomous driving, yet uncertainty in agent behavior and perception noise makes it inherently challenging. While multi-modal trajectory prediction models generate multiple plausible future paths with associated probabilities, effectively quantifying uncertainty remains an open problem. In this work, we propose a novel multi-modal trajectory prediction approach based on evidential deep learning that estimates both positional and mode probability uncertainty in real time. Our approach leverages a Normal Inverse Gamma distribution for positional uncertainty and a Dirichlet distribution for mode uncertainty. Unlike sampling-based methods, it infers both types of uncertainty in a single forward pass, significantly improving efficiency. Additionally, we experimented with uncertainty-driven importance sampling to improve training efficiency by prioritizing underrepresented high-uncertainty samples over redundant ones. We perform extensive evaluations of our method on the Argoverse 1 and Argoverse 2 datasets, demonstrating that it provides reliable uncertainty estimates while maintaining high trajectory prediction accuracy.', 'abstract_zh': '基于证据深度学习的实时位置和模式概率不确定性估计多模态轨迹预测方法', 'title_zh': '多模态轨迹预测的证据不确定性估计'}
{'arxiv_id': 'arXiv:2503.05251', 'title': 'A Map-free Deep Learning-based Framework for Gate-to-Gate Monocular Visual Navigation aboard Miniaturized Aerial Vehicles', 'authors': 'Lorenzo Scarciglia, Antonio Paolillo, Daniele Palossi', 'link': 'https://arxiv.org/abs/2503.05251', 'abstract': 'Palm-sized autonomous nano-drones, i.e., sub-50g in weight, recently entered the drone racing scenario, where they are tasked to avoid obstacles and navigate as fast as possible through gates. However, in contrast with their bigger counterparts, i.e., kg-scale drones, nano-drones expose three orders of magnitude less onboard memory and compute power, demanding more efficient and lightweight vision-based pipelines to win the race. This work presents a map-free vision-based (using only a monocular camera) autonomous nano-drone that combines a real-time deep learning gate detection front-end with a classic yet elegant and effective visual servoing control back-end, only relying on onboard resources. Starting from two state-of-the-art tiny deep learning models, we adapt them for our specific task, and after a mixed simulator-real-world training, we integrate and deploy them aboard our nano-drone. Our best-performing pipeline costs of only 24M multiply-accumulate operations per frame, resulting in a closed-loop control performance of 30 Hz, while achieving a gate detection root mean square error of 1.4 pixels, on our ~20k real-world image dataset. In-field experiments highlight the capability of our nano-drone to successfully navigate through 15 gates in 4 min, never crashing and covering a total travel distance of ~100m, with a peak flight speed of 1.9 m/s. Finally, to stress the generalization capability of our system, we also test it in a never-seen-before environment, where it navigates through gates for more than 4 min.', 'abstract_zh': '手掌大小自主纳米无人机：基于单目视觉的无地图自主纳米无人机，结合实时深度学习门检测前端与经典的直观有效视觉伺服控制后端，仅依赖于机载资源', 'title_zh': '基于深度学习的无地图门到门单目视觉导航框架（适用于微型空中车辆）'}
{'arxiv_id': 'arXiv:2503.05231', 'title': 'Kaiwu: A Multimodal Manipulation Dataset and Framework for Robot Learning and Human-Robot Interaction', 'authors': 'Shuo Jiang, Haonan Li, Ruochen Ren, Yanmin Zhou, Zhipeng Wang, Bin He', 'link': 'https://arxiv.org/abs/2503.05231', 'abstract': 'Cutting-edge robot learning techniques including foundation models and imitation learning from humans all pose huge demands on large-scale and high-quality datasets which constitute one of the bottleneck in the general intelligent robot fields. This paper presents the Kaiwu multimodal dataset to address the missing real-world synchronized multimodal data problems in the sophisticated assembling scenario,especially with dynamics information and its fine-grained labelling. The dataset first provides an integration of human,environment and robot data collection framework with 20 subjects and 30 interaction objects resulting in totally 11,664 instances of integrated actions. For each of the demonstration,hand motions,operation pressures,sounds of the assembling process,multi-view videos, high-precision motion capture information,eye gaze with first-person videos,electromyography signals are all recorded. Fine-grained multi-level annotation based on absolute timestamp,and semantic segmentation labelling are performed. Kaiwu dataset aims to facilitate robot learning,dexterous manipulation,human intention investigation and human-robot collaboration research.', 'abstract_zh': 'Kaiwu多模态数据集：针对复杂装配场景中缺失的真实同步多模态数据及其详细标注问题', 'title_zh': '激发：面向机器人学习和人机交互的多模态操作数据集与框架'}
{'arxiv_id': 'arXiv:2503.05229', 'title': 'Discrete Contrastive Learning for Diffusion Policies in Autonomous Driving', 'authors': 'Kalle Kujanpää, Daulet Baimukashev, Farzeen Munir, Shoaib Azam, Tomasz Piotr Kucner, Joni Pajarinen, Ville Kyrki', 'link': 'https://arxiv.org/abs/2503.05229', 'abstract': "Learning to perform accurate and rich simulations of human driving behaviors from data for autonomous vehicle testing remains challenging due to human driving styles' high diversity and variance. We address this challenge by proposing a novel approach that leverages contrastive learning to extract a dictionary of driving styles from pre-existing human driving data. We discretize these styles with quantization, and the styles are used to learn a conditional diffusion policy for simulating human drivers. Our empirical evaluation confirms that the behaviors generated by our approach are both safer and more human-like than those of the machine-learning-based baseline methods. We believe this has the potential to enable higher realism and more effective techniques for evaluating and improving the performance of autonomous vehicles.", 'abstract_zh': '基于对比学习从数据中提取驾驶风格词典以进行人类驾驶行为的精确和丰富模拟，以应对自动驾驶车辆测试的挑战', 'title_zh': '离散对比学习在自主驾驶中的扩散策略'}
{'arxiv_id': 'arXiv:2503.05226', 'title': 'Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments', 'authors': 'Xibai Wang', 'link': 'https://arxiv.org/abs/2503.05226', 'abstract': 'Monte Carlo Tree Search (MCTS) has emerged as a powerful tool for decision-making in robotics, enabling efficient exploration of large search spaces. However, traditional MCTS methods struggle in environments characterized by high uncertainty and noisy data due to their reliance on final-step reward evaluation. The lack of intermediate feedback during search often results in suboptimal decision-making and computational inefficiencies.\nThis paper introduces Reward-Centered ReST-MCTS, a novel framework that enhances MCTS by incorporating intermediate reward shaping. The core of our approach is the Rewarding Center, which refines search trajectories by dynamically assigning partial rewards using rule-based validation, heuristic guidance, and neural estimation. By integrating these mechanisms, our method enables real-time optimization of search paths, mitigating the effects of error propagation.\nWe evaluate Reward-Centered ReST-MCTS in robotic manipulation tasks under high uncertainty, demonstrating consistent improvements in decision accuracy. Compared to baseline methods, including Chain-of-Thought (CoT) prompting and Vanilla ReST-MCTS, our framework achieves a 2-4% accuracy improvement while maintaining computational feasibility. Ablation studies confirm the effectiveness of intermediate feedback in search refinement, particularly in pruning incorrect decision paths early. Furthermore, robustness tests show that our method retains high performance across varying levels of uncertainty.', 'abstract_zh': '基于奖励中心的ReST-MCTS：一种增强的蒙特卡洛树搜索框架', 'title_zh': '面向奖励的稳健ReST-MCTS决策框架：高不确定性环境中的机器人 manipulation 决策方法'}
{'arxiv_id': 'arXiv:2503.05189', 'title': 'Persistent Object Gaussian Splat (POGS) for Tracking Human and Robot Manipulation of Irregularly Shaped Objects', 'authors': 'Justin Yu, Kush Hari, Karim El-Refai, Arnav Dalal, Justin Kerr, Chung Min Kim, Richard Cheng, Muhammad Zubair Irshad, Ken Goldberg', 'link': 'https://arxiv.org/abs/2503.05189', 'abstract': 'Tracking and manipulating irregularly-shaped, previously unseen objects in dynamic environments is important for robotic applications in manufacturing, assembly, and logistics. Recently introduced Gaussian Splats efficiently model object geometry, but lack persistent state estimation for task-oriented manipulation. We present Persistent Object Gaussian Splat (POGS), a system that embeds semantics, self-supervised visual features, and object grouping features into a compact representation that can be continuously updated to estimate the pose of scanned objects. POGS updates object states without requiring expensive rescanning or prior CAD models of objects. After an initial multi-view scene capture and training phase, POGS uses a single stereo camera to integrate depth estimates along with self-supervised vision encoder features for object pose estimation. POGS supports grasping, reorientation, and natural language-driven manipulation by refining object pose estimates, facilitating sequential object reset operations with human-induced object perturbations and tool servoing, where robots recover tool pose despite tool perturbations of up to 30°. POGS achieves up to 12 consecutive successful object resets and recovers from 80% of in-grasp tool perturbations.', 'abstract_zh': '在动态环境中跟踪和操控未见过的不规则形状物体对于制造、组装和物流领域的机器人应用至关重要。我们提出了一种持久对象高斯点系统（POGS），该系统将语义信息、自监督视觉特征和物体分组特征嵌入到紧凑的表示中，可以连续更新以估计扫描物体的姿态。POGS在无需昂贵的重新扫描或物体的先验CAD模型的情况下更新物体状态。在初始的多视角场景捕获和训练阶段之后，POGS使用单目立体相机结合自监督视觉编码特征来估计物体姿态。通过细化物体姿态估计，POGS支持抓取、重新定向和自然语言驱动的操控，实现基于人工干预的物体重定位操作和工具伺服控制，即使在工具姿态高达30°的干扰下，机器人也能恢复工具姿态。POGS能够连续成功进行多达12次物体重定位操作，并从80%的抓持中工具干扰中恢复。', 'title_zh': '持久对象高斯体绘制（POGS）用于追踪人类和机器人操作不规则形状物体'}
{'arxiv_id': 'arXiv:2503.05180', 'title': 'Safety-Critical Traffic Simulation with Adversarial Transfer of Driving Intentions', 'authors': 'Zherui Huang, Xing Gao, Guanjie Zheng, Licheng Wen, Xuemeng Yang, Xiao Sun', 'link': 'https://arxiv.org/abs/2503.05180', 'abstract': 'Traffic simulation, complementing real-world data with a long-tail distribution, allows for effective evaluation and enhancement of the ability of autonomous vehicles to handle accident-prone scenarios. Simulating such safety-critical scenarios is nontrivial, however, from log data that are typically regular scenarios, especially in consideration of dynamic adversarial interactions between the future motions of autonomous vehicles and surrounding traffic participants. To address it, this paper proposes an innovative and efficient strategy, termed IntSim, that explicitly decouples the driving intentions of surrounding actors from their motion planning for realistic and efficient safety-critical simulation. We formulate the adversarial transfer of driving intention as an optimization problem, facilitating extensive exploration of diverse attack behaviors and efficient solution convergence. Simultaneously, intention-conditioned motion planning benefits from powerful deep models and large-scale real-world data, permitting the simulation of realistic motion behaviors for actors. Specially, through adapting driving intentions based on environments, IntSim facilitates the flexible realization of dynamic adversarial interactions with autonomous vehicles. Finally, extensive open-loop and closed-loop experiments on real-world datasets, including nuScenes and Waymo, demonstrate that the proposed IntSim achieves state-of-the-art performance in simulating realistic safety-critical scenarios and further improves planners in handling such scenarios.', 'abstract_zh': '交通仿真，通过补充长尾分布的实际数据，能够有效评估和提升自动驾驶车辆处理事故多发场景的能力。然而，从通常为常规场景的日志数据中模拟这种安全关键场景是具有挑战性的，特别是在考虑未来自动驾驶车辆及其周围交通参与者之间动态对抗性互动的情况下。为了解决这一问题，本文提出了一种创新且高效的策略，称为IntSim，该策略明确地将周围行为者的驾驶意图与他们的运动规划解耦，以实现现实且高效的安全关键仿真。我们将驾驶意图的对抗转移建模为一个优化问题，从而促进对多种攻击行为的广泛探索和高效解算收敛。同时，基于驾驶意图的运动规划得益于强大的深度模型和大规模的实际数据，使得能够模拟行为者的现实运动行为。特别地，通过根据环境调整驾驶意图，IntSim 支持灵活实现动态对抗性互动与自动驾驶车辆的互动。最后，通过在nuScenes和Waymo等现实世界数据集上的开环和闭环实验，证明所提出的IntSim 在模拟现实安全关键场景方面达到了最先进的性能，并进一步提升了规划器处理这些场景的能力。', 'title_zh': '基于敌对迁移驾驶意图的安全关键交通模拟'}
{'arxiv_id': 'arXiv:2503.05164', 'title': 'A Comprehensive LLM-powered Framework for Driving Intelligence Evaluation', 'authors': 'Shanhe You, Xuewen Luo, Xinhe Liang, Jiashu Yu, Chen Zheng, Jiangtao Gong', 'link': 'https://arxiv.org/abs/2503.05164', 'abstract': 'Evaluation methods for autonomous driving are crucial for algorithm optimization. However, due to the complexity of driving intelligence, there is currently no comprehensive evaluation method for the level of autonomous driving intelligence. In this paper, we propose an evaluation framework for driving behavior intelligence in complex traffic environments, aiming to fill this gap. We constructed a natural language evaluation dataset of human professional drivers and passengers through naturalistic driving experiments and post-driving behavior evaluation interviews. Based on this dataset, we developed an LLM-powered driving evaluation framework. The effectiveness of this framework was validated through simulated experiments in the CARLA urban traffic simulator and further corroborated by human assessment. Our research provides valuable insights for evaluating and designing more intelligent, human-like autonomous driving agents. The implementation details of the framework and detailed information about the dataset can be found at Github.', 'abstract_zh': '自主驾驶行为智能在复杂交通环境中的评估方法对于算法优化至关重要。然而，由于驾驶智能的复杂性，目前尚未有全面的自主驾驶智能评估方法。本文提出了一种用于复杂交通环境下的驾驶行为智能评估框架，旨在填补这一空白。通过自然驾驶实验和后续的行为评价访谈，我们构建了一个基于人类专业驾驶员和乘客的自然语言评价数据集。基于该数据集，我们开发了一种基于LLM的驾驶评估框架。该框架的有效性通过CARLA城市交通模拟器中的模拟实验得到验证，并通过人工评估进一步证实。我们的研究为评估和设计更具智能且类人的自主驾驶代理提供了有价值的见解。框架的实现细节和数据集的详细信息可在Github上找到。', 'title_zh': '基于大语言模型的综合智能评估框架'}
{'arxiv_id': 'arXiv:2503.05153', 'title': 'Generative Trajectory Stitching through Diffusion Composition', 'authors': 'Yunhao Luo, Utkarsh A. Mishra, Yilun Du, Danfei Xu', 'link': 'https://arxiv.org/abs/2503.05153', 'abstract': 'Effective trajectory stitching for long-horizon planning is a significant challenge in robotic decision-making. While diffusion models have shown promise in planning, they are limited to solving tasks similar to those seen in their training data. We propose CompDiffuser, a novel generative approach that can solve new tasks by learning to compositionally stitch together shorter trajectory chunks from previously seen tasks. Our key insight is modeling the trajectory distribution by subdividing it into overlapping chunks and learning their conditional relationships through a single bidirectional diffusion model. This allows information to propagate between segments during generation, ensuring physically consistent connections. We conduct experiments on benchmark tasks of various difficulties, covering different environment sizes, agent state dimension, trajectory types, training data quality, and show that CompDiffuser significantly outperforms existing methods.', 'abstract_zh': '长 horizon 规划中有效的轨迹拼接是机器人决策中的一个显著挑战。虽然扩散模型在规划方面展现了前景，但它们局限于解决与其训练数据相似的任务。我们提出 CompDiffuser，这是一种新颖的生成方法，能够通过学习将之前见过的任务中的较短轨迹片段组合起来解决新任务。我们的关键是通过将轨迹分布划分为重叠片段，并通过单一的双向扩散模型学习它们的条件关系，从而在生成过程中使信息在片段之间传播，确保物理上一致的连接。我们在涵盖不同环境规模、代理状态维度、轨迹类型、训练数据质量的各种难度基准任务上进行实验，结果显示 CompDiffuser 显著优于现有方法。', 'title_zh': '通过扩散合成的生成性轨迹拼接'}
{'arxiv_id': 'arXiv:2503.05152', 'title': 'GSplatVNM: Point-of-View Synthesis for Visual Navigation Models Using Gaussian Splatting', 'authors': 'Kohei Honda, Takeshi Ishita, Yasuhiro Yoshimura, Ryo Yonitani', 'link': 'https://arxiv.org/abs/2503.05152', 'abstract': 'This paper presents a novel approach to image-goal navigation by integrating 3D Gaussian Splatting (3DGS) with Visual Navigation Models (VNMs), a method we refer to as GSplatVNM. VNMs offer a promising paradigm for image-goal navigation by guiding a robot through a sequence of point-of-view images without requiring metrical localization or environment-specific training. However, constructing a dense and traversable sequence of target viewpoints from start to goal remains a central challenge, particularly when the available image database is sparse. To address these challenges, we propose a 3DGS-based viewpoint synthesis framework for VNMs that synthesizes intermediate viewpoints to seamlessly bridge gaps in sparse data while significantly reducing storage overhead. Experimental results in a photorealistic simulator demonstrate that our approach not only enhances navigation efficiency but also exhibits robustness under varying levels of image database sparsity.', 'abstract_zh': '基于3D Gaussian Splatting的视觉导航模型的新型图像目标导航方法：GSplatVNM', 'title_zh': 'GSplatVNM: 基于高斯插值的视角合成用于视觉导航模型'}
{'arxiv_id': 'arXiv:2503.05146', 'title': 'Unity RL Playground: A Versatile Reinforcement Learning Framework for Mobile Robots', 'authors': 'Linqi Ye, Rankun Li, Xiaowen Hu, Jiayi Li, Boyang Xing, Yan Peng, Bin Liang', 'link': 'https://arxiv.org/abs/2503.05146', 'abstract': 'This paper introduces Unity RL Playground, an open-source reinforcement learning framework built on top of Unity ML-Agents. Unity RL Playground automates the process of training mobile robots to perform various locomotion tasks such as walking, running, and jumping in simulation, with the potential for seamless transfer to real hardware. Key features include one-click training for imported robot models, universal compatibility with diverse robot configurations, multi-mode motion learning capabilities, and extreme performance testing to aid in robot design optimization and morphological evolution. The attached video can be found at this https URL and the code is coming soon.', 'abstract_zh': '本文介绍了基于Unity ML-Agents构建的开源强化学习框架Unity RL Playground。Unity RL Playground自动训练移动机器人执行各种运动任务（如行走、跑步和跳跃）的模拟过程，并具有无缝转移到实际硬件的潜力。主要特点包括一键训练导入的机器人模型、适用于多种机器人配置的通用兼容性、多模式运动学习能力和极端性能测试，以助于机器人设计优化和形态演化。有关视频链接请参见此处：https://this-url/ 代码即将发布。', 'title_zh': 'Unity RL Playground: 一种适用于移动机器人的多功能 reinforcement learning 框架'}
{'arxiv_id': 'arXiv:2503.05117', 'title': 'HyperGraph ROS: An Open-Source Robot Operating System for Hybrid Parallel Computing based on Computational HyperGraph', 'authors': 'Shufang Zhang, Jiazheng Wu, Jiacheng He, Kaiyi Wang, Shan An', 'link': 'https://arxiv.org/abs/2503.05117', 'abstract': 'This paper presents HyperGraph ROS, an open-source robot operating system that unifies intra-process, inter-process, and cross-device computation into a computational hypergraph for efficient message passing and parallel execution. In order to optimize communication, HyperGraph ROS dynamically selects the optimal communication mechanism while maintaining a consistent API. For intra-process messages, Intel-TBB Flow Graph is used with C++ pointer passing, which ensures zero memory copying and instant delivery. Meanwhile, inter-process and cross-device communication seamlessly switch to ZeroMQ. When a node receives a message from any source, it is immediately activated and scheduled for parallel execution by Intel-TBB. The computational hypergraph consists of nodes represented by TBB flow graph nodes and edges formed by TBB pointer-based connections for intra-process communication, as well as ZeroMQ links for inter-process and cross-device communication. This structure enables seamless distributed parallelism. Additionally, HyperGraph ROS provides ROS-like utilities such as a parameter server, a coordinate transformation tree, and visualization tools. Evaluation in diverse robotic scenarios demonstrates significantly higher transmission and throughput efficiency compared to ROS 2. Our work is available at this https URL.', 'abstract_zh': 'HyperGraph ROS：一种统一进程内、进程间及跨设备计算的开源机器人操作系统', 'title_zh': '基于计算超图的混合并行计算开源机器人操作系统HyperGraph ROS'}
{'arxiv_id': 'arXiv:2503.05114', 'title': 'Look Before You Leap: Using Serialized State Machine for Language Conditioned Robotic Manipulation', 'authors': 'Tong Mu, Yihao Liu, Mehran Armand', 'link': 'https://arxiv.org/abs/2503.05114', 'abstract': 'Imitation learning frameworks for robotic manipulation have drawn attention in the recent development of language model grounded robotics. However, the success of the frameworks largely depends on the coverage of the demonstration cases: When the demonstration set does not include examples of how to act in all possible situations, the action may fail and can result in cascading errors. To solve this problem, we propose a framework that uses serialized Finite State Machine (FSM) to generate demonstrations and improve the success rate in manipulation tasks requiring a long sequence of precise interactions. To validate its effectiveness, we use environmentally evolving and long-horizon puzzles that require long sequential actions. Experimental results show that our approach achieves a success rate of up to 98 in these tasks, compared to the controlled condition using existing approaches, which only had a success rate of up to 60, and, in some tasks, almost failed completely.', 'abstract_zh': '基于语言模型驱动的机器人操纵中的模仿学习框架在近期得到了关注，但这些框架的成功主要取决于示范案例的覆盖率：当示范集不包括所有可能情况下的行动示例时，行动可能失败并导致连锁错误。为了解决这个问题，我们提出了一种使用序列化的有限状态机（FSM）来生成示范并提高需要长期精确交互的操纵任务的成功率的框架。为了验证其有效性，我们使用了环境演变且具有长期展望的拼图问题，这些问题需要长期序列操作。实验结果表明，与现有方法的受控条件相比，我们的方法在这些任务中的成功率最高可达98%，而现有方法的成功率最高仅为60%，在某些任务中几乎完全失败。', 'title_zh': '未雨绸缪：利用序列化状态机进行语言 conditioning 的机器人操作'}
{'arxiv_id': 'arXiv:2503.05112', 'title': 'THE-SEAN: A Heart Rate Variation-Inspired Temporally High-Order Event-Based Visual Odometry with Self-Supervised Spiking Event Accumulation Networks', 'authors': 'Chaoran Xiong, Litao Wei, Kehui Ma, Zhen Sun, Yan Xiang, Zihan Nan, Trieu-Kien Truong, Ling Pei', 'link': 'https://arxiv.org/abs/2503.05112', 'abstract': 'Event-based visual odometry has recently gained attention for its high accuracy and real-time performance in fast-motion systems. Unlike traditional synchronous estimators that rely on constant-frequency (zero-order) triggers, event-based visual odometry can actively accumulate information to generate temporally high-order estimation triggers. However, existing methods primarily focus on adaptive event representation after estimation triggers, neglecting the decision-making process for efficient temporal triggering itself. This oversight leads to the computational redundancy and noise accumulation. In this paper, we introduce a temporally high-order event-based visual odometry with spiking event accumulation networks (THE-SEAN). To the best of our knowledge, it is the first event-based visual odometry capable of dynamically adjusting its estimation trigger decision in response to motion and environmental changes. Inspired by biological systems that regulate hormone secretion to modulate heart rate, a self-supervised spiking neural network is designed to generate estimation triggers. This spiking network extracts temporal features to produce triggers, with rewards based on block matching points and Fisher information matrix (FIM) trace acquired from the estimator itself. Finally, THE-SEAN is evaluated across several open datasets, thereby demonstrating average improvements of 13\\% in estimation accuracy, 9\\% in smoothness, and 38\\% in triggering efficiency compared to the state-of-the-art methods.', 'abstract_zh': '基于事件的视觉里程计：时空高阶事件累积网络（THE-SEAN）及其在动态调整估计触发决策方面的应用', 'title_zh': 'THE-SEAN：一种受心率变化启发的时域高阶事件基于视觉里程计及其自监督尖峰事件累积网络'}
{'arxiv_id': 'arXiv:2503.05092', 'title': 'Multi-Robot Collaboration through Reinforcement Learning and Abstract Simulation', 'authors': 'Adam Labiosa, Josiah P. Hanna', 'link': 'https://arxiv.org/abs/2503.05092', 'abstract': "Teams of people coordinate to perform complex tasks by forming abstract mental models of world and agent dynamics. The use of abstract models contrasts with much recent work in robot learning that uses a high-fidelity simulator and reinforcement learning (RL) to obtain policies for physical robots. Motivated by this difference, we investigate the extent to which so-called abstract simulators can be used for multi-agent reinforcement learning (MARL) and the resulting policies successfully deployed on teams of physical robots. An abstract simulator models the robot's target task at a high-level of abstraction and discards many details of the world that could impact optimal decision-making. Policies are trained in an abstract simulator then transferred to the physical robot by making use of separately-obtained low-level perception and motion control modules. We identify three key categories of modifications to the abstract simulator that enable policy transfer to physical robots: simulation fidelity enhancements, training optimizations and simulation stochasticity. We then run an empirical study with extensive ablations to determine the value of each modification category for enabling policy transfer in cooperative robot soccer tasks. We also compare the performance of policies produced by our method with a well-tuned non-learning-based behavior architecture from the annual RoboCup competition and find that our approach leads to a similar level of performance. Broadly we show that MARL can be use to train cooperative physical robot behaviors using highly abstract models of the world.", 'abstract_zh': '基于抽象模拟的多智能体强化学习在物理机器人团队中的应用研究', 'title_zh': '多机器人协作通过强化学习与抽象仿真'}
{'arxiv_id': 'arXiv:2503.05088', 'title': 'An End-to-End Learning-Based Multi-Sensor Fusion for Autonomous Vehicle Localization', 'authors': 'Changhong Lin, Jiarong Lin, Zhiqiang Sui, XiaoZhi Qu, Rui Wang, Kehua Sheng, Bo Zhang', 'link': 'https://arxiv.org/abs/2503.05088', 'abstract': 'Multi-sensor fusion is essential for autonomous vehicle localization, as it is capable of integrating data from various sources for enhanced accuracy and reliability. The accuracy of the integrated location and orientation depends on the precision of the uncertainty modeling. Traditional methods of uncertainty modeling typically assume a Gaussian distribution and involve manual heuristic parameter tuning. However, these methods struggle to scale effectively and address long-tail scenarios. To address these challenges, we propose a learning-based method that encodes sensor information using higher-order neural network features, thereby eliminating the need for uncertainty estimation. This method significantly eliminates the need for parameter fine-tuning by developing an end-to-end neural network that is specifically designed for multi-sensor fusion. In our experiments, we demonstrate the effectiveness of our approach in real-world autonomous driving scenarios. Results show that the proposed method outperforms existing multi-sensor fusion methods in terms of both accuracy and robustness. A video of the results can be viewed at this https URL.', 'abstract_zh': '多传感器融合对于自动驾驶车辆定位是必不可少的，因为它能够通过集成多种来源的数据来增强精度和可靠性。集成的位置和方向的准确性取决于不确定性建模的精度。传统的不确定性建模方法通常假设高斯分布，并涉及手动经验参数调整。然而，这些方法难以有效扩展并应对长尾场景。为了解决这些挑战，我们提出了一种基于学习的方法，使用高阶神经网络特征编码传感器信息，从而消除了不确定性估计的需求。该方法通过开发一个特定于多传感器融合的端到端神经网络，显著减少了参数微调的需要。在我们的实验中，我们展示了该方法在真实世界的自动驾驶场景中的有效性。结果显示，与现有的多传感器融合方法相比，所提出的方法在准确性和稳健性方面表现更优。结果视频请访问此链接：[此处链接]。', 'title_zh': '基于端到端学习的多传感器融合自主车辆定位'}
{'arxiv_id': 'arXiv:2503.05077', 'title': 'Adaptive-LIO: Enhancing Robustness and Precision through Environmental Adaptation in LiDAR Inertial Odometry', 'authors': 'Chengwei Zhao, Kun Hu, Jie Xu, Lijun Zhao, Baiwen Han, Kaidi Wu, Maoshan Tian, Shenghai Yuan', 'link': 'https://arxiv.org/abs/2503.05077', 'abstract': 'The emerging Internet of Things (IoT) applications, such as driverless cars, have a growing demand for high-precision positioning and navigation. Nowadays, LiDAR inertial odometry becomes increasingly prevalent in robotics and autonomous driving. However, many current SLAM systems lack sufficient adaptability to various scenarios. Challenges include decreased point cloud accuracy with longer frame intervals under the constant velocity assumption, coupling of erroneous IMU information when IMU saturation occurs, and decreased localization accuracy due to the use of fixed-resolution maps during indoor-outdoor scene transitions. To address these issues, we propose a loosely coupled adaptive LiDAR-Inertial-Odometry named \\textbf{Adaptive-LIO}, which incorporates adaptive segmentation to enhance mapping accuracy, adapts motion modality through IMU saturation and fault detection, and adjusts map resolution adaptively using multi-resolution voxel maps based on the distance from the LiDAR center. Our proposed method has been tested in various challenging scenarios, demonstrating the effectiveness of the improvements we introduce. The code is open-source on GitHub: \\href{this https URL}{Adaptive-LIO}.', 'abstract_zh': '新兴的物联网（IoT）应用，如无人驾驶汽车，对高精度定位和导航提出了日益增长的需求。如今，激光雷达惯性里程计（LiDAR inertial odometry）在机器人技术与自动驾驶中越来越普遍。然而，当前许多SLAM系统在应对各种场景时缺乏足够的适应性。这些挑战包括在恒定速度假设下，随着帧间隔延长导致点云精度下降；在IMU饱和时耦合错误的IMU信息；以及在室内外场景过渡期间使用固定分辨率地图导致的定位精度下降。为了解决这些问题，我们提出了一种松耦合自适应激光雷达-惯性-里程计（Loosely coupled adaptive LiDAR-Inertial-Odometry，命名为Adaptive-LIO），该方法结合了自适应分割以增强建图精度，通过IMU饱和和故障检测来适应运动模式，并使用基于LiDAR中心距离的多分辨率体素地图来自适应调整地图分辨率。我们的方法已在各种挑战场景中进行测试，展示了所提出改进措施的有效性。代码已开源在GitHub上：Adaptive-LIO。', 'title_zh': '自适应-LIO：通过环境适应提高激光雷达惯性里程计的稳健性和精度'}
{'arxiv_id': 'arXiv:2503.05064', 'title': 'Perceiving, Reasoning, Adapting: A Dual-Layer Framework for VLM-Guided Precision Robotic Manipulation', 'authors': 'Qingxuan Jia, Guoqin Tang, Zeyuan Huang, Zixuan Hao, Ning Ji, Shihang, Gang Chen', 'link': 'https://arxiv.org/abs/2503.05064', 'abstract': 'Vision-Language Models (VLMs) demonstrate remarkable potential in robotic manipulation, yet challenges persist in executing complex fine manipulation tasks with high speed and precision. While excelling at high-level planning, existing VLM methods struggle to guide robots through precise sequences of fine motor actions. To address this limitation, we introduce a progressive VLM planning algorithm that empowers robots to perform fast, precise, and error-correctable fine manipulation. Our method decomposes complex tasks into sub-actions and maintains three key data structures: task memory structure, 2D topology graphs, and 3D spatial networks, achieving high-precision spatial-semantic fusion. These three components collectively accumulate and store critical information throughout task execution, providing rich context for our task-oriented VLM interaction mechanism. This enables VLMs to dynamically adjust guidance based on real-time feedback, generating precise action plans and facilitating step-wise error correction. Experimental validation on complex assembly tasks demonstrates that our algorithm effectively guides robots to rapidly and precisely accomplish fine manipulation in challenging scenarios, significantly advancing robot intelligence for precision tasks.', 'abstract_zh': 'Vision-Language模型（VLMs）在机器人 manipulation 方面展现了巨大的潜力，但在执行复杂精细 manipulation 任务时，仍存在高速高精度执行的挑战。尽管在高层规划方面表现出色，现有的VLM方法在指导机器人进行精确的精细动作序列方面仍然力有未逮。为了解决这一局限性，我们提出了一种逐步的VLM规划算法，使机器人能够执行快速、精确且可纠正误差的精细 manipulation。我们的方法将复杂任务分解为子动作，并维持三种关键的数据结构：任务记忆结构、2D拓扑图和3D空间网络，实现高精度的空间语义融合。这三个组件在整个任务执行过程中共同积累和存储关键信息，为我们的任务导向的VLM交互机制提供丰富的上下文。这使得VLM能够根据实时反馈动态调整指导，生成精确的动作计划，并促进逐步误差纠正。在复杂装配任务上的实验验证表明，我们的算法有效指导机器人在挑战性场景中快速且精确地完成精细 manipulation，显著提升了机器人在精确任务方面的智能水平。', 'title_zh': '感知、推理、适应：基于VLM引导的精密机器人操作双层框架'}
{'arxiv_id': 'arXiv:2503.05057', 'title': 'Prismatic-Bending Transformable (PBT) Joint for a Modular, Foldable Manipulator with Enhanced Reachability and Dexterity', 'authors': 'Jianshu Zhou, Junda Huang, Boyuan Liang, Xiang Zhang, Xin Ma, Masayoshi Tomizuka', 'link': 'https://arxiv.org/abs/2503.05057', 'abstract': 'Robotic manipulators, traditionally designed with classical joint-link articulated structures, excel in industrial applications but face challenges in human-centered and general-purpose tasks requiring greater dexterity and adaptability. Addressing these limitations, we introduce the Prismatic-Bending Transformable (PBT) Joint, a novel design inspired by the scissors mechanism, enabling transformable kinematic chains. Each PBT joint module provides three degrees of freedom-bending, rotation, and elongation/contraction-allowing scalable and reconfigurable assemblies to form diverse kinematic configurations tailored to specific tasks. This innovative design surpasses conventional systems, delivering superior flexibility and performance across various applications. We present the design, modeling, and experimental validation of the PBT joint, demonstrating its integration into modular and foldable robotic arms. The PBT joint functions as a single SKU, enabling manipulators to be constructed entirely from standardized PBT joints without additional customized components. It also serves as a modular extension for existing systems, such as wrist modules, streamlining design, deployment, transportation, and maintenance. Three sizes-large, medium, and small-have been developed and integrated into robotic manipulators, highlighting their enhanced dexterity, reachability, and adaptability for manipulation tasks. This work represents a significant advancement in robotic design, offering scalable and efficient solutions for dynamic and unstructured environments.', 'abstract_zh': '普ismatic-Bending Transformable (PBT) 关节: 一种受剪刀机制启发的创新设计及其在可重构机器人 manipulator 中的应用', 'title_zh': '可变换棱柱弯曲接口（PBT接口）：一种具有增强可达性和灵活度的模块化折叠 manipulator'}
{'arxiv_id': 'arXiv:2503.05046', 'title': 'A Convex Formulation of Material Points and Rigid Bodies with GPU-Accelerated Async-Coupling for Interactive Simulation', 'authors': 'Chang Yu, Wenxin Du, Zeshun Zong, Alejandro Castro, Chenfanfu Jiang, Xuchen Han', 'link': 'https://arxiv.org/abs/2503.05046', 'abstract': 'We present a novel convex formulation that weakly couples the Material Point Method (MPM) with rigid body dynamics through frictional contact, optimized for efficient GPU parallelization. Our approach features an asynchronous time-splitting scheme to integrate MPM and rigid body dynamics under different time step sizes. We develop a globally convergent quasi-Newton solver tailored for massive parallelization, achieving up to 500x speedup over previous convex formulations without sacrificing stability. Our method enables interactive-rate simulations of robotic manipulation tasks with diverse deformable objects including granular materials and cloth, with strong convergence guarantees. We detail key implementation strategies to maximize performance and validate our approach through rigorous experiments, demonstrating superior speed, accuracy, and stability compared to state-of-the-art MPM simulators for robotics. We make our method available in the open-source robotics toolkit, Drake.', 'abstract_zh': '我们提出了一种新型凸优化公式，通过摩擦接触弱化地将物质点方法（MPM）与刚体动力学耦合，适用于高效的GPU并行化。该方法采用异步时间分裂方案，在不同的时间步长下集成MPM和刚体动力学。我们开发了一种全局收敛的拟牛顿求解器，适用于大规模并行化，比之前没有牺牲稳定性的凸优化公式快高达500倍。该方法允许以交互速率模拟具有多种变形物体（包括颗粒材料和布料）的机器人操作任务，并具有强大的收敛保证。我们详细介绍了关键实现策略以最大化性能，并通过严格的实验验证了该方法，表明与最先进的MPM模拟器相比，我们的方法在速度、准确性和稳定性方面表现出优越性。我们将在开源机器人工具包Drake中提供该方法。', 'title_zh': '基于GPU加速异步耦合的材料点与刚体凸规划表示的交互模拟'}
{'arxiv_id': 'arXiv:2503.05035', 'title': 'QuietPaw: Learning Quadrupedal Locomotion with Versatile Noise Preference Alignment', 'authors': 'Yuyou Zhang, Yihang Yao, Shiqi Liu, Yaru Niu, Changyi Lin, Yuxiang Yang, Wenhao Yu, Tingnan Zhang, Jie Tan, Ding Zhao', 'link': 'https://arxiv.org/abs/2503.05035', 'abstract': 'When operating at their full capacity, quadrupedal robots can produce loud footstep noise, which can be disruptive in human-centered environments like homes, offices, and hospitals. As a result, balancing locomotion performance with noise constraints is crucial for the successful real-world deployment of quadrupedal robots. However, achieving adaptive noise control is challenging due to (a) the trade-off between agility and noise minimization, (b) the need for generalization across diverse deployment conditions, and (c) the difficulty of effectively adjusting policies based on noise requirements. We propose QuietPaw, a framework incorporating our Conditional Noise-Constrained Policy (CNCP), a constrained learning-based algorithm that enables flexible, noise-aware locomotion by conditioning policy behavior on noise-reduction levels. We leverage value representation decomposition in the critics, disentangling state representations from condition-dependent representations and this allows a single versatile policy to generalize across noise levels without retraining while improving the Pareto trade-off between agility and noise reduction. We validate our approach in simulation and the real world, demonstrating that CNCP can effectively balance locomotion performance and noise constraints, achieving continuously adjustable noise reduction.', 'abstract_zh': '四足机器人全速运行时会产生较大的脚步噪音，这在以人类为中心的环境，如家庭、办公室和医院中可能会造成干扰。因此，在噪声限制条件下平衡四足机器人运动性能对于其实用部署至关重要。但由于（a）敏捷性与噪声最小化之间的权衡，（b）在多样化部署条件下的一般化需求，以及（c）基于噪声要求有效调整策略的难度，实现适应性的噪声控制具有挑战性。我们提出了QuietPaw框架，该框架包含我们的条件噪声约束策略（CNCP），这是一种基于约束学习的算法，可通过根据噪声减少水平调整策略行为，实现灵活且噪声感知的运动。我们利用评论者的价值表示分解，将状态表示与条件依赖表示分离，使得单个通用策略可以在无需重新训练的情况下泛化到不同的噪声水平，并提高敏捷性与噪声减少之间的帕累托权衡。我们在仿真和实际环境中验证了这种方法，证明CNCP能够有效平衡运动性能和噪声约束，实现连续可调的噪声减少。', 'title_zh': 'QuietPaw: 学习多用途噪声偏好对齐的四足运动控制'}
{'arxiv_id': 'arXiv:2503.05026', 'title': 'Ergodic Exploration over Meshable Surfaces', 'authors': 'Dayi Dong, Albert Xu, Geordan Gutow, Howie Choset, Ian Abraham', 'link': 'https://arxiv.org/abs/2503.05026', 'abstract': 'Robotic search and rescue, exploration, and inspection require trajectory planning across a variety of domains. A popular approach to trajectory planning for these types of missions is ergodic search, which biases a trajectory to spend time in parts of the exploration domain that are believed to contain more information. Most prior work on ergodic search has been limited to searching simple surfaces, like a 2D Euclidean plane or a sphere, as they rely on projecting functions defined on the exploration domain onto analytically obtained Fourier basis functions. In this paper, we extend ergodic search to any surface that can be approximated by a triangle mesh. The basis functions are approximated through finite element methods on a triangle mesh of the domain. We formally prove that this approximation converges to the continuous case as the mesh approximation converges to the true domain. We demonstrate that on domains where analytical basis functions are available (plane, sphere), the proposed method obtains equivalent results, and while on other domains (torus, bunny, wind turbine), the approach is versatile enough to still search effectively. Lastly, we also compare with an existing ergodic search technique that can handle complex domains and show that our method results in a higher quality exploration.', 'abstract_zh': '机器人搜索与救援、探索和检查任务需要在多种领域进行路径规划。本文将遍历搜索方法扩展到可以由三角网格近似的任何曲面。基函数通过领域三角网格上的有限元方法进行近似。我们正式证明，在网格逼近趋向真领域时，该近似收敛到连续情况。我们在可获取解析基函数的领域（平面、球面）上验证了所提方法获得等效结果，并在其他领域（环面、兔形模型、风力涡轮机）上展示了方法的灵活性和有效性。最后，我们将我们的方法与一种现有的适用于复杂领域的遍历搜索技术进行了比较，结果表明我们的方法在探索质量上更具优越性。', 'title_zh': '可遍历网格化曲面的探索'}
{'arxiv_id': 'arXiv:2503.05020', 'title': 'GRIP: A General Robotic Incremental Potential Contact Simulation Dataset for Unified Deformable-Rigid Coupled Grasping', 'authors': 'Siyu Ma, Wenxin Du, Chang Yu, Ying Jiang, Zeshun Zong, Tianyi Xie, Yunuo Chen, Yin Yang, Xuchen Han, Chenfanfu Jiang', 'link': 'https://arxiv.org/abs/2503.05020', 'abstract': 'Grasping is fundamental to robotic manipulation, and recent advances in large-scale grasping datasets have provided essential training data and evaluation benchmarks, accelerating the development of learning-based methods for robust object grasping. However, most existing datasets exclude deformable bodies due to the lack of scalable, robust simulation pipelines, limiting the development of generalizable models for compliant grippers and soft manipulands. To address these challenges, we present GRIP, a General Robotic Incremental Potential contact simulation dataset for universal grasping. GRIP leverages an optimized Incremental Potential Contact (IPC)-based simulator for multi-environment data generation, achieving up to 48x speedup while ensuring efficient, intersection- and inversion-free simulations for compliant grippers and deformable objects. Our fully automated pipeline generates and evaluates diverse grasp interactions across 1,200 objects and 100,000 grasp poses, incorporating both soft and rigid grippers. The GRIP dataset enables applications such as neural grasp generation and stress field prediction.', 'abstract_zh': 'GRIP：通用机器人增量潜力接触仿真数据集及其在通用抓取中的应用', 'title_zh': 'GRIP: 一种通用的机器人增量潜在接触模拟数据集，用于统一的可变形-刚性耦合抓取'}
{'arxiv_id': 'arXiv:2503.04998', 'title': 'Multi-Agent Ergodic Exploration under Smoke-Based, Time-Varying Sensor Visibility Constraints', 'authors': 'Elena Wittemyer, Ananya Rao, Ian Abraham, Howie Choset', 'link': 'https://arxiv.org/abs/2503.04998', 'abstract': 'In this work, we consider the problem of multi-agent informative path planning (IPP) for robots whose sensor visibility continuously changes as a consequence of a time-varying natural phenomenon. We leverage ergodic trajectory optimization (ETO), which generates paths such that the amount of time an agent spends in an area is proportional to the expected information in that area. We focus specifically on the problem of multi-agent drone search of a wildfire, where we use the time-varying environmental process of smoke diffusion to construct a sensor visibility model. This sensor visibility model is used to repeatedly calculate an expected information distribution (EID) to be used in the ETO algorithm. Our experiments show that our exploration method achieves improved information gathering over both baseline search methods and naive ergodic search formulations.', 'abstract_zh': '本研究考虑了传感器可视性因时间 varying 自然现象而连续变化的机器人多智能体信息路径规划问题。我们利用遍历轨迹优化（ETO），生成路径使得智能体在某一区域停留的时间与其预期信息量成正比。我们具体关注野火多智能体无人机搜索问题，利用烟雾扩散的时间 varying 环境过程构建传感器可视性模型。该传感器可视性模型用于反复计算预期信息分布（EID），并应用于ETO算法。实验表明，我们的探索方法在信息收集方面优于基准搜索方法和简单的遍历搜索形式。', 'title_zh': '基于烟雾引起的时间 varying 传感器可见性约束的多智能体遍历探索'}
{'arxiv_id': 'arXiv:2503.04994', 'title': 'Quantifying and Modeling Driving Styles in Trajectory Forecasting', 'authors': 'Laura Zheng, Hamidreza Yaghoubi Araghi, Tony Wu, Sandeep Thalapanane, Tianyi Zhou, Ming C. Lin', 'link': 'https://arxiv.org/abs/2503.04994', 'abstract': 'Trajectory forecasting has become a popular deep learning task due to its relevance for scenario simulation for autonomous driving. Specifically, trajectory forecasting predicts the trajectory of a short-horizon future for specific human drivers in a particular traffic scenario. Robust and accurate future predictions can enable autonomous driving planners to optimize for low-risk and predictable outcomes for human drivers around them. Although some work has been done to model driving style in planning and personalized autonomous polices, a gap exists in explicitly modeling human driving styles for trajectory forecasting of human behavior. Human driving style is most certainly a correlating factor to decision making, especially in edge-case scenarios where risk is nontrivial, as justified by the large amount of traffic psychology literature on risky driving. So far, the current real-world datasets for trajectory forecasting lack insight on the variety of represented driving styles. While the datasets may represent real-world distributions of driving styles, we posit that fringe driving style types may also be correlated with edge-case safety scenarios. In this work, we conduct analyses on existing real-world trajectory datasets for driving and dissect these works from the lens of driving styles, which is often intangible and non-standardized.', 'abstract_zh': '轨迹预测已成为自动驾驶场景模拟相关的流行深度学习任务，尽管在规划和个性化自主政策建模中已经开展了一些工作，但在轨迹预测中显式建模人类驾驶风格仍存在差距。迄今为止，现有的真实世界轨迹预测数据集缺乏对驾驶风格多样性的见解。尽管数据集可能代表了真实世界的驾驶风格分布，我们认为边缘驾驶风格类型也可能与边缘情况下的安全场景相关。本文从驾驶风格的角度分析现有的真实世界轨迹数据集，并从常难以量化且非标准化的驾驶风格视角剖析这些工作。', 'title_zh': '量化与建模轨迹预测中的驾驶风格'}
{'arxiv_id': 'arXiv:2503.04969', 'title': 'Data-Efficient Learning from Human Interventions for Mobile Robots', 'authors': 'Zhenghao Peng, Zhizheng Liu, Bolei Zhou', 'link': 'https://arxiv.org/abs/2503.04969', 'abstract': 'Mobile robots are essential in applications such as autonomous delivery and hospitality services. Applying learning-based methods to address mobile robot tasks has gained popularity due to its robustness and generalizability. Traditional methods such as Imitation Learning (IL) and Reinforcement Learning (RL) offer adaptability but require large datasets, carefully crafted reward functions, and face sim-to-real gaps, making them challenging for efficient and safe real-world deployment. We propose an online human-in-the-loop learning method PVP4Real that combines IL and RL to address these issues. PVP4Real enables efficient real-time policy learning from online human intervention and demonstration, without reward or any pretraining, significantly improving data efficiency and training safety. We validate our method by training two different robots -- a legged quadruped, and a wheeled delivery robot -- in two mobile robot tasks, one of which even uses raw RGBD image as observation. The training finishes within 15 minutes. Our experiments show the promising future of human-in-the-loop learning in addressing the data efficiency issue in real-world robotic tasks. More information is available at: this https URL', 'abstract_zh': '基于在线人机交互的学习方法PVP4Real在移动机器人任务中的应用', 'title_zh': '基于人类干预的数据高效学习方法在移动机器人中的应用'}
{'arxiv_id': 'arXiv:2503.04944', 'title': 'MarsLGPR: Mars Rover Localization with Ground Penetrating Radar', 'authors': 'Anja Sheppard, Katherine A. Skinner', 'link': 'https://arxiv.org/abs/2503.04944', 'abstract': 'In this work, we propose the use of Ground Penetrating Radar (GPR) for rover localization on Mars. Precise pose estimation is an important task for mobile robots exploring planetary surfaces, as they operate in GPS-denied environments. Although visual odometry provides accurate localization, it is computationally expensive and can fail in dim or high-contrast lighting. Wheel encoders can also provide odometry estimation, but are prone to slipping on the sandy terrain encountered on Mars. Although traditionally a scientific surveying sensor, GPR has been used on Earth for terrain classification and localization through subsurface feature matching. The Perseverance rover and the upcoming ExoMars rover have GPR sensors already equipped to aid in the search of water and mineral resources. We propose to leverage GPR to aid in Mars rover localization. Specifically, we develop a novel GPR-based deep learning model that predicts 1D relative pose translation. We fuse our GPR pose prediction method with inertial and wheel encoder data in a filtering framework to output rover localization. We perform experiments in a Mars analog environment and demonstrate that our GPR-based displacement predictions both outperform wheel encoders and improve multi-modal filtering estimates in high-slip environments. Lastly, we present the first dataset aimed at GPR-based localization in Mars analog environments, which will be made publicly available upon publication.', 'abstract_zh': '基于地面穿透雷达的火星探测车定位方法研究', 'title_zh': '火星LGPR：火星车地面穿透雷达定位'}
{'arxiv_id': 'arXiv:2503.04942', 'title': 'SAFE-TAXI: A Hierarchical Multi-UAS Safe Auto-Taxiing Framework with Runtime Safety Assurance and Conflict Resolution', 'authors': 'Kartik A. Pant, Li-Yu Lin, Worawis Sribunma, Sabine Brunswicker, James M. Goppert, Inseok Hwang', 'link': 'https://arxiv.org/abs/2503.04942', 'abstract': 'We present a hierarchical safe auto-taxiing framework to enhance the automated ground operations of multiple unmanned aircraft systems (multi-UAS). The auto-taxiing problem becomes particularly challenging due to (i) unknown disturbances, such as crosswind affecting the aircraft dynamics, (ii) taxiway incursions due to unplanned obstacles, and (iii) spatiotemporal conflicts at the intersections between multiple entry points in the taxiway. To address these issues, we propose a hierarchical framework, i.e., SAFE-TAXI, combining centralized spatiotemporal planning with decentralized MPC-CBF-based control to safely navigate the aircraft through the taxiway while avoiding intersection conflicts and unplanned obstacles (e.g., other aircraft or ground vehicles). Our proposed framework decouples the auto-taxiing problem temporally into conflict resolution and motion planning, respectively. Conflict resolution is handled in a centralized manner by computing conflict-aware reference trajectories for each aircraft. In contrast, safety assurance from unplanned obstacles is handled by an MPC-CBF-based controller implemented in a decentralized manner. We demonstrate the effectiveness of our proposed framework through numerical simulations and experimentally validate it using Night Vapor, a small-scale fixed-wing test platform.', 'abstract_zh': '一种分层安全自主滑行框架：增强多无人机系统（multi-UAS）的自主地面操作', 'title_zh': 'SAFE-TAXI：一种具有运行时安全保证和冲突解决的分层多无人飞行器自动 Taxiing 框架'}
{'arxiv_id': 'arXiv:2503.04933', 'title': 'Learning-based GNSS Uncertainty Quantification using Continuous-Time Factor Graph Optimization', 'authors': 'Haoming Zhang', 'link': 'https://arxiv.org/abs/2503.04933', 'abstract': 'This short paper presents research findings on two learning-based methods for quantifying measurement uncertainties in global navigation satellite systems (GNSS). We investigate two learning strategies: offline learning for outlier prediction and online learning for noise distribution approximation, specifically applied to GNSS pseudorange observations. To develop and evaluate these learning methods, we introduce a novel multisensor state estimator that accurately and robustly estimates trajectory from multiple sensor inputs, critical for deriving GNSS measurement residuals used to train the uncertainty models. We validate the proposed learning-based models using real-world sensor data collected in diverse urban environments. Experimental results demonstrate that both models effectively handle GNSS outliers and improve state estimation performance. Furthermore, we provide insightful discussions to motivate future research toward developing a federated framework for robust vehicle localization in challenging environments.', 'abstract_zh': '基于学习的方法在全球导航卫星系统中量化测量不确定性的研究', 'title_zh': '基于学习的连续时间因子图优化GNSS不确定性量化'}
{'arxiv_id': 'arXiv:2503.04931', 'title': 'Curiosity-Driven Imagination: Discovering Plan Operators and Learning Associated Policies for Open-World Adaptation', 'authors': 'Pierrick Lorang, Hong Lu, Matthias Scheutz', 'link': 'https://arxiv.org/abs/2503.04931', 'abstract': 'Adapting quickly to dynamic, uncertain environments-often called "open worlds"-remains a major challenge in robotics. Traditional Task and Motion Planning (TAMP) approaches struggle to cope with unforeseen changes, are data-inefficient when adapting, and do not leverage world models during learning. We address this issue with a hybrid planning and learning system that integrates two models: a low level neural network based model that learns stochastic transitions and drives exploration via an Intrinsic Curiosity Module (ICM), and a high level symbolic planning model that captures abstract transitions using operators, enabling the agent to plan in an "imaginary" space and generate reward machines. Our evaluation in a robotic manipulation domain with sequential novelty injections demonstrates that our approach converges faster and outperforms state-of-the-art hybrid methods.', 'abstract_zh': '快速适应动态不确定性环境——通常称为“开放世界”——仍然是机器人领域的一项主要挑战。传统的任务与运动规划（TAMP）方法难以应对未预见的变化，适应过程数据利用率低，并且在学习过程中不利用世界模型。我们通过一个结合了两种模型的混合规划与学习系统来解决这一问题：一种基于低层神经网络的学习随机过渡模型，并通过内在好奇心模块（ICM）驱动探索；另一种高层符号规划模型利用操作符捕捉抽象过渡，使代理能够在“想象”空间中规划并生成奖励自动机。在带有序列新颖性注入的机器人操作域中的评估表明，我们的方法收敛速度更快，并超越了最先进的混合方法。', 'title_zh': '好奇驱动的想象：发现开放世界适应的计划操作及其关联策略'}
{'arxiv_id': 'arXiv:2503.04929', 'title': 'Neural Configuration-Space Barriers for Manipulation Planning and Control', 'authors': 'Kehan Long, Ki Myung Brian Lee, Nikola Raicevic, Niyas Attasseri, Melvin Leok, Nikolay Atanasov', 'link': 'https://arxiv.org/abs/2503.04929', 'abstract': 'Planning and control for high-dimensional robot manipulators in cluttered, dynamic environments require both computational efficiency and robust safety guarantees. Inspired by recent advances in learning configuration-space distance functions (CDFs) as robot body representations, we propose a unified framework for motion planning and control that formulates safety constraints as CDF barriers. A CDF barrier approximates the local free configuration space, substantially reducing the number of collision-checking operations during motion planning. However, learning a CDF barrier with a neural network and relying on online sensor observations introduce uncertainties that must be considered during control synthesis. To address this, we develop a distributionally robust CDF barrier formulation for control that explicitly accounts for modeling errors and sensor noise without assuming a known underlying distribution. Simulations and hardware experiments on a 6-DoF xArm manipulator show that our neural CDF barrier formulation enables efficient planning and robust real-time safe control in cluttered and dynamic environments, relying only on onboard point-cloud observations.', 'abstract_zh': '高维度机器人 manipulator 在复杂动态环境中的规划与控制需要高效性和稳健的安全保证。受学习配置空间距离函数(CDF)作为机器人体表征的recent进展启发，我们提出了一种统一框架，将安全约束形式化为CDF拦阻器。CDF拦阻器近似局部自由配置空间，大幅减少了运动规划中的碰撞检测操作。然而，使用神经网络学习CDF拦阻器并依赖于在线传感器观测引入了不确定性，必须在控制综合时予以考虑。为此，我们开发了一种分布鲁棒的CDF拦阻器形式化方法，在不假设已知底层分布的情况下，显式考虑建模误差和传感器噪声。仿真和基于xArm六自由度 manipulator 的硬件实验表明，我们的神经CDF拦阻器形式化方法能够在复杂动态环境中实现高效的规划和实时稳健的安全控制，仅依赖于板载点云观测。', 'title_zh': '基于神经网络的配置空间障碍物在操作规划与控制中的应用'}
{'arxiv_id': 'arXiv:2503.04879', 'title': 'Modeling Dynamic Hand-Object Interactions with Applications to Human-Robot Handovers', 'authors': 'Sammy Christen', 'link': 'https://arxiv.org/abs/2503.04879', 'abstract': 'Humans frequently grasp, manipulate, and move objects. Interactive systems assist humans in these tasks, enabling applications in Embodied AI, human-robot interaction, and virtual reality. However, current methods in hand-object synthesis often neglect dynamics and focus on generating static grasps. The first part of this dissertation introduces dynamic grasp synthesis, where a hand grasps and moves an object to a target pose. We approach this task using physical simulation and reinforcement learning. We then extend this to bimanual manipulation and articulated objects, requiring fine-grained coordination between hands. In the second part of this dissertation, we study human-to-robot handovers. We integrate captured human motion into simulation and introduce a student-teacher framework that adapts to human behavior and transfers from sim to real. To overcome data scarcity, we generate synthetic interactions, increasing training diversity by 100x. Our user study finds no difference between policies trained on synthetic vs. real motions.', 'abstract_zh': '人类频繁地抓握、操作和移动物体。交互系统协助人类完成这些任务，推动了具身AI、人机交互和虚拟现实等应用的发展。然而，当前的手物合成方法往往忽视了动力学效果，专注于生成静态抓握。本文的第一部分介绍了动态抓握合成方法，该方法涉及手抓住并移动物体到达目标姿态。我们采用了物理仿真和强化学习来完成这项任务。随后，我们将这种方法扩展到双臂操作和有连杆的物体，需要双手之间细致的协调。本文的第二部分研究了人向机器人传递手部动作的问题。我们整合了捕捉到的人类动作到仿真中，并引入了一种学生-教师框架，该框架能够适应人类行为并在仿真到现实场景之间进行转移。为了解决数据稀缺问题，我们生成了合成交互，将训练多样性提高了100倍。我们的用户研究发现，基于合成动作为训练政策与基于真实动作训练政策之间没有差异。', 'title_zh': '基于动态手-物体交互建模及其在人-机器人交接中的应用'}
{'arxiv_id': 'arXiv:2503.04803', 'title': 'An energy-efficient learning solution for the Agile Earth Observation Satellite Scheduling Problem', 'authors': 'Antonio M. Mercado-Martínez, Beatriz Soret, Antonio Jurado-Navas', 'link': 'https://arxiv.org/abs/2503.04803', 'abstract': "The Agile Earth Observation Satellite Scheduling Problem (AEOSSP) entails finding the subset of observation targets to be scheduled along the satellite's orbit while meeting operational constraints of time, energy and memory. The problem of deciding what and when to observe is inherently complex, and becomes even more challenging when considering several issues that compromise the quality of the captured images, such as cloud occlusion, atmospheric turbulence, and image resolution. This paper presents a Deep Reinforcement Learning (DRL) approach for addressing the AEOSSP with time-dependent profits, integrating these three factors to optimize the use of energy and memory resources. The proposed method involves a dual decision-making process: selecting the sequence of targets and determining the optimal observation time for each. Our results demonstrate that the proposed algorithm reduces the capture of images that fail to meet quality requirements by > 60% and consequently decreases energy waste from attitude maneuvers by up to 78%, all while maintaining strong observation performance.", 'abstract_zh': '敏捷地球观测卫星调度问题(AEOSSP)涉及在满足时间、能量和内存运营约束条件下确定要调度的目标子集。决定观察什么和何时观察的问题本身就很复杂，当考虑影响捕获图像质量的因素时，如云遮挡、大气湍流和图像分辨率，问题变得更为棘手。本文提出了一种基于深度强化学习(DRL)的方法来解决具有时变收益的AEOSSP，综合考虑这三个因素以优化能量和内存资源的使用。所提出的方法涉及双重决策过程：选择目标序列并确定每个目标的最佳观测时间。我们的结果显示，所提出的算法通过降低约60%无法满足质量要求的图像捕获比例，从而最多减少78%的姿态机动能耗，同时保持强大的观测性能。', 'title_zh': '敏捷地球观测卫星调度问题的能效学习解决方案'}
{'arxiv_id': 'arXiv:2503.04798', 'title': 'Advancing MAPF towards the Real World: A Scalable Multi-Agent Realistic Testbed (SMART)', 'authors': 'Jingtian Yan, Zhifei Li, William Kang, Yulun Zhang, Stephen Smith, Jiaoyang Li', 'link': 'https://arxiv.org/abs/2503.04798', 'abstract': 'We present Scalable Multi-Agent Realistic Testbed (SMART), a realistic and efficient software tool for evaluating Multi-Agent Path Finding (MAPF) algorithms. MAPF focuses on planning collision-free paths for a group of agents. While state-of-the-art MAPF algorithms can plan paths for hundreds of robots in seconds, they often rely on simplified robot models, making their real-world performance unclear. Researchers typically lack access to hundreds of physical robots in laboratory settings to evaluate the algorithms. Meanwhile, industrial professionals who lack expertise in MAPF require an easy-to-use simulator to efficiently test and understand the performance of MAPF algorithms in their specific settings. SMART fills this gap with several advantages: (1) SMART uses a physics-engine-based simulator to create realistic simulation environments, accounting for complex real-world factors such as robot kinodynamics and execution uncertainties, (2) SMART uses an execution monitor framework based on the Action Dependency Graph, facilitating seamless integration with various MAPF algorithms and robot models, and (3) SMART scales to thousands of robots. In addition, we use SMART to explore and demonstrate research questions about the execution of MAPF algorithms in real-world scenarios. The code is publicly available at this https URL.', 'abstract_zh': '一种面向多agent路径规划算法评估的可扩展多agent现实测试床（SMART）', 'title_zh': '面向现实世界的MAPF问题进展：大规模多 Agents 现实测试床（SMART）'}
{'arxiv_id': 'arXiv:2503.04794', 'title': 'Runtime Learning of Quadruped Robots in Wild Environments', 'authors': 'Yihao Cai, Yanbing Mao, Lui Sha, Hongpeng Cao, Marco Caccamo', 'link': 'https://arxiv.org/abs/2503.04794', 'abstract': "This paper presents a runtime learning framework for quadruped robots, enabling them to learn and adapt safely in dynamic wild environments. The framework integrates sensing, navigation, and control, forming a closed-loop system for the robot. The core novelty of this framework lies in two interactive and complementary components within the control module: the high-performance (HP)-Student and the high-assurance (HA)-Teacher. HP-Student is a deep reinforcement learning (DRL) agent that engages in self-learning and teaching-to-learn to develop a safe and high-performance action policy. HA-Teacher is a simplified yet verifiable physics-model-based controller, with the role of teaching HP-Student about safety while providing a backup for the robot's safe locomotion. HA-Teacher is innovative due to its real-time physics model, real-time action policy, and real-time control goals, all tailored to respond effectively to real-time wild environments, ensuring safety. The framework also includes a coordinator who effectively manages the interaction between HP-Student and HA-Teacher. Experiments involving a Unitree Go2 robot in Nvidia Isaac Gym and comparisons with state-of-the-art safe DRLs demonstrate the effectiveness of the proposed runtime learning framework.", 'abstract_zh': '一种用于四足机器人的运行时学习框架：安全适应动态野外地形的能力', 'title_zh': '野生环境中四足机器人运行时学习'}
{'arxiv_id': 'arXiv:2503.05696', 'title': 'Multi-Fidelity Policy Gradient Algorithms', 'authors': 'Xinjie Liu, Cyrus Neary, Kushagra Gupta, Christian Ellis, Ufuk Topcu, David Fridovich-Keil', 'link': 'https://arxiv.org/abs/2503.05696', 'abstract': 'Many reinforcement learning (RL) algorithms require large amounts of data, prohibiting their use in applications where frequent interactions with operational systems are infeasible, or high-fidelity simulations are expensive or unavailable. Meanwhile, low-fidelity simulators--such as reduced-order models, heuristic reward functions, or generative world models--can cheaply provide useful data for RL training, even if they are too coarse for direct sim-to-real transfer. We propose multi-fidelity policy gradients (MFPGs), an RL framework that mixes a small amount of data from the target environment with a large volume of low-fidelity simulation data to form unbiased, reduced-variance estimators (control variates) for on-policy policy gradients. We instantiate the framework by developing multi-fidelity variants of two policy gradient algorithms: REINFORCE and proximal policy optimization. Experimental results across a suite of simulated robotics benchmark problems demonstrate that when target-environment samples are limited, MFPG achieves up to 3.9x higher reward and improves training stability when compared to baselines that only use high-fidelity data. Moreover, even when the baselines are given more high-fidelity samples--up to 10x as many interactions with the target environment--MFPG continues to match or outperform them. Finally, we observe that MFPG is capable of training effective policies even when the low-fidelity environment is drastically different from the target environment. MFPG thus not only offers a novel paradigm for efficient sim-to-real transfer but also provides a principled approach to managing the trade-off between policy performance and data collection costs.', 'abstract_zh': '多保真度策略梯度算法（MFPG）：一种混合目标环境和低保真度模拟数据的强化学习框架', 'title_zh': '多保真度策略梯度算法'}
{'arxiv_id': 'arXiv:2503.05578', 'title': 'Novel Object 6D Pose Estimation with a Single Reference View', 'authors': 'Jian Liu, Wei Sun, Kai Zeng, Jin Zheng, Hui Yang, Lin Wang, Hossein Rahmani, Ajmal Mian', 'link': 'https://arxiv.org/abs/2503.05578', 'abstract': 'Existing novel object 6D pose estimation methods typically rely on CAD models or dense reference views, which are both difficult to acquire. Using only a single reference view is more scalable, but challenging due to large pose discrepancies and limited geometric and spatial information. To address these issues, we propose a Single-Reference-based novel object 6D (SinRef-6D) pose estimation method. Our key idea is to iteratively establish point-wise alignment in the camera coordinate system based on state space models (SSMs). Specifically, iterative camera-space point-wise alignment can effectively handle large pose discrepancies, while our proposed RGB and Points SSMs can capture long-range dependencies and spatial information from a single view, offering linear complexity and superior spatial modeling capability. Once pre-trained on synthetic data, SinRef-6D can estimate the 6D pose of a novel object using only a single reference view, without requiring retraining or a CAD model. Extensive experiments on six popular datasets and real-world robotic scenes demonstrate that we achieve on-par performance with CAD-based and dense reference view-based methods, despite operating in the more challenging single reference setting. Code will be released at this https URL.', 'abstract_zh': '基于单参考视图的新型对象6D姿态估计方法（SinRef-6D）', 'title_zh': '基于单参考视角的新型对象6D姿态估计'}
{'arxiv_id': 'arXiv:2503.05540', 'title': 'Riemann$^2$: Learning Riemannian Submanifolds from Riemannian Data', 'authors': 'Leonel Rozo, Miguel González-Duque, Noémie Jaquier, Søren Hauberg', 'link': 'https://arxiv.org/abs/2503.05540', 'abstract': 'Latent variable models are powerful tools for learning low-dimensional manifolds from high-dimensional data. However, when dealing with constrained data such as unit-norm vectors or symmetric positive-definite matrices, existing approaches ignore the underlying geometric constraints or fail to provide meaningful metrics in the latent space. To address these limitations, we propose to learn Riemannian latent representations of such geometric data. To do so, we estimate the pullback metric induced by a Wrapped Gaussian Process Latent Variable Model, which explicitly accounts for the data geometry. This enables us to define geometry-aware notions of distance and shortest paths in the latent space, while ensuring that our model only assigns probability mass to the data manifold. This generalizes previous work and allows us to handle complex tasks in various domains, including robot motion synthesis and analysis of brain connectomes.', 'abstract_zh': '潜变量模型是学习高维数据低维流形的强大工具。然而，在处理如单位范数向量或对称正定矩阵等受限数据时，现有方法要么忽略了潜在的数据几何约束，要么无法在潜在空间中提供有意义的度量。为了克服这些限制，我们提出学习此类几何数据的黎曼潜表示。为此，我们估计由包裹高斯过程潜变量模型诱导的拉普拉斯度量，明确考虑了数据几何。这使得我们能够在潜空间中定义几何感知的距离和最短路径，并确保我们的模型仅将概率质量分配给数据流形。这扩展了之前的工作，并允许我们在包括机器人运动合成和脑连接组分析在内的各种领域处理复杂任务。', 'title_zh': '黎曼²：从黎曼数据学习黎曼子流形'}
{'arxiv_id': 'arXiv:2503.05174', 'title': 'SplatPose: Geometry-Aware 6-DoF Pose Estimation from Single RGB Image via 3D Gaussian Splatting', 'authors': 'Linqi Yang, Xiongwei Zhao, Qihao Sun, Ke Wang, Ao Chen, Peng Kang', 'link': 'https://arxiv.org/abs/2503.05174', 'abstract': '6-DoF pose estimation is a fundamental task in computer vision with wide-ranging applications in augmented reality and robotics. Existing single RGB-based methods often compromise accuracy due to their reliance on initial pose estimates and susceptibility to rotational ambiguity, while approaches requiring depth sensors or multi-view setups incur significant deployment costs. To address these limitations, we introduce SplatPose, a novel framework that synergizes 3D Gaussian Splatting (3DGS) with a dual-branch neural architecture to achieve high-precision pose estimation using only a single RGB image. Central to our approach is the Dual-Attention Ray Scoring Network (DARS-Net), which innovatively decouples positional and angular alignment through geometry-domain attention mechanisms, explicitly modeling directional dependencies to mitigate rotational ambiguity. Additionally, a coarse-to-fine optimization pipeline progressively refines pose estimates by aligning dense 2D features between query images and 3DGS-synthesized views, effectively correcting feature misalignment and depth errors from sparse ray sampling. Experiments on three benchmark datasets demonstrate that SplatPose achieves state-of-the-art 6-DoF pose estimation accuracy in single RGB settings, rivaling approaches that depend on depth or multi-view images.', 'abstract_zh': '6-DoF 姿态估计是计算机视觉中的一个基本任务，广泛应用于增强现实和机器人领域。现有的单RGB方法常常由于依赖初始姿态估计和易受旋转歧义性的影响而牺牲精度，而需要深度传感器或多视图设置的方法则会带来显著的部署成本。为了解决这些问题，我们引入了 SplatPose，这是一种新颖的框架，将3D 高斯点积（3DGS）与双分支神经架构相结合，仅使用单张RGB图像即可实现高精度的姿态估计。我们方法的核心是双注意射线评分网络（DARS-Net），这是一种创新的方法，通过几何域注意机制解耦位置和角度对齐，明确建模方向依赖性以减轻旋转歧义性。此外，从粗到细的优化pipeline逐步通过查询图像和3DGS合成视图之间的密集2D特征对齐，有效地纠正了稀疏射线采样引起的特征错位和深度误差。在三个基准数据集上的实验表明，SplatPose 在单RGB设置下的6-DoF姿态估计精度达到了最先进的水平，与依赖深度图像或多视图图像的方法不相上下。', 'title_zh': 'SplatPose：基于几何 aware 的单张 RGB 图像六自由度姿态估计方法通过 3D 高斯绘制'}
{'arxiv_id': 'arXiv:2503.04952', 'title': 'INTENT: Trajectory Prediction Framework with Intention-Guided Contrastive Clustering', 'authors': 'Yihong Tang, Wei Ma', 'link': 'https://arxiv.org/abs/2503.04952', 'abstract': 'Accurate trajectory prediction of road agents (e.g., pedestrians, vehicles) is an essential prerequisite for various intelligent systems applications, such as autonomous driving and robotic navigation. Recent research highlights the importance of environmental contexts (e.g., maps) and the "multi-modality" of trajectories, leading to increasingly complex model structures. However, real-world deployments require lightweight models that can quickly migrate and adapt to new environments. Additionally, the core motivations of road agents, referred to as their intentions, deserves further exploration. In this study, we advocate that understanding and reasoning road agents\' intention plays a key role in trajectory prediction tasks, and the main challenge is that the concept of intention is fuzzy and abstract. To this end, we present INTENT, an efficient intention-guided trajectory prediction model that relies solely on information contained in the road agent\'s trajectory. Our model distinguishes itself from existing models in several key aspects: (i) We explicitly model road agents\' intentions through contrastive clustering, accommodating the fuzziness and abstraction of human intention in their trajectories. (ii) The proposed INTENT is based solely on multi-layer perceptrons (MLPs), resulting in reduced training and inference time, making it very efficient and more suitable for real-world deployment. (iii) By leveraging estimated intentions and an innovative algorithm for transforming trajectory observations, we obtain more robust trajectory representations that lead to superior prediction accuracy. Extensive experiments on real-world trajectory datasets for pedestrians and autonomous vehicles demonstrate the effectiveness and efficiency of INTENT.', 'abstract_zh': '基于轨迹的意图引导的道路代理准确轨迹预测', 'title_zh': '意图引导对比聚类的轨迹预测框架'}
{'arxiv_id': 'arXiv:2503.04877', 'title': 'Adapt3R: Adaptive 3D Scene Representation for Domain Transfer in Imitation Learning', 'authors': 'Albert Wilcox, Mohamed Ghanem, Masoud Moghani, Pierre Barroso, Benjamin Joffe, Animesh Garg', 'link': 'https://arxiv.org/abs/2503.04877', 'abstract': "Imitation Learning (IL) has been very effective in training robots to perform complex and diverse manipulation tasks. However, its performance declines precipitously when the observations are out of the training distribution. 3D scene representations that incorporate observations from calibrated RGBD cameras have been proposed as a way to improve generalizability of IL policies, but our evaluations in cross-embodiment and novel camera pose settings found that they show only modest improvement. To address those challenges, we propose Adaptive 3D Scene Representation (Adapt3R), a general-purpose 3D observation encoder which uses a novel architecture to synthesize data from one or more RGBD cameras into a single vector that can then be used as conditioning for arbitrary IL algorithms. The key idea is to use a pretrained 2D backbone to extract semantic information about the scene, using 3D only as a medium for localizing this semantic information with respect to the end-effector. We show that when trained end-to-end with several SOTA multi-task IL algorithms, Adapt3R maintains these algorithms' multi-task learning capacity while enabling zero-shot transfer to novel embodiments and camera poses. Furthermore, we provide a detailed suite of ablation and sensitivity experiments to elucidate the design space for point cloud observation encoders.", 'abstract_zh': '仿生学习（IL）在训练机器人执行复杂多样的操作任务方面非常有效。然而，当观测数据超出训练分布时，其性能会急剧下降。带有校准RGBD摄像头观测的3D场景表示已被提出以提高IL策略的泛化能力，但我们在跨体素和新型摄像头姿态设置下的评估发现，它们仅显示出适度的改进。为应对这些挑战，我们提出了自适应3D场景表示（Adapt3R），这是一种通用的3D观测编码器，使用一种新型架构将一个或多个RGBD摄像头的数据综合成一个向量，该向量可以作为任意IL算法的条件输入。核心理念是使用预训练的2D主干网络提取场景的语义信息，并仅使用3D信息作为相对于末端执行器局部化这些语义信息的媒介。我们展示了当与几种SOTA多任务IL算法端到端训练时，Adapt3R能够保持这些算法的多任务学习能力，并实现对新型体态和摄像头姿态的零样本迁移。此外，我们提供了详细的消融和敏感性实验来阐明点云观测编码器的设计空间。', 'title_zh': 'Adapt3R: 适应性三维场景表示在模仿学习领域迁移中的应用'}
{'arxiv_id': 'arXiv:2503.04862', 'title': 'High-Precision Transformer-Based Visual Servoing for Humanoid Robots in Aligning Tiny Objects', 'authors': 'Jialong Xue, Wei Gao, Yu Wang, Chao Ji, Dongdong Zhao, Shi Yan, Shiwu Zhang', 'link': 'https://arxiv.org/abs/2503.04862', 'abstract': "High-precision tiny object alignment remains a common and critical challenge for humanoid robots in real-world. To address this problem, this paper proposes a vision-based framework for precisely estimating and controlling the relative position between a handheld tool and a target object for humanoid robots, e.g., a screwdriver tip and a screw head slot. By fusing images from the head and torso cameras on a robot with its head joint angles, the proposed Transformer-based visual servoing method can correct the handheld tool's positional errors effectively, especially at a close distance. Experiments on M4-M8 screws demonstrate an average convergence error of 0.8-1.3 mm and a success rate of 93\\%-100\\%. Through comparative analysis, the results validate that this capability of high-precision tiny object alignment is enabled by the Distance Estimation Transformer architecture and the Multi-Perception-Head mechanism proposed in this paper.", 'abstract_zh': '基于视觉的高精度小型物体对齐框架： humanoid机器人手持工具与目标物体之间相对位置的精确估计与控制', 'title_zh': '基于高精度Transformer的类人机器人微小物体对齐视觉伺服方法'}
{'arxiv_id': 'arXiv:2503.04838', 'title': 'Combined Physics and Event Camera Simulator for Slip Detection', 'authors': 'Thilo Reinold, Suman Ghosh, Guillermo Gallego', 'link': 'https://arxiv.org/abs/2503.04838', 'abstract': "Robot manipulation is a common task in fields like industrial manufacturing. Detecting when objects slip from a robot's grasp is crucial for safe and reliable operation. Event cameras, which register pixel-level brightness changes at high temporal resolution (called ``events''), offer an elegant feature when mounted on a robot's end effector: since they only detect motion relative to their viewpoint, a properly grasped object produces no events, while a slipping object immediately triggers them. To research this feature, representative datasets are essential, both for analytic approaches and for training machine learning models. The majority of current research on slip detection with event-based data is done on real-world scenarios and manual data collection, as well as additional setups for data labeling. This can result in a significant increase in the time required for data collection, a lack of flexibility in scene setups, and a high level of complexity in the repetition of experiments. This paper presents a simulation pipeline for generating slip data using the described camera-gripper configuration in a robot arm, and demonstrates its effectiveness through initial data-driven experiments. The use of a simulator, once it is set up, has the potential to reduce the time spent on data collection, provide the ability to alter the setup at any time, simplify the process of repetition and the generation of arbitrarily large data sets. Two distinct datasets were created and validated through visual inspection and artificial neural networks (ANNs). Visual inspection confirmed photorealistic frame generation and accurate slip modeling, while three ANNs trained on this data achieved high validation accuracy and demonstrated good generalization capabilities on a separate test set, along with initial applicability to real-world data. Project page: this https URL", 'abstract_zh': '基于事件相机的机器人滑落数据生成仿真管道及其初步实验', 'title_zh': '滑动检测的联合物理与事件相机模拟器'}
