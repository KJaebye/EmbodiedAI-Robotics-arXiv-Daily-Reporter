{'arxiv_id': 'arXiv:2505.03570', 'title': 'OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents', 'authors': 'Mariya Davydova, Daniel Jeffries, Patrick Barker, Arturo Márquez Flores, Sinéad Ryan', 'link': 'https://arxiv.org/abs/2505.03570', 'abstract': 'In this paper, we introduce OSUniverse: a benchmark of complex, multimodal desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on ease of use, extensibility, comprehensive coverage of test cases, and automated validation. We divide the tasks in increasing levels of complexity, from basic precision clicking to multistep, multiapplication tests requiring dexterity, precision, and clear thinking from the agent. In version one of the benchmark, presented here, we have calibrated the complexity of the benchmark test cases to ensure that the SOTA (State of the Art) agents (at the time of publication) do not achieve results higher than 50%, while the average white collar worker can perform all these tasks with perfect accuracy. The benchmark can be scored manually, but we also introduce an automated validation mechanism that has an average error rate less than 2%. Therefore, this benchmark presents solid ground for fully automated measuring of progress, capabilities and the effectiveness of GUI-navigation AI agents over the short and medium-term horizon. The source code of the benchmark is available at this https URL.', 'abstract_zh': 'OSUniverse：面向高级GUI导航AI代理的复杂多模态桌面任务基准', 'title_zh': 'OSUniverse：多模态GUI导航AI代理的基准测试'}
{'arxiv_id': 'arXiv:2505.03020', 'title': 'The Multimodal Paradox: How Added and Missing Modalities Shape Bias and Performance in Multimodal AI', 'authors': 'Kishore Sampath, Pratheesh, Ayaazuddin Mohammad, Resmi Ramachandranpillai', 'link': 'https://arxiv.org/abs/2505.03020', 'abstract': 'Multimodal learning, which integrates diverse data sources such as images, text, and structured data, has proven superior to unimodal counterparts in high-stakes decision-making. However, while performance gains remain the gold standard for evaluating multimodal systems, concerns around bias and robustness are frequently overlooked. In this context, this paper explores two key research questions (RQs): (i) RQ1 examines whether adding a modality con-sistently enhances performance and investigates its role in shaping fairness measures, assessing whether it mitigates or amplifies bias in multimodal models; (ii) RQ2 investigates the impact of missing modalities at inference time, analyzing how multimodal models generalize in terms of both performance and fairness. Our analysis reveals that incorporating new modalities during training consistently enhances the performance of multimodal models, while fairness trends exhibit variability across different evaluation measures and datasets. Additionally, the absence of modalities at inference degrades performance and fairness, raising concerns about its robustness in real-world deployment. We conduct extensive experiments using multimodal healthcare datasets containing images, time series, and structured information to validate our findings.', 'abstract_zh': '多模态学习，融合图像、文本和结构化数据等多样化的数据源，在高风险决策中已证明优于单模态系统。然而，尽管性能提升仍然是评估多模态系统的主要标准，但关于偏见和鲁棒性的问题却经常被忽视。在此背景下，本文探讨了两个关键研究问题（RQs）：（i）RQ1探讨添加模态是否始终能增强性能，并研究其在塑造公平性指标中的作用，评估它是否减轻或放大了多模态模型中的偏见；（ii）RQ2探讨推理过程中缺失模态的影响，分析多模态模型在性能和公平性上的泛化能力。我们的分析表明，在训练过程中纳入新模态始终能提升多模态模型的性能，而公平性趋势在不同评估指标和数据集中表现出差异性。此外，推理过程中缺失模态会降低性能和公平性，反映出其在实际部署中的鲁棒性问题。我们使用包含图像、时间序列和结构化信息的多模态医疗数据集进行了大量实验以验证我们的发现。', 'title_zh': '多模态悖论：增加和缺失的模态如何塑造多模态AI中的偏差与性能'}
{'arxiv_id': 'arXiv:2505.03492', 'title': 'Augmenting Human Cognition through Everyday AR', 'authors': 'Xiaoan Liu', 'link': 'https://arxiv.org/abs/2505.03492', 'abstract': 'As spatial computing and multimodal LLMs mature, AR is tending to become an intuitive "thinking tool," embedding semantic and context-aware intelligence directly into everyday environments. This paper explores how always-on AR can seamlessly bridge digital cognition and physical affordances, enabling proactive, context-sensitive interactions that enhance human task performance and understanding.', 'abstract_zh': '随着空间计算和多模态大语言模型的成熟，AR正趋向于成为一种直观的“思维工具”，将语义和上下文感知的智能直接嵌入到日常环境中。本文探讨了始终可用的AR如何无缝地连接数字认知和物理可用性，实现前瞻性的、上下文敏感的交互，从而增强人类的任务绩效和理解。', 'title_zh': '通过日常AR增强人类认知'}
{'arxiv_id': 'arXiv:2505.03380', 'title': 'Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant', 'authors': 'Haonan Wang, Jiaji Mao, Lehan Wang, Qixiang Zhang, Marawan Elbatel, Yi Qin, Huijun Hu, Baoxun Li, Wenhui Deng, Weifeng Qin, Hongrui Li, Jialin Liang, Jun Shen, Xiaomeng Li', 'link': 'https://arxiv.org/abs/2505.03380', 'abstract': "Medical AI assistants support doctors in disease diagnosis, medical image analysis, and report generation. However, they still face significant challenges in clinical use, including limited accuracy with multimodal content and insufficient validation in real-world settings. We propose RCMed, a full-stack AI assistant that improves multimodal alignment in both input and output, enabling precise anatomical delineation, accurate localization, and reliable diagnosis through hierarchical vision-language grounding. A self-reinforcing correlation mechanism allows visual features to inform language context, while language semantics guide pixel-wise attention, forming a closed loop that refines both modalities. This correlation is enhanced by a color region description strategy, translating anatomical structures into semantically rich text to learn shape-location-text relationships across scales. Trained on 20 million image-mask-description triplets, RCMed achieves state-of-the-art precision in contextualizing irregular lesions and subtle anatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It achieved a 23.5% relative improvement in cell segmentation from microscopy images over prior methods. RCMed's strong vision-language alignment enables exceptional generalization, with state-of-the-art performance in external validation across 20 clinically significant cancer types, including novel tasks. This work demonstrates how integrated multimodal models capture fine-grained patterns, enabling human-level interpretation in complex scenarios and advancing human-centric AI healthcare.", 'abstract_zh': 'Medical AI助手支持医生进行疾病诊断、医学图像分析和报告生成，但在临床应用中仍面临显著挑战，包括多模态内容的限制造准确性以及在现实环境中的不足验证。我们提出RCMed全栈AI助手，通过在输入和输出中改进多模态对齐，实现精确的解剖轮廓化、准确的定位和可靠的诊断，借助层级视觉-语言定位。自我强化的相关机制允许视觉特征指导语言上下文，而语言语义引导像素级注意力，形成封闭循环以精炼两种模态。这种相关性通过颜色区域描述策略增强，将解剖结构转化为语义丰富的文本以学习跨尺度的形状-位置-文本关系。RCMed基于2000万个图像-掩码-描述三元组训练，在165项临床任务的9种模态下实现最先进的上下文信息关联精确度，特别是在显微镜图像中的细胞分割任务上相对改进了23.5%。RCMed强大的视觉-语言对齐使其具有出色的泛化能力，在20种临床显著的癌症类型中的外部验证中达到最先进的表现，包括新的任务。这项工作展示了一个集成多模态模型如何捕捉细微特征，以复杂场景中实现人类级别的解释，并推进以人类为中心的人工智能医疗健康。', 'title_zh': '视觉与语言强化关联以实现精准医疗AI助手'}
{'arxiv_id': 'arXiv:2505.03242', 'title': 'Seeing the Abstract: Translating the Abstract Language for Vision Language Models', 'authors': 'Davide Talon, Federico Girella, Ziyue Liu, Marco Cristani, Yiming Wang', 'link': 'https://arxiv.org/abs/2505.03242', 'abstract': 'Natural language goes beyond dryly describing visual content. It contains rich abstract concepts to express feeling, creativity and properties that cannot be directly perceived. Yet, current research in Vision Language Models (VLMs) has not shed light on abstract-oriented language. Our research breaks new ground by uncovering its wide presence and under-estimated value, with extensive analysis. Particularly, we focus our investigation on the fashion domain, a highly-representative field with abstract expressions. By analyzing recent large-scale multimodal fashion datasets, we find that abstract terms have a dominant presence, rivaling the concrete ones, providing novel information, and being useful in the retrieval task. However, a critical challenge emerges: current general-purpose or fashion-specific VLMs are pre-trained with databases that lack sufficient abstract words in their text corpora, thus hindering their ability to effectively represent abstract-oriented language. We propose a training-free and model-agnostic method, Abstract-to-Concrete Translator (ACT), to shift abstract representations towards well-represented concrete ones in the VLM latent space, using pre-trained models and existing multimodal databases. On the text-to-image retrieval task, despite being training-free, ACT outperforms the fine-tuned VLMs in both same- and cross-dataset settings, exhibiting its effectiveness with a strong generalization capability. Moreover, the improvement introduced by ACT is consistent with various VLMs, making it a plug-and-play solution.', 'abstract_zh': '自然语言超越了对视觉内容的枯燥描述，包含了丰富的抽象概念以表达感觉、创造力和无法直接感知的属性。然而，当前的视觉语言模型研究尚未关注抽象导向的语言。我们的研究开拓了新的领域，揭示了抽象导向语言的广泛存在及其被低估的价值，并进行了广泛的分析。特别是，我们将调查集中在时尚领域，这是一个富含抽象表达的高度代表性领域。通过分析大规模多模态时尚数据集，我们发现抽象词汇占据了主导地位，与具体的词汇不相上下，提供新的信息，并在检索任务中具有实用价值。然而，一个关键挑战出现了：当前的通用或特定于时尚的视觉语言模型是基于数据库进行预训练的，而这些数据库在文本语料库中缺乏足够的抽象词汇，从而阻碍了它们对抽象导向语言的有效表示。我们提出了一种无需训练且模型无关的方法——抽象到具体翻译器（ACT），通过使用预训练模型和现有的多模态数据库，将抽象表示移向了代表充分的具体表示。在文本到图像检索任务中，尽管ACT是无需训练的，但在同数据库和跨数据库设置中，其性能均超过了微调的视觉语言模型，展示了其强大的泛化能力。此外，由ACT带来的改进在各种视觉语言模型中是一致的，使其成为即插即用的解决方案。', 'title_zh': '看清摘要：为视觉语言模型翻译摘要语言'}
{'arxiv_id': 'arXiv:2505.03173', 'title': 'RAVU: Retrieval Augmented Video Understanding with Compositional Reasoning over Graph', 'authors': 'Sameer Malik, Moyuru Yamada, Ayush Singh, Dishank Aggarwal', 'link': 'https://arxiv.org/abs/2505.03173', 'abstract': 'Comprehending long videos remains a significant challenge for Large Multi-modal Models (LMMs). Current LMMs struggle to process even minutes to hours videos due to their lack of explicit memory and retrieval mechanisms. To address this limitation, we propose RAVU (Retrieval Augmented Video Understanding), a novel framework for video understanding enhanced by retrieval with compositional reasoning over a spatio-temporal graph. We construct a graph representation of the video, capturing both spatial and temporal relationships between entities. This graph serves as a long-term memory, allowing us to track objects and their actions across time. To answer complex queries, we decompose the queries into a sequence of reasoning steps and execute these steps on the graph, retrieving relevant key information. Our approach enables more accurate understanding of long videos, particularly for queries that require multi-hop reasoning and tracking objects across frames. Our approach demonstrate superior performances with limited retrieved frames (5-10) compared with other SOTA methods and baselines on two major video QA datasets, NExT-QA and EgoSchema.', 'abstract_zh': '理解长时间视频仍然是大型多模态模型（LMMs）的一个显著挑战。现有的LMMs由于缺乏显式的记忆和检索机制，难以处理甚至几分钟到几小时的视频。为了解决这一限制，我们提出了RAVu（检索增强视频理解）框架，该框架通过在时空图上进行组合推理来增强视频理解。我们构建了一个视频的图表示，捕捉实体之间的空间和时间关系。此图作为长期记忆，使我们能够在时间上跟踪物体及其动作。为了回答复杂的查询，我们将查询分解为一系列推理步骤，并在图上执行这些步骤，检索相关关键信息。我们的方法能够更准确地理解长时间视频，特别是在需要多跳推理和跨帧跟踪物体的查询方面。与NExT-QA和EgoSchema这两个主要视频问答数据集中的其他最先进方法和基线方法相比，我们的方法在使用有限检索帧（5-10帧）的情况下展示了更优越的性能。', 'title_zh': 'RAVU：基于图的组合理论增强视频理解'}
