{'arxiv_id': 'arXiv:2505.03738', 'title': 'AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control', 'authors': 'Jialong Li, Xuxin Cheng, Tianshu Huang, Shiqi Yang, Ri-Zhao Qiu, Xiaolong Wang', 'link': 'https://arxiv.org/abs/2505.03738', 'abstract': "Humanoid robots derive much of their dexterity from hyper-dexterous whole-body movements, enabling tasks that require a large operational workspace: such as picking objects off the ground. However, achieving these capabilities on real humanoids remains challenging due to their high degrees of freedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization (AMO), a framework that integrates sim-to-real reinforcement learning (RL) with trajectory optimization for real-time, adaptive whole-body control. To mitigate distribution bias in motion imitation RL, we construct a hybrid AMO dataset and train a network capable of robust, on-demand adaptation to potentially O.O.D. commands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid robot, demonstrating superior stability and an expanded workspace compared to strong baselines. Finally, we show that AMO's consistent performance supports autonomous task execution via imitation learning, underscoring the system's versatility and robustness.", 'abstract_zh': '人形机器人通过超灵巧的全身运动获得其大部分灵巧性，使得在较大的操作工作空间内执行任务（如拾取地面物体）成为可能。然而，由于其高自由度和非线性动力学，要在实际人形机器人上实现这些能力仍然具有挑战性。我们提出了自适应运动优化（AMO）框架，该框架将仿真到现实世界的强化学习（RL）与轨迹优化结合，以实现实时、自适应的全身控制。为缓解运动模仿RL中的分布偏差，我们构建了混合AMO数据集，并训练了一个能够在潜在的O.O.D.命令下实现鲁棒、按需自适应的网络。我们在 simulink 和 29-DoF 的 Unitee G1 人形机器人上验证了 AMO，结果显示其稳定性更优且操作工作空间更广，优于强大的基准模型。最后，我们展示了 AMO 的一致性能支持通过模仿学习进行自主任务执行，突显了该系统的多样性和鲁棒性。', 'title_zh': 'AMO：自适应运动优化在超灵巧人形全身控制中的应用'}
{'arxiv_id': 'arXiv:2505.03729', 'title': 'Visual Imitation Enables Contextual Humanoid Control', 'authors': 'Arthur Allshire, Hongsuk Choi, Junyi Zhang, David McAllister, Anthony Zhang, Chung Min Kim, Trevor Darrell, Pieter Abbeel, Jitendra Malik, Angjoo Kanazawa', 'link': 'https://arxiv.org/abs/2505.03729', 'abstract': 'How can we teach humanoids to climb staircases and sit on chairs using the surrounding environment context? Arguably, the simplest way is to just show them-casually capture a human motion video and feed it to humanoids. We introduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday videos, jointly reconstructs the humans and the environment, and produces whole-body control policies for humanoid robots that perform the corresponding skills. We demonstrate the results of our pipeline on real humanoid robots, showing robust, repeatable contextual control such as staircase ascents and descents, sitting and standing from chairs and benches, as well as other dynamic whole-body skills-all from a single policy, conditioned on the environment and global root commands. VIDEOMIMIC offers a scalable path towards teaching humanoids to operate in diverse real-world environments.', 'abstract_zh': '如何利用环境上下文教机器人爬楼梯和坐在椅子上？一种方法是直接向它们展示—随意捕捉人类动作视频并输入机器人中。我们提出了一种从现实到仿真再到现实的管道——VIDEOMIMIC，它挖掘日常生活中的视频，联合重建人类和环境，并生成使得类人机器人执行相应技能的全身控制策略。我们在真实的类人机器人上展示了该管道的结果，展示了如爬楼梯、上下楼梯、从椅子和长凳上坐下和站立等鲁棒且可重复的上下文控制，以及其他动态全身技能—这一切都来自一个单一的策略，该策略基于环境和全局基础命令进行条件控制。VIDEOMIMIC 提供了一条可扩展的路径，以便教会类人机器人操作多样化的现实世界环境。', 'title_zh': '视觉模仿使情境驱动的人形控制成为可能'}
{'arxiv_id': 'arXiv:2505.03728', 'title': 'PyRoki: A Modular Toolkit for Robot Kinematic Optimization', 'authors': 'Chung Min Kim, Brent Yi, Hongsuk Choi, Yi Ma, Ken Goldberg, Angjoo Kanazawa', 'link': 'https://arxiv.org/abs/2505.03728', 'abstract': "Robot motion can have many goals. Depending on the task, we might optimize for pose error, speed, collision, or similarity to a human demonstration. Motivated by this, we present PyRoki: a modular, extensible, and cross-platform toolkit for solving kinematic optimization problems. PyRoki couples an interface for specifying kinematic variables and costs with an efficient nonlinear least squares optimizer. Unlike existing tools, it is also cross-platform: optimization runs natively on CPU, GPU, and TPU. In this paper, we present (i) the design and implementation of PyRoki, (ii) motion retargeting and planning case studies that highlight the advantages of PyRoki's modularity, and (iii) optimization benchmarking, where PyRoki can be 1.4-1.7x faster and converges to lower errors than cuRobo, an existing GPU-accelerated inverse kinematics library.", 'abstract_zh': '机器人运动可以有多种目标。根据任务的不同，我们可能需要优化姿态误差、速度、碰撞或者与人类演示的相似度。基于这一点，我们介绍了PyRoki：一种模块化、扩展性强且跨平台的工具包，用于求解运动学优化问题。PyRoki 结合了一个用于指定运动学变量和代价的功能接口，以及一个高效非线性最小二乘优化器。与现有的工具不同，它也是跨平台的：优化可以直接在CPU、GPU和TPU上本地运行。在本文中，我们介绍了(i) PyRoki 的设计和实现，(ii) 动作重定位和规划案例研究，展示了PyRoki模块性的优势，以及(iii) 优化基准测试，在这些测试中，PyRoki 比现有的GPU加速逆运动学库cuRobo 快1.4-1.7倍，并且收敛到较低的误差。', 'title_zh': 'PyRoki: 一种模块化机器人运动学优化工具包'}
{'arxiv_id': 'arXiv:2505.03725', 'title': 'Meta-Optimization and Program Search using Language Models for Task and Motion Planning', 'authors': 'Denis Shcherba, Eckart Cobo-Briesewitz, Cornelius V. Braun, Marc Toussaint', 'link': 'https://arxiv.org/abs/2505.03725', 'abstract': 'Intelligent interaction with the real world requires robotic agents to jointly reason over high-level plans and low-level controls. Task and motion planning (TAMP) addresses this by combining symbolic planning and continuous trajectory generation. Recently, foundation model approaches to TAMP have presented impressive results, including fast planning times and the execution of natural language instructions. Yet, the optimal interface between high-level planning and low-level motion generation remains an open question: prior approaches are limited by either too much abstraction (e.g., chaining simplified skill primitives) or a lack thereof (e.g., direct joint angle prediction). Our method introduces a novel technique employing a form of meta-optimization to address these issues by: (i) using program search over trajectory optimization problems as an interface between a foundation model and robot control, and (ii) leveraging a zero-order method to optimize numerical parameters in the foundation model output. Results on challenging object manipulation and drawing tasks confirm that our proposed method improves over prior TAMP approaches.', 'abstract_zh': '智能与现实世界交互需要机器人代理联合推理高层规划和低层控制。任务与运动规划（TAMP）通过结合符号规划和连续轨迹生成来解决这一问题。近期，基础模型在TAMP方面的应用取得了令人印象深刻的成果，包括快速规划时间和执行自然语言指令。然而，高层规划与低层运动生成的最佳接口仍是一个开放问题：先前的方法要么过于抽象（例如，组合简化技能原语），要么不够抽象（例如，直接预测关节角度）。我们的方法引入了一种新颖的技术，通过元优化解决这些问题，包括：（i）使用轨迹优化问题上的程序搜索作为基础模型与机器人控制之间的接口；（ii）利用零阶方法优化基础模型输出中的数值参数。在具有挑战性的物体操作和绘画任务上的结果证实，我们提出的方法优于先前的TAMP方法。', 'title_zh': '使用语言模型进行元优化和程序搜索以实现任务与运动规划'}
{'arxiv_id': 'arXiv:2505.03702', 'title': 'Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid Geometric-Neural Approach', 'authors': 'Srecharan Selvam, Abhishesh Silwal, George Kanter', 'link': 'https://arxiv.org/abs/2505.03702', 'abstract': 'Automating leaf manipulation in agricultural settings faces significant challenges, including the variability of plant morphologies and deformable leaves. We propose a novel hybrid geometric-neural approach for autonomous leaf grasping that combines traditional computer vision with neural networks through self-supervised learning. Our method integrates YOLOv8 for instance segmentation and RAFT-Stereo for 3D depth estimation to build rich leaf representations, which feed into both a geometric feature scoring pipeline and a neural refinement module (GraspPointCNN). The key innovation is our confidence-weighted fusion mechanism that dynamically balances the contribution of each approach based on prediction certainty. Our self-supervised framework uses the geometric pipeline as an expert teacher to automatically generate training data. Experiments demonstrate that our approach achieves an 88.0% success rate in controlled environments and 84.7% in real greenhouse conditions, significantly outperforming both purely geometric (75.3%) and neural (60.2%) methods. This work establishes a new paradigm for agricultural robotics where domain expertise is seamlessly integrated with machine learning capabilities, providing a foundation for fully automated crop monitoring systems.', 'abstract_zh': '在农业生产环境中自动执行叶片操作面临显著挑战，包括植物形态的变异性与可变形的叶片。我们提出了一种结合传统计算机视觉与神经网络的新型几何-神经混合方法，通过自监督学习将两者结合起来进行自主叶片抓取。该方法利用YOLOv8进行实例分割和RAFT-Stereo进行3D深度估计，构建丰富的叶片表示，为几何特征评分管道和神经精炼模块（GraspPointCNN）提供输入。关键创新在于我们的置信加权融合机制，该机制基于预测的确定性动态平衡每种方法的贡献。我们的自监督框架使用几何管道作为专家教师，自动生成训练数据。实验表明，该方法在受控环境中的成功率达到了88.0%，在实际温室条件下的成功率为84.7%，显著优于纯几何方法（75.3%）和纯神经方法（60.2%）。这项研究建立了农业机器人领域的新范式，将领域专业知识无缝集成到机器学习能力中，为完全自动化的农作物监测系统提供了基础。', 'title_zh': '自主监督学习在机器人树叶操作中的应用：几何-神经混合方法'}
{'arxiv_id': 'arXiv:2505.03695', 'title': 'Frenet Corridor Planner: An Optimal Local Path Planning Framework for Autonomous Driving', 'authors': 'Faizan M. Tariq, Zheng-Hang Yeh, Avinash Singh, David Isele, Sangjae Bae', 'link': 'https://arxiv.org/abs/2505.03695', 'abstract': 'Motivated by the requirements for effectiveness and efficiency, path-speed decomposition-based trajectory planning methods have widely been adopted for autonomous driving applications. While a global route can be pre-computed offline, real-time generation of adaptive local paths remains crucial. Therefore, we present the Frenet Corridor Planner (FCP), an optimization-based local path planning strategy for autonomous driving that ensures smooth and safe navigation around obstacles. Modeling the vehicles as safety-augmented bounding boxes and pedestrians as convex hulls in the Frenet space, our approach defines a drivable corridor by determining the appropriate deviation side for static obstacles. Thereafter, a modified space-domain bicycle kinematics model enables path optimization for smoothness, boundary clearance, and dynamic obstacle risk minimization. The optimized path is then passed to a speed planner to generate the final trajectory. We validate FCP through extensive simulations and real-world hardware experiments, demonstrating its efficiency and effectiveness.', 'abstract_zh': '基于路径-速度分解的轨迹规划方法旨在满足自主驾驶应用中效果和效率的要求。尽管全局路线可以在线预计算，实时生成适应性的局部路径仍然至关重要。因此，我们提出了Frenét走廊规划器（FCP），这是一种基于优化的局部路径规划策略，确保在避开障碍物时实现平滑和安全的导航。在Frenét空间中，我们将车辆建模为安全增强的边界框，行人建模为凸包，我们的方法通过确定静态障碍物的适当偏离侧来定义可通行走廊。之后，修改后的空间域自行车动力学模型使路径优化能够实现平滑性、边界 clearance 和动态障碍物风险最小化。优化后的路径随后传递给速度规划器以生成最终轨迹。通过对FCP进行广泛的仿真实验和实地硬件实验，我们验证了其效率和有效性。', 'title_zh': 'Frenet 栅栏规划器：自主驾驶的最优局部路径规划框架'}
{'arxiv_id': 'arXiv:2505.03694', 'title': 'Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and Avoid', 'authors': 'Parv Kapoor, Ian Higgins, Nikhil Keetha, Jay Patrikar, Brady Moon, Zelin Ye, Yao He, Ivan Cisneros, Yaoyu Hu, Changliu Liu, Eunsuk Kang, Sebastian Scherer', 'link': 'https://arxiv.org/abs/2505.03694', 'abstract': "Assured safe-separation is essential for achieving seamless high-density operation of airborne vehicles in a shared airspace. To equip resource-constrained aerial systems with this safety-critical capability, we present ViSafe, a high-speed vision-only airborne collision avoidance system. ViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by tightly integrating a learning-based edge-AI framework with a custom multi-camera hardware prototype designed under SWaP-C constraints. By leveraging perceptual input-focused control barrier functions (CBF) to design, encode, and enforce safety thresholds, ViSafe can provide provably safe runtime guarantees for self-separation in high-speed aerial operations. We evaluate ViSafe's performance through an extensive test campaign involving both simulated digital twins and real-world flight scenarios. By independently varying agent types, closure rates, interaction geometries, and environmental conditions (e.g., weather and lighting), we demonstrate that ViSafe consistently ensures self-separation across diverse scenarios. In first-of-its-kind real-world high-speed collision avoidance tests with closure rates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous collision avoidance, establishing a new standard for safety in high-speed aerial navigation.", 'abstract_zh': '确保安全分离对于实现空中共享空域中的无缝高密度运行至关重要。为了给资源受限的飞行系统配备这一关键安全能力，我们提出了一种高速视觉-only空中碰撞避免系统ViSafe。ViSafe通过将基于学习的边缘AI框架与SWaP-C约束下的自定义多摄像头硬件原型紧密集成，为Detect and Avoid (DAA) 问题提供全流程解决方案。通过利用感知输入焦点的控制屏障函数（CBF）设计、编码和执行安全阈值，ViSafe可以为高速空中操作中的自我分离提供可验证的安全运行保证。通过广泛的测试活动，包括模拟数字双胞胎和真实飞行场景，我们验证了ViSafe在各种场景中的一致性自分离性能。在首次实现的高速碰撞避免测试中，闭合速度达到144 km/h，ViSafe确立了视觉-only自主碰撞避免的新基准，为高速空中导航的安全性设立了新标准。', 'title_zh': '基于视觉的安全性演示：高 speeding 检测与避免'}
{'arxiv_id': 'arXiv:2505.03673', 'title': 'RoboOS: A Hierarchical Embodied Framework for Cross-Embodiment and Multi-Agent Collaboration', 'authors': 'Huajie Tan, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Yaoxu Lyu, Mingyu Cao, Zhongyuan Wang, Shanghang Zhang', 'link': 'https://arxiv.org/abs/2505.03673', 'abstract': "The dawn of embodied intelligence has ushered in an unprecedented imperative for resilient, cognition-enabled multi-agent collaboration across next-generation ecosystems, revolutionizing paradigms in autonomous manufacturing, adaptive service robotics, and cyber-physical production architectures. However, current robotic systems face significant limitations, such as limited cross-embodiment adaptability, inefficient task scheduling, and insufficient dynamic error correction. While End-to-end VLA models demonstrate inadequate long-horizon planning and task generalization, hierarchical VLA models suffer from a lack of cross-embodiment and multi-agent coordination capabilities. To address these challenges, we introduce RoboOS, the first open-source embodied system built on a Brain-Cerebellum hierarchical architecture, enabling a paradigm shift from single-agent to multi-agent intelligence. Specifically, RoboOS consists of three key components: (1) Embodied Brain Model (RoboBrain), a MLLM designed for global perception and high-level decision-making; (2) Cerebellum Skill Library, a modular, plug-and-play toolkit that facilitates seamless execution of multiple skills; and (3) Real-Time Shared Memory, a spatiotemporal synchronization mechanism for coordinating multi-agent states. By integrating hierarchical information flow, RoboOS bridges Embodied Brain and Cerebellum Skill Library, facilitating robust planning, scheduling, and error correction for long-horizon tasks, while ensuring efficient multi-agent collaboration through Real-Time Shared Memory. Furthermore, we enhance edge-cloud communication and cloud-based distributed inference to facilitate high-frequency interactions and enable scalable deployment. Extensive real-world experiments across various scenarios, demonstrate RoboOS's versatility in supporting heterogeneous embodiments. Project website: this https URL", 'abstract_zh': '拟人化智能的兴起带来了对未来代际生态系统中具备认知能力的多智能体协作的新迫切需求，革新了自主制造、自适应服务机器人和网络物理生产架构的范式。然而，现有机器人系统面临诸多限制，如跨体适应性有限、任务调度效率低下以及动态错误校正能力不足。尽管端到端的VLA模型在长期规划和任务通用性方面表现不足，而分层的VLA模型则缺乏跨体和多智能体协调能力。为应对这些挑战，我们提出了RoboOS，这是首个建立在脑-小脑分层架构上的开源拟人化系统，推动了从单智能体到多智能体智能的范式转变。RoboOS 包含三个关键组件：(1) 拟人化脑模型（RoboBrain），一种面向全局感知和高层次决策的MLLM；(2) 小脑技能库，一种模块化、即插即用工具包，使多技能无缝执行成为可能；(3) 实时共享内存，一种时空同步机制，用于协调多智能体状态。通过集成分层信息流，RoboOS 连接了拟人化脑和小脑技能库，支持长期任务的稳健规划、调度和错误校正，同时借助实时共享内存确保高效的多智能体协作。此外，我们增强了边缘-云通信和基于云的分布式推理，以促进高频交互并实现可扩展部署。在各种场景下的广泛真实世界实验证明了RoboOS 在支持异构体方面的灵活性。项目网站：this https URL。', 'title_zh': 'RoboOS: 一种多层次 embodied 框架for 跨身躯和多智能体协作'}
{'arxiv_id': 'arXiv:2505.03587', 'title': 'Meta-reasoning Using Attention Maps and Its Applications in Cloud Robotics', 'authors': 'Adrian Lendinez, Renxi Qiu, Lanfranco Zanzi, Dayou Li', 'link': 'https://arxiv.org/abs/2505.03587', 'abstract': "Metareasoning, a branch of AI, focuses on reasoning about reasons. It has the potential to enhance robots' decision-making processes in unexpected situations. However, the concept has largely been confined to theoretical discussions and case-by-case investigations, lacking general and practical solutions when the Value of Computation (VoC) is undefined, which is common in unexpected situations. In this work, we propose a revised meta-reasoning framework that significantly improves the scalability of the original approach in unexpected situations. This is achieved by incorporating semantic attention maps and unsupervised 'attention' updates into the metareasoning processes. To accommodate environmental dynamics, 'lines of thought' are used to bridge context-specific objects with abstracted attentions, while meta-information is monitored and controlled at the meta-level for effective reasoning. The practicality of the proposed approach is demonstrated through cloud robots deployed in real-world scenarios, showing improved performance and robustness.", 'abstract_zh': '元推理，人工智能的一个分支，专注于关于原因的推理。它有可能在意外情况下增强机器人的决策过程。然而，这一概念主要局限于理论讨论和个案研究，在计算价值（VoC）未定义的情况下缺乏一般性和实用性，这种情况在意外情况下很常见。在本工作中，我们提出了一种修订的元推理框架，该框架在意外情况下显著提高了原始方法的可伸缩性。这通过将语义注意力图和无监督的“注意力”更新纳入元推理过程而实现。为了适应环境动态，“思路线”被用于连接具体环境对象与抽象的注意力，同时在元层次上监控和控制元信息以实现有效的推理。通过实际部署在真实场景中的云机器人，证明了所提方法的实用性，并展示了其改进的性能和鲁棒性。', 'title_zh': '基于注意力图的元推理及其在云机器人中的应用'}
{'arxiv_id': 'arXiv:2505.03565', 'title': 'Thermal-LiDAR Fusion for Robust Tunnel Localization in GNSS-Denied and Low-Visibility Conditions', 'authors': 'Lukas Schichler, Karin Festl, Selim Solmaz, Daniel Watzenig', 'link': 'https://arxiv.org/abs/2505.03565', 'abstract': 'Despite significant progress in autonomous navigation, a critical gap remains in ensuring reliable localization in hazardous environments such as tunnels, urban disaster zones, and underground structures. Tunnels present a uniquely difficult scenario: they are not only prone to GNSS signal loss, but also provide little features for visual localization due to their repetitive walls and poor lighting. These conditions degrade conventional vision-based and LiDAR-based systems, which rely on distinguishable environmental features. To address this, we propose a novel sensor fusion framework that integrates a thermal camera with a LiDAR to enable robust localization in tunnels and other perceptually degraded environments. The thermal camera provides resilience in low-light or smoke conditions, while the LiDAR delivers precise depth perception and structural awareness. By combining these sensors, our framework ensures continuous and accurate localization across diverse and dynamic environments. We use an Extended Kalman Filter (EKF) to fuse multi-sensor inputs, and leverages visual odometry and SLAM (Simultaneous Localization and Mapping) techniques to process the sensor data, enabling robust motion estimation and mapping even in GNSS-denied environments. This fusion of sensor modalities not only enhances system resilience but also provides a scalable solution for cyber-physical systems in connected and autonomous vehicles (CAVs). To validate the framework, we conduct tests in a tunnel environment, simulating sensor degradation and visibility challenges. The results demonstrate that our method sustains accurate localization where standard approaches deteriorate due to the tunnels featureless geometry. The frameworks versatility makes it a promising solution for autonomous vehicles, inspection robots, and other cyber-physical systems operating in constrained, perceptually poor environments.', 'abstract_zh': '尽管在自主导航方面取得了显著进展，但在隧道、城市灾难区域和地下结构等危险环境中的可靠定位仍存在关键差距。隧道呈现了一个独特的难题：它们不仅容易失去GNSS信号，而且由于重复的墙面和照明不良，几乎没有可用于视觉定位的特征。这些条件会削弱依赖可区分环境特征的视觉和LiDAR系统。为了解决这一问题，我们提出了一种新颖的传感器融合框架，将热成像相机与LiDAR集成，以在隧道和其他感知退化的环境中实现稳健的定位。热成像相机在低光或烟雾条件下提供鲁棒性，而LiDAR提供精确的深度感知和结构意识。通过结合这些传感器，我们的框架确保在多种动态环境中持续且准确的定位。我们使用扩展卡尔曼滤波器（EKF）来融合多传感器输入，并利用视觉里程计和SLAM（同时定位与 mapping）技术处理传感器数据，使在GNSS受限环境中也能实现稳健的运动估计和建图。通过将不同类型的传感器数据相融合，该框架不仅提升了系统的鲁棒性，还为连接和自主车辆中的赛博物理系统提供了一个可扩展的解决方案。为了验证该框架，我们在隧道环境中进行了测试，模拟了传感器降级和能见度挑战。结果表明，我们的方法在隧道特征较少的几何结构中能够保持准确的定位，而标准方法则因隧道的特征较少而表现下降。该框架的通用性使其成为在约束和感知较差环境中操作的自主车辆、检查机器人和其他赛博物理系统的有前景的解决方案。', 'title_zh': 'GNSS受限制和低能见度条件下的热成像-LiDAR融合隧道稳固定位'}
{'arxiv_id': 'arXiv:2505.03537', 'title': 'Automated Action Generation based on Action Field for Robotic Garment Manipulation', 'authors': 'Hu Cheng, Fuyuki Tokuda, Kazuhiro Kosuge', 'link': 'https://arxiv.org/abs/2505.03537', 'abstract': 'Garment manipulation using robotic systems is a challenging task due to the diverse shapes and deformable nature of fabric. In this paper, we propose a novel method for robotic garment manipulation that significantly improves the accuracy while reducing computational time compared to previous approaches. Our method features an action generator that directly interprets scene images and generates pixel-wise end-effector action vectors using a neural network. The network also predicts a manipulation score map that ranks potential actions, allowing the system to select the most effective action. Extensive simulation experiments demonstrate that our method achieves higher unfolding and alignment performances and faster computation time than previous approaches. Real-world experiments show that the proposed method generalizes well to different garment types and successfully flattens garments.', 'abstract_zh': '使用机器人系统进行衣物 manipulation 是一项具有挑战性的任务，因为面料具有多样的形状和可变形的性质。本文提出了一种新的机器人衣物 manipulation 方法，与之前的approaches相比，该方法显著提高了准确性并减少了计算时间。该方法包括一个动作生成器，该生成器直接解释场景图像并使用神经网络生成像素级末端执行器动作向量。网络还预测一个操作分数图，对潜在操作进行排名，从而使系统能够选择最有效的操作。广泛的仿真实验表明，与之前的approaches相比，本文方法在开衣和对齐性能上更高，并且计算时间更快。实验证明，所提出的方法能够很好地适应不同类型的衣物，并成功使其展平。', 'title_zh': '基于动作域的机器人服装操作自动动作生成'}
{'arxiv_id': 'arXiv:2505.03500', 'title': 'Task Reconstruction and Extrapolation for $π_0$ using Text Latent', 'authors': 'Quanyi Li', 'link': 'https://arxiv.org/abs/2505.03500', 'abstract': "Vision-language-action models (VLAs) often achieve high performance on demonstrated tasks but struggle significantly when required to extrapolate, combining skills learned from different tasks in novel ways. For instance, VLAs might successfully put the cream cheese in the bowl and put the bowl on top of the cabinet, yet still fail to put the cream cheese on top of the cabinet. In this work, we demonstrate that behaviors from distinct tasks can be effectively recombined by manipulating the VLA's internal representations at inference time. Concretely, we identify the text latent by averaging the text tokens' hidden states across all demonstrated trajectories for a specific base task. For executing an extrapolated task, we can temporally interpolate the text latent of the two base tasks and add it back to the text hidden states, so sub-behaviors from the two tasks will be activated sequentially. We evaluate this approach using the newly created libero-ood benchmark, featuring 20 tasks extrapolated from standard LIBERO suites. The results on libero-ood show that all SOTA VLAs achieve < 15% success rate, while $\\pi0$ with text latent interpolation reaches an 83% success rate. Further qualitative analysis reveals a tendency for VLAs to exhibit spatial overfitting, mapping object names to demonstrated locations rather than achieving genuine object and goal understanding. Additionally, we find that decoding the text latent yields human-unreadable prompts that can nevertheless instruct the VLA to achieve a 70% success rate on standard LIBERO suites, enabling private instruction or backdoor attacks.", 'abstract_zh': 'Vision-语言-动作模型（VLAs）在执行示范任务时通常表现出高水平性能，但在需要外推、以新颖方式结合来自不同任务所学技能时会面临显著挑战。例如，VLAs 可能能够成功地将奶油奶酪放入碗中，再将碗放在橱柜上，但仍无法将奶油奶酪放在橱柜上。在本项工作中，我们证明了可以通过在推断时操控 VLAs 的内部表示，从而有效重组来自不同任务的行为。具体而言，我们通过在特定基础任务的所有演示轨迹中平均文本标记的隐藏状态来识别文本潜在表示。执行外推任务时，我们可以按时间插值来自两个基础任务的文本潜在表示并重新添加到文本隐藏状态中，这样两个任务的子行为将依次被激活。我们使用包含20个从标准LIBERO套件外推而来的新创建的libero-ood基准对其进行评估。在libero-ood上的结果显示，所有最新VLAs的成功率均低于15%，而$\\pi0$结合文本潜在表示插值法达到了83%的成功率。进一步的定性分析表明，VLAs可能表现出空间过拟合倾向，将物体名称映射到演示的位置上，而不是真正理解物体和目标。此外，我们发现解码文本潜在表示可以生成人类难以理解的提示，但这些提示仍能指导VLAs在标准LIBERO套件中达到70%的成功率，从而实现私人指导或后门攻击。', 'title_zh': '$π_0$ 的任务重建与外推 Using Text Latent'}
{'arxiv_id': 'arXiv:2505.03460', 'title': 'LogisticsVLN: Vision-Language Navigation For Low-Altitude Terminal Delivery Based on Agentic UAVs', 'authors': 'Xinyuan Zhang, Yonglin Tian, Fei Lin, Yue Liu, Jing Ma, Kornélia Sára Szatmáry, Fei-Yue Wang', 'link': 'https://arxiv.org/abs/2505.03460', 'abstract': 'The growing demand for intelligent logistics, particularly fine-grained terminal delivery, underscores the need for autonomous UAV (Unmanned Aerial Vehicle)-based delivery systems. However, most existing last-mile delivery studies rely on ground robots, while current UAV-based Vision-Language Navigation (VLN) tasks primarily focus on coarse-grained, long-range goals, making them unsuitable for precise terminal delivery. To bridge this gap, we propose LogisticsVLN, a scalable aerial delivery system built on multimodal large language models (MLLMs) for autonomous terminal delivery. LogisticsVLN integrates lightweight Large Language Models (LLMs) and Visual-Language Models (VLMs) in a modular pipeline for request understanding, floor localization, object detection, and action-decision making. To support research and evaluation in this new setting, we construct the Vision-Language Delivery (VLD) dataset within the CARLA simulator. Experimental results on the VLD dataset showcase the feasibility of the LogisticsVLN system. In addition, we conduct subtask-level evaluations of each module of our system, offering valuable insights for improving the robustness and real-world deployment of foundation model-based vision-language delivery systems.', 'abstract_zh': 'Growing需求下的智能物流特别是精细末端配送强化了自主无人机交付系统的需求。然而，目前大多数最后一公里配送研究依赖地面机器人，而当前基于视觉-语言导航(VLN)的无人机任务主要侧重于粗粒度的远距离目标，使其不适合精确的末端配送。为弥合这一差距，我们提出LogisticsVLN，这是一种基于多模态大规模语言模型（MLLMs）构建的可扩展的空中交付系统，用于自主末端配送。LogisticsVLN 在模块化流水线中整合了轻量级的大规模语言模型（LLMs）和视觉-语言模型（VLMs），用于请求理解、楼层定位、物体检测和动作决策。为了支持这一新环境中的研究和评估，我们在CARLA模拟器中构建了Vision-Language Delivery (VLD) 数据集。LogisticsVLN系统在VLD数据集上的实验结果展示了该系统的可行性。此外，我们还对系统中每个模块的子任务水平进行了评估，为提高基于基础模型的视觉-语言交付系统的鲁棒性和实际部署提供了有价值的见解。', 'title_zh': 'LogisticsVLN：基于代理型无人机的低空终端配送视觉语言导航'}
{'arxiv_id': 'arXiv:2505.03448', 'title': 'AquaticVision: Benchmarking Visual SLAM in Underwater Environment with Events and Frames', 'authors': 'Yifan Peng, Yuze Hong, Ziyang Hong, Apple Pui-Yi Chui, Junfeng Wu', 'link': 'https://arxiv.org/abs/2505.03448', 'abstract': 'Many underwater applications, such as offshore asset inspections, rely on visual inspection and detailed 3D reconstruction. Recent advancements in underwater visual SLAM systems for aquatic environments have garnered significant attention in marine robotics research. However, existing underwater visual SLAM datasets often lack groundtruth trajectory data, making it difficult to objectively compare the performance of different SLAM algorithms based solely on qualitative results or COLMAP reconstruction. In this paper, we present a novel underwater dataset that includes ground truth trajectory data obtained using a motion capture system. Additionally, for the first time, we release visual data that includes both events and frames for benchmarking underwater visual positioning. By providing event camera data, we aim to facilitate the development of more robust and advanced underwater visual SLAM algorithms. The use of event cameras can help mitigate challenges posed by extremely low light or hazy underwater conditions. The webpage of our dataset is this https URL.', 'abstract_zh': '许多水下应用，如离岸资产检查，依赖于视觉检查和详细的3D重建。近年来，适用于水下环境的视觉SLAM系统在水下机器人研究领域引起了广泛关注。然而，现有的水下视觉SLAM数据集往往缺乏地面truth轨迹数据，这使得仅凭定性结果或COLMAP重建难以客观比较不同SLAM算法的性能。在这篇论文中，我们提出了一种新的水下数据集，其中包含使用运动捕捉系统获得的地面truth轨迹数据。此外，我们首次发布了包含事件和帧数据的视觉数据，用于水下视觉定位的基准测试。通过提供事件摄像头数据，我们旨在促进更 robust和先进的水下视觉SLAM算法的发展。事件摄像头的使用有助于缓解极低光照或浑浊水下条件带来的挑战。我们的数据集网址为：https://www.example.com。', 'title_zh': 'AquaticVision: 在水下环境中基于事件和帧的视觉SLAM基准测试'}
{'arxiv_id': 'arXiv:2505.03400', 'title': 'Close-Fitting Dressing Assistance Based on State Estimation of Feet and Garments with Semantic-based Visual Attention', 'authors': 'Takuma Tsukakoshi, Tamon Miyake, Tetsuya Ogata, Yushi Wang, Takumi Akaishi, Shigeki Sugano', 'link': 'https://arxiv.org/abs/2505.03400', 'abstract': "As the population continues to age, a shortage of caregivers is expected in the future. Dressing assistance, in particular, is crucial for opportunities for social participation. Especially dressing close-fitting garments, such as socks, remains challenging due to the need for fine force adjustments to handle the friction or snagging against the skin, while considering the shape and position of the garment. This study introduces a method uses multi-modal information including not only robot's camera images, joint angles, joint torques, but also tactile forces for proper force interaction that can adapt to individual differences in humans. Furthermore, by introducing semantic information based on object concepts, rather than relying solely on RGB data, it can be generalized to unseen feet and background. In addition, incorporating depth data helps infer relative spatial relationship between the sock and the foot. To validate its capability for semantic object conceptualization and to ensure safety, training data were collected using a mannequin, and subsequent experiments were conducted with human subjects. In experiments, the robot successfully adapted to previously unseen human feet and was able to put socks on 10 participants, achieving a higher success rate than Action Chunking with Transformer and Diffusion Policy. These results demonstrate that the proposed model can estimate the state of both the garment and the foot, enabling precise dressing assistance for close-fitting garments.", 'abstract_zh': '随着人口老龄化趋势的加剧，未来护理人员短缺将是一个严峻的问题。特别是穿衣援助对于提高老年人的社会参与机会至关重要。尤其是穿紧身袜子等衣物仍然具有挑战性，因为需要精细地调整力量来处理与皮肤的摩擦或钩挂，同时考虑衣物的形状和位置。本研究介绍了一种方法，该方法利用多模态信息，包括不仅限于机器人摄像头图像、关节角度、关节扭矩，还利用触觉力进行适当的力量交互，以适应人类个体差异。此外，通过引入基于对象概念的语义信息，而不是仅依赖RGB数据，该方法可以泛化到未见过的脚和背景。此外，结合深度数据有助于推断袜子和脚之间的相对空间关系。为了验证其实现语义对象概念化的能力并确保安全，在人体模型上采集了训练数据，并在后续实验中使用人类受试者进行了实验。实验结果显示，机器人能够适应之前未见过的人类脚，并成功为10名参与者穿上了袜子，其成功率为Action Chunking with Transformer和Diffusion Policy的更高。这些结果表明，所提出模型能够估计衣物和脚的状态，从而实现对紧身衣物的精确穿衣辅助。', 'title_zh': '基于足部和服装状态估计的语义引导视觉注意力适配穿着辅助'}
{'arxiv_id': 'arXiv:2505.03356', 'title': 'Effective Reinforcement Learning Control using Conservative Soft Actor-Critic', 'authors': 'Xinyi Yuan, Zhiwei Shang, Wenjun Huang, Yunduan Cui, Di Chen, Meixin Zhu', 'link': 'https://arxiv.org/abs/2505.03356', 'abstract': 'Reinforcement Learning (RL) has shown great potential in complex control tasks, particularly when combined with deep neural networks within the Actor-Critic (AC) framework. However, in practical applications, balancing exploration, learning stability, and sample efficiency remains a significant challenge. Traditional methods such as Soft Actor-Critic (SAC) and Proximal Policy Optimization (PPO) address these issues by incorporating entropy or relative entropy regularization, but often face problems of instability and low sample efficiency. In this paper, we propose the Conservative Soft Actor-Critic (CSAC) algorithm, which seamlessly integrates entropy and relative entropy regularization within the AC framework. CSAC improves exploration through entropy regularization while avoiding overly aggressive policy updates with the use of relative entropy regularization. Evaluations on benchmark tasks and real-world robotic simulations demonstrate that CSAC offers significant improvements in stability and efficiency over existing methods. These findings suggest that CSAC provides strong robustness and application potential in control tasks under dynamic environments.', 'abstract_zh': '保守Soft Actor-Critic (CSAC)算法在Actor-Critic框架中的应用', 'title_zh': '使用保守软Actor-Critic的有效强化学习控制'}
{'arxiv_id': 'arXiv:2505.03344', 'title': 'RIFT: Closed-Loop RL Fine-Tuning for Realistic and Controllable Traffic Simulation', 'authors': 'Keyu Chen, Wenchao Sun, Hao Cheng, Sifa Zheng', 'link': 'https://arxiv.org/abs/2505.03344', 'abstract': 'Achieving both realism and controllability in interactive closed-loop traffic simulation remains a key challenge in autonomous driving. Data-driven simulation methods reproduce realistic trajectories but suffer from covariate shift in closed-loop deployment, compounded by simplified dynamics models that further reduce reliability. Conversely, physics-based simulation methods enhance reliable and controllable closed-loop interactions but often lack expert demonstrations, compromising realism. To address these challenges, we introduce a dual-stage AV-centered simulation framework that conducts open-loop imitation learning pre-training in a data-driven simulator to capture trajectory-level realism and multimodality, followed by closed-loop reinforcement learning fine-tuning in a physics-based simulator to enhance controllability and mitigate covariate shift. In the fine-tuning stage, we propose RIFT, a simple yet effective closed-loop RL fine-tuning strategy that preserves the trajectory-level multimodality through a GRPO-style group-relative advantage formulation, while enhancing controllability and training stability by replacing KL regularization with the dual-clip mechanism. Extensive experiments demonstrate that RIFT significantly improves the realism and controllability of generated traffic scenarios, providing a robust platform for evaluating autonomous vehicle performance in diverse and interactive scenarios.', 'abstract_zh': '在自主驾驶中实现互动闭环交通模拟的现实性和可控性兼具仍是一项关键挑战。数据驱动的模拟方法可以重现现实轨迹，但在闭环部署中易受协变量偏移的影响，且简化的动力学模型进一步降低了可靠性。相反，基于物理的模拟方法可以增强可靠的闭环交互，但往往缺乏专家示范，影响现实性。为解决这些问题，我们提出了一种双阶段以自主车辆为中心的模拟框架，该框架首先在数据驱动的模拟器中进行开环模仿学习预训练，以捕捉轨迹级别的现实性和多模态性，随后在基于物理的模拟器中进行闭环强化学习微调，以增强可控性和减轻协变量偏移。在微调阶段，我们提出了一种名为RIFT的简单而有效的闭环RL微调策略，通过一种组相对优势的GRPO风格形式保留了轨迹级别的多模态性，同时通过使用双重剪辑机制替代KL正则化来增强可控性和训练稳定性。大量实验表明，RIFT显著提高了生成的交通场景的现实性和可控性，提供了一个在多样且交互的场景中评估自主车辆性能的稳健平台。', 'title_zh': 'RIFT: 闭环RL微调方法用于真实且可控的交通模拟'}
{'arxiv_id': 'arXiv:2505.03331', 'title': 'Miniature multihole airflow sensor for lightweight aircraft over wide speed and angular range', 'authors': 'Lukas Stuber, Simon Jeger, Raphael Zufferey, Dario Floreano', 'link': 'https://arxiv.org/abs/2505.03331', 'abstract': "An aircraft's airspeed, angle of attack, and angle of side slip are crucial to its safety, especially when flying close to the stall regime. Various solutions exist, including pitot tubes, angular vanes, and multihole pressure probes. However, current sensors are either too heavy (>30 g) or require large airspeeds (>20 m/s), making them unsuitable for small uncrewed aerial vehicles. We propose a novel multihole pressure probe, integrating sensing electronics in a single-component structure, resulting in a mechanically robust and lightweight sensor (9 g), which we released to the public domain. Since there is no consensus on two critical design parameters, tip shape (conical vs spherical) and hole spacing (distance between holes), we provide a study on measurement accuracy and noise generation using wind tunnel experiments. The sensor is calibrated using a multivariate polynomial regression model over an airspeed range of 3-27 m/s and an angle of attack/sideslip range of +-35°, achieving a mean absolute error of 0.44 m/s and 0.16°. Finally, we validated the sensor in outdoor flights near the stall regime. Our probe enabled accurate estimations of airspeed, angle of attack and sideslip during different acrobatic manoeuvres. Due to its size and weight, this sensor will enable safe flight for lightweight, uncrewed aerial vehicles flying at low speeds close to the stall regime.", 'abstract_zh': '一种新型多孔压力探头及其在轻小型无人机低速临近失速飞行中的气动参数精确测量与验证', 'title_zh': '轻型航空器宽速域和角范围微型多孔气流传感器'}
{'arxiv_id': 'arXiv:2505.03296', 'title': 'The Unreasonable Effectiveness of Discrete-Time Gaussian Process Mixtures for Robot Policy Learning', 'authors': 'Jan Ole von Hartz, Adrian Röfer, Joschka Boedecker, Abhinav Valada', 'link': 'https://arxiv.org/abs/2505.03296', 'abstract': 'We present Mixture of Discrete-time Gaussian Processes (MiDiGap), a novel approach for flexible policy representation and imitation learning in robot manipulation. MiDiGap enables learning from as few as five demonstrations using only camera observations and generalizes across a wide range of challenging tasks. It excels at long-horizon behaviors such as making coffee, highly constrained motions such as opening doors, dynamic actions such as scooping with a spatula, and multimodal tasks such as hanging a mug. MiDiGap learns these tasks on a CPU in less than a minute and scales linearly to large datasets. We also develop a rich suite of tools for inference-time steering using evidence such as collision signals and robot kinematic constraints. This steering enables novel generalization capabilities, including obstacle avoidance and cross-embodiment policy transfer. MiDiGap achieves state-of-the-art performance on diverse few-shot manipulation benchmarks. On constrained RLBench tasks, it improves policy success by 76 percentage points and reduces trajectory cost by 67%. On multimodal tasks, it improves policy success by 48 percentage points and increases sample efficiency by a factor of 20. In cross-embodiment transfer, it more than doubles policy success. We make the code publicly available at this https URL.', 'abstract_zh': '我们提出了一种新的机器人操控策略表示和模仿学习方法——离散时间高斯过程混合模型（MiDiGap），该方法能够从少量（仅五次）演示中学习并通过相机观测进行泛化，适用于一系列具有挑战性的任务。MiDiGap 在长期行为、高度受限动作、动态动作以及多模态任务中表现出色。MiDiGap 可在 CPU 上少于一分钟内学习这些任务，并线性扩展到大量数据集。我们还开发了一套丰富的工具，在推理时使用碰撞信号和机器人运动学约束进行控制，这使MiDiGap具备了新颖的泛化能力，包括避障和跨体模策略转移。在多种少样本操控基准测试中，MiDiGap 达到了最先进的性能。在受限RLBench任务中，它将策略成功率提高了76个百分点，并将轨迹成本降低了67%。在多模态任务中，它将策略成功率提高了48个百分点，提高了20倍的样本效率。在跨体模转移中，它将策略成功率翻了一番。源代码已在此网址公开。', 'title_zh': '离散时间高斯过程混合模型在机器人策略学习中的不合常规的有效性'}
{'arxiv_id': 'arXiv:2505.03283', 'title': 'Enabling Robots to Autonomously Search Dynamic Cluttered Post-Disaster Environments', 'authors': 'Karlo Rado, Mirko Baglioni, Anahita Jamshidnejad', 'link': 'https://arxiv.org/abs/2505.03283', 'abstract': 'Robots will bring search and rescue (SaR) in disaster response to another level, in case they can autonomously take over dangerous SaR tasks from humans. A main challenge for autonomous SaR robots is to safely navigate in cluttered environments with uncertainties, while avoiding static and moving obstacles. We propose an integrated control framework for SaR robots in dynamic, uncertain environments, including a computationally efficient heuristic motion planning system that provides a nominal (assuming there are no uncertainties) collision-free trajectory for SaR robots and a robust motion tracking system that steers the robot to track this reference trajectory, taking into account the impact of uncertainties. The control architecture guarantees a balanced trade-off among various SaR objectives, while handling the hard constraints, including safety. The results of various computer-based simulations, presented in this paper, showed significant out-performance (of up to 42.3%) of the proposed integrated control architecture compared to two commonly used state-of-the-art methods (Rapidly-exploring Random Tree and Artificial Potential Function) in reaching targets (e.g., trapped victims in SaR) safely, collision-free, and in the shortest possible time.', 'abstract_zh': '机器人将在灾害救援响应中将搜索与救援（SaR）提升到一个新的水平，前提是它们能够自主接管危险的SaR任务。自主SaR机器人的主要挑战是在具有不确定性且杂乱的环境中安全导航，同时避免静态和移动障碍物。我们提出了一种集成控制框架，用于动态、不确定环境中的SaR机器人，包括一个计算高效的启发式运动规划系统，该系统为SaR机器人提供了一个名义上的（假设没有不确定性）无碰撞轨迹，并且包括一个鲁棒运动跟踪系统，该系统引导机器人跟踪参考轨迹，同时考虑不确定性的影响。该控制架构保证在处理各种SaR目标的同时实现平衡的权衡，并满足包括安全在内的严格约束。本文中提出的各种计算机仿真的结果表明，与两种常用的先进方法（快速扩展随机树和人工势场方法）相比，提出的集成控制架构在安全、无碰撞和最短时间到达目标（如SaR中的被困受害者）方面表现出显著的优越性（最高达42.3%）。', 'title_zh': '使机器人能够自主搜索动态杂乱灾害后环境'}
{'arxiv_id': 'arXiv:2505.03238', 'title': 'RobotxR1: Enabling Embodied Robotic Intelligence on Large Language Models through Closed-Loop Reinforcement Learning', 'authors': 'Liam Boyle, Nicolas Baumann, Paviththiren Sivasothilingam, Michele Magno, Luca Benini', 'link': 'https://arxiv.org/abs/2505.03238', 'abstract': 'Future robotic systems operating in real-world environments will require on-board embodied intelligence without continuous cloud connection, balancing capabilities with constraints on computational power and memory. This work presents an extension of the R1-zero approach, which enables the usage of low parameter-count Large Language Models (LLMs) in the robotic domain. The R1-Zero approach was originally developed to enable mathematical reasoning in LLMs using static datasets. We extend it to the robotics domain through integration in a closed-loop Reinforcement Learning (RL) framework. This extension enhances reasoning in Embodied Artificial Intelligence (Embodied AI) settings without relying solely on distillation of large models through Supervised Fine-Tuning (SFT). We show that small-scale LLMs can achieve effective reasoning performance by learning through closed-loop interaction with their environment, which enables tasks that previously required significantly larger models. In an autonomous driving setting, a performance gain of 20.2%-points over the SFT-based baseline is observed with a Qwen2.5-1.5B model. Using the proposed training procedure, Qwen2.5-3B achieves a 63.3% control adaptability score, surpassing the 58.5% obtained by the much larger, cloud-bound GPT-4o. These results highlight that practical, on-board deployment of small LLMs is not only feasible but can outperform larger models if trained through environmental feedback, underscoring the importance of an interactive learning framework for robotic Embodied AI, one grounded in practical experience rather than static supervision.', 'abstract_zh': '未来在现实世界环境中运行的机器人系统将需要在不依赖连续云连接的情况下具备嵌入式智能，平衡计算能力和内存约束下的能力。本文扩展了R1-zero方法，使其能够在机器人领域使用低参数量大型语言模型（LLMs）。R1-Zero方法最初是为了解决在静态数据集上使用大语言模型进行数学推理的问题。通过将其集成到闭环强化学习（RL）框架中，我们将其扩展到机器人领域。此扩展在无需依赖通过监督微调（SFT）缩小大模型的基础上增强了嵌入式人工智能（Embodied AI）环境中的推理能力。实验表明，通过与环境的闭环交互学习，小型语言模型可以实现有效的推理性能，并能够执行先前需要更大模型的任务。在自动驾驶设置中，Qwen2.5-1.5B模型相较于基于SFT的基线模型性能提高了20.2%。使用建议的训练程序，Qwen2.5-3B实现了63.3%的控制适应性得分，超过了更大且依赖云服务的GPT-4o所获得的58.5%。这些结果表明，在实践中将小型LLMs部署在机器人嵌入式智能中不仅是可行的，而且通过环境反馈进行训练可以超越更大模型，突显了闭环学习框架的重要性，这一框架应基于实际经验而非静态监督。', 'title_zh': 'RobotxR1：通过闭环强化学习在大规模语言模型中实现机器人的体験智能'}
{'arxiv_id': 'arXiv:2505.03233', 'title': 'GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data', 'authors': 'Shengliang Deng, Mi Yan, Songlin Wei, Haixin Ma, Yuxin Yang, Jiayi Chen, Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, Heming Cui, Zhizheng Zhang, He Wang', 'link': 'https://arxiv.org/abs/2505.03233', 'abstract': "Embodied foundation models are gaining increasing attention for their zero-shot generalization, scalability, and adaptability to new tasks through few-shot post-training. However, existing models rely heavily on real-world data, which is costly and labor-intensive to collect. Synthetic data offers a cost-effective alternative, yet its potential remains largely underexplored. To bridge this gap, we explore the feasibility of training Vision-Language-Action models entirely with large-scale synthetic action data. We curate SynGrasp-1B, a billion-frame robotic grasping dataset generated in simulation with photorealistic rendering and extensive domain randomization. Building on this, we present GraspVLA, a VLA model pretrained on large-scale synthetic action data as a foundational model for grasping tasks. GraspVLA integrates autoregressive perception tasks and flow-matching-based action generation into a unified Chain-of-Thought process, enabling joint training on synthetic action data and Internet semantics data. This design helps mitigate sim-to-real gaps and facilitates the transfer of learned actions to a broader range of Internet-covered objects, achieving open-vocabulary generalization in grasping. Extensive evaluations across real-world and simulation benchmarks demonstrate GraspVLA's advanced zero-shot generalizability and few-shot adaptability to specific human preferences. We will release SynGrasp-1B dataset and pre-trained weights to benefit the community.", 'abstract_zh': '基于体态的本体模型因其实现零样本泛化、扩展性和通过少样本后训练适应新任务的能力而日益受到关注。然而，现有模型高度依赖真实世界数据，收集这些数据既昂贵又耗时。合成数据提供了一种成本效益较高的替代方案，但其潜力尚未得到充分探索。为解决这一问题，我们探索了使用大规模合成动作数据完全训练视觉-语言-行动模型的可能性。我们编纂了SynGrasp-1B，这是一个在模拟中通过照片写实渲染和广泛领域随机化生成的一百亿帧的机器人抓取数据集。在此基础上，我们提出了GraspVLA，这是一种基于大规模合成动作数据预训练的视觉-语言-行动模型，作为抓取任务的基础模型。GraspVLA 将自回归感知任务和流动匹配基于的动作生成统一到一个思考链过程中，使其能够在合成动作数据和互联网语义数据上进行联合训练。这种设计有助于缩小模拟与现实之间的差距，并促进学习到的动作转移到更广泛的互联网覆盖对象上，实现开放词汇的抓取泛化。针对现实世界和模拟基准的广泛评估展示了GraspVLA 高级的零样本泛化能力和特定人类偏好的少样本适应能力。我们将发布SynGrasp-1B 数据集和预训练权重以造福社区。', 'title_zh': 'GraspVLA：一种预训练在十亿规模合成行动数据上的抓取基础模型'}
{'arxiv_id': 'arXiv:2505.03174', 'title': 'Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets', 'authors': 'Guillermo Roque, Erika Maquiling, Jose Giovanni Tapia Lopez, Ross Greer', 'link': 'https://arxiv.org/abs/2505.03174', 'abstract': 'Instruction-Action (IA) data pairs are valuable for training robotic systems, especially autonomous vehicles (AVs), but having humans manually annotate this data is costly and time-inefficient. This paper explores the potential of using mobile application Global Positioning System (GPS) references and Natural Language Processing (NLP) to automatically generate large volumes of IA commands and responses without having a human generate or retroactively tag the data. In our pilot data collection, by driving to various destinations and collecting voice instructions from GPS applications, we demonstrate a means to collect and categorize the diverse sets of instructions, further accompanied by video data to form complete vision-language-action triads. We provide details on our completely automated data collection prototype system, ADVLAT-Engine. We characterize collected GPS voice instructions into eight different classifications, highlighting the breadth of commands and referentialities available for curation from freely available mobile applications. Through research and exploration into the automation of IA data pairs using GPS references, the potential to increase the speed and volume at which high-quality IA datasets are created, while minimizing cost, can pave the way for robust vision-language-action (VLA) models to serve tasks in vision-language navigation (VLN) and human-interactive autonomous systems.', 'abstract_zh': '基于GPS参考和自然语言处理的指令-行动数据对自动生成研究', 'title_zh': '基于GPS与NLP的自动数据整理方法生成指令-操作对以构建自主车辆视觉-语言导航数据集'}
{'arxiv_id': 'arXiv:2505.03159', 'title': 'Systematic Evaluation of Initial States and Exploration-Exploitation Strategies in PID Auto-Tuning: A Framework-Driven Approach Applied on Mobile Robots', 'authors': 'Zaid Ghazal, Ali Al-Bustami, Khouloud Gaaloul, Jaerock Kwon', 'link': 'https://arxiv.org/abs/2505.03159', 'abstract': 'PID controllers are widely used in control systems because of their simplicity and effectiveness. Although advanced optimization techniques such as Bayesian Optimization and Differential Evolution have been applied to address the challenges of automatic tuning of PID controllers, the influence of initial system states on convergence and the balance between exploration and exploitation remains underexplored. Moreover, experimenting the influence directly on real cyber-physical systems such as mobile robots is crucial for deriving realistic insights. In the present paper, a novel framework is introduced to evaluate the impact of systematically varying these factors on the PID auto-tuning processes that utilize Bayesian Optimization and Differential Evolution. Testing was conducted on two distinct PID-controlled robotic platforms, an omnidirectional robot and a differential drive mobile robot, to assess the effects on convergence rate, settling time, rise time, and overshoot percentage. As a result, the experimental outcomes yield evidence on the effects of the systematic variations, thereby providing an empirical basis for future research studies in the field.', 'abstract_zh': '一种新型框架：评估系统变化因素对基于贝叶斯优化和差分进化的PID自调谐过程的影响', 'title_zh': '基于框架驱动方法对PID自动 tuning初始状态和探索-利用策略的系统性评估：应用于移动机器人'}
{'arxiv_id': 'arXiv:2505.03146', 'title': 'Learn to Swim: Data-Driven LSTM Hydrodynamic Model for Quadruped Robot Gait Optimization', 'authors': 'Fei Han, Pengming Guo, Hao Chen, Weikun Li, Jingbo Ren, Naijun Liu, Ning Yang, Dixia Fan', 'link': 'https://arxiv.org/abs/2505.03146', 'abstract': "This paper presents a Long Short-Term Memory network-based Fluid Experiment Data-Driven model (FED-LSTM) for predicting unsteady, nonlinear hydrodynamic forces on the underwater quadruped robot we constructed. Trained on experimental data from leg force and body drag tests conducted in both a recirculating water tank and a towing tank, FED-LSTM outperforms traditional Empirical Formulas (EF) commonly used for flow prediction over flat surfaces. The model demonstrates superior accuracy and adaptability in capturing complex fluid dynamics, particularly in straight-line and turning-gait optimizations via the NSGA-II algorithm. FED-LSTM reduces deflection errors during straight-line swimming and improves turn times without increasing the turning radius. Hardware experiments further validate the model's precision and stability over EF. This approach provides a robust framework for enhancing the swimming performance of legged robots, laying the groundwork for future advances in underwater robotic locomotion.", 'abstract_zh': '基于长短期记忆网络的流体实验数据驱动模型（FED-LSTM）用于预测我们构建的水下四足机器人不稳态、非线性水动力力', 'title_zh': '学会游泳：基于数据驱动的LSTM水动力模型在四足机器人步态优化中的应用'}
{'arxiv_id': 'arXiv:2505.03128', 'title': 'HCOA*: Hierarchical Class-ordered A* for Navigation in Semantic Environments', 'authors': 'Evangelos Psomiadis, Panagiotis Tsiotras', 'link': 'https://arxiv.org/abs/2505.03128', 'abstract': 'This paper addresses the problem of robot navigation in mixed geometric and semantic 3D environments. Given a hierarchical representation of the environment, the objective is to navigate from a start position to a goal while minimizing the computational cost. We introduce Hierarchical Class-ordered A* (HCOA*), an algorithm that leverages the environmental hierarchy for efficient path-planning in semantic graphs, significantly reducing computational effort. We use a total order over the semantic classes and prove theoretical performance guarantees for the algorithm. We propose two approaches for higher-layer node classification based on the node semantics of the lowest layer: a Graph Neural Network-based method and a Majority-Class method. We evaluate our approach through simulations on a 3D Scene Graph (3DSG), comparing it to the state-of-the-art and assessing its performance against our classification approaches. Results show that HCOA* can find the optimal path while reducing the number of expanded nodes by 25% and achieving a 16% reduction in computational time on the uHumans2 3DSG dataset.', 'abstract_zh': '本文探讨了在混合几何和语义3D环境下的机器人导航问题。给定环境的层次化表示，目标是从起始位置导航到目标位置同时最小化计算成本。我们引入了层次类别有序A* (HCOA*)算法，该算法利用环境层次信息进行语义图中的高效路径规划，显著降低计算开销。我们使用语义类的全序，并证明了该算法的理论性能保证。我们提出了两种基于低层节点语义的高层节点分类方法：基于图神经网络的方法和多数类方法。我们通过在3D场景图（3DSG）上的仿真实验评估了该方法，并将其与最先进的方法进行了比较，同时评估了其性能对分类方法的影响。结果表明，HCOA*能够找到最优路径，并在uHumans2 3DSG数据集中将展开节点数减少25%，同时计算时间减少16%。', 'title_zh': 'HCOA*: 分层类序A*在语义环境中的导航算法'}
{'arxiv_id': 'arXiv:2505.03087', 'title': 'Fabrication and Characterization of Additively Manufactured Stretchable Strain Sensors Towards the Shape Sensing of Continuum Robots', 'authors': 'Daniel C. Moyer, Wenpeng Wang, Logan S. Karschner, Loris Fichera, Pratap M. Rao', 'link': 'https://arxiv.org/abs/2505.03087', 'abstract': 'This letter describes the manufacturing and experimental characterization of novel stretchable strain sensors for continuum robots. The overarching goal of this research is to provide a new solution for the shape sensing of these devices. The sensors are fabricated via direct ink writing, an extrusion-based additive manufacturing technique. Electrically conductive material (i.e., the \\textit{ink}) is printed into traces whose electrical resistance varies in response to mechanical deformation. The principle of operation of stretchable strain sensors is analogous to that of conventional strain gauges, but with a significantly larger operational window thanks to their ability to withstand larger strain. Among the different conductive materials considered for this study, we opted to fabricate the sensors with a high-viscosity eutectic Gallium-Indium ink, which in initial testing exhibited high linearity ($R^2 \\approx$ 0.99), gauge factor $\\approx$ 1, and negligible drift. Benefits of the proposed sensors include (i) ease of fabrication, as they can be conveniently printed in a matter of minutes; (ii) ease of installation, as they can simply be glued to the outside body of a robot; (iii) ease of miniaturization, which enables integration into millimiter-sized continuum robots.', 'abstract_zh': '这种信件描述了新型可拉伸应变传感器的制造与实验表征，这些传感器用于连续机器人。本研究的总体目标是为这些设备提供新的形状感知解决方案。传感器通过直接墨水书写制造，这是一种基于挤出的增材制造技术。电导材料（即“墨水”）被打印成痕迹，其电气电阻会根据机械变形进行变化。可拉伸应变传感器的工作原理类似于传统的应变片，但由于其能够承受更大的应变，因此具有显著更大的工作窗口。在为本研究考虑的不同导电材料中，我们选择使用高黏度共晶镓铟墨水来制造传感器，在初始测试中，该传感器表现出高线性度（$R^2 \\approx$ 0.99）、约1的灵敏度因子和可忽略不计的漂移。所提出传感器的优点包括：（i）易于制造，可以在几分钟内方便地打印；（ii）易于安装，可以直接粘贴在机器人外部；（iii）易于微型化，使其能够集成到毫米级的连续机器人中。', 'title_zh': '基于增材制造的可拉伸应变传感器制备及其在连续机器人形态感知中的应用'}
{'arxiv_id': 'arXiv:2505.03077', 'title': 'Latent Adaptive Planner for Dynamic Manipulation', 'authors': 'Donghun Noh, Deqian Kong, Minglu Zhao, Andrew Lizarraga, Jianwen Xie, Ying Nian Wu, Dennis Hong', 'link': 'https://arxiv.org/abs/2505.03077', 'abstract': 'This paper presents Latent Adaptive Planner (LAP), a novel approach for dynamic nonprehensile manipulation tasks that formulates planning as latent space inference, effectively learned from human demonstration videos. Our method addresses key challenges in visuomotor policy learning through a principled variational replanning framework that maintains temporal consistency while efficiently adapting to environmental changes. LAP employs Bayesian updating in latent space to incrementally refine plans as new observations become available, striking an optimal balance between computational efficiency and real-time adaptability. We bridge the embodiment gap between humans and robots through model-based proportional mapping that regenerates accurate kinematic-dynamic joint states and object positions from human demonstrations. Experimental evaluations across multiple complex manipulation benchmarks demonstrate that LAP achieves state-of-the-art performance, outperforming existing approaches in success rate, trajectory smoothness, and energy efficiency, particularly in dynamic adaptation scenarios. Our approach enables robots to perform complex interactions with human-like adaptability while providing an expandable framework applicable to diverse robotic platforms using the same human demonstration videos.', 'abstract_zh': '本文提出了一种新颖的方法——潜在自适应规划器（LAP），该方法将规划形式化为潜在空间推断，有效地从人类演示视频中学习。我们的方法通过一个原则性的变分重规划框架解决了visuomotor策略学习中的关键挑战，该框架在保持时间一致性的同时高效地适应环境变化。LAP通过潜在空间中的贝叶斯更新来逐步细化计划，随着新观测数据的不断可用，实现了计算效率与实时适应性的最佳平衡。通过基于模型的比例映射，我们跨越了人类与机器人之间的体化差距，能够从人类演示中再生出精确的动力学关节状态和物体位置。在多个复杂操作基准上的实验评估表明，LAP在成功率、轨迹平滑度和能源效率方面表现出卓越性能，特别是在动态适应场景中优于现有方法。我们的方法使机器人能够以类似人类的适应性执行复杂交互，同时提供了一个可扩展的框架，可以使用相同的人类演示视频适用于多种机器人平台。', 'title_zh': '潜适应规划器for动态操作'}
{'arxiv_id': 'arXiv:2505.03046', 'title': 'Sim2Real Transfer for Vision-Based Grasp Verification', 'authors': 'Pau Amargant, Peter Hönig, Markus Vincze', 'link': 'https://arxiv.org/abs/2505.03046', 'abstract': "The verification of successful grasps is a crucial aspect of robot manipulation, particularly when handling deformable objects. Traditional methods relying on force and tactile sensors often struggle with deformable and non-rigid objects. In this work, we present a vision-based approach for grasp verification to determine whether the robotic gripper has successfully grasped an object. Our method employs a two-stage architecture; first YOLO-based object detection model to detect and locate the robot's gripper and then a ResNet-based classifier determines the presence of an object. To address the limitations of real-world data capture, we introduce HSR-GraspSynth, a synthetic dataset designed to simulate diverse grasping scenarios. Furthermore, we explore the use of Visual Question Answering capabilities as a zero-shot baseline to which we compare our model. Experimental results demonstrate that our approach achieves high accuracy in real-world environments, with potential for integration into grasping pipelines. Code and datasets are publicly available at this https URL .", 'abstract_zh': '基于视觉的抓取验证在机器人操作中的研究：一种用于确定机器人成功抓取物体的方法', 'title_zh': '基于视觉的抓取验证的Sim2Real迁移学习'}
{'arxiv_id': 'arXiv:2505.03044', 'title': 'A Modal-Space Formulation for Momentum Observer Contact Estimation and Effects of Uncertainty for Continuum Robots', 'authors': 'Garrison L.H. Johnston, Neel Shihora, Nabil Simaan', 'link': 'https://arxiv.org/abs/2505.03044', 'abstract': "Contact detection for continuum and soft robots has been limited in past works to statics or kinematics-based methods with assumed circular bending curvature or known bending profiles. In this paper, we adapt the generalized momentum observer contact estimation method to continuum robots. This is made possible by leveraging recent results for real-time shape sensing of continuum robots along with a modal-space representation of the robot dynamics. In addition to presenting an approach for estimating the generalized forces due to contact via a momentum observer, we present a constrained optimization method to identify the wrench imparted on the robot during contact. We also present an approach for investigating the effects of unmodeled deviations in the robot's dynamic state on the contact detection method and we validate our algorithm by simulations and experiments. We also compare the performance of the momentum observer to the joint force deviation method, a direct estimation approach using the robot's full dynamic model. We also demonstrate a basic extension of the method to multisegment continuum robots. Results presented in this work extend dynamic contact detection to the domain of continuum and soft robots and can be used to improve the safety of large-scale continuum robots for human-robot collaboration.", 'abstract_zh': '连续体和软体机器人中的接触检测在过去的研究中主要局限于基于静态或运动学的方法，这些方法假定弯曲曲率为圆形或已知弯曲轮廓。本文中，我们通过利用连续体机器人实时形状感知的最新结果以及机器人动力学的模态空间表示，将广义动量观察器接触估计方法应用于连续体机器人。除了提出一种通过动量观察器估计接触引起的广义力的方法外，我们还提出了一种约束优化方法，用于识别接触期间施加在机器人上的 wrench。此外，我们提出了一种方法来研究机器人动力学状态中未建模偏差对接触检测方法的影响，并通过仿真和实验验证了我们的算法。我们还将动量观察器的性能与关节力偏差方法进行了比较，后者是一种直接使用机器人完整动力学模型的估计方法。我们还展示了该方法的基本扩展，适用于多节段的连续体机器人。本文中提出的结果将动态接触检测扩展到了连续体和软体机器人的领域，可以用于提高大规模连续体机器人在人机协作中的安全性。', 'title_zh': '模态空间表示法在连续机器人接触估测中的动量观测器及其不确定性影响研究'}
{'arxiv_id': 'arXiv:2505.03035', 'title': 'MORE: Mobile Manipulation Rearrangement Through Grounded Language Reasoning', 'authors': 'Mohammad Mohammadi, Daniel Honerkamp, Martin Büchner, Matteo Cassinelli, Tim Welschehold, Fabien Despinoy, Igor Gilitschenski, Abhinav Valada', 'link': 'https://arxiv.org/abs/2505.03035', 'abstract': 'Autonomous long-horizon mobile manipulation encompasses a multitude of challenges, including scene dynamics, unexplored areas, and error recovery. Recent works have leveraged foundation models for scene-level robotic reasoning and planning. However, the performance of these methods degrades when dealing with a large number of objects and large-scale environments. To address these limitations, we propose MORE, a novel approach for enhancing the capabilities of language models to solve zero-shot mobile manipulation planning for rearrangement tasks. MORE leverages scene graphs to represent environments, incorporates instance differentiation, and introduces an active filtering scheme that extracts task-relevant subgraphs of object and region instances. These steps yield a bounded planning problem, effectively mitigating hallucinations and improving reliability. Additionally, we introduce several enhancements that enable planning across both indoor and outdoor environments. We evaluate MORE on 81 diverse rearrangement tasks from the BEHAVIOR-1K benchmark, where it becomes the first approach to successfully solve a significant share of the benchmark, outperforming recent foundation model-based approaches. Furthermore, we demonstrate the capabilities of our approach in several complex real-world tasks, mimicking everyday activities. We make the code publicly available at this https URL.', 'abstract_zh': '自主长期horizon移动操作涵盖多种挑战，包括场景动力学、未探索区域以及错误恢复。最近的研究利用基础模型进行场景级别的机器人推理和规划。然而，当处理大量对象和大规模环境时，这些方法的性能会下降。为解决这些限制，我们提出 MORE，一种增强语言模型能力的新方法，用于解决重新排列任务的零样本移动操作规划。MORE 利用场景图表示环境，引入实例差异化，并引入一种主动过滤方案，提取与任务相关的对象和区域实例子图。这些步骤生成了一个有界规划问题，有效地减轻了幻觉并提高了可靠性。此外，我们引入了若干增强功能，使规划能够跨越室内外环境。我们在BEHAVIOR-1K基准测试中的81种多样重新排列任务上评估MORE，该方法成为第一个成功解决基准测试中显著份额的方法，优于最近的基础模型方法。此外，我们展示了我们方法在几种复杂的现实世界任务中的能力，模拟日常活动。代码已公开发布在该网址：https://。', 'title_zh': '基于地面语言推理的移动 manipulator 排列重排'}
{'arxiv_id': 'arXiv:2505.02915', 'title': 'Zero-shot Sim2Real Transfer for Magnet-Based Tactile Sensor on Insertion Tasks', 'authors': 'Beining Han, Abhishek Joshi, Jia Deng', 'link': 'https://arxiv.org/abs/2505.02915', 'abstract': 'Tactile sensing is an important sensing modality for robot manipulation. Among different types of tactile sensors, magnet-based sensors, like u-skin, balance well between high durability and tactile density. However, the large sim-to-real gap of tactile sensors prevents robots from acquiring useful tactile-based manipulation skills from simulation data, a recipe that has been successful for achieving complex and sophisticated control policies. Prior work has implemented binarization techniques to bridge the sim-to-real gap for dexterous in-hand manipulation. However, binarization inherently loses much information that is useful in many other tasks, e.g., insertion. In our work, we propose GCS, a novel sim-to-real technique to learn contact-rich skills with dense, distributed, 3-axis tactile readings. We evaluate our approach on blind insertion tasks and show zero-shot sim-to-real transfer of RL policies with raw tactile reading as input.', 'abstract_zh': '基于触觉感知的机器人 manipulation 是一个重要传感模态。在不同类型的触觉传感器中，像 u-skin 这样的基于磁性的传感器在耐用性和触觉密度之间取得了良好的平衡。然而，触觉传感器中的仿真到现实的巨大差距阻碍了机器人通过仿真数据获取有用的基于触觉的 manipulation 技能，这一点在实现复杂和高级的控制策略方面已被证明是有效的。先前的工作通过实现二值化技术来解决灵巧的在手 manipulation 中的仿真到现实差距。然而，二值化会内在地丢失许多在其他任务中很重要的信息，例如插入。在我们的工作中，我们提出了 GCS，这是一种新颖的仿真到现实的技术，用于学习富含接触的技能，这些技能具有密集且分布在三轴触觉读取。我们在盲插入任务上评估了该方法，并展示了使用原始触觉读取作为输入的 RL 政策的零样本仿真到现实转移。', 'title_zh': '基于磁传感器的插入任务零样本Sim2Real迁移学习'}
{'arxiv_id': 'arXiv:2505.03692', 'title': 'Matching Distance and Geometric Distribution Aided Learning Multiview Point Cloud Registration', 'authors': 'Shiqi Li, Jihua Zhu, Yifan Xie, Naiwen Hu, Di Wang', 'link': 'https://arxiv.org/abs/2505.03692', 'abstract': 'Multiview point cloud registration plays a crucial role in robotics, automation, and computer vision fields. This paper concentrates on pose graph construction and motion synchronization within multiview registration. Previous methods for pose graph construction often pruned fully connected graphs or constructed sparse graph using global feature aggregated from local descriptors, which may not consistently yield reliable results. To identify dependable pairs for pose graph construction, we design a network model that extracts information from the matching distance between point cloud pairs. For motion synchronization, we propose another neural network model to calculate the absolute pose in a data-driven manner, rather than optimizing inaccurate handcrafted loss functions. Our model takes into account geometric distribution information and employs a modified attention mechanism to facilitate flexible and reliable feature interaction. Experimental results on diverse indoor and outdoor datasets confirm the effectiveness and generalizability of our approach. The source code is available at this https URL.', 'abstract_zh': '多视图点云配准在机器人学、自动化和计算机视觉领域中起着关键作用。本文集中于多视图配准中的姿态图构造和运动同步。现有的姿态图构造方法通常会剪枝全连接图或将稀疏图构建基于局部描述子的全局特征聚合，这可能无法稳定地产生可靠的结果。为识别用于姿态图构造的可信配对，我们设计了一个网络模型，从中提取点云配对之间的匹配距离信息。对于运动同步，我们提出了一种基于数据驱动的方法来计算绝对姿态，而不是优化不准确的手工设计损失函数。我们的模型考虑了几何分布信息，并采用了修改后的注意机制来促进灵活可靠的特征交互。在多种室内和室外数据集上的实验结果证实了我们方法的有效性和泛化能力。相关源代码可在此处获取。', 'title_zh': '基于匹配距离和几何分布辅助的多视点点云配准学习'}
{'arxiv_id': 'arXiv:2505.03539', 'title': 'Panoramic Out-of-Distribution Segmentation', 'authors': 'Mengfei Duan, Kailun Yang, Yuheng Zhang, Yihong Cao, Fei Teng, Kai Luo, Jiaming Zhang, Zhiyong Li, Shutao Li', 'link': 'https://arxiv.org/abs/2505.03539', 'abstract': 'Panoramic imaging enables capturing 360° images with an ultra-wide Field-of-View (FoV) for dense omnidirectional perception. However, current panoramic semantic segmentation methods fail to identify outliers, and pinhole Out-of-distribution Segmentation (OoS) models perform unsatisfactorily in the panoramic domain due to background clutter and pixel distortions. To address these issues, we introduce a new task, Panoramic Out-of-distribution Segmentation (PanOoS), achieving OoS for panoramas. Furthermore, we propose the first solution, POS, which adapts to the characteristics of panoramic images through text-guided prompt distribution learning. Specifically, POS integrates a disentanglement strategy designed to materialize the cross-domain generalization capability of CLIP. The proposed Prompt-based Restoration Attention (PRA) optimizes semantic decoding by prompt guidance and self-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL) refines the manifold of per-pixel mask embeddings via semantic prototype supervision. Besides, to compensate for the scarcity of PanOoS datasets, we establish two benchmarks: DenseOoS, which features diverse outliers in complex environments, and QuadOoS, captured by a quadruped robot with a panoramic annular lens system. Extensive experiments demonstrate superior performance of POS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS, outperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves leading closed-set segmentation capabilities. Code and datasets will be available at this https URL.', 'abstract_zh': '全景影像 Enables 360° 图像的超广视野密集全方位感知。然而，现有的全景语义分割方法无法识别异常值，而针孔相机域外分割模型由于背景杂乱和像素失真，在全景域表现不佳。为解决这些问题，我们引入了一个新的任务——全景域外分割（PanOoS），以实现全景域的域外分割。此外，我们提出了第一个解决方案——POS，通过基于文本的提示分布学习适应全景图像的特性。具体来说，POS 结合了一种脱离纠缠策略，旨在实现 CLIP 的跨域泛化能力。提出的基于提示的恢复注意力（PRA）通过对提示引导和自适应校正优化语义解码，而双层提示分布学习（BPDL）通过语义原型监督细化每个像素掩码嵌入流形。另外，为弥补 PanOoS 数据集的稀缺性，我们建立了两个基准： DenseOoS，其中包含复杂环境中的多种异常值；以及由四足机器人使用全景环形镜头系统拍摄的 QuadOoS。大量实验证明了 POS 的优越性能，在 DenseOoS 上，AuPRC 提高了 34.25%，FPR95 降低了 21.42%，超越了最先进的针孔域外分割方法。此外，POS 达到了领先的数据闭集分割能力。代码和数据集将在此链接提供。', 'title_zh': '全景分布外分割'}
{'arxiv_id': 'arXiv:2505.03512', 'title': 'Artificial Protozoa Optimizer (APO): A novel bio-inspired metaheuristic algorithm for engineering optimization', 'authors': 'Xiaopeng Wang, Vaclav Snasel, Seyedali Mirjalili, Jeng-Shyang Pan, Lingping Kong, Hisham A. Shehadeh', 'link': 'https://arxiv.org/abs/2505.03512', 'abstract': 'This study proposes a novel artificial protozoa optimizer (APO) that is inspired by protozoa in nature. The APO mimics the survival mechanisms of protozoa by simulating their foraging, dormancy, and reproductive behaviors. The APO was mathematically modeled and implemented to perform the optimization processes of metaheuristic algorithms. The performance of the APO was verified via experimental simulations and compared with 32 state-of-the-art algorithms. Wilcoxon signed-rank test was performed for pairwise comparisons of the proposed APO with the state-of-the-art algorithms, and Friedman test was used for multiple comparisons. First, the APO was tested using 12 functions of the 2022 IEEE Congress on Evolutionary Computation benchmark. Considering practicality, the proposed APO was used to solve five popular engineering design problems in a continuous space with constraints. Moreover, the APO was applied to solve a multilevel image segmentation task in a discrete space with constraints. The experiments confirmed that the APO could provide highly competitive results for optimization problems. The source codes of Artificial Protozoa Optimizer are publicly available at this https URL and this https URL.', 'abstract_zh': '本研究提出了一种新型的人工原生动物优化器（APO），该优化器受自然界原生动物的行为启发。APO通过模拟原生动物的觅食、休眠和生殖行为来模仿其生存机制。APO通过数学建模并实现为元启发式算法的优化过程。通过实验仿真验证了APO的性能，并将其与32种最先进的算法进行了比较。Wilcoxon符号秩检验用于比较所提出的APO与最先进的算法，Friedman检验用于多重比较。首先，APO使用2022年IEEE演化计算大会基准测试中的12个函数进行了测试。考虑到实用性，提出的APO被用于解决连续空间中有约束条件的五个流行工程设计问题。此外，APO还应用于解决离散空间中有约束条件的多级图像分割任务。实验结果证实，APO在优化问题上可以提供具有很强竞争力的结果。人工原生动物优化器的源代码可从此处和此处公开获取。', 'title_zh': '人工原生动物优化算法（APO）：一种新型生物启发式元启发算法在工程优化中的应用'}
{'arxiv_id': 'arXiv:2505.03422', 'title': 'LiftFeat: 3D Geometry-Aware Local Feature Matching', 'authors': 'Yepeng Liu, Wenpeng Lai, Zhou Zhao, Yuxuan Xiong, Jinchi Zhu, Jun Cheng, Yongchao Xu', 'link': 'https://arxiv.org/abs/2505.03422', 'abstract': 'Robust and efficient local feature matching plays a crucial role in applications such as SLAM and visual localization for robotics. Despite great progress, it is still very challenging to extract robust and discriminative visual features in scenarios with drastic lighting changes, low texture areas, or repetitive patterns. In this paper, we propose a new lightweight network called \\textit{LiftFeat}, which lifts the robustness of raw descriptor by aggregating 3D geometric feature. Specifically, we first adopt a pre-trained monocular depth estimation model to generate pseudo surface normal label, supervising the extraction of 3D geometric feature in terms of predicted surface normal. We then design a 3D geometry-aware feature lifting module to fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D geometric feature enhances the discriminative ability of 2D feature description in extreme conditions. Extensive experimental results on relative pose estimation, homography estimation, and visual localization tasks, demonstrate that our LiftFeat outperforms some lightweight state-of-the-art methods. Code will be released at : this https URL.', 'abstract_zh': '鲁棒高效的局部特征匹配在机器人SLAM和视觉定位应用中起着关键作用。尽管取得了很大进展，但在剧烈光照变化、低纹理区域或重复模式场景中提取鲁棒性和区分性视觉特征仍极具挑战性。本文提出了一种新的轻量级网络LiftFeat，通过聚合三维几何特征增强原始描述子的鲁棒性。具体而言，我们首先采用预训练的单目深度估计模型生成伪表面法线标签，监督预测表面法线的三维几何特征提取。然后设计了一个三维几何感知特征提升模块，将表面法线特征与原始二维描述子特征融合。结合这种三维几何特征在极端条件下游提升了二维特征描述的区分能力。在相对姿态估计、仿射变换估计和视觉定位任务上的广泛实验结果表明，我们的LiftFeat在某些轻量级的最先进的方法中表现出色。代码将在以下链接发布：this https URL。', 'title_zh': 'LiftFeat: 3D几何感知局部特征匹配'}
{'arxiv_id': 'arXiv:2505.03295', 'title': 'Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for Reusing Existing Libraries and Interfaces', 'authors': 'Luis Miguel Vieira da Silva, Aljosha Köcher, Nicolas König, Felix Gehlhoff, Alexander Fay', 'link': 'https://arxiv.org/abs/2505.03295', 'abstract': 'Modern automation systems increasingly rely on modular architectures, with capabilities and skills as one solution approach. Capabilities define the functions of resources in a machine-readable form and skills provide the concrete implementations that realize those capabilities. However, the development of a skill implementation conforming to a corresponding capability remains a time-consuming and challenging task. In this paper, we present a method that treats capabilities as contracts for skill implementations and leverages large language models to generate executable code based on natural language user input. A key feature of our approach is the integration of existing software libraries and interface technologies, enabling the generation of skill implementations across different target languages. We introduce a framework that allows users to incorporate their own libraries and resource interfaces into the code generation process through a retrieval-augmented generation architecture. The proposed method is evaluated using an autonomous mobile robot controlled via Python and ROS 2, demonstrating the feasibility and flexibility of the approach.', 'abstract_zh': '现代自动化系统越来越多地依赖模块化架构，能力与技能是其中一个解决方案。能力以机器可读的形式定义资源的功能，技能则提供实现这些能力的具体实现方式。然而，根据相应的能力开发技能实现仍然是一个耗时且具有挑战性的工作。本文提出了一种方法，将能力视为技能实现的契约，并利用大型语言模型根据自然语言用户输入生成可执行代码。我们方法的关键特性是整合现有的软件库和接口技术，从而能够在不同的目标语言中生成技能实现。我们提出了一种框架，通过检索增强生成架构允许用户将自己的库和资源接口纳入代码生成过程。所提出的方法通过一个由自主移动机器人（通过Python和ROS 2控制）组成的评估进行了验证，展示了该方法的可行性和灵活性。', 'title_zh': '基于RAG的 capability-driven 技能生成方法：重用现有库和接口的approach'}
{'arxiv_id': 'arXiv:2505.03284', 'title': 'OccCylindrical: Multi-Modal Fusion with Cylindrical Representation for 3D Semantic Occupancy Prediction', 'authors': 'Zhenxing Ming, Julie Stephany Berrio, Mao Shan, Yaoqi Huang, Hongyu Lyu, Nguyen Hoang Khoi Tran, Tzu-Yun Tseng, Stewart Worrall', 'link': 'https://arxiv.org/abs/2505.03284', 'abstract': "The safe operation of autonomous vehicles (AVs) is highly dependent on their understanding of the surroundings. For this, the task of 3D semantic occupancy prediction divides the space around the sensors into voxels, and labels each voxel with both occupancy and semantic information. Recent perception models have used multisensor fusion to perform this task. However, existing multisensor fusion-based approaches focus mainly on using sensor information in the Cartesian coordinate system. This ignores the distribution of the sensor readings, leading to a loss of fine-grained details and performance degradation. In this paper, we propose OccCylindrical that merges and refines the different modality features under cylindrical coordinates. Our method preserves more fine-grained geometry detail that leads to better performance. Extensive experiments conducted on the nuScenes dataset, including challenging rainy and nighttime scenarios, confirm our approach's effectiveness and state-of-the-art performance. The code will be available at: this https URL", 'abstract_zh': '自动驾驶车辆的安全运行高度依赖于其对周围环境的理解。为此，3D语义占位预测任务将传感器周围的空间划分成体素，并为每个体素标注兼具占用和语义信息的内容。最近的感知模型利用多传感器融合来执行此任务。然而，现有的基于多传感器融合的方法主要集中在使用笛卡尔坐标系中的传感器信息上。这忽视了传感器读数的分布，导致丢失了细粒度的细节并导致性能下降。本文提出OccCylindrical，在圆柱坐标系下合并和细化不同模态的特征。我们的方法保留了更多的细粒度几何细节，从而提高了性能。通过对nuScenes数据集进行广泛实验，包括挑战性的雨天和夜间场景，证实了我们方法的有效性和最先进的性能。代码将于以下链接提供：this https URL', 'title_zh': 'OccCylindrical: 基于圆柱表示的多模态融合三维语义 occupancy 预测'}
{'arxiv_id': 'arXiv:2505.03257', 'title': 'Model Predictive Fuzzy Control: A Hierarchical Multi-Agent Control Architecture for Outdoor Search-and-Rescue Robots', 'authors': 'Craig Maxwell, Mirko Baglioni, Anahita Jamshidnejad', 'link': 'https://arxiv.org/abs/2505.03257', 'abstract': 'Autonomous robots deployed in unknown search-and-rescue (SaR) environments can significantly improve the efficiency of the mission by assisting in fast localisation and rescue of the trapped victims. We propose a novel integrated hierarchical control architecture, called model predictive fuzzy control (MPFC), for autonomous mission planning of multi-robot SaR systems that should efficiently map an unknown environment: We combine model predictive control (MPC) and fuzzy logic control (FLC), where the robots are locally controlled by computationally efficient FLC controllers, and the parameters of these local controllers are tuned via a centralised MPC controller, in a regular or event-triggered manner. The proposed architecture provides three main advantages: (1) The control decisions are made by the FLC controllers, thus the real-time computation time is affordable. (2) The centralised MPC controller optimises the performance criteria with a global and predictive vision of the system dynamics, and updates the parameters of the FLC controllers accordingly. (3) FLC controllers are heuristic by nature and thus do not take into account optimality in their decisions, while the tuned parameters via the MPC controller can indirectly incorporate some level of optimality in local decisions of the robots. A simulation environment for victim detection in a disaster environment was designed in MATLAB using discrete, 2-D grid-based models. While being comparable from the point of computational efficiency, the integrated MPFC architecture improves the performance of the multi-robot SaR system compared to decentralised FLC controllers. Moreover, the performance of MPFC is comparable to the performance of centralised MPC for path planning of SaR robots, whereas MPFC requires significantly less computational resources, since the number of the optimisation variables in the control problem are reduced.', 'abstract_zh': '自主部署在未知搜索与救援环境中的机器人可以通过快速定位和救援被困受害者显著提高任务效率。我们提出了一种新颖的集成分层控制架构，称为模型预测模糊控制（MPFC），用于多机器人搜索与救援系统自主任务规划，以高效地映射未知环境。该架构结合了模型预测控制（MPC）和模糊逻辑控制（FLC），其中机器人由计算高效的FLC控制器局部控制，这些局部控制器的参数通过集中式的MPC控制器进行定期或事件触发式的调优。提出的架构具有以下三大优势：（1）控制决策由FLC控制器做出，因此实时计算时间可承受。（2）集中式的MPC控制器以全局和预测的方式优化系统动力学的性能指标，并相应地调整FLC控制器的参数。（3）FLC控制器本质上是启发式的，因此在其决策中不考虑最优性，而通过MPC控制器调优后的参数可以在一定程度上间接地将最优性纳入到机器人局部决策中。在MATLAB中使用离散的二维网格模型设计了一个用于灾后受害者检测的仿真环境。虽然在计算效率上具有可比性，但集成的MPFC架构在多机器人搜索与救援系统的性能上优于分布式FLC控制器。此外，MPFC在搜索与救援机器人路径规划上的性能与集中式MPC相当，但需要显著较少的计算资源，因为控制问题中的优化变量数量减少了。', 'title_zh': '基于模型预测模糊控制的户外搜索救援机器人分层多代理控制架构'}
{'arxiv_id': 'arXiv:2505.03178', 'title': 'RADE: Learning Risk-Adjustable Driving Environment via Multi-Agent Conditional Diffusion', 'authors': 'Jiawei Wang, Xintao Yan, Yao Mu, Haowei Sun, Zhong Cao, Henry X. Liu', 'link': 'https://arxiv.org/abs/2505.03178', 'abstract': "Generating safety-critical scenarios in high-fidelity simulations offers a promising and cost-effective approach for efficient testing of autonomous vehicles. Existing methods typically rely on manipulating a single vehicle's trajectory through sophisticated designed objectives to induce adversarial interactions, often at the cost of realism and scalability. In this work, we propose the Risk-Adjustable Driving Environment (RADE), a simulation framework that generates statistically realistic and risk-adjustable traffic scenes. Built upon a multi-agent diffusion architecture, RADE jointly models the behavior of all agents in the environment and conditions their trajectories on a surrogate risk measure. Unlike traditional adversarial methods, RADE learns risk-conditioned behaviors directly from data, preserving naturalistic multi-agent interactions with controllable risk levels. To ensure physical plausibility, we incorporate a tokenized dynamics check module that efficiently filters generated trajectories using a motion vocabulary. We validate RADE on the real-world rounD dataset, demonstrating that it preserves statistical realism across varying risk levels and naturally increases the likelihood of safety-critical events as the desired risk level grows up. Our results highlight RADE's potential as a scalable and realistic tool for AV safety evaluation.", 'abstract_zh': '在高保真模拟中生成安全关键场景为自动驾驶车辆高效测试提供了有前途且成本效益高的方法。现有方法通常依赖于通过复杂设计的目标操控单个车辆的轨迹以诱导对抗性交互，往往牺牲了真实性和可扩展性。在这项工作中，我们提出了风险可调驾驶环境（RADE），一种生成统计上现实且风险可调交通场景的模拟框架。基于多智能体扩散架构，RADE 联合建模环境中所有智能体的行为，并基于代理风险度量条件化其轨迹。与传统的对抗性方法不同，RADE 直接从数据中学习风险条件下的行为，同时保持自然的多智能体交互并具备可控的风险水平。为了确保物理可行性，我们引入了一个标记化动力学检查模块，该模块利用运动词汇表高效地过滤生成的轨迹。我们使用现实世界中的 rounD 数据集验证了 RADE，结果表明它在不同风险水平下保持了统计现实性，并且随着期望的风险水平提高，自然地增加了安全关键事件发生的可能性。我们的结果突显了 RADE 作为自动驾驶车辆安全评估可扩展且现实工具的潜力。', 'title_zh': 'RADE：通过多Agent条件扩散学习可调整风险的驾驶环境'}
{'arxiv_id': 'arXiv:2505.03088', 'title': 'Global Task-aware Fault Detection, Identification For On-Orbit Multi-Spacecraft Collaborative Inspection', 'authors': 'Akshita Gupta, Yashwanth Kumar Nakka, Changrak Choi, Amir Rahmani', 'link': 'https://arxiv.org/abs/2505.03088', 'abstract': 'In this paper, we present a global-to-local task-aware fault detection and identification algorithm to detect failures in a multi-spacecraft system performing a collaborative inspection (referred to as global) task. The inspection task is encoded as a cost functional $\\costH$ that informs global (task allocation and assignment) and local (agent-level) decision-making. The metric $\\costH$ is a function of the inspection sensor model, and the agent full-pose. We use the cost functional $\\costH$ to design a metric that compares the expected and actual performance to detect the faulty agent using a threshold. We use higher-order cost gradients $\\costH$ to derive a new metric to identify the type of fault, including task-specific sensor fault, an agent-level actuator, and sensor faults. Furthermore, we propose an approach to design adaptive thresholds for each fault mentioned above to incorporate the time dependence of the inspection task. We demonstrate the efficacy of the proposed method empirically, by simulating and detecting faults (such as inspection sensor faults, actuators, and sensor faults) in a low-Earth orbit collaborative spacecraft inspection task using the metrics and the threshold designed using the global task cost $\\costH$.', 'abstract_zh': '在多卫星系统执行协作检查任务（简称全局任务）中的一种全局到局部任务感知故障检测与识别算法', 'title_zh': '基于轨道多卫星协同检查的全局任务意识故障检测与识别'}
{'arxiv_id': 'arXiv:2505.02842', 'title': 'Evaluation of Coordination Strategies for Underground Automated Vehicle Fleets in Mixed Traffic', 'authors': 'Olga Mironenko, Hadi Banaee, Amy Loutfi', 'link': 'https://arxiv.org/abs/2505.02842', 'abstract': 'This study investigates the efficiency and safety outcomes of implementing different adaptive coordination models for automated vehicle (AV) fleets, managed by a centralized coordinator that dynamically responds to human-controlled vehicle behavior. The simulated scenarios replicate an underground mining environment characterized by narrow tunnels with limited connectivity. To address the unique challenges of such settings, we propose a novel metric - Path Overlap Density (POD) - to predict efficiency and potentially the safety performance of AV fleets. The study also explores the impact of map features on AV fleets performance. The results demonstrate that both AV fleet coordination strategies and underground tunnel network characteristics significantly influence overall system performance. While map features are critical for optimizing efficiency, adaptive coordination strategies are essential for ensuring safe operations.', 'abstract_zh': '本研究探讨了由集中协调器动态响应人类控制车辆行为的不同自适应协调模型在自动化车辆（AV）车队中的效率和安全性能。模拟场景再现了由狭窄隧道和有限连接性组成的地下采矿环境。为应对此类环境的独特挑战，我们提出了一种新的指标——路径重叠密度（POD），以预测AV车队的效率和潜在的安全性能。研究还探讨了地图特征对AV车队性能的影响。结果表明，AV车队协调策略和地下隧道网络特性显著影响整体系统性能。虽然地图特征对于优化效率至关重要，但适应性协调策略对于确保安全运营也必不可少。', 'title_zh': '地下自动车辆车队在混行交通中的协调策略评估'}
