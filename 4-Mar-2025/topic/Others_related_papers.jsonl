{'arxiv_id': 'arXiv:2503.01729', 'title': 'FLAME: A Federated Learning Benchmark for Robotic Manipulation', 'authors': 'Santiago Bou Betran, Alberta Longhini, Miguel Vasco, Yuchong Zhang, Danica Kragic', 'link': 'https://arxiv.org/abs/2503.01729', 'abstract': 'Recent progress in robotic manipulation has been fueled by large-scale datasets collected across diverse environments. Training robotic manipulation policies on these datasets is traditionally performed in a centralized manner, raising concerns regarding scalability, adaptability, and data privacy. While federated learning enables decentralized, privacy-preserving training, its application to robotic manipulation remains largely unexplored. We introduce FLAME (Federated Learning Across Manipulation Environments), the first benchmark designed for federated learning in robotic manipulation. FLAME consists of: (i) a set of large-scale datasets of over 160,000 expert demonstrations of multiple manipulation tasks, collected across a wide range of simulated environments; (ii) a training and evaluation framework for robotic policy learning in a federated setting. We evaluate standard federated learning algorithms in FLAME, showing their potential for distributed policy learning and highlighting key challenges. Our benchmark establishes a foundation for scalable, adaptive, and privacy-aware robotic learning.', 'abstract_zh': '近期，机器人操作方面的进展得益于跨多种环境收集的大规模数据集。在这些数据集上训练机器人操作策略通常采用集中式方式，这引起了可扩展性、适应性和数据隐私方面的担忧。尽管联邦学习可以实现分布式、隐私保护的训练，但其在机器人操作中的应用尚未得到充分探索。我们介绍了FLAME（跨操作环境的联邦学习），这是首个为机器人操作中的联邦学习设计的基准测试。FLAME包含：(i) 一个包含超过160,000个专家演示的数据集，涵盖了多种操作任务和广泛模拟环境；(ii) 一种用于联邦设置中机器人策略学习的训练和评估框架。我们在FLAME中评估了标准联邦学习算法，展示了其在分布式策略学习中的潜力，并指出了关键挑战。该基准测试为可扩展、适应性和隐私意识的机器人学习奠定了基础。', 'title_zh': 'FLAME：用于机器人 manipulation 的联邦学习基准'}
{'arxiv_id': 'arXiv:2503.01293', 'title': 'Stone Soup Multi-Target Tracking Feature Extraction For Autonomous Search And Track In Deep Reinforcement Learning Environment', 'authors': 'Jan-Hendrik Ewers, Joe Gibbs, David Anderson', 'link': 'https://arxiv.org/abs/2503.01293', 'abstract': 'Management of sensing resources is a non-trivial problem for future military air assets with future systems deploying heterogeneous sensors to generate information of the battlespace. Machine learning techniques including deep reinforcement learning (DRL) have been identified as promising approaches, but require high-fidelity training environments and feature extractors to generate information for the agent. This paper presents a deep reinforcement learning training approach, utilising the Stone Soup tracking framework as a feature extractor to train an agent for a sensor management task. A general framework for embedding Stone Soup tracker components within a Gymnasium environment is presented, enabling fast and configurable tracker deployments for RL training using Stable Baselines3. The approach is demonstrated in a sensor management task where an agent is trained to search and track a region of airspace utilising track lists generated from Stone Soup trackers. A sample implementation using three neural network architectures in a search-and-track scenario demonstrates the approach and shows that RL agents can outperform simple sensor search and track policies when trained within the Gymnasium and Stone Soup environment.', 'abstract_zh': '未来的军事空中资产在部署异构传感器生成战场信息时，感知资源管理是一个非平凡的问题。机器学习技术包括深度强化学习（DRL）被认为是有前景的方法，但需要高保真度的训练环境和特征提取器来为代理生成信息。本文提出了一种基于深度强化学习的训练方法，利用Stone Soup跟踪框架作为特征提取器，训练一个用于传感器管理任务的代理。我们介绍了一个将Stone Soup跟踪组件嵌入Gymnasium环境中的通用框架，使得使用Stable Baselines3进行强化学习训练时能够快速且配置灵活地部署跟踪器。该方法在一项传感器管理任务中进行了演示，任务中代理被训练以利用Stone Soup跟踪器生成的航迹列表搜索和跟踪空中区域。在搜索和跟踪场景中，使用三种神经网络架构的示例实现展示了该方法，并且表明在Gymnasium和Stone Soup环境中训练的RL代理能够优于简单的传感器搜索和跟踪策略。', 'title_zh': '石 Soup 多目标跟踪特征提取在深度强化学习环境中的自主搜索与跟踪'}
{'arxiv_id': 'arXiv:2503.01274', 'title': 'DnD Filter: Differentiable State Estimation for Dynamic Systems using Diffusion Models', 'authors': 'Ziyu Wan, Lin Zhao', 'link': 'https://arxiv.org/abs/2503.01274', 'abstract': 'This paper proposes the DnD Filter, a differentiable filter that utilizes diffusion models for state estimation of dynamic systems. Unlike conventional differentiable filters, which often impose restrictive assumptions on process noise (e.g., Gaussianity), DnD Filter enables a nonlinear state update without such constraints by conditioning a diffusion model on both the predicted state and observational data, capitalizing on its ability to approximate complex distributions. We validate its effectiveness on both a simulated task and a real-world visual odometry task, where DnD Filter consistently outperforms existing baselines. Specifically, it achieves a 25\\% improvement in estimation accuracy on the visual odometry task compared to state-of-the-art differentiable filters, and even surpasses differentiable smoothers that utilize future measurements. To the best of our knowledge, DnD Filter represents the first successful attempt to leverage diffusion models for state estimation, offering a flexible and powerful framework for nonlinear estimation under noisy measurements.', 'abstract_zh': '基于扩散模型的可微分滤波器DnD滤波器：一种用于动态系统状态估计的可微分滤波器', 'title_zh': 'DnD Filter: 基于扩散模型的差分状态估计方法'}
{'arxiv_id': 'arXiv:2503.01206', 'title': 'Action Tokenizer Matters in In-Context Imitation Learning', 'authors': 'An Dinh Vuong, Minh Nhat Vu, Dong An, Ian Reid', 'link': 'https://arxiv.org/abs/2503.01206', 'abstract': 'In-context imitation learning (ICIL) is a new paradigm that enables robots to generalize from demonstrations to unseen tasks without retraining. A well-structured action representation is the key to capturing demonstration information effectively, yet action tokenizer (the process of discretizing and encoding actions) remains largely unexplored in ICIL. In this work, we first systematically evaluate existing action tokenizer methods in ICIL and reveal a critical limitation: while they effectively encode action trajectories, they fail to preserve temporal smoothness, which is crucial for stable robotic execution. To address this, we propose LipVQ-VAE, a variational autoencoder that enforces the Lipschitz condition in the latent action space via weight normalization. By propagating smoothness constraints from raw action inputs to a quantized latent codebook, LipVQ-VAE generates more stable and smoother actions. When integrating into ICIL, LipVQ-VAE improves performance by more than 5.3% in high-fidelity simulators, with real-world experiments confirming its ability to produce smoother, more reliable trajectories. Code and checkpoints will be released.', 'abstract_zh': '基于上下文的模仿学习中动作分词器的Lipschitz正则化变分自编码器研究', 'title_zh': 'Action Tokenizer 对于在上下文模仿学习中很重要。'}
{'arxiv_id': 'arXiv:2503.00614', 'title': 'Sampling-Based Motion Planning with Discrete Configuration-Space Symmetries', 'authors': 'Thomas Cohn, Russ Tedrake', 'link': 'https://arxiv.org/abs/2503.00614', 'abstract': 'When planning motions in a configuration space that has underlying symmetries (e.g. when manipulating one or multiple symmetric objects), the ideal planning algorithm should take advantage of those symmetries to produce shorter trajectories. However, finite symmetries lead to complicated changes to the underlying topology of configuration space, preventing the use of standard algorithms. We demonstrate how the key primitives used for sampling-based planning can be efficiently implemented in spaces with finite symmetries. A rigorous theoretical analysis, building upon a study of the geometry of the configuration space, shows improvements in the sample complexity of several standard algorithms. Furthermore, a comprehensive slate of experiments demonstrates the practical improvements in both path length and runtime.', 'abstract_zh': '在具有有限对称性的配置空间中高效实施基于采样的运动规划基本原语及其理论分析与实验验证', 'title_zh': '基于采样的运动规划与离散配置空间对称性'}
{'arxiv_id': 'arXiv:2503.00077', 'title': 'Navigating the Edge with the State-of-the-Art Insights into Corner Case Identification and Generation for Enhanced Autonomous Vehicle Safety', 'authors': 'Gabriel Kenji Godoy Shimanuki, Alexandre Moreira Nascimento, Lucio Flavio Vismari, Joao Batista Camargo Junior, Jorge Rady de Almeida Junior, Paulo Sergio Cugnasca', 'link': 'https://arxiv.org/abs/2503.00077', 'abstract': 'In recent years, there has been significant development of autonomous vehicle (AV) technologies. However, despite the notable achievements of some industry players, a strong and appealing body of evidence that demonstrate AVs are actually safe is lacky, which could foster public distrust in this technology and further compromise the entire development of this industry, as well as related social impacts. To improve the safety of AVs, several techniques are proposed that use synthetic data in virtual simulation. In particular, the highest risk data, known as corner cases (CCs), are the most valuable for developing and testing AV controls, as they can expose and improve the weaknesses of these autonomous systems. In this context, the present paper presents a systematic literature review aiming to comprehensively analyze methodologies for CC identifi cation and generation, also pointing out current gaps and further implications of synthetic data for AV safety and reliability. Based on a selection criteria, 110 studies were picked from an initial sample of 1673 papers. These selected paper were mapped into multiple categories to answer eight inter-linked research questions. It concludes with the recommendation of a more integrated approach focused on safe development among all stakeholders, with active collaboration between industry, academia and regulatory bodies.', 'abstract_zh': '近年来，自主车辆（AV）技术取得了显著发展。然而，尽管一些行业参与者取得了显著成就，证明AV实际上安全的有力证据仍然不足，这可能促进公众对该技术的信任缺失，进而损害整个行业的发展以及相关社会影响。为了提高AV的安全性，提出了几种使用合成数据在虚拟仿真中应用的技术。特别是被称为极端案例（CCs）的最高风险数据是最有价值的，因为它们能够暴露并改进这些自主系统中的薄弱环节。在此背景下，本文进行了一项系统文献综述，旨在全面分析CC识别与生成的方法学，同时也指出现存空白并探讨合成数据对AV安全性和可靠性的进一步影响。根据选择标准，从1673篇论文中选择了110篇进行分析。这些被选中的论文被划分为多个类别，以回答八个相关联的研究问题。最后，本文推荐一种更集成的方法，所有利益相关方共同关注安全发展，并且在行业、学术界和监管机构之间进行积极合作。', 'title_zh': '基于前沿见解提升自动驾驶车辆安全性的边缘案例识别与生成导航'}
{'arxiv_id': 'arXiv:2503.01650', 'title': 'CAPS: Context-Aware Priority Sampling for Enhanced Imitation Learning in Autonomous Driving', 'authors': 'Hamidreza Mirkhani, Behzad Khamidehi, Ehsan Ahmadi, Fazel Arasteh, Mohammed Elmahgiubi, Weize Zhang, Umar Rajguru, Kasra Rezaee', 'link': 'https://arxiv.org/abs/2503.01650', 'abstract': 'In this paper, we introduce CAPS (Context-Aware Priority Sampling), a novel method designed to enhance data efficiency in learning-based autonomous driving systems. CAPS addresses the challenge of imbalanced training datasets in imitation learning by leveraging Vector Quantized Variational Autoencoders (VQ-VAEs). The use of VQ-VAE provides a structured and interpretable data representation, which helps reveal meaningful patterns in the data. These patterns are used to group the data into clusters, with each sample being assigned a cluster ID. The cluster IDs are then used to re-balance the dataset, ensuring that rare yet valuable samples receive higher priority during training. By ensuring a more diverse and informative training set, CAPS improves the generalization of the trained planner across a wide range of driving scenarios. We evaluate our method through closed-loop simulations in the CARLA environment. The results on Bench2Drive scenarios demonstrate that our framework outperforms state-of-the-art methods, leading to notable improvements in model performance.', 'abstract_zh': '基于上下文感知优先采样的自动驾驶数据效率提升方法（CAPS）', 'title_zh': 'CAPS：面向情境的优先采样方法以增强自主驾驶中的 imitation learning'}
{'arxiv_id': 'arXiv:2503.01626', 'title': 'A Note on the Time Complexity of Using Subdivision Methods for the Approximation of Fibers', 'authors': 'Michael M. Bilevich, Dan Halperin', 'link': 'https://arxiv.org/abs/2503.01626', 'abstract': 'Subdivision methods such as quadtrees, octrees, and higher-dimensional orthrees are standard practice in different domains of computer science. We can use these methods to represent given geometries, such as curves, meshes, or surfaces. This representation is achieved by splitting some bounding voxel recursively while further splitting only sub-voxels that intersect with the given geometry. It is fairly known that subdivision methods are more efficient than traversing a fine-grained voxel grid. In this short note, we propose another outlook on analyzing the construction time complexity of orthrees to represent implicitly defined geometries that are fibers (preimages) of some function. This complexity is indeed asymptotically better than traversing dense voxel grids, under certain conditions, which we specify in the note. In fact, the complexity is output sensitive, and is closely related to the Hausdorff measure and Hausdorff dimension of the resulting geometry.', 'abstract_zh': '细分方法，如四叉树、八叉树和高维正交树在计算机科学的不同领域都是标准实践。我们可以通过这些方法来表示给定的几何形状，如曲线、网格或曲面。这种表示是通过对一些边界体素进行递归分割，仅进一步分割与给定几何形状相交的子体素来实现的。已知细分方法比遍历细粒度体素网格更高效。在本文简要说明中，我们提出了另一种分析使用正交树表示由某些函数定义的隐式几何（纤维或前image）的构建时间复杂性的视角。在某些条件下，这种复杂性确实比遍历密集体素网格更具渐近优势，我们将在说明中指定这些条件。实际上，这种复杂性对输出敏感，并且与所得几何的豪斯多夫测度和豪斯多夫维数密切相关。', 'title_zh': '关于使用分划方法近似纤维的时间复杂性注记'}
{'arxiv_id': 'arXiv:2503.01476', 'title': 'Trajectory Planning with Signal Temporal Logic Costs using Deterministic Path Integral Optimization', 'authors': 'Patrick Halder, Hannes Homburger, Lothar Kiltz, Johannes Reuter, Matthias Althoff', 'link': 'https://arxiv.org/abs/2503.01476', 'abstract': 'Formulating the intended behavior of a dynamic system can be challenging. Signal temporal logic (STL) is frequently used for this purpose due to its suitability in formalizing comprehensible, modular, and versatile spatiotemporal specifications. Due to scaling issues with respect to the complexity of the specifications and the potential occurrence of non-differentiable terms, classical optimization methods often solve STL-based problems inefficiently. Smoothing and approximation techniques can alleviate these issues but require changing the optimization problem. This paper proposes a novel sampling-based method based on model predictive path integral control to solve optimal control problems with STL cost functions. We demonstrate the effectiveness of our method on benchmark motion planning problems and compare its performance with state-of-the-art methods. The results show that our method efficiently solves optimal control problems with STL costs.', 'abstract_zh': '基于模型预测路径积分控制的采样方法求解带有STL成本函数的最优控制问题', 'title_zh': '使用确定性路径积分优化的信号时序逻辑代价轨迹规划'}
{'arxiv_id': 'arXiv:2503.01450', 'title': 'POPGym Arcade: Parallel Pixelated POMDPs', 'authors': 'Zekang Wang, Zhe He, Edan Toledo, Steven Morad', 'link': 'https://arxiv.org/abs/2503.01450', 'abstract': 'We introduce POPGym Arcade, a benchmark consisting of 7 pixel-based environments each with three difficulties, utilizing a single observation and action space. Each environment offers both fully observable and partially observable variants, enabling counterfactual studies on partial observability. POPGym Arcade utilizes JIT compilation on hardware accelerators to achieve substantial speedups over CPU-bound environments. Moreover, this enables Podracer-style architectures to further increase hardware utilization and training speed. We evaluate memory models on our environments using a Podracer variant of Q learning, and examine the results. Finally, we generate memory saliency maps, uncovering how memories propagate through policies. Our library is available at this https URL popgym_arcade.', 'abstract_zh': 'POPGym Arcade：一个基于像素的多难度环境基准，包含7个环境，每环境有三种难度，利用单一的观察和动作空间，支持全可观测和部分可观测变体，利用硬件加速器的JIT编译实现显著加速，并促进提升硬件利用率和训练速度。我们使用Podracer变体的Q学习评估记忆模型，并生成记忆显著性图，揭示记忆在策略中的传播方式。相关库可访问此链接：popgym_arcade。', 'title_zh': 'POPGym 赌框：并行像素化部分可观测马尔可夫决策过程'}
{'arxiv_id': 'arXiv:2503.00684', 'title': 'Factorized Deep Q-Network for Cooperative Multi-Agent Reinforcement Learning in Victim Tagging', 'authors': 'Maria Ana Cardei, Afsaneh Doryab', 'link': 'https://arxiv.org/abs/2503.00684', 'abstract': 'Mass casualty incidents (MCIs) are a growing concern, characterized by complexity and uncertainty that demand adaptive decision-making strategies. The victim tagging step in the emergency medical response must be completed quickly and is crucial for providing information to guide subsequent time-constrained response actions. In this paper, we present a mathematical formulation of multi-agent victim tagging to minimize the time it takes for responders to tag all victims. Five distributed heuristics are formulated and evaluated with simulation experiments. The heuristics considered are on-the go, practical solutions that represent varying levels of situational uncertainty in the form of global or local communication capabilities, showcasing practical constraints. We further investigate the performance of a multi-agent reinforcement learning (MARL) strategy, factorized deep Q-network (FDQN), to minimize victim tagging time as compared to baseline heuristics. Extensive simulations demonstrate that between the heuristics, methods with local communication are more efficient for adaptive victim tagging, specifically choosing the nearest victim with the option to replan. Analyzing all experiments, we find that our FDQN approach outperforms heuristics in smaller-scale scenarios, while heuristics excel in more complex scenarios. Our experiments contain diverse complexities that explore the upper limits of MARL capabilities for real-world applications and reveal key insights.', 'abstract_zh': '大规模伤亡事件中的多代理受害者标记：数学建模与算法比较', 'title_zh': '基于因子化的深度Q网络在受害者标记的协同多智能体 reinforcement 学习中'}
{'arxiv_id': 'arXiv:2503.00654', 'title': 'ExAMPC: the Data-Driven Explainable and Approximate NMPC with Physical Insights', 'authors': 'Jean Pierre Allamaa, Panagiotis Patrinos, Tong Duy Son', 'link': 'https://arxiv.org/abs/2503.00654', 'abstract': "Amidst the surge in the use of Artificial Intelligence (AI) for control purposes, classical and model-based control methods maintain their popularity due to their transparency and deterministic nature. However, advanced controllers like Nonlinear Model Predictive Control (NMPC), despite proven capabilities, face adoption challenges due to their computational complexity and unpredictable closed-loop performance in complex validation systems. This paper introduces ExAMPC, a methodology bridging classical control and explainable AI by augmenting the NMPC with data-driven insights to improve the trustworthiness and reveal the optimization solution and closed-loop performance's sensitivities to physical variables and system parameters. By employing a low-order spline embedding to reduce the open-loop trajectory dimensionality by over 95%, and integrating it with SHAP and Symbolic Regression from eXplainable AI (XAI) for an approximate NMPC, we enable intuitive physical insights into the NMPC's optimization routine. The prediction accuracy of the approximate NMPC is enhanced through physics-inspired continuous-time constraints penalties, reducing the predicted continuous trajectory violations by 93%. ExAMPC enables accurate forecasting of the NMPC's computational requirements with explainable insights on worst-case scenarios. Experimental validation on automated valet parking and autonomous racing with lap-time optimization NMPC, demonstrates the methodology's practical effectiveness in real-world applications.", 'abstract_zh': '伴随人工智能（AI）在控制领域应用的增加，经典和模型参考控制方法由于其透明性和确定性仍保持受欢迎。然而，先进的控制器如非线性模型预测控制（NMPC），尽管已证明其能力，但由于计算复杂性和在复杂验证系统中的不可预测闭环性能，其应用面临挑战。本文提出了ExAMPC方法，通过将数据驱动见解嵌入NMPC以增强其可信度，并揭示优化解和闭环性能对物理变量和系统参数的灵敏度，从而在经典控制和可解释AI之间架起桥梁。通过采用低阶样条嵌入减少开放轨迹维度超过95%，并与可解释AI（XAI）中的SHAP和符号回归相结合构建近似NMPC，我们使NMPC的优化过程具有直观的物理洞察。通过引入基于物理的连续时间约束惩罚，近似NMPC的预测准确性提高，预测连续轨迹违规减少93%。ExAMPC能够提供关于NMPC计算需求和最坏情况场景的可解释洞察。在自动valet停车和基于圈速优化的自主赛车试验验证中，该方法展示了其实用有效性。', 'title_zh': 'ExAMPC：基于数据的可解释性和近似NMPC，融入物理洞察'}
{'arxiv_id': 'arXiv:2503.00400', 'title': 'Inteval Analysis for two spherical functions arising from robust Perspective-n-Lines problem', 'authors': 'Xiang Zheng, Haodong Jiang, Junfeng Wu', 'link': 'https://arxiv.org/abs/2503.00400', 'abstract': 'This report presents a comprehensive interval analysis of two spherical functions derived from the robust Perspective-n-Lines (PnL) problem. The study is motivated by the application of a dimension-reduction technique to achieve global solutions for the robust PnL problem. We establish rigorous theoretical results, supported by detailed proofs, and validate our findings through extensive numerical simulations.', 'abstract_zh': '本报告对两sphere函数从鲁棒Perspective-n-Lines (PnL)问题导出的区间进行全面分析。该研究受到将降维技术应用于实现鲁棒PnL问题全局解的动机驱动。我们建立了严格的理论结果，并通过广泛的数值仿真验证了这些结果。', 'title_zh': '两球面函数在鲁棒透视-n-线问题中的区间分析'}
{'arxiv_id': 'arXiv:2503.01792', 'title': 'Generating Counterfactual Explanations Under Temporal Constraints', 'authors': 'Andrei Buliga, Chiara Di Francescomarino, Chiara Ghidini, Marco Montali, Massimiliano Ronzani', 'link': 'https://arxiv.org/abs/2503.01792', 'abstract': 'Counterfactual explanations are one of the prominent eXplainable Artificial Intelligence (XAI) techniques, and suggest changes to input data that could alter predictions, leading to more favourable outcomes. Existing counterfactual methods do not readily apply to temporal domains, such as that of process mining, where data take the form of traces of activities that must obey to temporal background knowledge expressing which dynamics are possible and which not. Specifically, counterfactuals generated off-the-shelf may violate the background knowledge, leading to inconsistent explanations. This work tackles this challenge by introducing a novel approach for generating temporally constrained counterfactuals, guaranteed to comply by design with background knowledge expressed in Linear Temporal Logic on process traces (LTLp). We do so by infusing automata-theoretic techniques for LTLp inside a genetic algorithm for counterfactual generation. The empirical evaluation shows that the generated counterfactuals are temporally meaningful and more interpretable for applications involving temporal dependencies.', 'abstract_zh': '基于时间约束的-counterfactual解释：一种符合线性时序逻辑的新型生成方法', 'title_zh': '基于时间约束的事实解释生成'}
{'arxiv_id': 'arXiv:2503.01722', 'title': 'Learning Exposure Mapping Functions for Inferring Heterogeneous Peer Effects', 'authors': 'Shishir Adhikari, Sourav Medya, Elena Zheleva', 'link': 'https://arxiv.org/abs/2503.01722', 'abstract': "In causal inference, interference refers to the phenomenon in which the actions of peers in a network can influence an individual's outcome. Peer effect refers to the difference in counterfactual outcomes of an individual for different levels of peer exposure, the extent to which an individual is exposed to the treatments, actions, or behaviors of peers. Estimating peer effects requires deciding how to represent peer exposure. Typically, researchers define an exposure mapping function that aggregates peer treatments and outputs peer exposure. Most existing approaches for defining exposure mapping functions assume peer exposure based on the number or fraction of treated peers. Recent studies have investigated more complex functions of peer exposure which capture that different peers can exert different degrees of influence. However, none of these works have explicitly considered the problem of automatically learning the exposure mapping function. In this work, we focus on learning this function for the purpose of estimating heterogeneous peer effects, where heterogeneity refers to the variation in counterfactual outcomes for the same peer exposure but different individual's contexts. We develop EgoNetGNN, a graph neural network (GNN)-based method, to automatically learn the appropriate exposure mapping function allowing for complex peer influence mechanisms that, in addition to peer treatments, can involve the local neighborhood structure and edge attributes. We show that GNN models that use peer exposure based on the number or fraction of treated peers or learn peer exposure naively face difficulty accounting for such influence mechanisms. Our comprehensive evaluation on synthetic and semi-synthetic network data shows that our method is more robust to different unknown underlying influence mechanisms when estimating heterogeneous peer effects when compared to state-of-the-art baselines.", 'abstract_zh': '因果推断中的干扰现象指的是网络中同伴的行为会影响个体的结局。同伴效应指的是在不同同伴暴露水平下个体的反事实结局差异，即个体受到同伴治疗、行为或行动影响的程度。估计同伴效应需要决定如何表示同伴暴露。通常，研究人员定义一个暴露映射函数，汇总同伴治疗并输出同伴暴露。现有大多数方法假设同伴暴露基于受治疗同伴的数量或比例。近期研究探索了更复杂的同伴暴露函数，捕捉不同同伴可能施加不同影响的程度。然而，这些工作均未明确考虑自动学习暴露映射函数的问题。在本文中，我们专注于为估计异质同伴效应学习该函数，其中异质性指的是相同同伴暴露但不同个体上下文中反事实结局的变异。我们开发了EgoNetGNN，这是一种基于图神经网络（GNN）的方法，能够自动学习允许复杂同伴影响机制的适当暴露映射函数，这些机制不仅涉及同伴治疗，还可能包括局部邻域结构和边属性。我们证明使用基于受治疗同伴数量或比例的同伴暴露或简单学习同伴暴露的GNN模型难以捕捉这些影响机制。我们在合成和半合成网络数据上的全面评估表明，我们的方法在估计异质同伴效应时相较于最先进的基线方法具有更强的鲁棒性。', 'title_zh': '学习暴露映射函数以推断异质性 peers 效应'}
{'arxiv_id': 'arXiv:2503.01508', 'title': 'Enabling AI Scientists to Recognize Innovation: A Domain-Agnostic Algorithm for Assessing Novelty', 'authors': 'Yao Wang, Mingxuan Cui, Arthur Jiang', 'link': 'https://arxiv.org/abs/2503.01508', 'abstract': 'In the pursuit of Artificial General Intelligence (AGI), automating the generation and evaluation of novel research ideas is a key challenge in AI-driven scientific discovery. This paper presents Relative Neighbor Density (RND), a domain-agnostic algorithm for novelty assessment in research ideas that overcomes the limitations of existing approaches by analyzing the distribution patterns of semantic neighbors rather than simple distances. We first developed a scalable methodology to create validation datasets without expert labeling, addressing a fundamental challenge in novelty assessment. Using these datasets, we demonstrate that our RND algorithm achieves state-of-the-art (SOTA) performance in computer science (AUROC=0.808) and biomedical research (AUROC=0.757) domains. Most significantly, while SOTA models like Sonnet-3.7 and existing metrics show domain-specific performance degradation, RND maintains consistent effectiveness across domains, outperforming all benchmarks by a substantial margin (0.782 v.s. 0.597) on cross-domain evaluation. These results validate RND as a generalizable solution for automated novelty assessment in scientific research.', 'abstract_zh': '在追求人工通用智能（AGI）的过程中，自动化新型研究理念的生成与评估是AI驱动科学发现中的关键挑战。本文提出了基于相对邻居密度（RND）的通用新颖性评估算法，该算法通过分析语义邻居的分布模式而非简单的距离来克服现有方法的局限。我们首先开发了一种可扩展的方法来创建无需专家标注的验证数据集，解决了新颖性评估中的根本性挑战。利用这些数据集，我们展示了RND算法在计算机科学（AUROC=0.808）和生物医学研究（AUROC=0.757）领域的SOTA性能。尤为重要的是，尽管如Sonnet-3.7等SOTA模型和现有指标在不同领域表现出性能下降，RND在跨领域评估中保持了一致的有效性，相较于所有基准模型取得了显著的性能优势（0.782 vs. 0.597）。这些结果验证了RND作为科学科研中自动化新颖性评估的通用解决方案的有效性。', 'title_zh': '使AI科学家识别创新：一种跨领域新颖性评估算法'}
{'arxiv_id': 'arXiv:2503.01475', 'title': 'ProRCA: A Causal Python Package for Actionable Root Cause Analysis in Real-world Business Scenarios', 'authors': 'Ahmed Dawoud, Shravan Talupula', 'link': 'https://arxiv.org/abs/2503.01475', 'abstract': "Root Cause Analysis (RCA) is becoming ever more critical as modern systems grow in complexity, volume of data, and interdependencies. While traditional RCA methods frequently rely on correlation-based or rule-based techniques, these approaches can prove inadequate in highly dynamic, multi-layered environments. In this paper, we present a pathway-tracing package built on the DoWhy causal inference library. Our method integrates conditional anomaly scoring, noise-based attribution, and depth-first path exploration to reveal multi-hop causal chains. By systematically tracing entire causal pathways from an observed anomaly back to the initial triggers, our approach provides a comprehensive, end-to-end RCA solution. Experimental evaluations with synthetic anomaly injections demonstrate the package's ability to accurately isolate triggers and rank root causes by their overall significance.", 'abstract_zh': '基于DoWhy因果推断库的路径追踪包：多跳因果链的揭示与根因分析', 'title_zh': 'ProRCA：一种用于实际商业场景可行动因分析的因果Python包'}
{'arxiv_id': 'arXiv:2503.01458', 'title': 'SrSv: Integrating Sequential Rollouts with Sequential Value Estimation for Multi-agent Reinforcement Learning', 'authors': 'Xu Wan, Chao Yang, Cheng Yang, Jie Song, Mingyang Sun', 'link': 'https://arxiv.org/abs/2503.01458', 'abstract': 'Although multi-agent reinforcement learning (MARL) has shown its success across diverse domains, extending its application to large-scale real-world systems still faces significant challenges. Primarily, the high complexity of real-world environments exacerbates the credit assignment problem, substantially reducing training efficiency. Moreover, the variability of agent populations in large-scale scenarios necessitates scalable decision-making mechanisms. To address these challenges, we propose a novel framework: Sequential rollout with Sequential value estimation (SrSv). This framework aims to capture agent interdependence and provide a scalable solution for cooperative MARL. Specifically, SrSv leverages the autoregressive property of the Transformer model to handle varying populations through sequential action rollout. Furthermore, to capture the interdependence of policy distributions and value functions among multiple agents, we introduce an innovative sequential value estimation methodology and integrates the value approximation into an attention-based sequential model. We evaluate SrSv on three benchmarks: Multi-Agent MuJoCo, StarCraft Multi-Agent Challenge, and DubinsCars. Experimental results demonstrate that SrSv significantly outperforms baseline methods in terms of training efficiency without compromising convergence performance. Moreover, when implemented in a large-scale DubinsCar system with 1,024 agents, our framework surpasses existing benchmarks, highlighting the excellent scalability of SrSv.', 'abstract_zh': '虽然多智能体强化学习（MARL）已经在多种领域展现了其成功，将其应用扩展到大规模实际系统仍然面临显著挑战。具体而言，真实世界环境的高复杂性加剧了信用分配问题，显著降低了训练效率。此外，大规模场景中智能体群体的多样性要求可扩展的决策机制。为应对这些挑战，我们提出了一种新的框架：顺序展开与顺序价值估计（SrSv）。该框架旨在捕捉智能体之间的相互依赖性，并为合作MARL提供可扩展的解决方案。具体而言，SrSv 利用Transformer模型的自回归特性，通过顺序行动展开处理智能体群体的变化。此外，为了捕捉智能体之间策略分布和价值函数之间的相互依赖性，我们引入了一种创新的顺序价值估计方法，并将价值近似整合到基于注意力的顺序模型中。我们在三个基准上评估了SrSv：Multi-Agent MuJoCo、StarCraft 多智能体挑战和DubinsCars。实验结果表明，SrSv 在不牺牲收敛性能的情况下，在训练效率方面显著优于基线方法。此外，当在包含1024个智能体的大规模DubinsCar系统中实现时，该框架超越了现有的基准，突显了SrSv的卓越可扩展性。', 'title_zh': 'SrSv: 将顺序 rollout 与顺序价值估计集成到多智能体强化学习中'}
{'arxiv_id': 'arXiv:2503.01424', 'title': 'From Hypothesis to Publication: A Comprehensive Survey of AI-Driven Research Support Systems', 'authors': 'Zekun Zhou, Xiaocheng Feng, Lei Huang, Xiachong Feng, Ziyun Song, Ruihan Chen, Liang Zhao, Weitao Ma, Yuxuan Gu, Baoxin Wang, Dayong Wu, Guoping Hu, Ting Liu, Bing Qin', 'link': 'https://arxiv.org/abs/2503.01424', 'abstract': 'Research is a fundamental process driving the advancement of human civilization, yet it demands substantial time and effort from researchers. In recent years, the rapid development of artificial intelligence (AI) technologies has inspired researchers to explore how AI can accelerate and enhance research. To monitor relevant advancements, this paper presents a systematic review of the progress in this domain. Specifically, we organize the relevant studies into three main categories: hypothesis formulation, hypothesis validation, and manuscript publication. Hypothesis formulation involves knowledge synthesis and hypothesis generation. Hypothesis validation includes the verification of scientific claims, theorem proving, and experiment validation. Manuscript publication encompasses manuscript writing and the peer review process. Furthermore, we identify and discuss the current challenges faced in these areas, as well as potential future directions for research. Finally, we also offer a comprehensive overview of existing benchmarks and tools across various domains that support the integration of AI into the research process. We hope this paper serves as an introduction for beginners and fosters future research. Resources have been made publicly available at this https URL.', 'abstract_zh': '人工智能技术加速与增强研究的进展综述：从假设形成到手稿发表面临的挑战与未来方向及现有基准与工具概述', 'title_zh': '从假设到发表：AI驱动的研究支持系统综述'}
{'arxiv_id': 'arXiv:2503.01413', 'title': 'Building Interval Type-2 Fuzzy Membership Function: A Deck of Cards based Co-constructive Approach', 'authors': 'Bapi Dutta, Diego García-Zamora, José Rui Figueira, Luis Martínez', 'link': 'https://arxiv.org/abs/2503.01413', 'abstract': "Since its inception, Fuzzy Set has been widely used to handle uncertainty and imprecision in decision-making. However, conventional fuzzy sets, often referred to as type-1 fuzzy sets (T1FSs) have limitations in capturing higher levels of uncertainty, particularly when decision-makers (DMs) express hesitation or ambiguity in membership degree. To address this, Interval Type-2 Fuzzy Sets (IT2FSs) have been introduced by incorporating uncertainty in membership degree allocation, which enhanced flexibility in modelling subjective judgments. Despite their advantages, existing IT2FS construction methods often lack active involvement from DMs and that limits the interpretability and effectiveness of decision models. This study proposes a socio-technical co-constructive approach for developing IT2FS models of linguistic terms by facilitating the active involvement of DMs in preference elicitation and its application in multicriteria decision-making (MCDM) problems. Our methodology is structured in two phases. The first phase involves an interactive process between the DM and the decision analyst, in which a modified version of Deck-of-Cards (DoC) method is proposed to construct T1FS membership functions on a ratio scale. We then extend this method to incorporate ambiguity in subjective judgment and that resulted in an IT2FS model that better captures uncertainty in DM's linguistic assessments. The second phase formalizes the constructed IT2FS model for application in MCDM by defining an appropriate mathematical representation of such information, aggregation rules, and an admissible ordering principle. The proposed framework enhances the reliability and effectiveness of fuzzy decision-making not only by accurately representing DM's personalized semantics of linguistic information.", 'abstract_zh': '基于社会技术共构的区间型二值模糊集语言术语模型在多准则决策中的应用', 'title_zh': '基于纸牌共构的方法构建区间型2模糊隶属函数'}
{'arxiv_id': 'arXiv:2503.01389', 'title': 'Learning Conjecturing from Scratch', 'authors': 'Thibault Gauthier, Josef Urban', 'link': 'https://arxiv.org/abs/2503.01389', 'abstract': "We develop a self-learning approach for conjecturing of induction predicates on a dataset of 16197 problems derived from the OEIS. These problems are hard for today's SMT and ATP systems because they require a combination of inductive and arithmetical reasoning.\nStarting from scratch, our approach consists of a feedback loop that iterates between (i) training a neural translator to learn the correspondence between the problems solved so far and the induction predicates useful for them, (ii) using the trained neural system to generate many new induction predicates for the problems, (iii) fast runs of the z3 prover attempting to prove the problems using the generated predicates, (iv) using heuristics such as predicate size and solution speed on the proved problems to choose the best predicates for the next iteration of training.\nThe algorithm discovers on its own many interesting induction predicates, ultimately solving 5565 problems, compared to 2265 problems solved by CVC5, Vampire or Z3 in 60 seconds.", 'abstract_zh': '我们开发了一种自学习方法，用于从OEIS来源的16197个问题数据集上推断归纳谓词。这些问题是今天SMT和ATP系统的难点，因为它们需要结合归纳和算术推理。\n从头开始，我们的方法包括一个反馈循环，该循环交替进行以下步骤：(i) 训练一个神经翻译器来学习迄今为止解决的问题与对它们有用的归纳谓词之间的对应关系；(ii) 使用训练好的神经系统生成许多新的归纳谓词；(iii) 快速运行z3证明器尝试使用生成的谓词证明问题；(iv) 使用诸如谓词大小和证明速度等启发式方法，在证明的问题上选择最佳谓词以进行下一训练迭代。\n该算法自行发现了许多有趣的归纳谓词，最终解决了5565个问题，而CVC5、Vampire或Z3在60秒内只解决了2265个问题。', 'title_zh': '从零学习提出猜想'}
{'arxiv_id': 'arXiv:2503.01176', 'title': 'Prognostics and Health Management of Wafer Chemical-Mechanical Polishing System using Autoencoder', 'authors': 'Kart-Leong Lim, Rahul Dutta', 'link': 'https://arxiv.org/abs/2503.01176', 'abstract': 'The Prognostics and Health Management Data Challenge (PHM) 2016 tracks the health state of components of a semiconductor wafer polishing process. The ultimate goal is to develop an ability to predict the measurement on the wafer surface wear through monitoring the components health state. This translates to cost saving in large scale production. The PHM dataset contains many time series measurements not utilized by traditional physics based approach. On the other hand task, applying a data driven approach such as deep learning to the PHM dataset is non-trivial. The main issue with supervised deep learning is that class label is not available to the PHM dataset. Second, the feature space trained by an unsupervised deep learner is not specifically targeted at the predictive ability or regression. In this work, we propose using the autoencoder based clustering whereby the feature space trained is found to be more suitable for performing regression. This is due to having a more compact distribution of samples respective to their nearest cluster means. We justify our claims by comparing the performance of our proposed method on the PHM dataset with several baselines such as the autoencoder as well as state-of-the-art approaches.', 'abstract_zh': '2016年 prognostics 和健康管理系统数据挑战（PHM）中的组件状态跟踪：基于自编码器的聚类在半导体晶圆抛光过程中的表面磨损预测', 'title_zh': '基于自动编码器的晶圆化学机械抛光系统预测性维护与健康管理'}
{'arxiv_id': 'arXiv:2503.01126', 'title': 'Constrained multi-fidelity Bayesian optimization with automatic stop condition', 'authors': 'Zahra Zanjani Foumani, Ramin Bostanabad', 'link': 'https://arxiv.org/abs/2503.01126', 'abstract': "Bayesian optimization (BO) is increasingly employed in critical applications to find the optimal design with minimal cost. While BO is known for its sample efficiency, relying solely on costly high-fidelity data can still result in high costs. This is especially the case in constrained search spaces where BO must not only optimize but also ensure feasibility. A related issue in the BO literature is the lack of a systematic stopping criterion. To solve these challenges, we develop a constrained cost-aware multi-fidelity BO (CMFBO) framework whose goal is to minimize overall sampling costs by utilizing inexpensive low-fidelity sources while ensuring feasibility. In our case, the constraints can change across the data sources and may be even black-box functions. We also introduce a systematic stopping criterion that addresses the long-lasting issue associated with BO's convergence assessment. Our framework is publicly available on GitHub through the GP+ Python package and herein we validate it's efficacy on multiple benchmark problems.", 'abstract_zh': '代价感知的约束多保真度贝叶斯优化框架', 'title_zh': '约束多保真度贝叶斯优化及其自动停止条件'}
{'arxiv_id': 'arXiv:2503.01121', 'title': 'Hybrid Metaheuristic Vehicle Routing Problem for Security Dispatch Operations', 'authors': 'Nguyen Gia Hien Vu, Yifan Tang, Rey Lim, G. Gary Wang', 'link': 'https://arxiv.org/abs/2503.01121', 'abstract': 'This paper investigates the optimization of the Vehicle Routing Problem for Security Dispatch (VRPSD). VRPSD focuses on security and patrolling applications which involve challenging constraints including precise timing and strict time windows. We propose three algorithms based on different metaheuristics, which are Adaptive Large Neighborhood Search (ALNS), Tabu Search (TS), and Threshold Accepting (TA). The first algorithm combines single-phase ALNS with TA, the second employs a multiphase ALNS with TA, and the third integrates multiphase ALNS, TS, and TA. Experiments are conducted on an instance comprising 251 customer requests. The results demonstrate that the third algorithm, the hybrid multiphase ALNS-TS-TA algorithm, delivers the best performance. This approach simultaneously leverages the large-area search capabilities of ALNS for exploration and effectively escapes local optima when the multiphase ALNS is coupled with TS and TA. Furthermore, in our experiments, the hybrid multiphase ALNS-TS-TA algorithm is the only one that shows potential for improving results with increased computation time across all attempts.', 'abstract_zh': '本文研究了保安调度车辆路线问题（VRPSD）的优化问题。VRPSD涉及包括精确时间要求和严格时间窗在内的挑战性约束条件。我们提出三种基于不同元启发式的算法，分别是自适应大邻域搜索（ALNS）、禁忌搜索（TS）和阈值接受法（TA）。第一个算法结合了一阶段ALNS与TA，第二个算法采用多阶段ALNS与TA，第三个算法则整合了多阶段ALNS、TS和TA。实验在包含251个客户请求的实例上进行。结果显示，第三种算法，即混合多阶段ALNS-TS-TA算法，性能最优。该方法利用ALNS的大范围搜索能力进行探索，并通过将多阶段ALNS与TS和TA相结合有效地跳出局部最优。此外，在我们的实验中，只有混合多阶段ALNS-TS-TA算法在所有尝试中随着计算时间增加显示出改进结果的潜力。', 'title_zh': '混合元启发式车辆路由问题在安全调度运营中的应用'}
{'arxiv_id': 'arXiv:2503.01086', 'title': 'FAIR: Facilitating Artificial Intelligence Resilience in Manufacturing Industrial Internet', 'authors': 'Yingyan Zeng, Ismini Lourentzou, Xinwei Deng, Ran Jin', 'link': 'https://arxiv.org/abs/2503.01086', 'abstract': 'Artificial intelligence (AI) systems have been increasingly adopted in the Manufacturing Industrial Internet (MII). Investigating and enabling the AI resilience is very important to alleviate profound impact of AI system failures in manufacturing and Industrial Internet of Things (IIoT) operations, leading to critical decision making. However, there is a wide knowledge gap in defining the resilience of AI systems and analyzing potential root causes and corresponding mitigation strategies. In this work, we propose a novel framework for investigating the resilience of AI performance over time under hazard factors in data quality, AI pipelines, and the cyber-physical layer. The proposed method can facilitate effective diagnosis and mitigation strategies to recover AI performance based on a multimodal multi-head self latent attention model. The merits of the proposed method are elaborated using an MII testbed of connected Aerosol Jet Printing (AJP) machines, fog nodes, and Cloud with inference tasks via AI pipelines.', 'abstract_zh': '人工智能系统在制造业工业互联网（MII）中的应用日益增多。研究和增强AI系统的韧性对于减轻AI系统故障对制造业和工业物联网（IIoT）运营的深刻影响至关重要，特别是在关键决策过程中。然而，有关AI系统韧性定义的知识和潜在根本原因及相应的缓解策略分析存在广泛的知识空白。本文提出了一种新颖的框架，用于在数据质量、AI管道和网络物理层的威胁因素下，研究AI性能随时间的韧性。所提出的方法可以基于多模态多头自潜注意模型促进有效诊断和恢复AI性能的策略。利用连接的 aerosol jet printing（AJP）机器、雾节点和云的MII测试床，并通过AI管道执行推理任务，详细阐述了所提出方法的优势。', 'title_zh': 'FAIR: 促进制造业工业互联网中的人工智能韧性'}
{'arxiv_id': 'arXiv:2503.01009', 'title': 'An Exact Solver for Satisfiability Modulo Counting with Probabilistic Circuits', 'authors': 'Jinzhao Li, Nan Jiang, Yexiang Xue', 'link': 'https://arxiv.org/abs/2503.01009', 'abstract': 'Satisfiability Modulo Counting (SMC) is a recently proposed general language to reason about problems integrating statistical and symbolic artificial intelligence. An SMC formula is an extended SAT formula in which the truth values of a few Boolean variables are determined by probabilistic inference. Existing approximate solvers optimize surrogate objectives, which lack formal guarantees. Current exact solvers directly integrate SAT solvers and probabilistic inference solvers resulting in slow performance because of many back-and-forth invocations of both solvers. We propose KOCO-SMC, an integrated exact SMC solver that efficiently tracks lower and upper bounds in the probabilistic inference process. It enhances computational efficiency by enabling early estimation of probabilistic inference using only partial variable assignments, whereas existing methods require full variable assignments. In the experiment, we compare KOCO-SMC with currently available approximate and exact SMC solvers on large-scale datasets and real-world applications. Our approach delivers high-quality solutions with high efficiency.', 'abstract_zh': '计数模合 satisfiability modulo counting (SMC) 是一种 recently 提出的一般性语言，用于处理结合统计和符号人工智能的问题。一个 SMC 公式是一个扩展的 SAT 公式，其中少数布尔变量的真值由概率推理决定。现有的近似求解器优化代理目标，缺乏正式保证。当前的精确求解器直接将 SAT 求解器和概率推理求解器相结合，导致由于两个求解器频繁调用而性能缓慢。我们提出了一种名为 KOCO-SMC 的集成精确 SMC 求解器，它可以高效地在概率推理过程中跟踪上下界。通过仅使用部分变量赋值来提前估计概率推理，它可以增强计算效率。现有的方法需要完整的变量赋值。在实验中，我们将 KOCO-SMC 与当前可用的近似和精确 SMC 求解器在大规模数据集和实际应用上进行比较。我们的方法在高效性方面提供了高质量的解决方案。', 'title_zh': '计数概率电路中的满足性精确求解器'}
{'arxiv_id': 'arXiv:2503.00753', 'title': 'Rethinking Light Decoder-based Solvers for Vehicle Routing Problems', 'authors': 'Ziwei Huang, Jianan Zhou, Zhiguang Cao, Yixin Xu', 'link': 'https://arxiv.org/abs/2503.00753', 'abstract': 'Light decoder-based solvers have gained popularity for solving vehicle routing problems (VRPs) due to their efficiency and ease of integration with reinforcement learning algorithms. However, they often struggle with generalization to larger problem instances or different VRP variants. This paper revisits light decoder-based approaches, analyzing the implications of their reliance on static embeddings and the inherent challenges that arise. Specifically, we demonstrate that in the light decoder paradigm, the encoder is implicitly tasked with capturing information for all potential decision scenarios during solution construction within a single set of embeddings, resulting in high information density. Furthermore, our empirical analysis reveals that the overly simplistic decoder struggles to effectively utilize this dense information, particularly as task complexity increases, which limits generalization to out-of-distribution (OOD) settings. Building on these insights, we show that enhancing the decoder capacity, with a simple addition of identity mapping and a feed-forward layer, can considerably alleviate the generalization issue. Experimentally, our method significantly enhances the OOD generalization of light decoder-based approaches on large-scale instances and complex VRP variants, narrowing the gap with the heavy decoder paradigm. Our code is available at: this https URL.', 'abstract_zh': '基于轻量解码器的方法由于其效率和与强化学习算法的集成简便性，在解决车辆路线问题（VRPs）方面受到了广泛关注。然而，它们在处理更大规模的问题实例或不同类型的VRP时往往表现出较差的泛化能力。本文重新审视了基于轻量解码器的方法，分析了它们依赖静态嵌入所带来的影响及其固有的挑战。具体而言，我们展示了在轻量解码器框架下，编码器在构建解决方案过程中隐式地需要在一个嵌入集中捕获所有潜在决策场景的信息，导致了高信息密度。此外，我们的实证分析表明，过于简化的解码器难以有效地利用这种密集信息，尤其是在任务复杂度增加时，这限制了它们在离分布（OOD）设置中的泛化能力。基于这些见解，我们展示了通过简单的添加身份映射和前馈层来增强解码器能力，可以显著缓解泛化问题。实验结果显示，我们的方法显著改善了基于轻量解码器的方法在大规模实例和复杂VRP变体中的离分布泛化能力，缩小了与重量解码器方法的差距。我们的代码可在以下链接获取：this https URL。', 'title_zh': '基于光线解码器的方法再思考：车辆路径问题求解器的设计'}
{'arxiv_id': 'arXiv:2503.00248', 'title': 'Human-AI Collaboration: Trade-offs Between Performance and Preferences', 'authors': 'Lukas William Mayer, Sheer Karny, Jackie Ayoub, Miao Song, Danyang Tian, Ehsan Moradi-Pari, Mark Steyvers', 'link': 'https://arxiv.org/abs/2503.00248', 'abstract': "Despite the growing interest in collaborative AI, designing systems that seamlessly integrate human input remains a major challenge. In this study, we developed a task to systematically examine human preferences for collaborative agents. We created and evaluated five collaborative AI agents with strategies that differ in the manner and degree they adapt to human actions. Participants interacted with a subset of these agents, evaluated their perceived traits, and selected their preferred agent. We used a Bayesian model to understand how agents' strategies influence the Human-AI team performance, AI's perceived traits, and the factors shaping human-preferences in pairwise agent comparisons. Our results show that agents who are more considerate of human actions are preferred over purely performance-maximizing agents. Moreover, we show that such human-centric design can improve the likability of AI collaborators without reducing performance. We find evidence for inequality-aversion effects being a driver of human choices, suggesting that people prefer collaborative agents which allow them to meaningfully contribute to the team. Taken together, these findings demonstrate how collaboration with AI can benefit from development efforts which include both subjective and objective metrics.", 'abstract_zh': '尽管对协作AI的兴趣日益增长，设计能够无缝整合人类输入的系统仍是一项重大挑战。本研究开发了一项任务，系统性地探索人类对协作代理的偏好。我们创建并评估了五种不同策略的协作AI代理，这些策略在适应人类行动的方式和程度上有所不同。参与者与这些代理中的部分进行了互动，评估了它们的感知特征，并选择了他们偏好的代理。我们使用贝叶斯模型来了解代理策略对人类-AI团队效能、AI的感知特征以及在两两代理比较中塑造人类偏好的因素的影响。研究结果表明，更考虑人类行动的代理比纯粹以性能最大化为目标的代理更受欢迎。此外，我们展示了以人类为中心的设计可以提高AI合作者的可接受性而不降低性能。我们发现了不平等厌恶效应可能是人类选择的驱动力，表明人们更偏好能够让他们在团队中做出有意义贡献的协作代理。综上所述，这些发现展示了在包括主观和客观指标的开发努力下，与AI的合作有望受益。', 'title_zh': '人类与人工智能协作：性能与偏好之间的权衡'}
{'arxiv_id': 'arXiv:2503.00237', 'title': 'Agentic AI Needs a Systems Theory', 'authors': 'Erik Miehling, Karthikeyan Natesan Ramamurthy, Kush R. Varshney, Matthew Riemer, Djallel Bouneffouf, John T. Richards, Amit Dhurandhar, Elizabeth M. Daly, Michael Hind, Prasanna Sattigeri, Dennis Wei, Ambrish Rawat, Jasmina Gajcin, Werner Geyer', 'link': 'https://arxiv.org/abs/2503.00237', 'abstract': 'The endowment of AI with reasoning capabilities and some degree of agency is widely viewed as a path toward more capable and generalizable systems. Our position is that the current development of agentic AI requires a more holistic, systems-theoretic perspective in order to fully understand their capabilities and mitigate any emergent risks. The primary motivation for our position is that AI development is currently overly focused on individual model capabilities, often ignoring broader emergent behavior, leading to a significant underestimation in the true capabilities and associated risks of agentic AI. We describe some fundamental mechanisms by which advanced capabilities can emerge from (comparably simpler) agents simply due to their interaction with the environment and other agents. Informed by an extensive amount of existing literature from various fields, we outline mechanisms for enhanced agent cognition, emergent causal reasoning ability, and metacognitive awareness. We conclude by presenting some key open challenges and guidance for the development of agentic AI. We emphasize that a systems-level perspective is essential for better understanding, and purposefully shaping, agentic AI systems.', 'abstract_zh': '赋予AI推理能力及一定程度的自主性被视为通往更强泛化能力系统的途径。我们认为，当前的自主AI发展需要采用更为整体的系统理论视角，以便全面理解其能力并缓解任何 emergent 风险。我们立场的主要动机是，当前的AI开发过于关注个体模型的能力，往往忽视了更广泛的 emergent 行为，导致对自主AI的真实能力和相关风险存在显著低估。我们描述了一些基本机制，说明先进的能力如何仅仅由于代理与环境及其他代理的交互而涌现（即使这些代理本身相对较简单）。基于大量来自不同领域的现有文献，我们概述了增强代理认知、涌现因果推理能力和元认知意识的机制。最后，我们提出了自主AI开发中一些关键的开放挑战和指导原则，并强调系统层面的视角对于更好地理解和有意塑造自主AI系统至关重要。', 'title_zh': '代理型AI需要系统理论'}
{'arxiv_id': 'arXiv:2503.01822', 'title': 'Projecting Assumptions: The Duality Between Sparse Autoencoders and Concept Geometry', 'authors': 'Sai Sumedh R. Hindupur, Ekdeep Singh Lubana, Thomas Fel, Demba Ba', 'link': 'https://arxiv.org/abs/2503.01822', 'abstract': 'Sparse Autoencoders (SAEs) are widely used to interpret neural networks by identifying meaningful concepts from their representations. However, do SAEs truly uncover all concepts a model relies on, or are they inherently biased toward certain kinds of concepts? We introduce a unified framework that recasts SAEs as solutions to a bilevel optimization problem, revealing a fundamental challenge: each SAE imposes structural assumptions about how concepts are encoded in model representations, which in turn shapes what it can and cannot detect. This means different SAEs are not interchangeable -- switching architectures can expose entirely new concepts or obscure existing ones. To systematically probe this effect, we evaluate SAEs across a spectrum of settings: from controlled toy models that isolate key variables, to semi-synthetic experiments on real model activations and finally to large-scale, naturalistic datasets. Across this progression, we examine two fundamental properties that real-world concepts often exhibit: heterogeneity in intrinsic dimensionality (some concepts are inherently low-dimensional, others are not) and nonlinear separability. We show that SAEs fail to recover concepts when these properties are ignored, and we design a new SAE that explicitly incorporates both, enabling the discovery of previously hidden concepts and reinforcing our theoretical insights. Our findings challenge the idea of a universal SAE and underscores the need for architecture-specific choices in model interpretability. Overall, we argue an SAE does not just reveal concepts -- it determines what can be seen at all.', 'abstract_zh': '稀疏自编码器（SAEs）广泛用于通过识别其表示中的有意义概念来解释神经网络。然而，SAEs真的能够揭示模型依赖的所有概念，还是倾向于某些类型的概念？我们引入了一个统一框架，将SAEs重新表述为两层优化问题的解，揭示了一个根本性挑战：每个SAE对概念如何编码到模型表示中的结构假设，反过来决定了它能够和不能够检测的内容。这意味着不同的SAEs并不是互换的——更改架构可能会暴露全新的概念或使现有概念变得不明显。为了系统地探查这种影响，我们在从隔离关键变量的受控玩具模型，到基于真实模型激活的半合成实验，再到大规模自然数据集等多个场合评估SAEs。在整个过程中，我们考察了现实世界概念常常具有的两种基本属性：内在维度的异构性（一些概念固然是低维的，而其他概念则不是）和非线性可分离性。我们证明当忽略这些属性时，SAEs无法恢复概念，并设计了一个新的SAE，明确地结合了这两方面，从而发现了此前隐藏的概念，并强化了我们的理论洞察。我们的研究挑战了普遍适用的SAE的概念，并强调了在模型解释中需要针对架构进行选择的必要性。总的来说，我们认为SAE不仅仅揭示概念，它还决定了能够被观察到的内容。', 'title_zh': '投影假设：稀疏自编码器与概念几何的对偶性'}
{'arxiv_id': 'arXiv:2503.01805', 'title': 'Depth-Width tradeoffs in Algorithmic Reasoning of Graph Tasks with Transformers', 'authors': 'Gilad Yehudai, Clayton Sanford, Maya Bechler-Speicher, Orr Fischer, Ran Gilad-Bachrach, Amir Globerson', 'link': 'https://arxiv.org/abs/2503.01805', 'abstract': 'Transformers have revolutionized the field of machine learning. In particular, they can be used to solve complex algorithmic problems, including graph-based tasks. In such algorithmic tasks a key question is what is the minimal size of a transformer that can implement a task. Recent work has begun to explore this problem for graph-based tasks, showing that for sub-linear embedding dimension (i.e., model width) logarithmic depth suffices. However, an open question, which we address here, is what happens if width is allowed to grow linearly. Here we analyze this setting, and provide the surprising result that with linear width, constant depth suffices for solving a host of graph-based problems. This suggests that a moderate increase in width can allow much shallower models, which are advantageous in terms of inference time. For other problems, we show that quadratic width is required. Our results demonstrate the complex and intriguing landscape of transformer implementations of graph-based algorithms. We support our theoretical results with empirical evaluations.', 'abstract_zh': '变压器已经革命性地改变了机器学习领域。特别是在基于图的任务中，一个关键问题是实现一个任务所需的最小变压器尺寸是多少。近期的研究已经开始探讨这一问题，表明对于次线性嵌入维度（即模型宽度）而言，对数深度就足够了。然而，一个未解决的问题是，如果允许宽度线性增长会怎样，这也是我们在此研究所要解决的问题。我们分析了这一设置，并提供了令人惊讶的结果：对于许多基于图的问题，常数深度就足够了。这表明适度增加宽度可以使模型更浅，从推断时间的角度来看有着显著优势。对于其他问题，我们证明需要二次宽度。我们的结果展示了基于图算法的变压器实现的复杂而引人入胜的景观。我们通过实证评估支持了我们的理论成果。', 'title_zh': '图任务中Transformer算法推理的深度-宽度权衡'}
{'arxiv_id': 'arXiv:2503.01776', 'title': 'Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation', 'authors': 'Tiansheng Wen, Yifei Wang, Zequn Zeng, Zhong Peng, Yudi Su, Xinyang Liu, Bo Chen, Hongwei Liu, Stefanie Jegelka, Chenyu You', 'link': 'https://arxiv.org/abs/2503.01776', 'abstract': 'Many large-scale systems rely on high-quality deep representations (embeddings) to facilitate tasks like retrieval, search, and generative modeling. Matryoshka Representation Learning (MRL) recently emerged as a solution for adaptive embedding lengths, but it requires full model retraining and suffers from noticeable performance degradations at short lengths. In this paper, we show that sparse coding offers a compelling alternative for achieving adaptive representation with minimal overhead and higher fidelity. We propose Contrastive Sparse Representation (CSR), a method that sparsifies pre-trained embeddings into a high-dimensional but selectively activated feature space. By leveraging lightweight autoencoding and task-aware contrastive objectives, CSR preserves semantic quality while allowing flexible, cost-effective inference at different sparsity levels. Extensive experiments on image, text, and multimodal benchmarks demonstrate that CSR consistently outperforms MRL in terms of both accuracy and retrieval speed-often by large margins-while also cutting training time to a fraction of that required by MRL. Our results establish sparse coding as a powerful paradigm for adaptive representation learning in real-world applications where efficiency and fidelity are both paramount. Code is available at this https URL', 'abstract_zh': 'Sparse Coding for Adaptive Representation Learning with Enhanced Efficiency and Fidelity', 'title_zh': '超越马特罗什卡：重新审视稀疏编码以实现适应性表示'}
{'arxiv_id': 'arXiv:2503.01758', 'title': 'Zero-Trust Artificial Intelligence Model Security Based on Moving Target Defense and Content Disarm and Reconstruction', 'authors': 'Daniel Gilkarov, Ran Dubin', 'link': 'https://arxiv.org/abs/2503.01758', 'abstract': 'This paper examines the challenges in distributing AI models through model zoos and file transfer mechanisms. Despite advancements in security measures, vulnerabilities persist, necessitating a multi-layered approach to mitigate risks effectively. The physical security of model files is critical, requiring stringent access controls and attack prevention solutions. This paper proposes a novel solution architecture composed of two prevention approaches. The first is Content Disarm and Reconstruction (CDR), which focuses on disarming serialization attacks that enable attackers to run malicious code as soon as the model is loaded. The second is protecting the model architecture and weights from attacks by using Moving Target Defense (MTD), alerting the model structure, and providing verification steps to detect such attacks. The paper focuses on the highly exploitable Pickle and PyTorch file formats. It demonstrates a 100% disarm rate while validated against known AI model repositories and actual malware attacks from the HuggingFace model zoo.', 'abstract_zh': '本文探讨了通过模型动物园和文件传输机制分布AI模型所面临的挑战。尽管安全性措施取得了进展，但依然存在漏洞，需要采用多层次的方法来有效缓解风险。模型文件的物理安全至关重要，需要严格的访问控制和攻击防护解决方案。本文提出了一种新颖的解决方案架构，由两种防护方法组成。首先是内容拆解与重组（CDR），专注于拆解序列化攻击，这种攻击允许攻击者在模型加载时立即运行恶意代码。其次是通过移动目标防御（MTD）保护模型架构和权重免受攻击，这将提示模型结构并提供验证步骤以检测此类攻击。本文专注于高度可利用的Pickle和PyTorch文件格式。它在验证已知的AI模型仓库和HuggingFace模型动物园的实际恶意软件攻击后，实现了100%的拆解率。', 'title_zh': '基于移动目标防御和内容卸载与重构的零信任人工智能模型安全'}
{'arxiv_id': 'arXiv:2503.01733', 'title': 'DISCOVER: Data-driven Identification of Sub-activities via Clustering and Visualization for Enhanced Activity Recognition in Smart Homes', 'authors': 'Alexander Karpekov, Sonia Chernova, Thomas Plötz', 'link': 'https://arxiv.org/abs/2503.01733', 'abstract': "Human Activity Recognition (HAR) using ambient sensors has great potential for practical applications, particularly in elder care and independent living. However, deploying HAR systems in real-world settings remains challenging due to the high cost of labeled data, the need for pre-segmented sensor streams, and the lack of flexibility in activity granularity. To address these limitations, we introduce DISCOVER, a method designed to discover fine-grained human sub-activities from unlabeled sensor data without relying on pre-segmentation. DISCOVER combines unsupervised feature extraction and clustering with a user-friendly visualization tool to streamline the labeling process. DISCOVER enables domain experts to efficiently annotate only a minimal set of representative cluster centroids, reducing the annotation workload to a small number of samples (0.05% of our dataset). We demonstrate DISCOVER's effectiveness through a re-annotation exercise on widely used HAR datasets, showing that it uncovers finer-grained activities and produces more nuanced annotations than traditional coarse labels. DISCOVER represents a step toward practical, deployable HAR systems that adapt to diverse real environments.", 'abstract_zh': '基于环境传感器的细粒度人类亚活动发现（DISCOVER）在实际应用中的潜力，特别是在养老和独立生活领域的应用。然而，在真实环境中部署人类活动识别（HAR）系统仍面临挑战，主要原因包括标注数据的成本高、需要预分段的传感器流以及活动粒度的灵活性不足。为了解决这些限制，我们提出了DISCOVER方法，该方法能够在无需依赖预分段的情况下从未标注的传感器数据中发现细粒度的人类亚活动。DISCOVER结合了无监督特征提取和聚类，并通过用户友好的可视化工具简化标注过程。DISCOVER使领域专家仅需标注少量具有代表性的聚类质心，从而将标注工作量减少到极少数样本（约0.05%的数据集）。通过在广泛使用的HAR数据集上进行重新标注实验，我们展示了DISCOVER的有效性，表明它能够发现更细粒度的活动并生成比传统粗粒度标签更细腻的标注。DISCOVER代表了迈向适应多元真实环境的实用可部署HAR系统的一步。', 'title_zh': 'DISCOVER：通过聚类和可视化实现的数据驱动的子活动识别以增强智能家居中的活动识别'}
{'arxiv_id': 'arXiv:2503.01702', 'title': 'Relating Piecewise Linear Kolmogorov Arnold Networks to ReLU Networks', 'authors': 'Nandi Schoots, Mattia Jacopo Villani, Niels uit de Bos', 'link': 'https://arxiv.org/abs/2503.01702', 'abstract': 'Kolmogorov-Arnold Networks are a new family of neural network architectures which holds promise for overcoming the curse of dimensionality and has interpretability benefits (arXiv:2404.19756). In this paper, we explore the connection between Kolmogorov Arnold Networks (KANs) with piecewise linear (univariate real) functions and ReLU networks. We provide completely explicit constructions to convert a piecewise linear KAN into a ReLU network and vice versa.', 'abstract_zh': 'Kolmogorov-Arnold网络是一种新的神经网络架构，有望克服维度灾难并具有可解释性优势（arXiv:2404.19756）。在这篇文章中，我们探讨了Kolmogorov Arnold网络（KANs）与分段线性（单变量实数）函数及ReLU网络之间的联系，提供了将分段线性KAN完全显式转换为ReLU网络的方法，反之亦然。', 'title_zh': '将分段线性柯尔莫哥洛夫-阿诺德网络与ReLU网络联系起来'}
{'arxiv_id': 'arXiv:2503.01669', 'title': 'An Efficient Continual Learning Framework for Multivariate Time Series Prediction Tasks with Application to Vehicle State Estimation', 'authors': 'Arvin Hosseinzadeh, Ladan Khoshnevisan, Mohammad Pirani, Shojaeddin Chenouri, Amir Khajepour', 'link': 'https://arxiv.org/abs/2503.01669', 'abstract': 'In continual time series analysis using neural networks, catastrophic forgetting (CF) of previously learned models when training on new data domains has always been a significant challenge. This problem is especially challenging in vehicle estimation and control, where new information is sequentially introduced to the model. Unfortunately, existing work on continual learning has not sufficiently addressed the adverse effects of catastrophic forgetting in time series analysis, particularly in multivariate output environments. In this paper, we present EM-ReSeleCT (Efficient Multivariate Representative Selection for Continual Learning in Time Series Tasks), an enhanced approach designed to handle continual learning in multivariate environments. Our approach strategically selects representative subsets from old and historical data and incorporates memory-based continual learning techniques with an improved optimization algorithm to adapt the pre-trained model on new information while preserving previously acquired information. Additionally, we develop a sequence-to-sequence transformer model (autoregressive model) specifically designed for vehicle state estimation. Moreover, we propose an uncertainty quantification framework using conformal prediction to assess the sensitivity of the memory size and to showcase the robustness of the proposed method. Experimental results from tests on an electric Equinox vehicle highlight the superiority of our method in continually learning new information while retaining prior knowledge, outperforming state-of-the-art continual learning methods. Furthermore, EM-ReSeleCT significantly reduces training time, a critical advantage in continual learning applications.', 'abstract_zh': '基于神经网络的连续时间序列分析中，训练新数据域时前学习模型的灾难性遗忘（CF）一直是重大挑战。特别是在车辆估计与控制中，新信息按序引入模型，使这一问题尤为棘手。现有的连续学习工作在时间序列分析中，特别是多变量输出环境中，尚未充分解决灾难性遗忘带来的负面影响。本文提出了一种增强方法EM-ReSeleCT（高效多变量代表性选择在时间序列任务中的连续学习），旨在处理多变量环境中的连续学习问题。该方法战略性地从旧数据和历史数据中选择代表性子集，并结合基于记忆的连续学习技术和改进的优化算法，在适应新信息的同时保留先前获取的信息。此外，我们还开发了一种特别为车辆状态估计设计的序列到序列变换模型（自回归模型），并提出了一种基于校准预测的不确定性量化框架来评估内存大小的敏感性，并展示所提方法的鲁棒性。实验结果表明，EM-ReSeleCT在保留先前知识的同时连续学习新信息方面优于现有的先进连续学习方法，且显著减少了训练时间，这是连续学习应用中的一大优势。', 'title_zh': '多变量时间序列预测任务的高效连续学习框架及其在车辆状态估计中的应用'}
{'arxiv_id': 'arXiv:2503.01619', 'title': 'Advancing vision-language models in front-end development via data synthesis', 'authors': 'Tong Ge, Yashu Liu, Jieping Ye, Tianyi Li, Chao Wang', 'link': 'https://arxiv.org/abs/2503.01619', 'abstract': 'Modern front-end (FE) development, especially when leveraging the unique features of frameworks like React and Vue, presents distinctive challenges. These include managing modular architectures, ensuring synchronization between data and visual outputs for declarative rendering, and adapting reusable components to various scenarios. Such complexities make it particularly difficult for state-of-the-art large vision-language models (VLMs) to generate accurate and functional code directly from design images. To address these challenges, we propose a reflective agentic workflow that synthesizes high-quality image-text data to capture the diverse characteristics of FE development. This workflow automates the extraction of self-contained\\footnote{A \\textbf{self-contained} code snippet is one that encapsulates all necessary logic, styling, and dependencies, ensuring it functions independently without requiring external imports or context.} code snippets from real-world projects, renders the corresponding visual outputs, and generates detailed descriptions that link design elements to functional code. To further expand the scope and utility of the synthesis, we introduce three data synthesis strategies: Evolution-based synthesis, which enables scalable and diverse dataset expansion; Waterfall-Model-based synthesis, which generates logically coherent code derived from system requirements; and Additive Development synthesis, which iteratively increases the complexity of human-authored components. We build a large vision-language model, Flame, trained on the synthesized datasets and demonstrate its effectiveness in generating React code via the $\\text{pass}@k$ metric. Our results suggest that a code VLM trained to interpret images before code generation may achieve better performance.', 'abstract_zh': '现代前端（FE）开发，特别是利用React和Vue等框架的独特功能时，面临着独特的挑战。这些挑战包括管理模块化架构、确保数据与视觉输出之间的同步以支持声明性渲染，以及将可重用组件适应各种场景。这些复杂性使得最先进的大型 vision-language 模型（VLMs）难以直接从设计图像生成准确且功能性的代码。为了解决这些挑战，我们提出了一种反思性代理工作流，以综合高质量的图像-文本数据来捕捉FE开发的多样化特征。该工作流自动提取来自真实项目且自包含的代码片段，渲染相应的视觉输出，并生成详细描述，将设计元素与功能代码关联起来。为了进一步扩大综合的范围和实用性，我们介绍了三种数据综合策略：基于进化的方法，实现可扩展且多样化的数据集扩展；瀑布模型方法，生成逻辑连贯的代码，源自系统需求；以及逐步开发方法，逐步增加人类撰写的组件复杂度。我们构建了一个基于综合数据集训练的大型 vision-language 模型Flame，并通过$\\text{pass}@k$指标展示了其在生成React代码方面的有效性。我们的结果显示，一个在代码生成前解释图像的代码VLM可能表现出更好的性能。', 'title_zh': '通过数据合成推动前端开发中的视觉语言模型发展'}
{'arxiv_id': 'arXiv:2503.01595', 'title': 'STAR: Stability-Inducing Weight Perturbation for Continual Learning', 'authors': 'Masih Eskandar, Tooba Imtiaz, Davin Hill, Zifeng Wang, Jennifer Dy', 'link': 'https://arxiv.org/abs/2503.01595', 'abstract': 'Humans can naturally learn new and varying tasks in a sequential manner. Continual learning is a class of learning algorithms that updates its learned model as it sees new data (on potentially new tasks) in a sequence. A key challenge in continual learning is that as the model is updated to learn new tasks, it becomes susceptible to catastrophic forgetting, where knowledge of previously learned tasks is lost. A popular approach to mitigate forgetting during continual learning is to maintain a small buffer of previously-seen samples and to replay them during training. However, this approach is limited by the small buffer size, and while forgetting is reduced, it is still present. In this paper, we propose a novel loss function, STAR, that exploits the worst-case parameter perturbation that reduces the KL-divergence of model predictions with that of its local parameter neighborhood to promote stability and alleviate forgetting. STAR can be combined with almost any existing rehearsal-based method as a plug-and-play component. We empirically show that STAR consistently improves the performance of existing methods by up to 15% across varying baselines and achieves superior or competitive accuracy to that of state-of-the-art methods aimed at improving rehearsal-based continual learning.', 'abstract_zh': '人类可以自然地以序贯方式学习新的和变化的任务。连续学习是一类在序列中遇到新数据（可能是新任务）时更新其学习模型的算法。连续学习中的一个关键挑战是在模型被更新以学习新任务时，它会变得容易出现灾难性遗忘，即先前学习的任务的知识被丢失。减轻连续学习中遗忘的一个流行方法是维护一个小型先前见过的样本缓冲区，并在训练期间重新播放它们。然而，这种方法受限于小缓冲区的大小，虽然遗忘减少了，但仍然存在。在本文中，我们提出了一种新颖的损失函数STAR，它利用最坏情况参数扰动来减小模型预测与其局部参数邻域之间的KL散度，从而促进稳定性和减轻遗忘。STAR可以与几乎所有现有的重温方法无缝结合。我们实验证明，STAR可以在多种基线方法上提高现有方法的性能多达15%，并在目标于改进重温式连续学习的最新方法中实现了优于或可竞争的准确率。', 'title_zh': 'STAR：诱导稳定性的权重扭动用于连续学习'}
{'arxiv_id': 'arXiv:2503.01580', 'title': 'A Selective Learning Method for Temporal Graph Continual Learning', 'authors': 'Hanmo Liu, Shimin Di, Haoyang Li, Xun Jian, Yue Wang, Lei Chen', 'link': 'https://arxiv.org/abs/2503.01580', 'abstract': 'Node classification is a key task in temporal graph learning (TGL). Real-life temporal graphs often introduce new node classes over time, but existing TGL methods assume a fixed set of classes. This assumption brings limitations, as updating models with full data is costly, while focusing only on new classes results in forgetting old ones. Graph continual learning (GCL) methods mitigate forgetting using old-class subsets but fail to account for their evolution. We define this novel problem as temporal graph continual learning (TGCL), which focuses on efficiently maintaining up-to-date knowledge of old classes. To tackle TGCL, we propose a selective learning framework that substitutes the old-class data with its subsets, Learning Towards the Future (LTF). We derive an upper bound on the error caused by such replacement and transform it into objectives for selecting and learning subsets that minimize classification error while preserving the distribution of the full old-class data. Experiments on three real-world datasets validate the effectiveness of LTF on TGCL.', 'abstract_zh': '时间图持续学习（TGCL）是时间图学习（TGL）中的一个关键任务。现实中的时间图往往会随时间引入新的节点类，但现有TGL方法假设类集是固定的。这一假设带来了局限性，因为用全数据更新模型代价高昂，而仅专注于新类会导致遗忘旧类。图持续学习（GCL）方法通过使用旧类子集来减轻遗忘，但未能考虑到类的演变。我们定义了这个新的问题为时间图持续学习（TGCL），专注于高效维持旧类的最新知识。为解决TGCL，我们提出了一种选择性学习框架，用旧类子集替代旧类数据，称为“向未来学习”（Learning Towards the Future，LTF）。我们推导出由于这种替代引起的误差上限，并将其转化为目标，以选择和学习最小化分类误差的同时保持全旧类数据分布的子集。在三个真实世界的数据集上的实验验证了LTF在TGCL上的有效性。', 'title_zh': '时空图持续学习的选择性学习方法'}
{'arxiv_id': 'arXiv:2503.01557', 'title': 'MoCFL: Mobile Cluster Federated Learning Framework for Highly Dynamic Network', 'authors': 'Kai Fang, Jiangtao Deng, Chengzu Dong, Usman Naseem, Tongcun Liu, Hailin Feng, Wei Wang', 'link': 'https://arxiv.org/abs/2503.01557', 'abstract': 'Frequent fluctuations of client nodes in highly dynamic mobile clusters can lead to significant changes in feature space distribution and data drift, posing substantial challenges to the robustness of existing federated learning (FL) strategies. To address these issues, we proposed a mobile cluster federated learning framework (MoCFL). MoCFL enhances feature aggregation by introducing an affinity matrix that quantifies the similarity between local feature extractors from different clients, addressing dynamic data distribution changes caused by frequent client churn and topology changes. Additionally, MoCFL integrates historical and current feature information when training the global classifier, effectively mitigating the catastrophic forgetting problem frequently encountered in mobile scenarios. This synergistic combination ensures that MoCFL maintains high performance and stability in dynamically changing mobile environments. Experimental results on the UNSW-NB15 dataset show that MoCFL excels in dynamic environments, demonstrating superior robustness and accuracy while maintaining reasonable training costs.', 'abstract_zh': '频繁的客户端节点波动在高度动态的移动集群中会导致特征空间分布和数据漂移的显著变化，对现有联邦学习（FL）策略的稳健性构成了重大挑战。为了解决这些问题，我们提出了一种移动集群联邦学习框架（MoCFL）。MoCFL通过引入量化不同客户端本地特征提取器之间相似性的亲和矩阵，增强了特征聚合，以应对由频繁的客户端波动和拓扑变化引起的数据分布变化。此外，MoCFL在训练全局分类器时结合历史和当前的特征信息，有效地缓解了移动场景中经常遇到的灾难性遗忘问题。这种协同组合确保MoCFL在动态变化的移动环境中保持高性能和稳定性。实验结果表明，MoCFL在动态环境中表现出色，展现了优越的鲁棒性和准确性，同时保持了合理的训练成本。', 'title_zh': '移动簇联邦学习框架：高度动态网络环境下的学习'}
{'arxiv_id': 'arXiv:2503.01556', 'title': 'Effective High-order Graph Representation Learning for Credit Card Fraud Detection', 'authors': 'Yao Zou, Dawei Cheng', 'link': 'https://arxiv.org/abs/2503.01556', 'abstract': "Credit card fraud imposes significant costs on both cardholders and issuing banks. Fraudsters often disguise their crimes, such as using legitimate transactions through several benign users to bypass anti-fraud detection. Existing graph neural network (GNN) models struggle with learning features of camouflaged, indirect multi-hop transactions due to their inherent over-smoothing issues in deep multi-layer aggregation, presenting a major challenge in detecting disguised relationships. Therefore, in this paper, we propose a novel High-order Graph Representation Learning model (HOGRL) to avoid incorporating excessive noise during the multi-layer aggregation process. In particular, HOGRL learns different orders of \\emph{pure} representations directly from high-order transaction graphs. We realize this goal by effectively constructing high-order transaction graphs first and then learning the \\emph{pure} representations of each order so that the model could identify fraudsters' multi-hop indirect transactions via multi-layer \\emph{pure} feature learning. In addition, we introduce a mixture-of-expert attention mechanism to automatically determine the importance of different orders for jointly optimizing fraud detection performance. We conduct extensive experiments in both the open source and real-world datasets, the result demonstrates the significant improvements of our proposed HOGRL compared with state-of-the-art fraud detection baselines. HOGRL's superior performance also proves its effectiveness in addressing high-order fraud camouflage criminals.", 'abstract_zh': '信用卡欺诈对持卡人和发卡银行造成了显著的经济损失。欺诈者常常掩藏其犯罪行为，例如通过几个良性用户的合法交易进行多跳间接操作以规避反欺诈检测。现有的图神经网络（GNN）模型在进行深层多层聚合时由于固有的过度平滑问题，在学习隐藏的、间接的多跳交易特征时存在困难，给检测隐藏关系带来了重大挑战。因此，在本文中，我们提出了一种新的高阶图表示学习模型（HOGRL），以避免在多层聚合过程中引入过多噪声。特别是，HOGRL 直接从高阶交易图中学习不同阶次的纯净表示。我们通过有效构建高阶交易图并学习每一阶的纯净表示来实现这一目标，使得模型能够通过多层纯净特征学习来识别欺诈者的多跳间接交易。此外，我们引入了一种专家混合注意力机制，以自动确定不同阶次的重要性，从而联合优化欺诈检测性能。我们在开源数据集和真实世界数据集中进行了广泛的实验，结果表明，与最先进的欺诈检测基准相比，我们提出的HOGRL在欺诈检测性能上具有显著的改进。HOGRL 的出色性能进一步证明了其在应对高阶欺诈伪装犯罪方面的有效性。', 'title_zh': '有效的高阶图表示学习在信用卡欺诈检测中的应用'}
{'arxiv_id': 'arXiv:2503.01544', 'title': 'Compositional Reasoning with Transformers, RNNs, and Chain of Thought', 'authors': 'Gilad Yehudai, Noah Amsel, Joan Bruna', 'link': 'https://arxiv.org/abs/2503.01544', 'abstract': 'We study and compare the expressive power of transformers, RNNs, and transformers with chain of thought tokens on a simple and natural class of problems we term Compositional Reasoning Questions (CRQ). This family captures problems like evaluating Boolean formulas and multi-step word problems. Assuming standard hardness assumptions from circuit complexity and communication complexity, we prove that none of these three architectures is capable of solving CRQs unless some hyperparameter (depth, embedding dimension, and number of chain of thought tokens, respectively) grows with the size of the input. We also provide a construction for each architecture that solves CRQs. For transformers, our construction uses depth that is logarithmic in the problem size. For RNNs, logarithmic embedding dimension is necessary and sufficient, so long as the inputs are provided in a certain order. (Otherwise, a linear dimension is necessary). For transformers with chain of thought, our construction uses $n$ CoT tokens. These results show that, while CRQs are inherently hard, there are several different ways for language models to overcome this hardness. Even for a single class of problems, each architecture has strengths and weaknesses, and none is strictly better than the others.', 'abstract_zh': '我们研究并比较了Transformer、RNN以及带有链式推理标记的Transformer在一类称为组合推理问题（CRQ）的简单自然问题上的表达能力。我们证明，除非某些超参数（深度、嵌入维度和链式推理标记的数量分别）随着输入规模的增长而增长，否则这三种架构均无法解决CRQ。我们还为每种架构提供了解决CRQ的构造方法。对于Transformer，我们的构造使用深度与问题规模的对数成比例。对于RNN，嵌入维度需要是问题规模的对数比例，前提是输入按照某种顺序提供（否则，嵌入维度需要是线性的）。对于带有链式推理的Transformer，我们的构造使用n个链式推理标记。这些结果表明，虽然CRQ本质上是困难的，但语言模型可以通过多种方式克服这种困难。即使对于单一类别的问题，每种架构也有各自的优缺点，没有一种架构在所有情况下都优于其他架构。', 'title_zh': 'Transformer、RNN和思维链的组合推理'}
{'arxiv_id': 'arXiv:2503.01536', 'title': 'Entailment vs. Verification for Partial-assignment Satisfiability and Enumeration', 'authors': 'Roberto Sebastiani', 'link': 'https://arxiv.org/abs/2503.01536', 'abstract': 'Many procedures for SAT-related problems, in particular for those requiring the complete enumeration of satisfying truth assignments, rely their efficiency and effectiveness on the detection of (possibly small) partial assignments satisfying an input formula. Surprisingly, there seems to be no unique universally-agreed definition of formula satisfaction by a partial assignment in the literature. In this paper we analyze in deep the issue of satisfaction by partial assignments, raising a flag about some ambiguities and subtleties of this concept, and investigating their practical consequences. We identify two alternative notions that are implicitly used in the literature, namely verification and entailment, which coincide if applied to CNF formulas but differ and present complementary properties if applied to non-CNF or to existentially-quantified formulas. We show that, although the former is easier to check and as such is implicitly used by most current search procedures, the latter has better theoretical properties, and can improve the efficiency and effectiveness of enumeration procedures.', 'abstract_zh': '关于部分赋值满足性的歧义与影响：从验证到蕴含的分析', 'title_zh': '部分赋值满足性与枚举的蕴含 vs. 验证'}
{'arxiv_id': 'arXiv:2503.01507', 'title': 'Compare different SG-Schemes based on large least square problems', 'authors': 'Ramkrishna Acharya', 'link': 'https://arxiv.org/abs/2503.01507', 'abstract': 'This study reviews some of the popular stochastic gradient-based schemes based on large least-square problems. These schemes, often called optimizers in machine learning play a crucial role in finding better parameters of a model. Hence this study focuses on viewing such optimizers with different hyper-parameters and analyzing them based on least square problems. Codes that produced results in this work are available on this https URL.', 'abstract_zh': '本研究回顾了一些基于大型最小二乘问题的流行随机梯度方案。这些方案在机器学习中通常被称为优化器，对于找到模型更好的参数起着关键作用。因此，本研究重点在于从不同超参数的角度审视这些优化器，并基于最小二乘问题进行分析。本工作中产生的代码可在以下链接获取。', 'title_zh': '基于大规模最小二乘问题的不同SG-Schemes比较'}
{'arxiv_id': 'arXiv:2503.01505', 'title': 'Lossy Neural Compression for Geospatial Analytics: A Review', 'authors': 'Carlos Gomes, Isabelle Wittmann, Damien Robert, Johannes Jakubik, Tim Reichelt, Michele Martone, Stefano Maurogiovanni, Rikard Vinge, Jonas Hurst, Erik Scheurer, Rocco Sedona, Thomas Brunschwiler, Stefan Kesselheim, Matej Batic, Philip Stier, Jan Dirk Wegner, Gabriele Cavallaro, Edzer Pebesma, Michael Marszalek, Miguel A Belenguer-Plomer, Kennedy Adriko, Paolo Fraccaro, Romeo Kienzler, Rania Briq, Sabrina Benassou, Michele Lazzarini, Conrad M Albrecht', 'link': 'https://arxiv.org/abs/2503.01505', 'abstract': 'Over the past decades, there has been an explosion in the amount of available Earth Observation (EO) data. The unprecedented coverage of the Earth\'s surface and atmosphere by satellite imagery has resulted in large volumes of data that must be transmitted to ground stations, stored in data centers, and distributed to end users. Modern Earth System Models (ESMs) face similar challenges, operating at high spatial and temporal resolutions, producing petabytes of data per simulated day.\nData compression has gained relevance over the past decade, with neural compression (NC) emerging from deep learning and information theory, making EO data and ESM outputs ideal candidates due to their abundance of unlabeled data.\nIn this review, we outline recent developments in NC applied to geospatial data. We introduce the fundamental concepts of NC including seminal works in its traditional applications to image and video compression domains with focus on lossy compression. We discuss the unique characteristics of EO and ESM data, contrasting them with "natural images", and explain the additional challenges and opportunities they present. Moreover, we review current applications of NC across various EO modalities and explore the limited efforts in ESM compression to date.\nThe advent of self-supervised learning (SSL) and foundation models (FM) has advanced methods to efficiently distill representations from vast unlabeled data. We connect these developments to NC for EO, highlighting the similarities between the two fields and elaborate on the potential of transferring compressed feature representations for machine--to--machine communication.\nBased on insights drawn from this review, we devise future directions relevant to applications in EO and ESM.', 'abstract_zh': '过去几十年间，地球观测（EO）数据的数量出现了爆炸性增长。卫星影像前所未有的地球表面和大气覆盖范围产生了大量需要传输到地面站、存储在数据中心并分发给最终用户的数据。现代地球系统模型（ESMs）面临着类似的挑战，它们以高空间和时间分辨率运行，每日生成数 PB 的数据。\n数据压缩在过去十年中变得尤为重要，神经压缩（NC）从深度学习和信息论中发展而来，使得由于其丰富的未标注数据，EO数据和ESM输出成为了潜在的理想应用对象。\n在这篇综述中，我们概述了神经压缩（NC）在地理空间数据中的最新发展。我们介绍了神经压缩的基本概念，包括其传统应用到图像和视频压缩领域的经典工作，并着重讨论了具有损益压缩的特征。我们讨论了EO和ESM数据的独特特性，将它们与“自然图像”进行对比，解释了它们所带来的额外挑战与机会。此外，我们回顾了NC在各种EO模态中的当前应用，并探讨了到目前为止在ESM压缩方面所做的有限努力。\n自监督学习（SSL）和基础模型（FM）的发展促进了从大量未标注数据中高效提取表示的方法。我们将这些进展与EO中的神经压缩（NC）联系起来，强调了两个领域之间的相似之处，并阐述了传递压缩特征表示以实现机器对机器通信的潜力。\n基于本文综述所得的见解，我们提出了与EO和ESM应用相关的发展方向。', 'title_zh': '地理空间分析中的有损神经压缩：一个综述'}
{'arxiv_id': 'arXiv:2503.01470', 'title': 'Position: Ensuring mutual privacy is necessary for effective external evaluation of proprietary AI systems', 'authors': 'Ben Bucknall, Robert F. Trager, Michael A. Osborne', 'link': 'https://arxiv.org/abs/2503.01470', 'abstract': "The external evaluation of AI systems is increasingly recognised as a crucial approach for understanding their potential risks. However, facilitating external evaluation in practice faces significant challenges in balancing evaluators' need for system access with AI developers' privacy and security concerns. Additionally, evaluators have reason to protect their own privacy - for example, in order to maintain the integrity of held-out test sets. We refer to the challenge of ensuring both developers' and evaluators' privacy as one of providing mutual privacy. In this position paper, we argue that (i) addressing this mutual privacy challenge is essential for effective external evaluation of AI systems, and (ii) current methods for facilitating external evaluation inadequately address this challenge, particularly when it comes to preserving evaluators' privacy. In making these arguments, we formalise the mutual privacy problem; examine the privacy and access requirements of both model owners and evaluators; and explore potential solutions to this challenge, including through the application of cryptographic and hardware-based approaches.", 'abstract_zh': 'AI系统外部评估中外隐私保护挑战及其应对策略', 'title_zh': '确保相互隐私是有效评估私有AI系统的必要条件。'}
{'arxiv_id': 'arXiv:2503.01461', 'title': 'Towards Widening The Distillation Bottleneck for Reasoning Models', 'authors': 'Huifeng Yin, Yu Zhao, Minghao Wu, Xuanfan Ni, Bo Zeng, Hao Wang, Tianqi Shi, Liangying Shao, Chenyang Lyu, Longyue Wang, Weihua Luo, Kaifu Zhang', 'link': 'https://arxiv.org/abs/2503.01461', 'abstract': 'Large Reasoning Models(LRMs) such as OpenAI o1 and DeepSeek-R1 have shown remarkable reasoning capabilities by scaling test-time compute and generating long Chain-of-Thought(CoT). Distillation--post-training on LRMs-generated data--is a straightforward yet effective method to enhance the reasoning abilities of smaller models, but faces a critical bottleneck: we found that distilled long CoT data poses learning difficulty for small models and leads to the inheritance of biases (i.e. over-thinking) when using Supervised Fine-tuning(SFT) and Reinforcement Learning(RL) methods. To alleviate this bottleneck, we propose constructing tree-based CoT data from scratch via Monte Carlo Tree Search(MCTS). We then exploit a set of CoT-aware approaches, including Thoughts Length Balance, Fine-grained DPO, and Joint Post-training Objective, to enhance SFT and RL on the construted data.', 'abstract_zh': '大规模推理模型（LRMs）如OpenAI o1和DeepSeek-R1通过扩展测试时计算资源并生成长链推理（CoT）展示了显著的推理能力。从LRMs生成的数据进行蒸馏是一种简单而有效的方法来提升小型模型的推理能力，但面临一个关键瓶颈：我们发现蒸馏的长链推理数据对小型模型的学习构成了困难，并在使用监督微调（SFT）和强化学习（RL）方法时导致了偏见的传承（即过度推理）。为了缓解这一瓶颈，我们提出通过蒙特卡洛树搜索（MCTS）从头构建基于树的链推理数据。然后我们利用一系列链推理（CoT）感知的方法，包括思路长度平衡、细粒度DPO和联合后训练目标，来增强在构建数据上进行的SFT和RL。', 'title_zh': '扩展推理模型中的蒸馏瓶颈'}
{'arxiv_id': 'arXiv:2503.01457', 'title': 'Structural Deep Encoding for Table Question Answering', 'authors': 'Raphaël Mouravieff, Benjamin Piwowarski, Sylvain Lamprier', 'link': 'https://arxiv.org/abs/2503.01457', 'abstract': 'Although Transformers-based architectures excel at processing textual information, their naive adaptation for tabular data often involves flattening the table structure. This simplification can lead to the loss of essential inter-dependencies between rows, columns, and cells, while also posing scalability challenges for large tables. To address these issues, prior works have explored special tokens, structured embeddings, and sparse attention patterns. In this paper, we conduct a comprehensive analysis of tabular encoding techniques, which highlights the crucial role of attention sparsity in preserving structural information of tables. We also introduce a set of novel sparse attention mask designs for tabular data, that not only enhance computational efficiency but also preserve structural integrity, leading to better overall performance.', 'abstract_zh': '基于Transformer的表数据编码技术：注意力稀疏性的关键作用与新颖设计', 'title_zh': '表格问题回答的结构深度编码'}
{'arxiv_id': 'arXiv:2503.01453', 'title': 'AC-Lite : A Lightweight Image Captioning Model for Low-Resource Assamese Language', 'authors': 'Pankaj Choudhury, Yogesh Aggarwal, Prithwijit Guha, Sukumar Nandi', 'link': 'https://arxiv.org/abs/2503.01453', 'abstract': "Neural networks have significantly advanced AI applications, yet their real-world adoption remains constrained by high computational demands, hardware limitations, and accessibility challenges. In image captioning, many state-of-the-art models have achieved impressive performances while relying on resource-intensive architectures. This made them impractical for deployment on resource-constrained devices. This limitation is particularly noticeable for applications involving low-resource languages. We demonstrate the case of image captioning in Assamese language, where lack of effective, scalable systems can restrict the accessibility of AI-based solutions for native Assamese speakers. This work presents AC-Lite, a computationally efficient model for image captioning in low-resource Assamese language. AC-Lite reduces computational requirements by replacing computation-heavy visual feature extractors like FasterRCNN with lightweight ShuffleNetv2x1.5. Additionally, Gated Recurrent Units (GRUs) are used as the caption decoder to further reduce computational demands and model parameters. Furthermore, the integration of bilinear attention enhances the model's overall performance. AC-Lite can operate on edge devices, thereby eliminating the need for computation on remote servers. The proposed AC-Lite model achieves 82.3 CIDEr score on the COCO-AC dataset with 1.098 GFLOPs and 25.65M parameters.", 'abstract_zh': '神经网络虽然显著推动了AI应用的发展，但在实际应用中仍受限于高计算需求、硬件限制和可访问性挑战。在图像_caption方面，许多最先进的模型虽然实现了令人印象深刻的性能，但依赖于计算密集型架构，这使它们在资源受限的设备上难以部署。这一限制在涉及低资源语言的应用中尤为明显。本文以奥里姆语（Assamese）图像Caption为例，展示了缺乏有效可扩展系统的限制如何阻碍了基于AI的解决方案对原生奥里姆语使用者的访问。本文提出了AC-Lite，一种针对低资源奥里姆语图像Caption的计算高效模型。AC-Lite通过使用轻量级的ShuffleNetv2x1.5替代计算密集型的视觉特征提取器如FasterRCNN，减少了计算需求。同时，使用门控循环单元（GRUs）作为Caption解码器进一步降低了计算需求和模型参数数量。此外，引入双线性注意力机制提高了模型的整体性能。AC-Lite可以在边缘设备上运行，从而消除了在远程服务器上进行计算的需求。所提出的AC-Lite模型在COCO-AC数据集上的CIDEr得分为82.3，计算量为1.098 GFLOPs，参数量为25.65M。', 'title_zh': 'AC-Lite：一种用于低资源阿萨姆语的轻量级图像 Captioning 模型'}
{'arxiv_id': 'arXiv:2503.01407', 'title': 'Divide and Conquer: Heterogeneous Noise Integration for Diffusion-based Adversarial Purification', 'authors': 'Gaozheng Pei, Shaojie Lyu, Gong Chen, Ke Ma, Qianqian Xu, Yingfei Sun, Qingming Huang', 'link': 'https://arxiv.org/abs/2503.01407', 'abstract': 'Existing diffusion-based purification methods aim to disrupt adversarial perturbations by introducing a certain amount of noise through a forward diffusion process, followed by a reverse process to recover clean examples. However, this approach is fundamentally flawed: the uniform operation of the forward process across all pixels compromises normal pixels while attempting to combat adversarial perturbations, resulting in the target model producing incorrect predictions. Simply relying on low-intensity noise is insufficient for effective defense. To address this critical issue, we implement a heterogeneous purification strategy grounded in the interpretability of neural networks. Our method decisively applies higher-intensity noise to specific pixels that the target model focuses on while the remaining pixels are subjected to only low-intensity noise. This requirement motivates us to redesign the sampling process of the diffusion model, allowing for the effective removal of varying noise levels. Furthermore, to evaluate our method against strong adaptative attack, our proposed method sharply reduces time cost and memory usage through a single-step resampling. The empirical evidence from extensive experiments across three datasets demonstrates that our method outperforms most current adversarial training and purification techniques by a substantial margin.', 'abstract_zh': '现有的基于扩散的方法旨在通过前向扩散过程引入一定量的噪声来破坏对抗性扰动，随后通过反向过程恢复干净的样本。然而，这种方法根本上是存在问题的：前向过程在所有像素上的均匀操作会损害正常像素，同时试图对抗对抗性扰动，导致目标模型产生错误预测。仅仅依赖低强度噪声不足以实现有效防御。为解决这一关键问题，我们提出了一种基于神经网络可解释性的异质净化策略。我们的方法在目标模型关注的特定像素上应用高强度噪声，而其他像素仅受到低强度噪声的影响。这种要求促使我们重新设计扩散模型的采样过程，以有效地去除不同噪声水平。此外，为了评估我们的方法对抗强适应性攻击的效果，我们提出的方法通过一次采样显著降低了时间和内存使用成本。广泛的实验结果表明，我们的方法在三个数据集上明显优于大多数现有的对抗训练和净化技术。', 'title_zh': '分而治之：基于异质噪声集成的扩散 adversarial 纯化方法'}
{'arxiv_id': 'arXiv:2503.01394', 'title': 'Enhancing Social Media Rumor Detection: A Semantic and Graph Neural Network Approach for the 2024 Global Election', 'authors': 'Liu Yan, Liu Yunpeng, Zhao Liang', 'link': 'https://arxiv.org/abs/2503.01394', 'abstract': 'The development of social media platforms has revolutionized the speed and manner in which information is disseminated, leading to both beneficial and detrimental effects on society. While these platforms facilitate rapid communication, they also accelerate the spread of rumors and extremist speech, impacting public perception and behavior significantly. This issue is particularly pronounced during election periods, where the influence of social media on election outcomes has become a matter of global concern. With the unprecedented number of elections in 2024, against this backdrop, the election ecosystem has encountered unprecedented challenges. This study addresses the urgent need for effective rumor detection on social media by proposing a novel method that combines semantic analysis with graph neural networks. We have meticulously collected a dataset from PolitiFact and Twitter, focusing on politically relevant rumors. Our approach involves semantic analysis using a fine-tuned BERT model to vectorize text content and construct a directed graph where tweets and comments are nodes, and interactions are edges. The core of our method is a graph neural network, SAGEWithEdgeAttention, which extends the GraphSAGE model by incorporating first-order differences as edge attributes and applying an attention mechanism to enhance feature aggregation. This innovative approach allows for the fine-grained analysis of the complex social network structure, improving rumor detection accuracy. The study concludes that our method significantly outperforms traditional content analysis and time-based models, offering a theoretically sound and practically efficient solution.', 'abstract_zh': '社交媒体平台的发展革新了信息传播的速度和方式，对社会产生了有益和有害的影响。这些平台促进了快速的沟通，但也加速了谣言和极端言论的传播，对公众的认知和行为产生了显著影响。这一问题在选举期间尤为突出，社交媒体对选举结果的影响已成为全球关注的问题。面对2024年前所未有的选举次数，选举生态系统遭遇了前所未有的挑战。本研究针对社交媒体中有效检测谣言的迫切需求，提出了一种结合语义分析和图神经网络的新型方法。我们精心从PolitiFact和Twitter收集了相关数据集，重点关注政治相关的谣言。我们的方法包括使用微调的BERT模型进行语义分析以矢量化文本内容，并构建一个有向图，其中推文和评论作为节点，互动作为边。我们的方法的核心是一种图神经网络——SAGEWithEdgeAttention，它通过引入一阶差作为边的属性，并应用注意力机制来增强特征聚合，从而扩展了GraphSAGE模型。这一创新方法能够精细分析复杂的社交网络结构，提高谣言检测的准确性。研究结论表明，我们的方法显著优于传统的内容分析和基于时间的模型，提供了一个理论依据充分且实际高效的解决方案。', 'title_zh': '增强社交媒体谣言检测：面向2024全球选举的语义与图神经网络方法'}
{'arxiv_id': 'arXiv:2503.01386', 'title': 'Geo-Semantic-Parsing: AI-powered geoparsing by traversing semantic knowledge graphs', 'authors': 'Leonardo Nizzoli, Marco Avvenuti, Maurizio Tesconi, Stefano Cresci', 'link': 'https://arxiv.org/abs/2503.01386', 'abstract': 'Online social networks convey rich information about geospatial facets of reality. However in most cases, geographic information is not explicit and structured, thus preventing its exploitation in real-time applications. We address this limitation by introducing a novel geoparsing and geotagging technique called Geo-Semantic-Parsing (GSP). GSP identifies location references in free text and extracts the corresponding geographic coordinates. To reach this goal, we employ a semantic annotator to identify relevant portions of the input text and to link them to the corresponding entity in a knowledge graph. Then, we devise and experiment with several efficient strategies for traversing the knowledge graph, thus expanding the available set of information for the geoparsing task. Finally, we exploit all available information for learning a regression model that selects the best entity with which to geotag the input text. We evaluate GSP on a well-known reference dataset including almost 10k event-related tweets, achieving $F1=0.66$. We extensively compare our results with those of 2 baselines and 3 state-of-the-art geoparsing techniques, achieving the best performance. On the same dataset, competitors obtain $F1 \\leq 0.55$. We conclude by providing in-depth analyses of our results, showing that the overall superior performance of GSP is mainly due to a large improvement in recall, with respect to existing techniques.', 'abstract_zh': '基于语义解析的地理标注技术研究：Geo-Semantic-Parsing (GSP)及其应用', 'title_zh': '地理语义解析：通过遍历语义知识图谱的AI驱动地名解析'}
{'arxiv_id': 'arXiv:2503.01375', 'title': 'Combining Flow Matching and Transformers for Efficient Solution of Bayesian Inverse Problems', 'authors': 'Daniil Sherki, Ivan Oseledets, Ekaterina Muravleva', 'link': 'https://arxiv.org/abs/2503.01375', 'abstract': 'Solving Bayesian inverse problems efficiently remains a significant challenge due to the complexity of posterior distributions and the computational cost of traditional sampling methods. Given a series of observations and the forward model, we want to recover the distribution of the parameters, conditioned on observed experimental data. We show, that combining Conditional Flow Mathching (CFM) with transformer-based architecture, we can efficiently sample from such kind of distribution, conditioned on variable number of observations.', 'abstract_zh': '高效解决贝叶斯逆问题仍然是一个重大挑战，这归因于后验分布的复杂性和传统采样方法的计算成本。给定一系列观测值和前向模型，我们希望恢复参数的分布，条件是观察到的实验数据。我们展示了将条件流匹配（CFM）与基于变压器的架构相结合，可以在条件观测值数量变化的情况下有效地从此类分布中进行采样。', 'title_zh': '结合流匹配与变换器高效求解贝叶斯逆问题'}
{'arxiv_id': 'arXiv:2503.01353', 'title': 'Dendron: Enhancing Human Activity Recognition with On-Device TinyML Learning', 'authors': 'Hazem Hesham Yousef Shalby, Manuel Roveri', 'link': 'https://arxiv.org/abs/2503.01353', 'abstract': 'Human activity recognition (HAR) is a research field that employs Machine Learning (ML) techniques to identify user activities. Recent studies have prioritized the development of HAR solutions directly executed on wearable devices, enabling the on-device activity recognition. This approach is supported by the Tiny Machine Learning (TinyML) paradigm, which integrates ML within embedded devices with limited resources. However, existing approaches in the field lack in the capability for on-device learning of new HAR tasks, particularly when supervised data are scarce. To address this limitation, our paper introduces Dendron, a novel TinyML methodology designed to facilitate the on-device learning of new tasks for HAR, even in conditions of limited supervised data. Experimental results on two public-available datasets and an off-the-shelf device (STM32-NUCLEO-F401RE) show the effectiveness and efficiency of the proposed solution.', 'abstract_zh': '人类活动识别（HAR）是一个采用机器学习（ML）技术来识别用户活动的研究领域。近期的研究优先考虑开发直接在可穿戴设备上执行的HAR解决方案，以实现设备端的活动识别。这一方法得到了微型机器学习（TinyML）范式的支持，该范式将机器学习集成到资源有限的嵌入式设备中。然而，该领域的现有方法在稀少的监督数据条件下，缺乏对新HAR任务的设备端学习能力。为解决这一不足，本文提出了一种新颖的TinyML方法Dendron，旨在即使在监督数据有限的条件下也能支持新HAR任务的设备端学习。在两个公开的数据集和一个商用现成设备（STM32-NUCLEO-F401RE）上的实验结果表明，所提出的方法的有效性和效率。', 'title_zh': 'Dendron: 增强设备端TinyML学习的人类活动识别'}
{'arxiv_id': 'arXiv:2503.01328', 'title': 'PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization', 'authors': 'Xinyi Wan, Penghui Qi, Guangxing Huang, Jialin Li, Min Lin', 'link': 'https://arxiv.org/abs/2503.01328', 'abstract': 'Pipeline parallelism (PP) is widely used for training large language models (LLMs), yet its scalability is often constrained by high activation memory consumption as the number of in-flight microbatches grows with the degree of PP. In this paper, we focus on addressing this challenge by leveraging the under-explored memory offload strategy in PP. With empirical study, we discover that in the majority of standard configurations, at least half, and potentially all, of the activations can be offloaded with negligible overhead. In the cases where full overload is not possible, we introduce a novel selective offload strategy that decreases peak activation memory in a better-than-linear manner. Furthermore, we integrate memory offload with other techniques to jointly consider overall throughput and memory limitation. Our experiments proves that the per-device activation memory effectively reduces with the total number of stages, making PP a stronger alternative than TP, offering up to a 19\\% acceleration with even lower memory consumption. The implementation is open-sourced at \\href{this https URL}{this url}.', 'abstract_zh': 'Pipeline并行性中的内存卸载策略在大规模语言模型训练中的应用：超越传输并行性的新方法', 'title_zh': 'PipeOffload: 通过内存优化提升管道并行性的可扩展性'}
{'arxiv_id': 'arXiv:2503.01323', 'title': 'CacheQuant: Comprehensively Accelerated Diffusion Models', 'authors': 'Xuewen Liu, Zhikai Li, Qingyi Gu', 'link': 'https://arxiv.org/abs/2503.01323', 'abstract': 'Diffusion models have gradually gained prominence in the field of image synthesis, showcasing remarkable generative capabilities. Nevertheless, the slow inference and complex networks, resulting from redundancy at both temporal and structural levels, hinder their low-latency applications in real-world scenarios. Current acceleration methods for diffusion models focus separately on temporal and structural levels. However, independent optimization at each level to further push the acceleration limits results in significant performance degradation. On the other hand, integrating optimizations at both levels can compound the acceleration effects. Unfortunately, we find that the optimizations at these two levels are not entirely orthogonal. Performing separate optimizations and then simply integrating them results in unsatisfactory performance. To tackle this issue, we propose CacheQuant, a novel training-free paradigm that comprehensively accelerates diffusion models by jointly optimizing model caching and quantization techniques. Specifically, we employ a dynamic programming approach to determine the optimal cache schedule, in which the properties of caching and quantization are carefully considered to minimize errors. Additionally, we propose decoupled error correction to further mitigate the coupled and accumulated errors step by step. Experimental results show that CacheQuant achieves a 5.18 speedup and 4 compression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP score. Our code are open-sourced: this https URL .', 'abstract_zh': 'CacheQuant：一种综合加速扩散模型的新颖无训练范式', 'title_zh': 'CacheQuant: 全面加速扩散模型'}
{'arxiv_id': 'arXiv:2503.01290', 'title': 'ACTIVA: Amortized Causal Effect Estimation without Graphs via Transformer-based Variational Autoencoder', 'authors': 'Andreas Sauter, Saber Salehkaleybar, Aske Plaat, Erman Acar', 'link': 'https://arxiv.org/abs/2503.01290', 'abstract': 'Predicting the distribution of outcomes under hypothetical interventions is crucial in domains like healthcare, economics, and policy-making. Current methods often rely on strong assumptions, such as known causal graphs or parametric models, and lack amortization across problem instances, limiting their practicality. We propose a novel transformer-based conditional variational autoencoder architecture, named ACTIVA, that extends causal transformer encoders to predict causal effects as mixtures of Gaussians. Our method requires no causal graph and predicts interventional distributions given only observational data and a queried intervention. By amortizing over many simulated instances, it enables zero-shot generalization to novel datasets without retraining. Experiments demonstrate accurate predictions for synthetic and semi-synthetic data, showcasing the effectiveness of our graph-free, amortized causal inference approach.', 'abstract_zh': '基于假设干预预测结果分布对于医疗、经济和政策制定等领域至关重要。当前方法往往依赖于强假设，如已知因果图或参数模型，并且缺乏跨问题实例的 amortization，限制了其实用性。我们提出了一种新的基于变换器的条件变异自动编码器架构，名为ACTIVA，该架构将因果变换器编码器扩展为预测高斯混合的因果效果。该方法不需要因果图，并且仅通过观测数据和查询的干预就能预测干预分布。通过模拟多个实例的 amortization，它能够在无需重新训练的情况下对新数据集进行零样本泛化。实验显示，该方法对于合成和半合成数据实现了准确的预测，展示了我们的无图自 amortization 因果推理方法的有效性。', 'title_zh': 'ACTIVA：基于Transformer变分自编码器的无图因果效应拟合'}
{'arxiv_id': 'arXiv:2503.01287', 'title': 'Robust Simulation-Based Inference under Missing Data via Neural Processes', 'authors': 'Yogesh Verma, Ayush Bharti, Vikas Garg', 'link': 'https://arxiv.org/abs/2503.01287', 'abstract': 'Simulation-based inference (SBI) methods typically require fully observed data to infer parameters of models with intractable likelihood functions. However, datasets often contain missing values due to incomplete observations, data corruptions (common in astrophysics), or instrument limitations (e.g., in high-energy physics applications). In such scenarios, missing data must be imputed before applying any SBI method. We formalize the problem of missing data in SBI and demonstrate that naive imputation methods can introduce bias in the estimation of SBI posterior. We also introduce a novel amortized method that addresses this issue by jointly learning the imputation model and the inference network within a neural posterior estimation (NPE) framework. Extensive empirical results on SBI benchmarks show that our approach provides robust inference outcomes compared to standard baselines for varying levels of missing data. Moreover, we demonstrate the merits of our imputation model on two real-world bioactivity datasets (Adrenergic and Kinase assays). Code is available at this https URL.', 'abstract_zh': '基于仿真推理的缺失数据处理：一种联合学习填补模型和推理网络的方法', 'title_zh': '基于神经过程的在缺失数据情况下稳健的仿真推理'}
{'arxiv_id': 'arXiv:2503.01268', 'title': 'Multi-Level Collaboration in Model Merging', 'authors': 'Qi Li, Runpeng Yu, Xinchao Wang', 'link': 'https://arxiv.org/abs/2503.01268', 'abstract': 'Parameter-level model merging is an emerging paradigm in multi-task learning with significant promise. Previous research has explored its connections with prediction-level model ensembling-commonly viewed as the upper bound for merging-to reveal the potential of achieving performance consistency between the two. However, this observation relies on certain preconditions, such as being limited to two models, using ViT-based models, and all models are fine-tuned from the same pre-trained checkpoint. To further understand the intrinsic connections between model merging and model ensembling, this paper explores an interesting possibility: If these restrictions are removed, can performance consistency still be achieved between merging and ensembling? To answer this question, we first theoretically establish a performance correlation between merging and ensembling. We find that even when previous restrictions are not met, there is still a way for model merging to attain a near-identical and superior performance similar to that of ensembling. To verify whether our findings are practical, we introduce a validation framework termed Neural Ligand (NeuLig). The learning process of NeuLig is meticulously designed with a specialized loss function supported by theoretical foundations. Experimental results demonstrate the robust resilience of NeuLig in terms of both model scale and the number of collaborating models. For instance, for the case involving 5 CLIP-ViT-B/32 models, parameter-level merging achieves the same performance as prediction-level ensembling (merging: 95.44% vs. ensembling: 95.46%).', 'abstract_zh': '参数级模型融合是多任务学习中一个 emerging 的范式，具有显著潜力。先前的研究探索了模型融合与预测级模型集成之间的联系——通常被视为融合的上限，以揭示两者之间性能一致性的潜力。然而，这一观察依赖于某些前提条件，如仅限于两个模型、使用基于ViT的模型，以及所有模型均从相同的预训练检查点进行微调。为了进一步理解模型融合与模型集成之间的内在联系，本文探索了一个有趣的可能性：如果移除这些限制，模型融合和模型集成之间是否仍能实现性能一致性？为了回答这一问题，我们首先从理论上建立了模型融合和模型集成之间的性能关系。我们发现即使不满足先前的限制条件，模型融合仍能找到一种方式，实现与集成相媲美的近乎一致的优异性能。为了验证我们发现的实用性，我们引入了一种名为Neural Ligand（NeuLig）的验证框架。NeuLig的学习过程在理论上得到了支撑，并设计了专门的损失函数。实验结果展示了NeuLig在模型规模和协作模型数量方面的鲁棒鲁棒性。例如，在涉及5个CLIP-ViT-B/32模型的情况下，参数级融合达到与预测级集成相同的性能（融合：95.44% vs. 集成：95.46%）。', 'title_zh': '多级协作的模型合并'}
{'arxiv_id': 'arXiv:2503.01266', 'title': 'Voice Cloning for Dysarthric Speech Synthesis: Addressing Data Scarcity in Speech-Language Pathology', 'authors': 'Birger Moell, Fredrik Sand Aronsson', 'link': 'https://arxiv.org/abs/2503.01266', 'abstract': 'This study explores voice cloning to generate synthetic speech replicating the unique patterns of individuals with dysarthria. Using the TORGO dataset, we address data scarcity and privacy challenges in speech-language pathology. Our contributions include demonstrating that voice cloning preserves dysarthric speech characteristics, analyzing differences between real and synthetic data, and discussing implications for diagnostics, rehabilitation, and communication. We cloned voices from dysarthric and control speakers using a commercial platform, ensuring gender-matched synthetic voices. A licensed speech-language pathologist (SLP) evaluated a subset for dysarthria, speaker gender, and synthetic indicators. The SLP correctly identified dysarthria in all cases and speaker gender in 95% but misclassified 30% of synthetic samples as real, indicating high realism. Our results suggest synthetic speech effectively captures disordered characteristics and that voice cloning has advanced to produce high-quality data resembling real speech, even to trained professionals. This has critical implications for healthcare, where synthetic data can mitigate data scarcity, protect privacy, and enhance AI-driven diagnostics. By enabling the creation of diverse, high-quality speech datasets, voice cloning can improve generalizable models, personalize therapy, and advance assistive technologies for dysarthria.\nWe publicly release our synthetic dataset to foster further research and collaboration, aiming to develop robust models that improve patient outcomes in speech-language pathology.', 'abstract_zh': '本研究探索语音克隆技术生成模拟失语症个体独特语音模式的合成语音。利用TORGO数据集，我们解决了言语-语言病理学中的数据稀缺性和隐私挑战。我们的贡献包括证明语音克隆能够保留失语症语音特征、分析真实数据与合成数据之间的差异，并讨论其在诊断、康复和沟通中的影响。我们使用商业平台克隆了失语症和对照说话者的语音，确保合成语音匹配性别。经过认证的言语-语言病理学家（SLP）评估了一部分样本的失语症、说话者性别和合成指标。SLP 在所有失语症鉴定中均正确，在性别鉴定中正确率为95%，但在30%的合成样本中将其误分类为真实样本，表明合成语音具有高度的真实性。研究结果表明，合成语音能够有效捕捉失常特征，并且语音克隆技术已经发展到能够生成与真实语音高度相似的高质量数据，甚至对专业人员也是如此。这对于医疗保健具有重要意义，合成数据能够缓解数据稀缺问题、保护隐私并增强基于AI的诊断能力。通过生成多样化的高质量语音数据集，语音克隆技术可以提高通用模型的性能、个性化治疗方案，并促进失语症辅助技术的进步。我们将公开发布我们的合成数据集，以促进进一步的研究和合作，旨在开发出稳健的模型以改善言语-语言病理学中的患者治疗效果。', 'title_zh': '语音克隆在构音障碍语音合成中的应用：解决言语病理学中的数据稀缺性问题'}
{'arxiv_id': 'arXiv:2503.01232', 'title': 'Learning Covariance-Based Multi-Scale Representation of Neuroimaging Measures for Alzheimer Classification', 'authors': 'Seunghun Baek, Injun Choi, Mustafa Dere, Minjeong Kim, Guorong Wu, Won Hwa Kim', 'link': 'https://arxiv.org/abs/2503.01232', 'abstract': "Stacking excessive layers in DNN results in highly underdetermined system when training samples are limited, which is very common in medical applications. In this regard, we present a framework capable of deriving an efficient high-dimensional space with reasonable increase in model size. This is done by utilizing a transform (i.e., convolution) that leverages scale-space theory with covariance structure. The overall model trains on this transform together with a downstream classifier (i.e., Fully Connected layer) to capture the optimal multi-scale representation of the original data which corresponds to task-specific components in a dual space. Experiments on neuroimaging measures from Alzheimer's Disease Neuroimaging Initiative (ADNI) study show that our model performs better and converges faster than conventional models even when the model size is significantly reduced. The trained model is made interpretable using gradient information over the multi-scale transform to delineate personalized AD-specific regions in the brain.", 'abstract_zh': '在训练样本受限时，DNN中堆叠过多层会导致高度欠定系统，这在医疗应用中非常常见。为此，我们提出了一种框架，能够在合理增加模型规模的同时提取出高效高维空间。通过利用基于尺度空间理论和协方差结构的变换（即卷积），整体模型在该变换及下游分类器（即全连接层）的共同作用下训练，以捕获原始数据的最优多尺度表示，这对应于双空间中的任务特定组件。实验结果表明，即使模型规模显著减小，我们的模型在阿尔茨海默病神经影像学测量数据上的性能也优于传统模型，并且收敛速度更快。通过多尺度变换的梯度信息，训练好的模型可以被解释以界定个性化的AD特定脑区。', 'title_zh': '基于协方差的多尺度神经影像表示学习及其在阿尔茨海默病分类中的应用'}
{'arxiv_id': 'arXiv:2503.01220', 'title': 'Tera-MIND: Tera-scale mouse brain simulation via spatial mRNA-guided diffusion', 'authors': 'Jiqing Wu, Ingrid Berg, Yawei Li, Ender Konukoglu, Viktor H. Koelzer', 'link': 'https://arxiv.org/abs/2503.01220', 'abstract': 'Holistic 3D modeling of molecularly defined brain structures is crucial for understanding complex brain functions. Emerging tissue profiling technologies enable the construction of a comprehensive atlas of the mammalian brain with sub-cellular resolution and spatially resolved gene expression data. However, such tera-scale volumetric datasets present significant computational challenges in understanding complex brain functions within their native 3D spatial context. Here, we propose the novel generative approach $\\textbf{Tera-MIND}$, which can simulate $\\textbf{Tera}$-scale $\\textbf{M}$ouse bra$\\textbf{IN}s$ in 3D using a patch-based and boundary-aware $\\textbf{D}$iffusion model. Taking spatial transcriptomic data as the conditional input, we generate virtual mouse brains with comprehensive cellular morphological detail at teravoxel scale. Through the lens of 3D $gene$-$gene$ self-attention, we identify spatial molecular interactions for key transcriptomic pathways in the murine brain, exemplified by glutamatergic and dopaminergic neuronal systems. Importantly, these $in$-$silico$ biological findings are consistent and reproducible across three tera-scale virtual mouse brains. Therefore, Tera-MIND showcases a promising path toward efficient and generative simulations of whole organ systems for biomedical research. Project website: $\\href{this http URL}{https}$', 'abstract_zh': '巨规模三维建模对于理解复杂脑功能至关重要。新兴的组织 profiling 技术使我们能够以亚细胞分辨率和空间分辨的基因表达数据构建哺乳动物脑的综合图谱。然而，如此大的体积数据集在其原生的三维空间上下文中理解复杂脑功能带来了显著的计算挑战。在这里，我们提出了一种新颖的生成方法 $\\textbf{Tera-MIND}$，该方法使用基于 patch 的和边界意识的 $\\textbf{D}$ 扩散模型在三维中模拟 $\\textbf{Tera}$-规模 $\\textbf{M}$ 鼠脑。通过空间转录组学数据作为条件输入，我们生成了具有全面细胞形态细节的巨尺度虚拟鼠脑。借助三维 $gene$-$gene$ 自注意力机制，我们识别了小鼠脑中关键转录组路径的空间分子互作，例如谷氨酸能和多巴胺能神经元系统。重要的是，这些 $in$-$silico$ 生物发现可以在三个巨尺度虚拟鼠脑中一致且可重复地观察到。因此，$\\textbf{Tera-MIND}$ 展示了为生物医药研究高效且生成性模拟整个器官系统的前景。项目网址：$\\href{this http URL}{https}$。', 'title_zh': 'Tera-MIND: 经空间mRNA引导扩散的tera规模小鼠大脑模拟'}
{'arxiv_id': 'arXiv:2503.01152', 'title': 'STGAN: Spatial-temporal Graph Autoregression Network for Pavement Distress Deterioration Prediction', 'authors': 'Shilin Tong, Difei Wu, Xiaona Liu, Le Zheng, Yuchuan Du, Difan Zou', 'link': 'https://arxiv.org/abs/2503.01152', 'abstract': 'Pavement distress significantly compromises road integrity and poses risks to drivers. Accurate prediction of pavement distress deterioration is essential for effective road management, cost reduction in maintenance, and improvement of traffic safety. However, real-world data on pavement distress is usually collected irregularly, resulting in uneven, asynchronous, and sparse spatial-temporal datasets. This hinders the application of existing spatial-temporal models, such as DCRNN, since they are only applicable to regularly and synchronously collected data. To overcome these challenges, we propose the Spatial-Temporal Graph Autoregression Network (STGAN), a novel graph neural network model designed for accurately predicting irregular pavement distress deterioration using complex spatial-temporal data. Specifically, STGAN integrates the temporal domain into the spatial domain, creating a larger graph where nodes are represented by spatial-temporal tuples and edges are formed based on a similarity-based connection mechanism. Furthermore, based on the constructed spatiotemporal graph, we formulate pavement distress deterioration prediction as a graph autoregression task, i.e., the graph size increases incrementally and the prediction is performed sequentially. This is accomplished by a novel spatial-temporal attention mechanism deployed by STGAN. Utilizing the ConTrack dataset, which contains pavement distress records collected from different locations in Shanghai, we demonstrate the superior performance of STGAN in capturing spatial-temporal correlations and addressing the aforementioned challenges. Experimental results further show that STGAN outperforms baseline models, and ablation studies confirm the effectiveness of its novel modules. Our findings contribute to promoting proactive road maintenance decision-making and ultimately enhancing road safety and resilience.', 'abstract_zh': '基于时空图自回归网络的不规则 pavement 状况退化预测', 'title_zh': 'STGAN: 空间-时间图自回归网络在路面病害衰退预测中的应用'}
{'arxiv_id': 'arXiv:2503.01148', 'title': 'Dynamic spillovers and investment strategies across artificial intelligence ETFs, artificial intelligence tokens, and green markets', 'authors': 'Ying-Hui Shao, Yan-Hong Yang, Wei-Xing Zhou', 'link': 'https://arxiv.org/abs/2503.01148', 'abstract': 'This paper investigates the risk spillovers among AI ETFs, AI tokens, and green markets using the R2 decomposition method. We reveal several key insights. First, the overall transmission connectedness index (TCI) closely aligns with the contemporaneous TCI, while the lagged TCI is significantly lower. Second, AI ETFs and clean energy act as risk transmitters, whereas AI tokens and green bond function as risk receivers. Third, AI tokens are difficult to hedge and provide limited hedging ability compared to AI ETFs and green assets. However, multivariate portfolios effectively reduce AI tokens investment risk. Among them, the minimum correlation portfolio outperforms the minimum variance and minimum connectedness portfolios.', 'abstract_zh': '本文使用R2分解方法研究了AI ETFs、AI代币和绿色市场之间的风险溢出效应，并揭示了几项关键洞察。首先，总体传播连通性指数（TCI）与 contemporaneous TCI 高度一致，而滞后 TCI 显著较低。其次，AI ETFs 和清洁能源作为风险传递者，而 AI 代币和绿色债券则作为风险接收者。第三，与 AI ETFs 和绿色资产相比，AI 代币难以对冲，并提供有限的对冲能力。然而，多元 portfolios 有效地减少了 AI 代币投资风险，在这些 portfolios 中，最小相关性 portfolio 的表现优于最小方差和最小连通性 portfolio。', 'title_zh': '人工智能ETFs、人工智能代币和绿色市场间的动态溢出效应及投资策略'}
{'arxiv_id': 'arXiv:2503.01134', 'title': 'Statistical Tractability of Off-policy Evaluation of History-dependent Policies in POMDPs', 'authors': 'Yuheng Zhang, Nan Jiang', 'link': 'https://arxiv.org/abs/2503.01134', 'abstract': "We investigate off-policy evaluation (OPE), a central and fundamental problem in reinforcement learning (RL), in the challenging setting of Partially Observable Markov Decision Processes (POMDPs) with large observation spaces. Recent works of Uehara et al. (2023a); Zhang & Jiang (2024) developed a model-free framework and identified important coverage assumptions (called belief and outcome coverage) that enable accurate OPE of memoryless policies with polynomial sample complexities, but handling more general target policies that depend on the entire observable history remained an open problem. In this work, we prove information-theoretic hardness for model-free OPE of history-dependent policies in several settings, characterized by additional assumptions imposed on the behavior policy (memoryless vs. history-dependent) and/or the state-revealing property of the POMDP (single-step vs. multi-step revealing). We further show that some hardness can be circumvented by a natural model-based algorithm -- whose analysis has surprisingly eluded the literature despite the algorithm's simplicity -- demonstrating provable separation between model-free and model-based OPE in POMDPs.", 'abstract_zh': '我们在部分可观测马尔可夫决策过程（POMDPs）的大观测空间设定中研究不可策略评估（OPE）问题，这是一种强化学习（RL）中的核心问题。', 'title_zh': '基于POMDP的历史依赖策略离策评估的统计可处理性'}
{'arxiv_id': 'arXiv:2503.01098', 'title': 'SolBench: A Dataset and Benchmark for Evaluating Functional Correctness in Solidity Code Completion and Repair', 'authors': 'Zaoyu Chen, Haoran Qin, Nuo Chen, Xiangyu Zhao, Lei Xue, Xiapu Luo, Xiao-Ming Wu', 'link': 'https://arxiv.org/abs/2503.01098', 'abstract': 'Smart contracts are crucial programs on blockchains, and their immutability post-deployment makes functional correctness vital. Despite progress in code completion models, benchmarks for Solidity, the primary smart contract language, are lacking. Existing metrics like BLEU do not adequately assess the functional correctness of generated smart contracts. To fill this gap, we introduce SolBench, a benchmark for evaluating the functional correctness of Solidity smart contracts generated by code completion models. SolBench includes 4,178 functions from 1,155 Ethereum-deployed contracts. Testing advanced models revealed challenges in generating correct code without context, as Solidity functions rely on context-defined variables and interfaces. To address this, we propose a Retrieval-Augmented Code Repair framework. In this framework, an executor verifies functional correctness, and if necessary, an LLM repairs the code using retrieved snippets informed by executor traces. We conduct a comprehensive evaluation of both closed-source and open-source LLMs across various model sizes and series to assess their performance in smart contract completion. The results show that code repair and retrieval techniques effectively enhance the correctness of smart contract completion while reducing computational costs.', 'abstract_zh': '基于代码补全模型的Solidity智能合约功能正确性评估基准SolBench', 'title_zh': 'SolBench: 一个用于评估Solidity代码补全和修复功能性正确性的数据集和基准测试'}
{'arxiv_id': 'arXiv:2503.01079', 'title': "Depth-Adaptive Graph Neural Networks via Learnable Bakry-'Emery Curvature", 'authors': 'Asela Hevapathige, Ahad N. Zehmakan, Qing Wang', 'link': 'https://arxiv.org/abs/2503.01079', 'abstract': 'Graph Neural Networks (GNNs) have demonstrated strong representation learning capabilities for graph-based tasks. Recent advances on GNNs leverage geometric properties, such as curvature, to enhance its representation capabilities by modeling complex connectivity patterns and information flow within graphs. However, most existing approaches focus solely on discrete graph topology, overlooking diffusion dynamics and task-specific dependencies essential for effective learning. To address this, we propose integrating Bakry-Émery curvature, which captures both structural and task-driven aspects of information propagation. We develop an efficient, learnable approximation strategy, making curvature computation scalable for large graphs. Furthermore, we introduce an adaptive depth mechanism that dynamically adjusts message-passing layers per vertex based on its curvature, ensuring efficient propagation. Our theoretical analysis establishes a link between curvature and feature distinctiveness, showing that high-curvature vertices require fewer layers, while low-curvature ones benefit from deeper propagation. Extensive experiments on benchmark datasets validate the effectiveness of our approach, showing consistent performance improvements across diverse graph learning tasks.', 'abstract_zh': '基于贝克里-エ米里曲率的图神经网络', 'title_zh': '可学习的贝克ry-埃米里曲率的深度自适应图神经网络'}
{'arxiv_id': 'arXiv:2503.01046', 'title': 'MAPS: Multi-Fidelity AI-Augmented Photonic Simulation and Inverse Design Infrastructure', 'authors': 'Pingchuan Ma, Zhengqi Gao, Meng Zhang, Haoyu Yang, Mark Ren, Rena Huang, Duane S. Boning, Jiaqi Gu', 'link': 'https://arxiv.org/abs/2503.01046', 'abstract': 'Inverse design has emerged as a transformative approach for photonic device optimization, enabling the exploration of high-dimensional, non-intuitive design spaces to create ultra-compact devices and advance photonic integrated circuits (PICs) in computing and interconnects. However, practical challenges, such as suboptimal device performance, limited manufacturability, high sensitivity to variations, computational inefficiency, and lack of interpretability, have hindered its adoption in commercial hardware. Recent advancements in AI-assisted photonic simulation and design offer transformative potential, accelerating simulations and design generation by orders of magnitude over traditional numerical methods. Despite these breakthroughs, the lack of an open-source, standardized infrastructure and evaluation benchmark limits accessibility and cross-disciplinary collaboration. To address this, we introduce MAPS, a multi-fidelity AI-augmented photonic simulation and inverse design infrastructure designed to bridge this gap. MAPS features three synergistic components: (1) MAPS-Data: A dataset acquisition framework for generating multi-fidelity, richly labeled devices, providing high-quality data for AI-for-optics research. (2) MAPS-Train: A flexible AI-for-photonics training framework offering a hierarchical data loading pipeline, customizable model construction, support for data- and physics-driven losses, and comprehensive evaluations. (3) MAPS-InvDes: An advanced adjoint inverse design toolkit that abstracts complex physics but exposes flexible optimization steps, integrates pre-trained AI models, and incorporates fabrication variation models. This infrastructure MAPS provides a unified, open-source platform for developing, benchmarking, and advancing AI-assisted photonic design workflows, accelerating innovation in photonic hardware optimization and scientific machine learning.', 'abstract_zh': '逆设计已成为光电设备优化的一种变革性方法，使研究人员能够探索高维、非直观的设计空间，创建超紧凑的器件，并推进计算和互连领域的光子集成电路（PICs）。然而，实际应用中如设备性能欠佳、制造难度大、对变化敏感、计算效率低以及缺乏可解释性等因素限制了其在商用硬件中的应用。近年来，借助AI辅助的光子仿真和设计方法展现出变革性的潜力，极大地加速了仿真和设计生成的速度。尽管取得了这些突破，但缺乏开放源代码、标准化的基础设施和评估基准限制了其可访问性和跨学科合作。为解决这一问题，我们引入了MAPS，一种多精度AI增强光子仿真与逆设计基础设施，旨在填补这一空白。MAPS 包含三个协同的组件：(1) MAPS-Data：一种数据集获取框架，用于生成多精度、丰富标签的器件，提供高质量数据以支持光子学中的AI研究。(2) MAPS-Train：一种灵活的AI辅助光子学训练框架，提供分层数据加载管道、可定制的模型构建、数据和物理驱动的损失支持以及全面的评估。(3) MAPS-InvDes：一种先进的伴随逆设计工具包，通过抽象复杂的物理现象同时暴露灵活的优化步骤，集成预训练的AI模型并整合制造变异性模型。该基础设施MAPS提供了一个统一的开放源代码平台，用于开发、基准测试和推进AI辅助光子设计流程，加速光子硬件优化和科学机器学习的创新。', 'title_zh': 'MAPS：多保真度人工智能增强光子模拟与逆设计基础设施'}
{'arxiv_id': 'arXiv:2503.01003', 'title': 'A Semantic Search Pipeline for Causality-driven Adhoc Information Retrieval', 'authors': 'Dhairya Dalal, Sharmi Dev Gupta, Bentolhoda Binaei', 'link': 'https://arxiv.org/abs/2503.01003', 'abstract': 'We present a unsupervised semantic search pipeline for the Causality-driven Adhoc Information Retrieval (CAIR-2021) shared task. The CAIR shared task expands traditional information retrieval to support the retrieval of documents containing the likely causes of a query event. A successful system must be able to distinguish between topical documents and documents containing causal descriptions of events that are causally related to the query event. Our approach involves aggregating results from multiple query strategies over a semantic and lexical index. The proposed approach leads the CAIR-2021 leaderboard and outperformed both traditional IR and pure semantic embedding-based approaches.', 'abstract_zh': '我们提出了一个无监督语义搜索管道用于因果驱动的即兴信息检索（CAIR-2021）共享任务。CAIR共享任务扩展了传统信息检索，以支持检索包含查询事件可能原因的文档。一个成功的系统必须能够区分主题相关文档和包含与查询事件因果相关的事件描述的文档。我们的方法涉及在语义和词汇索引上聚合多种查询策略的结果。所提出的方法在CAIR-2021排行榜上名列前茅，并且优于传统的IR方法和纯粹基于语义嵌入的方法。', 'title_zh': '因果驱动的即席信息检索的语义搜索管道'}
{'arxiv_id': 'arXiv:2503.00962', 'title': 'Using Synthetic Images to Augment Small Medical Image Datasets', 'authors': 'Minh H. Vu, Lorenzo Tronchin, Tufve Nyholm, Tommy Löfstedt', 'link': 'https://arxiv.org/abs/2503.00962', 'abstract': 'Recent years have witnessed a growing academic and industrial interest in deep learning (DL) for medical imaging. To perform well, DL models require very large labeled datasets. However, most medical imaging datasets are small, with a limited number of annotated samples. The reason they are small is usually because delineating medical images is time-consuming and demanding for oncologists. There are various techniques that can be used to augment a dataset, for example, to apply affine transformations or elastic transformations to available images, or to add synthetic images generated by a Generative Adversarial Network (GAN). In this work, we have developed a novel conditional variant of a current GAN method, the StyleGAN2, to generate multi-modal high-resolution medical images with the purpose to augment small medical imaging datasets with these synthetic images. We use the synthetic and real images from six datasets to train models for the downstream task of semantic segmentation. The quality of the generated medical images and the effect of this augmentation on the segmentation performance were evaluated afterward. Finally, the results indicate that the downstream segmentation models did not benefit from the generated images. Further work and analyses are required to establish how this augmentation affects the segmentation performance.', 'abstract_zh': '近年来，深度学习（DL）在医学影像领域的学术和工业兴趣不断增加。为了表现良好，DL模型需要非常大的标注数据集。然而，大多数医学影像数据集较小，标注样本数量有限。这些数据集较小的原因通常是，医学图像的勾画对肿瘤学家来说既耗时又具有挑战性。可以使用各种技术来扩充数据集，例如对现有图像应用仿射变换或弹性变换，或者通过生成对抗网络（GAN）生成合成图像。在本工作中，我们开发了一种新型的条件变体StyleGAN2方法，以生成多模态高分辨率医学图像，目的是使用这些合成图像扩充小型医学影像数据集。我们使用来自六个数据集的合成图像和真实图像来训练用于下游语义分割任务的模型。之后评估了生成的医学图像质量以及此扩充对分割性能的影响。最终结果表明，下游分割模型并未从生成的图像中受益。进一步的工作和分析是必要的，以确定此扩充如何影响分割性能。', 'title_zh': '使用合成图像扩充小型医疗图像数据集'}
{'arxiv_id': 'arXiv:2503.00957', 'title': 'Exploiting Vulnerabilities in Speech Translation Systems through Targeted Adversarial Attacks', 'authors': 'Chang Liu, Haolin Wu, Xi Yang, Kui Zhang, Cong Wu, Weiming Zhang, Nenghai Yu, Tianwei Zhang, Qing Guo, Jie Zhang', 'link': 'https://arxiv.org/abs/2503.00957', 'abstract': 'As speech translation (ST) systems become increasingly prevalent, understanding their vulnerabilities is crucial for ensuring robust and reliable communication. However, limited work has explored this issue in depth. This paper explores methods of compromising these systems through imperceptible audio manipulations. Specifically, we present two innovative approaches: (1) the injection of perturbation into source audio, and (2) the generation of adversarial music designed to guide targeted translation, while also conducting more practical over-the-air attacks in the physical world. Our experiments reveal that carefully crafted audio perturbations can mislead translation models to produce targeted, harmful outputs, while adversarial music achieve this goal more covertly, exploiting the natural imperceptibility of music. These attacks prove effective across multiple languages and translation models, highlighting a systemic vulnerability in current ST architectures. The implications of this research extend beyond immediate security concerns, shedding light on the interpretability and robustness of neural speech processing systems. Our findings underscore the need for advanced defense mechanisms and more resilient architectures in the realm of audio systems. More details and samples can be found at this https URL.', 'abstract_zh': '随着语音翻译（ST）系统的日益普及，理解其脆弱性对于确保通信的稳健性和可靠性至关重要。然而，有限的工作深入探讨了这一问题。本文探讨了通过不可察觉的音频操纵破坏这些系统的办法。具体来说，我们提出了两种创新的方法：（1）源音频中注入扰动，（2）生成旨在引导目标翻译的对抗性音乐，并且还在实际物理世界中进行了更加实际的空中攻击。实验结果表明，精心构造的音频扰动可以误导翻译模型生成针对性的有害输出，而对抗性音乐则更加隐蔽地实现了这一目标，利用了音乐的自然不可察觉性。这些攻击在多种语言和翻译模型中证明有效，突显了当前ST架构中的系统性漏洞。这一研究的含义超越了即时的安全问题，揭示了神经语音处理系统可解释性和稳健性的必要性。我们的发现强调了在音频系统领域需要更先进的防御机制和更稳健的架构。更多信息和样本请访问此网址。', 'title_zh': '通过针对性 adversarial 攻击利用语音翻译系统的漏洞'}
{'arxiv_id': 'arXiv:2503.00940', 'title': 'Can AI Model the Complexities of Human Moral Decision-Making? A Qualitative Study of Kidney Allocation Decisions', 'authors': 'Vijay Keswani, Vincent Conitzer, Walter Sinnott-Armstrong, Breanna K. Nguyen, Hoda Heidari, Jana Schaich Borg', 'link': 'https://arxiv.org/abs/2503.00940', 'abstract': "A growing body of work in Ethical AI attempts to capture human moral judgments through simple computational models. The key question we address in this work is whether such simple AI models capture {the critical} nuances of moral decision-making by focusing on the use case of kidney allocation. We conducted twenty interviews where participants explained their rationale for their judgments about who should receive a kidney. We observe participants: (a) value patients' morally-relevant attributes to different degrees; (b) use diverse decision-making processes, citing heuristics to reduce decision complexity; (c) can change their opinions; (d) sometimes lack confidence in their decisions (e.g., due to incomplete information); and (e) express enthusiasm and concern regarding AI assisting humans in kidney allocation decisions. Based on these findings, we discuss challenges of computationally modeling moral judgments {as a stand-in for human input}, highlight drawbacks of current approaches, and suggest future directions to address these issues.", 'abstract_zh': '伦理人工智能中的一项 growing body of work 试图通过简单的计算模型捕捉人类的道德判断。我们在这项工作中关注的关键问题是，此类简单的AI模型是否能够通过以肾源分配为例，捕捉到道德决策中的关键细微差别。我们在20次访谈中，让参与者解释他们关于谁应接受肾源的判断依据。我们观察到参与者：（a）以不同程度的价值患者的相关道德属性；（b）采用多种决策过程，引用启发式方法以减少决策复杂性；（c）可以改变他们的观点；（d）有时对他们的决策缺乏信心（如因信息不完整）；以及（e）对AI在肾源分配决策中辅助人类运用表现出热情和关注。基于这些发现，我们讨论了将道德判断计算建模作为替代人类输入的挑战，指出现有方法的局限性，并建议未来的研究方向以解决这些问题。', 'title_zh': 'AI能否模拟人类道德决策的复杂性？一项关于肾脏分配决策的定性研究'}
{'arxiv_id': 'arXiv:2503.00932', 'title': 'Improving the Transferability of Adversarial Attacks by an Input Transpose', 'authors': 'Qing Wan, Shilong Deng, Xun Wang', 'link': 'https://arxiv.org/abs/2503.00932', 'abstract': "Deep neural networks (DNNs) are highly susceptible to adversarial examples--subtle perturbations applied to inputs that are often imperceptible to humans yet lead to incorrect model predictions. In black-box scenarios, however, existing adversarial examples exhibit limited transferability and struggle to effectively compromise multiple unseen DNN models. Previous strategies enhance the cross-model generalization of adversarial examples by introducing versatility into adversarial perturbations, thereby improving transferability. However, further refining perturbation versatility often demands intricate algorithm development and substantial computation consumption. In this work, we propose an input transpose method that requires almost no additional labor and computation costs but can significantly improve the transferability of existing adversarial strategies. Even without adding adversarial perturbations, our method demonstrates considerable effectiveness in cross-model attacks. Our exploration finds that on specific datasets, a mere $1^\\circ$ left or right rotation might be sufficient for most adversarial examples to deceive unseen models. Our further analysis suggests that this transferability improvement triggered by rotating only $1^\\circ$ may stem from visible pattern shifts in the DNN's low-level feature maps. Moreover, this transferability exhibits optimal angles that, when identified under unrestricted query conditions, could potentially yield even greater performance.", 'abstract_zh': '深度神经网络（DNNs）对 adversarial 例子高度敏感——这种细微的输入扰动往往对人类不可感知，但会导致模型预测错误。然而，在黑盒场景中，现有的 adversarial 例子迁移性有限，难以有效攻破多个未见过的 DNN 模型。以往的方法通过增加 adversarial 扰动的多样性来增强跨模型通用性，从而提高迁移性。然而，进一步细化扰动的多样性往往需要复杂的算法开发和大量的计算消耗。在本工作中，我们提出了一种输入转置方法，几乎不需要额外的劳动和计算成本，但可以显著提高现有 adversarial 策略的迁移性。即使不添加 adversarial 扰动，我们的方法在跨模型攻击中也显示出显著的效果。我们的研究发现，在特定数据集上，简单的 $1^\\circ$ 左右旋转可能足以使大多数 adversarial 例子欺骗未见过的模型。进一步的分析表明，这种由 $1^\\circ$ 旋转引起的迁移性提升可能源于 DNN 低层特征图中可观察到的模式变化。此外，这种迁移性在不受限制的查询条件下可能会表现出最优的角度，这可能进一步提高性能。', 'title_zh': '通过输入转置提高对抗攻击的转移性'}
{'arxiv_id': 'arXiv:2503.00900', 'title': 'S4M: S4 for multivariate time series forecasting with Missing values', 'authors': 'Jing Peng, Meiqi Yang, Qiong Zhang, Xiaoxiao Li', 'link': 'https://arxiv.org/abs/2503.00900', 'abstract': 'Multivariate time series data play a pivotal role in a wide range of real-world applications. However, the presence of block missing data introduces significant challenges, often compromising the performance of predictive models. Traditional two-step approaches, which first impute missing values and then perform forecasting, are prone to error accumulation, particularly in complex multivariate settings characterized by high missing ratios and intricate dependency structures. In this work, we introduce S4M, an end-to-end time series forecasting framework that seamlessly integrates missing data handling into the Structured State Space Sequence (S4) model architecture. Unlike conventional methods that treat imputation as a separate preprocessing step, S4M leverages the latent space of S4 models to directly recognize and represent missing data patterns, thereby more effectively capturing the underlying temporal and multivariate dependencies. Our framework comprises two key components: the Adaptive Temporal Prototype Mapper (ATPM) and the Missing-Aware Dual Stream S4 (MDS-S4). The ATPM employs a prototype bank to derive robust and informative representations from historical data patterns, while the MDS-S4 processes these representations alongside missingness masks as dual input streams to enable accurate forecasting. Through extensive empirical evaluations on diverse real-world datasets, we demonstrate that S4M consistently achieves state-of-the-art performance. These results underscore the efficacy of our integrated approach in handling missing data, showcasing its robustness and superiority over traditional imputation-based methods. Our findings highlight the potential of S4M to advance reliable time series forecasting in practical applications, offering a promising direction for future research and deployment. Code is available at this https URL.', 'abstract_zh': '多变量时间序列数据在广泛的实际应用中扮演着关键角色。然而，块缺失数据的存在引入了重大挑战，常常影响预测模型的表现。传统的两步方法首先填补缺失值，然后再进行预测，容易在复杂多变量设置中累积误差，特别是在高缺失比例和复杂的依赖结构中。在这项工作中，我们提出了一种名为S4M的端到端时间序列预测框架，该框架将缺失数据处理无缝集成到结构化状态空间序列（S4）模型架构中。与传统方法将填补视为单独的预处理步骤不同，S4M利用S4模型的潜在空间直接识别和表示缺失数据模式，更有效地捕获潜在的时序和多变量依赖关系。该框架包括两个关键组件：自适应时序原型映射器（ATPM）和缺省感知双流S4（MDS-S4）。ATPM利用原型库从历史数据模式中推导出稳健且有信息的表示，而MDS-S4则与缺失性掩码一起作为双输入流处理这些表示，以实现准确的预测。通过对多种真实世界数据集的广泛实证评估，我们证明了S4M在性能上始终处于领先地位。这些结果突显了我们集成方法在处理缺失数据方面的有效性，展示了其鲁棒性和传统基于填补的方法相比的优势。我们的研究结果强调了S4M在实际应用中实现可靠时间序列预测的潜力，为未来的研究和部署提供了有前途的方向。代码可在以下链接获取。', 'title_zh': 'S4M: 使用缺失值处理的多变量时间序列预测'}
{'arxiv_id': 'arXiv:2503.00890', 'title': 'Estimating Blood Pressure with a Camera: An Exploratory Study of Ambulatory Patients with Cardiovascular Disease', 'authors': 'Theodore Curran, Chengqian Ma, Xin Liu, Daniel McDuff, Girish Narayanswamy, George Stergiou, Shwetak Patel, Eugene Yang', 'link': 'https://arxiv.org/abs/2503.00890', 'abstract': "Hypertension is a leading cause of morbidity and mortality worldwide. The ability to diagnose and treat hypertension in the ambulatory population is hindered by limited access and poor adherence to current methods of monitoring blood pressure (BP), specifically, cuff-based devices. Remote photoplethysmography (rPPG) evaluates an individual's pulse waveform through a standard camera without physical contact. Cameras are readily available to the majority of the global population via embedded technologies such as smartphones, thus rPPG is a scalable and promising non-invasive method of BP monitoring. The few studies investigating rPPG for BP measurement have excluded high-risk populations, including those with cardiovascular disease (CVD) or its risk factors, as well as subjects in active cardiac arrhythmia. The impact of arrhythmia, like atrial fibrillation, on the prediction of BP using rPPG is currently uncertain. We performed a study to better understand the relationship between rPPG and BP in a real-world sample of ambulatory patients from a cardiology clinic with established CVD or risk factors for CVD. We collected simultaneous rPPG, PPG, BP, ECG, and other vital signs data from 143 subjects while at rest, and used this data plus demographics to train a deep learning model to predict BP. We report that facial rPPG yields a signal that is comparable to finger PPG. Pulse wave analysis (PWA)-based BP estimates on this cohort performed comparably to studies on healthier subjects, and notably, the accuracy of BP prediction in subjects with atrial fibrillation was not inferior to subjects with normal sinus rhythm. In a binary classification task, the rPPG model identified subjects with systolic BP $\\geq$ 130 mm Hg with a positive predictive value of 71% (baseline prevalence 48.3%), highlighting the potential of rPPG for hypertension monitoring.", 'abstract_zh': '远程光电容抗图在心血管疾病患者中的血压监测研究', 'title_zh': '使用摄像头估计血压：心血管疾病门诊患者的探索性研究'}
{'arxiv_id': 'arXiv:2503.00871', 'title': 'CyberCScope: Mining Skewed Tensor Streams and Online Anomaly Detection in Cybersecurity Systems', 'authors': 'Kota Nakamura, Koki Kawabata, Shungo Tanaka, Yasuko Matsubara, Yasushi Sakurai', 'link': 'https://arxiv.org/abs/2503.00871', 'abstract': 'Cybersecurity systems are continuously producing a huge number of time-stamped events in the form of high-order tensors, such as {count; time, port, flow duration, packet size, . . . }, and so how can we detect anomalies/intrusions in real time? How can we identify multiple types of intrusions and capture their characteristic behaviors? The tensor data consists of categorical and continuous attributes and the data distributions of continuous attributes typically exhibit skew. These data properties require handling skewed infinite and finite dimensional spaces simultaneously. In this paper, we propose a novel streaming method, namely CyberCScope. The method effectively decomposes incoming tensors into major trends while explicitly distinguishing between categorical and skewed continuous attributes. To our knowledge, it is the first to compute hybrid skewed infinite and finite dimensional decomposition. Based on this decomposition, it streamingly finds distinct time-evolving patterns, enabling the detection of multiple types of anomalies. Extensive experiments on large-scale real datasets demonstrate that CyberCScope detects various intrusions with higher accuracy than state-of-the-art baselines while providing meaningful summaries for the intrusions that occur in practice.', 'abstract_zh': '基于高阶张量的实时网络异常检测方法：CyberCScope', 'title_zh': 'CyberCScope: 网络安全系统中倾斜张量流的挖掘与在线异常检测'}
{'arxiv_id': 'arXiv:2503.00804', 'title': 'DELST: Dual Entailment Learning for Hyperbolic Image-Gene Pretraining in Spatial Transcriptomics', 'authors': 'Xulin Chen, Junzhou Huang', 'link': 'https://arxiv.org/abs/2503.00804', 'abstract': 'Spatial transcriptomics (ST) maps gene expression within tissue at individual spots, making it a valuable resource for multimodal representation learning. Additionally, ST inherently contains rich hierarchical information both across and within modalities. For instance, different spots exhibit varying numbers of nonzero gene expressions, corresponding to different levels of cellular activity and semantic hierarchies. However, existing methods rely on contrastive alignment of image-gene pairs, failing to accurately capture the intricate hierarchical relationships in ST data. Here, we propose DELST, the first framework to embed hyperbolic representations while modeling hierarchy for image-gene pretraining at two levels: (1) Cross-modal entailment learning, which establishes an order relationship between genes and images to enhance image representation generalization; (2) Intra-modal entailment learning, which encodes gene expression patterns as hierarchical relationships, guiding hierarchical learning across different samples at a global scale and integrating biological insights into single-modal representations. Extensive experiments on ST benchmarks annotated by pathologists demonstrate the effectiveness of our framework, achieving improved predictive performance compared to existing methods. Our code and models are available at: this https URL.', 'abstract_zh': '基于空间转录组学的双层次嵌入式层级表示学习框架（DELST）', 'title_zh': 'DELST: 双ionale推理学习在空间转录组学中基于双曲空间的图像-基因预训练'}
{'arxiv_id': 'arXiv:2503.00788', 'title': 'Taming Infinity one Chunk at a Time: Concisely Represented Strategies in One-Counter MDPs', 'authors': 'Michal Ajdarów, James C. A. Main, Petr Novotný, Mickael Randour', 'link': 'https://arxiv.org/abs/2503.00788', 'abstract': 'Markov decision processes (MDPs) are a canonical model to reason about decision making within a stochastic environment. We study a fundamental class of infinite MDPs: one-counter MDPs (OC-MDPs). They extend finite MDPs via an associated counter taking natural values, thus inducing an infinite MDP over the set of configurations (current state and counter value). We consider two characteristic objectives: reaching a target state (state-reachability), and reaching a target state with counter value zero (selective termination). The synthesis problem for the latter is not known to be decidable and connected to major open problems in number theory. Furthermore, even seemingly simple strategies (e.g., memoryless ones) in OC-MDPs might be impossible to build in practice (due to the underlying infinite configuration space): we need finite, and preferably small, representations.\nTo overcome these obstacles, we introduce two natural classes of concisely represented strategies based on a (possibly infinite) partition of counter values in intervals. For both classes, and both objectives, we study the verification problem (does a given strategy ensure a high enough probability for the objective?), and two synthesis problems (does there exist such a strategy?): one where the interval partition is fixed as input, and one where it is only parameterized. We develop a generic approach based on a compression of the induced infinite MDP that yields decidability in all cases, with all complexities within PSPACE.', 'abstract_zh': '马尔可夫决策过程（MDPs）是用于在随机环境中进行决策推理的标准模型。我们研究了一类无限MDPs的基本类型：一计数器MDPs（OC-MDPs）。它们通过关联一个取自然数值的计数器来扩展有限MDPs，从而在状态集合（当前状态和计数器值）上诱导一个无限MDP。我们考虑了两种特征目标：达到目标状态（状态可达性）和达到目标状态且计数器值为零（选择性终止）。后者的目标综合问题尚未被证明是可判定的，并且与数论中的主要开放问题相关。此外，即使在OC-MDPs中看似简单的策略（例如，无记忆策略）在实践中也可能难以构建（由于潜在的无限配置空间）：我们需要有限的，并且最好是很小的表示。\n\n为了克服这些障碍，我们引入了两类基于计数器值区间划分的简明表示策略。对于这两类策略和两种目标，我们研究了验证问题（给定的策略能否确保足够高的目标概率？）以及两种综合问题（是否存在这样的策略？）：一类是区间划分作为输入固定下来，另一类是参数化仅限于区间划分。我们基于诱导的无限MDP压缩提出了一种通用方法，在所有情况下都保证可判定性，并且所有复杂性都在PSPACE之内。', 'title_zh': '逐块驯服无限：One-Counter MDP中简洁表示的策略'}
{'arxiv_id': 'arXiv:2503.00786', 'title': 'Graph Attention Networks Unleashed: A Fast and Explainable Vulnerability Assessment Framework for Microgrids', 'authors': 'Wei Liu, Tao Zhang, Chenhui Lin, Kaiwen Li, Rui Wang', 'link': 'https://arxiv.org/abs/2503.00786', 'abstract': 'Independent microgrids are crucial for supplying electricity by combining distributed energy resources and loads in scenarios like isolated islands and field combat. Fast and accurate assessments of microgrid vulnerability against intentional attacks or natural disasters are essential for effective risk prevention and design optimization. However, conventional Monte Carlo simulation (MCS) methods are computationally expensive and time-consuming, while existing machine learning-based approaches often lack accuracy and explainability. To address these challenges, this study proposes a fast and explainable vulnerability assessment framework that integrates MCS with a graph attention network enhanced by self-attention pooling (GAT-S). MCS generates training data, while the GAT-S model learns the structural and electrical characteristics of the microgrid and further assesses its vulnerability intelligently. The GAT-S improves explainability and computational efficiency by dynamically assigning attention weights to critical nodes. Comprehensive experimental evaluations across various microgrid configurations demonstrate that the proposed framework provides accurate vulnerability assessments, achieving a mean squared error as low as 0.001, real-time responsiveness within 1 second, and delivering explainable results.', 'abstract_zh': '独立微电网在孤立岛屿和战场等领域通过结合分布式能源和负荷供电至关重要。快速准确评估微电网在故意攻击或自然灾害下的脆弱性对于有效的风险预防和设计优化至关重要。然而，传统的蒙特卡洛模拟（MCS）方法计算成本高且耗时，而现有的基于机器学习的方法通常缺乏准确性和可解释性。为解决这些挑战，本研究提出了一种结合MCS并由自注意力聚合增强的图注意力网络（GAT-S）的快速可解释脆弱性评估框架。MCS生成训练数据，GAT-S模型学习微电网的结构和电气特性，并进一步智能地评估其脆弱性。GAT-S通过动态分配注意力权重给关键节点来提高可解释性和计算效率。在多种微电网配置下的全面实验评估表明，所提出的方法提供了准确的脆弱性评估，均方误差低至0.001，可以在1秒内实现即时响应，并提供可解释的结果。', 'title_zh': 'Graph Attention Networks 解锁：一种快速且可解释的微电网漏洞评估框架'}
{'arxiv_id': 'arXiv:2503.00762', 'title': 'MR-EIT: Multi-Resolution Reconstruction for Electrical Impedance Tomography via Data-Driven and Unsupervised Dual-Mode Neural Networks', 'authors': 'Fangming Shi, Jinzhen Liu, Xiangqian Meng, Yapeng Zhou, Hui Xiong', 'link': 'https://arxiv.org/abs/2503.00762', 'abstract': 'This paper presents a multi-resolution reconstruction method for Electrical Impedance Tomography (EIT), referred to as MR-EIT, which is capable of operating in both supervised and unsupervised learning modes. MR-EIT integrates an ordered feature extraction module and an unordered coordinate feature expression module. The former achieves the mapping from voltage to two-dimensional conductivity features through pre-training, while the latter realizes multi-resolution reconstruction independent of the order and size of the input sequence by utilizing symmetric functions and local feature extraction mechanisms. In the data-driven mode, MR-EIT reconstructs high-resolution images from low-resolution data of finite element meshes through two stages of pre-training and joint training, and demonstrates excellent performance in simulation experiments. In the unsupervised learning mode, MR-EIT does not require pre-training data and performs iterative optimization solely based on measured voltages to rapidly achieve image reconstruction from low to high resolution. It shows robustness to noise and efficient super-resolution reconstruction capabilities in both simulation and real water tank experiments. Experimental results indicate that MR-EIT outperforms the comparison methods in terms of Structural Similarity (SSIM) and Relative Image Error (RIE), especially in the unsupervised learning mode, where it can significantly reduce the number of iterations and improve image reconstruction quality.', 'abstract_zh': '基于多分辨率重建的电气阻抗断层成像方法（MR-EIT）', 'title_zh': '基于数据驱动和无监督双模式神经网络的多分辨率 Electrical Impedance Tomography 重建方法'}
{'arxiv_id': 'arXiv:2503.00751', 'title': 'RAPID: Efficient Retrieval-Augmented Long Text Generation with Writing Planning and Information Discovery', 'authors': 'Hongchao Gu, Dexun Li, Kuicai Dong, Hao Zhang, Hang Lv, Hao Wang, Defu Lian, Yong Liu, Enhong Chen', 'link': 'https://arxiv.org/abs/2503.00751', 'abstract': 'Generating knowledge-intensive and comprehensive long texts, such as encyclopedia articles, remains significant challenges for Large Language Models. It requires not only the precise integration of facts but also the maintenance of thematic coherence throughout the article. Existing methods, such as direct generation and multi-agent discussion, often struggle with issues like hallucinations, topic incoherence, and significant latency. To address these challenges, we propose RAPID, an efficient retrieval-augmented long text generation framework. RAPID consists of three main modules: (1) Retrieval-augmented preliminary outline generation to reduce hallucinations, (2) Attribute-constrained search for efficient information discovery, (3) Plan-guided article generation for enhanced coherence. Extensive experiments on our newly compiled benchmark dataset, FreshWiki-2024, demonstrate that RAPID significantly outperforms state-of-the-art methods across a wide range of evaluation metrics (e.g. long-text generation, outline quality, latency, etc). Our work provides a robust and efficient solution to the challenges of automated long-text generation.', 'abstract_zh': '大规模语言模型生成知识密集且全面的长文本（例如百科文章）仍然存在显著挑战。这不仅需要精确整合事实，还需要在整个文章中维护主题连贯性。现有方法，如直接生成和多智能体讨论，常常遇到幻觉、主题不连贯和显著延迟等问题。为应对这些挑战，我们提出了一种高效的检索增强长文本生成框架RAPID。RAPID由三个主要模块组成：（1）检索增强初步大纲生成以减少幻觉，（2）属性约束搜索以提高信息发现效率，（3）计划导向的文章生成以增强连贯性。在我们新编撰的基准数据集FreshWiki-2024上进行的大量实验表明，RAPID在多种评估指标（如长文本生成、大纲质量、延迟等）上显著优于现有最先进的方法。本工作为自动化长文本生成面临的挑战提供了稳健且高效的解决方案。', 'title_zh': 'RAPID:高效的检索增强长文本生成with写作规划和信息发现'}
{'arxiv_id': 'arXiv:2503.00750', 'title': 'Edge Prompt Tuning for Graph Neural Networks', 'authors': 'Xingbo Fu, Yinhan He, Jundong Li', 'link': 'https://arxiv.org/abs/2503.00750', 'abstract': 'Pre-training powerful Graph Neural Networks (GNNs) with unlabeled graph data in a self-supervised manner has emerged as a prominent technique in recent years. However, inevitable objective gaps often exist between pre-training and downstream tasks. To bridge this gap, graph prompt tuning techniques design and learn graph prompts by manipulating input graphs or reframing downstream tasks as pre-training tasks without fine-tuning the pre-trained GNN models. While recent graph prompt tuning methods have proven effective in adapting pre-trained GNN models for downstream tasks, they overlook the crucial role of edges in graph prompt design, which can significantly affect the quality of graph representations for downstream tasks. In this study, we propose EdgePrompt, a simple yet effective graph prompt tuning method from the perspective of edges. Unlike previous studies that design prompt vectors on node features, EdgePrompt manipulates input graphs by learning additional prompt vectors for edges and incorporates the edge prompts through message passing in the pre-trained GNN models to better embed graph structural information for downstream tasks. Our method is compatible with prevalent GNN architectures pre-trained under various pre-training strategies and is universal for different downstream tasks. We provide comprehensive theoretical analyses of our method regarding its capability of handling node classification and graph classification as downstream tasks. Extensive experiments on ten graph datasets under four pre-training strategies demonstrate the superiority of our proposed method against six baselines. Our code is available at this https URL.', 'abstract_zh': '使用边提示调谐预训练图神经网络以自监督方式利用未标label的图数据已成为近年来一种 prominient 技术。然而，预训练与下游任务之间常常存在不可避免的目标差距。为弥补这一差距，图提示调谐技术通过操控输入图或将下游任务重新框定为预训练任务来设计和学习图提示，而不对预训练的图神经网络模型进行微调。尽管最近的图提示调谐方法已证明可以有效地适应预训练的图神经网络模型以应对下游任务，但它们忽视了边在图提示设计中的关键作用，这可能会显著影响下游任务的图表示质量。在此研究中，我们提出了一种从边的角度出发的简单而有效的图提示调谐方法——EdgePrompt。与之前设计提示向量基于节点特征的研究不同，EdgePrompt 通过学习边的额外提示向量并将其通过预训练的图神经网络模型中的消息传递机制整合进来，从而更好地嵌入图结构信息以应对下游任务。我们的方法与各种预训练策略下预训练的不同图 neural 网络架构兼容，并适用于不同的下游任务。我们从理论上全面分析了该方法在处理节点分类和图分类等下游任务时的能力。在四种预训练策略下的十个图数据集上进行的广泛实验表明，与六个基线方法相比，我们提出的方法具有明显优势。我们的代码可在此 https URL 获取。', 'title_zh': '基于边缘提示调整的图神经网络'}
{'arxiv_id': 'arXiv:2503.00744', 'title': 'Confounder-Aware Medical Data Selection for Fine-Tuning Pretrained Vision Models', 'authors': 'Anyang Ji, Qingbo Kang, Wei Xu, Changfan Wang, Kang Li, Qicheng Lao', 'link': 'https://arxiv.org/abs/2503.00744', 'abstract': 'The emergence of large-scale pre-trained vision foundation models has greatly advanced the medical imaging field through the pre-training and fine-tuning paradigm. However, selecting appropriate medical data for downstream fine-tuning remains a significant challenge considering its annotation cost, privacy concerns, and the detrimental effects of confounding variables. In this work, we present a confounder-aware medical data selection approach for medical dataset curation aiming to select minimal representative data by strategically mitigating the undesirable impact of confounding variables while preserving the natural distribution of the dataset. Our approach first identifies confounding variables within data and then develops a distance-based data selection strategy for confounder-aware sampling with a constrained budget in the data size. We validate the superiority of our approach through extensive experiments across diverse medical imaging modalities, highlighting its effectiveness in addressing the substantial impact of confounding variables and enhancing the fine-tuning efficiency in the medical imaging domain, compared to other data selection approaches.', 'abstract_zh': '大规模预训练视觉基础模型的出现通过预训练和微调范式极大地推动了医学影像领域的发展。然而，选择合适的医学数据进行下游微调仍然是一个重大挑战，考虑到标注成本、隐私问题以及混杂变量的负面影响。本文提出了一种混杂变量意识下的医学数据选择方法，旨在通过策略性地减轻混杂变量的不利影响，同时保持数据集的自然分布，来选择最小的代表性数据。我们的方法首先在数据中识别出混杂变量，然后开发了一种基于距离的数据选择策略，以在数据量受限的预算下实现混杂变量意识下的采样。通过在多种医学影像模态下进行广泛的实验，验证了我们方法的优势，证明了其在处理混杂变量的显著影响和提高医学影像领域微调效率方面的有效性， compared to other data selection approaches。', 'title_zh': '带有混杂因素意识的医学数据选择以微调预训练视觉模型'}
{'arxiv_id': 'arXiv:2503.00711', 'title': 'OpenECG: Benchmarking ECG Foundation Models with Public 1.2 Million Records', 'authors': 'Zhijiang Wan, Qianhao Yu, Jia Mao, Wenfeng Duan, Cheng Ding', 'link': 'https://arxiv.org/abs/2503.00711', 'abstract': 'This study introduces OpenECG, a large-scale benchmark of 1.2 million 12-lead ECG recordings from nine centers, to evaluate ECG foundation models (ECG-FMs) trained on public datasets. We investigate three self-supervised learning methods (SimCLR, BYOL, MAE) with ResNet-50 and Vision Transformer architectures, assessing model generalization through leave-one-dataset-out experiments and data scaling analysis. Results show that pre-training on diverse datasets significantly improves generalization, with BYOL and MAE outperforming SimCLR, highlighting the efficacy of feature-consistency and generative learning over contrastive approaches. Data scaling experiments reveal that performance saturates at 60-70% of total data for BYOL and MAE, while SimCLR requires more data. These findings demonstrate that publicly available ECG data can match or surpass proprietary datasets in training robust ECG-FMs, paving the way for scalable, clinically meaningful AI-driven ECG analysis.', 'abstract_zh': '本研究介绍了包含九个中心的1.2百万条12导联ECG记录的大规模基准OpenECG，以评估在公共数据集中训练的心电图基础模型（ECG-FMs）。我们研究了三种自监督学习方法（SimCLR、BYOL、MAE）结合ResNet-50和视觉变换器架构，并通过留一数据集法实验和数据扩展分析评估模型泛化能力。结果显示，在多样化的数据集上预训练显著提高了泛化能力，BYOL和MAE优于SimCLR，表明特征一致性及生成学习优于对比学习方法。数据扩展实验表明，对于BYOL和MAE，性能在60-70%的总数据量时达到饱和，而SimCLR需要更多的数据。这些发现展示了公开可用的心电图数据在训练稳健的心电图基础模型方面可以匹配或超越专有数据集的能力，为可扩展的、具有临床意义的AI驱动心电图分析铺平了道路。', 'title_zh': 'OpenECG: 基于公共120万条记录评估心电图基础模型'}
{'arxiv_id': 'arXiv:2503.00699', 'title': 'Parameter Expanded Stochastic Gradient Markov Chain Monte Carlo', 'authors': 'Hyunsu Kim, Giung Nam, Chulhee Yun, Hongseok Yang, Juho Lee', 'link': 'https://arxiv.org/abs/2503.00699', 'abstract': 'Bayesian Neural Networks (BNNs) provide a promising framework for modeling predictive uncertainty and enhancing out-of-distribution robustness (OOD) by estimating the posterior distribution of network parameters. Stochastic Gradient Markov Chain Monte Carlo (SGMCMC) is one of the most powerful methods for scalable posterior sampling in BNNs, achieving efficiency by combining stochastic gradient descent with second-order Langevin dynamics. However, SGMCMC often suffers from limited sample diversity in practice, which affects uncertainty estimation and model performance. We propose a simple yet effective approach to enhance sample diversity in SGMCMC without the need for tempering or running multiple chains. Our approach reparameterizes the neural network by decomposing each of its weight matrices into a product of matrices, resulting in a sampling trajectory that better explores the target parameter space. This approach produces a more diverse set of samples, allowing faster mixing within the same computational budget. Notably, our sampler achieves these improvements without increasing the inference cost compared to the standard SGMCMC. Extensive experiments on image classification tasks, including OOD robustness, diversity, loss surface analyses, and a comparative study with Hamiltonian Monte Carlo, demonstrate the superiority of the proposed approach.', 'abstract_zh': '贝叶斯神经网络（BNNs）提供了一种前景广阔的框架，用于通过估算网络参数的后验分布来建模预测不确定性并增强分布外鲁棒性（OOD）。随机梯度马尔可夫链蒙特卡洛（SGMCMC）是BNNs中可扩展后验采样的最强大方法之一，通过将随机梯度下降与二阶拉angevin动力学相结合，实现了高效性。然而，SGMCMC在实践中常遭受样本多样性有限的困扰，这影响了不确定性估计和模型性能。我们提出了一种简单有效的方法，可以在无需退火或运行多个链的情况下增强SGMCMC中的样本多样性。该方法通过将神经网络的每个权重矩阵分解为矩阵的乘积来重新参数化神经网络，从而产生更好地探索目标参数空间的采样轨迹。该方法生成了一组更多样化的样本，允许在同一计算预算内更快地混合。值得注意的是，我们的采样器在与标准SGMCMC相比不增加推理成本的情况下实现了这些改进。广泛的图像分类任务实验，包括分布外鲁棒性、多样性、损失表面分析以及与哈密尔顿蒙特卡洛的对比研究，展示了所提方法的优势。', 'title_zh': '参数扩展随机梯度马尔可夫链蒙特卡洛'}
{'arxiv_id': 'arXiv:2503.00697', 'title': 'CREATE-FFPE: Cross-Resolution Compensated and Multi-Frequency Enhanced FS-to-FFPE Stain Transfer for Intraoperative IHC Images', 'authors': 'Yiyang Lin, Danling Jiang, Xinyu Liu, Yun Miao, Yixuan Yuan', 'link': 'https://arxiv.org/abs/2503.00697', 'abstract': "In the immunohistochemical (IHC) analysis during surgery, frozen-section (FS) images are used to determine the benignity or malignancy of the tumor. However, FS image faces problems such as image contamination and poor nuclear detail, which may disturb the pathologist's diagnosis. In contrast, formalin-fixed and paraffin-embedded (FFPE) image has a higher staining quality, but it requires quite a long time to prepare and thus is not feasible during surgery. To help pathologists observe IHC images with high quality in surgery, this paper proposes a Cross-REsolution compensATed and multi-frequency Enhanced FS-to-FFPE (CREATE-FFPE) stain transfer framework, which is the first FS-to-FFPE method for the intraoperative IHC images. To solve the slide contamination and poor nuclear detail mentioned above, we propose the cross-resolution compensation module (CRCM) and the wavelet detail guidance module (WDGM). Specifically, CRCM compensates for information loss due to contamination by providing more tissue information across multiple resolutions, while WDGM produces the desirable details in a wavelet way, and the details can be used to guide the stain transfer to be more precise. Experiments show our method can beat all the competing methods on our dataset. In addition, the FID has decreased by 44.4%, and KID*100 has decreased by 71.2% by adding the proposed CRCM and WDGM in ablation studies, and the performance of a downstream microsatellite instability prediction task with public dataset can be greatly improved by performing our FS-to-FFPE stain transfer.", 'abstract_zh': '基于跨分辨率补偿和多频率增强的术中冷冻切片到石蜡切片染色转移框架（CREATE-FFPE）', 'title_zh': 'CREATE-FFPE：跨分辨率补偿和多频增强的FS-to-FFPE染色转移方法用于术中IHC图像'}
{'arxiv_id': 'arXiv:2503.00674', 'title': 'OrdRankBen: A Novel Ranking Benchmark for Ordinal Relevance in NLP', 'authors': 'Yan Wang, Lingfei Qian, Xueqing Peng, Jimin Huang, Dongji Feng', 'link': 'https://arxiv.org/abs/2503.00674', 'abstract': 'The evaluation of ranking tasks remains a significant challenge in natural language processing (NLP), particularly due to the lack of direct labels for results in real-world scenarios. Benchmark datasets play a crucial role in providing standardized testbeds that ensure fair comparisons, enhance reproducibility, and enable progress tracking, facilitating rigorous assessment and continuous improvement of ranking models. Existing NLP ranking benchmarks typically use binary relevance labels or continuous relevance scores, neglecting ordinal relevance scores. However, binary labels oversimplify relevance distinctions, while continuous scores lack a clear ordinal structure, making it challenging to capture nuanced ranking differences effectively. To address these challenges, we introduce OrdRankBen, a novel benchmark designed to capture multi-granularity relevance distinctions. Unlike conventional benchmarks, OrdRankBen incorporates structured ordinal labels, enabling more precise ranking evaluations. Given the absence of suitable datasets for ordinal relevance ranking in NLP, we constructed two datasets with distinct ordinal label distributions. We further evaluate various models for three model types, ranking-based language models, general large language models, and ranking-focused large language models on these datasets. Experimental results show that ordinal relevance modeling provides a more precise evaluation of ranking models, improving their ability to distinguish multi-granularity differences among ranked items-crucial for tasks that demand fine-grained relevance differentiation.', 'abstract_zh': 'ordinal relevance ranking benchmarks in natural language processing: introducing OrdRankBen', 'title_zh': 'OrdRankBen：一种新的顺序相关性排名基准在NLP领域'}
{'arxiv_id': 'arXiv:2503.00664', 'title': 'Generative Artificial Intelligence for Academic Research: Evidence from Guidance Issued for Researchers by Higher Education Institutions in the United States', 'authors': 'Amrita Ganguly, Aditya Johri, Areej Ali, Nora McDonald', 'link': 'https://arxiv.org/abs/2503.00664', 'abstract': "The recent development and use of generative AI (GenAI) has signaled a significant shift in research activities such as brainstorming, proposal writing, dissemination, and even reviewing. This has raised questions about how to balance the seemingly productive uses of GenAI with ethical concerns such as authorship and copyright issues, use of biased training data, lack of transparency, and impact on user privacy. To address these concerns, many Higher Education Institutions (HEIs) have released institutional guidance for researchers. To better understand the guidance that is being provided we report findings from a thematic analysis of guidelines from thirty HEIs in the United States that are classified as R1 or 'very high research activity.' We found that guidance provided to researchers: (1) asks them to refer to external sources of information such as funding agencies and publishers to keep updated and use institutional resources for training and education; (2) asks them to understand and learn about specific GenAI attributes that shape research such as predictive modeling, knowledge cutoff date, data provenance, and model limitations, and educate themselves about ethical concerns such as authorship, attribution, privacy, and intellectual property issues; and (3) includes instructions on how to acknowledge sources and disclose the use of GenAI, how to communicate effectively about their GenAI use, and alerts researchers to long term implications such as over reliance on GenAI, legal consequences, and risks to their institutions from GenAI use. Overall, guidance places the onus of compliance on individual researchers making them accountable for any lapses, thereby increasing their responsibility.", 'abstract_zh': '最近生成AI（GenAI）的发展及其应用标志着研究活动如头脑风暴、提案写作、传播甚至评审方面的一个重大转变。这引发了如何平衡看起来具有生产力的GenAI使用与作者身份和版权问题、偏见训练数据的使用、缺乏透明度以及对用户隐私的影响之间的关系的伦理关切。为了应对这些关切，许多高等教育机构（HEIs）已经发布了针对研究人员的指导意见。为了更好地了解提供的指导，我们报告了对美国R1或“非常高研究活动”分类的三十所高等教育机构发布的指导方针进行主题分析的结果。我们发现，这些指导方针要求研究人员：（1）参考外部信息来源如资助机构和出版商以保持更新，并利用机构资源进行培训和教育；（2）了解和学习影响研究的特定GenAI属性，例如预测建模、知识截止日期、数据来源和模型限制，并了解相关的伦理关切，如作者身份、归属、隐私和知识产权问题；（3）包含如何承认来源、披露使用GenAI、有效沟通其使用GenAI的方法，以及提醒研究人员长期影响，如过度依赖GenAI、法律后果以及GenAI使用对他们的机构带来的风险。总体而言，这些指导方针将合规的责任放在个人研究人员身上，使他们对其任何疏忽负责，从而增加了他们的责任。', 'title_zh': '生成型人工智能在学术研究中的应用：来自美国高等教育机构对研究人员指导意见的实证证据'}
{'arxiv_id': 'arXiv:2503.00643', 'title': 'Deep Change Monitoring: A Hyperbolic Representative Learning Framework and a Dataset for Long-term Fine-grained Tree Change Detection', 'authors': 'Yante Li, Hanwen Qi, Haoyu Chen, Xinlian Liang, Guoying Zhao', 'link': 'https://arxiv.org/abs/2503.00643', 'abstract': 'In environmental protection, tree monitoring plays an essential role in maintaining and improving ecosystem health. However, precise monitoring is challenging because existing datasets fail to capture continuous fine-grained changes in trees due to low-resolution images and high acquisition costs. In this paper, we introduce UAVTC, a large-scale, long-term, high-resolution dataset collected using UAVs equipped with cameras, specifically designed to detect individual Tree Changes (TCs). UAVTC includes rich annotations and statistics based on biological knowledge, offering a fine-grained view for tree monitoring. To address environmental influences and effectively model the hierarchical diversity of physiological TCs, we propose a novel Hyperbolic Siamese Network (HSN) for TC detection, enabling compact and hierarchical representations of dynamic tree changes.\nExtensive experiments show that HSN can effectively capture complex hierarchical changes and provide a robust solution for fine-grained TC detection. In addition, HSN generalizes well to cross-domain face anti-spoofing task, highlighting its broader significance in AI. We believe our work, combining ecological insights and interdisciplinary expertise, will benefit the community by offering a new benchmark and innovative AI technologies.', 'abstract_zh': '基于无人机的大规模高分辨率树变化检测数据集及超球面西蒙网络（UAVTC及其Hyperbolic Siamese Network在树变化检测中的应用）', 'title_zh': '深空变化监测：一种双曲代表学习框架及长时间细粒度树变化检测数据集'}
{'arxiv_id': 'arXiv:2503.00583', 'title': 'Space-Time Graphs of Convex Sets for Multi-Robot Motion Planning', 'authors': 'Jingtao Tang, Zining Mao, Lufan Yang, Hang Ma', 'link': 'https://arxiv.org/abs/2503.00583', 'abstract': 'We address the Multi-Robot Motion Planning (MRMP) problem of computing collision-free trajectories for multiple robots in shared continuous environments. While existing frameworks effectively decompose MRMP into single-robot subproblems, spatiotemporal motion planning with dynamic obstacles remains challenging, particularly in cluttered or narrow-corridor settings. We propose Space-Time Graphs of Convex Sets (ST-GCS), a novel planner that systematically covers the collision-free space-time domain with convex sets instead of relying on random sampling. By extending Graphs of Convex Sets (GCS) into the time dimension, ST-GCS formulates time-optimal trajectories in a unified convex optimization that naturally accommodates velocity bounds and flexible arrival times. We also propose Exact Convex Decomposition (ECD) to "reserve" trajectories as spatiotemporal obstacles, maintaining a collision-free space-time graph of convex sets for subsequent planning. Integrated into two prioritized-planning frameworks, ST-GCS consistently achieves higher success rates and better solution quality than state-of-the-art sampling-based planners -- often at orders-of-magnitude faster runtimes -- underscoring its benefits for MRMP in challenging settings.', 'abstract_zh': '多机器人时空路径规划：基于凸集的空间-时间图规划方法', 'title_zh': '凸集的时空图在多机器人运动规划中的应用'}
{'arxiv_id': 'arXiv:2503.00563', 'title': 'A Guide to Failure in Machine Learning: Reliability and Robustness from Foundations to Practice', 'authors': 'Eric Heim, Oren Wright, David Shriver', 'link': 'https://arxiv.org/abs/2503.00563', 'abstract': 'One of the main barriers to adoption of Machine Learning (ML) is that ML models can fail unexpectedly. In this work, we aim to provide practitioners a guide to better understand why ML models fail and equip them with techniques they can use to reason about failure. Specifically, we discuss failure as either being caused by lack of reliability or lack of robustness. Differentiating the causes of failure in this way allows us to formally define why models fail from first principles and tie these definitions to engineering concepts and real-world deployment settings. Throughout the document we provide 1) a summary of important theoretic concepts in reliability and robustness, 2) a sampling current techniques that practitioners can utilize to reason about ML model reliability and robustness, and 3) examples that show how these concepts and techniques can apply to real-world settings.', 'abstract_zh': '机器学习模型失败的主要障碍之一是这些模型可能会意外失败。在这项工作中，我们旨在为实践者提供一个指南，帮助他们更好地理解为何机器学习模型会失败，并使他们能够运用技术来推理模型失败的原因。具体来说，我们将失败分为可靠性不足和鲁棒性不足两种原因。通过这种方式区分失败的原因，使我们能够从第一原理出发正式定义模型失败的原因，并将其与工程概念和实际部署场景联系起来。在整个文档中，我们提供了1）可靠性与鲁棒性的重要理论概念总结，2）实践者可以利用来推理机器学习模型可靠性和鲁棒性的当前技术样本，以及3）这些概念和技术在实际应用场景中的示例。', 'title_zh': '机器学习中可靠性与鲁棒性基础到实践的失败指南'}
{'arxiv_id': 'arXiv:2503.00535', 'title': 'What Makes a Good Diffusion Planner for Decision Making?', 'authors': 'Haofei Lu, Dongqi Han, Yifei Shen, Dongsheng Li', 'link': 'https://arxiv.org/abs/2503.00535', 'abstract': 'Diffusion models have recently shown significant potential in solving decision-making problems, particularly in generating behavior plans -- also known as diffusion planning. While numerous studies have demonstrated the impressive performance of diffusion planning, the mechanisms behind the key components of a good diffusion planner remain unclear and the design choices are highly inconsistent in existing studies. In this work, we address this issue through systematic empirical experiments on diffusion planning in an offline reinforcement learning (RL) setting, providing practical insights into the essential components of diffusion planning. We trained and evaluated over 6,000 diffusion models, identifying the critical components such as guided sampling, network architecture, action generation and planning strategy. We revealed that some design choices opposite to the common practice in previous work in diffusion planning actually lead to better performance, e.g., unconditional sampling with selection can be better than guided sampling and Transformer outperforms U-Net as denoising network. Based on these insights, we suggest a simple yet strong diffusion planning baseline that achieves state-of-the-art results on standard offline RL benchmarks.', 'abstract_zh': '扩散模型在解决决策问题方面 recently demonstrated significant potential, particularly in generating behavior plans -- also known as diffusion planning. 通过系统性的离线强化学习实验，我们探讨了扩散规划的关键组件及其设计选择，提供了实用的洞察。我们训练和评估了超过6,000个扩散模型，确定了关键组件如引导采样、网络架构、动作生成和规划策略。我们发现，一些与以往扩散规划工作中常见做法相反的设计选择实际上会导致更好的性能，例如无条件采样结合选择可能优于引导采样，Transformer 在去噪网络中表现优于U-Net。基于这些洞察，我们提出了一种简单但强大的扩散规划基线，该基线在标准的离线强化学习基准上达到了最先进的结果。', 'title_zh': '什么是好的扩散规划者以供决策使用？'}
{'arxiv_id': 'arXiv:2503.00524', 'title': 'End-To-End Learning of Gaussian Mixture Priors for Diffusion Sampler', 'authors': 'Denis Blessing, Xiaogang Jia, Gerhard Neumann', 'link': 'https://arxiv.org/abs/2503.00524', 'abstract': 'Diffusion models optimized via variational inference (VI) have emerged as a promising tool for generating samples from unnormalized target densities. These models create samples by simulating a stochastic differential equation, starting from a simple, tractable prior, typically a Gaussian distribution. However, when the support of this prior differs greatly from that of the target distribution, diffusion models often struggle to explore effectively or suffer from large discretization errors. Moreover, learning the prior distribution can lead to mode-collapse, exacerbated by the mode-seeking nature of reverse Kullback-Leibler divergence commonly used in VI. To address these challenges, we propose end-to-end learnable Gaussian mixture priors (GMPs). GMPs offer improved control over exploration, adaptability to target support, and increased expressiveness to counteract mode collapse. We further leverage the structure of mixture models by proposing a strategy to iteratively refine the model by adding mixture components during training. Our experimental results demonstrate significant performance improvements across a diverse range of real-world and synthetic benchmark problems when using GMPs without requiring additional target evaluations.', 'abstract_zh': '通过变分推断优化的扩散模型：面向未归一化目标密度的样本生成', 'title_zh': '端到端学习高斯混合先验用于扩散采样'}
{'arxiv_id': 'arXiv:2503.00509', 'title': 'Functional multi-armed bandit and the best function identification problems', 'authors': 'Yuriy Dorn, Aleksandr Katrutsa, Ilgam Latypov, Anastasiia Soboleva', 'link': 'https://arxiv.org/abs/2503.00509', 'abstract': "Bandit optimization usually refers to the class of online optimization problems with limited feedback, namely, a decision maker uses only the objective value at the current point to make a new decision and does not have access to the gradient of the objective function. While this name accurately captures the limitation in feedback, it is somehow misleading since it does not have any connection with the multi-armed bandits (MAB) problem class. We propose two new classes of problems: the functional multi-armed bandit problem (FMAB) and the best function identification problem. They are modifications of a multi-armed bandit problem and the best arm identification problem, respectively, where each arm represents an unknown black-box function. These problem classes are a surprisingly good fit for modeling real-world problems such as competitive LLM training. To solve the problems from these classes, we propose a new reduction scheme to construct UCB-type algorithms, namely, the F-LCB algorithm, based on algorithms for nonlinear optimization with known convergence rates. We provide the regret upper bounds for this reduction scheme based on the base algorithms' convergence rates. We add numerical experiments that demonstrate the performance of the proposed scheme.", 'abstract_zh': '功能型多臂老虎机问题和最佳函数识别问题', 'title_zh': '功能多臂老虎机和最佳函数识别问题'}
{'arxiv_id': 'arXiv:2503.00489', 'title': 'Embracing Diversity: A Multi-Perspective Approach with Soft Labels', 'authors': 'Benedetta Muscato, Praveen Bushipaka, Gizem Gezici, Lucia Passaro, Fosca Giannotti, Tommaso Cucinotta', 'link': 'https://arxiv.org/abs/2503.00489', 'abstract': 'Prior studies show that adopting the annotation diversity shaped by different backgrounds and life experiences and incorporating them into the model learning, i.e. multi-perspective approach, contribute to the development of more responsible models. Thus, in this paper we propose a new framework for designing and further evaluating perspective-aware models on stance detection task,in which multiple annotators assign stances based on a controversial topic. We also share a new dataset established through obtaining both human and LLM annotations. Results show that the multi-perspective approach yields better classification performance (higher F1-scores), outperforming the traditional approaches that use a single ground-truth, while displaying lower model confidence scores, probably due to the high level of subjectivity of the stance detection task.', 'abstract_zh': '先前研究显示，采用由不同背景和生活经验塑造的标注多样性，并将其融入模型学习中，即多视角方法，有助于发展更具责任感的模型。因此，在本文中，我们提出了一种新框架，用于在观点检测任务中设计和进一步评估视角 Awareness 模型，在此框架中，多名标注者基于争议性话题分配观点。我们还共享了一个通过获得人类和大语言模型标注的新数据集。结果显示，多视角方法在分类性能（较高的 F1 分数）上优于使用单一事实标准的传统方法，同时模型的置信度分数较低，这可能归因于观点检测任务的高度主观性。', 'title_zh': '拥抱多样性：一种基于软标签的多视角方法'}
{'arxiv_id': 'arXiv:2503.00483', 'title': 'Interacting with AI Reasoning Models: Harnessing "Thoughts" for AI-Driven Software Engineering', 'authors': 'Christoph Treude, Raula Gaikovina Kula', 'link': 'https://arxiv.org/abs/2503.00483', 'abstract': "Recent advances in AI reasoning models provide unprecedented transparency into their decision-making processes, transforming them from traditional black-box systems into models that articulate step-by-step chains of thought rather than producing opaque outputs. This shift has the potential to improve software quality, explainability, and trust in AI-augmented development. However, software engineers rarely have the time or cognitive bandwidth to analyze, verify, and interpret every AI-generated thought in detail. Without an effective interface, this transparency could become a burden rather than a benefit.\nIn this paper, we propose a vision for structuring the interaction between AI reasoning models and software engineers to maximize trust, efficiency, and decision-making power. We argue that simply exposing AI's reasoning is not enough -- software engineers need tools and frameworks that selectively highlight critical insights, filter out noise, and facilitate rapid validation of key assumptions. To illustrate this challenge, we present motivating examples in which AI reasoning models state their assumptions when deciding which external library to use and produce divergent reasoning paths and recommendations about security vulnerabilities, highlighting the need for an interface that prioritizes actionable insights while managing uncertainty and resolving conflicts. We then outline a research roadmap for integrating automated summarization, assumption validation, and multi-model conflict resolution into software engineering workflows. Achieving this vision will unlock the full potential of AI reasoning models to enable software engineers to make faster, more informed decisions without being overwhelmed by unnecessary detail.", 'abstract_zh': 'Recent advances in AI reasoning models提供前所未有的透明度，使其决策过程得以清晰呈现，从而将传统黑盒系统转化为能逐步展示思考链条的模型。这一转变有望提高软件质量、可解释性和对AI增强开发的信任。然而，软件工程师通常没有足够的时间和认知资源来详细分析、验证和解读每一步AI生成的思考过程。如果没有有效的界面，这种透明度可能会成为负担而非助力。\n\n在本文中，我们提出了一种结构化AI推理模型与软件工程师互动的方式，以最大化信任、效率和决策能力。我们认为，仅仅展示AI的推理过程是不够的——软件工程师需要工具和框架来有选择性地突出关键见解、过滤噪声并快速验证关键假设。为说明这一挑战，我们呈现了激励性示例，展示了AI推理模型在决定使用哪个外部库时陈述其假设，并产生不同推理路径和关于安全漏洞的建议，突显了需要优先突出可操作性见解并管理不确定性及解决冲突的界面。随后，我们阐述了一条将自动总结、假设验证和多模型冲突解决整合到软件工程工作流中的研究路线图。实现这一愿景将释放AI推理模型的全部潜力，使软件工程师能够在不过度关注不必要的细节前提下做出更快、更明智的决策。', 'title_zh': '与AI推理模型互动：利用“思考过程”驱动软件工程'}
{'arxiv_id': 'arXiv:2503.00436', 'title': 'HalCECE: A Framework for Explainable Hallucination Detection through Conceptual Counterfactuals in Image Captioning', 'authors': 'Maria Lymperaiou, Giorgos FIlandrianos, Angeliki Dimitriou, Athanasios Voulodimos, Giorgos Stamou', 'link': 'https://arxiv.org/abs/2503.00436', 'abstract': 'In the dynamic landscape of artificial intelligence, the exploration of hallucinations within vision-language (VL) models emerges as a critical frontier. This work delves into the intricacies of hallucinatory phenomena exhibited by widely used image captioners, unraveling interesting patterns. Specifically, we step upon previously introduced techniques of conceptual counterfactual explanations to address VL hallucinations. The deterministic and efficient nature of the employed conceptual counterfactuals backbone is able to suggest semantically minimal edits driven by hierarchical knowledge, so that the transition from a hallucinated caption to a non-hallucinated one is performed in a black-box manner. HalCECE, our proposed hallucination detection framework is highly interpretable, by providing semantically meaningful edits apart from standalone numbers, while the hierarchical decomposition of hallucinated concepts leads to a thorough hallucination analysis. Another novelty tied to the current work is the investigation of role hallucinations, being one of the first works to involve interconnections between visual concepts in hallucination detection. Overall, HalCECE recommends an explainable direction to the crucial field of VL hallucination detection, thus fostering trustworthy evaluation of current and future VL systems.', 'abstract_zh': '在人工智能动态landscape中，视觉-语言模型中的幻觉探索 emerged as a critical frontier. 本文探讨了广泛使用的图像描述器中展现的幻觉现象的复杂性，揭示了有趣的模式。具体地，我们利用先前介绍的概念反事实解释技术来应对视觉-语言幻觉。所采用的概念反事实解释框架具有确定性和高效性，能够提供由层次知识驱动的语义最小编辑，从而使幻觉描述向非幻觉描述的转换在黑箱方式下进行。HalCECE，我们提出的一种幻觉检测框架，具有高可解释性，不仅提供语义相关的编辑，还通过分级分解幻觉概念，实现了全面的幻觉分析。本文的另一项 novelty 是对角色幻觉的调查，这可能是首次将视觉概念之间的相互作用纳入幻觉检测的研究之一。总体而言，HalCECE 为视觉-语言幻觉检测这一关键领域提供了一个可解释的方向，从而促进当前和未来视觉-语言系统的可信评估。', 'title_zh': 'HalCECE：一种基于概念反事实语句的可解释幻觉检测框架在图像标题生成中的应用'}
{'arxiv_id': 'arXiv:2503.00433', 'title': "Unveiling AI's Threats to Child Protection: Regulatory efforts to Criminalize AI-Generated CSAM and Emerging Children's Rights Violations", 'authors': 'Emmanouela Kokolaki, Paraskevi Fragopoulou', 'link': 'https://arxiv.org/abs/2503.00433', 'abstract': "This paper aims to present new alarming trends in the field of child sexual abuse through imagery, as part of SafeLine's research activities in the field of cybercrime, child sexual abuse material and the protection of children's rights to safe online experiences. It focuses primarily on the phenomenon of AI-generated CSAM, sophisticated ways employed for its production which are discussed in dark web forums and the crucial role that the open-source AI models play in the evolution of this overwhelming phenomenon. The paper's main contribution is a correlation analysis between the hotline's reports and domain names identified in dark web forums, where users' discussions focus on exchanging information specifically related to the generation of AI-CSAM. The objective was to reveal the close connection of clear net and dark web content, which was accomplished through the use of the ATLAS dataset of the Voyager system. Furthermore, through the analysis of a set of posts' content drilled from the above dataset, valuable conclusions on forum members' techniques employed for the production of AI-generated CSAM are also drawn, while users' views on this type of content and routes followed in order to overcome technological barriers set with the aim of preventing malicious purposes are also presented. As the ultimate contribution of this research, an overview of the current legislative developments in all country members of the INHOPE organization and the issues arising in the process of regulating the AI- CSAM is presented, shedding light in the legal challenges regarding the regulation and limitation of the phenomenon.", 'abstract_zh': '本文旨在通过视觉图像呈现儿童性虐待领域的新令人震惊的趋势，作为SafeLine在网络安全犯罪、儿童性虐待材料和保护儿童安全在线体验权利研究活动的一部分。主要探讨了AI生成的儿童性虐待材料（AI-CSAM）的现象、其生产中采用的复杂方法以及在暗网论坛讨论的开放源代码AI模型的关键作用。论文的主要贡献是对热线报告和暗网论坛中识别的域名进行相关性分析，其中用户讨论集中在交换与AI-CSAM生成相关的信息。研究通过使用Voyager系统的ATLAS数据集揭示了明网和暗网内容之间的密切联系，并通过对数据集一系列帖子内容的分析，得出论坛成员生产AI生成CSAM所采用的技术手段结论，同时展示了用户对这类内容的看法以及克服技术障碍、防止恶意用途的途径。该研究最终贡献包括概述INHOPE组织所有成员国当前关于AI-CSAM的立法发展情况及其在监管过程中的问题，揭示了对该现象进行监管和限制所面临的法律挑战。', 'title_zh': '揭示AI对儿童保护的威胁：监管努力以刑事手段打击AI生成的CSAM及新兴的儿童权利侵犯行为'}
{'arxiv_id': 'arXiv:2503.00426', 'title': 'Auto-encoding Molecules: Graph-Matching Capabilities Matter', 'authors': 'Magnus Cunow, Gerrit Großmann', 'link': 'https://arxiv.org/abs/2503.00426', 'abstract': 'Autoencoders are effective deep learning models that can function as generative models and learn latent representations for downstream tasks. The use of graph autoencoders - with both encoder and decoder implemented as message passing networks - is intriguing due to their ability to generate permutation-invariant graph representations. However, this approach faces difficulties because decoding a graph structure from a single vector is challenging, and comparing input and output graphs requires an effective permutation-invariant similarity measure. As a result, many studies rely on approximate methods.\nIn this work, we explore the effect of graph matching precision on the training behavior and generation capabilities of a Variational Autoencoder (VAE). Our contribution is two-fold: (1) we propose a transformer-based message passing graph decoder as an alternative to a graph neural network decoder, that is more robust and expressive by leveraging global attention mechanisms. (2) We show that the precision of graph matching has significant impact on training behavior and is essential for effective de novo (molecular) graph generation.\nCode is available at this https URL', 'abstract_zh': '基于图配准精度的变分自动编码器训练行为及生成能力研究：提出基于变换器的图传递解码器并探讨图配准精度的影响', 'title_zh': '自动编码分子：图匹配能力很重要'}
{'arxiv_id': 'arXiv:2503.00420', 'title': 'A physics-informed Bayesian optimization method for rapid development of electrical machines', 'authors': 'Pedram Asef, Christopher Vagg', 'link': 'https://arxiv.org/abs/2503.00420', 'abstract': 'Advanced slot and winding designs are imperative to create future high performance electrical machines (EM). As a result, the development of methods to design and improve slot filling factor (SFF) has attracted considerable research. Recent developments in manufacturing processes, such as additive manufacturing and alternative materials, has also highlighted a need for novel high-fidelity design techniques to develop high performance complex geometries and topologies. This study therefore introduces a novel physics-informed machine learning (PIML) design optimization process for improving SFF in traction electrical machines used in electric vehicles. A maximum entropy sampling algorithm (MESA) is used to seed a physics-informed Bayesian optimization (PIBO) algorithm, where the target function and its approximations are produced by Gaussian processes (GP)s. The proposed PIBO-MESA is coupled with a 2D finite element model (FEM) to perform a GP-based surrogate and provide the first demonstration of the optimal combination of complex design variables for an electrical machine. Significant computational gains were achieved using the new PIBO-MESA approach, which is 45% faster than existing stochastic methods, such as the non-dominated sorting genetic algorithm II (NSGA-II). The FEM results confirm that the new design optimization process and keystone shaped wires lead to a higher SFF (i.e. by 20%) and electromagnetic improvements (e.g. maximum torque by 12%) with similar resistivity. The newly developed PIBO-MESA design optimization process therefore presents significant benefits in the design of high-performance electric machines, with reduced development time and costs.', 'abstract_zh': '先进的槽型和绕组设计对于创建未来的高性能电气机器至关重要。因此，设计和提高槽填充因数（SFF）的方法的研究引起了广泛关注。制造工艺的 recent 发展，如增材制造和替代材料，也强调了需要新型高保真设计技术来开发高性能复杂几何形状和拓扑结构。因此，本研究提出了一种新型物理信息机器学习（PIML）设计优化过程，以提高用于电动汽车牵引电气机器的 SFF。最大熵采样算法（MESA）用于初始化物理信息贝叶斯优化（PIBO）算法，其中目标函数及其近似值由高斯过程（GP）生成。提出的 PIBO-MESA 与二维有限元模型（FEM）耦合，以执行基于高斯过程的元模型，并首次展示了电气机器复杂设计变量的最佳组合。与现有的非支配排序遗传算法 II（NSGA-II）等随机方法相比，新提出的 PIBO-MESA 方法在计算上快45%。有限元结果证实，新的设计优化过程和关键形状的导线可提高 SFF（即提高20%）和电磁性能（例如最大扭矩提高12%）而电阻率相似。因此，新开发的 PIBO-MESA 设计优化过程在高性能电气机器的设计中具有显著优势，减少了开发时间和成本。', 'title_zh': '基于物理信息的贝叶斯优化方法用于电气机器的快速开发'}
{'arxiv_id': 'arXiv:2503.00393', 'title': 'Reservoir Network with Structural Plasticity for Human Activity Recognition', 'authors': 'Abdullah M. Zyarah, Alaa M. Abdul-Hadi, Dhireesha Kudithipudi', 'link': 'https://arxiv.org/abs/2503.00393', 'abstract': 'The unprecedented dissemination of edge devices is accompanied by a growing demand for neuromorphic chips that can process time-series data natively without cloud support. Echo state network (ESN) is a class of recurrent neural networks that can be used to identify unique patterns in time-series data and predict future events. It is known for minimal computing resource requirements and fast training, owing to the use of linear optimization solely at the readout stage. In this work, a custom-design neuromorphic chip based on ESN targeting edge devices is proposed. The proposed system supports various learning mechanisms, including structural plasticity and synaptic plasticity, locally on-chip. This provides the network with an additional degree of freedom to continuously learn, adapt, and alter its structure and sparsity level, ensuring high performance and continuous stability. We demonstrate the performance of the proposed system as well as its robustness to noise against real-world time-series datasets while considering various topologies of data movement. An average accuracy of 95.95% and 85.24% are achieved on human activity recognition and prosthetic finger control, respectively. We also illustrate that the proposed system offers a throughput of 6x10^4 samples/sec with a power consumption of 47.7mW on a 65nm IBM process.', 'abstract_zh': '基于ESN的边缘设备定制神经形态芯片及其在时间序列数据处理中的性能与鲁棒性研究', 'title_zh': '具有结构可塑性的蓄水池网络的人类活动识别'}
{'arxiv_id': 'arXiv:2503.00389', 'title': 'BGM2Pose: Active 3D Human Pose Estimation with Non-Stationary Sounds', 'authors': 'Yuto Shibata, Yusuke Oumi, Go Irie, Akisato Kimura, Yoshimitsu Aoki, Mariko Isogawa', 'link': 'https://arxiv.org/abs/2503.00389', 'abstract': 'We propose BGM2Pose, a non-invasive 3D human pose estimation method using arbitrary music (e.g., background music) as active sensing signals. Unlike existing approaches that significantly limit practicality by employing intrusive chirp signals within the audible range, our method utilizes natural music that causes minimal discomfort to humans. Estimating human poses from standard music presents significant challenges. In contrast to sound sources specifically designed for measurement, regular music varies in both volume and pitch. These dynamic changes in signals caused by music are inevitably mixed with alterations in the sound field resulting from human motion, making it hard to extract reliable cues for pose estimation. To address these challenges, BGM2Pose introduces a Contrastive Pose Extraction Module that employs contrastive learning and hard negative sampling to eliminate musical components from the recorded data, isolating the pose information. Additionally, we propose a Frequency-wise Attention Module that enables the model to focus on subtle acoustic variations attributable to human movement by dynamically computing attention across frequency bands. Experiments suggest that our method outperforms the existing methods, demonstrating substantial potential for real-world applications. Our datasets and code will be made publicly available.', 'abstract_zh': 'BGM2Pose：使用任意音乐作为主动感知信号的无侵入性3D人体姿态估计方法', 'title_zh': 'BGM2Pose: 基于非stationary声音的主动3D人体姿态估计'}
{'arxiv_id': 'arXiv:2503.00387', 'title': 'LNUCB-TA: Linear-nonlinear Hybrid Bandit Learning with Temporal Attention', 'authors': 'Hamed Khosravi, Mohammad Reza Shafie, Ahmed Shoyeb Raihan, Srinjoy Das, Imtiaz Ahmed', 'link': 'https://arxiv.org/abs/2503.00387', 'abstract': 'Existing contextual multi-armed bandit (MAB) algorithms fail to effectively capture both long-term trends and local patterns across all arms, leading to suboptimal performance in environments with rapidly changing reward structures. They also rely on static exploration rates, which do not dynamically adjust to changing conditions. To overcome these limitations, we propose LNUCB-TA, a hybrid bandit model integrating a novel nonlinear component (adaptive k-Nearest Neighbors (k-NN)) for reducing time complexity, alongside a global-and-local attention-based exploration mechanism. Our approach uniquely combines linear and nonlinear estimation techniques, with the nonlinear module dynamically adjusting k based on reward variance to enhance spatiotemporal pattern recognition. This reduces the likelihood of selecting suboptimal arms while improving reward estimation accuracy and computational efficiency. The attention-based mechanism ranks arms by past performance and selection frequency, dynamically adjusting exploration and exploitation in real time without requiring manual tuning of exploration rates. By integrating global attention (assessing all arms collectively) and local attention (focusing on individual arms), LNUCB-TA efficiently adapts to temporal and spatial complexities. Empirical results show LNUCB-TA significantly outperforms state-of-the-art linear, nonlinear, and hybrid bandits in cumulative and mean reward, convergence, and robustness across different exploration rates. Theoretical analysis further confirms its reliability with a sub-linear regret bound.', 'abstract_zh': 'LNUCB-TA：结合非线性组件的全局-局部注意力多方探索模型', 'title_zh': 'LNUCB-TA：具有时间注意力的线性-非线性混合_bandit学习'}
{'arxiv_id': 'arXiv:2503.00383', 'title': 'Theoretical Insights in Model Inversion Robustness and Conditional Entropy Maximization for Collaborative Inference Systems', 'authors': 'Song Xia, Yi Yu, Wenhan Yang, Meiwen Ding, Zhuo Chen, Lingyu Duan, Alex C. Kot, Xudong Jiang', 'link': 'https://arxiv.org/abs/2503.00383', 'abstract': 'By locally encoding raw data into intermediate features, collaborative inference enables end users to leverage powerful deep learning models without exposure of sensitive raw data to cloud servers. However, recent studies have revealed that these intermediate features may not sufficiently preserve privacy, as information can be leaked and raw data can be reconstructed via model inversion attacks (MIAs). Obfuscation-based methods, such as noise corruption, adversarial representation learning, and information filters, enhance the inversion robustness by obfuscating the task-irrelevant redundancy empirically. However, methods for quantifying such redundancy remain elusive, and the explicit mathematical relation between this redundancy minimization and inversion robustness enhancement has not yet been established. To address that, this work first theoretically proves that the conditional entropy of inputs given intermediate features provides a guaranteed lower bound on the reconstruction mean square error (MSE) under any MIA. Then, we derive a differentiable and solvable measure for bounding this conditional entropy based on the Gaussian mixture estimation and propose a conditional entropy maximization (CEM) algorithm to enhance the inversion robustness. Experimental results on four datasets demonstrate the effectiveness and adaptability of our proposed CEM; without compromising feature utility and computing efficiency, plugging the proposed CEM into obfuscation-based defense mechanisms consistently boosts their inversion robustness, achieving average gains ranging from 12.9\\% to 48.2\\%. Code is available at \\href{this https URL}{this https URL}.', 'abstract_zh': '通过局部编码原始数据为中间特征，协作推理使终端用户能够利用强大的深度学习模型，同时避免将敏感原始数据暴露给云服务器。然而，近期研究发现，这些中间特征可能无法充分保护隐私，因为信息可以通过模型反转攻击（MIA）被泄露并重建原始数据。基于模糊化的方法，如噪声污染、对抗性表示学习和信息过滤，通过经验性地模糊化与任务无关的冗余来增强反转鲁棒性。然而，量化这种冗余的方法仍然难以捉摸，这一冗余最小化与反转鲁棒性增强之间的显式数学关系仍未建立。为此，本文首先理论上证明输入给定中间特征的条件熵提供了任何MIA下重建均方误差（MSE）的有保证的下界。然后，我们基于高斯混合估计推导出一种可微和可求解的边界度量，并提出条件熵最大化（CEM）算法来增强反转鲁棒性。在四个数据集上的实验结果证明了我们提出的CEM的有效性和适应性；在不牺牲特征效用和计算效率的情况下，将提出的CEM嵌入到基于模糊化的防御机制中，可以一致地提高其反转鲁棒性，平均增幅从12.9%到48.2%不等。代码可在https://this-url-available-only-for-code-sharing.com/ 获取。', 'title_zh': '模型反转稳健性与条件熵最大化在协作推理系统中的理论洞察'}
{'arxiv_id': 'arXiv:2503.00378', 'title': 'Conditioning on Local Statistics for Scalable Heterogeneous Federated Learning', 'authors': 'Rickard Brännvall', 'link': 'https://arxiv.org/abs/2503.00378', 'abstract': "Federated learning is a distributed machine learning approach where multiple clients collaboratively train a model without sharing their local data, which contributes to preserving privacy. A challenge in federated learning is managing heterogeneous data distributions across clients, which can hinder model convergence and performance due to the need for the global model to generalize well across diverse local datasets. We propose to use local characteristic statistics, by which we mean some statistical properties calculated independently by each client using only their local training dataset. These statistics, such as means, covariances, and higher moments, are used to capture the characteristics of the local data distribution. They are not shared with other clients or a central node. During training, these local statistics help the model learn how to condition on the local data distribution, and during inference, they guide the client's predictions. Our experiments show that this approach allows for efficient handling of heterogeneous data across the federation, has favorable scaling compared to approaches that directly try to identify peer nodes that share distribution characteristics, and maintains privacy as no additional information needs to be communicated.", 'abstract_zh': '联邦学习是一种分布式机器学习方法，其中多个客户端协作训练模型而不共享其本地数据，从而有助于保护隐私。联邦学习中的一个挑战是管理客户端之间的异构数据分布，这可能导致全球模型在多样化的本地数据集上难以收敛和表现出色。我们提出了使用局部特征统计量的方法，即每个客户端仅使用其本地训练数据集独立计算的一些统计性质。这些统计量如均值、协方差和高阶矩用于捕捉局部数据分布的特征。它们不会与其他客户端或中心节点共享。在训练过程中，这些局部统计量帮助模型学习如何根据局部数据分布进行调整，而在推理过程中，它们指导客户端的预测。我们的实验表明，这种方法能够有效处理联邦中的异构数据，与直接尝试识别具有相似分布特性的对等节点的方法相比具有更 favorable的扩展性，并且能够维护隐私，因为不需要额外的信息交流。', 'title_zh': '基于局部统计条件化的大规模异构联邦学习'}
{'arxiv_id': 'arXiv:2503.00372', 'title': 'Nucleolus Credit Assignment for Effective Coalitions in Multi-agent Reinforcement Learning', 'authors': 'Yugu Li, Zehong Cao, Jianglin Qiao, Siyi Hu', 'link': 'https://arxiv.org/abs/2503.00372', 'abstract': 'In cooperative multi-agent reinforcement learning (MARL), agents typically form a single grand coalition based on credit assignment to tackle a composite task, often resulting in suboptimal performance. This paper proposed a nucleolus-based credit assignment grounded in cooperative game theory, enabling the autonomous partitioning of agents into multiple small coalitions that can effectively identify and complete subtasks within a larger composite task. Specifically, our designed nucleolus Q-learning could assign fair credits to each agent, and the nucleolus Q-operator provides theoretical guarantees with interpretability for both learning convergence and the stability of the formed small coalitions. Through experiments on Predator-Prey and StarCraft scenarios across varying difficulty levels, our approach demonstrated the emergence of multiple effective coalitions during MARL training, leading to faster learning and superior performance in terms of win rate and cumulative rewards especially in hard and super-hard environments, compared to four baseline methods. Our nucleolus-based credit assignment showed the promise for complex composite tasks requiring effective subteams of agents.', 'abstract_zh': '基于核子博弈的信用分配在合作多智能体强化学习中的应用', 'title_zh': '核质信用分配在多智能体强化学习中有效联盟的构建'}
{'arxiv_id': 'arXiv:2503.00358', 'title': 'CRUPL: A Semi-Supervised Cyber Attack Detection with Consistency Regularization and Uncertainty-aware Pseudo-Labeling in Smart Grid', 'authors': 'Smruti P. Dash, Kedar V. Khandeparkar, Nipun Agrawal', 'link': 'https://arxiv.org/abs/2503.00358', 'abstract': 'The modern power grids are integrated with digital technologies and automation systems. The inclusion of digital technologies has made the smart grids vulnerable to cyber-attacks. Cyberattacks on smart grids can compromise data integrity and jeopardize the reliability of the power supply. Traditional intrusion detection systems often need help to effectively detect novel and sophisticated attacks due to their reliance on labeled training data, which may only encompass part of the spectrum of potential threats. This work proposes a semi-supervised method for cyber-attack detection in smart grids by leveraging the labeled and unlabeled measurement data. We implement consistency regularization and pseudo-labeling to identify deviations from expected behavior and predict the attack classes. We use a curriculum learning approach to improve pseudo-labeling performance, capturing the model uncertainty. We demonstrate the efficiency of the proposed method in detecting different types of cyberattacks, minimizing the false positives by implementing them on publicly available datasets. The method proposes a promising solution by improving the detection accuracy to 99% in the presence of unknown samples and significantly reducing false positives.', 'abstract_zh': '现代电力网格集成了数字技术和自动化系统。数字技术的纳入使智能电网容易受到网络攻击。对智能电网的网络攻击可能损害数据 integrity 并危及电力供应的可靠性。传统的入侵检测系统由于依赖标记的训练数据，常常需要帮助来有效检测新型和复杂的攻击，而这些标记的训练数据可能只能涵盖潜在威胁的一小部分。本研究提出了一种半监督方法，通过利用标记和未标记的测量数据来检测智能电网的网络攻击。我们实现了一致性正则化和伪标签标注，以识别预期行为的偏差并预测攻击类别。我们采用序列学习方法来提高伪标签标注的性能，捕捉模型的不确定性。我们在公开可用的数据集上实施该方法，证明了所提方法在检测不同类型的网络攻击方面的效率，并通过降低假正例，将检测准确性提高到99%，特别是在面对未知样本时。该方法提出了一种有前景的解决方案，显著减少了假正例。', 'title_zh': 'CRUPL：一种基于一致性正则化和不确定性感知伪标签的半监督智能电网网络攻击检测方法'}
{'arxiv_id': 'arXiv:2503.00334', 'title': 'MCNet: Monotonic Calibration Networks for Expressive Uncertainty Calibration in Online Advertising', 'authors': 'Quanyu Dai, Jiaren Xiao, Zhaocheng Du, Jieming Zhu, Chengxiao Luo, Xiao-Ming Wu, Zhenhua Dong', 'link': 'https://arxiv.org/abs/2503.00334', 'abstract': "In online advertising, uncertainty calibration aims to adjust a ranking model's probability predictions to better approximate the true likelihood of an event, e.g., a click or a conversion. However, existing calibration approaches may lack the ability to effectively model complex nonlinear relations, consider context features, and achieve balanced performance across different data subsets. To tackle these challenges, we introduce a novel model called Monotonic Calibration Networks, featuring three key designs: a monotonic calibration function (MCF), an order-preserving regularizer, and a field-balance regularizer. The nonlinear MCF is capable of naturally modeling and universally approximating the intricate relations between uncalibrated predictions and the posterior probabilities, thus being much more expressive than existing methods. MCF can also integrate context features using a flexible model architecture, thereby achieving context awareness. The order-preserving and field-balance regularizers promote the monotonic relationship between adjacent bins and the balanced calibration performance on data subsets, respectively. Experimental results on both public and industrial datasets demonstrate the superior performance of our method in generating well-calibrated probability predictions.", 'abstract_zh': '在线广告中的不确定性校准旨在调整排名模型的概率预测，使其更好地逼近事件（如点击或转化）的真实概率。然而，现有的校准方法可能无法有效地建模复杂的非线性关系、考虑上下文特征，并在不同数据子集上实现均衡性能。为应对这些挑战，我们提出了一种新颖的模型——单调校准网络，该模型包含三个关键设计：单调校准函数（MCF）、有序保持正则化器和领域均衡正则化器。非线性MCF能够自然地建模和普遍逼近未校准预测与后验概率之间的复杂关系，因此比现有方法更具表现力。MCF还能够通过灵活的模型架构整合上下文特征，从而实现上下文awareness。有序保持和领域均衡正则化器分别促进相邻区间之间的单调关系和不同数据子集上的均衡校准性能。在公共和工业数据集上的实验结果表明，我们的方法在生成准确的概率预测方面具有优越性能。', 'title_zh': 'MCNet：单调校准网络在在线广告中表达不确定性校准的研究'}
{'arxiv_id': 'arXiv:2503.00332', 'title': 'Investigating the contribution of terrain-following coordinates and conservation schemes in AI-driven precipitation forecasts', 'authors': 'Yingkai Sha, John S. Schreck, William Chapman, David John Gagne II', 'link': 'https://arxiv.org/abs/2503.00332', 'abstract': "Artificial Intelligence (AI) weather prediction (AIWP) models often produce ``blurry'' precipitation forecasts that overestimate drizzle and underestimate extremes. This study provides a novel solution to tackle this problem -- integrating terrain-following coordinates with global mass and energy conservation schemes into AIWP models. Forecast experiments are conducted to evaluate the effectiveness of this solution using FuXi, an example AIWP model, adapted to 1.0$^\\circ$ grid spacing data. Verification results show large performance gains. The conservation schemes are found to reduce drizzle bias, whereas using terrain-following coordinates improves the estimation of extreme events and precipitation intensity spectra. Furthermore, a case study reveals that terrain-following coordinates capture near-surface winds better over mountains, offering AIWP models more accurate information on understanding the dynamics of precipitation processes. The proposed solution of this study can benefit a wide range of AIWP models and bring insights into how atmospheric domain knowledge can support the development of AIWP models.", 'abstract_zh': '人工智能天气预测模型中地形跟随坐标与全球质量和能量守恒方案的集成：一种改善降水预报模糊性的新方法', 'title_zh': '基于地形跟随坐标和保守方案在AI驱动降水预报中的贡献研究'}
{'arxiv_id': 'arXiv:2503.00331', 'title': 'PINN-DT: Optimizing Energy Consumption in Smart Building Using Hybrid Physics-Informed Neural Networks and Digital Twin Framework with Blockchain Security', 'authors': 'Hajar Kazemi Naeini, Roya Shomali, Abolhassan Pishahang, Hamidreza Hasanzadeh, Mahdieh Mohammadi, Saeid Asadi, Ahmad Gholizadeh Lonbar', 'link': 'https://arxiv.org/abs/2503.00331', 'abstract': "The advancement of smart grid technologies necessitates the integration of cutting-edge computational methods to enhance predictive energy optimization. This study proposes a multi-faceted approach by incorporating (1) Deep Reinforcement Learning (DRL) agents trained using data from Digital Twins (DTs) to optimize energy consumption in real time, (2) Physics-Informed Neural Networks (PINNs) to seamlessly embed physical laws within the optimization process, ensuring model accuracy and interpretability, and (3) Blockchain (BC) technology to facilitate secure and transparent communication across the smart grid infrastructure. The model was trained and validated using comprehensive datasets, including smart meter energy consumption data, renewable energy outputs, dynamic pricing, and user preferences collected from IoT devices. The proposed framework achieved superior predictive performance with a Mean Absolute Error (MAE) of 0.237 kWh, Root Mean Square Error (RMSE) of 0.298 kWh, and an R-squared (R2) value of 0.978, indicating a 97.8% explanation of data variance. Classification metrics further demonstrated the model's robustness, achieving 97.7% accuracy, 97.8% precision, 97.6% recall, and an F1 Score of 97.7%. Comparative analysis with traditional models like Linear Regression, Random Forest, SVM, LSTM, and XGBoost revealed the superior accuracy and real-time adaptability of the proposed method. In addition to enhancing energy efficiency, the model reduced energy costs by 35%, maintained a 96% user comfort index, and increased renewable energy utilization to 40%. This study demonstrates the transformative potential of integrating PINNs, DT, and Blockchain technologies to optimize energy consumption in smart grids, paving the way for sustainable, secure, and efficient energy management systems.", 'abstract_zh': '智能电网技术的进步 necessitates the integration of cutting-edge computational methods to enhance predictive energy optimization.本研究提出了一种多方面的方法，通过结合使用（1）基于数字孪生（DTs）数据训练的深度强化学习（DRL）代理以实现实时能源消耗优化，（2）物理信息神经网络（PINNs）以无缝嵌入物理定律，确保模型准确性和可解释性，以及（3）区块链（BC）技术以促进智能电网基础设施的安全和透明通信。该模型使用包括智能电表能源消耗数据、可再生能源输出、动态定价和从物联网设备收集的用户偏好在内的全面数据集进行训练和验证。所提出的框架取得了优越的预测性能，平均绝对误差（MAE）为0.237 kWh，均方根误差（RMSE）为0.298 kWh，R²值为0.978，表明其解释了97.8%的数据变异。分类指标进一步证明了该模型的稳健性，准确率达到了97.7%，精确率为97.8%，召回率为97.6%，F1分数为97.7%。与传统的线性回归、随机森林、SVM、LSTM和XGBoost等模型的比较分析表明，所提出的方法具有更高的准确性和实时适应性。除了提高能源效率，该模型还将能源成本降低了35%，保持了96%的用户舒适度指数，并将可再生能源利用率提高到了40%。本研究展示了结合使用PINNs、DT和区块链技术以优化智能电网能源消耗的变革潜力，为可持续、安全和高效的能源管理系统铺平了道路。\n\n标题：\nIntegrating Deep Reinforcement Learning, Physics-Informed Neural Networks, and Blockchain for Predictive Energy Optimization in Smart Grids', 'title_zh': 'PINN-DT: 基于混合物理感知神经网络和数字孪生框架的智能建筑能耗优化及区块链安全研究'}
{'arxiv_id': 'arXiv:2503.00323', 'title': 'FLStore: Efficient Federated Learning Storage for non-training workloads', 'authors': 'Ahmad Faraz Khan, Samuel Fountain, Ahmed M. Abdelmoniem, Ali R. Butt, Ali Anwar', 'link': 'https://arxiv.org/abs/2503.00323', 'abstract': 'Federated Learning (FL) is an approach for privacy-preserving Machine Learning (ML), enabling model training across multiple clients without centralized data collection. With an aggregator server coordinating training, aggregating model updates, and storing metadata across rounds. In addition to training, a substantial part of FL systems are the non-training workloads such as scheduling, personalization, clustering, debugging, and incentivization. Most existing systems rely on the aggregator to handle non-training workloads and use cloud services for data storage. This results in high latency and increased costs as non-training workloads rely on large volumes of metadata, including weight parameters from client updates, hyperparameters, and aggregated updates across rounds, making the situation even worse. We propose FLStore, a serverless framework for efficient FL non-training workloads and storage. FLStore unifies the data and compute planes on a serverless cache, enabling locality-aware execution via tailored caching policies to reduce latency and costs. Per our evaluations, compared to cloud object store based aggregator server FLStore reduces per request average latency by 71% and costs by 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to an in-memory cloud cache based aggregator server, FLStore reduces average latency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and 99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks with minimal modifications, while also being fault-tolerant and highly scalable.', 'abstract_zh': 'FLStore：面向高效联邦学习非训练工作负载及存储的无服务器框架', 'title_zh': 'FLStore: 非训练工作负载的高效联邦学习存储'}
{'arxiv_id': 'arXiv:2503.00322', 'title': 'T-REX: A 68-567 μs/token, 0.41-3.95 μJ/token Transformer Accelerator with Reduced External Memory Access and Enhanced Hardware Utilization in 16nm FinFET', 'authors': 'Seunghyun Moon, Mao Li, Gregory Chen, Phil Knag, Ram Krishnamurthy, Mingoo Seok', 'link': 'https://arxiv.org/abs/2503.00322', 'abstract': 'This work introduces novel training and post-training compression schemes to reduce external memory access during transformer model inference. Additionally, a new control flow mechanism, called dynamic batching, and a novel buffer architecture, termed a two-direction accessible register file, further reduce external memory access while improving hardware utilization.', 'abstract_zh': '本工作介绍了新型训练时和后训练压缩方案，以减少变压器模型推断过程中的外部内存访问。此外，引入了一种新的控制流机制，称为动态批量处理，并提出了一种新的缓冲区架构，称为双向可访问寄存器文件，进一步减少了外部内存访问并提高了硬件利用率。', 'title_zh': 'T-REX：一种具有减少外部内存访问和增强硬件利用率的16纳米FinFET工艺变压器加速器，每token耗时68.567 μs，每token能耗0.41-3.95 μJ'}
{'arxiv_id': 'arXiv:2503.00299', 'title': 'Hidden Convexity of Fair PCA and Fast Solver via Eigenvalue Optimization', 'authors': 'Junhui Shen, Aaron J. Davis, Ding Lu, Zhaojun Bai', 'link': 'https://arxiv.org/abs/2503.00299', 'abstract': 'Principal Component Analysis (PCA) is a foundational technique in machine learning for dimensionality reduction of high-dimensional datasets. However, PCA could lead to biased outcomes that disadvantage certain subgroups of the underlying datasets. To address the bias issue, a Fair PCA (FPCA) model was introduced by Samadi et al. (2018) for equalizing the reconstruction loss between subgroups. The semidefinite relaxation (SDR) based approach proposed by Samadi et al. (2018) is computationally expensive even for suboptimal solutions. To improve efficiency, several alternative variants of the FPCA model have been developed. These variants often shift the focus away from equalizing the reconstruction loss. In this paper, we identify a hidden convexity in the FPCA model and introduce an algorithm for convex optimization via eigenvalue optimization. Our approach achieves the desired fairness in reconstruction loss without sacrificing performance. As demonstrated in real-world datasets, the proposed FPCA algorithm runs $8\\times$ faster than the SDR-based algorithm, and only at most 85% slower than the standard PCA.', 'abstract_zh': 'Fair PCA模型中的隐含凸性及基于特征值优化的凸优化算法：提高效率的同时保持公平性和性能', 'title_zh': '隐藏凸性公平PCA及其通过特征值优化的快速求解方法'}
{'arxiv_id': 'arXiv:2503.00286', 'title': 'A Unified Framework for Heterogeneous Semi-supervised Learning', 'authors': 'Marzi Heidari, Abdullah Alchihabi, Hao Yan, Yuhong Guo', 'link': 'https://arxiv.org/abs/2503.00286', 'abstract': 'In this work, we introduce a novel problem setup termed as Heterogeneous Semi-Supervised Learning (HSSL), which presents unique challenges by bridging the semi-supervised learning (SSL) task and the unsupervised domain adaptation (UDA) task, and expanding standard semi-supervised learning to cope with heterogeneous training data. At its core, HSSL aims to learn a prediction model using a combination of labeled and unlabeled training data drawn separately from heterogeneous domains that share a common set of semantic categories; this model is intended to differentiate the semantic categories of test instances sampled from both the labeled and unlabeled domains. In particular, the labeled and unlabeled domains have dissimilar label distributions and class feature distributions. This heterogeneity, coupled with the assorted sources of the test data, introduces significant challenges to standard SSL and UDA methods. Therefore, we propose a novel method, Unified Framework for Heterogeneous Semi-supervised Learning (Uni-HSSL), to address HSSL by directly learning a fine-grained classifier from the heterogeneous data, which adaptively handles the inter-domain heterogeneity while leveraging both the unlabeled data and the inter-domain semantic class relationships for cross-domain knowledge transfer and adaptation. We conduct comprehensive experiments and the experimental results validate the efficacy and superior performance of the proposed Uni-HSSL over state-of-the-art semi-supervised learning and unsupervised domain adaptation methods.', 'abstract_zh': '异构半监督学习问题设置（Heterogeneous Semi-Supervised Learning, HSSL）：统一框架方法（Uni-HSSL）', 'title_zh': '统一的异质半监督学习框架'}
{'arxiv_id': 'arXiv:2503.00268', 'title': 'Input Specific Neural Networks', 'authors': 'Asghar A. Jadoon, D. Thomas Seidl, Reese E. Jones, Jan N. Fuhg', 'link': 'https://arxiv.org/abs/2503.00268', 'abstract': "The black-box nature of neural networks limits the ability to encode or impose specific structural relationships between inputs and outputs. While various studies have introduced architectures that ensure the network's output adheres to a particular form in relation to certain inputs, the majority of these approaches impose constraints on only a single set of inputs. This paper introduces a novel neural network architecture, termed the Input Specific Neural Network (ISNN), which extends this concept by allowing scalar-valued outputs to be subject to multiple constraints. Specifically, the ISNN can enforce convexity in some inputs, non-decreasing monotonicity combined with convexity with respect to others, and simple non-decreasing monotonicity or arbitrary relationships with additional inputs. The paper presents two distinct ISNN architectures, along with equations for the first and second derivatives of the output with respect to the inputs. These networks are broadly applicable.\nIn this work, we restrict their usage to solving problems in computational mechanics. In particular, we show how they can be effectively applied to fitting data-driven constitutive models. We then embed our trained data-driven constitutive laws into a finite element solver where significant time savings can be achieved by using explicit manual differentiation using the derived equations as opposed to automatic differentiation. We also show how ISNNs can be used to learn structural relationships between inputs and outputs via a binary gating mechanism. Particularly, ISNNs are employed to model an anisotropic free energy potential to get the homogenized macroscopic response in a decoupled multiscale setting, where the network learns whether or not the potential should be modeled as polyconvex, and retains only the relevant layers while using the minimum number of inputs.", 'abstract_zh': '具有特定输入约束的神经网络架构（ISNN）及其在计算力学中的应用', 'title_zh': '特定输入神经网络'}
{'arxiv_id': 'arXiv:2503.00240', 'title': '1-Lipschitz Network Initialization for Certifiably Robust Classification Applications: A Decay Problem', 'authors': 'Marius F. R. Juston, William R. Norris, Dustin Nottage, Ahmet Soylemezoglu', 'link': 'https://arxiv.org/abs/2503.00240', 'abstract': 'This paper discusses the weight parametrization of two standard 1-Lipschitz network structure methodologies, the Almost-Orthogonal-Layers (AOL) and the SDP-based Lipschitz Layers (SLL), and derives their impact on the initialization for deep 1-Lipschitz feedforward networks in addition to discussing underlying issues surrounding this initialization. These networks are mainly used in certifiably robust classification applications to combat adversarial attacks by limiting the effects of perturbations on the output classification result. An exact and an upper bound for the parameterized weight variance was calculated assuming a standard Normal distribution initialization; additionally, an upper bound was computed assuming a Generalized Normal Distribution, generalizing the proof for Uniform, Laplace, and Normal distribution weight initializations. It is demonstrated that the weight variance holds no bearing on the output variance distribution and that only the dimension of the weight matrices matters. Additionally, this paper demonstrates that the weight initialization always causes deep 1-Lipschitz networks to decay to zero.', 'abstract_zh': '本文讨论了两种标准的1-Lipschitz网络结构方法——Almost-Orthogonal-Layers (AOL) 和基于SDP的Lipschitz层 (SLL) 的权重参数化，并推导了它们对深层1-Lipschitz前馈网络初始化的影响，同时探讨了与此初始化相关的问题。这些网络主要用于通过限制扰动对输出分类结果的影响来抵抗对抗性攻击的应用场景，如可验证鲁棒分类。在假设标准正态分布初始化的情况下，计算了参数化权重方差的确切值和上界；此外，在假设广义正态分布的情况下，计算了上界，并将证明推广到均匀分布、拉普拉斯分布和正态分布权重初始化。本文证明了权重方差不影响输出方差分布，只有权重重度矩阵的维度才起作用。此外，本文还证明了权重初始化总是导致深层1-Lipschitz网络衰减为零。', 'title_zh': '1-Lipschitz 网络初始化对于可认证鲁棒分类应用：一个衰减问题'}
{'arxiv_id': 'arXiv:2503.00234', 'title': 'Towards Fairness for the Right Reasons: Using Saliency Maps to Evaluate Bias Removal in Neural Networks', 'authors': 'Lukasz Sztukiewicz, Ignacy Stępka, Michał Wiliński, Jerzy Stefanowski', 'link': 'https://arxiv.org/abs/2503.00234', 'abstract': "The widespread adoption of machine learning systems has raised critical concerns about fairness and bias, making mitigating harmful biases essential for AI development. In this paper, we investigate the relationship between fairness improvement and the removal of harmful biases in neural networks applied to computer vision tasks. First, we introduce a set of novel XAI-based metrics that analyze saliency maps to assess shifts in a model's decision-making process. Then, we demonstrate that successful debiasing methods systematically redirect model focus away from protected attributes. Additionally, we show that techniques originally developed for artifact removal can be effectively repurposed for fairness. These findings underscore the importance of ensuring that models are fair for the right reasons, contributing to the development of more ethical and trustworthy AI systems.", 'abstract_zh': '机器学习系统的广泛应用引发了对公平性和偏见的严重关切，减轻有害偏见对于人工智能发展至关重要。本文探讨了在计算机视觉任务中应用神经网络时，公平性改进与有害偏见消除之间的关系。首先，我们引入了一组基于可解释性人工智能的新型指标，通过分析显著性图来评估模型决策过程的变化。然后，我们证明了成功的去偏方法会系统性地将模型的关注点从受保护属性中转移开。此外，我们展示了原本用于去除伪影的方法可以有效重新应用于公平性。这些发现强调了确保模型因正确的理由而公平的重要性，促进了更加伦理和可信赖的人工智能系统的开发。', 'title_zh': '基于恰当原因的公平性：使用显著性图评估神经网络中的偏见去除'}
{'arxiv_id': 'arXiv:2503.00210', 'title': 'Foundation-Model-Boosted Multimodal Learning for fMRI-based Neuropathic Pain Drug Response Prediction', 'authors': 'Wenrui Fan, L. M. Riza Rizky, Jiayang Zhang, Chen Chen, Haiping Lu, Kevin Teh, Dinesh Selvarajah, Shuo Zhou', 'link': 'https://arxiv.org/abs/2503.00210', 'abstract': "Neuropathic pain, affecting up to 10% of adults, remains difficult to treat due to limited therapeutic efficacy and tolerability. Although resting-state functional MRI (rs-fMRI) is a promising non-invasive measurement of brain biomarkers to predict drug response in therapeutic development, the complexity of fMRI demands machine learning models with substantial capacity. However, extreme data scarcity in neuropathic pain research limits the application of high-capacity models. To address the challenge of data scarcity, we propose FMM$_{TC}$, a Foundation-Model-boosted Multimodal learning framework for fMRI-based neuropathic pain drug response prediction, which leverages both internal multimodal information in pain-specific data and external knowledge from large pain-agnostic data. Specifically, to maximize the value of limited pain-specific data, FMM$_{TC}$ integrates complementary information from two rs-fMRI modalities: Time series and functional Connectivity. FMM$_{TC}$ is further boosted by an fMRI foundation model with its external knowledge from extensive pain-agnostic fMRI datasets enriching limited pain-specific information. Evaluations with an in-house dataset and a public dataset from OpenNeuro demonstrate FMM$_{TC}$'s superior representation ability, generalizability, and cross-dataset adaptability over existing unimodal fMRI models that only consider one of the rs-fMRI modalities. The ablation study validates the effectiveness of multimodal learning and foundation-model-powered external knowledge transfer in FMM$_{TC}$. An integrated gradient-based interpretation study explains how FMM$_{TC}$'s cross-dataset dynamic behaviors enhance its adaptability. In conclusion, FMM$_{TC}$ boosts clinical trials in neuropathic pain therapeutic development by accurately predicting drug responses to improve the participant stratification efficiency.", 'abstract_zh': '基于fMRI的神经病理性疼痛药物反应预测的FMM$_{TC}$多重模态学习框架', 'title_zh': '基于基础模型增强的多模态学习在基于fMRI的神经病理性疼痛药物响应预测中的应用'}
{'arxiv_id': 'arXiv:2503.00154', 'title': 'Fed-KAN: Federated Learning with Kolmogorov-Arnold Networks for Traffic Prediction', 'authors': 'Engin Zeydan, Cristian J. Vaca-Rubio, Luis Blanco, Roberto Pereira, Marius Caus, Kapal Dev', 'link': 'https://arxiv.org/abs/2503.00154', 'abstract': 'Non-Terrestrial Networks (NTNs) are becoming a critical component of modern communication infrastructures, especially with the advent of Low Earth Orbit (LEO) satellite systems. Traditional centralized learning approaches face major challenges in such networks due to high latency, intermittent connectivity and limited bandwidth. Federated Learning (FL) is a promising alternative as it enables decentralized training while maintaining data privacy. However, existing FL models, such as Federated Learning with Multi-Layer Perceptrons (Fed-MLP), can struggle with high computational complexity and poor adaptability to dynamic NTN environments. This paper provides a detailed analysis for Federated Learning with Kolmogorov-Arnold Networks (Fed-KAN), its implementation and performance improvements over traditional FL models in NTN environments for traffic forecasting. The proposed Fed-KAN is a novel approach that utilises the functional approximation capabilities of KANs in a FL framework. We evaluate Fed-KAN compared to Fed-MLP on a traffic dataset of real satellite operator and show a significant reduction in training and test loss. Our results show that Fed-KAN can achieve a 77.39% reduction in average test loss compared to Fed-MLP, highlighting its improved performance and better generalization ability. At the end of the paper, we also discuss some potential applications of Fed-KAN within O-RAN and Fed-KAN usage for split functionalities in NTN architecture.', 'abstract_zh': '非地表网络中的联邦学习：基于柯尔莫戈罗夫-阿诺尔德网络的交通预测', 'title_zh': 'Fed-KAN：基于柯尔莫哥洛夫-阿诺尔德网络的联邦学习交通预测'}
{'arxiv_id': 'arXiv:2503.00144', 'title': 'Learner and Instructor Needs in AI-Supported Programming Learning Tools: Design Implications for Features and Adaptive Control', 'authors': 'Zihan Wu, Yicheng Tang, Barbara Ericson', 'link': 'https://arxiv.org/abs/2503.00144', 'abstract': "AI-supported tools can help learners overcome challenges in programming education by providing adaptive assistance. However, existing research often focuses on individual tools rather than deriving broader design recommendations. A key challenge in designing these systems is balancing learner control with system-driven guidance. To explore user preferences for AI-supported programming learning tools, we conducted a participatory design study with 15 undergraduate novice programmers and 10 instructors to gather insights on their desired help features and control preferences, as well as a follow-up survey with 172 introductory programming students.\nOur qualitative findings show that learners prefer help that is encouraging, incorporates visual aids, and includes peer-related insights, whereas instructors prioritize scaffolding that reflects learners' progress and reinforces best practices. Both groups favor shared control, though learners generally prefer more autonomy, while instructors lean toward greater system guidance to prevent cognitive overload. Additionally, our interviews revealed individual differences in control preferences.\nBased on our findings, we propose design guidelines for AI-supported programming tools, particularly regarding user-centered help features and adaptive control mechanisms. Our work contributes to the human-centered design of AI-supported learning environments by informing the development of systems that effectively balance autonomy and guidance, enhancing AI-supported educational tools for programming and beyond.", 'abstract_zh': 'AI支持工具可以通过提供适应性辅助帮助学习者克服编程教育中的挑战，但现有研究往往关注单个工具而非提炼更广泛的设计建议。设计这些系统的关键挑战是平衡学习控制与系统驱动的指导。为了探索用户对AI支持的编程学习工具的偏好，我们与15名本科新手程序员和10名教师进行了参与式设计研究，收集了他们希望的帮助特征和控制偏好方面的见解，并对172名入门级编程学生进行了后续调查。', 'title_zh': 'AI支持的编程学习工具中学习者和教师的需求：功能和适应性控制的设计 implications'}
{'arxiv_id': 'arXiv:2503.00128', 'title': 'AnnoCaseLaw: A Richly-Annotated Dataset For Benchmarking Explainable Legal Judgment Prediction', 'authors': 'Magnus Sesodia, Alina Petrova, John Armour, Thomas Lukasiewicz, Oana-Maria Camburu, Puneet K. Dokania, Philip Torr, Christian Schroeder de Witt', 'link': 'https://arxiv.org/abs/2503.00128', 'abstract': "Legal systems worldwide continue to struggle with overwhelming caseloads, limited judicial resources, and growing complexities in legal proceedings. Artificial intelligence (AI) offers a promising solution, with Legal Judgment Prediction (LJP) -- the practice of predicting a court's decision from the case facts -- emerging as a key research area. However, existing datasets often formulate the task of LJP unrealistically, not reflecting its true difficulty. They also lack high-quality annotation essential for legal reasoning and explainability. To address these shortcomings, we introduce AnnoCaseLaw, a first-of-its-kind dataset of 471 meticulously annotated U.S. Appeals Court negligence cases. Each case is enriched with comprehensive, expert-labeled annotations that highlight key components of judicial decision making, along with relevant legal concepts. Our dataset lays the groundwork for more human-aligned, explainable LJP models. We define three legally relevant tasks: (1) judgment prediction; (2) concept identification; and (3) automated case annotation, and establish a performance baseline using industry-leading large language models (LLMs). Our results demonstrate that LJP remains a formidable task, with application of legal precedent proving particularly difficult. Code and data are available at this https URL.", 'abstract_zh': '全球各国的法律系统继续面临案多人少、司法资源有限和法律程序日益复杂的问题。人工智能（AI）提供了潜在的解决方案，而法律判决预测（LJP）——从案件事实预测法院的判决——已成为一个关键的研究领域。然而，现有数据集往往不切实际地定义了LJP任务，未能反映其实质难度。它们还缺乏进行法律推理和解释所需的高质量标注。为解决这些问题，我们介绍了AnnoCaseLaw，这是一个包含471个精心标注的美国上诉法院过失案件的新颖数据集。每个案件都附有全面的专家标注注释，突出了司法决策的关键要素以及相关法律概念。我们的数据集为更符合人类行为、具有解释能力的LJP模型奠定了基础。我们定义了三个法律相关任务：（1）判决预测；（2）概念识别；和（3）自动化案例标注，并使用业内领先的大语言模型（LLMs）建立了性能基线。我们的结果表明，LJP仍是一个艰巨的任务，应用法律先例尤其困难。代码和数据请参见此链接。', 'title_zh': 'AnnoCaseLaw: 一个用于可解释法律判决预测基准测试的丰富标注数据集'}
{'arxiv_id': 'arXiv:2503.00089', 'title': 'Protein Structure Tokenization: Benchmarking and New Recipe', 'authors': 'Xinyu Yuan, Zichen Wang, Marcus Collins, Huzefa Rangwala', 'link': 'https://arxiv.org/abs/2503.00089', 'abstract': 'Recent years have witnessed a surge in the development of protein structural tokenization methods, which chunk protein 3D structures into discrete or continuous representations. Structure tokenization enables the direct application of powerful techniques like language modeling for protein structures, and large multimodal models to integrate structures with protein sequences and functional texts. Despite the progress, the capabilities and limitations of these methods remain poorly understood due to the lack of a unified evaluation framework. We first introduce StructTokenBench, a framework that comprehensively evaluates the quality and efficiency of structure tokenizers, focusing on fine-grained local substructures rather than global structures, as typical in existing benchmarks. Our evaluations reveal that no single model dominates all benchmarking perspectives. Observations of codebook under-utilization led us to develop AminoAseed, a simple yet effective strategy that enhances codebook gradient updates and optimally balances codebook size and dimension for improved tokenizer utilization and quality. Compared to the leading model ESM3, our method achieves an average of 6.31% performance improvement across 24 supervised tasks, with sensitivity and utilization rates increased by 12.83% and 124.03%, respectively.', 'abstract_zh': '近来，蛋白质结构分词方法的发展呈现出快速增长的趋势，这些方法将蛋白质3D结构分割为离散或连续的表示。结构分词使得直接将强大的语言建模技术应用于蛋白质结构，并通过大型多模态模型整合结构、蛋白质序列和功能文本成为可能。尽管取得了进步，但由于缺乏统一的评估框架，这些方法的能力和局限性仍不够清楚。我们首先介绍了StructTokenBench框架，该框架全面评估结构分词的质量和效率，重点关注精细粒度的局部子结构，而不是全局结构，这与现有基准中的典型做法不同。我们的评估表明，没有单一模型能够在所有基准视角中占据支配地位。代码书利用率低的观察促使我们开发了AminoAseed策略，这是一种简单有效的策略，可以增强代码书梯度更新，并优化代码书大小和维度的平衡，以提高分词器的利用率和质量。与领先模型ESM3相比，我们的方法在24个受监督任务中平均实现了6.31%的性能改进，敏感性和利用率分别提高了12.83%和124.03%。', 'title_zh': '蛋白质结构-token化：基准测试与新方法'}
{'arxiv_id': 'arXiv:2503.00084', 'title': 'InspireMusic: Integrating Super Resolution and Large Language Model for High-Fidelity Long-Form Music Generation', 'authors': 'Chong Zhang, Yukun Ma, Qian Chen, Wen Wang, Shengkui Zhao, Zexu Pan, Hao Wang, Chongjia Ni, Trung Hieu Nguyen, Kun Zhou, Yidi Jiang, Chaohong Tan, Zhifu Gao, Zhihao Du, Bin Ma', 'link': 'https://arxiv.org/abs/2503.00084', 'abstract': 'We introduce InspireMusic, a framework integrated super resolution and large language model for high-fidelity long-form music generation. A unified framework generates high-fidelity music, songs, and audio, which incorporates an autoregressive transformer with a super-resolution flow-matching model. This framework enables the controllable generation of high-fidelity long-form music at a higher sampling rate from both text and audio prompts. Our model differs from previous approaches, as we utilize an audio tokenizer with one codebook that contains richer semantic information, thereby reducing training costs and enhancing efficiency. This combination enables us to achieve high-quality audio generation with long-form coherence of up to $8$ minutes. Then, an autoregressive transformer model based on Qwen 2.5 predicts audio tokens. Next, we employ a super-resolution flow-matching model to generate high-sampling rate audio with fine-grained details learned from an acoustic codec model. Comprehensive experiments show that the InspireMusic-1.5B-Long model has a comparable performance to recent top-tier open-source systems, including MusicGen and Stable Audio 2.0, on subjective and objective evaluations. The code and pre-trained models are released at this https URL.', 'abstract_zh': '我们介绍了一种集成超分辨率和大型语言模型的InspireMusic框架，用于高保真长格式音乐生成。该统一框架结合了自回归Transformer和超分辨率流匹配模型，能够从文本和音频提示生成高质量的音乐、歌曲和音频，并支持高采样率下的长格式音乐可控生成。与以往方法不同，我们利用包含更丰富语义信息的一维码本音频分词器，从而降低训练成本并提高效率。这种组合使我们能够生成长达8分钟的长格式音频，并保持高保真度。随后，基于Qwen 2.5的自回归Transformer模型预测音频分词。接着，我们使用超分辨率流匹配模型从声码器模型中学习精细的细节生成高采样率音频。综合实验表明，InspireMusic-1.5B-Long模型在主观和客观评估中与当前顶级开源系统（包括MusicGen和Stable Audio 2.0）具有 comparable 性能。代码和预训练模型可在以下链接获取。', 'title_zh': 'InspireMusic：集成超分辨率和大规模语言模型的高保真长格式音乐生成'}
{'arxiv_id': 'arXiv:2503.00081', 'title': 'Experiences with Content Development and Assessment Design in the Era of GenAI', 'authors': 'Aakanksha Sharma, Samar Shailendra, Rajan Kadel', 'link': 'https://arxiv.org/abs/2503.00081', 'abstract': 'Generative Artificial Intelligence (GenAI) has the potential to transform higher education by generating human-like content. The advancement in GenAI has revolutionised several aspects of education, especially subject and assessment design. In this era, it is crucial to design assessments that challenge students and cannot be solved using GenAI tools. This makes it necessary to update the educational content with rapidly evolving technology. The assessment plays a significant role in ensuring the students learning, as it encourages students to engage actively, leading to the achievement of learning outcomes. The paper intends to determine how effectively GenAI can design a subject, including lectures, labs and assessments, using prompts and custom-based training. This paper aims to elucidate the direction to educators so they can leverage GenAI to create subject content. Additionally, we provided our experiential learning for educators to develop content, highlighting the importance of prompts and fine-tuning to ensure output quality. It has also been observed that expert evaluation is essential for assessing the quality of GenAI-generated materials throughout the content generation process.', 'abstract_zh': '生成式人工智能（GenAI）有望通过生成类人类内容来变革高等教育。随着生成式人工智能的进步，教育的多个方面尤其是课程和评估设计已经被彻底革新。在这个时代，设计出能够挑战学生且无法通过GenAI工具解决的评估至关重要。因此，需要随着技术的快速发展不断地更新教育内容。评估在确保学生学习效果方面起着重要作用，它激励学生积极参与，从而达成学习目标。本文旨在探讨GenAI如何有效设计一门课程，包括讲座、实验室和评估，利用提示和基于定制的训练。本文旨在为教育者指明方向，以便他们可以利用GenAI创建课程内容。此外，我们还提供了实践经验，帮助教育者开发内容，强调提示和微调的重要性以确保输出质量，并观察到在整个内容生成过程中专家评估对于评估GenAI生成材料的质量至关重要。', 'title_zh': '在生成式人工智能时代的内容开发与评估设计经验分享'}
{'arxiv_id': 'arXiv:2503.00079', 'title': 'AI Literacy in K-12 and Higher Education in the Wake of Generative AI: An Integrative Review', 'authors': 'Xingjian, Barbara J. Ericson', 'link': 'https://arxiv.org/abs/2503.00079', 'abstract': 'Even though AI literacy has emerged as a prominent education topic in the wake of generative AI, its definition remains vague. There is little consensus among researchers and practitioners on how to discuss and design AI literacy interventions. The term has been used to describe both learning activities that train undergraduate students to use ChatGPT effectively and having kindergarten children interact with social robots. This paper applies an integrative review method to examine empirical and theoretical AI literacy studies published since 2020. In synthesizing the 124 reviewed studies, three ways to conceptualize literacy-functional, critical, and indirectly beneficial-and three perspectives on AI-technical detail, tool, and sociocultural-were identified, forming a framework that reflects the spectrum of how AI literacy is approached in practice. The framework highlights the need for more specialized terms within AI literacy discourse and indicates research gaps in certain AI literacy objectives.', 'abstract_zh': '尽管生成式AI兴起后促进了AI素养这一教育议题的重要性，但其定义仍然模糊不清。研究人员和实践者在讨论和设计AI素养干预措施时缺乏共识。该术语被用来描述从训练大学生有效使用ChatGPT到让幼儿园儿童与社会机器人互动的学习活动。本文采用综合审查方法，研究自2020年以来发表的AI素养实证和理论研究。在综合分析124项研究的基础上，我们识别出三种素养认知方式：功能、批判性和间接益处，以及三种AI视角：技术细节、工具和社会文化，形成了一种框架，反映了实际操作中AI素养的多样性。该框架强调了AI素养领域内需要更专门术语的必要性，并指出了某些AI素养目标的研究空白。', 'title_zh': 'K-12及高等教育背景下生成式AI兴起之时的AI素养：一项综合审查'}
{'arxiv_id': 'arXiv:2503.00070', 'title': 'Systematic Review of Cybersecurity in Banking: Evolution from Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain, Policies and Practice', 'authors': 'Tue Nhi Tran', 'link': 'https://arxiv.org/abs/2503.00070', 'abstract': 'Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.', 'abstract_zh': '从工业4.0前到工业4.0后，银行网络安全经历了显著变化：从工业4.0前依赖高度手动且准确性低的个体安全方法，到工业4.0后结合了人工智能、区块链、物联网等技术，自动化必要流程并显著增加银行防御层。然而，随着新技术的发展，银行网络安全当前面临的挑战包括可扩展性、高昂的研发成本和时间投入，以及高科技网络犯罪分子的威胁加大。本报告将从介绍银行网络安全的重要性、分析其管理、运营和业务目标、评估工业4.0前用于银行网络安全的技术，到评估工业4.0后以人工智能和区块链为重点的技术，讨论当前政策和实践，并最终讨论4.0技术的关键优势和挑战，以及对进一步发展银行网络安全的建议。', 'title_zh': 'Banking领域的网络安全系统综述：从工业4.0前到工业4.0后的演变——人工智能、区块链、政策与实践'}
{'arxiv_id': 'arXiv:2503.00052', 'title': 'RURA-Net: A general disease diagnosis method based on Zero-Shot Learning', 'authors': 'Yan Su, Qiulin Wu, Weizhen Li, Chengchang Pan, Honggang Qi', 'link': 'https://arxiv.org/abs/2503.00052', 'abstract': 'The training of deep learning models relies on a large amount of labeled data. However, the high cost of medical labeling seriously hinders the development of deep learning in the medical field. Our study proposes a general disease diagnosis approach based on Zero-Shot Learning. The Siamese neural network is used to find similar diseases for the target diseases, and the U-Net segmentation model is used to accurately segment the key lesions of the disease. Finally, based on the ResNet-Agglomerative clustering algorithm, a clustering model is trained on a large number of sample data of similar diseases to obtain a approximate diagnosis of the target disease. Zero-Shot Learning of the target disease is then successfully achieved. To evaluate the validity of the model, we validated our method on a dataset of ophthalmic diseases in CFP modality. The external dataset was used to test its performance, and the accuracy=0.8395, precision=0.8094, recall=0.8463, F1 Score=0.8274, AUC=0.9226, which exceeded the indexes of most Few-Shot Learning and One-Shot Learning models. It proves that our method has great potential and reference value in the medical field, where annotation data is usually scarce and expensive to obtain.', 'abstract_zh': '基于零样本学习的疾病诊断通用方法研究', 'title_zh': 'RURA-Net：一种基于零样本学习的通用疾病诊断方法'}
{'arxiv_id': 'arXiv:2503.00036', 'title': 'A Novel Spatiotemporal Correlation Anomaly Detection Method Based on Time-Frequency-Domain Feature Fusion and a Dynamic Graph Neural Network in Wireless Sensor Network', 'authors': 'Miao Ye, Zhibang Jiang, Xingsi Xue, Xingwang Li, Peng Wen, Yong Wang', 'link': 'https://arxiv.org/abs/2503.00036', 'abstract': 'Attention-based transformers have played an important role in wireless sensor network (WSN) timing anomaly detection due to their ability to capture long-term dependencies. However, there are several issues that must be addressed, such as the fact that their ability to capture long-term dependencies is not completely reliable, their computational complexity levels are high, and the spatiotemporal features of WSN timing data are not sufficiently extracted for detecting the correlation anomalies of multinode WSN timing data. To address these limitations, this paper proposes a WSN anomaly detection method that integrates frequency-domain features with dynamic graph neural networks (GNN) under a designed self-encoder reconstruction framework. First, the discrete wavelet transform effectively decomposes trend and seasonal components of time series to solve the poor long-term reliability of transformers. Second, a frequency-domain attention mechanism is designed to make full use of the difference between the amplitude distributions of normal data and anomalous data in this domain. Finally, a multimodal fusion-based dynamic graph convolutional network (MFDGCN) is designed by combining an attention mechanism and a graph convolutional network (GCN) to adaptively extract spatial correlation features. A series of experiments conducted on public datasets and their results demonstrate that the anomaly detection method designed in this paper exhibits superior precision and recall than the existing methods do, with an F1 score of 93.5%, representing an improvement of 2.9% over that of the existing models.', 'abstract_zh': '基于频率域特征与动态图神经网络的无线传感器网络定时异常检测方法', 'title_zh': '基于时频域特征融合和动态图神经网络的新型时空相关异常检测方法在无线传感器网络中的应用'}
{'arxiv_id': 'arXiv:2503.00029', 'title': 'Streaming Looking Ahead with Token-level Self-reward', 'authors': 'Hongming Zhang, Ruixin Hong, Dong Yu', 'link': 'https://arxiv.org/abs/2503.00029', 'abstract': "Autoregressive decoding algorithms that use only past information often cannot guarantee the best performance. Recently, people discovered that looking-ahead algorithms such as Monte Carlo Tree Search (MCTS) with external reward models (RMs) can significantly improve models' output by allowing them to think ahead and leverage future outputs and associated rewards to guide the current generation. Such techniques can help the reinforcement fine-tuning phase by sampling better trajectories and the inference phase by selecting the better output. However, their high computational cost limits their applications, especially in streaming scenarios. To address this issue, we propose equipping the policy model with token-level self-reward modeling (TRM) capability to eliminate the need for external models and extra communication. We name the new architecture as Reward Transformer. In addition, we propose a streaming-looking-ahead (SLA) algorithm to further boost search efficiency with better parallelization. Experiments show that SLA achieves an overall win rate of 79.7\\% against the baseline greedy decoding algorithm on three general-domain datasets with a frozen policy model while maintaining streaming efficiency. If we combine SLA with reinforcement fine-tuning techniques such as DPO, SLA achieves an overall win rate of 89.4\\%.", 'abstract_zh': '自回归解码算法通常仅使用过去信息，难以保证最佳性能。最近的研究发现，通过引入前瞻算法如结合外部奖励模型（RMs）的蒙特卡罗树搜索（MCTS），可以在允许模型提前思考并利用未来输出及其相关奖励来指导当前生成的过程中显著提高模型输出。这些技术可以通过采样更好的轨迹来帮助强化微调阶段，并通过选择更好的输出来提高推理阶段的效率。然而，它们的高计算成本限制了它们的应用，尤其是在流式场景中。为解决这一问题，我们提出为策略模型配备令牌级别自奖励建模（TRM）能力，以消除对外部模型和额外通信的需求。我们将这一新架构命名为奖励变换器。此外，我们提出了一种流式前瞻（SLA）算法，以通过更好的并行化进一步提升搜索效率。实验结果显示，SLA在三个通用领域数据集上对基线贪婪解码算法实现了79.7%的整体胜率，同时保持了流式效率。如果将SLA与DPO等强化微调技术结合使用，SLA实现了89.4%的整体胜率。', 'title_zh': 'Streaming Looking Ahead with Token-level Self-Reward'}
{'arxiv_id': 'arXiv:2503.00028', 'title': 'Genetics-Driven Personalized Disease Progression Model', 'authors': 'Haoyu Yang, Sanjoy Dey, Pablo Meyer', 'link': 'https://arxiv.org/abs/2503.00028', 'abstract': "Modeling disease progression through multiple stages is critical for clinical decision-making for chronic diseases, e.g., cancer, diabetes, chronic kidney diseases, and so on. Existing approaches often model the disease progression as a uniform trajectory pattern at the population level. However, chronic diseases are highly heterogeneous and often have multiple progression patterns depending on a patient's individual genetics and environmental effects due to lifestyles. We propose a personalized disease progression model to jointly learn the heterogeneous progression patterns and groups of genetic profiles. In particular, an end-to-end pipeline is designed to simultaneously infer the characteristics of patients from genetic markers using a variational autoencoder and how it drives the disease progressions using an RNN-based state-space model based on clinical observations. Our proposed model shows improvement on real-world and synthetic clinical data.", 'abstract_zh': '通过多阶段建模慢性疾病进展对于临床决策至关重要：一种个性化疾病进展模型', 'title_zh': '遗传驱动的个性化疾病进展模型'}
{'arxiv_id': 'arXiv:2503.00020', 'title': 'A Systematic Review of Open Datasets Used in Text-to-Image (T2I) Gen AI Model Safety', 'authors': 'Rakeen Rouf, Trupti Bavalatti, Osama Ahmed, Dhaval Potdar, Faraz Jawed', 'link': 'https://arxiv.org/abs/2503.00020', 'abstract': 'Novel research aimed at text-to-image (T2I) generative AI safety often relies on publicly available datasets for training and evaluation, making the quality and composition of these datasets crucial. This paper presents a comprehensive review of the key datasets used in the T2I research, detailing their collection methods, compositions, semantic and syntactic diversity of prompts and the quality, coverage, and distribution of harm types in the datasets. By highlighting the strengths and limitations of the datasets, this study enables researchers to find the most relevant datasets for a use case, critically assess the downstream impacts of their work given the dataset distribution, particularly regarding model safety and ethical considerations, and also identify the gaps in dataset coverage and quality that future research may address.', 'abstract_zh': '面向文本到图像生成AI安全的新型研究往往依赖于公开数据集进行训练和评估，因此这些数据集的质量和组成至关重要。本文对文本到图像研究中使用的关键数据集进行了全面回顾，详细介绍了这些数据集的采集方法、组成、提示的语义和语法多样性，以及数据集中危害类型的质量、覆盖范围和分布。通过突出这些数据集的优势和局限性，本研究使研究人员能够找到最相关的数据集用于特定应用场景，批判性地评估其工作下游影响，特别是关于模型安全和伦理考量，并识别未来研究可能解决的数据集覆盖范围和质量的不足。', 'title_zh': 'Open 数据集在文本到图像（T2I）生成式AI模型安全中的系统性Review'}
