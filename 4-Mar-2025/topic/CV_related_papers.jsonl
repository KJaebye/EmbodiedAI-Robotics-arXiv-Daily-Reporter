{'arxiv_id': 'arXiv:2503.01571', 'title': 'MLINE-VINS: Robust Monocular Visual-Inertial SLAM With Flow Manhattan and Line Features', 'authors': 'Chao Ye, Haoyuan Li, Weiyang Lin, Xianqiang Yang', 'link': 'https://arxiv.org/abs/2503.01571', 'abstract': 'In this paper we introduce MLINE-VINS, a novel monocular visual-inertial odometry (VIO) system that leverages line features and Manhattan Word assumption. Specifically, for line matching process, we propose a novel geometric line optical flow algorithm that efficiently tracks line features with varying lengths, whitch is do not require detections and descriptors in every frame. To address the instability of Manhattan estimation from line features, we propose a tracking-by-detection module that consistently tracks and optimizes Manhattan framse in consecutive images. By aligning the Manhattan World with the VIO world frame, the tracking could restart using the latest pose from back-end, simplifying the coordinate transformations within the system. Furthermore, we implement a mechanism to validate Manhattan frames and a novel global structural constraints back-end optimization. Extensive experiments results on vairous datasets, including benchmark and self-collected datasets, show that the proposed approach outperforms existing methods in terms of accuracy and long-range robustness. The source code of our method is available at: this https URL.', 'abstract_zh': '本文介绍了一种新的单目视觉惯性里程计（VIO）系统MLINE-VINS，该系统利用直线特征和曼哈顿词假设。对于直线匹配过程，我们提出了一种新颖的几何直线光学流算法，该算法可以高效地跟踪各种长度的直线特征，且不需要每帧都进行检测和描述符匹配。针对从直线特征估计曼哈顿结构的不稳定性，我们提出了一种跟踪-检测模块，该模块可以一致地跟踪和优化连续图像中的曼哈顿帧。通过将曼哈顿世界与VIO世界坐标帧对齐，跟踪可以在后端最新姿态的指导下重启，简化系统的坐标转换。此外，我们实现了一种验证曼哈顿帧的机制和一种新的全局结构约束后端优化方法。在各种数据集（包括基准数据集和自采集数据集）上进行的广泛实验结果显示，所提出的方法在准确性和长距离鲁棒性方面优于现有方法。我们的方法源代码可在以下链接获取：这个 https URL。', 'title_zh': 'MLINE-VINS：具有流动曼哈顿和平面线特征的鲁棒单目视觉-惯性SLAM'}
{'arxiv_id': 'arXiv:2503.01562', 'title': 'VF-Plan: Bridging the Art Gallery Problem and Static LiDAR Scanning with Visibility Field Optimization', 'authors': 'Biao Xionga, Longjun Zhanga, Ruiqi Huanga, Junwei Zhoua, Bojian Wub, Fashuai Lic', 'link': 'https://arxiv.org/abs/2503.01562', 'abstract': 'Viewpoint planning is crucial for 3D data collection and autonomous navigation, yet existing methods often miss key optimization objectives for static LiDAR, resulting in suboptimal network designs. The Viewpoint Planning Problem (VPP), which builds upon the Art Gallery Problem (AGP), requires not only full coverage but also robust registrability and connectivity under limited sensor views. We introduce a greedy optimization algorithm that tackles these VPP and AGP challenges through a novel Visibility Field (VF) approach. The VF captures visibility characteristics unique to static LiDAR, enabling a reduction from 2D to 1D by focusing on medial axis and joints. This leads to a minimal, fully connected viewpoint network with comprehensive coverage and minimal redundancy. Experiments across diverse environments show that our method achieves high efficiency and scalability, matching or surpassing expert designs. Compared to state-of-the-art methods, our approach achieves comparable viewpoint counts (VC) while reducing Weighted Average Path Length (WAPL) by approximately 95\\%, indicating a much more compact and connected network. Dataset and source code will be released upon acceptance.', 'abstract_zh': '视点规划对于3D数据采集和自主导航至关重要，现有方法往往忽视了静态LiDAR的关键优化目标，导致网络设计欠佳。视点规划问题（VPP），基于艺术画廊问题（AGP），除了要求全面覆盖外，还需要在有限传感器视角下具备鲁棒的可注册性和连通性。我们提出了一种贪婪优化算法，通过一种新颖的可见性场（VF）方法来解决这些VPP和AGP挑战。VF捕捉静态LiDAR特有的可见性特征，使得问题从二维减少到一维，通过集中于中轴线和节点。这导致了一个最小的、完全连通的视点网络，具备全面覆盖且冗余最少。实验结果表明，我们的方法在多种环境下表现出高效性和可扩展性，能够达到或超越专家设计。与现有的最先进的方法相比，我们的方法在视点计数（VC）相当的同时，平均路径长度加权平均值（WAPL）减少了约95%，表明一个更为紧凑和连通的网络。数据集和源代码将在接受后发布。', 'title_zh': 'VF-Plan: 将视野场优化与艺术画廊问题和静态LiDAR扫描相结合'}
{'arxiv_id': 'arXiv:2503.00862', 'title': 'Efficient End-to-end Visual Localization for Autonomous Driving with Decoupled BEV Neural Matching', 'authors': 'Jinyu Miao, Tuopu Wen, Ziang Luo, Kangan Qian, Zheng Fu, Yunlong Wang, Kun Jiang, Mengmeng Yang, Jin Huang, Zhihua Zhong, Diange Yang', 'link': 'https://arxiv.org/abs/2503.00862', 'abstract': 'Accurate localization plays an important role in high-level autonomous driving systems. Conventional map matching-based localization methods solve the poses by explicitly matching map elements with sensor observations, generally sensitive to perception noise, therefore requiring costly hyper-parameter tuning. In this paper, we propose an end-to-end localization neural network which directly estimates vehicle poses from surrounding images, without explicitly matching perception results with HD maps. To ensure efficiency and interpretability, a decoupled BEV neural matching-based pose solver is proposed, which estimates poses in a differentiable sampling-based matching module. Moreover, the sampling space is hugely reduced by decoupling the feature representation affected by each DoF of poses. The experimental results demonstrate that the proposed network is capable of performing decimeter level localization with mean absolute errors of 0.19m, 0.13m and 0.39 degree in longitudinal, lateral position and yaw angle while exhibiting a 68.8% reduction in inference memory usage.', 'abstract_zh': '准确的定位在高级自动驾驶系统中发挥着重要作用。面向高效可解释的端到端定位神经网络：通过解耦BEV神经匹配模块直接从周围图像中估计车辆姿态，显著减少推理内存使用并实现分米级定位准确性。', 'title_zh': '自主驾驶中基于解耦BEV神经匹配的高效端到端视觉定位'}
{'arxiv_id': 'arXiv:2503.00709', 'title': 'ICanC: Improving Camera-based Object Detection and Energy Consumption in Low-Illumination Environments', 'authors': 'Daniel Ma, Ren Zhong, Weisong Shi', 'link': 'https://arxiv.org/abs/2503.00709', 'abstract': 'This paper introduces ICanC (pronounced "I Can See"), a novel system designed to enhance object detection and optimize energy efficiency in autonomous vehicles (AVs) operating in low-illumination environments. By leveraging the complementary capabilities of LiDAR and camera sensors, ICanC improves detection accuracy under conditions where camera performance typically declines, while significantly reducing unnecessary headlight usage. This approach aligns with the broader objective of promoting sustainable transportation.\nICanC comprises three primary nodes: the Obstacle Detector, which processes LiDAR point cloud data to fit bounding boxes onto detected objects and estimate their position, velocity, and orientation; the Danger Detector, which evaluates potential threats using the information provided by the Obstacle Detector; and the Light Controller, which dynamically activates headlights to enhance camera visibility solely when a threat is detected.\nExperiments conducted in physical and simulated environments demonstrate ICanC\'s robust performance, even in the presence of significant noise interference. The system consistently achieves high accuracy in camera-based object detection when headlights are engaged, while significantly reducing overall headlight energy consumption. These results position ICanC as a promising advancement in autonomous vehicle research, achieving a balance between energy efficiency and reliable object detection.', 'abstract_zh': '本文介绍了一种新型系统ICanC（发音为“I Can See”），该系统旨在增强自主车辆（AVs）在低照明环境中物体检测的准确性和优化光能效率。ICanC通过结合LiDAR和摄像头传感器的优势，在传统摄像头性能下降的情况下提高检测准确性，同时大幅减少不必要的前灯使用。该方法符合推动可持续交通的总体目标。', 'title_zh': 'ICanC: 提高低光照环境中基于相机的目标检测和能耗性能'}
{'arxiv_id': 'arXiv:2503.00551', 'title': 'PL-VIWO: A Lightweight and Robust Point-Line Monocular Visual Inertial Wheel Odometry', 'authors': 'Zhixin Zhang, Wenzhi Bai, Liang Zhao, Pawel Ladosz', 'link': 'https://arxiv.org/abs/2503.00551', 'abstract': 'This paper presents a novel tightly coupled Filter-based monocular visual-inertial-wheel odometry (VIWO) system for ground robots, designed to deliver accurate and robust localization in long-term complex outdoor navigation scenarios. As an external sensor, the camera enhances localization performance by introducing visual constraints. However, obtaining a sufficient number of effective visual features is often challenging, particularly in dynamic or low-texture environments. To address this issue, we incorporate the line features for additional geometric constraints. Unlike traditional approaches that treat point and line features independently, our method exploits the geometric relationships between points and lines in 2D images, enabling fast and robust line matching and triangulation. Additionally, we introduce Motion Consistency Check (MCC) to filter out potential dynamic points, ensuring the effectiveness of point feature updates. The proposed system was evaluated on publicly available datasets and benchmarked against state-of-the-art methods. Experimental results demonstrate superior performance in terms of accuracy, robustness, and efficiency. The source code is publicly available at: this https URL', 'abstract_zh': '本文提出了一种新颖的紧密耦合滤波器为基础的单目视觉-惯性-陀螺仪里程计（VIWO）系统，旨在为地面机器人在长期复杂户外导航场景中提供准确可靠的定位。作为外部传感器，摄像机通过引入视觉约束来提升定位性能。然而，在动态或低纹理环境中获得足够的有效视觉特征往往是具有挑战性的。为解决这一问题，我们引入了线特征以提供额外的几何约束。与传统方法将点特征和线特征独立处理不同，我们的方法利用二维图像中点和线之间的几何关系，实现快速稳健的线匹配和三角化。此外，我们引入了运动一致性检查（MCC）来滤除潜在的动态点，确保点特征更新的有效性。所提出系统在公开可用的数据集上进行了评估，并与最先进的方法进行了基准测试。实验结果在准确性、稳健性和效率方面展现了优越性能。源代码可在以下链接获取：this https URL', 'title_zh': 'PL-VIWO: 一种轻量级稳健的点线单目视觉惯性轮 odometer'}
{'arxiv_id': 'arXiv:2503.00397', 'title': 'Floorplan-SLAM: A Real-Time, High-Accuracy, and Long-Term Multi-Session Point-Plane SLAM for Efficient Floorplan Reconstruction', 'authors': 'Haolin Wang, Zeren Lv, Hao Wei, Haijiang Zhu, Yihong Wu', 'link': 'https://arxiv.org/abs/2503.00397', 'abstract': 'Floorplan reconstruction provides structural priors essential for reliable indoor robot navigation and high-level scene understanding. However, existing approaches either require time-consuming offline processing with a complete map, or rely on expensive sensors and substantial computational resources. To address the problems, we propose Floorplan-SLAM, which incorporates floorplan reconstruction tightly into a multi-session SLAM system by seamlessly interacting with plane extraction, pose estimation, and back-end optimization, achieving real-time, high-accuracy, and long-term floorplan reconstruction using only a stereo camera. Specifically, we present a robust plane extraction algorithm that operates in a compact plane parameter space and leverages spatially complementary features to accurately detect planar structures, even in weakly textured scenes. Furthermore, we propose a floorplan reconstruction module tightly coupled with the SLAM system, which uses continuously optimized plane landmarks and poses to formulate and solve a novel optimization problem, thereby enabling real-time incremental floorplan reconstruction. Note that by leveraging the map merging capability of multi-session SLAM, our method supports long-term floorplan reconstruction across multiple sessions without redundant data collection. Experiments on the VECtor and the self-collected datasets indicate that Floorplan-SLAM significantly outperforms state-of-the-art methods in terms of plane extraction robustness, pose estimation accuracy, and floorplan reconstruction fidelity and speed, achieving real-time performance at 25-45 FPS without GPU acceleration, which reduces the floorplan reconstruction time for a 1000 square meters scene from over 10 hours to just 9.44 minutes.', 'abstract_zh': 'Floorplan-SLAM：结合多时段SLAM系统的紧凑平面参数空间中稳健的平面提取与实时高精度长期楼层平面重建', 'title_zh': '地板平面-SLAM：一种高效地板平面重构的实时、高精度和长时间多会话点-平面SLAM'}
{'arxiv_id': 'arXiv:2503.00315', 'title': 'XIRVIO: Critic-guided Iterative Refinement for Visual-Inertial Odometry with Explainable Adaptive Weighting', 'authors': 'Chit Yuen Lam, Ronald Clark, Basaran Bahadir Kocer', 'link': 'https://arxiv.org/abs/2503.00315', 'abstract': "We introduce XIRVIO, a transformer-based Generative Adversarial Network (GAN) framework for monocular visual inertial odometry (VIO). By taking sequences of images and 6-DoF inertial measurements as inputs, XIRVIO's generator predicts pose trajectories through an iterative refinement process which are then evaluated by the critic to select the iteration with the optimised prediction. Additionally, the self-emergent adaptive sensor weighting reveals how XIRVIO attends to each sensory input based on contextual cues in the data, making it a promising approach for achieving explainability in safety-critical VIO applications. Evaluations on the KITTI dataset demonstrate that XIRVIO matches well-known state-of-the-art learning-based methods in terms of both translation and rotation errors.", 'abstract_zh': 'XIRVIO：一种基于变换器的生成对抗网络框架，用于单目视觉惯性里程计', 'title_zh': 'XIRVIO：具有可解释自适应加权的批评指导迭代增强视觉-惯性测地术'}
{'arxiv_id': 'arXiv:2503.00200', 'title': 'Unified Video Action Model', 'authors': 'Shuang Li, Yihuai Gao, Dorsa Sadigh, Shuran Song', 'link': 'https://arxiv.org/abs/2503.00200', 'abstract': 'A unified video and action model holds significant promise for robotics, where videos provide rich scene information for action prediction, and actions provide dynamics information for video prediction. However, effectively combining video generation and action prediction remains challenging, and current video generation-based methods struggle to match the performance of direct policy learning in action accuracy and inference this http URL bridge this gap, we introduce the Unified Video Action model (UVA), which jointly optimizes video and action predictions to achieve both high accuracy and efficient action inference. The key lies in learning a joint video-action latent representation and decoupling video-action decoding. The joint latent representation bridges the visual and action domains, effectively modeling the relationship between video and action sequences. Meanwhile, the decoupled decoding, powered by two lightweight diffusion heads, enables high-speed action inference by bypassing video generation during inference. Such a unified framework further enables versatile functionality through masked input training. By selectively masking actions or videos, a single model can tackle diverse tasks beyond policy learning, such as forward and inverse dynamics modeling and video generation. Via an extensive set of experiments, we demonstrate that UVA can serve as a general-purpose solution for a wide range of robotics tasks, such as policy learning, forward/inverse dynamics and video observation prediction, without compromising performance compared to methods tailored for specific applications. Results are best viewed on this https URL.', 'abstract_zh': '一种统一的视频和动作模型在机器人领域具有重要潜力，视频提供了丰富的场景信息以进行动作预测，而动作提供了视频预测的动力学信息。然而，有效地结合视频生成和动作预测仍然具有挑战性，当前基于视频生成的方法在动作准确性及推理速度上难以匹配合乎政策学习的方法。为了弥合这一差距，我们提出了统一视频动作模型（UVA），该模型同时优化视频和动作预测，以实现高准确性和高效的动作推理。关键在于学习联合视频-动作潜在表示并解耦视频-动作解码。联合潜在表示连接视觉和动作领域，有效地建模了视频和动作序列之间的关系。同时，由两个轻量级扩散头驱动的解耦解码能够在推理时不生成视频，从而实现高速动作推理。这种统一框架还通过蒙版输入训练增强了多功能性。通过选择性地蒙版动作或视频，单个模型可以解决超出策略学习的各种任务，如前向/逆向动力学建模和视频生成。通过一系列广泛实验，我们证明UVA可以用作解决一系列机器人任务的一般解决方案，例如策略学习、前向/逆向动力学建模和视频观测预测，且与专门针对特定应用的方法相比不牺牲性能。结果请参阅此链接：https://openaccess.thecvf.com/content/CVPR2023/papers/Peng_Unified_Video_Action_Models_for_Robotic_Task_Completion_CVPR_2023_paper.pdf。', 'title_zh': '统一视频动作模型'}
{'arxiv_id': 'arXiv:2503.00045', 'title': 'Glad: A Streaming Scene Generator for Autonomous Driving', 'authors': 'Bin Xie, Yingfei Liu, Tiancai Wang, Jiale Cao, Xiangyu Zhang', 'link': 'https://arxiv.org/abs/2503.00045', 'abstract': 'The generation and simulation of diverse real-world scenes have significant application value in the field of autonomous driving, especially for the corner cases. Recently, researchers have explored employing neural radiance fields or diffusion models to generate novel views or synthetic data under driving scenes. However, these approaches suffer from unseen scenes or restricted video length, thus lacking sufficient adaptability for data generation and simulation. To address these issues, we propose a simple yet effective framework, named Glad, to generate video data in a frame-by-frame style. To ensure the temporal consistency of synthetic video, we introduce a latent variable propagation module, which views the latent features of previous frame as noise prior and injects it into the latent features of current frame. In addition, we design a streaming data sampler to orderly sample the original image in a video clip at continuous iterations. Given the reference frame, our Glad can be viewed as a streaming simulator by generating the videos for specific scenes. Extensive experiments are performed on the widely-used nuScenes dataset. Experimental results demonstrate that our proposed Glad achieves promising performance, serving as a strong baseline for online video generation. We will release the source code and models publicly.', 'abstract_zh': '生成和模拟多样化的现实场景在自动驾驶领域具有重要的应用价值，尤其是在处理corner cases方面。最近，研究人员探索了利用神经辐射场或扩散模型在驾驶场景下生成新颖视图或合成数据。然而，这些方法受到未见过的场景或视频长度限制的影响，因而缺乏足够的数据生成和模拟适应性。为解决这些问题，我们提出了一种简单有效的框架Glad，以帧级别的方式生成视频数据。为了确保合成视频的时间连贯性，我们引入了一个潜在变量传播模块，将前一帧的潜在特征视为噪声先验，并将其注入当前帧的潜在特征中。此外，我们设计了流式数据采样器，在连续迭代中有序地从视频片段中采样原始图像。给定参考帧，我们的Glad可以被视为一个流式模拟器，通过为特定场景生成视频。我们在广泛使用的nuScenes数据集上进行了 extensive 实验。实验结果表明，我们提出的Glad在在线视频生成方面取得了令人鼓舞的性能，作为在线视频生成的强基线，我们将在未来公开发布源代码和模型。', 'title_zh': 'Glad: 自动驾驶的流式场景生成器'}
{'arxiv_id': 'arXiv:2503.01582', 'title': 'Category-level Meta-learned NeRF Priors for Efficient Object Mapping', 'authors': 'Saad Ejaz, Hriday Bavle, Laura Ribeiro, Holger Voos, Jose Luis Sanchez-Lopez', 'link': 'https://arxiv.org/abs/2503.01582', 'abstract': 'In 3D object mapping, category-level priors enable efficient object reconstruction and canonical pose estimation, requiring only a single prior per semantic category (e.g., chair, book, laptop). Recently, DeepSDF has predominantly been used as a category-level shape prior, but it struggles to reconstruct sharp geometry and is computationally expensive. In contrast, NeRFs capture fine details but have yet to be effectively integrated with category-level priors in a real-time multi-object mapping framework. To bridge this gap, we introduce PRENOM, a Prior-based Efficient Neural Object Mapper that integrates category-level priors with object-level NeRFs to enhance reconstruction efficiency while enabling canonical object pose estimation. PRENOM gets to know objects on a first-name basis by meta-learning on synthetic reconstruction tasks generated from open-source shape datasets. To account for object category variations, it employs a multi-objective genetic algorithm to optimize the NeRF architecture for each category, balancing reconstruction quality and training time. Additionally, prior-based probabilistic ray sampling directs sampling toward expected object regions, accelerating convergence and improving reconstruction quality under constrained resources. Experimental results on a low-end GPU highlight the ability of PRENOM to achieve high-quality reconstructions while maintaining computational feasibility. Specifically, comparisons with prior-free NeRF-based approaches on a synthetic dataset show a 21% lower Chamfer distance, demonstrating better reconstruction quality. Furthermore, evaluations against other approaches using shape priors on a noisy real-world dataset indicate a 13% improvement averaged across all reconstruction metrics, a boost in rotation estimation accuracy, and comparable translation and size estimation performance, while being trained for 5x less time.', 'abstract_zh': '基于先验的高效神经对象.mapper：将类别级先验与对象级NeRF集成以增强重建效率并实现标准化对象姿态估计', 'title_zh': '类别级元学习NERF先验用于高效物体映射'}
{'arxiv_id': 'arXiv:2503.01254', 'title': 'Convex Hull-based Algebraic Constraint for Visual Quadric SLAM', 'authors': 'Xiaolong Yu, Junqiao Zhao, Shuangfu Song, Zhongyang Zhu, Zihan Yuan, Chen Ye, Tiantian Feng', 'link': 'https://arxiv.org/abs/2503.01254', 'abstract': 'Using Quadrics as the object representation has the benefits of both generality and closed-form projection derivation between image and world spaces. Although numerous constraints have been proposed for dual quadric reconstruction, we found that many of them are imprecise and provide minimal improvements to this http URL scrutinizing the existing constraints, we introduce a concise yet more precise convex hull-based algebraic constraint for object landmarks, which is applied to object reconstruction, frontend pose estimation, and backend bundle this http URL constraint is designed to fully leverage precise semantic segmentation, effectively mitigating mismatches between complex-shaped object contours and dual this http URL on public datasets demonstrate that our approach is applicable to both monocular and RGB-D SLAM and achieves improved object mapping and localization than existing quadric SLAM methods. The implementation of our method is available at this https URL.', 'abstract_zh': '将双四面体重建中的对象表示用quadrics表示，兼具通用性和图像空间与世界空间闭式投影推导的优点。尽管为双四面体重建提出了众多约束条件，我们发现其中许多约束不够精确且提供的改进有限。通过对现有约束条件进行仔细分析，我们引入了一种更为精确且简洁的基于凸包的代数约束，将其应用于对象重建、前端姿态估计和后端 bundle 调整。该约束旨在充分利用精确的语义分割，有效缓解复杂形状对象轮廓与双四面体之间的不匹配问题。在公共数据集上的实验表明，我们的方法适用于单目和RGB-D SLAM，并在对象建图和定位方面优于现有四面体SLAM方法。我们的方法实现可从这个 <https://www.example.com> 获取。', 'title_zh': '基于凸包的代数约束视觉四次多项式SLAM'}
{'arxiv_id': 'arXiv:2503.01202', 'title': 'A Multi-Sensor Fusion Approach for Rapid Orthoimage Generation in Large-Scale UAV Mapping', 'authors': 'Jialei He, Zhihao Zhan, Zhituo Tu, Xiang Zhu, Jie Yuan', 'link': 'https://arxiv.org/abs/2503.01202', 'abstract': 'Rapid generation of large-scale orthoimages from Unmanned Aerial Vehicles (UAVs) has been a long-standing focus of research in the field of aerial mapping. A multi-sensor UAV system, integrating the Global Positioning System (GPS), Inertial Measurement Unit (IMU), 4D millimeter-wave radar and camera, can provide an effective solution to this problem. In this paper, we utilize multi-sensor data to overcome the limitations of conventional orthoimage generation methods in terms of temporal performance, system robustness, and geographic reference accuracy. A prior-pose-optimized feature matching method is introduced to enhance matching speed and accuracy, reducing the number of required features and providing precise references for the Structure from Motion (SfM) process. The proposed method exhibits robustness in low-texture scenes like farmlands, where feature matching is difficult. Experiments show that our approach achieves accurate feature matching orthoimage generation in a short time. The proposed drone system effectively aids in farmland detection and management.', 'abstract_zh': '基于多传感器的无人机系统快速生成大规模正射影像的研究', 'title_zh': '大规模无人机测绘中基于多传感器融合的快速正射影像生成方法'}
{'arxiv_id': 'arXiv:2503.01109', 'title': 'FGS-SLAM: Fourier-based Gaussian Splatting for Real-time SLAM with Sparse and Dense Map Fusion', 'authors': 'Yansong Xu, Junlin Li, Wei Zhang, Siyu Chen, Shengyong Zhang, Yuquan Leng, Weijia Zhou', 'link': 'https://arxiv.org/abs/2503.01109', 'abstract': '3D gaussian splatting has advanced simultaneous localization and mapping (SLAM) technology by enabling real-time positioning and the construction of high-fidelity maps. However, the uncertainty in gaussian position and initialization parameters introduces challenges, often requiring extensive iterative convergence and resulting in redundant or insufficient gaussian representations. To address this, we introduce a novel adaptive densification method based on Fourier frequency domain analysis to establish gaussian priors for rapid convergence. Additionally, we propose constructing independent and unified sparse and dense maps, where a sparse map supports efficient tracking via Generalized Iterative Closest Point (GICP) and a dense map creates high-fidelity visual representations. This is the first SLAM system leveraging frequency domain analysis to achieve high-quality gaussian mapping in real-time. Experimental results demonstrate an average frame rate of 36 FPS on Replica and TUM RGB-D datasets, achieving competitive accuracy in both localization and mapping.', 'abstract_zh': '3D 高斯 splatting 已经通过实现实时定位和构建高保真地图来推动同时定位与地图构建（SLAM）技术的发展。然而，高斯位置和初始化参数的不确定性带来了挑战，通常需要大量的迭代才能收敛，并导致高斯表示冗余或不足。为解决这一问题，我们提出了一种基于傅里叶频域分析的新型自适应加密方法，以建立高斯先验，实现快速收敛。此外，我们还提出构建独立统一的稀疏和密集地图，其中稀疏地图通过广义迭代最近点（GICP）支持高效的追踪，而密集地图创建高保真视觉表示。这是首个利用频域分析实现实时高质量化 Gauss 映射的 SLAM 系统。实验结果表明，在 Replica 和 TUM RGB-D 数据集上平均帧率为 36 FPS，同时在定位和建图方面达到竞争力的精度。', 'title_zh': '基于傅里叶变换的高斯点云SLAM：稀疏与密集地图融合的实时SLAM方法'}
{'arxiv_id': 'arXiv:2503.00803', 'title': 'HiMo: High-Speed Objects Motion Compensation in Point Clouds', 'authors': 'Qingwen Zhang, Ajinkya Khoche, Yi Yang, Li Ling, Sina Sharif Mansouri, Olov Andersson, Patric Jensfelt', 'link': 'https://arxiv.org/abs/2503.00803', 'abstract': 'LiDAR point clouds often contain motion-induced distortions, degrading the accuracy of object appearances in the captured data. In this paper, we first characterize the underlying reasons for the point cloud distortion and show that this is present in public datasets. We find that this distortion is more pronounced in high-speed environments such as highways, as well as in multi-LiDAR configurations, a common setup for heavy vehicles. Previous work has dealt with point cloud distortion from the ego-motion but fails to consider distortion from the motion of other objects. We therefore introduce a novel undistortion pipeline, HiMo, that leverages scene flow estimation for object motion compensation, correcting the depiction of dynamic objects. We further propose an extension of a state-of-the-art self-supervised scene flow method. Due to the lack of well-established motion distortion metrics in the literature, we also propose two metrics for compensation performance evaluation: compensation accuracy at a point level and shape similarity on objects. To demonstrate the efficacy of our method, we conduct extensive experiments on the Argoverse 2 dataset and a new real-world dataset. Our new dataset is collected from heavy vehicles equipped with multi-LiDARs and on highways as opposed to mostly urban settings in the existing datasets. The source code, including all methods and the evaluation data, will be provided upon publication. See this https URL for more details.', 'abstract_zh': 'LiDAR 点云中的运动引起的失真会降低捕获数据中物体外观的准确性。本文首先分析点云失真的根本原因，并表明这种失真存在于公共数据集中。我们发现，这种失真在高速公路等高速环境中以及多LiDAR配置中更为明显，这是重型车辆中常见的配置。以往的工作已经处理了来自自身运动的点云失真，但未能考虑到其他物体运动引起的失真。因此，我们引入了一种名为HiMo的新型解畸变流水线，该流水线利用场景流估计进行物体运动补偿，纠正动态物体的描述。此外，我们提出了最先进的自监督场景流方法的一个扩展。由于文献中缺乏成熟的运动失真评估指标，我们还提出了两种性能评估指标：基于点的补偿准确性和对象的形状相似度。为了证明我们方法的有效性，我们在Argoverse 2数据集和一个新的真实世界数据集上进行了广泛的实验。我们的新数据集源自配备了多LiDAR的重型车辆，并在高速公路上收集，而现有的数据集主要集中在城市环境中。论文发表后，将提供源代码，包括所有方法和评估数据。更多细节请见：见此链接。', 'title_zh': 'HiMo: 高速点云中物体运动补偿'}
{'arxiv_id': 'arXiv:2503.00793', 'title': 'Bridging Spectral-wise and Multi-spectral Depth Estimation via Geometry-guided Contrastive Learning', 'authors': 'Ukcheol Shin, Kyunghyun Lee, Jean Oh', 'link': 'https://arxiv.org/abs/2503.00793', 'abstract': 'Deploying depth estimation networks in the real world requires high-level robustness against various adverse conditions to ensure safe and reliable autonomy. For this purpose, many autonomous vehicles employ multi-modal sensor systems, including an RGB camera, NIR camera, thermal camera, LiDAR, or Radar. They mainly adopt two strategies to use multiple sensors: modality-wise and multi-modal fused inference. The former method is flexible but memory-inefficient, unreliable, and vulnerable. Multi-modal fusion can provide high-level reliability, yet it needs a specialized architecture. In this paper, we propose an effective solution, named align-and-fuse strategy, for the depth estimation from multi-spectral images. In the align stage, we align embedding spaces between multiple spectrum bands to learn shareable representation across multi-spectral images by minimizing contrastive loss of global and spatially aligned local features with geometry cue. After that, in the fuse stage, we train an attachable feature fusion module that can selectively aggregate the multi-spectral features for reliable and robust prediction results. Based on the proposed method, a single-depth network can achieve both spectral-invariant and multi-spectral fused depth estimation while preserving reliability, memory efficiency, and flexibility.', 'abstract_zh': '多光谱图像深度估计中的对齐与融合策略', 'title_zh': '基于几何引导对比学习的谱域与多谱深估计桥梁方法'}
{'arxiv_id': 'arXiv:2503.00747', 'title': 'Unifying Light Field Perception with Field of Parallax', 'authors': 'Fei Teng, Buyin Deng, Boyuan Zheng, Kai Luo, Kunyu Peng, Jiaming Zhang, Kailun Yang', 'link': 'https://arxiv.org/abs/2503.00747', 'abstract': 'Field of Parallax (FoP)}, a spatial field that distills the common features from different LF representations to provide flexible and consistent support for multi-task learning. FoP is built upon three core features--projection difference, adjacency divergence, and contextual consistency--which are essential for cross-task adaptability. To implement FoP, we design a two-step angular adapter: the first step captures angular-specific differences, while the second step consolidates contextual consistency to ensure robust representation. Leveraging the FoP-based representation, we introduce the LFX framework, the first to handle arbitrary LF representations seamlessly, unifying LF multi-task vision. We evaluated LFX across three different tasks, achieving new state-of-the-art results, compared with previous task-specific architectures: 84.74% in mIoU for semantic segmentation on UrbanLF, 0.84% in AP for object detection on PKU, and 0.030 in MAE and 0.026 in MAE for salient object detection on Duftv2 and PKU, respectively. The source code will be made publicly available at this https URL.', 'abstract_zh': '基于偏移场（FoP）的空间场：一种从不同超分辨率表示中提炼共性特征以提供灵活一致多任务学习支持的场。FoP基于投影差异、相邻差异和上下文一致性三种核心特征构建，这些特征对于跨任务适应性至关重要。为了实现FoP，我们设计了一种两步角度适配器：第一步捕捉角度特定差异，第二步 consolidates 上下文一致性以确保稳健的表示。利用基于FoP的表示，我们引入了LFX框架，这是首个能够无缝处理任意超分辨率表示、统一超分辨率多任务视觉的框架。我们在三个不同任务上评估了LFX，取得了之前任务特定架构的新最佳结果：在UrbanLF语义分割上的mIoU为84.74%，在PKU物体检测上的AP为0.84%，在Duftv2和PKU上的显著对象检测上的MAE分别为0.030和0.026。源代码将在以下网址公开： this https URL。', 'title_zh': '统一视差场与光场感知'}
{'arxiv_id': 'arXiv:2503.00675', 'title': 'Dur360BEV: A Real-world Single 360-degree Camera Dataset and Benchmark for Bird-Eye View Mapping in Autonomous Driving', 'authors': 'Wenke E, Chao Yuan, Li Li, Yixin Sun, Yona Falinie A. Gaus, Amir Atapour-Abarghouei, Toby P. Breckon', 'link': 'https://arxiv.org/abs/2503.00675', 'abstract': 'We present Dur360BEV, a novel spherical camera autonomous driving dataset equipped with a high-resolution 128-channel 3D LiDAR and a RTK-refined GNSS/INS system, along with a benchmark architecture designed to generate Bird-Eye-View (BEV) maps using only a single spherical camera. This dataset and benchmark address the challenges of BEV generation in autonomous driving, particularly by reducing hardware complexity through the use of a single 360-degree camera instead of multiple perspective cameras. Within our benchmark architecture, we propose a novel spherical-image-to-BEV (SI2BEV) module that leverages spherical imagery and a refined sampling strategy to project features from 2D to 3D. Our approach also includes an innovative application of Focal Loss, specifically adapted to address the extreme class imbalance often encountered in BEV segmentation tasks. Through extensive experiments, we demonstrate that this application of Focal Loss significantly improves segmentation performance on the Dur360BEV dataset. The results show that our benchmark not only simplifies the sensor setup but also achieves competitive performance.', 'abstract_zh': 'Dur360BEV：一种配备高分辨率128通道3D LiDAR和RTK校准的GNSS/INS系统的新型球形 cameras 自动驾驶数据集及单目球形图像到BEV地图生成基准架构', 'title_zh': 'Dur360BEV：用于自动驾驶中鸟瞰图建图的真实世界单个360度摄像头数据集及基准'}
{'arxiv_id': 'arXiv:2503.00167', 'title': 'EVLoc: Event-based Visual Localization in LiDAR Maps via Event-Depth Registration', 'authors': 'Kuangyi Chen, Jun Zhang, Friedrich Fraundorfer', 'link': 'https://arxiv.org/abs/2503.00167', 'abstract': 'Event cameras are bio-inspired sensors with some notable features, including high dynamic range and low latency, which makes them exceptionally suitable for perception in challenging scenarios such as high-speed motion and extreme lighting conditions. In this paper, we explore their potential for localization within pre-existing LiDAR maps, a critical task for applications that require precise navigation and mobile manipulation. Our framework follows a paradigm based on the refinement of an initial pose. Specifically, we first project LiDAR points into 2D space based on a rough initial pose to obtain depth maps, and then employ an optical flow estimation network to align events with LiDAR points in 2D space, followed by camera pose estimation using a PnP solver. To enhance geometric consistency between these two inherently different modalities, we develop a novel frame-based event representation that improves structural clarity. Additionally, given the varying degrees of bias observed in the ground truth poses, we design a module that predicts an auxiliary variable as a regularization term to mitigate the impact of this bias on network convergence. Experimental results on several public datasets demonstrate the effectiveness of our proposed method. To facilitate future research, both the code and the pre-trained models are made available online.', 'abstract_zh': '事件相机是受生物启发的传感器，具有高动态范围和低延迟等显著特点，使其在高速运动和极端光照等挑战性场景下的感知任务中表现出色。本文探讨了其在已有LiDAR地图中实现定位的潜在应用，这对于需要精确导航和移动操作的应用至关重要。我们的框架基于初始姿态 refinement 的 paradigm。具体而言，我们首先根据粗略的初始姿态将LiDAR点投影到二维空间以获取深度图，然后使用光流估计网络将事件与二维空间中的LiDAR点对齐，最后利用PnP求解器估计相机姿态。为了在这些本质上不同的模态之间增强几何一致性，我们开发了一种新颖的基于帧的事件表示方法，以提高结构清晰度。此外，由于观察到的地面真实姿态偏差程度不一，我们设计了一个模块来预测辅助变量作为正则项，以减轻这种偏差对网络收敛的影响。在多个公开数据集上的实验结果表明了我们所提出方法的有效性，并为未来研究提供了代码和预训练模型。', 'title_zh': 'EVLoc：基于事件深度注册的LiDAR地图中事件视觉定位'}
{'arxiv_id': 'arXiv:2503.00132', 'title': 'CNSv2: Probabilistic Correspondence Encoded Neural Image Servo', 'authors': 'Anzhe Chen, Hongxiang Yu, Shuxin Li, Yuxi Chen, Zhongxiang Zhou, Wentao Sun, Rong Xiong, Yue Wang', 'link': 'https://arxiv.org/abs/2503.00132', 'abstract': 'Visual servo based on traditional image matching methods often requires accurate keypoint correspondence for high precision control. However, keypoint detection or matching tends to fail in challenging scenarios with inconsistent illuminations or textureless objects, resulting significant performance degradation. Previous approaches, including our proposed Correspondence encoded Neural image Servo policy (CNS), attempted to alleviate these issues by integrating neural control strategies. While CNS shows certain improvement against error correspondence over conventional image-based controllers, it could not fully resolve the limitations arising from poor keypoint detection and matching. In this paper, we continue to address this problem and propose a new solution: Probabilistic Correspondence Encoded Neural Image Servo (CNSv2). CNSv2 leverages probabilistic feature matching to improve robustness in challenging scenarios. By redesigning the architecture to condition on multimodal feature matching, CNSv2 achieves high precision, improved robustness across diverse scenes and runs in real-time. We validate CNSv2 with simulations and real-world experiments, demonstrating its effectiveness in overcoming the limitations of detector-based methods in visual servo tasks.', 'abstract_zh': '基于传统图像匹配方法的视觉伺服往往需要准确的关键点对应以实现高精度控制。然而，在不一致光照或无纹理物体等具有挑战性的场景中，关键点检测或匹配往往会失败，导致性能显著下降。先前的方法，包括我们提出的编码对应神经图像伺服策略（CNS），试图通过集成神经控制策略来缓解这些问题。尽管CNS在传统基于图像的控制器上对错误对应具有一定的改善，但它无法完全解决由不良关键点检测和匹配引起的限制。在本文中，我们继续解决这一问题并提出一种新的解决方案：概率编码神经图像伺服（CNSv2）。CNSv2利用概率特征匹配以提高在具有挑战性场景中的鲁棒性。通过重新设计架构以基于多模态特征匹配进行条件处理，CNSv2实现了高精度、跨多种场景的增强鲁棒性并能在实时运行。我们通过模拟和真实world实验验证CNSv2，展示了其在视觉伺服任务中克服基于检测的方法限制的有效性。', 'title_zh': 'CNSv2: 聚合概率对应关系的神经图像伺服'}
{'arxiv_id': 'arXiv:2503.00051', 'title': 'Correspondence-Free Pose Estimation with Patterns: A Unified Approach for Multi-Dimensional Vision', 'authors': 'Quan Quan, Dun Dai', 'link': 'https://arxiv.org/abs/2503.00051', 'abstract': '6D pose estimation is a central problem in robot vision. Compared with pose estimation based on point correspondences or its robust versions, correspondence-free methods are often more flexible. However, existing correspondence-free methods often rely on feature representation alignment or end-to-end regression. For such a purpose, a new correspondence-free pose estimation method and its practical algorithms are proposed, whose key idea is the elimination of unknowns by process of addition to separate the pose estimation from correspondence. By taking the considered point sets as patterns, feature functions used to describe these patterns are introduced to establish a sufficient number of equations for optimization. The proposed method is applicable to nonlinear transformations such as perspective projection and can cover various pose estimations from 3D-to-3D points, 3D-to-2D points, and 2D-to-2D points. Experimental results on both simulation and actual data are presented to demonstrate the effectiveness of the proposed method.', 'abstract_zh': '无对应点的6D姿态估计是机器人视觉中的一个核心问题。与基于点对应或其稳健版本的姿态估计方法相比，无对应点的方法通常更为灵活。然而，现有的无对应点方法往往依赖于特征表示对齐或端到端回归。为此，提出了一种新的无对应点姿态估计方法及其实际算法，其关键思想是通过加法消除未知数，从而将姿态估计与对应分离。通过将考虑的点集视为模式，引入用于描述这些模式的特征函数以建立足够的方程进行优化。所提出的方法适用于如透视投影等非线性变换，并可以涵盖从3D到3D点、3D到2D点和2D到2D点的各种姿态估计。基于仿真和实际数据的实验结果证明了所提出方法的有效性。', 'title_zh': '无对应关系的位姿估计：一种多维视觉的统一方法'}
{'arxiv_id': 'arXiv:2503.01632', 'title': 'CoT-VLM4Tar: Chain-of-Thought Guided Vision-Language Models for Traffic Anomaly Resolution', 'authors': 'Tianchi Ren, Haibo Hu, Jiacheng Zuo, Xinhong Chen, Jianping Wang, Chun Jason Xue, Jen-Ming Wu, Nan Guan', 'link': 'https://arxiv.org/abs/2503.01632', 'abstract': 'With the acceleration of urbanization, modern urban traffic systems are becoming increasingly complex, leading to frequent traffic anomalies. These anomalies encompass not only common traffic jams but also more challenging issues such as phantom traffic jams, intersection deadlocks, and accident liability analysis, which severely impact traffic flow, vehicular safety, and overall transportation efficiency. Currently, existing solutions primarily rely on manual intervention by traffic police or artificial intelligence-based detection systems. However, these methods often suffer from response delays and inconsistent management due to inadequate resources, while AI detection systems, despite enhancing efficiency to some extent, still struggle to handle complex traffic anomalies in a real-time and precise manner. To address these issues, we propose CoT-VLM4Tar: (Chain of Thought Visual-Language Model for Traffic Anomaly Resolution), this innovative approach introduces a new chain-of-thought to guide the VLM in analyzing, reasoning, and generating solutions for traffic anomalies with greater reasonable and effective solution, and to evaluate the performance and effectiveness of our method, we developed a closed-loop testing framework based on the CARLA simulator. Furthermore, to ensure seamless integration of the solutions generated by the VLM with the CARLA simulator, we implement an itegration module that converts these solutions into executable commands. Our results demonstrate the effectiveness of VLM in the resolution of real-time traffic anomalies, providing a proof-of-concept for its integration into autonomous traffic management systems.', 'abstract_zh': '随着城市化的加速，现代城市交通系统变得越来越复杂，导致频繁出现交通异常。这些异常不仅包括常见的交通拥堵，还包括虚拟交通拥堵、交叉口死锁和事故责任分析等问题，严重影响了交通流、车辆安全和整体运输效率。目前，现有的解决方案主要依赖于交通警察的手动干预或基于人工智能的检测系统。然而，这些方法往往因资源不足而存在响应延迟和管理不一致的问题，虽然人工智能检测系统在一定程度上提高了效率，但在处理复杂的交通异常时依然难以实现实时和精确的管理。为了应对这些问题，我们提出了CoT-VLM4Tar：（基于链式思考的视觉语言模型用于交通异常解决），这一创新方法引入了一种新的链式思考来引导VLM对交通异常进行分析、推理和生成更合理有效的解决方案，并基于CARLA模拟器建立了闭环测试框架以评估我们方法的性能和有效性。此外，为了确保VLM生成的解决方案能够无缝集成到CARLA模拟器中，我们实现了一个集成模块，将这些解决方案转换为可执行命令。我们的结果证明了VLM在解决实时交通异常方面的有效性，为其集成到自动驾驶交通管理系统中提供了概念验证。', 'title_zh': 'Chain-of-Thought 引导的视觉-语言模型在交通异常解决中的应用'}
{'arxiv_id': 'arXiv:2503.01655', 'title': 'Enhancing Object Detection Accuracy in Underwater Sonar Images through Deep Learning-based Denoising', 'authors': 'Ziyu Wang, Tao Xue, Yanbin Wang, Jingyuan Li, Haibin Zhang, Zhiqiang Xu, Gaofei Xu', 'link': 'https://arxiv.org/abs/2503.01655', 'abstract': 'Sonar image object detection is crucial for underwater robotics and other applications. However, various types of noise in sonar images can affect the accuracy of object detection. Denoising, as a critical preprocessing step, aims to remove noise while retaining useful information to improve detection accuracy. Although deep learning-based denoising algorithms perform well on optical images, their application to underwater sonar images remains underexplored. This paper systematically evaluates the effectiveness of several deep learning-based denoising algorithms, originally designed for optical images, in the context of underwater sonar image object detection. We apply nine trained denoising models to images from five open-source sonar datasets, each processing different types of noise. We then test the denoised images using four object detection algorithms. The results show that different denoising models have varying effects on detection performance. By combining the strengths of multiple denoising models, the detection results can be optimized, thus more effectively suppressing noise. Additionally, we adopt a multi-frame denoising technique, using different outputs generated by multiple denoising models as multiple frames of the same scene for further processing to enhance detection accuracy. This method, originally designed for optical images, leverages complementary noise-reduction effects. Experimental results show that denoised sonar images improve the performance of object detection algorithms compared to the original sonar images.', 'abstract_zh': '基于深学习的声纳图像去噪算法在水下声纳图像目标检测中的有效性研究', 'title_zh': '基于深度学习的去噪方法提升水下声纳图像目标检测准确性'}
{'arxiv_id': 'arXiv:2503.01646', 'title': 'OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for Object-Level Scene Understanding', 'authors': 'Dianyi Yang, Yu Gao, Xihan Wang, Yufeng Yue, Yi Yang, Mengyin Fu', 'link': 'https://arxiv.org/abs/2503.01646', 'abstract': 'Recent advancements in 3D Gaussian Splatting have significantly improved the efficiency and quality of dense semantic SLAM. However, previous methods are generally constrained by limited-category pre-trained classifiers and implicit semantic representation, which hinder their performance in open-set scenarios and restrict 3D object-level scene understanding. To address these issues, we propose OpenGS-SLAM, an innovative framework that utilizes 3D Gaussian representation to perform dense semantic SLAM in open-set environments. Our system integrates explicit semantic labels derived from 2D foundational models into the 3D Gaussian framework, facilitating robust 3D object-level scene understanding. We introduce Gaussian Voting Splatting to enable fast 2D label map rendering and scene updating. Additionally, we propose a Confidence-based 2D Label Consensus method to ensure consistent labeling across multiple views. Furthermore, we employ a Segmentation Counter Pruning strategy to improve the accuracy of semantic scene representation. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of our method in scene understanding, tracking, and mapping, achieving 10 times faster semantic rendering and 2 times lower storage costs compared to existing methods. Project page: this https URL.', 'abstract_zh': 'Recent advancements in 3D Gaussian Splatting have significantly improved the efficiency and quality of dense semantic SLAM. However, previous methods are generally constrained by limited-category pre-trained classifiers and implicit semantic representation, which hinder their performance in open-set scenarios and restrict 3D object-level scene understanding. To address these issues, we propose OpenGS-SLAM, an innovative framework that utilizes 3D Gaussian representation to perform dense semantic SLAM in open-set environments. Our system integrates explicit semantic labels derived from 2D foundational models into the 3D Gaussian framework, facilitating robust 3D object-level scene understanding. We introduce Gaussian Voting Splatting to enable fast 2D label map rendering and scene updating. Additionally, we propose a Confidence-based 2D Label Consensus method to ensure consistent labeling across multiple views. Furthermore, we employ a Segmentation Counter Pruning strategy to improve the accuracy of semantic scene representation. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of our method in scene understanding, tracking, and mapping, achieving 10 times faster semantic rendering and 2 times lower storage costs compared to existing methods. Project page: [this https URL].', 'title_zh': 'OpenGS-SLAM: 基于3D高斯点云的开放集密集语义SLAM及其在对象级场景理解中的应用'}
{'arxiv_id': 'arXiv:2503.01603', 'title': 'Triple-Stream Deep Feature Selection with Metaheuristic Optimization and Machine Learning for Multi-Stage Hypertensive Retinopathy Diagnosis', 'authors': 'Suleyman Burcin Suyun, Mustafa Yurdakul, Sakir Tasdemir, Serkan Bilic', 'link': 'https://arxiv.org/abs/2503.01603', 'abstract': "Hypertensive retinopathy (HR) is a severe eye disease that may cause permanent vision loss if not diagnosed early. Traditional diagnostic methods are time-consuming and subjective, highlighting the need for an automated, reliable system. Existing studies often use a single Deep Learning (DL) model, struggling to distinguish HR stages. This study introduces a three-stage approach to enhance HR diagnosis accuracy. Initially, 14 CNN models were tested, identifying DenseNet169, MobileNet, and ResNet152 as the most effective. DenseNet169 achieved 87.73% accuracy, 87.75% precision, 87.73% recall, 87.67% F1-score, and 0.8359 Cohen's Kappa. MobileNet followed with 86.40% accuracy, 86.60% precision, 86.40% recall, 86.31% F1-score, and 0.8180 Cohen's Kappa. ResNet152 ranked third with 85.87% accuracy, 86.01% precision, 85.87% recall, 85.83% F1-score, and 0.8188 Cohen's Kappa. In the second stage, deep features from these models were fused and classified using Machine Learning (ML) algorithms (SVM, RF, XGBoost). SVM (sigmoid kernel) performed best with 92.00% accuracy, 91.93% precision, 92.00% recall, 91.91% F1-score, and 0.8930 Cohen's Kappa. The third stage applied meta-heuristic optimization (GA, ABC, PSO, HHO) for feature selection. HHO yielded 94.66% accuracy, precision, and recall, 94.64% F1-score, and 0.9286 Cohen's Kappa. The proposed approach surpassed single CNN models and previous studies in HR diagnosis accuracy and generalization.", 'abstract_zh': '高血压视网膜病变（HR）的三维诊断方法：提高诊断准确性和泛化能力', 'title_zh': '基于元启发式优化和机器学习的三流深特征选择多阶段高血压视网膜病变诊断'}
{'arxiv_id': 'arXiv:2503.01592', 'title': 'An Efficient Approach to Detecting Lung Nodules Using Swin Transformer', 'authors': 'Saeed Shakuri, Alireza Rezvanian', 'link': 'https://arxiv.org/abs/2503.01592', 'abstract': 'Lung cancer has the highest rate of cancer-caused deaths, and early-stage diagnosis could increase the survival rate. Lung nodules are common indicators of lung cancer, making their detection crucial. Various lung nodule detection models exist, but many lack efficiency. Hence, we propose a more efficient approach by leveraging 2D CT slices, reducing computational load and complexity in training and inference. We employ the tiny version of Swin Transformer to benefit from Vision Transformers (ViT) while maintaining low computational cost. A Feature Pyramid Network is added to enhance detection, particularly for small nodules. Additionally, Transfer Learning is used to accelerate training. Our experimental results show that the proposed model outperforms state-of-the-art methods, achieving higher mAP and mAR for small nodules by 1.3% and 1.6%, respectively. Overall, our model achieves the highest mAP of 94.7% and mAR of 94.9%.', 'abstract_zh': '肺癌是癌症致死率最高的疾病，早期诊断可以提高生存率。肺结节是肺癌的常见指标，因此其检测至关重要。存在多种肺结节检测模型，但许多模型缺乏效率。因此，我们提出了一种更高效的方法，通过利用2D CT切片，减少训练和推断的计算负担和复杂性。我们采用Swin Transformer的Tiny版本，利用Vision Transformers的优势同时保持低计算成本。添加了Feature Pyramid Network以增强检测，特别是对小结节的检测。此外，使用迁移学习加速训练。实验结果表明，所提出模型优于现有方法，对于小结节分别提高mAP和mAR性能1.3%和1.6%。总体而言，我们的模型实现了最高的mAP为94.7%和mAR为94.9%。', 'title_zh': '基于Swin Transformer的肺结节检测高效方法'}
{'arxiv_id': 'arXiv:2503.01306', 'title': 'From Claims to Evidence: A Unified Framework and Critical Analysis of CNN vs. Transformer vs. Mamba in Medical Image Segmentation', 'authors': 'Pooya Mohammadi Kazaj, Giovanni Baj, Yazdan Salimi, Anselm W. Stark, Waldo Valenzuela, George CM. Siontis, Habib Zaidi, Mauricio Reyes, Christoph Graeni, Isaac Shiri', 'link': 'https://arxiv.org/abs/2503.01306', 'abstract': 'While numerous architectures for medical image segmentation have been proposed, achieving competitive performance with state-of-the-art models networks such as nnUNet, still leave room for further innovation. In this work, we introduce nnUZoo, an open source benchmarking framework built upon nnUNet, which incorporates various deep learning architectures, including CNNs, Transformers, and Mamba-based models. Using this framework, we provide a fair comparison to demystify performance claims across different medical image segmentation tasks. Additionally, in an effort to enrich the benchmarking, we explored five new architectures based on Mamba and Transformers, collectively named X2Net, and integrated them into nnUZoo for further evaluation. The proposed models combine the features of conventional U2Net, nnUNet, CNN, Transformer, and Mamba layers and architectures, called X2Net (UNETR2Net (UNETR), SwT2Net (SwinTransformer), SS2D2Net (SwinUMamba), Alt1DM2Net (LightUMamba), and MambaND2Net (MambaND)). We extensively evaluate the performance of different models on six diverse medical image segmentation datasets, including microscopy, ultrasound, CT, MRI, and PET, covering various body parts, organs, and labels. We compare their performance, in terms of dice score and computational efficiency, against their baseline models, U2Net, and nnUNet. CNN models like nnUNet and U2Net demonstrated both speed and accuracy, making them effective choices for medical image segmentation tasks. Transformer-based models, while promising for certain imaging modalities, exhibited high computational costs. Proposed Mamba-based X2Net architecture (SS2D2Net) achieved competitive accuracy with no significantly difference from nnUNet and U2Net, while using fewer parameters. However, they required significantly longer training time, highlighting a trade-off between model efficiency and computational cost.', 'abstract_zh': '虽然已经提出了多种医疗图像分割架构，但与最先进的模型网络（如nnUNet）相比，仍存在进一步创新的空间。在这项工作中，我们引入了nnUZoo，这是一个基于nnUNet构建的开源基准框架，集成了包括CNN、Transformers和Mamba基模型在内的各种深度学习架构。通过该框架，我们提供了一次公平比较，以澄清不同医疗图像分割任务中性能声明的分歧。此外，为了丰富基准测试，我们探索了基于Mamba和Transformers的五种新架构，并将它们统称为X2Net，整合到nnUZoo中进行进一步评估。提出的模型结合了传统U2Net、nnUNet、CNN、Transformer和Mamba层及架构的特点，统称为X2Net（UNETR2Net（UNETR）、SwT2Net（SwinTransformer）、SS2D2Net（SwinUMamba）、Alt1DM2Net（LightUMamba）、MambaND2Net（MambaND）。我们对六种不同的医疗图像分割数据集进行了广泛评估，涵盖了显微镜、超声、CT、MRI和PET，涉及不同身体部位、器官和标签。我们比较了这些模型在Dice分数和计算效率方面的性能，与基线模型U2Net和nnUNet进行对比。像nnUNet和U2Net这样的CNN模型展示了速度和准确性的良好平衡，使其成为医疗图像分割任务的有效选择。基于Transformers的模型虽然对某些成像模态具有前景，但表现出较高的计算成本。基于Mamba的X2Net架构（SS2D2Net）实现了竞争力的准确度，与nnUNet和U2Net相比，参数量更少，但需要显著更长的训练时间，凸显了模型效率与计算成本之间的权衡。', 'title_zh': '从声明到证据：医学图像分割中CNN、Transformer和Mamba的统一框架与批判性分析'}
{'arxiv_id': 'arXiv:2503.01294', 'title': 'Fine-Grained Controllable Apparel Showcase Image Generation via Garment-Centric Outpainting', 'authors': 'Rong Zhang, Jingnan Wang, Zhiwen Zuo, Jianfeng Dong, Wei Li, Chi Wang, Weiwei Xu, Xun Wang', 'link': 'https://arxiv.org/abs/2503.01294', 'abstract': "In this paper, we propose a novel garment-centric outpainting (GCO) framework based on the latent diffusion model (LDM) for fine-grained controllable apparel showcase image generation. The proposed framework aims at customizing a fashion model wearing a given garment via text prompts and facial images. Different from existing methods, our framework takes a garment image segmented from a dressed mannequin or a person as the input, eliminating the need for learning cloth deformation and ensuring faithful preservation of garment details. The proposed framework consists of two stages. In the first stage, we introduce a garment-adaptive pose prediction model that generates diverse poses given the garment. Then, in the next stage, we generate apparel showcase images, conditioned on the garment and the predicted poses, along with specified text prompts and facial images. Notably, a multi-scale appearance customization module (MS-ACM) is designed to allow both overall and fine-grained text-based control over the generated model's appearance. Moreover, we leverage a lightweight feature fusion operation without introducing any extra encoders or modules to integrate multiple conditions, which is more efficient. Extensive experiments validate the superior performance of our framework compared to state-of-the-art methods.", 'abstract_zh': '基于潜在扩散模型的服装中心扩展框架：精细可控的服装展示图像生成', 'title_zh': '基于衣物中心的扩展生成精细可控的服装展示图像'}
{'arxiv_id': 'arXiv:2503.01144', 'title': 'One-shot In-context Part Segmentation', 'authors': 'Zhenqi Dai, Ting Liu, Xingxing Zhang, Yunchao Wei, Yanning Zhang', 'link': 'https://arxiv.org/abs/2503.01144', 'abstract': "In this paper, we present the One-shot In-context Part Segmentation (OIParts) framework, designed to tackle the challenges of part segmentation by leveraging visual foundation models (VFMs). Existing training-based one-shot part segmentation methods that utilize VFMs encounter difficulties when faced with scenarios where the one-shot image and test image exhibit significant variance in appearance and perspective, or when the object in the test image is partially visible. We argue that training on the one-shot example often leads to overfitting, thereby compromising the model's generalization capability. Our framework offers a novel approach to part segmentation that is training-free, flexible, and data-efficient, requiring only a single in-context example for precise segmentation with superior generalization ability. By thoroughly exploring the complementary strengths of VFMs, specifically DINOv2 and Stable Diffusion, we introduce an adaptive channel selection approach by minimizing the intra-class distance for better exploiting these two features, thereby enhancing the discriminatory power of the extracted features for the fine-grained parts. We have achieved remarkable segmentation performance across diverse object categories. The OIParts framework not only eliminates the need for extensive labeled data but also demonstrates superior generalization ability. Through comprehensive experimentation on three benchmark datasets, we have demonstrated the superiority of our proposed method over existing part segmentation approaches in one-shot settings.", 'abstract_zh': '基于视觉基础模型的一次性上下文部分分割框架：OIParts', 'title_zh': '单次情境内部分段'}
{'arxiv_id': 'arXiv:2503.01100', 'title': 'Fence Theorem: Preprocessing is Dual-Objective Semantic Structure Isolator in 3D Anomaly Detection', 'authors': 'Hanzhe Liang, Jie Zhou, Xuanxin Chen, Jinbao Wang, Can Gao', 'link': 'https://arxiv.org/abs/2503.01100', 'abstract': "3D anomaly detection (AD) is prominent but difficult due to lacking a unified theoretical foundation for preprocessing design. We establish the Fence Theorem, formalizing preprocessing as a dual-objective semantic isolator: (1) mitigating cross-semantic interference to the greatest extent feasible and (2) confining anomaly judgments to aligned semantic spaces wherever viable, thereby establishing intra-semantic comparability. Any preprocessing approach achieves this goal through a two-stage process of Emantic-Division and Spatial-Constraints stage. Through systematic deconstruction, we theoretically and experimentally subsume existing preprocessing methods under this theorem via tripartite evidence: qualitative analyses, quantitative studies, and mathematical proofs. Guided by the Fence Theorem, we implement Patch3D, consisting of Patch-Cutting and Patch-Matching modules, to segment semantic spaces and consolidate similar ones while independently modeling normal features within each space. Experiments on Anomaly-ShapeNet and Real3D-AD with different settings demonstrate that progressively finer-grained semantic alignment in preprocessing directly enhances point-level AD accuracy, providing inverse validation of the theorem's causal logic.", 'abstract_zh': '3D异常检测的预处理设计缺乏统一的理论基础，因此颇具挑战性。我们建立了篱笆定理，将预处理形式化为一种双目标语义隔离器：（1）最大限度地减轻跨语义干扰；（2）在可行的情况下将异常判断限制在对齐的语义空间中，从而建立同语义内的可比性。任何预处理方法均通过语义分割和空间约束的两阶段过程来实现这一目标。通过系统的分解，我们通过定性分析、定量研究和数学证明三种证据，理论上和实验性地将现有的预处理方法归置于这一定理之下。遵循篱笆定理的指导，我们实现了Patch3D，包含Patch-Cutting和Patch-Matching模块，用于分割语义空间并将相似的空间合并，同时独立建模每个空间内的正常特征。在不同设置下的Anomaly-ShapeNet和Real3D-AD实验中表明，预处理中逐渐细化的语义对齐直接提升了点级异常检测的准确性，这间接验证了定理因果逻辑的有效性。', 'title_zh': '围栏定理：预处理是三维异常检测中的双目标语义结构隔离器'}
{'arxiv_id': 'arXiv:2503.01075', 'title': 'Tackling Hallucination from Conditional Models for Medical Image Reconstruction with DynamicDPS', 'authors': 'Seunghoi Kim, Henry F. J. Tregidgo, Matteo Figini, Chen Jin, Sarang Joshi, Daniel C. Alexander', 'link': 'https://arxiv.org/abs/2503.01075', 'abstract': "Hallucinations are spurious structures not present in the ground truth, posing a critical challenge in medical image reconstruction, especially for data-driven conditional models. We hypothesize that combining an unconditional diffusion model with data consistency, trained on a diverse dataset, can reduce these hallucinations. Based on this, we propose DynamicDPS, a diffusion-based framework that integrates conditional and unconditional diffusion models to enhance low-quality medical images while systematically reducing hallucinations. Our approach first generates an initial reconstruction using a conditional model, then refines it with an adaptive diffusion-based inverse problem solver. DynamicDPS skips early stage in the reverse process by selecting an optimal starting time point per sample and applies Wolfe's line search for adaptive step sizes, improving both efficiency and image fidelity. Using diffusion priors and data consistency, our method effectively reduces hallucinations from any conditional model output. We validate its effectiveness in Image Quality Transfer for low-field MRI enhancement. Extensive evaluations on synthetic and real MR scans, including a downstream task for tissue volume estimation, show that DynamicDPS reduces hallucinations, improving relative volume estimation by over 15% for critical tissues while using only 5% of the sampling steps required by baseline diffusion models. As a model-agnostic and fine-tuning-free approach, DynamicDPS offers a robust solution for hallucination reduction in medical imaging. The code will be made publicly available upon publication.", 'abstract_zh': '幻觉是 ground truth 中不存在的虚假结构，对医学图像重建构成了严峻挑战，尤其是在数据驱动的条件模型中。我们假设将无条件扩散模型与数据一致性结合，并在多样化的数据集上训练，可以减少这些幻觉。基于此，我们提出了一种基于扩散的 DynamicDPS 框架，该框架结合了条件和无条件扩散模型，以提升低质量医学图像的同时系统地减少幻觉。该方法首先使用条件模型生成初始重建，然后用自适应扩散基逆问题求解器对其进行细化。DynamicDPS 通过为每个样本选择最优的起始时间点跳过逆过程中早期阶段，并采用沃尔夫线搜索方法进行自适应步长调整，从而提高效率和图像保真度。利用扩散先验和数据一致性，该方法有效地减少了任何条件模型输出的幻觉。我们在低场 MRI 增强中的图像质量转移任务中验证了其有效性。在合成和真实 MRI 扫描的广泛评估中，包括用于组织体积估算的下游任务，结果显示 DynamicDPS 减少了幻觉，对于关键组织的相对体积估算提高了超过 15%，同时仅使用了基本扩散模型所需采样步骤的 5%。作为一种模型无关且无需微调的方法，DynamicDPS 提供了一种在医学成像中减少幻觉的稳健解决方案。代码将在发表后公开。', 'title_zh': '基于动态DPS的条件模型在医疗图像重建中应对幻觉问题'}
{'arxiv_id': 'arXiv:2503.00881', 'title': 'Evolving High-Quality Rendering and Reconstruction in a Unified Framework with Contribution-Adaptive Regularization', 'authors': 'You Shen, Zhipeng Zhang, Xinyang Li, Yansong Qu, Yu Lin, Shengchuan Zhang, Liujuan Cao', 'link': 'https://arxiv.org/abs/2503.00881', 'abstract': 'Representing 3D scenes from multiview images is a core challenge in computer vision and graphics, which requires both precise rendering and accurate reconstruction. Recently, 3D Gaussian Splatting (3DGS) has garnered significant attention for its high-quality rendering and fast inference speed. Yet, due to the unstructured and irregular nature of Gaussian point clouds, ensuring accurate geometry reconstruction remains difficult. Existing methods primarily focus on geometry regularization, with common approaches including primitive-based and dual-model frameworks. However, the former suffers from inherent conflicts between rendering and reconstruction, while the latter is computationally and storage-intensive. To address these challenges, we propose CarGS, a unified model leveraging Contribution-adaptive regularization to achieve simultaneous, high-quality rendering and surface reconstruction. The essence of our framework is learning adaptive contribution for Gaussian primitives by squeezing the knowledge from geometry regularization into a compact MLP. Additionally, we introduce a geometry-guided densification strategy with clues from both normals and Signed Distance Fields (SDF) to improve the capability of capturing high-frequency details. Our design improves the mutual learning of the two tasks, meanwhile its unified structure does not require separate models as in dual-model based approaches, guaranteeing efficiency. Extensive experiments demonstrate the ability to achieve state-of-the-art (SOTA) results in both rendering fidelity and reconstruction accuracy while maintaining real-time speed and minimal storage size.', 'abstract_zh': '从多视角图像表示3D场景：基于贡献适应正则化的一体化模型', 'title_zh': '统一框架中的贡献自适应正则化以促进高质量渲染和重建'}
{'arxiv_id': 'arXiv:2503.00853', 'title': 'MTReD: 3D Reconstruction Dataset for Fly-over Videos of Maritime Domain', 'authors': 'Rui Yi Yong, Samuel Picosson, Arnold Wiliem', 'link': 'https://arxiv.org/abs/2503.00853', 'abstract': 'This work tackles 3D scene reconstruction for a video fly-over perspective problem in the maritime domain, with a specific emphasis on geometrically and visually sound reconstructions. This will allow for downstream tasks such as segmentation, navigation, and localization. To our knowledge, there is no dataset available in this domain. As such, we propose a novel maritime 3D scene reconstruction benchmarking dataset, named as MTReD (Maritime Three-Dimensional Reconstruction Dataset). The MTReD comprises 19 fly-over videos curated from the Internet containing ships, islands, and coastlines. As the task is aimed towards geometrical consistency and visual completeness, the dataset uses two metrics: (1) Reprojection error; and (2) Perception based metrics. We find that existing perception-based metrics, such as Learned Perceptual Image Patch Similarity (LPIPS), do not appropriately measure the completeness of a reconstructed image. Thus, we propose a novel semantic similarity metric utilizing DINOv2 features coined DiFPS (DinoV2 Features Perception Similarity). We perform initial evaluation on two baselines: (1) Structured from Motion (SfM) through Colmap; and (2) the recent state-of-the-art MASt3R model. We find that the reconstructed scenes by MASt3R have higher reprojection errors, but superior perception based metric scores. To this end, some pre-processing methods are explored, and we find a pre-processing method which improves both the reprojection error and perception-based score. We envisage our proposed MTReD to stimulate further research in these directions. The dataset and all the code will be made available in this https URL.', 'abstract_zh': 'This工作解决了海洋领域视频俯瞰视角下的3D场景重建问题，特别强调了几何和视觉上合理的重建。这将有助于后续任务如分割、导航和定位。据我们所知，该领域没有可用的数据集。因此，我们提出了一种新的海洋3D场景重建基准数据集，命名为MTReD（Maritime Three-Dimensional Reconstruction Dataset）。MTReD包含19个从互联网精选的俯瞰视频，含有船只、岛屿和海岸线。由于任务旨在实现几何一致性和视觉完整性，该数据集使用了两种度量标准：（1）重投影误差；（2）基于感知的度量标准。我们发现现有的基于感知的度量标准，如Learned Perceptual Image Patch Similarity (LPIPS)，不能适当衡量重建图像的完整性。因此，我们提出了一种新的语义相似度度量标准，利用DINOv2特征称为DiFPS（DINOv2特征感知相似度）。我们在两种基线方法上进行了初步评估：（1）通过Colmap的结构从运动（SfM）方法；（2）最近的最先进的MASt3R模型。我们发现MASt3R重建的场景具有更高的重投影误差，但感知基于度量标准得分更高。为此，我们探索了一些预处理方法，并发现一种预处理方法可以同时提高重投影误差和感知基于得分。我们期望提出的MTReD能激发该领域的进一步研究。数据集和所有代码将在以下链接处提供。', 'title_zh': 'MTReD: 大洋领海飞越视频的3D重建数据集'}
{'arxiv_id': 'arXiv:2503.00780', 'title': 'Enhanced Multi-Class Classification of Gastrointestinal Endoscopic Images with Interpretable Deep Learning Model', 'authors': 'Astitva Kamble, Vani Bandodkar, Saakshi Dharmadhikary, Veena Anand, Pradyut Kumar Sanki, Mei X. Wu, Biswabandhu Jana', 'link': 'https://arxiv.org/abs/2503.00780', 'abstract': 'Endoscopy serves as an essential procedure for evaluating the gastrointestinal (GI) tract and plays a pivotal role in identifying GI-related disorders. Recent advancements in deep learning have demonstrated substantial progress in detecting abnormalities through intricate models and data augmentation this http URL research introduces a novel approach to enhance classification accuracy using 8,000 labeled endoscopic images from the Kvasir dataset, categorized into eight distinct classes. Leveraging EfficientNetB3 as the backbone, the proposed architecture eliminates reliance on data augmentation while preserving moderate model complexity. The model achieves a test accuracy of 94.25%, alongside precision and recall of 94.29% and 94.24% respectively. Furthermore, Local Interpretable Model-agnostic Explanation (LIME) saliency maps are employed to enhance interpretability by defining critical regions in the images that influenced model predictions. Overall, this work highlights the importance of AI in advancing medical imaging by combining high classification accuracy with interpretability.', 'abstract_zh': '内镜检查作为评估消化道（GI）的重要程序，在识别GI相关疾病中发挥着关键作用。近年来，深度学习的进步通过复杂的模型和数据增强在检测异常方面取得了显著进展。本研究引入了一种新颖的方法，使用来自Kvasir数据集的8,000张标注内镜图像，分类为八个不同的类别，以提高分类准确性。利用EfficientNetB3作为骨干网络，所提出的方法消除了对数据增强的依赖，同时保持了适度的模型复杂度。该模型在测试集上的准确率为94.25%，精确率为94.29%，召回率为94.24%。此外，使用局部可解释的模型无关解释（LIME）显著图来增强可解释性，通过定义影响模型预测的关键图像区域。总体而言，这项工作突显了AI在结合高分类准确性和可解释性方面推动医学影像的发展的重要性。', 'title_zh': '具有可解释性的深度学习模型增强的多类胃肠道内镜图像分类'}
{'arxiv_id': 'arXiv:2503.00726', 'title': 'Enhancing Monocular 3D Scene Completion with Diffusion Model', 'authors': 'Changlin Song, Jiaqi Wang, Liyun Zhu, He Weng', 'link': 'https://arxiv.org/abs/2503.00726', 'abstract': '3D scene reconstruction is essential for applications in virtual reality, robotics, and autonomous driving, enabling machines to understand and interact with complex environments. Traditional 3D Gaussian Splatting techniques rely on images captured from multiple viewpoints to achieve optimal performance, but this dependence limits their use in scenarios where only a single image is available. In this work, we introduce FlashDreamer, a novel approach for reconstructing a complete 3D scene from a single image, significantly reducing the need for multi-view inputs. Our approach leverages a pre-trained vision-language model to generate descriptive prompts for the scene, guiding a diffusion model to produce images from various perspectives, which are then fused to form a cohesive 3D reconstruction. Extensive experiments show that our method effectively and robustly expands single-image inputs into a comprehensive 3D scene, extending monocular 3D reconstruction capabilities without further training. Our code is available this https URL.', 'abstract_zh': '单张图像的完整三维场景重建对于虚拟现实、机器人技术和自动驾驶的应用至关重要，能够使机器理解并交互于复杂的环境。传统的三维高斯点云技术依赖于从多个视角捕获的图像以实现最佳性能，但在仅有一张图像可用的场景中，这种方法的使用受限。在这种工作中，我们提出了FlashDreamer，一种从单张图像重建完整三维场景的新型方法，显著减少了对多视图输入的依赖。我们的方法利用预训练的视觉-语言模型生成描述性的场景提示，指导扩散模型生成不同视角的图像，然后融合这些图像形成统一的三维重建。 extensive实验表明，我们的方法有效地且稳健地将单张图像扩展为完整的三维场景，无需进一步训练即可扩展单目三维重建能力。我们的代码可在此处访问：https://xxx/', 'title_zh': '基于扩散模型增强单目3D场景完成'}
{'arxiv_id': 'arXiv:2503.00495', 'title': 'Towards High-fidelity 3D Talking Avatar with Personalized Dynamic Texture', 'authors': 'Xuanchen Li, Jianyu Wang, Yuhao Cheng, Yikun Zeng, Xingyu Ren, Wenhan Zhu, Weiming Zhao, Yichao Yan', 'link': 'https://arxiv.org/abs/2503.00495', 'abstract': 'Significant progress has been made for speech-driven 3D face animation, but most works focus on learning the motion of mesh/geometry, ignoring the impact of dynamic texture. In this work, we reveal that dynamic texture plays a key role in rendering high-fidelity talking avatars, and introduce a high-resolution 4D dataset \\textbf{TexTalk4D}, consisting of 100 minutes of audio-synced scan-level meshes with detailed 8K dynamic textures from 100 subjects. Based on the dataset, we explore the inherent correlation between motion and texture, and propose a diffusion-based framework \\textbf{TexTalker} to simultaneously generate facial motions and dynamic textures from speech. Furthermore, we propose a novel pivot-based style injection strategy to capture the complicity of different texture and motion styles, which allows disentangled control. TexTalker, as the first method to generate audio-synced facial motion with dynamic texture, not only outperforms the prior arts in synthesising facial motions, but also produces realistic textures that are consistent with the underlying facial movements. Project page: this https URL.', 'abstract_zh': '基于语音的3D人脸动画在显著进步的基础上，大多数工作侧重于学习网格/几何的运动，而忽视了动态纹理的影响。在本工作中，我们揭示了动态纹理在渲染高保真 Talking Avatar 中起着关键作用，并引入了一个高分辨率4D数据集 TexTalk4D，该数据集包含100分钟与音频同步的扫描级网格，以及来自100个主体的详细8K动态纹理。基于该数据集，我们探索了运动与纹理之间的内在关联，并提出了一种基于扩散的框架 TexTalker，用于从语音同时生成面部运动和动态纹理。此外，我们提出了一种新颖的枢轴基风格注入策略，以捕捉不同纹理和运动风格的复杂性，从而实现分离的控制。作为首个生成与动态纹理同步的面部运动的方法，TexTalker 不仅在合成面部运动方面优于现有方法，还能生成与面部运动一致的现实纹理。项目页面: this https URL。', 'title_zh': '面向高保真3D互动Avatar的个性化动态纹理研究'}
{'arxiv_id': 'arXiv:2503.00384', 'title': 'A Survey of Adversarial Defenses in Vision-based Systems: Categorization, Methods and Challenges', 'authors': 'Nandish Chattopadhyay, Abdul Basit, Bassem Ouni, Muhammad Shafique', 'link': 'https://arxiv.org/abs/2503.00384', 'abstract': 'Adversarial attacks have emerged as a major challenge to the trustworthy deployment of machine learning models, particularly in computer vision applications. These attacks have a varied level of potency and can be implemented in both white box and black box approaches. Practical attacks include methods to manipulate the physical world and enforce adversarial behaviour by the corresponding target neural network models. Multiple different approaches to mitigate different kinds of such attacks are available in the literature, each with their own advantages and limitations. In this survey, we present a comprehensive systematization of knowledge on adversarial defenses, focusing on two key computer vision tasks: image classification and object detection. We review the state-of-the-art adversarial defense techniques and categorize them for easier comparison. In addition, we provide a schematic representation of these categories within the context of the overall machine learning pipeline, facilitating clearer understanding and benchmarking of defenses. Furthermore, we map these defenses to the types of adversarial attacks and datasets where they are most effective, offering practical insights for researchers and practitioners. This study is necessary for understanding the scope of how the available defenses are able to address the adversarial threats, and their shortcomings as well, which is necessary for driving the research in this area in the most appropriate direction, with the aim of building trustworthy AI systems for regular practical use-cases.', 'abstract_zh': 'adversarial 攻击已成为机器学习模型可靠部署的重大挑战，特别是在计算机视觉应用中。这些攻击具有不同的效力水平，并且可以在白盒和黑盒方法中实施。实用的攻击包括用于操控物理世界并强制相应目标神经网络模型表现出对抗行为的方法。文献中提供了多种不同的方法来减轻不同类型的此类攻击，每种方法均有其自身的优势和局限性。在本文综述中，我们对对抗防御的知识进行了综合整理，重点关注两类关键的计算机视觉任务：图像分类和对象检测。我们回顾了最先进的对抗防御技术，并对其进行分类以便于比较。此外，我们以整个机器学习流程为背景提供了这些分类的示意表示，有助于更好地理解和基准测试防御方法。此外，我们将这些防御与它们在最有效的攻击类型和数据集中进行映射，为研究人员和从业者提供了实用的见解。对于理解现有防御措施能够如何应对对抗威胁及其局限性，以及引导该领域研究方向，以构建适用于常规实际应用场景的可信人工智能系统，本研究是必要的。', 'title_zh': '基于视觉系统的对抗防御综述：分类、方法与挑战'}
{'arxiv_id': 'arXiv:2503.00366', 'title': 'AI-Augmented Thyroid Scintigraphy for Robust Classification', 'authors': 'Maziar Sabouri, Ghasem Hajianfar, Alireza Rafiei Sardouei, Milad Yazdani, Azin Asadzadeh, Soroush Bagheri, Mohsen Arabi, Seyed Rasoul Zakavi, Emran Askari, Atena Aghaee, Dena Shahriari, Habib Zaidi, Arman Rahmim', 'link': 'https://arxiv.org/abs/2503.00366', 'abstract': 'Thyroid scintigraphy is a key imaging modality for diagnosing thyroid disorders. Deep learning models for thyroid scintigraphy classification often face challenges due to limited and imbalanced datasets, leading to suboptimal generalization. In this study, we investigate the effectiveness of different data augmentation techniques including Stable Diffusion (SD), Flow Matching (FM), and Conventional Augmentation (CA) to enhance the performance of a ResNet18 classifier for thyroid condition classification. Our results showed that FM-based augmentation consistently outperforms SD-based approaches, particularly when combined with original (O) data and CA (O+FM+CA), achieving both high accuracy and fair classification across Diffuse Goiter (DG), Nodular Goiter (NG), Normal (NL), and Thyroiditis (TI) cases. The Wilcoxon statistical analysis further validated the superiority of O+FM and its variants (O+FM+CA) over SD-based augmentations in most scenarios. These findings highlight the potential of FM-based augmentation as a superior approach for generating high-quality synthetic thyroid scintigraphy images and improving model generalization in medical image classification.', 'abstract_zh': '甲状腺显像是一种用于诊断甲状腺疾病的关键成像技术。针对甲状腺显像分类的深度学习模型常常面临有限且不平衡的数据集挑战，导致泛化性能不佳。本研究探讨了包括稳定扩散(SD)、流匹配(FM)和传统增强(CA)在内的不同数据增强技术的有效性，以提高ResNet18分类器在甲状腺状况分类中的性能。结果显示，基于流匹配的增强方法始终优于基于稳定扩散的方法，尤其是在与原始数据和传统增强结合使用时(O+FM+CA)，能够实现高准确性和公平分类，适用于弥漫性甲状腺肿(DG)、结节性甲状腺肿(NG)、正常(NL)和甲状腺炎(TI)等情况。Wilcoxon统计分析进一步证实了在大多数情况下，基于原始数据和流匹配增强(O+FM)及其变种(O+FM+CA)优于基于稳定扩散的增强方法。这些发现突显了基于流匹配的增强方法在生成高质量合成甲状腺显像图像及提升医学图像分类模型泛化性能方面的潜在优势。', 'title_zh': 'AI增强甲状腺闪烁成像的稳健分类'}
{'arxiv_id': 'arXiv:2503.00196', 'title': 'PRISM: High-Resolution & Precise Counterfactual Medical Image Generation using Language-guided Stable Diffusion', 'authors': 'Amar Kumar, Anita Kriz, Mohammad Havaei, Tal Arbel', 'link': 'https://arxiv.org/abs/2503.00196', 'abstract': 'Developing reliable and generalizable deep learning systems for medical imaging faces significant obstacles due to spurious correlations, data imbalances, and limited text annotations in datasets. Addressing these challenges requires architectures robust to the unique complexities posed by medical imaging data. The rapid advancements in vision-language foundation models within the natural image domain prompt the question of how they can be adapted for medical imaging tasks. In this work, we present PRISM, a framework that leverages foundation models to generate high-resolution, language-guided medical image counterfactuals using Stable Diffusion. Our approach demonstrates unprecedented precision in selectively modifying spurious correlations (the medical devices) and disease features, enabling the removal and addition of specific attributes while preserving other image characteristics. Through extensive evaluation, we show how PRISM advances counterfactual generation and enables the development of more robust downstream classifiers for clinically deployable solutions. To facilitate broader adoption and research, we make our code publicly available at this https URL.', 'abstract_zh': '医学成像中开发可靠且可泛化的深度学习系统面临着显著挑战，这些挑战源于虚假相关性、数据失衡和数据集中的有限文本注释。克服这些挑战需要能够应对医学成像数据独特复杂性的稳健架构。在自然图像领域中视觉-语言基础模型的快速发展促使我们思考如何将这些模型适应到医学成像任务中。在本工作中，我们提出了一种名为PRISM的框架，该框架利用基础模型通过稳定扩散生成高分辨率的语言引导医学图像反事实。我们的方法在精确修改虚假相关性（医学设备）和疾病特征方面展现了前所未有的精度，能够删除和添加特定属性同时保留其他图像特征。通过广泛评估，我们展示了PRISM如何推进反事实生成并促进更稳健的下游分类器的发展，以支持临床部署解决方案。为了促进更广泛的采用和研究，我们在以下网址公开了我们的代码：这个 https URL。', 'title_zh': 'PRISM: 语言引导的稳定扩散高分辨率精准逆向医疗图像生成'}
{'arxiv_id': 'arXiv:2503.00159', 'title': "EXACT-CT: EXplainable Analysis for Crohn's and Tuberculosis using CT", 'authors': 'Shashwat Gupta, Sarthak Gupta, Akshan Agrawal, Mahim Naaz, Rajanikanth Yadav, Priyanka Bagade', 'link': 'https://arxiv.org/abs/2503.00159', 'abstract': "Crohn's disease and intestinal tuberculosis share many overlapping features such as clinical, radiological, endoscopic, and histological features - particularly granulomas, making it challenging to clinically differentiate them. Our research leverages 3D CTE scans, computer vision, and machine learning to improve this differentiation to avoid harmful treatment mismanagement such as unnecessary anti-tuberculosis therapy for Crohn's disease or exacerbation of tuberculosis with immunosuppressants. Our study proposes a novel method to identify radiologist - identified biomarkers such as VF to SF ratio, necrosis, calcifications, comb sign and pulmonary TB to enhance accuracy. We demonstrate the effectiveness by using different ML techniques on the features extracted from these biomarkers, computing SHAP on XGBoost for understanding feature importance towards predictions, and comparing against SOTA methods such as pretrained ResNet and CTFoundation.", 'abstract_zh': '克罗恩病和肠结核在临床、影像学、内镜和病理学特征上存在许多重叠，尤其是粒细胞肿，这使得临床诊断它们具有挑战性。本研究利用3D CTE扫描、计算机视觉和机器学习技术改善这一诊断，以避免对克罗恩病进行不必要的抗结核治疗或在使用免疫抑制剂时加重结核病等有害的治疗管理。本研究提出了一种新方法，通过识别放射科医生识别的生物标志物（如VF到SF的比例、坏死、钙化、梳状征和肺结核）来增强准确性。我们通过在从这些生物标志物提取的特征上使用不同的机器学习技术、计算XGBoost的SHAP以理解特征的重要性，并与预训练的ResNet和CTFoundation等最先进方法进行对比，来展示了该方法的有效性。', 'title_zh': 'EXACT-CT: 可解释的分析方法用于克罗恩病和结核病的CT影像研究'}
{'arxiv_id': 'arXiv:2503.00086', 'title': 'Generalization of CNNs on Relational Reasoning with Bar Charts', 'authors': 'Zhenxing Cui, Lu Chen, Yunhai Wang, Daniel Haehn, Yong Wang, Hanspeter Pfister', 'link': 'https://arxiv.org/abs/2503.00086', 'abstract': "This paper presents a systematic study of the generalization of convolutional neural networks (CNNs) and humans on relational reasoning tasks with bar charts. We first revisit previous experiments on graphical perception and update the benchmark performance of CNNs. We then test the generalization performance of CNNs on a classic relational reasoning task: estimating bar length ratios in a bar chart, by progressively perturbing the standard visualizations. We further conduct a user study to compare the performance of CNNs and humans. Our results show that CNNs outperform humans only when the training and test data have the same visual encodings. Otherwise, they may perform worse. We also find that CNNs are sensitive to perturbations in various visual encodings, regardless of their relevance to the target bars. Yet, humans are mainly influenced by bar lengths. Our study suggests that robust relational reasoning with visualizations is challenging for CNNs. Improving CNNs' generalization performance may require training them to better recognize task-related visual properties.", 'abstract_zh': '一种卷积神经网络和人类在条形图关系推理任务上泛化的系统研究', 'title_zh': 'CNNs在图表关系推理中的泛化研究'}
{'arxiv_id': 'arXiv:2503.00060', 'title': 'SAC-ViT: Semantic-Aware Clustering Vision Transformer with Early Exit', 'authors': 'Youbing Hu, Yun Cheng, Anqi Lu, Dawei Wei, Zhijun Li', 'link': 'https://arxiv.org/abs/2503.00060', 'abstract': "The Vision Transformer (ViT) excels in global modeling but faces deployment challenges on resource-constrained devices due to the quadratic computational complexity of its attention mechanism. To address this, we propose the Semantic-Aware Clustering Vision Transformer (SAC-ViT), a non-iterative approach to enhance ViT's computational efficiency. SAC-ViT operates in two stages: Early Exit (EE) and Semantic-Aware Clustering (SAC). In the EE stage, downsampled input images are processed to extract global semantic information and generate initial inference results. If these results do not meet the EE termination criteria, the information is clustered into target and non-target tokens. In the SAC stage, target tokens are mapped back to the original image, cropped, and embedded. These target tokens are then combined with reused non-target tokens from the EE stage, and the attention mechanism is applied within each cluster. This two-stage design, with end-to-end optimization, reduces spatial redundancy and enhances computational efficiency, significantly boosting overall ViT performance. Extensive experiments demonstrate the efficacy of SAC-ViT, reducing 62% of the FLOPs of DeiT and achieving 1.98 times throughput without compromising performance.", 'abstract_zh': '基于语义感知聚类的视觉变压器（SAC-ViT）：一种高效的非迭代方法', 'title_zh': '具有早期退出的语义感知聚类视觉变换器'}
{'arxiv_id': 'arXiv:2503.00058', 'title': 'African Gender Classification Using Clothing Identification Via Deep Learning', 'authors': 'Samuel Ozechi', 'link': 'https://arxiv.org/abs/2503.00058', 'abstract': 'Human attribute identification and classification are crucial in computer vision, driving the development of innovative recognition systems. Traditional gender classification methods primarily rely on facial recognition, which, while effective, struggles under non-ideal conditions such as blurriness, side views, or partial occlusions. This study explores an alternative approach by leveraging clothing identification, specifically focusing on African traditional attire, which carries culturally significant and gender-specific features.\nWe use the AFRIFASHION1600 dataset, a curated collection of 1,600 images of African traditional clothing labeled into two gender classes: male and female. A deep learning model, based on a modified VGG16 architecture and trained using transfer learning, was developed for classification. Data augmentation was applied to address the challenges posed by the relatively small dataset and to mitigate overfitting. The model achieved an accuracy of 87% on the test set, demonstrating strong predictive capability despite dataset imbalances favoring female samples.\nThese findings highlight the potential of clothing-based identification as a complementary technique to facial recognition for gender classification in African contexts. Future research should focus on expanding and balancing datasets to enhance classification robustness and improve the applicability of clothing-based gender recognition systems.', 'abstract_zh': '基于服装识别的非洲传统服饰性别分类研究', 'title_zh': '基于深度学习的非洲性别分类研究：通过服装识别'}
{'arxiv_id': 'arXiv:2503.00049', 'title': 'Omni-SILA: Towards Omni-scene Driven Visual Sentiment Identifying, Locating and Attributing in Videos', 'authors': 'Jiamin Luo, Jingjing Wang, Junxiao Ma, Yujie Jin, Shoushan Li, Guodong Zhou', 'link': 'https://arxiv.org/abs/2503.00049', 'abstract': 'Prior studies on Visual Sentiment Understanding (VSU) primarily rely on the explicit scene information (e.g., facial expression) to judge visual sentiments, which largely ignore implicit scene information (e.g., human action, objection relation and visual background), while such information is critical for precisely discovering visual sentiments. Motivated by this, this paper proposes a new Omni-scene driven visual Sentiment Identifying, Locating and Attributing in videos (Omni-SILA) task, aiming to interactively and precisely identify, locate and attribute visual sentiments through both explicit and implicit scene information. Furthermore, this paper believes that this Omni-SILA task faces two key challenges: modeling scene and highlighting implicit scene beyond explicit. To this end, this paper proposes an Implicit-enhanced Causal MoE (ICM) approach for addressing the Omni-SILA task. Specifically, a Scene-Balanced MoE (SBM) and an Implicit-Enhanced Causal (IEC) blocks are tailored to model scene information and highlight the implicit scene information beyond explicit, respectively. Extensive experimental results on our constructed explicit and implicit Omni-SILA datasets demonstrate the great advantage of the proposed ICM approach over advanced Video-LLMs.', 'abstract_zh': '基于全面场景驱动的视频情感识别、定位与归因任务（Omni-SILA）', 'title_zh': 'Omni-SILA：面向全场景的视觉情感识别、定位与归因'}
{'arxiv_id': 'arXiv:2503.00042', 'title': 'An Analysis of Segment Anything 2', 'authors': 'Clayton Bromley, Alexander Moore, Amar Saini, Doug Poland, Carmen Carrano', 'link': 'https://arxiv.org/abs/2503.00042', 'abstract': 'Video object segmentation (VOS) is a critical task in the development of video perception and understanding. The Segment-Anything Model 2 (SAM 2), released by Meta AI, is the current state-of-the-art architecture for end-to-end VOS. SAM 2 performs very well on both clean video data and augmented data, and completely intelligent video perception requires an understanding of how this architecture is capable of achieving such quality results. To better understand how each step within the SAM 2 architecture permits high-quality video segmentation, we pass a variety of complex video transformations through the architecture and measure the impact at each stage of the process. We observe that each progressive stage enables the filtering of complex transformation noise and the emphasis of the object of interest. Our contributions include the creation of complex transformation video datasets, an analysis of how each stage of the SAM 2 architecture interprets these transformations, and visualizations of segmented objects through each stage. By better understanding how each model structure impacts overall video understanding, VOS development can work to improve real-world applicability and performance tracking, localizing, and segmenting objects despite complex cluttered scenes and obscurations.', 'abstract_zh': '视频对象分割（VOS）是视频感知与理解发展中的一项关键任务。Meta AI发布的Segment-Anything Model 2 (SAM 2)是当前端到端VOS的最优架构。SAM 2在干净视频数据和增强数据上表现非常出色，实现如此高质量结果的能力对于完全智能化视频感知至关重要。为了更好地理解SAM 2架构中的每一阶段如何支持高质量视频分割，我们通过对架构施加各种复杂的视频变换并测量各阶段的影响，来分析这一过程。我们观察到每一逐步阶段都能过滤复杂变换噪声并强调目标对象。我们的贡献包括创建复杂的视频变换数据集、分析SAM 2架构每一阶段如何解释这些变换、并展示通过每一阶段分割对象的可视化。通过更好地理解每一模型结构对整体视频理解的影响，VOS开发可以致力于改进实际应用中的适用性和性能，即便是在复杂拥挤场景和遮挡下也能定位和分割对象。', 'title_zh': '对 Segment Anything 2 的分析'}
