{'arxiv_id': 'arXiv:2506.22364', 'title': 'Robotic Multimodal Data Acquisition for In-Field Deep Learning Estimation of Cover Crop Biomass', 'authors': 'Joe Johnson, Phanender Chalasani, Arnav Shah, Ram L. Ray, Muthukumar Bagavathiannan', 'link': 'https://arxiv.org/abs/2506.22364', 'abstract': 'Accurate weed management is essential for mitigating significant crop yield losses, necessitating effective weed suppression strategies in agricultural systems. Integrating cover crops (CC) offers multiple benefits, including soil erosion reduction, weed suppression, decreased nitrogen requirements, and enhanced carbon sequestration, all of which are closely tied to the aboveground biomass (AGB) they produce. However, biomass production varies significantly due to microsite variability, making accurate estimation and mapping essential for identifying zones of poor weed suppression and optimizing targeted management strategies. To address this challenge, developing a comprehensive CC map, including its AGB distribution, will enable informed decision-making regarding weed control methods and optimal application rates. Manual visual inspection is impractical and labor-intensive, especially given the extensive field size and the wide diversity and variation of weed species and sizes. In this context, optical imagery and Light Detection and Ranging (LiDAR) data are two prominent sources with unique characteristics that enhance AGB estimation. This study introduces a ground robot-mounted multimodal sensor system designed for agricultural field mapping. The system integrates optical and LiDAR data, leveraging machine learning (ML) methods for data fusion to improve biomass predictions. The best ML-based model for dry AGB estimation achieved a coefficient of determination value of 0.88, demonstrating robust performance in diverse field conditions. This approach offers valuable insights for site-specific management, enabling precise weed suppression strategies and promoting sustainable farming practices.', 'abstract_zh': '精确的杂草管理对于减轻作物产量损失至关重要，需要在农业系统中实施有效的杂草抑制策略。结合 cover crops (CC) 提供了多重益处，包括减少土壤侵蚀、抑制杂草、降低氮肥需求和增强碳封存，所有这些都与它们产生的地上生物量 (AGB) 密切相关。然而，由于微环境差异，地上生物量产量变化显著，因此准确的估计和制图对于识别杂草抑制效果差的区域并优化目标管理策略至关重要。为应对这一挑战，开发包括地上生物量分布在内的 comprehensive CC 地图，可以为杂草控制方法的选择和最优施用率提供科学依据。手动视觉检查在大面积田块和广泛多样且变化的杂草种类和大小下是不现实且劳动密集型的。在此背景下，光学图像和激光雷达 (LiDAR) 数据是两种具有独特特性的显著来源，能够增强地上生物量估测。本研究介绍了一种安装在地面上的多模态传感器系统，该系统用于农业田块制图，并结合光学和 LiDAR 数据，利用机器学习 (ML) 方法进行数据融合以提高生物量预测。基于 ML 的最佳干地上生物量估测模型获得了决定系数为 0.88 的结果，展示了该方法在多种田间条件下的稳健性能。该方法为现场特定管理提供了有价值的见解，能够促进精准杂草抑制策略和可持续农业实践。', 'title_zh': '田间多模态数据采集的机器人深度学习覆盖作物生物量估算'}
{'arxiv_id': 'arXiv:2506.21630', 'title': 'TOMD: A Trail-based Off-road Multimodal Dataset for Traversable Pathway Segmentation under Challenging Illumination Conditions', 'authors': 'Yixin Sun, Li Li, Wenke E, Amir Atapour-Abarghouei, Toby P. Breckon', 'link': 'https://arxiv.org/abs/2506.21630', 'abstract': 'Detecting traversable pathways in unstructured outdoor environments remains a significant challenge for autonomous robots, especially in critical applications such as wide-area search and rescue, as well as incident management scenarios like forest fires. Existing datasets and models primarily target urban settings or wide, vehicle-traversable off-road tracks, leaving a substantial gap in addressing the complexity of narrow, trail-like off-road scenarios. To address this, we introduce the Trail-based Off-road Multimodal Dataset (TOMD), a comprehensive dataset specifically designed for such environments. TOMD features high-fidelity multimodal sensor data -- including 128-channel LiDAR, stereo imagery, GNSS, IMU, and illumination measurements -- collected through repeated traversals under diverse conditions. We also propose a dynamic multiscale data fusion model for accurate traversable pathway prediction. The study analyzes the performance of early, cross, and mixed fusion strategies under varying illumination levels. Results demonstrate the effectiveness of our approach and the relevance of illumination in segmentation performance. We publicly release TOMD at this https URL to support future research in trail-based off-road navigation.', 'abstract_zh': '基于路径的越野多模态数据集（TOMD）及其在窄路径预测中的应用', 'title_zh': 'TOMD：基于轨迹的越野多模态数据集，用于在复杂光照条件下的可通行路径分割'}
{'arxiv_id': 'arXiv:2506.21885', 'title': 'Integrating Multi-Modal Sensors: A Review of Fusion Techniques for Intelligent Vehicles', 'authors': 'Chuheng Wei, Ziye Qin, Ziyan Zhang, Guoyuan Wu, Matthew J. Barth', 'link': 'https://arxiv.org/abs/2506.21885', 'abstract': 'Multi-sensor fusion plays a critical role in enhancing perception for autonomous driving, overcoming individual sensor limitations, and enabling comprehensive environmental understanding. This paper first formalizes multi-sensor fusion strategies into data-level, feature-level, and decision-level categories and then provides a systematic review of deep learning-based methods corresponding to each strategy. We present key multi-modal datasets and discuss their applicability in addressing real-world challenges, particularly in adverse weather conditions and complex urban environments. Additionally, we explore emerging trends, including the integration of Vision-Language Models (VLMs), Large Language Models (LLMs), and the role of sensor fusion in end-to-end autonomous driving, highlighting its potential to enhance system adaptability and robustness. Our work offers valuable insights into current methods and future directions for multi-sensor fusion in autonomous driving.', 'abstract_zh': '多传感器融合在增强自动驾驶感知中起着关键作用，可以克服单一传感器的限制并实现全面的环境理解。本文首先将多传感器融合策略正式化为数据级、特征级和决策级三类，然后系统地回顾了每个策略对应的基于深度学习的方法。我们介绍了关键的多模态数据集，并讨论了它们在应对现实挑战中的适用性，特别是在恶劣天气条件和复杂城市环境中。此外，我们探讨了新兴趋势，包括视图语言模型（VLMs）、大型语言模型（LLMs）的集成以及传感器融合在端到端自动驾驶中的作用，强调其增强系统适应性和鲁棒性的潜力。我们的工作为当前多传感器融合方法及其未来方向提供了宝贵见解。', 'title_zh': '多模态传感器集成：智能车辆融合技术综述'}
{'arxiv_id': 'arXiv:2506.22056', 'title': 'Universal Retrieval for Multimodal Trajectory Modeling', 'authors': 'Xuan Zhang, Ziyan Jiang, Rui Meng, Yifei Leng, Zhenbang Xiao, Zora Zhiruo Wang, Yanyi Shang, Dehan Kong', 'link': 'https://arxiv.org/abs/2506.22056', 'abstract': 'Trajectory data, capturing human actions and environmental states across various modalities, holds significant potential for enhancing AI agent capabilities, particularly in GUI environments. However, how to model the representation of trajectory-level data presents a significant challenge that has not been systematically addressed amid explosive trajectory data growth. In this work, we introduce Multimodal Trajectory Retrieval, bridging the gap between universal retrieval and agent-centric trajectory modeling. We construct the Unified Agent Trajectory Dataset (UATD) from annotated demonstrations and states across diverse real-world scenarios. Based on this, we present GAE-Bench, a benchmark containing a large number of trajectory-based retrieval pairs. In addition, we propose GAE-Retriever, a multimodal retrieval framework that adopts vision-language models and incorporates optimized contrastive learning through a token selection and the GradCache mechanism. Comprehensive evaluations across multiple datasets show that GAE-Retriever consistently outperforms strong baselines in retrieval recall, highlighting its effectiveness in advancing multimodal trajectory retrieval.', 'abstract_zh': '多模态轨迹检索：介于通用检索与代理中心轨迹建模之间的桥梁', 'title_zh': '多模态轨迹建模的通用检索'}
{'arxiv_id': 'arXiv:2506.22338', 'title': 'A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake', 'authors': 'Luigi Russo, Deodato Tapete, Silvia Liberata Ullo, Paolo Gamba', 'link': 'https://arxiv.org/abs/2506.22338', 'abstract': "Building damage identification shortly after a disaster is crucial for guiding emergency response and recovery efforts. Although optical satellite imagery is commonly used for disaster mapping, its effectiveness is often hampered by cloud cover or the absence of pre-event acquisitions. To overcome these challenges, we introduce a novel multimodal deep learning (DL) framework for detecting building damage using single-date very high resolution (VHR) Synthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI) COSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data. Our method integrates SAR image patches, OpenStreetMap (OSM) building footprints, digital surface model (DSM) data, and structural and exposure attributes from the Global Earthquake Model (GEM) to improve detection accuracy and contextual interpretation. Unlike existing approaches that depend on pre and post event imagery, our model utilizes only post event data, facilitating rapid deployment in critical scenarios. The framework effectiveness is demonstrated using a new dataset from the 2023 earthquake in Turkey, covering multiple cities with diverse urban settings. Results highlight that incorporating geospatial features significantly enhances detection performance and generalizability to previously unseen areas. By combining SAR imagery with detailed vulnerability and exposure information, our approach provides reliable and rapid building damage assessments without the dependency from available pre-event data. Moreover, the automated and scalable data generation process ensures the framework's applicability across diverse disaster-affected regions, underscoring its potential to support effective disaster management and recovery efforts. Code and data will be made available upon acceptance of the paper.", 'abstract_zh': '短灾后建筑损害识别对于指导紧急响应和恢复工作至关重要。尽管光学卫星影像常用于灾害映射，但其效果往往受云覆盖或缺乏事前影像的影响。为克服这些挑战，我们提出了一种新的多模态深度学习框架，利用意大利空间局（ASI）COSMO SkyMed（CSK）星座的单日期超高分辨率（VHR）合成孔径雷达（SAR）影像，并辅以辅助地理空间数据，来检测建筑损害。该方法整合了SAR影像patches、OpenStreetMap（OSM）建筑地块、数字表面模型（DSM）数据以及全球地震模型（GEM）的结构和暴露属性，以提高检测准确性和上下文解释能力。不同于依赖事前和事后影像的方法，我们的模型仅使用事后数据，便于在关键场景中快速部署。该框架的有效性通过2023年土耳其地震的新数据集进行了展示，涵盖了多个具有不同城市环境的地区。结果表明，纳入地理空间特征显著提升了检测性能和对未见过区域的泛化能力。通过结合SAR影像与详细的脆弱性和暴露信息，我们的方法能够在没有可用事前影像的情况下提供可靠的快速建筑损害评估。自动化的可扩展数据生成过程确保了该框架在各种灾害影响地区的适用性，突显了其在支持有效的灾害管理和恢复工作方面的潜力。代码和数据将在论文被接受后提供。', 'title_zh': '使用高分辨率 SAR 和地理空间数据的深学习框架在 2023 年土耳其地震损毁评估中的应用'}
{'arxiv_id': 'arXiv:2506.22146', 'title': 'Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs', 'authors': 'Amirmohammad Izadi, Mohammad Ali Banayeeanzade, Fatemeh Askari, Ali Rahimiakbar, Mohammad Mahdi Vahedi, Hosein Hasani, Mahdieh Soleymani Baghshah', 'link': 'https://arxiv.org/abs/2506.22146', 'abstract': 'Despite progress in Vision-Language Models (VLMs), their capacity for visual reasoning is often limited by the \\textit{binding problem}: the failure to reliably associate perceptual features with their correct visual referents. This limitation underlies persistent errors in tasks such as counting, visual search, scene description, and spatial relationship understanding. A key factor is that current VLMs process visual features largely in parallel, lacking mechanisms for spatially grounded, serial attention. This paper introduces a simple yet effective intervention: augmenting visual inputs with low-level spatial structures (e.g., horizontal lines) and pairing this with a textual prompt that encourages sequential, spatially-aware parsing. We empirically demonstrate substantial performance improvements across core visual reasoning tasks. Specifically, our method improves GPT-4o visual search accuracy by 25.00%, increases counting accuracy by 26.83%, reduces edit distance error in scene description by 0.32, and enhances performance on spatial relationship tasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the visual modification is essential for these gains; purely textual strategies, including Chain-of-Thought prompting, are insufficient and can even degrade performance. Our method enhances binding only with a single-query inference, underscoring the importance of visual input design over purely linguistically-based approaches. These findings suggest that low-level visual structuring is a powerful and underexplored direction for improving compositional visual reasoning and could serve as a general strategy for enhancing VLM performance on spatially grounded tasks.', 'abstract_zh': '尽管在视觉语言模型（VLMs）方面取得进展，但它们在视觉推理方面的能力仍受限于“绑定问题”：即感知特征与正确视觉实指之间的可靠关联失败。这一限制导致了计数、视觉搜索、场景描述和空间关系理解等任务中持续存在的错误。关键因素在于当前的VLMs主要以并行方式处理视觉特征，缺乏空间定位的序列注意力机制。本文介绍了一个简单而有效的干预方法：在视觉输入中加入低级空间结构（如水平线），并搭配一个文本提示，以促进序列化、空间意识的解析。我们实证展示了在核心视觉推理任务中性能显著提升。具体而言，我们的方法将GPT-4o的视觉搜索准确性提高了25.00%，计数准确性提高了26.83%，场景描述中的编辑距离误差降低了0.32，空间关系任务性能提高了9.50%（在2D合成数据集上）。此外，我们发现视觉修改对于这些提升至关重要；纯文本策略，包括链式思维提示，是不足的，甚至可能降低性能。我们的方法只需单次查询推理就能增强绑定，突显了视觉输入设计的重要性，而非纯粹基于语言的方法。这些发现表明，低级视觉结构化是一种强大而未被充分探索的途径，以改善组合视觉推理，并可能作为增强VLM在空间定位任务上性能的一般策略。', 'title_zh': '视觉结构有助于视觉推理：解决VLM中的绑定问题'}
{'arxiv_id': 'arXiv:2506.21604', 'title': 'Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding', 'authors': 'Varun Mannam, Fang Wang, Xin Chen', 'link': 'https://arxiv.org/abs/2506.21604', 'abstract': 'Current evaluation frameworks for multimodal generative AI struggle to establish trustworthiness, hindering enterprise adoption where reliability is paramount. We introduce a systematic, quantitative benchmarking framework to measure the trustworthiness of progressively integrating cross-modal inputs such as text, images, captions, and OCR within VisualRAG systems for enterprise document intelligence. Our approach establishes quantitative relationships between technical metrics and user-centric trust measures. Evaluation reveals that optimal modality weighting with weights of 30% text, 15% image, 25% caption, and 30% OCR improves performance by 57.3% over text-only baselines while maintaining computational efficiency. We provide comparative assessments of foundation models, demonstrating their differential impact on trustworthiness in caption generation and OCR extraction-a vital consideration for reliable enterprise AI. This work advances responsible AI deployment by providing a rigorous framework for quantifying and enhancing trustworthiness in multimodal RAG for critical enterprise applications.', 'abstract_zh': '当前的多模态生成AI评价框架难以建立信任度，阻碍了对企业级应用中可靠性至上的采用。我们引入了一种系统性的定量基准框架，用于测量在VisualRAG系统中逐步整合跨模态输入（如文本、图像、字幕和OCR）对企业文档智能的信任度。我们的方法建立了技术指标与用户中心的信任度指标之间的定量关系。评估显示，最优模态加权（文本30%，图像15%，字幕25%，OCR30%）相比仅文本基线提高了57.3%的性能，同时保持了计算效率。我们提供了基础模型的比较评估，展示了它们在字幕生成和OCR提取中对信任度的不同影响，这是可靠企业AI的重要考量因素。本工作通过提供一个严谨的框架来量化和提升关键企业应用中多模态RAG的信任度，促进了负责任的AI部署。', 'title_zh': '评估VisualRAG：企业文档理解的跨模态性能量化'}
{'arxiv_id': 'arXiv:2506.21586', 'title': 'Can Vision Language Models Understand Mimed Actions?', 'authors': 'Hyundong Cho, Spencer Lin, Tejas Srinivasan, Michael Saxon, Deuksin Kwon, Natali T. Chavez, Jonathan May', 'link': 'https://arxiv.org/abs/2506.21586', 'abstract': 'Nonverbal communication (NVC) plays an integral role in human language, but studying NVC in general is challenging because of its broad scope and high variance in interpretation among individuals and cultures. However, mime -- the theatrical technique of suggesting intent using only gesture, expression, and movement -- is a subset of NVC that consists of explicit and embodied actions with much lower human interpretation variance. We argue that a solid understanding of mimed actions is a crucial prerequisite for vision-language models capable of interpreting and commanding more subtle aspects of NVC. Hence, we propose Mime Identification Multimodal Evaluation (MIME), a novel video-based question answering benchmark comprising of 86 mimed actions. Constructed with motion capture data, MIME consists of variations of each action with perturbations applied to the character, background, and viewpoint for evaluating recognition robustness. We find that both open-weight and API-based vision-language models perform significantly worse than humans on MIME, motivating the need for increased research for instilling more robust understanding of human gestures.', 'abstract_zh': '基于视频的 Mime 识别多模态评价基准（MIME）：理解非言语沟通中的关键要素', 'title_zh': '视觉语言模型能理解模仿动作吗？'}
{'arxiv_id': 'arXiv:2506.21562', 'title': 'FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction', 'authors': 'Jun Yin, Pengyu Zeng, Jing Zhong, Peilin Li, Miao Zhang, Ran Luo, Shuai Lu', 'link': 'https://arxiv.org/abs/2506.21562', 'abstract': "In the architectural design process, floor plan generation is inherently progressive and iterative. However, existing generative models for floor plans are predominantly end-to-end generation that produce an entire pixel-based layout in a single pass. This paradigm is often incompatible with the incremental workflows observed in real-world architectural practice. To address this issue, we draw inspiration from the autoregressive 'next token prediction' mechanism commonly used in large language models, and propose a novel 'next room prediction' paradigm tailored to architectural floor plan modeling. Experimental evaluation indicates that FPDS demonstrates competitive performance in comparison to diffusion models and Tell2Design in the text-to-floorplan task, indicating its potential applicability in supporting future intelligent architectural design.", 'abstract_zh': '在建筑设计过程中，楼层平面图生成天然地具有渐进性和迭代性。然而，现有的平面图生成模型主要是一次完成整个像素布局的端到端生成。这种范式往往与现实世界建筑实践中观察到的增量工作流不兼容。为此，我们受到大型语言模型中常用的自回归“下一个词预测”机制的启发，提出了一种适用于建筑平面图建模的新型“下一个房间预测”范式。实验评价表明，FPDS 在文本到平面图任务中表现出与扩散模型和Tell2Design相当的竞争性能，表明其在未来智能建筑设计中的潜在应用价值。', 'title_zh': 'FloorPlan-DeepSeek (FPDS): 基于向量的下一步房间预测的多模态楼面图生成方法'}
