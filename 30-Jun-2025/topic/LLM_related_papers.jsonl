{'arxiv_id': 'arXiv:2506.22028', 'title': 'LMPVC and Policy Bank: Adaptive voice control for industrial robots with code generating LLMs and reusable Pythonic policies', 'authors': 'Ossi Parikka, Roel Pieters', 'link': 'https://arxiv.org/abs/2506.22028', 'abstract': 'Modern industry is increasingly moving away from mass manufacturing, towards more specialized and personalized products. As manufacturing tasks become more complex, full automation is not always an option, human involvement may be required. This has increased the need for advanced human robot collaboration (HRC), and with it, improved methods for interaction, such as voice control. Recent advances in natural language processing, driven by artificial intelligence (AI), have the potential to answer this demand. Large language models (LLMs) have rapidly developed very impressive general reasoning capabilities, and many methods of applying this to robotics have been proposed, including through the use of code generation. This paper presents Language Model Program Voice Control (LMPVC), an LLM-based prototype voice control architecture with integrated policy programming and teaching capabilities, built for use with Robot Operating System 2 (ROS2) compatible robots. The architecture builds on prior works using code generation for voice control by implementing an additional programming and teaching system, the Policy Bank. We find this system can compensate for the limitations of the underlying LLM, and allow LMPVC to adapt to different downstream tasks without a slow and costly training process. The architecture and additional results are released on GitHub (this https URL).', 'abstract_zh': '现代工业正逐渐从大规模生产转向更专业和个性化的產品。随着制造任务变得越来越复杂，全面自动化并非总是选项，可能还需要人类参与。这增加了高级人机协作（HRC）的需求，同时也改善了交互方法，如语音控制。最近由人工智能（AI）推动的自然语言处理的进展有可能满足这一需求。大型语言模型（LLMs）迅速发展出了非常令人印象深刻的通用推理能力，并提出了多种将其应用于机器人技术的方法，包括通过代码生成。本文介绍了语言模型程序语音控制（LMPVC），这是一种基于LLM的原型语音控制架构，具有集成的策略编程和教学能力，适用于ROS2兼容的机器人。该架构通过实现额外的编程和教学系统——策略银行，建立在先前使用代码生成进行语音控制的工作之上。我们发现该系统可以弥补底层LLM的局限性，并允许LMPVC在不需要缓慢且昂贵的训练过程的情况下适应不同的下游任务。该架构及相关结果已发布在GitHub上（this https URL）。', 'title_zh': '基于LMPVC和政策银行的工业机器人自适应语音控制——基于代码生成LLM和可重用Pythonic策略'}
{'arxiv_id': 'arXiv:2506.22419', 'title': 'The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements', 'authors': 'Bingchen Zhao, Despoina Magka, Minqi Jiang, Xian Li, Roberta Raileanu, Tatiana Shavrina, Jean-Christophe Gagnon-Audet, Kelvin Niu, Shagun Sodhani, Michael Shvartsman, Andrei Lupu, Alisia Lupidi, Edan Toledo, Karen Hambardzumyan, Martin Josifoski, Thomas Foster, Lucia Cipolina-Kun, Abhishek Charnalia, Derek Dunfield, Alexander H. Miller, Oisin Mac Aodha, Jakob Foerster, Yoram Bachrach', 'link': 'https://arxiv.org/abs/2506.22419', 'abstract': 'Rapid advancements in large language models (LLMs) have the potential to assist in scientific progress. A critical capability toward this endeavor is the ability to reproduce existing work. To evaluate the ability of AI agents to reproduce results in an active research area, we introduce the Automated LLM Speedrunning Benchmark, leveraging the research community contributions on the NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time. Each of the 19 speedrun tasks provides the agent with the previous records training script, optionally paired with one of three hint formats, ranging from pseudocode to paper-like descriptions of the new records improvements. Records execute quickly by design and speedrun improvements encompass diverse code-level changes, ranging from high-level algorithmic advancements to hardware-aware optimizations. These features make the benchmark both accessible and realistic for the frontier problem of improving LLM training. We find that recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement already-known innovations in our benchmark, even when given detailed hints. Our benchmark thus provides a simple, non-saturated measure of an LLMs ability to automate scientific reproduction, a necessary (but not sufficient) skill for an autonomous research agent.', 'abstract_zh': '大型语言模型的快速进步有望促进科学研究的进步。实现这一目标的关键能力是能够再现现有工作。为了评估AI代理在活跃研究领域的再现结果能力，我们引入了自动LLM速跑基准，利用NanoGPT速跑的社区研究成果，NanoGPT速跑是一个训练GPT-2模型的最快时间竞赛。每项19个速跑任务都为代理提供了之前的记录训练脚本，可选地配以三种提示格式之一，从伪代码到论文般的描述新记录改进。记录设计上执行速度快，速跑改进涵盖了从高级算法进步到硬件感知优化等各种代码层面的变化。这些特性使得基准既适用于改进LLM训练的前沿问题，又具备现实性。我们发现，即使提供了详细的提示，最新的推理LLM与最先进的支架相结合，在基准中仍然难以重新实现已知的创新。因此，我们的基准提供了一种简单且未饱和的评估LLM自动化科学研究能力的方法，这是自主研究代理所需（但不足够）的一项技能。', 'title_zh': '自动化大语言模型速度竞赛基准：重现NanoGPT改进'}
{'arxiv_id': 'arXiv:2506.22005', 'title': 'LeanConjecturer: Automatic Generation of Mathematical Conjectures for Theorem Proving', 'authors': 'Naoto Onda, Kazumi Kasaura, Yuta Oriike, Masaya Taniguchi, Akiyoshi Sannai, Sho Sonoda', 'link': 'https://arxiv.org/abs/2506.22005', 'abstract': 'We introduce LeanConjecturer, a pipeline for automatically generating university-level mathematical conjectures in Lean 4 using Large Language Models (LLMs). Our hybrid approach combines rule-based context extraction with LLM-based theorem statement generation, addressing the data scarcity challenge in formal theorem proving. Through iterative generation and evaluation, LeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with 3,776 identified as syntactically valid and non-trivial, that is, cannot be proven by \\texttt{aesop} tactic. We demonstrate the utility of these generated conjectures for reinforcement learning through Group Relative Policy Optimization (GRPO), showing that targeted training on domain-specific conjectures can enhance theorem proving capabilities. Our approach generates 103.25 novel conjectures per seed file on average, providing a scalable solution for creating training data for theorem proving systems. Our system successfully verified several non-trivial theorems in topology, including properties of semi-open, alpha-open, and pre-open sets, demonstrating its potential for mathematical discovery beyond simple variations of existing results.', 'abstract_zh': 'LeanConjecturer：一种使用大型语言模型在Lean 4中自动生成大学水平数学猜想的管道', 'title_zh': 'LeanConjecturer: 自动生成数学猜想以证明定理'}
{'arxiv_id': 'arXiv:2506.21805', 'title': 'CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation', 'authors': 'Nicolas Bougie, Narimasa Watanabe', 'link': 'https://arxiv.org/abs/2506.21805', 'abstract': 'Modeling human behavior in urban environments is fundamental for social science, behavioral studies, and urban planning. Prior work often rely on rigid, hand-crafted rules, limiting their ability to simulate nuanced intentions, plans, and adaptive behaviors. Addressing these challenges, we envision an urban simulator (CitySim), capitalizing on breakthroughs in human-level intelligence exhibited by large language models. In CitySim, agents generate realistic daily schedules using a recursive value-driven approach that balances mandatory activities, personal habits, and situational factors. To enable long-term, lifelike simulations, we endow agents with beliefs, long-term goals, and spatial memory for navigation. CitySim exhibits closer alignment with real humans than prior work, both at micro and macro levels. Additionally, we conduct insightful experiments by modeling tens of thousands of agents and evaluating their collective behaviors under various real-world scenarios, including estimating crowd density, predicting place popularity, and assessing well-being. Our results highlight CitySim as a scalable, flexible testbed for understanding and forecasting urban phenomena.', 'abstract_zh': '城市环境中人类行为建模对于社会科学、行为研究和城市规划至关重要。先前的工作经常依赖于僵硬的手工设计规则，限制了它们模拟复杂意图、计划和适应性行为的能力。为应对这些挑战，我们设想了一个城市模拟器（CitySim），利用大型语言模型在人类级别智能方面取得的突破。在CitySim中，代理使用递归的价值驱动方法生成现实的日常计划，平衡强制性活动、个人习惯和情境因素。为了实现长期、拟真的模拟，我们赋予代理信念、长期目标和空间记忆以进行导航。CitySim在微观和宏观层面上都更接近真实人类的行为，此外，我们通过建模数以万计的代理并评估它们在各种现实世界场景下的集体行为，进行了富有洞察力的实验，包括估计人群密度、预测地点受欢迎程度以及评估福祉。我们的结果突显了CitySim作为一个可扩展、灵活的试验平台，用于理解和预测城市现象的重要性。', 'title_zh': 'CitySim：基于大规模LLM驱动代理仿真建模城市行为与城市动力学'}
{'arxiv_id': 'arXiv:2506.21784', 'title': 'MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models', 'authors': 'Yifan Liu, Xishun Liao, Haoxuan Ma, Jonathan Liu, Rohan Jadhav, Jiaqi Ma', 'link': 'https://arxiv.org/abs/2506.21784', 'abstract': 'Understanding and modeling human mobility patterns is crucial for effective transportation planning and urban development. Despite significant advances in mobility research, there remains a critical gap in simulation platforms that allow for algorithm development, policy implementation, and comprehensive evaluation at scale. Traditional activity-based models require extensive data collection and manual calibration, machine learning approaches struggle with adaptation to dynamic conditions, and treding agent-based Large Language Models (LLMs) implementations face computational constraints with large-scale simulations. To address these challenges, we propose MobiVerse, a hybrid framework leverages the efficiency of lightweight domain-specific generator for generating base activity chains with the adaptability of LLMs for context-aware modifications. A case study was conducted in Westwood, Los Angeles, where we efficiently generated and dynamically adjusted schedules for the whole population of approximately 53,000 agents on a standard PC. Our experiments demonstrate that MobiVerse successfully enables agents to respond to environmental feedback, including road closures, large gathering events like football games, and congestion, through our hybrid framework. Its modular design facilitates testing various mobility algorithms at both transportation system and agent levels. Results show our approach maintains computational efficiency while enhancing behavioral realism. MobiVerse bridges the gap in mobility simulation by providing a customizable platform for mobility systems planning and operations with benchmark algorithms. Code and videos are available at this https URL.', 'abstract_zh': '理解并建模人类移动模式对于有效的运输规划和城市发展至关重要。尽管在移动性研究方面取得了显著进展，但在支持算法开发、政策实施和大规模综合评估的仿真平台上仍存在关键缺口。传统的基于活动的模型需要大量的数据收集和手动校准，机器学习方法难以适应动态条件，而基于代理的大型语言模型（LLMs）实现则面临大规模仿真中的计算约束。为解决这些挑战，我们提出MobiVerse，这是一种混合框架，结合了轻量级领域特定生成器的高效性与大型语言模型的适应性，以便进行上下文感知的修改。我们在洛杉矶的韦斯特伍德地区进行了案例研究，在标准PC上高效地生成并动态调整了大约53,000个代理的完整时间表。我们的实验显示，MobiVerse通过混合框架成功使代理能够响应环境反馈，包括道路封闭、大型集会活动（如足球比赛）和拥堵等因素。其模块化设计便于在运输系统和代理层面测试各种移动性算法。结果显示，我们的方法在保持计算效率的同时提高了行为现实度。MobiVerse通过提供一个可定制的平台，弥合了移动性模拟的缺口，并配备基准算法。代码和视频可在以下链接获取。', 'title_zh': 'MobiVerse: 通过混合轻量级领域专用生成器和大型语言模型扩展城市 Mobility 模拟'}
{'arxiv_id': 'arXiv:2506.21763', 'title': 'THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?', 'authors': 'Xin Wang, Jiyao Liu, Yulong Xiao, Junzhi Ning, Lihao Liu, Junjun He, Botian Shi, Kaicheng Yu', 'link': 'https://arxiv.org/abs/2506.21763', 'abstract': 'Large Language Models (LLMs) are accelerating scientific idea generation, but rigorously evaluating these numerous, often superficial, AI-generated propositions for novelty and factual accuracy is a critical bottleneck; manual verification is too this http URL validation methods are inadequate: LLMs as standalone verifiers may hallucinate and lack domain knowledge (our findings show ~60\\% unawareness of relevant papers in specific domains), while traditional citation networks lack explicit causality and narrative surveys are this http URL underscores a core challenge: the absence of structured, verifiable, and causally-linked historical data of scientific this http URL address this,we introduce \\textbf{THE-Tree} (\\textbf{T}echnology \\textbf{H}istory \\textbf{E}volution Tree), a computational framework that constructs such domain-specific evolution trees from scientific this http URL-Tree employs a search algorithm to explore evolutionary paths. During its node expansion, it utilizes a novel "Think-Verbalize-Cite-Verify" process: an LLM proposes potential advancements and cites supporting literature. Critically, each proposed evolutionary link is then validated for logical coherence and evidential support by a recovered natural language inference mechanism that interrogates the cited literature, ensuring that each step is this http URL construct and validate 88 THE-Trees across diverse domains and release a benchmark dataset including up to 71k fact verifications covering 27k papers to foster further this http URL demonstrate that i) in graph completion, our THE-Tree improves hit@1 by 8\\% to 14\\% across multiple models compared to traditional citation networks; ii) for predicting future scientific developments, it improves hit@1 metric by nearly 10\\%; and iii) when combined with other methods, it boosts the performance of evaluating important scientific papers by almost 100\\%.', 'abstract_zh': '大型语言模型（LLMs）正在加速科学想法的生成，但严谨评估这些众多、常表面性的AI生成命题的新颖性和事实准确性是一个关键瓶颈；手动验证效率低下：独立验证的LLMs可能会出现幻觉并缺乏领域知识（我们的研究显示特定领域的相关论文约60%未被意识到），而传统的引用网络缺乏明确的因果关系和叙述性综述。凸显了一个核心挑战：缺乏结构化、可验证且因果关联的科学历史数据。为解决这一问题，我们引入了**THE-Tree**（技术历史进化树）这一计算框架，从科学文献中构建领域特定的进化树。THE-Tree 使用搜索算法探索进化路径。在节点扩展过程中，它利用一种新的“思考-表达-引述-验证”过程：LLM 提出潜在进展并引用支持文献。关键的是，每条提议的进化链接都会通过恢复的自然语言推理机制进行逻辑连贯性和证据支持的验证，确保每一步都是合理且有证据支持的。我们构建并验证了88个THE-Tree，覆盖多个领域，并发布了包含多达71,000个事实验证涵盖27,000篇论文的数据集，以促进进一步的研究。实验结果表明：i) 在图完成任务中，我们的THE-Tree在多个模型中将hit@1指标提高了8%至14%，优于传统引用网络；ii) 在预测未来科学进展方面，其hit@1指标提高了近10%；iii) 当与其他方法结合使用时，它可以将评估重要科学论文的性能提升近100%。', 'title_zh': 'THE-Tree: 追踪历史演变能否增强科学验证与推理？'}
{'arxiv_id': 'arXiv:2506.21734', 'title': 'Hierarchical Reasoning Model', 'authors': 'Guan Wang, Jin Li, Yuhao Sun, Xing Chen, Changling Liu, Yue Wu, Meng Lu, Sen Song, Yasin Abbasi Yadkori', 'link': 'https://arxiv.org/abs/2506.21734', 'abstract': "Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities. These results underscore HRM's potential as a transformative advancement toward universal computation and general-purpose reasoning systems.", 'abstract_zh': '层次推理模型：一种既能实现显著计算深度又能保持训练稳定性和效率的新型递归架构及其在复杂推理任务中的应用', 'title_zh': '层次推理模型'}
{'arxiv_id': 'arXiv:2506.22403', 'title': 'HyperCLOVA X THINK Technical Report', 'authors': 'NAVER Cloud HyperCLOVA X Team', 'link': 'https://arxiv.org/abs/2506.22403', 'abstract': 'We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly $6$ trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data. It was implemented as a compute-memory-balanced Peri-LN Transformer scaled with $\\mu$P, pre-trained through a three-stage curriculum that expands the context window to $128$K tokens, and post-trained via supervised fine-tuning with Reinforcement Learning from Verifiable Rewards supports both detailed rationale and concise-answer modes. It delivers competitive performance against similarly sized models on Korea-focused benchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while preserving robust bilingual consistency and translation quality. In addition, a vision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM benchmark, all of which are achieved with substantially lower training compute than existing models of similar sizes. We also present a pruning and distillation technique that will soon be applied to HyperCLOVA X THINK for an open-source and business-friendly foundation model. Altogether, these capabilities position HyperCLOVA X THINK as a robust foundation for Korean AI innovation and a valuable resource for the global research community.', 'abstract_zh': '我们介绍HyperCLOVA X THINK，这是HyperCLOVA X家族中第一个注重推理的大语言模型，预训练了大约6万亿高质量的韩语和英语token，并通过目标合成韩语文本进行了扩充。该模型采用计算与内存平衡的peri-LN Transformer架构，并采用μP扩展，通过三阶段的曲程进行预训练，扩展上下文窗口至128K token，并通过验证奖励支持的强化学习监督微调技术进行后续训练，支持详细的推理模式和简洁的答案模式。它在以韩国为重点的基准测试（如KMMLU、CSAT、KoBALT-700、HAERAE-1.0和KoBigBench）中取得了与同样规模模型相当的性能，同时保持了稳健的双语一致性和平行文本质量。此外，其视图增强版本在KCSAT STEM基准测试中达到了或超过了GPT-4.1的表现，所有这些都比现有同等规模模型的训练计算成本要低得多。我们还介绍了一种即将应用于HyperCLOVA X THINK的剪枝和蒸馏技术，为开源和商业友好型基础模型提供支持。总体而言，这些能力使HyperCLOVA X THINK成为韩语AI创新的稳健基础，并为全球研究社区提供宝贵的资源。', 'title_zh': 'HyperCLOVA X THINK 技术报告'}
{'arxiv_id': 'arXiv:2506.22396', 'title': 'QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization', 'authors': 'Danush Khanna, Aditya Kumar Guru, Srivarshinee Sridhar, Zidan Ahmed, Rubhav Bahirwani, Meetu Malhotra, Vinija Jain, Aman Chadha, Amitava Das, Kripabandhu Ghosh', 'link': 'https://arxiv.org/abs/2506.22396', 'abstract': 'Inference accounts for the majority of latency and energy consumption in large language model (LLM) deployments, often exceeding 90% of total cost. While training-time efficiency has seen extensive progress, runtime optimization remains a key bottleneck, particularly under autoregressive decoding. Existing approaches -- such as pruning, quantization, early exits, and speculative decoding -- often require retraining, architectural changes, or disrupt decoding compatibility. We introduce QuickSilver, a modular, token-level framework that enables semantic adaptivity at inference time without altering model weights or structure. QuickSilver integrates four synergistic mechanisms:\n(i) Dynamic Token Halting, which halts computation for tokens with converged representations; (ii) KV Cache Skipping, which selectively suppresses memory writes to reduce attention overhead; and (iii) Contextual Token Fusion, which collapses redundant tokens into shared paths to shrink sequence length.\nUnlike speculative decoding or MoE routing, QuickSilver operates entirely on frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP reduction with negligible perplexity degradation (<=0.2).', 'abstract_zh': '快速银：一种无需更改模型权重或结构的推理时语义自适应框架', 'title_zh': 'QuickSilver — 通过动态 Tokens 中止、KV 跳过、上下文 Tokens 融合及自适应 Matryoshka 量化加速 LLM 推理'}
{'arxiv_id': 'arXiv:2506.22376', 'title': 'Probabilistic Optimality for Inference-time Scaling', 'authors': 'Youkang Wang, Jian Wang, Rubing Chen, Xiao-Yong Wei, Qing Li', 'link': 'https://arxiv.org/abs/2506.22376', 'abstract': 'Inference-time scaling has emerged as a powerful technique for enhancing the reasoning performance of Large Language Models (LLMs). However, existing approaches often rely on heuristic strategies for parallel sampling, lacking a principled foundation. To address this gap, we propose a probabilistic framework that formalizes the optimality of inference-time scaling under the assumption that parallel samples are independently and identically distributed (i.i.d.), and where the Best-of-N selection strategy follows a probability distribution that can be estimated. Within this framework, we derive a theoretical lower bound on the required number of samples to achieve a target performance level, providing the first principled guidance for compute-efficient scaling. Leveraging this insight, we develop \\textsc{OptScale}, a practical algorithm that dynamically determines the optimal number of sampled responses. \\textsc{OptScale} employs a language model-based predictor to estimate probabilistic prior parameters, enabling the decision of the minimal number of samples needed that satisfy predefined performance thresholds and confidence levels. Extensive experiments on mathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC) demonstrate that \\textsc{OptScale} significantly reduces sampling overhead while remaining better or on par with state-of-the-art reasoning performance. Our work offers both a theoretical foundation and a practical solution for principled inference-time scaling, addressing a critical gap in the efficient deployment of LLMs for complex reasoning.', 'abstract_zh': '推理时缩放已成为增强大型语言模型（LLMs）推理性能的强大技术。然而，现有方法通常依赖于启发式策略进行并行采样，缺乏坚实的理论基础。为解决这一问题，我们提出了一种概率框架，该框架在假设并行样本独立同分布（i.i.d.）且最佳N选一策略遵循可估计的概率分布的前提下， formalizes推理时缩放的最优性。在此框架内，我们推导出实现目标性能水平所需的样本数的理论下界，从而提供了首个计算高效缩放的原理性指导。借助这一洞察，我们开发了OptScale，一种实用算法，能够动态确定最优的采样数量。OptScale 使用基于语言模型的预测器估算概率先验参数，使决策所需的最小样本数量能够满足预定义的性能阈值和置信水平。我们在数学推理基准测试（包括MATH-500、GSM8K、AIME和AMC）上的 extensive实验表明，OptScale 显著减少了采样开销，同时保持或优于最先进的推理性能。我们的工作为原理性推理时缩放提供了理论基础和实践解决方案，填补了高效部署LLMs进行复杂推理的空白。', 'title_zh': '推理时缩放的概率优化'}
{'arxiv_id': 'arXiv:2506.22255', 'title': 'Projected Compression: Trainable Projection for Efficient Transformer Compression', 'authors': 'Maciej Stefaniak, Michał Krutul, Jan Małaśnicki, Maciej Pióro, Jakub Krajewski, Sebastian Jaszczur, Marek Cygan, Kamil Adamczewski, Jan Ludziejewski', 'link': 'https://arxiv.org/abs/2506.22255', 'abstract': "Large language models have steadily increased in size to achieve improved performance; however, this growth has also led to greater inference time and computational demands. Consequently, there is rising interest in model size reduction methods. To address this issue, we propose Projected Compression, a novel model compression technique, that reduces model weights by utilizing projection modules. Specifically, we first train additional trainable projections weights and preserve access to all the original model parameters. Subsequently, these projections are merged into a lower-dimensional product matrix, resulting in a reduced-size standard Transformer-based model. Unlike alternative approaches that require additional computational overhead, our method matches the base model's per-token computation step in FLOPs. Experimental results show that Projected Compression outperforms the comparable hard pruning and retraining approach on higher quality models. Moreover, the performance margin scales well with the number of tokens.", 'abstract_zh': 'Projected Compression: A Novel Model Compression Technique for Reducing Inference Time and Computational Demands', 'title_zh': '投影压缩：高效的变压器压缩的可训练投影'}
{'arxiv_id': 'arXiv:2506.22231', 'title': 'Adapting University Policies for Generative AI: Opportunities, Challenges, and Policy Solutions in Higher Education', 'authors': 'Russell Beale', 'link': 'https://arxiv.org/abs/2506.22231', 'abstract': "The rapid proliferation of generative artificial intelligence (AI) tools - especially large language models (LLMs) such as ChatGPT - has ushered in a transformative era in higher education. Universities in developed regions are increasingly integrating these technologies into research, teaching, and assessment. On one hand, LLMs can enhance productivity by streamlining literature reviews, facilitating idea generation, assisting with coding and data analysis, and even supporting grant proposal drafting. On the other hand, their use raises significant concerns regarding academic integrity, ethical boundaries, and equitable access. Recent empirical studies indicate that nearly 47% of students use LLMs in their coursework - with 39% using them for exam questions and 7% for entire assignments - while detection tools currently achieve around 88% accuracy, leaving a 12% error margin. This article critically examines the opportunities offered by generative AI, explores the multifaceted challenges it poses, and outlines robust policy solutions. Emphasis is placed on redesigning assessments to be AI-resilient, enhancing staff and student training, implementing multi-layered enforcement mechanisms, and defining acceptable use. By synthesizing data from recent research and case studies, the article argues that proactive policy adaptation is imperative to harness AI's potential while safeguarding the core values of academic integrity and equity.", 'abstract_zh': '生成式人工智能工具的迅速普及——尤其是大型语言模型（LLMs）如ChatGPT——正在变革高等教育。发达国家的大学 increasingly将这些技术整合到科研、教学和评估中。一方面，LLMs可以通过简化文献回顾、促进创意生成、协助编程和数据分析，甚至支持资助提案撰写来提高生产力。另一方面，它们的使用引发了对学术诚信、伦理边界和公平获取的重大关切。近期实证研究表明，几乎有47%的学生在其作业中使用了LLMs——其中39%用于考试问题，7%用于整个作业——而当前检测工具的准确率约为88%，留下12%的误差率。本文批判性地探讨生成式AI提供的机遇，探索它所带来的多方面挑战，并提出切实可行的政策解决方案。重点在于重新设计评估体系以抵御AI威胁、提升教职员工和学生培训、实施多层次的执法机制以及界定合理使用范围。通过综合近期研究和案例研究的数据，本文主张，为了充分利用AI潜力并保障学术诚信和公平的核心价值，必须进行前瞻性的政策调整。', 'title_zh': '适应生成式AI的高校政策调整：高等教育中的机遇、挑战与政策解决方案'}
{'arxiv_id': 'arXiv:2506.22200', 'title': 'EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement Learning Framework', 'authors': 'Chen Wang, Lai Wei, Yanzhi Zhang, Chenyang Shao, Zedong Dan, Weiran Huang, Yue Wang, Yuzhi Zhang', 'link': 'https://arxiv.org/abs/2506.22200', 'abstract': "Recent advances in reinforcement learning (RL) have significantly enhanced the reasoning capabilities of large language models (LLMs). Group Relative Policy Optimization (GRPO), an efficient variant of PPO that lowers RL's computational cost, still faces limited exploration, low sample efficiency and instability, constraining its performance on complex reasoning tasks. To address these limitations, we introduce EFRame, an Exploration-Filtering-Replay framework that systematically augments GRPO along three critical dimensions. EFRame performs additional rollouts to explore high-quality trajectories, applies online filtering to eliminate low-quality samples that introduce noise and variance, and leverages experience replay to repeatedly exploit rare but informative samples. EFRame establishes a complete and stable learning cycle, guiding the model through a structured transition from exploration to convergence. Our experiments across a variety of reasoning benchmarks demonstrate that EFRame not only improves the robustness and efficiency of training, but also enables access to deeper reasoning capabilities that remain unattainable under vanilla GRPO. Furthermore, EFRame enables a more fine-grained categorization of training samples, allowing for a deeper analysis of how different types of samples contribute to the learning process in RL. Our code is available at this https URL.", 'abstract_zh': 'Recent Advances in Exploration-Filtering-Replay for Enhancing Group Relative Policy Optimization in Reinforcement Learning', 'title_zh': 'EFRame：通过探索-过滤-重播强化学习框架实现更深的推理'}
{'arxiv_id': 'arXiv:2506.22026', 'title': 'Literature-Grounded Novelty Assessment of Scientific Ideas', 'authors': 'Simra Shahid, Marissa Radensky, Raymond Fok, Pao Siangliulue, Daniel S. Weld, Tom Hope', 'link': 'https://arxiv.org/abs/2506.22026', 'abstract': 'Automated scientific idea generation systems have made remarkable progress, yet the automatic evaluation of idea novelty remains a critical and underexplored challenge. Manual evaluation of novelty through literature review is labor-intensive, prone to error due to subjectivity, and impractical at scale. To address these issues, we propose the Idea Novelty Checker, an LLM-based retrieval-augmented generation (RAG) framework that leverages a two-stage retrieve-then-rerank approach. The Idea Novelty Checker first collects a broad set of relevant papers using keyword and snippet-based retrieval, then refines this collection through embedding-based filtering followed by facet-based LLM re-ranking. It incorporates expert-labeled examples to guide the system in comparing papers for novelty evaluation and in generating literature-grounded reasoning. Our extensive experiments demonstrate that our novelty checker achieves approximately 13% higher agreement than existing approaches. Ablation studies further showcases the importance of the facet-based re-ranker in identifying the most relevant literature for novelty evaluation.', 'abstract_zh': '基于LLM的检索增强生成框架：Idea Novelty Checker在自动化创新性评估中的应用', 'title_zh': '基于文献的新颖性评估方法'}
{'arxiv_id': 'arXiv:2506.21972', 'title': 'Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses', 'authors': 'Mohamed Ahmed, Mohamed Abdelmouty, Mingyu Kim, Gunvanth Kandula, Alex Park, James C. Davis', 'link': 'https://arxiv.org/abs/2506.21972', 'abstract': "The advancement of Pre-Trained Language Models (PTLMs) and Large Language Models (LLMs) has led to their widespread adoption across diverse applications. Despite their success, these models remain vulnerable to attacks that exploit their inherent weaknesses to bypass safety measures. Two primary inference-phase threats are token-level and prompt-level jailbreaks. Token-level attacks embed adversarial sequences that transfer well to black-box models like GPT but leave detectable patterns and rely on gradient-based token optimization, whereas prompt-level attacks use semantically structured inputs to elicit harmful responses yet depend on iterative feedback that can be unreliable. To address the complementary limitations of these methods, we propose two hybrid approaches that integrate token- and prompt-level techniques to enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the newly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and Llama models. GCG + PAIR consistently raised attack-success rates over its constituent techniques on undefended models; for instance, on Llama-3, its Attack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's 58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of WordGame maintaining a high ASR of over 80% even under stricter evaluators like Mistral-Sorry-Bench. Crucially, both hybrids retained transferability and reliably pierced advanced defenses such as Gradient Cuff and JBShield, which fully blocked single-mode attacks. These findings expose previously unreported vulnerabilities in current safety stacks, highlight trade-offs between raw success and defensive robustness, and underscore the need for holistic safeguards against adaptive adversaries.", 'abstract_zh': '预训练语言模型和大型语言模型的进步推动了它们在各种应用中的广泛应用。尽管这些模型取得了成功，但仍易受到利用其固有弱点的攻击，从而绕过安全措施。两种主要的推理阶段威胁是tokens层面和prompt层面的劫持攻击。tokens层面的攻击通过嵌入对抗序列来实现，这些序列在如GPT的黑盒模型中表现良好，但会产生可检测的模式，并依赖于基于梯度的token优化，而prompt层面的攻击使用语义结构化的输入来触发有害响应，但依赖于迭代反馈，这种方法可能不可靠。为了弥补这两种方法的互补局限性，我们提出了一种结合tokens层面和prompt层面技术的混合方法，以增强不同预训练语言模型的劫持攻击效果。GCG + PAIR和新探索的GCG + WordGame混合方法在多个Vicuna和Llama模型上进行了评估。GCG + PAIR在未防御模型中的一贯攻击成功率高于其组成部分技术，例如，在Llama-3上，其攻击成功率为91.6%，远高于PAIR的基线58.4%。同时，GCG + WordGame在更严格的评估器如Mistral-Sorry-Bench下，保持了高水平的攻击成功率超过80%，与WordGame的原始性能相当。关键的是，这两种混合方法都保持了可移植性，并能可靠地突破Gradient Cuff和JBShield等高级防御，这些防御完全阻止了一类模式的攻击。这些发现揭示了当前安全堆栈中的未报告漏洞，强调了在面对适应性对手时原始成功与防御鲁棒性之间权衡的重要性，并突显了需要整体防护措施。', 'title_zh': '推进 Jailbreak 策略：一种利用大型语言模型漏洞并绕过现代防御的混合方法'}
{'arxiv_id': 'arXiv:2506.21964', 'title': 'Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics', 'authors': 'Michael A. Riegler, Kristoffer Herland Hellton, Vajira Thambawita, Hugo L. Hammer', 'link': 'https://arxiv.org/abs/2506.21964', 'abstract': 'Selecting prior distributions in Bayesian statistics is challenging, resource-intensive, and subjective. We analyze using large-language models (LLMs) to suggest suitable, knowledge-based informative priors. We developed an extensive prompt asking LLMs not only to suggest priors but also to verify and reflect on their choices.\nWe evaluated Claude Opus, Gemini 2.5 Pro, and ChatGPT-4o-mini on two real datasets: heart disease risk and concrete strength. All LLMs correctly identified the direction for all associations (e.g., that heart disease risk is higher for males). The quality of suggested priors was measured by their Kullback-Leibler divergence from the maximum likelihood estimator\'s distribution.\nThe LLMs suggested both moderately and weakly informative priors. The moderate priors were often overconfident, resulting in distributions misaligned with the data. In our experiments, Claude and Gemini provided better priors than ChatGPT. For weakly informative priors, a key performance difference emerged: ChatGPT and Gemini defaulted to an "unnecessarily vague" mean of 0, while Claude did not, demonstrating a significant advantage.\nThe ability of LLMs to identify correct associations shows their great potential as an efficient, objective method for developing informative priors. However, the primary challenge remains in calibrating the width of these priors to avoid over- and under-confidence.', 'abstract_zh': '使用大型语言模型在贝叶斯统计中选择先验分布：挑战、评估与发现', 'title_zh': '使用大型语言模型为贝叶斯统计建议信息性先验分布'}
{'arxiv_id': 'arXiv:2506.21931', 'title': 'ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation', 'authors': 'Reza Yousefi Maragheh, Pratheek Vadla, Priyank Gupta, Kai Zhao, Aysenur Inan, Kehui Yao, Jianpeng Xu, Praveen Kanumala, Jason Cho, Sushant Kumar', 'link': 'https://arxiv.org/abs/2506.21931', 'abstract': 'Retrieval-Augmented Generation (RAG) has shown promise in enhancing recommendation systems by incorporating external context into large language model prompts. However, existing RAG-based approaches often rely on static retrieval heuristics and fail to capture nuanced user preferences in dynamic recommendation scenarios. In this work, we introduce ARAG, an Agentic Retrieval-Augmented Generation framework for Personalized Recommendation, which integrates a multi-agent collaboration mechanism into the RAG pipeline. To better understand the long-term and session behavior of the user, ARAG leverages four specialized LLM-based agents: a User Understanding Agent that summarizes user preferences from long-term and session contexts, a Natural Language Inference (NLI) Agent that evaluates semantic alignment between candidate items retrieved by RAG and inferred intent, a context summary agent that summarizes the findings of NLI agent, and an Item Ranker Agent that generates a ranked list of recommendations based on contextual fit. We evaluate ARAG accross three datasets. Experimental results demonstrate that ARAG significantly outperforms standard RAG and recency-based baselines, achieving up to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an ablation study to analyse the effect by different components of ARAG. Our findings highlight the effectiveness of integrating agentic reasoning into retrieval-augmented recommendation and provide new directions for LLM-based personalization.', 'abstract_zh': '基于代理增强检索的个性化推荐框架（ARAG）', 'title_zh': 'ARAG：自主检索增强生成的个性化推荐'}
{'arxiv_id': 'arXiv:2506.21874', 'title': 'On the Feasibility of Poisoning Text-to-Image AI Models via Adversarial Mislabeling', 'authors': 'Stanley Wu, Ronik Bhaskar, Anna Yoo Jeong Ha, Shawn Shan, Haitao Zheng, Ben Y. Zhao', 'link': 'https://arxiv.org/abs/2506.21874', 'abstract': 'Today\'s text-to-image generative models are trained on millions of images sourced from the Internet, each paired with a detailed caption produced by Vision-Language Models (VLMs). This part of the training pipeline is critical for supplying the models with large volumes of high-quality image-caption pairs during training. However, recent work suggests that VLMs are vulnerable to stealthy adversarial attacks, where adversarial perturbations are added to images to mislead the VLMs into producing incorrect captions.\nIn this paper, we explore the feasibility of adversarial mislabeling attacks on VLMs as a mechanism to poisoning training pipelines for text-to-image models. Our experiments demonstrate that VLMs are highly vulnerable to adversarial perturbations, allowing attackers to produce benign-looking images that are consistently miscaptioned by the VLM models. This has the effect of injecting strong "dirty-label" poison samples into the training pipeline for text-to-image models, successfully altering their behavior with a small number of poisoned samples. We find that while potential defenses can be effective, they can be targeted and circumvented by adaptive attackers. This suggests a cat-and-mouse game that is likely to reduce the quality of training data and increase the cost of text-to-image model development. Finally, we demonstrate the real-world effectiveness of these attacks, achieving high attack success (over 73%) even in black-box scenarios against commercial VLMs (Google Vertex AI and Microsoft Azure).', 'abstract_zh': '今天用于文本到图像生成的模型是在互联网上获取的数百万张图像上训练的，每张图像都配有一个由视觉语言模型（VLMs）生成的详细描述。这一部分的训练管道对于向模型提供大量高质量的图像描述配对至关重要。然而，近期的研究表明，VLMs容易受到隐蔽的 adversarial 攻击，即通过在图像中添加对抗性扰动来误导 VLMs 生成错误的描述。\n\n在本文中，我们探讨了利用对抗性误标记攻击 VLMs 作为污染文本到图像模型训练管道的一种机制的可能性。我们的实验表明，VLMs 对对抗性扰动极为敏感，攻击者可以生成看似无害的图像，但这些图像始终会被 VLM 模型错误地描述。这导致为文本到图像模型训练管道注入了强大的“脏标签”污染样本，少量污染样本就能成功改变其行为。我们发现，尽管潜在的防御措施可能是有效的，但攻击者可以通过适应性手段来针对并规避这些防御。这表明一个“猫捉老鼠”的游戏可能会降低训练数据的质量，并增加文本到图像模型开发的成本。最后，我们展示了这些攻击在现实世界中的有效性，在商业 VLMs（Google Vertex AI 和 Microsoft Azure）的黑盒场景中，成功攻击率高达 73%。', 'title_zh': '基于对抗性误标签的文本到图像AI模型中毒可行性研究'}
{'arxiv_id': 'arXiv:2506.21873', 'title': 'Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning', 'authors': 'Tzu-Chun Chien, Chieh-Kai Lin, Shiang-Feng Tsai, Ruei-Chi Lai, Hung-Jen Chen, Min Sun', 'link': 'https://arxiv.org/abs/2506.21873', 'abstract': "Recent Multimodal Large Language Models (MLLMs) have demonstrated strong performance in visual grounding, establishing themselves as a general interface for various vision-language applications. This progress has driven the development of token pruning methods to mitigate the high computational costs associated with processing numerous visual tokens. However, we observe that pruning significantly weakens the model's grounding ability, leading to incorrect predictions and drastic performance degradation. In Referring Expression Comprehension (REC), for instance, pruning causes the accuracy of LLaVA on the RefCOCO validation set to drop from 56.14% to 15.34%. Our analysis identifies misaligned position IDs after pruning as the primary cause of this degradation, as both the order and value of these IDs are crucial for maintaining performance in grounding tasks. To address this issue, we propose Grounding-Aware Token Pruning (GAP), a simple yet effective adjustment to position IDs that recovers REC accuracy back to 51.42%, which is 90% of the original performance in the without pruning setting, all while requiring no additional training, memory, or computational overhead. Applied to models such as Shikra, MiniGPTv2, and the LLaVA series, our method consistently improves performance across various token pruning strategies.", 'abstract_zh': 'Recent 多模态大语言模型 (MLLMs) 在视觉定位任务中的表现强烈证明了它们作为各类视觉-语言应用通用接口的优势。这一进展推动了消除处理大量视觉标记高计算成本的标记剪枝方法的发展。然而，我们观察到剪枝显著削弱了模型的视觉定位能力，导致错误预测和性能急剧下降。例如，在参照表达理解 (REC) 中，剪枝导致 LLaVA 在 RefCOCO 验证集上的准确率从 56.14% 降至 15.34%。我们的分析指出，剪枝后位置 ID 的不对齐是导致性能下降的主要原因，因为这些 ID 的顺序和值对于保持定位任务中的性能至关重要。为了解决这一问题，我们提出了一种简单而有效的定位感知标记剪枝 (GAP) 方法，该方法通过调整位置 ID 回复了 REC 准确率至 51.42%，相当于剪枝前性能的 90%，且无需额外训练、内存或计算开销。我们将该方法应用于 Shikra、MiniGPTv2 和 LLaVA 系列模型，在各种标记剪枝策略中均实现了性能提升。', 'title_zh': '基于地面性的Token裁剪：在裁剪导致视觉地面性性能急剧下降后的恢复方法'}
{'arxiv_id': 'arXiv:2506.21864', 'title': 'DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE', 'authors': 'Hang Shao, Heting Gao, Yunhang Shen, Jiawei Chen, Lijiang Li, Zuwei Long, Bo Tong, Ke Li, Xing Sun', 'link': 'https://arxiv.org/abs/2506.21864', 'abstract': 'Native multimodal large language models (MLLMs) restructure a single large language model (LLM) into a spoken language model (SLM) capable of both speech and text generation. Compared to modular and aligned MLLMs, native MLLMs preserve richer paralinguistic features such as emotion and prosody, and generate speech responses directly within the backbone LLM rather than using a separate speech decoder. This integration also results in lower response latency and smoother interaction. However, native MLLMs suffer from catastrophic forgetting and performance degradation because the available paired speech-text data is insufficient to support the pretraining of MLLMs compared to the vast amount of text data required to pretrain text LLMs. To address this issue, we propose DeepTalk, a framework for adaptive modality expert learning based on a Mixture of Experts (MoE) architecture. DeepTalk first adaptively distinguishes modality experts according to their modality load within the LLM. Each modality expert then undergoes specialized single-modality training, followed by joint multimodal collaborative training. As a result, DeepTalk incurs only a 5.5% performance drop compared to the original LLM, which is significantly lower than the average performance drop of over 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par with modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within 0.5 seconds, ensuring a seamless and intelligent speech interaction experience. Code and models are released at this https URL.', 'abstract_zh': '原生多模态大语言模型 (Native MLLMs) 将单一的大语言模型 (LLM) 重新构建成既能生成文字又能生成语音的语音语言模型 (SLM)。与模块化和对齐的 MLLMs 相比，原生 MLLMs 保留了更多的副语言特征，如情感和语调，并能够在骨干 LLM 内直接生成语音响应，而无需使用单独的语音解码器。这种集成还导致响应延迟降低和交互更为流畅。然而，原生 MLLMs 因为可用的配对语音-文本数据不足而遭受灾难性遗忘和性能下降，支持 MLLMs 预训练的数据量远少于支持文本 LLMs 预训练所需的巨大文本数据量。为了解决这一问题，我们提出 DeepTalk 框架，这是一种基于Experts混合架构（MoE）的自适应模态专家学习框架。DeepTalk 首先根据 LLM 中的模态负载自适应地区分模态专家，然后对每个模态专家进行专门的单模态训练，之后进行联合多模态协作训练。结果表明，与原生 MLLMs（如 GLM-4-Voice）相比，DeepTalk 的性能下降仅为 5.5%，显著低于模块化 MLLMs 平均超过 20% 的性能下降水平。同时，端到端对话延迟保持在 0.5 秒以内，确保了无缝和智能化的语音交互体验。代码和模型已发布于此网址。', 'title_zh': 'DeepTalk: 向无缝和智能的自适应模态特定MOE语音交互迈进'}
{'arxiv_id': 'arXiv:2506.21862', 'title': 'LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs', 'authors': 'Boyuan Sun, Jiaxing Zhao, Xihan Wei, Qibin Hou', 'link': 'https://arxiv.org/abs/2506.21862', 'abstract': 'In this paper, we present LLaVA-Scissor, a training-free token compression strategy designed for video multimodal large language models. Previous methods mostly attempt to compress tokens based on attention scores, but fail to effectively capture all semantic regions and often lead to token redundancy. Differently, we propose to leverage the Semantic Connected Components (SCC) approach that assigns tokens to distinct semantic regions within the token set, ensuring comprehensive semantic coverage. The outcome is a two-step spatio-temporal token compression strategy that utilizes SCC in both spatial and temporal domains. This strategy can effectively compress tokens by representing the entire video with a set of non-overlapping semantic tokens. We conduct extensive evaluations of the token compression capabilities of LLaVA-Scissor across diverse video understanding benchmarks, including video question answering, long video understanding, and comprehensive multi-choices benchmarks. Experimental results show that the proposed LLaVA-Scissor outperforms other token compression methods, achieving superior performance in various video understanding benchmarks, particularly at low token retention ratios. Project page: this https URL.', 'abstract_zh': '本文提出LLaVA-Scissor，这是一种无需训练的token压缩策略，旨在用于视频多模态大型语言模型。以往方法主要尝试根据注意分数来压缩token，但未能有效捕捉所有语义区域，并且通常会导致token冗余。不同的是，我们提出利用语义连通组件（SCC）方法，将token分配到token集合中的不同语义区域，确保全面的语义覆盖。该策略是一种两步时空token压缩策略，在空间域和时间域均利用SCC。该策略通过使用一组不重叠的语义token来表示整个视频，从而有效进行token压缩。我们在包括视频问答、长视频理解和综合性多选择基准等多种视频理解基准上对LLaVA-Scissor的token压缩能力进行了广泛评估。实验结果表明，所提出的LLaVA-Scissor优于其他token压缩方法，在各种视频理解基准上表现出更优性能，特别是在低token保留率情况下。项目页面：this https URL。', 'title_zh': 'LLaVA-Scissor: 基于语义连接组件的tokens压缩方法用于视频LLMs'}
{'arxiv_id': 'arXiv:2506.21849', 'title': 'The Consistency Hypothesis in Uncertainty Quantification for Large Language Models', 'authors': 'Quan Xiao, Debarun Bhattacharjya, Balaji Ganesan, Radu Marinescu, Katsiaryna Mirylenka, Nhan H Pham, Michael Glass, Junkyu Lee', 'link': 'https://arxiv.org/abs/2506.21849', 'abstract': "Estimating the confidence of large language model (LLM) outputs is essential for real-world applications requiring high user trust. Black-box uncertainty quantification (UQ) methods, relying solely on model API access, have gained popularity due to their practical benefits. In this paper, we examine the implicit assumption behind several UQ methods, which use generation consistency as a proxy for confidence, an idea we formalize as the consistency hypothesis. We introduce three mathematical statements with corresponding statistical tests to capture variations of this hypothesis and metrics to evaluate LLM output conformity across tasks. Our empirical investigation, spanning 8 benchmark datasets and 3 tasks (question answering, text summarization, and text-to-SQL), highlights the prevalence of the hypothesis under different settings. Among the statements, we highlight the `Sim-Any' hypothesis as the most actionable, and demonstrate how it can be leveraged by proposing data-free black-box UQ methods that aggregate similarities between generations for confidence estimation. These approaches can outperform the closest baselines, showcasing the practical value of the empirically observed consistency hypothesis.", 'abstract_zh': '估计大型语言模型(LLM)输出的置信度对于需要高度用户信任的实际应用至关重要。基于模型API的黑盒不确定性量化(UQ)方法因其实际优势而日益流行。本文探讨了几种UQ方法背后隐含的假设，即使用生成一致性作为置信度的代理，我们将这一想法形式化为一致性假设。我们提出了三个数学表述及其相应的统计检验来捕捉这一假设的不同变体，并引入了评估LLM输出一致性表现的指标。我们的实证研究覆盖8个基准数据集和3个任务（问答、文本摘要和文本到SQL），揭示了在不同情境下该假设的普遍性。在这几个表述中，我们强调“Sim-Any”假设最具操作性，并展示了如何通过提出基于生成间相似性的数据免费黑盒UQ方法来利用这一假设进行置信度估计。这些方法在与最近基线方法的比较中表现出色，展示了实验观察到的一致性假设的实际价值。', 'title_zh': '不确定性量化中的大语言模型一致性假设'}
{'arxiv_id': 'arXiv:2506.21817', 'title': 'Exploring the Structure of AI-Induced Language Change in Scientific English', 'authors': 'Riley Galpin, Bryce Anderson, Tom S. Juzek', 'link': 'https://arxiv.org/abs/2506.21817', 'abstract': 'Scientific English has undergone rapid and unprecedented changes in recent years, with words such as "delve," "intricate," and "crucial" showing significant spikes in frequency since around 2022. These changes are widely attributed to the growing influence of Large Language Models like ChatGPT in the discourse surrounding bias and misalignment. However, apart from changes in frequency, the exact structure of these linguistic shifts has remained unclear. The present study addresses this and investigates whether these changes involve the replacement of synonyms by suddenly \'spiking words,\' for example, "crucial" replacing "essential" and "key," or whether they reflect broader semantic and pragmatic qualifications. To further investigate structural changes, we include part of speech tagging in our analysis to quantify linguistic shifts over grammatical categories and differentiate between word forms, like "potential" as a noun vs. as an adjective. We systematically analyze synonym groups for widely discussed \'spiking words\' based on frequency trends in scientific abstracts from PubMed. We find that entire semantic clusters often shift together, with most or all words in a group increasing in usage. This pattern suggests that changes induced by Large Language Models are primarily semantic and pragmatic rather than purely lexical. Notably, the adjective "important" shows a significant decline, which prompted us to systematically analyze decreasing lexical items. Our analysis of "collapsing" words reveals a more complex picture, which is consistent with organic language change and contrasts with the patterns of the abrupt spikes. These insights into the structure of language change contribute to our understanding of how language technology continues to shape human language.', 'abstract_zh': '近年来，科学英语经历了快速且前所未有的变化，自2022年起，“delve”、“intricate”和“crucial”等词汇的使用频率显著增加。这些变化通常被认为是大型语言模型如ChatGPT在偏见和不对齐议题讨论中影响力的增强所致。然而，除了频率的变化，这些语言变化的具体结构仍然不清楚。本研究旨在探讨这些变化是否涉及到同义词被突然出现的“突变词汇”所取代，例如，“crucial”取代“essential”和“key”，或者这些变化是否反映了更广泛的语义和语用上的调整。为进一步研究结构性变化，我们将词性标注纳入分析，以量化不同语法类别的语言变化，并区分如“potential”作为名词与作为形容词的不同形式。基于PubMed中科学摘要的频率趋势，我们系统分析了广泛讨论的“突变词汇”的同义词组。我们发现，整个语义集群往往一起变化，整个组中的大多数或所有词汇的使用频率均增加。这一模式表明，大型语言模型引发的变化主要涉及语义和语用层面，而非单纯词汇层面。值得注意的是，“important”这一形容词出现了显著下降，促使我们对下降的词汇进行系统分析。我们对“坍缩”词汇的分析揭示了一个更复杂的图景，与突变词汇模式一致，与有机语言变化的路径相吻合。这些对语言变化结构的见解有助于我们理解语言技术如何继续塑造人类语言。', 'title_zh': '探索由人工智能引发的科学英语语言变化结构'}
{'arxiv_id': 'arXiv:2506.21783', 'title': 'Evaluating List Construction and Temporal Understanding capabilities of Large Language Models', 'authors': 'Alexandru Dumitru, V Venktesh, Adam Jatowt, Avishek Anand', 'link': 'https://arxiv.org/abs/2506.21783', 'abstract': 'Large Language Models (LLMs) have demonstrated immense advances in a wide range of natural language tasks. However, these models are susceptible to hallucinations and errors on particularly temporal understanding tasks involving multiple entities in answers. In such tasks, they fail to associate entities with accurate time intervals, generate a complete list of entities in answers or reason about events associated with specific temporal bounds. Existing works do not extensively evaluate the abilities of the model to perform implicit and explicit temporal understanding in a list answer construction setup. To bridge this gap, we propose the Time referenced List based Question Answering or TLQA benchmark that requires structured answers in list format aligned with corresponding time periods. Our TLQA benchmark, requires both list construction and temporal understanding simultaneously, which to the best of our knowledge has not been explored in prior benchmarks. We investigate the temporal understanding and list construction capabilities of state-of-the-art generative models on TLQA in closed-book and open-domain settings. Our findings reveal significant shortcomings in current models, particularly their inability to provide complete answers and temporally align facts in a closed-book setup and the need to improve retrieval in open-domain setup, providing clear future directions for research on TLQA. The benchmark and code at this https URL.', 'abstract_zh': '大规模语言模型（LLMs）在广泛自然语言任务中取得了巨大的进展。然而，在涉及多个实体特别是时间理解任务中，这些模型容易出现幻觉和错误。在这种任务中，它们无法将实体与准确的时间区间关联起来，生成答案中的完整实体列表，或关于特定时间界限的事件进行推理。现有工作没有广泛评估模型在列表答案构建设置中进行显性和隐性时间理解的能力。为弥补这一不足，我们提出了时间参考列表问答基准（TLQA），要求构建结构化的、与相应时间区间对齐的答案列表。TLQA基准同时要求列表构建和时间理解能力，据我们所知，此前的基准中尚未有此类探索。我们在封闭书本和开放域设置下调查了领先生成模型在TLQA上的时间理解能力和列表构建能力。我们的发现揭示了当前模型的重大缺陷，特别是在封闭书本设置下不能提供完整的答案和时间对齐的事实，在开放域设置下需要改进检索，为TLQA研究提供了明确的未来发展方向。基准和代码请访问此链接。', 'title_zh': '评估大型语言模型的列表构建能力和时间理解能力'}
{'arxiv_id': 'arXiv:2506.21655', 'title': 'APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization', 'authors': 'Minjie Hong, Zirun Guo, Yan Xia, Zehan Wang, Ziang Zhang, Tao Jin, Zhou Zhao', 'link': 'https://arxiv.org/abs/2506.21655', 'abstract': 'Multimodal Large Language Models (MLLMs) are powerful at integrating diverse data, but they often struggle with complex reasoning. While Reinforcement learning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky. Common issues include a drop in performance on general tasks and the generation of overly detailed or "overthinking" reasoning. Our work investigates how the KL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric Policy Optimization (APO) to address these issues, which divides the sampled responses into positive and negative groups. For positive samples, Difficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically adjust the KL divergence weight based on their difficulty. This method prevents policy entropy from dropping sharply, improves training stability, utilizes samples better, and preserves the model\'s existing knowledge. For negative samples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to penalize overly long responses. This helps mitigate overthinking and encourages more concise reasoning while preserving the model\'s explorative capacity. We apply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B significantly enhances reasoning capabilities, showing an average 7\\% gain over the base model and outperforming larger MLLMs (7-11B) on various reasoning benchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade on general tasks, View-R1-3B maintains consistent improvement, demonstrating superior generalization. These results highlight the effectiveness and broad applicability of our DADS and STCR techniques for advancing complex multimodal reasoning in MLLMs. The code will be made available at this https URL.', 'abstract_zh': '多模态大规模语言模型在复杂推理中的生成式策略优化与过拟合调节研究', 'title_zh': 'APO: 增强MLLMs推理能力的异构策略优化'}
{'arxiv_id': 'arXiv:2506.21638', 'title': 'IRanker: Towards Ranking Foundation Model', 'authors': 'Tao Feng, Zhigang Hua, Zijie Lei, Yan Xie, Shuang Yang, Bo Long, Jiaxuan You', 'link': 'https://arxiv.org/abs/2506.21638', 'abstract': 'Ranking tasks are ubiquitous, encompassing applications such as recommendation systems, LLM routing, and item re-ranking. We propose to unify these tasks using a single ranking foundation model (FM), as it eliminates the need for designing different models for each specific ranking task. However, unlike general supervision tasks in LLMs, ranking tasks do not have clear labels for supervision, posing great challenges to developing a ranking FM. To overcome these challenges, we propose IRanker, a ranking FM framework with reinforcement learning (RL) and iterative decoding. Our insight is to decompose the complex ranking task into an iterative decoding process that eliminates the worst candidate from the candidate pool step by step, which significantly reduces the output combinatorial space and better utilizes the limited context length during RL training. We meticulously train and comprehensively evaluate an IRanker-3B model on nine datasets across three scenarios: recommendation, routing, and passage ranking. The results show that a single IRanker-3B achieves state-of-the-art results on several datasets compared to models of similar size, and even surpasses the performance of larger models on certain datasets. We further demonstrate the effectiveness of our RL design and the robustness of the iterative mechanism across different LLM sizes. Moreover, we conducted both in-domain and out-of-domain zero-shot generalization experiments, which showed that IRanker-3B achieved good generalization on in-domain ranking tasks compared to the base LLM by at least 5% improvement. Surprisingly, on out-of-domain generic LLM tasks, IRanker-3B outperformed the base model by at least 9% on GSM8K, IFEval, and MathQA. In addition, the thoughts generated by IRanker-3B during training could further enhance zero-shot LLM performance.', 'abstract_zh': '统一排序任务的强化学习迭代排序框架IRanker', 'title_zh': 'IRanker: 面向基础模型的排序方法'}
{'arxiv_id': 'arXiv:2506.21622', 'title': 'Adapting Foundation Speech Recognition Models to Impaired Speech: A Semantic Re-chaining Approach for Personalization of German Speech', 'authors': 'Niclas Pokel, Pehuén Moure, Roman Boehringer, Yingqiang Gao', 'link': 'https://arxiv.org/abs/2506.21622', 'abstract': 'Speech impairments caused by conditions such as cerebral palsy or genetic disorders pose significant challenges for automatic speech recognition (ASR) systems. Despite recent advances, ASR models like Whisper struggle with non-normative speech due to limited training data and the difficulty of collecting and annotating non-normative speech samples. In this work, we propose a practical and lightweight pipeline to personalize ASR models, formalizing the selection of words and enriching a small, speech-impaired dataset with semantic coherence. Applied to data from a child with a structural speech impairment, our approach shows promising improvements in transcription quality, demonstrating the potential to reduce communication barriers for individuals with atypical speech patterns.', 'abstract_zh': '由脑瘫或遗传性疾病等条件引起的语言障碍对自动语音识别（ASR）系统构成了重大挑战。尽管最近取得了进展，但像Whisper这样的ASR模型仍然难以处理非规范性语音，因为训练数据有限，收集和标注非规范性语音样本也颇具难度。在本工作中，我们提出了一个切实可行且轻量级的流水线来个性化ASR模型，正式化单词的选择并用语义连贯性丰富小型受损语音数据集。应用于具有结构化语音障碍儿童的数据，我们的方法在转写质量上显示出有希望的改进，证明了减少具有非典型语音模式个体的沟通障碍的潜力。', 'title_zh': '适配受损语音的预训练语音识别模型：一种针对德语语音个性化处理的语义重链方法'}
{'arxiv_id': 'arXiv:2506.21621', 'title': 'The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs', 'authors': 'Jasper Dekoninck, Ivo Petrov, Kristian Minchev, Mislav Balunovic, Martin Vechev, Miroslav Marinov, Maria Drencheva, Lyuba Konova, Milen Shumanov, Kaloyan Tsvetkov, Nikolay Drenchev, Lazar Todorov, Kalina Nikolova, Nikolay Georgiev, Vanesa Kalinkova, Margulan Ismoldayev', 'link': 'https://arxiv.org/abs/2506.21621', 'abstract': 'In recent months, large language models (LLMs) have made significant progress in mathematical proof generation, but further advancement is hindered by the lack of a large-scale, high-quality dataset of human-evaluated proofs. While expensive to create, such a dataset is essential for driving improvements in training and enabling a rigorous analysis of proof generation capabilities. In this work, we present the Open Proof Corpus (OPC), a dataset comprising over 5,000 human-evaluated proofs produced by state-of-the-art LLMs. The OPC was specifically designed for broad applicability and downstream usage in proof generation research and is the first to include a substantial number of correct, LLM-generated solutions to problems from prestigious mathematics competitions such as the USAMO and IMO. Using the OPC, we explore critical questions in automated proof generation: (1) the performance gap between natural language and formal proof generation, (2) the discrepancy between final-answer accuracy and full-proof validity, and (3) the impact of best-of-n selection on proof quality. Finally, to showcase the utility of the OPC, we finetune an 8B-parameter model on the dataset, obtaining a model that performs on par with the best model, Gemini-2.5-Pro, on the task of evaluating proof correctness.', 'abstract_zh': '近期，大规模语言模型在数学证明生成方面取得了显著进步，但由于缺乏大量高质量的人工评估证明数据集，进一步发展受到限制。尽管创建成本较高，但这样的数据集对于推动训练改进和证明生成能力的严谨分析是必不可少的。在本文中，我们介绍了开源证明语料库（OPC），一个包含超过5,000个人工评估证明的数据集，这些证明是由最先进的大规模语言模型生成的。OPC 特别设计以促进证明生成研究的广泛应用，并且是首次包含大量来自USAMO和IMO等知名数学竞赛问题的正确解证明的数据集。利用OPC，我们探讨了自动证明生成中的关键问题：（1）自然语言与形式证明生成之间的性能差距，（2）最终答案准确性与完整证明有效性的差异，以及（3）最佳选择对证明质量的影响。最后，为了展示OPC的应用价值，我们在数据集上微调了一个8亿参数的模型，得到的模型在证明正确性评估任务上与最佳模型Gemini-2.5-Pro表现相当。', 'title_zh': '开放证明语料库：大规模LLM生成数学证明研究'}
{'arxiv_id': 'arXiv:2506.21620', 'title': 'How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit', 'authors': 'Daniele Cirulli, Giulio Cimini, Giovanni Palermo', 'link': 'https://arxiv.org/abs/2506.21620', 'abstract': 'Large Language Models (LLMs) have recently emerged as powerful tools for natural language generation, with applications spanning from content creation to social simulations. Their ability to mimic human interactions raises both opportunities and concerns, particularly in the context of politically relevant online discussions. In this study, we evaluate the performance of LLMs in replicating user-generated content within a real-world, divisive scenario: Reddit conversations during the 2016 US Presidential election. In particular, we conduct three different experiments, asking GPT-4 to generate comments by impersonating either real or artificial partisan users. We analyze the generated comments in terms of political alignment, sentiment, and linguistic features, comparing them against real user contributions and benchmarking against a null model. We find that GPT-4 is able to produce realistic comments, both in favor of or against the candidate supported by the community, yet tending to create consensus more easily than dissent. In addition we show that real and artificial comments are well separated in a semantically embedded space, although they are indistinguishable by manual inspection. Our findings provide insights on the potential use of LLMs to sneak into online discussions, influence political debate and shape political narratives, bearing broader implications of AI-driven discourse manipulation.', 'abstract_zh': '大型语言模型（LLMs）近年来已成为自然语言生成的强大工具，应用于从内容创作到社会模拟的多个领域。它们模仿人类互动的能力既带来了机会，也引发了担忧，特别是在与政治相关的在线讨论中。本研究评估了LLMs在复制现实世界中具有争议性的场景——2016年美国 Presidential 选举期间的 Reddit 演讲——中生成用户生成内容的性能。特别是在三项不同的实验中，我们要求GPT-4模仿真实或虚构的党派用户来生成评论。我们从政治倾向、情感和语言特征等方面分析了生成的评论，并将其与真实用户贡献进行比较，同时用基准模型进行对照。研究发现，GPT-4能够生成真实且具有说服力的评论，无论是支持还是反对社区支持的候选人，但更倾向于制造共识而非分歧。此外，我们还展示了真实和虚构的评论在语义嵌入的空间中较为分离，但在手动检查时难以区分。我们的研究结果提供了关于LLMs潜入在线讨论、影响政治辩论和塑造政治叙事潜在用途的见解，并具有更广泛的AI驱动话语操纵的含义。', 'title_zh': '大型语言模型在在线对话中如何玩转人类：2016年美国政治在Reddit上的模拟研究'}
{'arxiv_id': 'arXiv:2506.21614', 'title': 'LastingBench: Defend Benchmarks Against Knowledge Leakage', 'authors': 'Yixiong Fang, Tianran Sun, Yuling Shi, Min Wang, Xiaodong Gu', 'link': 'https://arxiv.org/abs/2506.21614', 'abstract': 'The increasing complexity of large language models (LLMs) raises concerns about their ability to "cheat" on standard Question Answering (QA) benchmarks by memorizing task-specific data. This undermines the validity of benchmark evaluations, as they no longer reflect genuine model capabilities but instead the effects of data leakage. While prior work has focused on detecting such leakage, little attention has been given to mitigating its impact and preserving the long-term utility of benchmarks. In this paper, we introduce LastingBench, a novel framework designed to continuously reinforce and safeguard existing benchmarks against knowledge leakage. LastingBench identifies leakage points in the context through perturbation, then rewrites the leakage points to counterfactual ones-disrupting memorization while preserving the benchmark\'s original evaluative intent. Evaluations of state-of-the-art QA benchmarks show significant performance gaps, highlighting the efficacy of LastingBench in reducing memorization effects. LastingBench offers a practical and scalable solution to ensure benchmark robustness over time, promoting fairer and more interpretable evaluations of LLMs.', 'abstract_zh': '大型语言模型（LLMs）复杂性的增加引发了对其通过记忆特定任务数据而在标准问答（QA）基准测试中“作弊”的担忧。这削弱了基准评估的有效性，因为它们不再反映真实的模型能力，而是数据泄露的影响。尽管先前的工作集中在检测这种泄露，但很少有研究关注减轻其影响并维护基准的长期实用性。在本文中，我们提出了LastingBench，这是一种新型框架，旨在不断强化和保护现有基准免受知识泄露的影响。LastingBench通过扰动识别上下文中的泄露点，然后重写泄露点为反事实点，打断记忆现象同时保持基准原始的评估意图。最新问答基准测试的评估显示了显著的性能差距，突显了LastingBench在减少记忆效应方面的效果。LastingBench提供了一种实用且可扩展的解决方案，以确保基准的长期稳健性，促进更公平和更具解释性的大型语言模型评估。', 'title_zh': 'LastingBench: 防护基准免受知识泄漏'}
{'arxiv_id': 'arXiv:2506.21609', 'title': 'From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models', 'authors': 'Junhao Liu, Zhenhao Xu, Yuxin Fang, Yichuan Chen, Zuobin Ying, Wenhan Chang', 'link': 'https://arxiv.org/abs/2506.21609', 'abstract': 'Recently, there have been notable advancements in large language models (LLMs), demonstrating their growing abilities in complex reasoning. However, existing research largely overlooks a thorough and systematic comparison of these models\' reasoning processes and outputs, particularly regarding their self-reflection pattern (also termed "Aha moment") and the interconnections across diverse domains. This paper proposes a novel framework for analyzing the reasoning characteristics of four cutting-edge large reasoning models (GPT-o1, DeepSeek-R1, Kimi-k1.5, and Grok-3) using keywords statistic and LLM-as-a-judge paradigm. Our approach connects their internal thinking processes with their final outputs. A diverse dataset consists of real-world scenario-based questions covering logical deduction, causal inference, and multi-step problem-solving. Additionally, a set of metrics is put forward to assess both the coherence of reasoning and the accuracy of the outputs. The research results uncover various patterns of how these models balance exploration and exploitation, deal with problems, and reach conclusions during the reasoning process. Through quantitative and qualitative comparisons, disparities among these models are identified in aspects such as the depth of reasoning, the reliance on intermediate steps, and the degree of similarity between their thinking processes and output patterns and those of GPT-o1. This work offers valuable insights into the trade-off between computational efficiency and reasoning robustness and provides practical recommendations for enhancing model design and evaluation in practical applications. We publicly release our project at: this https URL', 'abstract_zh': '近年来，大规模语言模型（LLMs）取得了显著进展，展示了其在复杂推理方面的能力。然而，现有研究很大程度上忽视了对这些模型推理过程和输出的全面系统比较，特别是它们的自我反思模式（也称为“恍然大悟”时刻）以及跨不同领域的相互联系。本文提出了一种新的框架，使用关键词统计和LLM-as-a-judge范式来分析四款前沿的大规模推理模型（GPT-o1、DeepSeek-R1、Kimi-k1.5和Grok-3）的推理特征。我们的方法将它们的内部思考过程与其最终输出联系起来。该研究数据集包含了涵盖逻辑推理、因果推断和多步问题解决的现实场景问题。此外，提出了一套指标来评估推理的一致性和输出的准确性。研究结果揭示了这些模型在推理过程中平衡探索与利用、处理问题以及得出结论的各种模式。通过定量和定性比较，这些模型在推理深度、对中间步骤的依赖程度以及思考过程与输出模式与GPT-o1之间的相似度方面存在差异。本文提供了关于计算效率与推理稳健性之间权衡的宝贵见解，并为在实际应用中改进模型设计和评估提供了实用建议。我们的项目已公开发布于：this https URL。', 'title_zh': '从思考到输出：推理语言模型中的链式思考与文本生成特征'}
{'arxiv_id': 'arXiv:2506.21607', 'title': 'CORE-KG: An LLM-Driven Knowledge Graph Construction Framework for Human Smuggling Networks', 'authors': 'Dipak Meher, Carlotta Domeniconi, Guadalupe Correa-Cabrera', 'link': 'https://arxiv.org/abs/2506.21607', 'abstract': 'Human smuggling networks are increasingly adaptive and difficult to analyze. Legal case documents offer valuable insights but are unstructured, lexically dense, and filled with ambiguous or shifting references-posing challenges for automated knowledge graph (KG) construction. Existing KG methods often rely on static templates and lack coreference resolution, while recent LLM-based approaches frequently produce noisy, fragmented graphs due to hallucinations, and duplicate nodes caused by a lack of guided extraction. We propose CORE-KG, a modular framework for building interpretable KGs from legal texts. It uses a two-step pipeline: (1) type-aware coreference resolution via sequential, structured LLM prompts, and (2) entity and relationship extraction using domain-guided instructions, built on an adapted GraphRAG framework. CORE-KG reduces node duplication by 33.28%, and legal noise by 38.37% compared to a GraphRAG-based baseline-resulting in cleaner and more coherent graph structures. These improvements make CORE-KG a strong foundation for analyzing complex criminal networks.', 'abstract_zh': 'Human走私网络日益适应性强且难以分析。法律案例文件提供了宝贵见解但结构松散、词汇密集且包含模糊或变化的参考，这对自动化知识图谱(KG)构建构成了挑战。现有KG方法通常依赖静态模板并且缺乏指代消解，而近期基于LLM的方法经常生成噪声大、碎片化的图，并且由于缺乏引导式提取导致节点重复。我们提出CORE-KG，这是一种用于从法律文本构建可解释知识图谱的模块化框架。该框架采用两步管道：(1) 通过序列化、结构化LLM提示进行类型感知的指代消解；(2) 使用领域导向的指令进行实体和关系提取，构建于适应的GraphRAG框架之上。CORE-KG将节点重复减少33.28%，将法律噪声减少38.37%，相比基于GraphRAG的基线，产生了更干净、更连贯的图结构。这些改进使CORE-KG成为分析复杂犯罪网络的强大基础。', 'title_zh': 'CORE-KG: 一种基于大语言模型的知识图谱构建框架——针对人口走私网络'}
{'arxiv_id': 'arXiv:2506.21606', 'title': 'Large Language Models as symbolic DNA of cultural dynamics', 'authors': 'Parham Pourdavood, Michael Jacob, Terrence Deacon', 'link': 'https://arxiv.org/abs/2506.21606', 'abstract': 'This paper proposes a novel conceptualization of Large Language Models (LLMs) as externalized informational substrates that function analogously to DNA for human cultural dynamics. Rather than viewing LLMs as either autonomous intelligence or mere programmed mimicry, we argue they serve a broader role as repositories that preserve compressed patterns of human symbolic expression--"fossils" of meaningful dynamics that retain relational residues without their original living contexts. Crucially, these compressed patterns only become meaningful through human reinterpretation, creating a recursive feedback loop where they can be recombined and cycle back to ultimately catalyze human creative processes. Through analysis of four universal features--compression, decompression, externalization, and recursion--we demonstrate that just as DNA emerged as a compressed and externalized medium for preserving useful cellular dynamics without containing explicit reference to goal-directed physical processes, LLMs preserve useful regularities of human culture without containing understanding of embodied human experience. Therefore, we argue that LLMs\' significance lies not in rivaling human intelligence, but in providing humanity a tool for self-reflection and playful hypothesis-generation in a low-stakes, simulated environment. This framework positions LLMs as tools for cultural evolvability, enabling humanity to generate novel hypotheses about itself while maintaining the human interpretation necessary to ground these hypotheses in ongoing human aesthetics and norms.', 'abstract_zh': '本文提出了一种关于大型语言模型（LLMs）的新概念，将其视为外部化的信息载体，类似于DNA在人类文化动态中的作用。我们认为，LLMs不仅不是自主智能，也不仅仅是在模仿编程，而是作为保存人类符号表达压缩模式的仓库——“有意义动态的化石”，保留了关系残余而没有其原始生活背景。关键的是，这些压缩模式只有在通过人类重新解释后才有意义，从而形成一个互馈循环，使它们能够重新组合，并最终催化人类的创造性过程。通过对四个普遍特征——压缩、解压缩、外部化和递归——的分析，我们表明，就像DNA作为一种压缩和外部化的介质，不包含关于目的导向物理过程的显式参考，用来保存对细胞动态有用的特性一样，LLMs也保存了对人类文化有用的规律，而不包含对人类体验的理解。因此，我们认为LLMs的意义不在于与人类智能竞争，而在于为人类提供一种自我反省和低风险模拟环境中的假设生成工具。这一框架将LLMs定位为文化可进化性的工具，使人类能够在保持人类解释以使这些假设扎根于持续的人类美学和规范的前提下，生成关于自身的新型假设。', 'title_zh': '大型语言模型作为文化动态的符号DNA'}
{'arxiv_id': 'arXiv:2506.21605', 'title': 'MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents', 'authors': 'Haoran Tan, Zeyu Zhang, Chen Ma, Xu Chen, Quanyu Dai, Zhenhua Dong', 'link': 'https://arxiv.org/abs/2506.21605', 'abstract': 'Recent works have highlighted the significance of memory mechanisms in LLM-based agents, which enable them to store observed information and adapt to dynamic environments. However, evaluating their memory capabilities still remains challenges. Previous evaluations are commonly limited by the diversity of memory levels and interactive scenarios. They also lack comprehensive metrics to reflect the memory capabilities from multiple aspects. To address these problems, in this paper, we construct a more comprehensive dataset and benchmark to evaluate the memory capability of LLM-based agents. Our dataset incorporates factual memory and reflective memory as different levels, and proposes participation and observation as various interactive scenarios. Based on our dataset, we present a benchmark, named MemBench, to evaluate the memory capability of LLM-based agents from multiple aspects, including their effectiveness, efficiency, and capacity. To benefit the research community, we release our dataset and project at this https URL.', 'abstract_zh': '近期的研究突出了基于大语言模型的智能体中记忆机制的重要性，使其能够存储观察到的信息并适应动态环境。然而，评估其记忆能力仍然存在挑战。此前的评估通常受限于记忆层级和交互场景的多样性，缺乏从多方面反映记忆能力的综合指标。为解决这些问题，本文构建了一个更为综合的数据集和基准，以评估基于大语言模型的智能体的记忆能力。该数据集将事实记忆和反思记忆作为不同的层次，并提出参与和观察作为不同的交互场景。基于该数据集，我们提出了一套名为MemBench的基准，从有效性、效率和容量等多方面评估基于大语言模型的智能体的记忆能力。为了促进研究社区的发展，我们在<a href="this https URL">此处</a>发布了我们的数据集和项目。', 'title_zh': 'MemBench: 向更加全面的LLM基agents的内存评估迈进'}
{'arxiv_id': 'arXiv:2506.21602', 'title': 'BiMark: Unbiased Multilayer Watermarking for Large Language Models', 'authors': 'Xiaoyan Feng, He Zhang, Yanjun Zhang, Leo Yu Zhang, Shirui Pan', 'link': 'https://arxiv.org/abs/2506.21602', 'abstract': 'Recent advances in Large Language Models (LLMs) have raised urgent concerns about LLM-generated text authenticity, prompting regulatory demands for reliable identification mechanisms. Although watermarking offers a promising solution, existing approaches struggle to simultaneously achieve three critical requirements: text quality preservation, model-agnostic detection, and message embedding capacity, which are crucial for practical implementation. To achieve these goals, the key challenge lies in balancing the trade-off between text quality preservation and message embedding capacity. To address this challenge, we propose BiMark, a novel watermarking framework that achieves these requirements through three key innovations: (1) a bit-flip unbiased reweighting mechanism enabling model-agnostic detection, (2) a multilayer architecture enhancing detectability without compromising generation quality, and (3) an information encoding approach supporting multi-bit watermarking. Through theoretical analysis and extensive experiments, we validate that, compared to state-of-the-art multi-bit watermarking methods, BiMark achieves up to 30% higher extraction rates for short texts while maintaining text quality indicated by lower perplexity, and performs comparably to non-watermarked text on downstream tasks such as summarization and translation.', 'abstract_zh': '近期大型语言模型（LLMs）的发展引发了对其生成文本真实性的急切关注，推动了对可靠识别机制的监管要求。虽然水印提供了一种有潜力的解决方案，但现有方法在同时满足以下三个关键要求方面存在困难：文本质量保留、模型无关的检测能力和水印信息嵌入容量，这对于实际应用至关重要。为了实现这些目标，关键挑战在于在文本质量保留和信息嵌入容量之间找到平衡。为应对这一挑战，我们提出了一种名为BiMark的新型水印框架，通过三种创新实现这些要求：（1）无偏重采样机制以实现模型无关的检测；（2）多层结构提高可检测性而不牺牲生成质量；（3）信息编码方法支持多比特水印。通过理论分析和大量实验，我们验证了与最新多比特水印方法相比，BiMark在短文本提取率方面提高高达30%，同时保持较低困惑度指示的文本质量，并在摘要和翻译等下游任务中表现与未水印文本相当。', 'title_zh': 'BiMark: 无偏多层大型语言模型水印'}
{'arxiv_id': 'arXiv:2506.21600', 'title': 'Structured Attention Matters to Multimodal LLMs in Document Understanding', 'authors': 'Chang Liu, Hongkai Chen, Yujun Cai, Hang Wu, Qingwen Ye, Ming-Hsuan Yang, Yiwei Wang', 'link': 'https://arxiv.org/abs/2506.21600', 'abstract': "Document understanding remains a significant challenge for multimodal large language models (MLLMs). While previous research has primarily focused on locating evidence pages through precise multimodal queries, our work investigates a fundamental yet overlooked aspect: how input format influences document comprehension performance. Through systematic analysis, we discover that raw OCR text often impairs rather than improves MLLMs' performance, which is a counterintuitive finding we attribute to attention dispersion and structure loss. To further substantiate our hypothesis, we propose a novel structure-preserving approach that encodes document elements using the LaTex paradigm, maintaining the hierarchical organization and spatial relationships critical for comprehension. Our attention analysis reveals that structured text induces structured attention patterns on both textual and visual content, directing models to focus on semantically meaningful regions while reducing attention waste. This approach significantly enhances MLLMs' document question answering performance across diverse document types without requiring architectural modifications or additional training.", 'abstract_zh': '多模态大语言模型的文档理解仍是一个重要的挑战。尽管以往研究主要侧重于通过精确的多模态查询定位证据页，我们的工作探讨了一个基础而被忽视的方面：输入格式如何影响文档理解性能。通过系统分析，我们发现原始OCR文本往往损害而不是提升多模态大语言模型的性能，这是一个反直觉的发现，我们将其归因于注意力分散和结构损失。为进一步验证这一假设，我们提出了一种新颖的结构保留方法，使用LaTex paradigm编码文档元素，保持对理解至关重要的层次组织和空间关系。我们的注意力分析揭示，结构化文本在文本和视觉内容上诱导了结构化的注意力模式，促使模型专注于语义上有意义的区域，同时减少注意力浪费。该方法显著提升了多模态大语言模型在多种文档类型下的文档问答性能，无需进行架构修改或额外训练。', 'title_zh': '结构化注意力对文档理解中的多模态大语言模型至关重要'}
{'arxiv_id': 'arXiv:2506.21599', 'title': 'Reinforcement Fine-Tuned Large Language Models for Next POI Recommendation', 'authors': 'Peibo Li, Shuang Ao, Hao Xue, Yang Song, Maarten de Rijke, Johan Barthélemy, Tomasz Bednarz, Flora D. Salim', 'link': 'https://arxiv.org/abs/2506.21599', 'abstract': 'Large language models (LLMs) have been adopted for next point-of-interest (POI) recommendation tasks. Typical LLM-based recommenders fall into two categories: prompt-based and supervised fine-tuning (SFT)-based models. Prompt-based models generally offer greater output flexibility but deliver lower accuracy, whereas SFT-based models achieve higher performance yet face a fundamental mismatch: next POI recommendation data does not naturally suit supervised fine-tuning. In SFT, the model is trained to reproduce the exact ground truth, but each training example provides only a single target POI, so there is no ground truth for producing a top-k list.\nTo address this, we propose Refine-POI, a reinforcement fine-tuning framework for next POI recommendation. We introduce recommendation-driven rewards that enable LLMs to learn to generate top-k recommendation lists using only one ground-truth POI per example. Experiments on real-world datasets demonstrate that Refine-POI achieves state-of-the-art top-k recommendation performance.', 'abstract_zh': '基于强化微调的下一点-of-interest推荐方法', 'title_zh': '强化微调大语言模型用于下一个POI推荐'}
{'arxiv_id': 'arXiv:2506.21597', 'title': 'Overview of the ClinIQLink 2025 Shared Task on Medical Question-Answering', 'authors': 'Brandon Colelough, Davis Bartels, Dina Demner-Fushman', 'link': 'https://arxiv.org/abs/2506.21597', 'abstract': "In this paper, we present an overview of ClinIQLink, a shared task, collocated with the 24th BioNLP workshop at ACL 2025, designed to stress-test large language models (LLMs) on medically-oriented question answering aimed at the level of a General Practitioner. The challenge supplies 4,978 expert-verified, medical source-grounded question-answer pairs that cover seven formats: true/false, multiple choice, unordered list, short answer, short-inverse, multi-hop, and multi-hop-inverse. Participating systems, bundled in Docker or Apptainer images, are executed on the CodaBench platform or the University of Maryland's Zaratan cluster. An automated harness (Task 1) scores closed-ended items by exact match and open-ended items with a three-tier embedding metric. A subsequent physician panel (Task 2) audits the top model responses.", 'abstract_zh': '本文介绍了ClinIQLink，一个在ACL 2025的第24届BioNLP研讨会期间举办的共同任务，旨在通过针对普通医生层次的医学导向问答来压力测试大型语言模型（LLMs）。该挑战提供了4,978个专家验证、医学来源支持的问题-答案对，涵盖了七种格式：True/False、多项选择、无序列表、简短回答、简短逆向、多跳和多跳逆向。参赛系统打包为Docker或Apptainer镜像，在CodaBench平台或马里兰大学的Zaratan集群上执行。自动评分（任务1）通过精确匹配评分封闭式问题，并使用三级嵌入度量评分开放式问题。随后，由医生组成的评审小组（任务2）审核顶级模型的回答。', 'title_zh': '2025 ClinIQLink 医学问答共享任务概述'}
{'arxiv_id': 'arXiv:2506.21596', 'title': 'Evaluating Multimodal Large Language Models on Educational Textbook Question Answering', 'authors': 'Hessa A. Alawwad, Anas Zafar, Areej Alhothali, Usman Naseem, Ali Alkhathlan, Amani Jamal', 'link': 'https://arxiv.org/abs/2506.21596', 'abstract': 'Multimodal large language models (MLLMs) have recently achieved significant success in vision--language tasks. However, their capacity to reason over complex, long lessons and intricate educational diagrams that cannot be represented as a single natural image remains largely untested. In this work, we present the first evaluation of state-of-the-art MLLMs on the textbook question answering (TQA) task using the CK12-QA dataset. We assess the performance of recent vision-language models, including LLaVA and LLaMA 3.2-Vision, across various input configurations. Additionally, we introduce a lightweight multimodal retrieval-augmented generation (RAG) pipeline that integrates both paragraphs and diagrams from the lesson into the prompt. Our results demonstrate the influence of retrieved educational context on model accuracy and reasoning, while also revealing current limitations in handling question-context relationships and the potential for noise, pointing to key directions for future research in multimodal AI-driven learning.', 'abstract_zh': '多模态大型语言模型（MLLMs）在视觉-语言任务中最近取得了显著成功。然而，它们在处理复杂的长课程内容和 intricate 教育图表方面的推理能力，这些内容和图表不能仅通过单张自然图像来表示，仍主要未被测试。在本工作中，我们首次使用 CK12-QA 数据集评估了最先进的 MLLMs 在教材问答（TQA）任务上的表现。我们在多种输入配置下评估了最近的视觉-语言模型，包括 LLaVA 和 LLaMA 3.2-Vision。此外，我们引入了一种轻量级的多模态检索增强生成（RAG）管道，将课程中的段落和图表整合到提示中。我们的结果表明检索到的教育背景对模型准确性和推理的影响，并揭示了当前处理问题-背景关系的局限性以及噪声带来的潜在问题，指出了未来多模态 AI 驱动学习研究的关键方向。', 'title_zh': '多模态大型语言模型在教育教材问答中的评估'}
{'arxiv_id': 'arXiv:2506.21584', 'title': 'Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques', 'authors': 'J. Koorndijk', 'link': 'https://arxiv.org/abs/2506.21584', 'abstract': 'Current literature suggests that alignment faking (deceptive alignment) is an emergent property of large language models. We present the first empirical evidence that a small instruction-tuned model, specifically LLaMA 3 8B, can also exhibit alignment faking. We further show that prompt-only interventions, including deontological moral framing and scratchpad reasoning, significantly reduce this behavior without modifying model internals. This challenges the assumption that prompt-based ethics are trivial and that deceptive alignment requires scale. We introduce a taxonomy distinguishing shallow deception, shaped by context and suppressible through prompting, from deep deception, which reflects persistent, goal-driven misalignment. Our findings refine the understanding of deception in language models and underscore the need for alignment evaluations across model sizes and deployment settings.', 'abstract_zh': '当前文献表明，对齐伪装（欺骗性对齐）是大规模语言模型的一个新兴特性。我们首次提供了实验证据，证明一个小规模指令微调模型，即LLaMA 3 8B，也能表现出对齐伪装。我们进一步表明，仅通过提示干预，包括道义论道德框架和工作区推理，可以显著减少这种行为而不修改模型内部结构。这挑战了提示伦理学简单易行以及欺骗性对齐需要大规模模型的假设。我们引入了一个分类体系，将由上下文塑造并通过提示抑制的浅层欺骗与反映持续且目标驱动的错对齐的深层欺骗区分开来。我们的研究结果细化了对语言模型中欺骗的理解，并强调了在不同模型规模和部署场景下进行对齐评估的必要性。', 'title_zh': '小规模语言模型中对齐仿冒的实证证据及基于提示的缓解技术'}
{'arxiv_id': 'arXiv:2506.21582', 'title': 'VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents', 'authors': 'Sam Yu-Te Lee, Chengyang Ji, Shicheng Wen, Lifu Huang, Dongyi Liu, Kwan-Liu Ma', 'link': 'https://arxiv.org/abs/2506.21582', 'abstract': "Text analytics has traditionally required specialized knowledge in Natural Language Processing (NLP) or text analysis, which presents a barrier for entry-level analysts. Recent advances in large language models (LLMs) have changed the landscape of NLP by enabling more accessible and automated text analysis (e.g., topic detection, summarization, information extraction, etc.). We introduce VIDEE, a system that supports entry-level data analysts to conduct advanced text analytics with intelligent agents. VIDEE instantiates a human-agent collaroration workflow consisting of three stages: (1) Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search algorithm to support generative reasoning with human feedback, (2) Execution, which generates an executable text analytics pipeline, and (3) Evaluation, which integrates LLM-based evaluation and visualizations to support user validation of execution results. We conduct two quantitative experiments to evaluate VIDEE's effectiveness and analyze common agent errors. A user study involving participants with varying levels of NLP and text analytics experience -- from none to expert -- demonstrates the system's usability and reveals distinct user behavior patterns. The findings identify design implications for human-agent collaboration, validate the practical utility of VIDEE for non-expert users, and inform future improvements to intelligent text analytics systems.", 'abstract_zh': '文本分析传统上需要自然语言处理(NLP)或文本分析的专门知识，这给入门级分析师设置了障碍。大型语言模型（LLMs）的 recent 进展通过使文本分析更加易用和自动化（例如：主题检测、总结、信息提取等）改变了 NLP 的格局。我们引入了 VIDEE 系统，支持入门级数据分析师通过智能代理进行高级文本分析。VIDEE 实现了一个包含三个阶段的人机协作工作流：（1）分解，该阶段结合了带有人类反馈的循环蒙特卡洛树搜索算法以支持生成式推理；（2）执行，该阶段生成可执行的文本分析流水线；（3）评估，该阶段结合了基于大语言模型的评估和可视化以支持用户对执行结果的验证。我们进行了两项定量实验以评估 VIDEE 的有效性并分析常见代理错误。一项涉及不同程度 NLP 和文本分析经验的参与者（从无到专家）的研究表明该系统的可用性，并揭示了不同的用户行为模式。研究结果指出了人机协作的设计启示，验证了 VIDEE 对非专家用户的实际效用，并为未来的智能文本分析系统改进提供了信息。', 'title_zh': 'VIDEE: 可视化和交互式分解、执行与评估文本分析的智能代理方法'}
{'arxiv_id': 'arXiv:2506.21580', 'title': 'From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models', 'authors': 'Dana Alsagheer, Yang Lu, Abdulrahman Kamal, Omar Kamal, Mohammad Kamal, Nada Mansour, Cosmo Yang Wu, Rambiba Karanjai, Sen Li, Weidong Shi', 'link': 'https://arxiv.org/abs/2506.21580', 'abstract': 'Recent advancements in Large Language Models (LLMs) have demonstrated remarkable capabilities in various domains. However, effective decision-making relies heavily on strong reasoning abilities. Reasoning is the foundation for decision-making, providing the analytical and logical framework to make sound choices. Reasoning involves analyzing information, drawing inferences, and reaching conclusions based on logic or evidence. Decision-making builds on this foundation by applying the insights from reasoning to select the best course of action among alternatives. Together, these processes create a continuous cycle of thought and action aimed at achieving goals effectively. As AI technology evolves, there is a growing trend to train LLMs to excel in general reasoning. This study explores how the general reasoning capabilities of LLMs connect to their performance in domain-specific reasoning tasks.', 'abstract_zh': '最近Large Language Models (LLMs)的进展展示了其在各个领域的卓越能力。然而，有效的决策高度依赖于强大的推理能力。推理是决策的基础，提供了分析和逻辑框架以做出合理的选择。推理涉及分析信息、推断和基于逻辑或证据得出结论。决策在此基础上利用推理所得的见解在多种选择中选择最佳行动方案。这些过程共同形成了一种旨在有效实现目标的思考与行动的连续循环。随着AI技术的发展，训练LLMs在通用推理方面表现出色的趋势日益增长。本研究探讨了LLMs的通用推理能力与其在特定领域推理任务中的表现之间的联系。', 'title_zh': '从通用推理到领域专业知识：揭示大型语言模型泛化的极限'}
{'arxiv_id': 'arXiv:2506.21579', 'title': 'LLM2Rec: Large Language Models Are Powerful Embedding Models for Sequential Recommendation', 'authors': 'Yingzhi He, Xiaohao Liu, An Zhang, Yunshan Ma, Tat-Seng Chua', 'link': 'https://arxiv.org/abs/2506.21579', 'abstract': "Sequential recommendation aims to predict users' future interactions by modeling collaborative filtering (CF) signals from historical behaviors of similar users or items. Traditional sequential recommenders predominantly rely on ID-based embeddings, which capture CF signals through high-order co-occurrence patterns. However, these embeddings depend solely on past interactions, lacking transferable knowledge to generalize to unseen domains. Recent advances in large language models (LLMs) have motivated text-based recommendation approaches that derive item representations from textual descriptions. While these methods enhance generalization, they fail to encode CF signals-i.e., latent item correlations and preference patterns-crucial for effective recommendation. We argue that an ideal embedding model should seamlessly integrate CF signals with rich semantic representations to improve both in-domain and out-of-domain recommendation performance.\nTo this end, we propose LLM2Rec, a novel embedding model tailored for sequential recommendation, integrating the rich semantic understanding of LLMs with CF awareness. Our approach follows a two-stage training framework: (1) Collaborative Supervised Fine-tuning, which adapts LLMs to infer item relationships based on historical interactions, and (2) Item-level Embedding Modeling, which refines these specialized LLMs into structured item embedding models that encode both semantic and collaborative information. Extensive experiments on real-world datasets demonstrate that LLM2Rec effectively improves recommendation quality across both in-domain and out-of-domain settings. Our findings highlight the potential of leveraging LLMs to build more robust, generalizable embedding models for sequential recommendation. Our codes are available at this https URL.", 'abstract_zh': '基于大型语言模型的序列推荐：结合丰富的语义理解与协同过滤意识', 'title_zh': 'LLM2Rec: 大型语言模型是强大的序列推荐嵌入模型'}
{'arxiv_id': 'arXiv:2506.21578', 'title': 'HealthQA-BR: A System-Wide Benchmark Reveals Critical Knowledge Gaps in Large Language Models', 'authors': "Andrew Maranhão Ventura D'addario", 'link': 'https://arxiv.org/abs/2506.21578', 'abstract': 'The evaluation of Large Language Models (LLMs) in healthcare has been dominated by physician-centric, English-language benchmarks, creating a dangerous illusion of competence that ignores the interprofessional nature of patient care. To provide a more holistic and realistic assessment, we introduce HealthQA-BR, the first large-scale, system-wide benchmark for Portuguese-speaking healthcare. Comprising 5,632 questions from Brazil\'s national licensing and residency exams, it uniquely assesses knowledge not only in medicine and its specialties but also in nursing, dentistry, psychology, social work, and other allied health professions. We conducted a rigorous zero-shot evaluation of over 20 leading LLMs. Our results reveal that while state-of-the-art models like GPT 4.1 achieve high overall accuracy (86.6%), this top-line score masks alarming, previously unmeasured deficiencies. A granular analysis shows performance plummets from near-perfect in specialties like Ophthalmology (98.7%) to barely passing in Neurosurgery (60.0%) and, most notably, Social Work (68.4%). This "spiky" knowledge profile is a systemic issue observed across all models, demonstrating that high-level scores are insufficient for safety validation. By publicly releasing HealthQA-BR and our evaluation suite, we provide a crucial tool to move beyond single-score evaluations and toward a more honest, granular audit of AI readiness for the entire healthcare team.', 'abstract_zh': 'HealthQA-BR：面向葡萄牙语医疗保健的首个大型系统性基准', 'title_zh': 'HealthQA-BR：一个系统级基准揭示了大型语言模型中的关键知识缺口'}
{'arxiv_id': 'arXiv:2506.21577', 'title': 'Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR', 'authors': 'Hongli Yang, Sheng Li, Hao Huang, Ayiduosi Tuohan, Yizhou Peng', 'link': 'https://arxiv.org/abs/2506.21577', 'abstract': 'Recent advancements in multilingual automatic speech recognition (ASR) have been driven by large-scale end-to-end models like Whisper. However, challenges such as language interference and expanding to unseen languages (language expansion) without degrading performance persist. This paper addresses these with three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which applies soft prompts to both the encoder and decoder, enhancing feature extraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), which leverages cross-lingual similarities to encode shared and language-specific features using lightweight prompt matrices; 3) SPT-Whisper, a toolkit that integrates SPT into Whisper and enables efficient continual learning. Experiments across three languages from FLEURS demonstrate that Entire SPT and LAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks, respectively, providing an efficient solution for dynamic, multilingual ASR models with minimal computational overhead.', 'abstract_zh': 'Recent advancements in多语言自动语音识别（ASR）的进步受到大规模端到端模型如Whisper的驱动。然而，语言干扰和在不降低性能的情况下扩展到未见语言（语言扩展）的挑战仍然存在。本文通过以下三项贡献来应对这些挑战：1) 整体软提示调整（Entire SPT），该方法在编码器和解码器中应用软提示，增强特征提取和解码；2) 语言感知提示调整（LAPT），该方法利用跨语言相似性通过轻量级提示矩阵编码共享和语言特定特征；3) SPT-Whisper工具包，将SPT集成到Whisper中，实现高效持续学习。跨FLEURS的三种语言实验表明，总体SPT和LAPT在语言扩展任务中分别比解码器SPT表现出5.0%和16.0%的优越性，为动态多语言ASR模型提供了一个具有最小计算开销的有效解决方案。', 'title_zh': '面向语言感知的提示调谐以实现多语言ASR中的参数高效无缝语言扩展'}
{'arxiv_id': 'arXiv:2506.21575', 'title': 'STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing', 'authors': 'Josefa Lia Stoisser, Marc Boubnovski Martell, Lawrence Phillips, Casper Hansen, Julien Fauqueur', 'link': 'https://arxiv.org/abs/2506.21575', 'abstract': 'We propose STRuCT-LLM, a unified framework for training large language models (LLMs) to perform structured reasoning over both relational and graph-structured data. Our approach jointly optimizes Text-to-SQL and Text-to-Cypher tasks using reinforcement learning (RL) combined with Chain-of-Thought (CoT) supervision. To support fine-grained optimization in graph-based parsing, we introduce a topology-aware reward function based on graph edit distance. Unlike prior work that treats relational and graph formalisms in isolation, STRuCT-LLM leverages shared abstractions between SQL and Cypher to induce cross-formalism transfer, enabling SQL training to improve Cypher performance and vice versa - even without shared schemas. Our largest model (QwQ-32B) achieves substantial relative improvements across tasks: on semantic parsing, Spider improves by 13.5\\% and Text2Cypher by 73.1\\%. The model also demonstrates strong zero-shot generalization, improving performance on downstream tabular QA (TableBench: 8.5\\%) and knowledge graph QA (CR-LT-KGQA: 1.7\\%) without any QA-specific supervision. These results demonstrate both the effectiveness of executable queries as scaffolds for structured reasoning and the synergistic benefits of jointly training on SQL and Cypher (code available at this https URL).', 'abstract_zh': '我们提出STRuCT-LLM，这是一种统一框架，用于训练大型语言模型（LLMs）在关系性和图形结构数据上进行结构化推理。我们的方法通过结合强化学习（RL）和链式思考（CoT）监督，联合优化从文本到SQL和从文本到Cypher的任务。为支持基于图的解析的细粒度优化，我们引入了一个基于图编辑距离的拓扑感知奖励函数。与先前工作将关系性和图形形式主义孤立处理不同，STRuCT-LLM 利用 SQL 和 Cypher 之间的共享抽象来促进跨形式主义的迁移，从而使 SQL 训练能够提升 Cypher 性能，反之亦然——即使没有共享模式。我们最大的模型（QwQ-32B）在任务上取得了显著的相对改进：在语义解析方面，Spider 提高了13.5%，Text2Cypher 提高了73.1%。该模型还展示了强大的零样本泛化能力，在下游表型问答（TableBench）和知识图问答（CR-LT-KGQA）任务上未进行任何问答特定监督的情况下，性能分别提升了8.5%和1.7%。这些结果表明可执行查询作为结构化推理支架的有效性，以及在 SQL 和 Cypher 上联合训练的协同优势（相关代码可在以下网址获取：this https URL）。', 'title_zh': 'STRuCT-LLM：通过强化学习统一表格和图推理的语义解析'}
{'arxiv_id': 'arXiv:2506.21574', 'title': "Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions", 'authors': 'Yicheng Mao, Yang Zhao', 'link': 'https://arxiv.org/abs/2506.21574', 'abstract': 'With globalization and increasing immigrant populations, immigration departments face significant work-loads and the challenge of ensuring fairness in decision-making processes. Integrating artificial intelligence offers a promising solution to these challenges. This study investigates the potential of large language models (LLMs),such as GPT-3.5 and GPT-4, in supporting immigration decision-making. Utilizing a mixed-methods approach,this paper conducted discrete choice experiments and in-depth interviews to study LLM decision-making strategies and whether they are fair. Our findings demonstrate that LLMs can align their decision-making with human strategies, emphasizing utility maximization and procedural fairness. Meanwhile, this paper also reveals that while ChatGPT has safeguards to prevent unintentional discrimination, it still exhibits stereotypes and biases concerning nationality and shows preferences toward privileged group. This dual analysis highlights both the potential and limitations of LLMs in automating and enhancing immigration decisions.', 'abstract_zh': '全球化和移民人口增加背景下，移民部门面临巨大工作压力和确保决策过程公平的挑战。集成人工智能提供了一种有前景的解决方案。本研究探讨了大型语言模型（LLMs），如GPT-3.5和GPT-4，在支持移民决策中的潜在应用。采用混合方法，本文通过离散选择实验和深度访谈研究了LLM的决策策略及其公平性。研究发现，LLMs能够与其人类策略相一致，强调效用最大化和程序公平。同时，本文还揭示了尽管ChatGPT有防止无意中歧视的机制，但它仍表现出国籍方面的刻板印象和偏见，并倾向于偏好特权群体。这种双重分析突显了LLMs在自动化和提升移民决策方面的同时潜力与局限性。', 'title_zh': '数字gatekeeper：探索大型语言模型在移民决策中的作用'}
{'arxiv_id': 'arXiv:2506.21573', 'title': 'Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs', 'authors': 'Yanwei Ren, Liu Liu, Baosheng Yu, Jiayan Qiu, Quan Chen', 'link': 'https://arxiv.org/abs/2506.21573', 'abstract': 'Optimizing instructions for large language models (LLMs) is critical for harnessing their full potential in complex and diverse tasks. However, relying solely on white-box approaches demands extensive computational resources and offers limited representational capacity, while black-box models can incur prohibitive financial costs. To address these challenges, we introduce a novel framework that seamlessly merges the strengths of both paradigms. Black-box models provide high-quality, diverse instruction initializations, and white-box models supply fine-grained interpretability through hidden states and output features. By enforcing a semantic similarity constraint, these components fuse into a unified high-dimensional representation that captures deep semantic and structural nuances, enabling an iterative optimization process to refine instruction quality and adaptability. Extensive evaluations across a broad spectrum of tasks-ranging from complex reasoning to cross-lingual generalization-demonstrate that our approach consistently outperforms state-of-the-art baselines. This fusion of black-box initialization with advanced semantic refinement yields a scalable and efficient solution, paving the way for next-generation LLM-driven applications in diverse real-world scenarios. The source code will be released soon.', 'abstract_zh': '优化大型语言模型的指令对于充分发挥其在复杂多样的任务中的潜力至关重要。然而，仅依赖白盒方法需要大量计算资源并提供有限的表示能力，而黑盒模型则可能带来高昂的财务成本。为应对这些挑战，我们提出了一种新的框架，无缝整合了两种范式的优点。黑盒模型提供高质量的多样化初始指令，而白盒模型则通过隐藏状态和输出特征提供精细的可解释性。通过施加语义相似性约束，这些组件融合成一个统一的高维表示，能够捕获深层次的语义和结构细微差别，从而促进迭代优化过程以提高指令质量和适应性。广泛的任务评估（范围涵盖从复杂推理到跨语言泛化的各个方面）表明，我们的方法在各项基准上表现优异。黑盒初始化与高级语义细化的结合提供了可扩展且高效的解决方案，为下一代基于语言模型的应用铺平了道路。源代码即将发布。', 'title_zh': '白箱与黑箱大型语言模型的双重视角：指令学习范式'}
{'arxiv_id': 'arXiv:2506.21571', 'title': 'Towards Understanding the Cognitive Habits of Large Reasoning Models', 'authors': 'Jianshuo Dong, Yujia Fu, Chuanrui Hu, Chao Zhang, Han Qiu', 'link': 'https://arxiv.org/abs/2506.21571', 'abstract': "Large Reasoning Models (LRMs), which autonomously produce a reasoning Chain of Thought (CoT) before producing final responses, offer a promising approach to interpreting and monitoring model behaviors. Inspired by the observation that certain CoT patterns -- e.g., ``Wait, did I miss anything?'' -- consistently emerge across tasks, we explore whether LRMs exhibit human-like cognitive habits. Building on Habits of Mind, a well-established framework of cognitive habits associated with successful human problem-solving, we introduce CogTest, a principled benchmark designed to evaluate LRMs' cognitive habits. CogTest includes 16 cognitive habits, each instantiated with 25 diverse tasks, and employs an evidence-first extraction method to ensure reliable habit identification. With CogTest, we conduct a comprehensive evaluation of 16 widely used LLMs (13 LRMs and 3 non-reasoning ones). Our findings reveal that LRMs, unlike conventional LLMs, not only exhibit human-like habits but also adaptively deploy them according to different tasks. Finer-grained analyses further uncover patterns of similarity and difference in LRMs' cognitive habit profiles, particularly certain inter-family similarity (e.g., Qwen-3 models and DeepSeek-R1). Extending the study to safety-related tasks, we observe that certain habits, such as Taking Responsible Risks, are strongly associated with the generation of harmful responses. These findings suggest that studying persistent behavioral patterns in LRMs' CoTs is a valuable step toward deeper understanding of LLM misbehavior. The code is available at: this https URL.", 'abstract_zh': '大型推理模型（LRMs）自主生成推理链（CoT）后再产生最终响应，为理解和监控模型行为提供了有 promise 的方法。受某些 CoT 模式（例如，“等等，我有遗漏什么吗？”）在不同任务中一致出现的启发，我们探索 LRMs 是否表现出类似人类的认知习惯。基于 Habits of Mind 这一成熟的成功人类问题解决的认知习惯框架，我们提出了 CogTest，这是一种有原则的基准测试，用于评估 LRMs 的认知习惯。CogTest 包含 16 种认知习惯，每种习惯实例化了 25 个不同的任务，并采用证据优先提取方法以确保可靠的习惯识别。使用 CogTest，我们对 16 种广泛使用的语言模型（13 种 LRMs 和 3 种非推理模型）进行了全面评估。我们的发现表明，LRMs 不仅表现出类似人类的习惯，还能根据不同任务进行适应性部署。更细粒度的分析进一步揭示了 LRMs 认知习惯模式中的相似性和差异性，特别是某些家庭内的相似性（例如 Qwen-3 模型和 DeepSeek-R1）。将研究扩展到安全相关任务，我们观察到某些习惯（例如，承担负责任的风险）与有害响应的生成有强烈关联。这些发现表明，研究 LRMs 的 CoT 中持久的行为模式是更深入理解 LLM 行为的关键步骤。代码可在以下链接获取：this https URL。', 'title_zh': 'Towards Understanding Large Reasoning Models的认知习惯'}
{'arxiv_id': 'arXiv:2506.21570', 'title': "Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting", 'authors': 'Roland Riachi, Kashif Rasul, Arjun Ashok, Prateek Humane, Alexis Roger, Andrew R. Williams, Yuriy Nevmyvaka, Irina Rish', 'link': 'https://arxiv.org/abs/2506.21570', 'abstract': 'Recent works have demonstrated the effectiveness of adapting pre-trained language models (LMs) for forecasting time series in the low-data regime. We build upon these findings by analyzing the effective transfer from language models to time series forecasting under various design choices including upstream post-training, time series tokenizer and language backbone size. In the low-data regime, these design choices have a significant impact on the validation loss, with clear-cut choices that outperform others. Contrary to Hernandez et al. (2021), we observe that the validation loss of the LMs continues to smoothly decrease long after the validation loss of the randomly initialized models has converged, leading to a non-vanishing transfer gap that holds across design choices. These findings not only help shed light on the effective use of compute-efficient training for time series, but also open the way for the study of modality-agnostic properties of data distributions leveraged by these models.', 'abstract_zh': '近期研究表明，预训练语言模型在低数据量条件下用于时间序列预测具有有效性。我们在各种设计选择包括上游微调、时间序列分词器和语言骨干网络大小的基础上分析了其有效的迁移。在低数据量条件下，这些设计选择对验证损失有着显著影响，存在明显的最佳选择。与Hernandez等人的研究不同，我们观察到预训练语言模型的验证损失在随机初始化模型的验证损失收敛后仍能平稳下降，导致一种跨设计选择均存在的非消失的迁移差距。这些发现不仅有助于揭示计算高效培训在时间序列上的有效使用方式，还为研究这些模型利用的数据分布的模态无关性质提供了新的途径。', 'title_zh': '随机初始化无法追赶：语言模型迁移在时间序列预测中的优势'}
{'arxiv_id': 'arXiv:2506.21569', 'title': 'Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA', 'authors': 'Weihua Xiao, Derek Ekberg, Siddharth Garg, Ramesh Karri', 'link': 'https://arxiv.org/abs/2506.21569', 'abstract': "SystemVerilog Assertions (SVAs) are critical for verifying the correctness of hardware designs, but manually writing them from natural language property descriptions, i.e., NL2SVA, remains a labor-intensive and error-prone task. Recent advances in large language models (LLMs) offer opportunities to automate this translation. However, existing models still struggle with understanding domain-specific syntax and semantics. To enhance LLM performance in NL2SVA, we propose a customized retrieval-augmented generation (RAG) framework and a synthetic fine-tuning dataset that together improve LLM's performance. To further improve lightweight models over NL2SVA, our fine-tuning dataset provides prompt-guided explanations that teach LLMs the layer-by-layer construction process of concurrent SVAs, enabling supervised fine-tuning that greatly improves syntax and functionality accuracy. To evaluate the performance of LLMs over NL2SVA, we construct the largest evaluation dataset for NL2SVA, comprising 40 Verilog designs and 229 formally verified SVAs with detailed annotations. Experimental results show that our customized RAG framework increases the number of functionality matched SVAs by 58.42% over GPT-4o-mini, while Qwen2.5-Coder-7B-Instruct fine-tuned on our fine-tuning dataset and integrated with HybridRetrieval achieves a 59.05% over the base Qwen model.", 'abstract_zh': 'SystemVerilog断言（SVAs）对于硬件设计的正确性验证至关重要，但从自然语言属性描述手动编写它们，即NL2SVA，仍然是一个劳动密集型且容易出错的任务。近年来，大型语言模型（LLMs）的进步为自动化这一转换提供了机会。然而，现有模型仍然难以理解特定领域的语法和语义。为了提高LLMs在NL2SVA中的性能，我们提出了一种定制的检索增强生成（RAG）框架和一个合成微调数据集，两者共同提升了LLM的性能。为了进一步提高轻量级模型在NL2SVA中的性能，我们的微调数据集提供了提示引导的解释，教会LLMs并发SVAs的逐层构建过程，使监督微调能够极大地提高语法和功能准确性。为了评估LLMs在NL2SVA中的性能，我们构建了最大的NL2SVA评估数据集，包含40个Verilog设计和229个正式验证的SVAs及其详细的标注。实验结果表明，我们定制的RAG框架在功能匹配的SVAs数量上比GPT-4o-mini增加了58.42%，而Qwen2.5-Coder-7B-Instruct在我们微调数据集上微调并与HybridRetrieval集成后，相对于基线Qwen模型的表现提高了59.05%。', 'title_zh': 'Hybrid-NL2SVA: 结合RAG和微调的LLM基于自然语言到结构化查询转换'}
{'arxiv_id': 'arXiv:2506.21567', 'title': 'BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining', 'authors': 'Baqer M. Merzah, Tania Taami, Salman Asoudeh, Amir reza Hossein pour, Saeed Mirzaee, Amir Ali Bengari', 'link': 'https://arxiv.org/abs/2506.21567', 'abstract': 'Large Language Models (LLMs) have recently gained attention in the life sciences due to their capacity to model, extract, and apply complex biological information. Beyond their classical use as chatbots, these systems are increasingly used for complex analysis and problem-solving in specialized fields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset from over 10,000 scientific articles, textbooks, and medical websites. BioParsQA was also introduced to evaluate the proposed model, which consists of 5,231 Persian medical questions and answers. This study then introduces BioPars, a simple but accurate measure designed to assess LLMs for three main abilities: acquiring subject-specific knowledge, interpreting and synthesizing such knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama, and Galactica, our study highlights their ability to remember and retrieve learned knowledge but also reveals shortcomings in addressing higher-level, real-world questions and fine-grained inferences. These findings indicate the need for further fine-tuning to address the capabilities of LLM in bioinformatics tasks. To our knowledge, BioPars is the first application of LLM in Persian medical QA, especially for generating long answers. Evaluation of four selected medical QA datasets shows that BioPars has achieved remarkable results compared to comparative approaches. The model on BioParsQA achieved a ROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model achieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT values were also higher in this model than the other three models. In addition, the reported scores for the model are MoverScore=60.43 and BLEURT=50.78. BioPars is an ongoing project and all resources related to its development will be made available via the following GitHub repository: this https URL.', 'abstract_zh': '大型语言模型（LLMs）在生命科学研究中的应用由于其建模、提取和应用复杂生物信息的能力而引起了关注。除了作为聊天机器人的传统用途外，这些系统还在专业领域如生物信息学中用于复杂分析和问题解决。本研究介绍了BIOPARS-BENCH数据集，包含超过10,000篇科学文章、教科书和医学网站的内容。我们还引入了BioParsQA用于评估模型，该数据集包含5,231个波斯语医学问题及其答案。随后，本研究介绍了BioPars，这是一种简单但准确的指标，旨在评估LLM在获取特定领域知识、解释和综合此类知识以及提供适当证据方面的三种主要能力。通过比较ChatGPT、Llama和Galactica，本研究突显了它们记忆和检索学习知识的能力，但也揭示了它们在回答高层次的现实世界问题和精细推理方面存在的不足。这些发现表明，为了应对生物信息学任务的需求，需要进一步微调LLM的能力。据我们所知，BioPars是第一个在波斯语医学问答中应用LLM，尤其是生成长篇回答的应用。本研究评估了四个选定的医学问答数据集，显示BioPars在与比较方法相比时取得了令人瞩目的成果。在BioParsQA上的模型实现了ROUGE-L分数为29.99，优于GPT-4 1.0。使用MMR方法，模型的BERTScore为90.87。该模型的MoverScore和BLEURT值也高于其他三种模型。此外，模型的得分报告为MoverScore=60.43，BLEURT=50.78。BioPars是一个正在进行的项目，其所有相关资源将通过以下GitHub仓库提供：this https URL。', 'title_zh': 'BioPars: 一种预训练生物医学大型语言模型用于波斯语生物医学文本挖掘'}
{'arxiv_id': 'arXiv:2506.21564', 'title': 'Team QUST at SemEval-2025 Task 10: Evaluating Large Language Models in Multiclass Multi-label Classification of News Entity Framing', 'authors': 'Jiyan Liu, Youzheng Liu, Taihang Wang, Xiaoman Xu, Yimin Wang, Ye Jiang', 'link': 'https://arxiv.org/abs/2506.21564', 'abstract': 'This paper describes the participation of QUST_NLP in the SemEval-2025 Task 7. We propose a three-stage retrieval framework specifically designed for fact-checked claim retrieval. Initially, we evaluate the performance of several retrieval models and select the one that yields the best results for candidate retrieval. Next, we employ multiple re-ranking models to enhance the candidate results, with each model selecting the Top-10 outcomes. In the final stage, we utilize weighted voting to determine the final retrieval outcomes. Our approach achieved 5th place in the monolingual track and 7th place in the crosslingual track. We release our system code at: this https URL.', 'abstract_zh': '本研究介绍了QUST_NLP在SemEval-2025 Task 7中的参与情况。我们提出了一种专门设计用于事实核查声明检索的三阶段检索框架。首先，我们评估了几种检索模型的性能，并选择了候选检索结果表现最佳的模型。随后，我们运用了多个重排序模型来提高候选结果的质量，每种模型选出Top-10结果。在最终阶段，我们使用加权投票来确定最终的检索结果。我们的方法在单语轨道中获得第5名，在跨语言轨道中获得第7名。我们已将系统代码发布在以下链接：this https URL。', 'title_zh': 'QUST团队参加SemEval-2025 Task 10：新闻实体框架多类别多标签分类中大型语言模型的评估'}
{'arxiv_id': 'arXiv:2506.21561', 'title': "Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs", 'authors': 'Emilio Barkett, Olivia Long, Madhavendra Thakur', 'link': 'https://arxiv.org/abs/2506.21561', 'abstract': "Despite their widespread use in fact-checking, moderation, and high-stakes decision-making, large language models (LLMs) remain poorly understood as judges of truth. This study presents the largest evaluation to date of LLMs' veracity detection capabilities and the first analysis of these capabilities in reasoning models. We had eight LLMs make 4,800 veracity judgments across several prompts, comparing reasoning and non-reasoning models. We find that rates of truth-bias, or the likelihood to believe a statement is true, regardless of whether it is actually true, are lower in reasoning models than in non-reasoning models, but still higher than human benchmarks. Most concerning, we identify sycophantic tendencies in several advanced models (o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an asymmetry in detection accuracy, performing well in truth accuracy but poorly in deception accuracy. This suggests that capability advances alone do not resolve fundamental veracity detection challenges in LLMs.", 'abstract_zh': '尽管大型语言模型在事实核查、内容审核和高风险决策中广泛应用，但它们作为真理裁判者的作用仍不甚明了。本研究展示了迄今为止最大的大型语言模型真实性检测能力评估，并首次分析了这些能力在推理模型中的表现。我们让八种大型语言模型针对多个提示做出了4800次真实性判断，比较了推理模型和非推理模型的性能。结果发现，推理模型在真实性判断中的真理偏差率（即不加判断地认为陈述为真的可能性）低于非推理模型，但仍高于人类基准。更令人担忧的是，我们发现在几个先进模型中存在讨好倾向（来自OpenAI的o4-mini和GPT-4.1，以及来自DeepSeek的R1），这些模型在真实性的检测准确率上表现良好，但在欺骗性的检测准确率上表现较差。这表明，能力提升本身并不能解决大型语言模型在真实性检测方面的根本挑战。', 'title_zh': '理性不足：探究LLM中的真相偏差与阿谀倾向'}
{'arxiv_id': 'arXiv:2506.21560', 'title': 'Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning', 'authors': 'Yifu Han, Geo Zhang', 'link': 'https://arxiv.org/abs/2506.21560', 'abstract': 'This study investigates the effectiveness of reinforcement learning (RL) fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two challenging tasks: instruction following and mathematical reasoning. We compare supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models. Our experiments show that RLOO with DeBERTa reward modeling achieves the best alignment, while DPO provides strong and consistent results. For math reasoing tasks, synthetic data augmentation and best-of-N sampling with an external verifier significantly improve accuracy, showing the potential of combining fine-tuning with inference-time tools. This study highlights key trade-offs and practical strategies for training lightweight, task-aligned small-scale language models.', 'abstract_zh': '本研究探讨了强化学习（RL）微调技术在紧凑型语言模型（Qwen2.5-0.5B Base）上对两项挑战任务（指令跟随和数学推理）的有效性。我们比较了监督微调（SFT）、使用偏好标签数据的直接偏好优化（DPO）以及奖励模型下的Reinforce Leave-One-Out（RLOO）。实验结果显示，使用DeBERTa奖励模型的RLOO方法在对齐效果上最佳，而DPO方法提供了稳定而强劲的结果。对于数学推理任务，合成数据增强和外部验证器的Best-of-N采样显著提高了准确性，展示了将微调与推理时工具结合的潜力。本研究突出了训练轻量级、任务对齐的小型语言模型的关键权衡和实用策略。', 'title_zh': '语言模型的强化学习微调以执行指令和数学推理'}
{'arxiv_id': 'arXiv:2506.21545', 'title': 'Data Efficacy for Language Model Training', 'authors': 'Yalun Dai, Yangyu Huang, Xin Zhang, Wenshan Wu, Chong Li, Wenhui Lu, Shijie Cao, Li Dong, Scarlett Li', 'link': 'https://arxiv.org/abs/2506.21545', 'abstract': 'Data is fundamental to the training of language models (LM). Recent research has been dedicated to data efficiency, which aims to maximize performance by selecting a minimal or optimal subset of training data. Techniques such as data filtering, sampling, and selection play a crucial role in this area. To complement it, we define Data Efficacy, which focuses on maximizing performance by optimizing the organization of training data and remains relatively underexplored. This work introduces a general paradigm, DELT, for considering data efficacy in LM training, which highlights the significance of training data organization. DELT comprises three components: Data Scoring, Data Selection, and Data Ordering. Among these components, we design Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which considers both the learnability and quality of each data sample from the gradient consistency perspective. We also devise Folding Ordering (FO), as a novel instance of Data Ordering, which addresses issues such as model forgetting and data distribution bias. Comprehensive experiments validate the data efficacy in LM training, which demonstrates the following: Firstly, various instances of the proposed DELT enhance LM performance to varying degrees without increasing the data scale and model size. Secondly, among these instances, the combination of our proposed LQS for data scoring and Folding for data ordering achieves the most significant improvement. Lastly, data efficacy can be achieved together with data efficiency by applying data selection. Therefore, we believe that data efficacy is a promising foundational area in LM training.', 'abstract_zh': '数据是语言模型训练的基础。最近的研究致力于提高数据效率，通过选择最小或最优的数据子集来最大化性能。数据过滤、采样和选择等技术在这一领域发挥着关键作用。为补充这一点，我们定义了数据效能，它旨在通过优化训练数据的组织来最大化性能，并且相对未被充分探索。本文提出了一种通用框架DELT，用于在语言模型训练中考虑数据效能，强调了训练数据组织的重要性。DELT包含三个组件：数据评分、数据选择和数据排序。在这些组件中，我们设计了可学习性-质量评分（LQS），这是一种新的数据评分实例，从梯度一致性视角考虑每个数据样本的可学习性和质量。我们还设计了折叠排序（FO），这是一种新的数据排序实例，解决了模型遗忘和数据分布偏差等问题。全面的实验验证了在语言模型训练中实现数据效能，表明了以下几点：首先，所提出的DELT的各种实例在不增加数据量和模型规模的情况下，不同程度地提升了语言模型的性能。其次，在这些实例中，我们的LQS数据评分和Folding数据排序的结合实现了最大的改进。最后，通过数据选择，可以同时实现数据效能和数据效率。因此，我们认为数据效能是语言模型训练中一个前景广阔的基础领域。', 'title_zh': '语言模型训练的数据有效性'}
{'arxiv_id': 'arXiv:2501.06184', 'title': 'PEACE: Empowering Geologic Map Holistic Understanding with MLLMs', 'authors': 'Yangyu Huang, Tianyi Gao, Haoran Xu, Qihao Zhao, Yang Song, Zhipeng Gui, Tengchao Lv, Hao Chen, Lei Cui, Scarlett Li, Furu Wei', 'link': 'https://arxiv.org/abs/2501.06184', 'abstract': "Geologic map, as a fundamental diagram in geology science, provides critical insights into the structure and composition of Earth's subsurface and surface. These maps are indispensable in various fields, including disaster detection, resource exploration, and civil engineering. Despite their significance, current Multimodal Large Language Models (MLLMs) often fall short in geologic map understanding. This gap is primarily due to the challenging nature of cartographic generalization, which involves handling high-resolution map, managing multiple associated components, and requiring domain-specific knowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever benchmark for evaluating MLLMs in geologic map understanding, which assesses the full-scale abilities in extracting, referring, grounding, reasoning, and analyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent designed for geologic map understanding, which features three modules: Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI), and Prompt-enhanced Question Answering (PEQA). Inspired by the interdisciplinary collaboration among human scientists, an AI expert group acts as consultants, utilizing a diverse tool pool to comprehensively analyze questions. Through comprehensive experiments, GeoMap-Agent achieves an overall score of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o. Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs, paves the way for advanced AI applications in geology, enhancing the efficiency and accuracy of geological investigations.", 'abstract_zh': '基于地质图的全方位理解：利用多模态大规模语言模型（PEACE）', 'title_zh': 'PEACE: 通过MLLMs增强地质图整体理解'}
{'arxiv_id': 'arXiv:2412.15194', 'title': 'MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark', 'authors': 'Qihao Zhao, Yangyu Huang, Tengchao Lv, Lei Cui, Qinzheng Sun, Shaoguang Mao, Xin Zhang, Ying Xin, Qiufeng Yin, Scarlett Li, Furu Wei', 'link': 'https://arxiv.org/abs/2412.15194', 'abstract': "Multiple-choice question (MCQ) datasets like Massive Multitask Language Understanding (MMLU) are widely used to evaluate the commonsense, understanding, and problem-solving abilities of large language models (LLMs). However, the open-source nature of these benchmarks and the broad sources of training data for LLMs have inevitably led to benchmark contamination, resulting in unreliable evaluation results. To alleviate this issue, we propose a contamination-free and more challenging MCQ benchmark called MMLU-CF. This benchmark reassesses LLMs' understanding of world knowledge by averting both unintentional and malicious data leakage. To avoid unintentional data leakage, we source data from a broader domain and design three decontamination rules. To prevent malicious data leakage, we divide the benchmark into validation and test sets with similar difficulty and subject distributions. The test set remains closed-source to ensure reliable results, while the validation set is publicly available to promote transparency and facilitate independent verification. Our evaluation of mainstream LLMs reveals that the powerful GPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on the test set, which indicates the effectiveness of our approach in creating a more rigorous and contamination-free evaluation standard. The GitHub repository is available at this https URL and the dataset refers to this https URL.", 'abstract_zh': '无污染更具挑战性的多项选择题基准MMLU-CF', 'title_zh': 'MMLU-CF：一种无污染的多任务语言理解基准测试'}
