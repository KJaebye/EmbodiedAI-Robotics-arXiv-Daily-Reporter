{'arxiv_id': 'arXiv:2506.21631', 'title': 'Real-Time 3D Guidewire Reconstruction from Intraoperative DSA Images for Robot-Assisted Endovascular Interventions', 'authors': 'Tianliang Yao, Bingrui Li, Bo Lu, Zhiqiang Pei, Yixuan Yuan, Peng Qi', 'link': 'https://arxiv.org/abs/2506.21631', 'abstract': 'Accurate three-dimensional (3D) reconstruction of guidewire shapes is crucial for precise navigation in robot-assisted endovascular interventions. Conventional 2D Digital Subtraction Angiography (DSA) is limited by the absence of depth information, leading to spatial ambiguities that hinder reliable guidewire shape sensing. This paper introduces a novel multimodal framework for real-time 3D guidewire reconstruction, combining preoperative 3D Computed Tomography Angiography (CTA) with intraoperative 2D DSA images. The method utilizes robust feature extraction to address noise and distortion in 2D DSA data, followed by deformable image registration to align the 2D projections with the 3D CTA model. Subsequently, the inverse projection algorithm reconstructs the 3D guidewire shape, providing real-time, accurate spatial information. This framework significantly enhances spatial awareness for robotic-assisted endovascular procedures, effectively bridging the gap between preoperative planning and intraoperative execution. The system demonstrates notable improvements in real-time processing speed, reconstruction accuracy, and computational efficiency. The proposed method achieves a projection error of 1.76$\\pm$0.08 pixels and a length deviation of 2.93$\\pm$0.15\\%, with a frame rate of 39.3$\\pm$1.5 frames per second (FPS). These advancements have the potential to optimize robotic performance and increase the precision of complex endovascular interventions, ultimately contributing to better clinical outcomes.', 'abstract_zh': '精确的三维引导线重建对于机器人辅助介入血管手术的精确导航至关重要。本论文提出了一种结合术前3D计算机断层血管成像(CTA)和术中2D数字减影血管造影(DSA)图像的新型多模态实时三维引导线重建框架。该方法利用稳健的特征提取来处理2D DSA数据中的噪声和失真，并通过可变形图像注册来对齐2D投影与3D CTA模型。随后，逆投影算法重建三维引导线形状，提供实时、准确的空间信息。该框架显著增强了机器人辅助介入血管手术的空间感知能力，有效地填补了术前计划与术中执行之间的差距。该系统在实时处理速度、重建准确性和计算效率方面表现出显著改进。所提出的方法实现了投影误差为1.76±0.08像素、长度偏差为2.93±0.15%以及每秒39.3±1.5帧的帧率。这些进步有可能优化机器人性能并提高复杂介入血管手术的精确度，最终有助于获得更好的临床效果。', 'title_zh': '基于术中DSA图像的实时3D导丝重建在机器人辅助血管内干预中的应用'}
{'arxiv_id': 'arXiv:2506.22191', 'title': 'Robust and Accurate Multi-view 2D/3D Image Registration with Differentiable X-ray Rendering and Dual Cross-view Constraints', 'authors': 'Yuxin Cui, Rui Song, Yibin Li, Max Q.-H. Meng, Zhe Min', 'link': 'https://arxiv.org/abs/2506.22191', 'abstract': 'Robust and accurate 2D/3D registration, which aligns preoperative models with intraoperative images of the same anatomy, is crucial for successful interventional navigation. To mitigate the challenge of a limited field of view in single-image intraoperative scenarios, multi-view 2D/3D registration is required by leveraging multiple intraoperative images. In this paper, we propose a novel multi-view 2D/3D rigid registration approach comprising two stages. In the first stage, a combined loss function is designed, incorporating both the differences between predicted and ground-truth poses and the dissimilarities (e.g., normalized cross-correlation) between simulated and observed intraoperative images. More importantly, additional cross-view training loss terms are introduced for both pose and image losses to explicitly enforce cross-view constraints. In the second stage, test-time optimization is performed to refine the estimated poses from the coarse stage. Our method exploits the mutual constraints of multi-view projection poses to enhance the robustness of the registration process. The proposed framework achieves a mean target registration error (mTRE) of $0.79 \\pm 2.17$ mm on six specimens from the DeepFluoro dataset, demonstrating superior performance compared to state-of-the-art registration algorithms.', 'abstract_zh': 'robust和准确的多视图2D/3D配准：用于同一解剖结构的术前模型与术中图像对齐，对于成功的介入导航至关重要。为了缓解单图像术中场景视野有限的挑战，需要利用多个术中图像进行多视图2D/3D配准。在本文中，我们提出了一种新颖的两阶段多视图2D/3D刚性配准方法。在第一阶段，设计了一种综合损失函数，结合了预测姿态与 ground-truth 姿态之间的差异以及模拟与观察的术中图像之间的差异性（如归一化交叉相关）。更重要的是，引入了姿态损失和图像损失的额外跨视图训练损失项，以明确施加跨视图约束。在第二阶段，进行测试时优化以细化粗配准阶段估计的姿态。该方法利用多视图投影姿态的相互约束，增强了配准过程的鲁棒性。所提出框架在 DeepFluoro 数据集上的六个标本上达到了均一目标配准误差 (mTRE) 为 $0.79 \\pm 2.17$ mm，并且与最先进的配准算法相比表现出优越的性能。', 'title_zh': '鲁棒且准确的多视图2D/3D图像注册：可微X射线渲染与双向跨视图约束'}
{'arxiv_id': 'arXiv:2506.22397', 'title': 'Dehazing Light Microscopy Images with Guided Conditional Flow Matching: finding a sweet spot between fidelity and realism', 'authors': 'Anirban Ray, Ashesh, Florian Jug', 'link': 'https://arxiv.org/abs/2506.22397', 'abstract': 'Fluorescence microscopy is a major driver of scientific progress in the life sciences. Although high-end confocal microscopes are capable of filtering out-of-focus light, cheaper and more accessible microscopy modalities, such as widefield microscopy, can not, which consequently leads to hazy image data. Computational dehazing is trying to combine the best of both worlds, leading to cheap microscopy but crisp-looking images. The perception-distortion trade-off tells us that we can optimize either for data fidelity, e.g. low MSE or high PSNR, or for data realism, measured by perceptual metrics such as LPIPS or FID. Existing methods either prioritize fidelity at the expense of realism, or produce perceptually convincing results that lack quantitative accuracy. In this work, we propose HazeMatching, a novel iterative method for dehazing light microscopy images, which effectively balances these objectives. Our goal was to find a balanced trade-off between the fidelity of the dehazing results and the realism of individual predictions (samples). We achieve this by adapting the conditional flow matching framework by guiding the generative process with a hazy observation in the conditional velocity field. We evaluate HazeMatching on 5 datasets, covering both synthetic and real data, assessing both distortion and perceptual quality. Our method is compared against 7 baselines, achieving a consistent balance between fidelity and realism on average. Additionally, with calibration analysis, we show that HazeMatching produces well-calibrated predictions. Note that our method does not need an explicit degradation operator to exist, making it easily applicable on real microscopy data. All data used for training and evaluation and our code will be publicly available under a permissive license.', 'abstract_zh': 'fluorescence显微镜是生命科学领域科学进步的主要驱动力。尽管高端共聚焦显微镜能够过滤掉焦外光，但较便宜且更易获取的显微成像方式，如宽场显微镜，则不能，这导致了模糊的图像数据。计算去雾尝试将两者的优势结合，从而实现低成本显微镜但清晰的图像效果。感知与失真是权衡关系，告诉我们可以优化数据保真度，例如低MSE或高PSNR，或者通过感知度量标准如LPIPS或FID来优化数据现实度。现有方法要么优先考虑保真度而牺牲现实度，要么生成视觉上令人信服但缺乏定量准确性的结果。在本文中，我们提出了一种新的迭代方法HazeMatching，用于去雾光学显微图像，该方法有效地平衡了这些目标。我们的目标是找到去雾结果保真度和个体预测（样本）现实度之间的平衡。我们通过将生成过程指导的条件流匹配框架与朦胧观测引导的条件速度场相结合来实现这一点。我们在5个数据集上评估了HazeMatching，涵盖了合成和真实数据，评估了失真和感知质量。我们的方法与7个基线进行了比较，在平均值上实现了保真度和现实度之间的稳定平衡。此外，通过校准分析，我们展示了HazeMatching产生的预测是具有良好校准的。值得注意的是，我们的方法不需要显式的降级操作符存在，使其易于应用于真实显微镜数据。所有用于训练和评估的数据以及我们的代码将在宽松的许可下公开。', 'title_zh': '基于引导条件流匹配的去雾光显微镜图像处理：在保真度与真实感之间的平衡'}
{'arxiv_id': 'arXiv:2506.22360', 'title': 'From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications', 'authors': 'Nouf Almesafri, Hector Figueiredo, Miguel Arana-Catania', 'link': 'https://arxiv.org/abs/2506.22360', 'abstract': 'This study investigates the performance of the two most relevant computer vision deep learning architectures, Convolutional Neural Network and Vision Transformer, for event-based cameras. These cameras capture scene changes, unlike traditional frame-based cameras with capture static images, and are particularly suited for dynamic environments such as UAVs and autonomous vehicles. The deep learning models studied in this work are ResNet34 and ViT B16, fine-tuned on the GEN1 event-based dataset. The research evaluates and compares these models under both standard conditions and in the presence of simulated noise. Initial evaluations on the clean GEN1 dataset reveal that ResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with ResNet34 showing a slight advantage in classification accuracy. However, the ViT B16 model demonstrates notable robustness, particularly given its pre-training on a smaller dataset. Although this study focuses on ground-based vehicle classification, the methodologies and findings hold significant promise for adaptation to UAV contexts, including aerial object classification and event-based vision systems for aviation-related tasks.', 'abstract_zh': '本研究探讨了事件相机环境下两种最相关计算机视觉深度学习架构——卷积神经网络和视觉变压器的表现。这些相机捕获场景变化，而非传统帧基相机捕捉静态图像，特别适用于动态环境，如无人机和自动驾驶车辆。本研究中研究的深度学习模型为ResNet34和ViT B16，基于GEN1事件相机数据集进行了微调。研究在标准条件下和模拟噪声环境下评估并比较了这些模型的性能。对干净的GEN1数据集的初步评估结果显示，ResNet34和ViT B16分别实现了88%和86%的准确率，ResNet34在分类准确率上稍占优势。然而，ViT B16模型表现出了显著的鲁棒性，特别是考虑到其在较小数据集上的预训练。尽管本研究集中在地面车辆分类，但本研究的方法和发现对无人机场景具有重要意义，包括空中物体分类和与航空相关的事件驱动视觉系统。', 'title_zh': '从地面到空中：基于事件的车辆分类中视觉变换器和CNN的噪声鲁棒性研究及其潜在的无人机应用'}
{'arxiv_id': 'arXiv:2506.22291', 'title': 'RoomCraft: Controllable and Complete 3D Indoor Scene Generation', 'authors': 'Mengqi Zhou, Xipeng Wang, Yuxi Wang, Zhaoxiang Zhang', 'link': 'https://arxiv.org/abs/2506.22291', 'abstract': 'Generating realistic 3D indoor scenes from user inputs remains a challenging problem in computer vision and graphics, requiring careful balance of geometric consistency, spatial relationships, and visual realism. While neural generation methods often produce repetitive elements due to limited global spatial reasoning, procedural approaches can leverage constraints for controllable generation but struggle with multi-constraint scenarios. When constraints become numerous, object collisions frequently occur, forcing the removal of furniture items and compromising layout completeness.\nTo address these limitations, we propose RoomCraft, a multi-stage pipeline that converts real images, sketches, or text descriptions into coherent 3D indoor scenes. Our approach combines a scene generation pipeline with a constraint-driven optimization framework. The pipeline first extracts high-level scene information from user inputs and organizes it into a structured format containing room type, furniture items, and spatial relations. It then constructs a spatial relationship network to represent furniture arrangements and generates an optimized placement sequence using a heuristic-based depth-first search (HDFS) algorithm to ensure layout coherence. To handle complex multi-constraint scenarios, we introduce a unified constraint representation that processes both formal specifications and natural language inputs, enabling flexible constraint-oriented adjustments through a comprehensive action space design. Additionally, we propose a Conflict-Aware Positioning Strategy (CAPS) that dynamically adjusts placement weights to minimize furniture collisions and ensure layout completeness.\nExtensive experiments demonstrate that RoomCraft significantly outperforms existing methods in generating realistic, semantically coherent, and visually appealing room layouts across diverse input modalities.', 'abstract_zh': 'Multi-Stage Pipeline for Generating Realistic 3D Indoor Scenes from User Inputs Through Constraint-Driven Optimization', 'title_zh': 'RoomCraft: 可控且完整的室内三维场景生成'}
{'arxiv_id': 'arXiv:2506.22179', 'title': 'Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition', 'authors': 'Wenhan Wu, Zhishuai Guo, Chen Chen, Hongfei Xue, Aidong Lu', 'link': 'https://arxiv.org/abs/2506.22179', 'abstract': 'Zero-shot skeleton-based action recognition aims to develop models capable of identifying actions beyond the categories encountered during training. Previous approaches have primarily focused on aligning visual and semantic representations but often overlooked the importance of fine-grained action patterns in the semantic space (e.g., the hand movements in drinking water and brushing teeth). To address these limitations, we propose a Frequency-Semantic Enhanced Variational Autoencoder (FS-VAE) to explore the skeleton semantic representation learning with frequency decomposition. FS-VAE consists of three key components: 1) a frequency-based enhancement module with high- and low-frequency adjustments to enrich the skeletal semantics learning and improve the robustness of zero-shot action recognition; 2) a semantic-based action description with multilevel alignment to capture both local details and global correspondence, effectively bridging the semantic gap and compensating for the inherent loss of information in skeleton sequences; 3) a calibrated cross-alignment loss that enables valid skeleton-text pairs to counterbalance ambiguous ones, mitigating discrepancies and ambiguities in skeleton and text features, thereby ensuring robust alignment. Evaluations on the benchmarks demonstrate the effectiveness of our approach, validating that frequency-enhanced semantic features enable robust differentiation of visually and semantically similar action clusters, improving zero-shot action recognition.', 'abstract_zh': '零样本基于骨架的动作识别旨在开发能够在训练中未遇到的类别中识别动作的模型。以往的方法主要集中在视觉表示和语义表示的对齐上，但往往忽视了语义空间中细粒度动作模式的重要性（如饮水和刷牙的手部动作）。为解决这些问题，我们提出了一种基于频率-语义增强的变分自编码器（FS-VAE）来探索通过频率分解进行的骨架语义表示学习。FS-VAE 包含三个关键组件：1）基于频率的增强模块，通过高低频调整丰富骨架语义学习并提高零样本动作识别的鲁棒性；2）基于语义的动作描述，采用多级对齐捕捉局部细节和全局对应关系，有效缩小语义鸿沟并补充骨架序列固有的信息损失；3）校准的交叉对齐损失，使有效的骨架-文本对能够抵消含糊不清的对齐，减轻骨架和文本特征之间的不一致性和模糊性，从而确保稳健的对齐。基准测试结果验证了该方法的有效性，表明频率增强的语义特征能够稳健地区分视觉上和语义上相似的动作簇，提高零样本动作识别的效果。', 'title_zh': '频率-语义增强变分自编码器在零样本骨架动作识别中的应用'}
{'arxiv_id': 'arXiv:2506.21945', 'title': 'SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images', 'authors': 'Naftaly Wambugu, Ruisheng Wang, Bo Guo, Tianshu Yu, Sheng Xu, Mohammed Elhassan', 'link': 'https://arxiv.org/abs/2506.21945', 'abstract': 'Land cover maps generated from semantic segmentation of high-resolution remotely sensed images have drawn mucon in the photogrammetry and remote sensing research community. Currently, massive fine-resolution remotely sensed (FRRS) images acquired by improving sensing and imaging technologies become available. However, accurate semantic segmentation of such FRRS images is greatly affected by substantial class disparities, the invisibility of key ground objects due to occlusion, and object size variation. Despite the extraordinary potential in deep convolutional neural networks (DCNNs) in image feature learning and representation, extracting sufficient features from FRRS images for accurate semantic segmentation is still challenging. These challenges demand the deep learning models to learn robust features and generate sufficient feature descriptors. Specifically, learning multi-contextual features to guarantee adequate coverage of varied object sizes from the ground scene and harnessing global-local contexts to overcome class disparities challenge even profound networks. Deeper networks significantly lose spatial details due to gradual downsampling processes resulting in poor segmentation results and coarse boundaries. This article presents a stacked deep residual network (SDRNet) for semantic segmentation from FRRS images. The proposed framework utilizes two stacked encoder-decoder networks to harness long-range semantics yet preserve spatial information and dilated residual blocks (DRB) between each encoder and decoder network to capture sufficient global dependencies thus improving segmentation performance. Our experimental results obtained using the ISPRS Vaihingen and Potsdam datasets demonstrate that the SDRNet performs effectively and competitively against current DCNNs in semantic segmentation.', 'abstract_zh': '基于高分辨率遥感图像语义分割的语义覆盖地图生成在摄影测量与遥感研究领域引起了广泛关注。尽管改进感测和成像技术获得的高分辨率细粒度遥感（FRRS）图像变得可用，但准确的语义分割仍受类别不均衡、关键地面对象因遮挡而不可见以及物体大小变化等挑战的影响。尽管深度卷积神经网络（DCNNs）在图像特征学习和表示方面具有非凡潜力，但从中提取足够的特征进行准确的语义分割仍然具有挑战性。这些挑战要求深度学习模型学习稳健的特征并生成足够的特征描述符。具体而言，学习多上下文特征以确保不同物体大小的充分覆盖，并利用全局-局部上下文来克服类别不均衡的挑战，即使是深层网络也不例外。由于逐级下采样过程导致空间细节大量丢失，深层网络会产生较差的分割结果和粗糙的边界。本文提出了一种堆叠深度残差网络（SDRNet）用于FRRS图像的语义分割。所提出的框架利用两个堆叠的编码-解码网络来捕获长范围语义同时保留空间信息，并在每个编码器和解码器网络之间使用膨胀残差块（DRB）来捕捉足够的全局依赖性，从而提高分割性能。使用ISPRS Vaihingen和Potsdam数据集进行的实验结果表明，SDRNet 在语义分割方面能够有效地与当前的DCNNs竞争。', 'title_zh': 'SDRNET：堆叠深度残差网络用于高分辨率遥感图像准确语义分割'}
{'arxiv_id': 'arXiv:2506.21892', 'title': 'SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation', 'authors': 'Adam Goodge, Xun Xu, Bryan Hooi, Wee Siong Ng, Jingyi Liao, Yongyi Su, Xulei Yang', 'link': 'https://arxiv.org/abs/2506.21892', 'abstract': 'As point cloud data increases in prevalence in a variety of applications, the ability to detect out-of-distribution (OOD) point cloud objects becomes critical for ensuring model safety and reliability. However, this problem remains under-explored in existing research. Inspired by success in the image domain, we propose to exploit advances in 3D vision-language models (3D VLMs) for OOD detection in point cloud objects. However, a major challenge is that point cloud datasets used to pre-train 3D VLMs are drastically smaller in size and object diversity than their image-based counterparts. Critically, they often contain exclusively computer-designed synthetic objects. This leads to a substantial domain shift when the model is transferred to practical tasks involving real objects scanned from the physical environment. In this paper, our empirical experiments show that synthetic-to-real domain shift significantly degrades the alignment of point cloud with their associated text embeddings in the 3D VLM latent space, hindering downstream performance. To address this, we propose a novel methodology called SODA which improves the detection of OOD point clouds through a neighborhood-based score propagation scheme. SODA is inference-based, requires no additional model training, and achieves state-of-the-art performance over existing approaches across datasets and problem settings.', 'abstract_zh': '随着点云数据在各种应用中日益增多，检测点云离群对象的能力对于确保模型安全性和可靠性变得至关重要。然而，这一问题在现有研究中仍被忽视。受图像领域成功经验的启发，我们提出利用三维视觉-语言模型（3D VLMs）的进步来解决点云离群检测问题。然而，一个主要挑战是，用于预训练3D VLMs的点云数据集在大小和对象多样性方面远不如基于图像的对应物。关键的是，这些数据集往往仅包含计算机设计的合成对象。这导致当模型转移到涉及实物环境扫描对象的实际任务时，存在显著的数据域迁移问题。本文通过实验证明，合成到现实的数据域迁移严重破坏了点云与其关联文本嵌入在3D VLM潜在空间中的对齐，从而妨碍下游性能。为了解决这一问题，我们提出了一种名为SODA的新型方法，通过基于邻域的分数传播方案来提高点云离群检测的效果。SODA是一种推理方法，无需额外的模型训练，并在不同数据集和问题设置上实现了现有方法的最先进性能。', 'title_zh': 'SODA: 在领域偏移点云中通过邻域传播进行异常分布检测'}
{'arxiv_id': 'arXiv:2506.21884', 'title': 'UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields', 'authors': 'Fabian Perez, Sara Rojas, Carlos Hinojosa, Hoover Rueda-Chacón, Bernard Ghanem', 'link': 'https://arxiv.org/abs/2506.21884', 'abstract': 'Neural Radiance Field (NeRF)-based segmentation methods focus on object semantics and rely solely on RGB data, lacking intrinsic material properties. This limitation restricts accurate material perception, which is crucial for robotics, augmented reality, simulation, and other applications. We introduce UnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling joint hyperspectral novel view synthesis and unsupervised material segmentation. Our method models spectral reflectance via diffuse and specular components, where a learned dictionary of global endmembers represents pure material signatures, and per-point abundances capture their distribution. For material segmentation, we use spectral signature predictions along learned endmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF enables scene editing by modifying learned endmember dictionaries for flexible material-based appearance manipulation. Extensive experiments validate our approach, demonstrating superior spectral reconstruction and material segmentation to existing methods. Project page: this https URL.', 'abstract_zh': '基于Neural Radiance Field (NeRF)的分段方法侧重于对象语义并仅依赖于RGB数据，缺乏内在材料特性。这一局限性限制了对材料的准确感知，这对机器人技术、增强现实、模拟及其他应用至关重要。我们引入了UnMix-NeRF框架，将光谱解混合集成到NeRF中，实现联合超光谱新视角合成和无监督材料分段。我们的方法通过漫反射和镜面反射成分建模光谱反射率，其中学习到的全局端元字典代表纯材料特征，每个点的丰度捕捉它们的分布。在材料分段方面，我们利用学习到端元的光谱特征进行无监督材料聚类。此外，UnMix-NeRF 通过修改学习到的端元字典来实现场景编辑，从而灵活地进行基于材料的外观操作。大量实验验证了我们的方法，展示了其在光谱重建和材料分段方面的优越性能。项目页面: [点击此处](this https URL)。', 'title_zh': 'UnMix-NeRF：光谱解混与神经辐射场的结合'}
{'arxiv_id': 'arXiv:2506.21826', 'title': 'Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models', 'authors': 'Rafael Sterzinger, Marco Peer, Robert Sablatnig', 'link': 'https://arxiv.org/abs/2506.21826', 'abstract': 'As rich sources of history, maps provide crucial insights into historical changes, yet their diverse visual representations and limited annotated data pose significant challenges for automated processing. We propose a simple yet effective approach for few-shot segmentation of historical maps, leveraging the rich semantic embeddings of large vision foundation models combined with parameter-efficient fine-tuning. Our method outperforms the state-of-the-art on the Siegfried benchmark dataset in vineyard and railway segmentation, achieving +5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20% in the more challenging 5-shot setting. Additionally, it demonstrates strong performance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3% for building block segmentation, despite not being optimized for this shape-sensitive metric, underscoring its generalizability. Notably, our approach maintains high performance even in extremely low-data regimes (10- & 5-shot), while requiring only 689k trainable parameters - just 0.21% of the total model size. Our approach enables precise segmentation of diverse historical maps while drastically reducing the need for manual annotations, advancing automated processing and analysis in the field. Our implementation is publicly available at: this https URL.', 'abstract_zh': '基于大规模视觉基础模型的参数高效微调，few-shot历史地图分割方法', 'title_zh': '基于视觉基础模型的线性探查的 Historical Maps 少-shot 分割'}
{'arxiv_id': 'arXiv:2506.21785', 'title': 'Comparing Learning Paradigms for Egocentric Video Summarization', 'authors': 'Daniel Wen', 'link': 'https://arxiv.org/abs/2506.21785', 'abstract': 'In this study, we investigate various computer vision paradigms - supervised learning, unsupervised learning, and prompt fine-tuning - by assessing their ability to understand and interpret egocentric video data. Specifically, we examine Shotluck Holmes (state-of-the-art supervised learning), TAC-SUM (state-of-the-art unsupervised learning), and GPT-4o (a prompt fine-tuned pre-trained model), evaluating their effectiveness in video summarization. Our results demonstrate that current state-of-the-art models perform less effectively on first-person videos compared to third-person videos, highlighting the need for further advancements in the egocentric video domain. Notably, a prompt fine-tuned general-purpose GPT-4o model outperforms these specialized models, emphasizing the limitations of existing approaches in adapting to the unique challenges of first-person perspectives. Although our evaluation is conducted on a small subset of egocentric videos from the Ego-Exo4D dataset due to resource constraints, the primary objective of this research is to provide a comprehensive proof-of-concept analysis aimed at advancing the application of computer vision techniques to first-person videos. By exploring novel methodologies and evaluating their potential, we aim to contribute to the ongoing development of models capable of effectively processing and interpreting egocentric perspectives.', 'abstract_zh': '本研究通过评估监督学习、无监督学习和提示微调等多种计算机视觉范式的理解与解释能力，探究其在第一人称视频数据上的表现，具体考察了当前最佳的监督学习模型Shotluck Holmes、最佳的无监督学习模型TAC-SUM以及微调后的通用模型GPT-4o在视频摘要中的效果。研究结果表明，当前最先进的模型在第一人称视频上的表现逊于第三人称视频，突显了在第一人称视频领域进一步发展的必要性。值得注意的是，一个微调后的通用GPT-4o模型在这些专业模型中表现更佳，突显了现有方法在适应第一人称视角的独特挑战时的局限性。尽管由于资源限制，评估是在Ego-Exo4D数据集的小子集上进行的，本研究的主要目的是提供一个全面的概念验证分析，旨在推动计算机视觉技术在第一人称视频中的应用。通过探索新的方法并评估其潜力，我们旨在为能够有效处理和解释第一人称视角的模型的持续开发做出贡献。', 'title_zh': '自视点视频摘要化学习范式比较'}
{'arxiv_id': 'arXiv:2506.21731', 'title': 'Exploring Image Generation via Mutually Exclusive Probability Spaces and Local Correlation Hypothesis', 'authors': 'Chenqiu Zhao, Anup Basu', 'link': 'https://arxiv.org/abs/2506.21731', 'abstract': 'We propose two theoretical frameworks, the Mutually Exclusive Probability Space (MESP) and the Local Correlation Hypothesis (LCH), to explore a potential limitation in probabilistic generative models; namely that learning global distributions leads to memorization rather than generative behavior. MESP emerges from our rethinking of the Variational Autoencoder (VAE). We observe that latent variable distributions in VAE exhibit overlap, which leads to an optimization conflict between the reconstruction loss and KL-divergence loss. A lower bound based on the overlap coefficient is proposed. We refer to this phenomenon as Mutually Exclusive Probability Spaces. Based on MESP, a Binary Latent Autoencoder (BL-AE) is proposed to encode images into binary latent representations. These binary latents are used as the input to our Autoregressive Random Variable Model (ARVM), a modified autoregressive model outputting histograms. Our ARVM achieves competitive FID scores, outperforming state-of-the-art methods on standard datasets. However, such scores reflect memorization rather than generation. To address this issue, we propose the Local Correlation Hypothesis (LCH), which posits that generative capability arising from local correlations among latent variables. Comprehensive experiments and discussions are conducted to validate our frameworks.', 'abstract_zh': '我们提出了两个理论框架：互斥概率空间（MESP）和局部关联假设（LCH），以探讨概率生成模型中潜在的局限性；即学习全局分布会导致记忆行为而非生成行为。MESP源自我们对变分自编码器（VAE）的重新思考。我们发现VAE中的潜在变量分布存在重叠，导致重构损失和KL散度损失之间的优化冲突。基于重叠系数提出了一个下界。我们将这种现象称为互斥概率空间。基于MESP，我们提出了二元潜自编码器（BL-AE）以将图像编码为二元潜表示。这些二元潜表示作为我们自回归随机变量模型（ARVM）的输入，ARVM是一个输出直方图的修改自回归模型。我们的ARVM在标准数据集上实现了竞争力的FID得分，超越了当前最好的方法。但这些得分反映的是记忆而非生成。为解决此问题，我们提出了局部关联假设（LCH），认为生成能力来源于潜在变量之间的局部关联。进行了全面的实验和讨论以验证我们的框架。', 'title_zh': '基于互斥概率空间和局部相关假设的图像生成探索'}
{'arxiv_id': 'arXiv:2506.21722', 'title': 'Elucidating and Endowing the Diffusion Training Paradigm for General Image Restoration', 'authors': 'Xin Lu, Xueyang Fu, Jie Xiao, Zihao Fan, Yurui Zhu, Zheng-Jun Zha', 'link': 'https://arxiv.org/abs/2506.21722', 'abstract': 'While diffusion models demonstrate strong generative capabilities in image restoration (IR) tasks, their complex architectures and iterative processes limit their practical application compared to mainstream reconstruction-based general ordinary IR networks. Existing approaches primarily focus on optimizing network architecture and diffusion paths but overlook the integration of the diffusion training paradigm within general ordinary IR frameworks. To address these challenges, this paper elucidates key principles for adapting the diffusion training paradigm to general IR training through systematic analysis of time-step dependencies, network hierarchies, noise-level relationships, and multi-restoration task correlations, proposing a new IR framework supported by diffusion-based training. To enable IR networks to simultaneously restore images and model generative representations, we introduce a series of regularization strategies that align diffusion objectives with IR tasks, improving generalization in single-task scenarios. Furthermore, recognizing that diffusion-based generation exerts varying influences across different IR tasks, we develop an incremental training paradigm and task-specific adaptors, further enhancing performance in multi-task unified IR. Experiments demonstrate that our method significantly improves the generalization of IR networks in single-task IR and achieves superior performance in multi-task unified IR. Notably, the proposed framework can be seamlessly integrated into existing general IR architectures.', 'abstract_zh': '基于扩散训练范式的通用图像恢复框架', 'title_zh': '阐明并赋予扩散训练范式以通用图像修复能力'}
{'arxiv_id': 'arXiv:2212.09525', 'title': 'FreeEnricher: Enriching Face Landmarks without Additional Cost', 'authors': 'Yangyu Huang, Xi Chen, Jongyoo Kim, Hao Yang, Chong Li, Jiaolong Yang, Dong Chen', 'link': 'https://arxiv.org/abs/2212.09525', 'abstract': 'Recent years have witnessed significant growth of face alignment. Though dense facial landmark is highly demanded in various scenarios, e.g., cosmetic medicine and facial beautification, most works only consider sparse face alignment. To address this problem, we present a framework that can enrich landmark density by existing sparse landmark datasets, e.g., 300W with 68 points and WFLW with 98 points. Firstly, we observe that the local patches along each semantic contour are highly similar in appearance. Then, we propose a weakly-supervised idea of learning the refinement ability on original sparse landmarks and adapting this ability to enriched dense landmarks. Meanwhile, several operators are devised and organized together to implement the idea. Finally, the trained model is applied as a plug-and-play module to the existing face alignment networks. To evaluate our method, we manually label the dense landmarks on 300W testset. Our method yields state-of-the-art accuracy not only in newly-constructed dense 300W testset but also in the original sparse 300W and WFLW testsets without additional cost.', 'abstract_zh': '最近几年，面部对齐领域取得了显著的增长。尽管密集面部 landmarks 在各种场景中高度需求，例如美容医学和面部美化，大多数工作仅考虑稀疏面部对齐。为了解决这一问题，我们提出了一种框架，可以通过现有的稀疏 landmarks 数据集（例如包含 68 个点的 300W 和包含 98 个点的 WFLW）来丰富 landmarks 密度。首先，我们观察到每条语义轮廓沿线的局部补丁在Appearance上具有高度相似性。然后，我们提出了一种弱监督方法来学习在原始稀疏 landmarks 上的细化能力，并适应这种能力以生产密集 landmarks。同时，设计并组织了一系列操作来实现这一理念。最后，训练好的模型被应用为即插即用模块到现有的面部对齐网络中。为了评估我们的方法，我们在 300W 测试集上手动标注了密集 landmarks。我们的方法不仅在新构建的密集 300W 测试集上达到了最先进的准确性，在原始稀疏的 300W 和 WFLW 测试集上也取得了该精度，而无需额外成本。', 'title_zh': 'FreeEnricher: 不额外增加成本的 facial landmarks 增强方法'}
{'arxiv_id': 'arXiv:2109.05721', 'title': 'ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment', 'authors': 'Yangyu Huang, Hao Yang, Chong Li, Jongyoo Kim, Fangyun Wei', 'link': 'https://arxiv.org/abs/2109.05721', 'abstract': 'The recent progress of CNN has dramatically improved face alignment performance. However, few works have paid attention to the error-bias with respect to error distribution of facial landmarks. In this paper, we investigate the error-bias issue in face alignment, where the distributions of landmark errors tend to spread along the tangent line to landmark curves. This error-bias is not trivial since it is closely connected to the ambiguous landmark labeling task. Inspired by this observation, we seek a way to leverage the error-bias property for better convergence of CNN model. To this end, we propose anisotropic direction loss (ADL) and anisotropic attention module (AAM) for coordinate and heatmap regression, respectively. ADL imposes strong binding force in normal direction for each landmark point on facial boundaries. On the other hand, AAM is an attention module which can get anisotropic attention mask focusing on the region of point and its local edge connected by adjacent points, it has a stronger response in tangent than in normal, which means relaxed constraints in the tangent. These two methods work in a complementary manner to learn both facial structures and texture details. Finally, we integrate them into an optimized end-to-end training pipeline named ADNet. Our ADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which demonstrates the effectiveness and robustness.', 'abstract_zh': 'recent progress of CNN在面部对齐性能上的 recent 进展显著提高。然而，很少有工作关注与面部关键点误差分布相关的误差偏差问题。在本文中，我们研究了面部对齐中的误差偏差问题，发现关键点误差的分布趋势沿着关键点曲线的切线方向扩展。这种误差偏差并不简单，因为它与模棱两可的关键点标签任务密切相关。受此观察的启发，我们寻求一种利用误差偏差特性以提高CNN模型收敛性的方法。为此，我们提出了各向异性方向损失（ADL）和各向异性注意力模块（AAM），分别应用于坐标和热图回归。ADL 对面部边界上的每个关键点施加强的法线方向约束力。另一方面，AAM 是一种注意力模块，能够在关键点及其相邻点形成的局部边缘连接的区域中获得各向异性注意力掩码，并在切线方向上具有更强的响应，这意味着在切线方向上的松弛约束条件。这两种方法相互补充，以学习面部结构和纹理细节。最后，我们将它们整合到一个优化的端到端训练管道ADNet 中。我们的ADNet 在300W、WFLW 和COFW 数据集上取得了最先进的结果，这证明了其有效性和鲁棒性。', 'title_zh': 'ADNet: 利用误差偏差朝向正常方向进行面部对齐'}
