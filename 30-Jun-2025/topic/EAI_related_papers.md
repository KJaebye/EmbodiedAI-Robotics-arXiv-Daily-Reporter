# Evaluating Pointing Gestures for Target Selection in Human-Robot Collaboration 

**Title (ZH)**: 评估人在机器人协作中使用指示手势进行目标选择的有效性 

**Authors**: Noora Sassali, Roel Pieters  

**Link**: [PDF](https://arxiv.org/pdf/2506.22116)  

**Abstract**: Pointing gestures are a common interaction method used in Human-Robot Collaboration for various tasks, ranging from selecting targets to guiding industrial processes. This study introduces a method for localizing pointed targets within a planar workspace. The approach employs pose estimation, and a simple geometric model based on shoulder-wrist extension to extract gesturing data from an RGB-D stream. The study proposes a rigorous methodology and comprehensive analysis for evaluating pointing gestures and target selection in typical robotic tasks. In addition to evaluating tool accuracy, the tool is integrated into a proof-of-concept robotic system, which includes object detection, speech transcription, and speech synthesis to demonstrate the integration of multiple modalities in a collaborative application. Finally, a discussion over tool limitations and performance is provided to understand its role in multimodal robotic systems. All developments are available at: this https URL. 

**Abstract (ZH)**: 点指手势是人类与机器人协作中用于各种任务的一种常见交互方法，范围从选择目标到引导工业过程。本文介绍了一种在平面工作空间中定位点指目标的方法。该方法采用姿态估计，并基于肩关节-腕关节延伸的简单几何模型从RGB-D流中提取手势数据。本文提出了一种严谨的方法论和全面的分析来评估点指手势和目标选择在典型机器人任务中的表现。除了评估工具精度外，还将工具集成到一个概念验证机器人系统中，该系统包括对象检测、语音转录和语音合成，以展示多模态在协作应用中的集成。最后，对工具的局限性和性能进行了讨论，以理解其在多模态机器人系统中的角色。所有开发成果可在以下链接获取：this https URL。 

---
# Multi-Robot Assembly of Deformable Linear Objects Using Multi-Modal Perception 

**Title (ZH)**: 基于多模态感知的可变形线性物体的多机器人组装 

**Authors**: Kejia Chen, Celina Dettmering, Florian Pachler, Zhuo Liu, Yue Zhang, Tailai Cheng, Jonas Dirr, Zhenshan Bing, Alois Knoll, Rüdiger Daub  

**Link**: [PDF](https://arxiv.org/pdf/2506.22034)  

**Abstract**: Industrial assembly of deformable linear objects (DLOs) such as cables offers great potential for many industries. However, DLOs pose several challenges for robot-based automation due to the inherent complexity of deformation and, consequentially, the difficulties in anticipating the behavior of DLOs in dynamic situations. Although existing studies have addressed isolated subproblems like shape tracking, grasping, and shape control, there has been limited exploration of integrated workflows that combine these individual processes. To address this gap, we propose an object-centric perception and planning framework to achieve a comprehensive DLO assembly process throughout the industrial value chain. The framework utilizes visual and tactile information to track the DLO's shape as well as contact state across different stages, which facilitates effective planning of robot actions. Our approach encompasses robot-based bin picking of DLOs from cluttered environments, followed by a coordinated handover to two additional robots that mount the DLOs onto designated fixtures. Real-world experiments employing a setup with multiple robots demonstrate the effectiveness of the approach and its relevance to industrial scenarios. 

**Abstract (ZH)**: 工业环境下可变形线性对象（DLOs）如电缆的装配具有很大的工业潜力。然而，DLOs给基于机器人的自动化带来了挑战，由于其固有的变形复杂性，导致在动态情况下预测DLOs的行为困难。尽管现有研究解决了形状跟踪、抓取和形状控制等孤立子问题，但尚未探讨将这些独立过程集成的综合工作流程。为填补这一空白，我们提出了一种以物体为中心的感知与规划框架，以在整个工业价值链中实现DLO装配过程的全面展开。该框架利用视觉和触觉信息，跟踪DLO的形状及其接触状态，从而促进机器人动作的有效规划。我们的方法包括从杂乱环境中基于机器人的DLO捡拾，然后与两个额外的机器人协调传递，将DLO安装到指定的工装上。基于多机器人设置的实地实验展示了该方法的有效性和其对工业场景的相关性。 

---
# Embodied Domain Adaptation for Object Detection 

**Title (ZH)**: 基于躯体适应的对象检测 

**Authors**: Xiangyu Shi, Yanyuan Qiao, Lingqiao Liu, Feras Dayoub  

**Link**: [PDF](https://arxiv.org/pdf/2506.21860)  

**Abstract**: Mobile robots rely on object detectors for perception and object localization in indoor environments. However, standard closed-set methods struggle to handle the diverse objects and dynamic conditions encountered in real homes and labs. Open-vocabulary object detection (OVOD), driven by Vision Language Models (VLMs), extends beyond fixed labels but still struggles with domain shifts in indoor environments. We introduce a Source-Free Domain Adaptation (SFDA) approach that adapts a pre-trained model without accessing source data. We refine pseudo labels via temporal clustering, employ multi-scale threshold fusion, and apply a Mean Teacher framework with contrastive learning. Our Embodied Domain Adaptation for Object Detection (EDAOD) benchmark evaluates adaptation under sequential changes in lighting, layout, and object diversity. Our experiments show significant gains in zero-shot detection performance and flexible adaptation to dynamic indoor conditions. 

**Abstract (ZH)**: 移动机器人依赖于物体检测器进行感知和室内环境中的物体定位。然而，标准的封闭集方法难以处理真实家庭和实验室中遇到的多样物体和动态条件。由视觉语言模型驱动的开放词汇物体检测（OVOD）超越了固定的标签，但在室内环境中的领域转移问题上仍然存在挑战。我们提出了一种源数据无关的领域适应（SFDA）方法，无需访问源数据即可调整预训练模型。我们通过时间聚类细化伪标签，采用多尺度阈值融合，并应用具有对比学习的Mean Teacher框架。我们的体验式领域适应物体检测基准（EDAOD）评估在照明、布局和物体多样性顺序变化下的适应性。我们的实验展示了在零样本检测性能上的显著提升，并能够灵活适应动态室内条件。 

---
# Skill-Nav: Enhanced Navigation with Versatile Quadrupedal Locomotion via Waypoint Interface 

**Title (ZH)**: Skill-Nav: 通过Waypoint接口增强的多功能四足运动导航 

**Authors**: Dewei Wang, Chenjia Ba, Chenhui Li, Jiyuan Shi, Yan Ding, Chi Zhang, Bin Zhao  

**Link**: [PDF](https://arxiv.org/pdf/2506.21853)  

**Abstract**: Quadrupedal robots have demonstrated exceptional locomotion capabilities through Reinforcement Learning (RL), including extreme parkour maneuvers. However, integrating locomotion skills with navigation in quadrupedal robots has not been fully investigated, which holds promise for enhancing long-distance movement capabilities. In this paper, we propose Skill-Nav, a method that incorporates quadrupedal locomotion skills into a hierarchical navigation framework using waypoints as an interface. Specifically, we train a waypoint-guided locomotion policy using deep RL, enabling the robot to autonomously adjust its locomotion skills to reach targeted positions while avoiding obstacles. Compared with direct velocity commands, waypoints offer a simpler yet more flexible interface for high-level planning and low-level control. Utilizing waypoints as the interface allows for the application of various general planning tools, such as large language models (LLMs) and path planning algorithms, to guide our locomotion policy in traversing terrains with diverse obstacles. Extensive experiments conducted in both simulated and real-world scenarios demonstrate that Skill-Nav can effectively traverse complex terrains and complete challenging navigation tasks. 

**Abstract (ZH)**: 基于技能的导航：将四足机器人运动技能融入层次导航框架中 

---
# Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation 

**Title (ZH)**: 基于姿态信息的强化学习在滑移转向视觉导航中的实验研究 

**Authors**: Ameya Salvi, Venkat Krovi  

**Link**: [PDF](https://arxiv.org/pdf/2506.21732)  

**Abstract**: Vision-based lane keeping is a topic of significant interest in the robotics and autonomous ground vehicles communities in various on-road and off-road applications. The skid-steered vehicle architecture has served as a useful vehicle platform for human controlled operations. However, systematic modeling, especially of the skid-slip wheel terrain interactions (primarily in off-road settings) has created bottlenecks for automation deployment. End-to-end learning based methods such as imitation learning and deep reinforcement learning, have gained prominence as a viable deployment option to counter the lack of accurate analytical models. However, the systematic formulation and subsequent verification/validation in dynamic operation regimes (particularly for skid-steered vehicles) remains a work in progress. To this end, a novel approach for structured formulation for learning visual navigation is proposed and investigated in this work. Extensive software simulations, hardware evaluations and ablation studies now highlight the significantly improved performance of the proposed approach against contemporary literature. 

**Abstract (ZH)**: 基于视觉的车道保持是机器人学和自主地面车辆社区在各种道路和非道路应用场景中的一个重要研究课题。滑移转向车架构为人类操作提供了一个有用的车辆平台。然而，尤其是对于非道路环境中的滑移打滑车轮地形交互的系统建模成为自动化部署的瓶颈。基于端到端学习的方法，如模仿学习和深度强化学习，已成为应对缺乏准确分析模型的可行部署选项。然而，滑移转向车辆在动态操作制度下的系统建模和后续验证仍然有待完善。为此，本文提出并探讨了一种新型结构化学习视觉导航的方法。大量的软件仿真、硬件评估和消融研究现在表明，所提出方法在当代文献中显著提高了性能。 

---
# Ark: An Open-source Python-based Framework for Robot Learning 

**Title (ZH)**: Ark：一种基于Python的机器人学习开源框架 

**Authors**: Magnus Dierking, Christopher E. Mower, Sarthak Das, Huang Helong, Jiacheng Qiu, Cody Reading, Wei Chen, Huidong Liang, Huang Guowei, Jan Peters, Quan Xingyue, Jun Wang, Haitham Bou-Ammar  

**Link**: [PDF](https://arxiv.org/pdf/2506.21628)  

**Abstract**: Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics Challenges to the first humanoid-robot kickboxing tournament-yet commercial autonomy still lags behind progress in machine learning. A major bottleneck is software: current robot stacks demand steep learning curves, low-level C/C++ expertise, fragmented tooling, and intricate hardware integration, in stark contrast to the Python-centric, well-documented ecosystems that propelled modern AI. We introduce ARK, an open-source, Python-first robotics framework designed to close that gap. ARK presents a Gym-style environment interface that allows users to collect data, preprocess it, and train policies using state-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy) while seamlessly toggling between high-fidelity simulation and physical robots. A lightweight client-server architecture provides networked publisher-subscriber communication, and optional C/C++ bindings ensure real-time performance when needed. ARK ships with reusable modules for control, SLAM, motion planning, system identification, and visualization, along with native ROS interoperability. Comprehensive documentation and case studies-from manipulation to mobile navigation-demonstrate rapid prototyping, effortless hardware swapping, and end-to-end pipelines that rival the convenience of mainstream machine-learning workflows. By unifying robotics and AI practices under a common Python umbrella, ARK lowers entry barriers and accelerates research and commercial deployment of autonomous robots. 

**Abstract (ZH)**: ARK：一个以Python为先的开源机器人框架 

---
# FrankenBot: Brain-Morphic Modular Orchestration for Robotic Manipulation with Vision-Language Models 

**Title (ZH)**: FrankenBot：类脑模块化 orchestration 及视觉-语言模型在机器人操作中的应用 

**Authors**: Shiyi Wang, Wenbo Li, Yiteng Chen, Qingyao Wu, Huiping Zhuang  

**Link**: [PDF](https://arxiv.org/pdf/2506.21627)  

**Abstract**: Developing a general robot manipulation system capable of performing a wide range of tasks in complex, dynamic, and unstructured real-world environments has long been a challenging task. It is widely recognized that achieving human-like efficiency and robustness manipulation requires the robotic brain to integrate a comprehensive set of functions, such as task planning, policy generation, anomaly monitoring and handling, and long-term memory, achieving high-efficiency operation across all functions. Vision-Language Models (VLMs), pretrained on massive multimodal data, have acquired rich world knowledge, exhibiting exceptional scene understanding and multimodal reasoning capabilities. However, existing methods typically focus on realizing only a single function or a subset of functions within the robotic brain, without integrating them into a unified cognitive architecture. Inspired by a divide-and-conquer strategy and the architecture of the human brain, we propose FrankenBot, a VLM-driven, brain-morphic robotic manipulation framework that achieves both comprehensive functionality and high operational efficiency. Our framework includes a suite of components, decoupling a part of key functions from frequent VLM calls, striking an optimal balance between functional completeness and system efficiency. Specifically, we map task planning, policy generation, memory management, and low-level interfacing to the cortex, cerebellum, temporal lobe-hippocampus complex, and brainstem, respectively, and design efficient coordination mechanisms for the modules. We conducted comprehensive experiments in both simulation and real-world robotic environments, demonstrating that our method offers significant advantages in anomaly detection and handling, long-term memory, operational efficiency, and stability -- all without requiring any fine-tuning or retraining. 

**Abstract (ZH)**: 开发能够在复杂、动态且未结构化的现实环境中执行广泛任务的通用机器人操作系统长久以来是一项具有挑战性的任务。广泛认可的观点认为，实现类似人类的效率和鲁棒性操作需要机器人“大脑”整合一系列功能，如任务规划、策略生成、异常监测与处理以及长期记忆，从而在所有功能上实现高效操作。视觉-语言模型（VLMs）在大规模多模态数据上进行了预训练，获得了丰富的世界知识，展示了卓越的场景理解和多模态推理能力。然而，现有方法通常仅专注于实现机器人“大脑”内的一项或多项功能，而不将它们整合到统一的认知架构中。受分而治之策略和人脑架构的启发，我们提出了一种由VLM驱动、模仿人脑结构的机器人操作框架——FrankenBot，该框架实现了全面的功能性和高操作效率。该框架包括一系列组件，从频繁的VLM调用中解耦出部分关键功能，平衡了功能完整性和系统效率之间的关系。具体而言，我们将任务规划、策略生成、记忆管理以及低级别接口分别映射到大脑皮层、小脑、颞叶-海马复合体和延髓，并设计了高效协调机制以优化模块之间的协作。我们在仿真和真实世界机器人环境中进行了全面的实验，结果表明，我们的方法在异常检测与处理、长期记忆、操作效率和稳定性方面具有显著优势——无需任何微调或重新训练。 

---
# Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation 

**Title (ZH)**: 基于物理 informant 符号程序先验的零样本无线室内导航强化学习 

**Authors**: Tao Li, Haozhe Lei, Mingsheng Yin, Yaqi Hu  

**Link**: [PDF](https://arxiv.org/pdf/2506.22365)  

**Abstract**: When using reinforcement learning (RL) to tackle physical control tasks, inductive biases that encode physics priors can help improve sample efficiency during training and enhance generalization in testing. However, the current practice of incorporating these helpful physics-informed inductive biases inevitably runs into significant manual labor and domain expertise, making them prohibitive for general users. This work explores a symbolic approach to distill physics-informed inductive biases into RL agents, where the physics priors are expressed in a domain-specific language (DSL) that is human-readable and naturally explainable. Yet, the DSL priors do not translate directly into an implementable policy due to partial and noisy observations and additional physical constraints in navigation tasks. To address this gap, we develop a physics-informed program-guided RL (PiPRL) framework with applications to indoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic integration, where a meta symbolic program receives semantically meaningful features from a neural perception module, which form the bases for symbolic programming that encodes physics priors and guides the RL process of a low-level neural controller. Extensive experiments demonstrate that PiPRL consistently outperforms purely symbolic or neural policies and reduces training time by over 26% with the help of the program-based inductive biases. 

**Abstract (ZH)**: 基于物理信息程序引导的强化学习（PiPRL）方法在室内导航中的应用 

---
# SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model 

**Title (ZH)**: SceneDiffuser++: 城市规模交通仿真通过生成世界模型 

**Authors**: Shuhan Tan, John Lambert, Hong Jeon, Sakshum Kulshrestha, Yijing Bai, Jing Luo, Dragomir Anguelov, Mingxing Tan, Chiyu Max Jiang  

**Link**: [PDF](https://arxiv.org/pdf/2506.21976)  

**Abstract**: The goal of traffic simulation is to augment a potentially limited amount of manually-driven miles that is available for testing and validation, with a much larger amount of simulated synthetic miles. The culmination of this vision would be a generative simulated city, where given a map of the city and an autonomous vehicle (AV) software stack, the simulator can seamlessly simulate the trip from point A to point B by populating the city around the AV and controlling all aspects of the scene, from animating the dynamic agents (e.g., vehicles, pedestrians) to controlling the traffic light states. We refer to this vision as CitySim, which requires an agglomeration of simulation technologies: scene generation to populate the initial scene, agent behavior modeling to animate the scene, occlusion reasoning, dynamic scene generation to seamlessly spawn and remove agents, and environment simulation for factors such as traffic lights. While some key technologies have been separately studied in various works, others such as dynamic scene generation and environment simulation have received less attention in the research community. We propose SceneDiffuser++, the first end-to-end generative world model trained on a single loss function capable of point A-to-B simulation on a city scale integrating all the requirements above. We demonstrate the city-scale traffic simulation capability of SceneDiffuser++ and study its superior realism under long simulation conditions. We evaluate the simulation quality on an augmented version of the Waymo Open Motion Dataset (WOMD) with larger map regions to support trip-level simulation. 

**Abstract (ZH)**: 交通模拟的目标是用大量的模拟合成英里来补充有限的手动驾驶英里，以供测试和验证使用。这一愿景的最终结果是一个生成的模拟城市，在给定城市地图和自动驾驶车辆（AV）软件栈的情况下，模拟器可以通过填充城市中的自动驾驶车辆并控制场景的所有方面，从动画动态代理（如车辆、行人）到控制交通灯状态，无缝地模拟从A点到B点的旅程。我们将其称为CitySim，这需要综合各种模拟技术：场景生成以填充初始场景、代理行为建模以动画场景、遮挡推理、动态场景生成以无缝生成和移除代理，以及环境模拟以考虑如交通灯等因素。虽然一些关键技术已在各种研究中分别进行过研究，但其他技术如动态场景生成和环境模拟在研究社区中受到的关注较少。我们提出了SceneDiffuser++，这是一个通过单一损失函数训练的第一个端到端生成世界模型，能够在城市尺度上集成所有上述要求进行A到B的模拟。我们展示了SceneDiffuser++的城市规模交通模拟能力，并在长时间模拟条件下研究了其更出色的现实感。我们使用增强后的Waymo Open Motion数据集（WOMD）进行评估，该数据集支持更大地图区域以支持行程级模拟。 

---
# Embodied AI Agents: Modeling the World 

**Title (ZH)**: 具身人工智能代理：建模世界 

**Authors**: Pascale Fung, Yoram Bachrach, Asli Celikyilmaz, Kamalika Chaudhuri, Delong Chen, Willy Chung, Emmanuel Dupoux, Hervé Jégou, Alessandro Lazaric, Arjun Majumdar, Andrea Madotto, Franziska Meier, Florian Metze, Théo Moutakanni, Juan Pino, Basile Terver, Joseph Tighe, Jitendra Malik  

**Link**: [PDF](https://arxiv.org/pdf/2506.22355)  

**Abstract**: This paper describes our research on AI agents embodied in visual, virtual or physical forms, enabling them to interact with both users and their environments. These agents, which include virtual avatars, wearable devices, and robots, are designed to perceive, learn and act within their surroundings, which makes them more similar to how humans learn and interact with the environments as compared to disembodied agents. We propose that the development of world models is central to reasoning and planning of embodied AI agents, allowing these agents to understand and predict their environment, to understand user intentions and social contexts, thereby enhancing their ability to perform complex tasks autonomously. World modeling encompasses the integration of multimodal perception, planning through reasoning for action and control, and memory to create a comprehensive understanding of the physical world. Beyond the physical world, we also propose to learn the mental world model of users to enable better human-agent collaboration. 

**Abstract (ZH)**: 本文描述了我们在以视觉、虚拟或物理形式体现的AI代理方面的研究，使它们能够与用户及其环境进行交互。这些代理包括虚拟化身、穿戴设备和机器人，设计用于感知、学习和在其环境中的行动，使其在与人类学习和与环境交互的方式上更为相似，而不是 disembodied 代理。我们提出，世界模型的发展对于体现的AI代理的推理和规划至关重要，这使这些代理能够理解并预测其环境，理解用户意图和社会背景，从而增强其执行复杂任务的能力。世界模型涵盖了多模态感知的综合、通过推理进行计划以实现动作控制、以及记忆的运用，从而创建对物理世界的全面理解。此外，我们还提出学习用户的心智世界模型，以实现更好的人机协作。 

---
# Artificial Intelligent Disobedience: Rethinking the Agency of Our Artificial Teammates 

**Title (ZH)**: 人工智能的叛逆：重思我们的机器同伴的自主性 

**Authors**: Reuth Mirsky  

**Link**: [PDF](https://arxiv.org/pdf/2506.22276)  

**Abstract**: Artificial intelligence has made remarkable strides in recent years, achieving superhuman performance across a wide range of tasks. Yet despite these advances, most cooperative AI systems remain rigidly obedient, designed to follow human instructions without question and conform to user expectations, even when doing so may be counterproductive or unsafe. This paper argues for expanding the agency of AI teammates to include \textit{intelligent disobedience}, empowering them to make meaningful and autonomous contributions within human-AI teams. It introduces a scale of AI agency levels and uses representative examples to highlight the importance and growing necessity of treating AI autonomy as an independent research focus in cooperative settings. The paper then explores how intelligent disobedience manifests across different autonomy levels and concludes by proposing initial boundaries and considerations for studying disobedience as a core capability of artificial agents. 

**Abstract (ZH)**: 人工智能在近年来取得了显著进展，实现了跨多种任务的超human表现。然而，尽管取得了这些进步，大多数协作性AI系统仍然僵化地顺从，被设计为未经质疑地遵循人类指令并符合用户期望，即使这样做可能是无效的或不安全的。本文主张扩大AI队友的自主权限，使其包括“智能不服从”，使其能够在人机团队中做出有意义且自主的贡献。文章介绍了一个AI自主权等级量表，并通过典型示例强调了在协作环境中将AI自主性作为独立研究重点的重要性以及其日益增长的必要性。随后，文章探讨了智能不服从在不同自主权级别上的表现，并提出了一些初步的研究边界和考虑，以将不服从作为人工代理的核心能力进行研究。 

---
# SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents 

**Title (ZH)**: SEEA-R1: 基于树结构强化细调的自进化体化代理 

**Authors**: Wanxin Tian, Shijie Zhang, Kevin Zhang, Xiaowei Chi, Yulin Luo, Junyu Lu, Chunkai Fan, Qiang Zhou, Yiming Zhao, Ning Liu Siyu Lin, Zhiyuan Qin, Xiaozhu Ju, Shanghang Zhang, Jian Tang  

**Link**: [PDF](https://arxiv.org/pdf/2506.21669)  

**Abstract**: Self-evolution, the ability of agents to autonomously improve their reasoning and behavior, is essential for the embodied domain with long-horizon, real-world tasks. Despite current advancements in reinforcement fine-tuning (RFT) showing strong performance in enhancing reasoning in LLMs, its potential to enable self-evolving embodied intelligence with multi-modal interactions remains largely unexplored. Specifically, reinforcement fine-tuning faces two fundamental obstacles in embodied settings: (i) the lack of accessible intermediate rewards in multi-step reasoning tasks limits effective learning signals, and (ii) reliance on hand-crafted reward functions restricts generalization to novel tasks and environments. To address these challenges, we present Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework designed for enabling the self-evolving capabilities of embodied agents. Specifically, to convert sparse delayed rewards into denser intermediate signals that improve multi-step reasoning, we propose Tree-based group relative policy optimization (Tree-GRPO), which integrates Monte Carlo Tree Search into GRPO. To generalize reward estimation across tasks and scenes, supporting autonomous adaptation and reward-driven self-evolution, we further introduce Multi-modal Generative Reward Model (MGRM). To holistically evaluate the effectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing state-of-the-art methods with scores of 85.07% (textual) and 36.19% (multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also achieves scores of 80.3% without environmental reward, surpassing all open-source baselines and highlighting its scalability as a self-evolving embodied agent. Additional experiments and qualitative analysis further support the potential of SEEA-R1 for future research in scalable embodied intelligence. 

**Abstract (ZH)**: 自进化能力：使代理能够在长时 horizon、真实世界任务中自主提升其推理和行为的能力对于体现域至关重要。尽管当前强化微调（RFT）在增强大语言模型的推理能力方面表现出色，但其在多模态交互中使代理具备自进化能力方面的潜力尚未得到充分探索。具体来说，强化微调在体现环境中面临两个根本障碍：（i）多步推理任务中缺乏可访问的中间奖励限制了有效学习信号，（ii）依赖于手工设计的奖励函数限制了对新任务和环境的泛化能力。为解决这些挑战，我们提出了Self-Evolving Embodied Agents-R1（SEEA-R1），这是第一个为使体现代理具备自进化能力设计的RFT框架。具体而言，为将稀疏的延迟奖励转换为改善多步推理的更密集的中间信号，我们提出了基于树结构的分组相对策略优化（Tree-GRPO），该方法将蒙特卡ロ树搜索整合到GRPO中。为了跨任务和场景泛化奖励估计，支持自主适应和奖励驱动的自进化，我们进一步引入了多模态生成奖励模型（MGRM）。为了全面评估SEEA-R1的有效性，我们在ALFWorld基准上进行评估，得分分别为85.07%（文本）和36.19%（多模态），超越了最先进的方法，并优于包括GPT-4o在内的先前模型。SEEA-R1在无环境奖励的情况下得分80.3%，超越所有开源基线，突显了其作为自进化体现代理的可扩展性。附加实验和定性分析进一步支持了SEEA-R1在可扩展体现智能方面的研究潜力。 

---
# Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation 

**Title (ZH)**: 视觉-语言模型拥有内部世界模型吗？向原子化评估迈进 

**Authors**: Qiyue Gao, Xinyu Pi, Kevin Liu, Junrong Chen, Ruolan Yang, Xinqi Huang, Xinyu Fang, Lu Sun, Gautham Kishore, Bo Ai, Stone Tao, Mengyang Liu, Jiaxi Yang, Chao-Jung Lai, Chuanyang Jin, Jiannan Xiang, Benhao Huang, Zeming Chen, David Danks, Hao Su, Tianmin Shu, Ziqiao Ma, Lianhui Qin, Zhiting Hu  

**Link**: [PDF](https://arxiv.org/pdf/2506.21876)  

**Abstract**: Internal world models (WMs) enable agents to understand the world's state and predict transitions, serving as the basis for advanced deliberative reasoning. Recent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and Gemini, exhibit potential as general-purpose WMs. While the latest studies have evaluated and shown limitations in specific capabilities such as visual understanding, a systematic evaluation of VLMs' fundamental WM abilities remains absent. Drawing on comparative psychology and cognitive science, we propose a two-stage framework that assesses Perception (visual, spatial, temporal, quantitative, and motion) and Prediction (mechanistic simulation, transitive inference, compositional inference) to provide an atomic evaluation of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse simulated environments with controlled counterfactual simulations. Through 660 experiments on 15 latest commercial and open-source VLMs, we find that these models exhibit striking limitations in basic world modeling abilities. For instance, almost all models perform at near-random accuracy when distinguishing motion trajectories. Additionally, they lack disentangled understanding -- e.g., some models tend to believe blue objects move faster than green ones. More rich results and analyses reveal significant gaps between VLMs and human-level world modeling. 

**Abstract (ZH)**: 基于内部世界模型的Vision-Language模型系统的原子性评估：从感知到预测 

---
# A Survey of Continual Reinforcement Learning 

**Title (ZH)**: 持续强化学习综述 

**Authors**: Chaofan Pan, Xin Yang, Yanhua Li, Wei Wei, Tianrui Li, Bo An, Jiye Liang  

**Link**: [PDF](https://arxiv.org/pdf/2506.21872)  

**Abstract**: Reinforcement Learning (RL) is an important machine learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in this field due to the rapid development of deep neural networks. However, the success of RL currently relies on extensive training data and computational resources. In addition, RL's limited ability to generalize across tasks restricts its applicability in dynamic and real-world environments. With the arisen of Continual Learning (CL), Continual Reinforcement Learning (CRL) has emerged as a promising research direction to address these limitations by enabling agents to learn continuously, adapt to new tasks, and retain previously acquired knowledge. In this survey, we provide a comprehensive examination of CRL, focusing on its core concepts, challenges, and methodologies. Firstly, we conduct a detailed review of existing works, organizing and analyzing their metrics, tasks, benchmarks, and scenario settings. Secondly, we propose a new taxonomy of CRL methods, categorizing them into four types from the perspective of knowledge storage and/or transfer. Finally, our analysis highlights the unique challenges of CRL and provides practical insights into future directions. 

**Abstract (ZH)**: 持续强化学习（CRL）：核心概念、挑战与方法綜述 

---
